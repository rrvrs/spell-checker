<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006443</article-id><article-id pub-id-type="pmc">PMC11860463</article-id><article-id pub-id-type="doi">10.3390/s25041215</article-id><article-id pub-id-type="publisher-id">sensors-25-01215</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Fully Incomplete Information for Multiview Clustering in Postoperative Liver Tumor Diagnoses</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-0226-7129</contrib-id><name><surname>Li</surname><given-names>Siyuan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-sensors-25-01215" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Xinde</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><xref rid="af1-sensors-25-01215" ref-type="aff">1</xref><xref rid="af2-sensors-25-01215" ref-type="aff">2</xref><xref rid="af3-sensors-25-01215" ref-type="aff">3</xref><xref rid="af4-sensors-25-01215" ref-type="aff">4</xref><xref rid="c1-sensors-25-01215" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Bevilacqua</surname><given-names>Alessandro</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01215"><label>1</label>School of Cyber Science and Engineering, Southeast University, Nanjing 211100, China; <email>lisiyuan1011@163.com</email></aff><aff id="af2-sensors-25-01215"><label>2</label>Key Laboratory of Measurement and Control of CSE, School of Automation, Southeast University, Nanjing 210018, China</aff><aff id="af3-sensors-25-01215"><label>3</label>Southeast University Shenzhen Research Institute, Shenzhen 518063, China</aff><aff id="af4-sensors-25-01215"><label>4</label>Nanjing Center for Applied Mathematics, Nanjing 211135, China</aff><author-notes><corresp id="c1-sensors-25-01215"><label>*</label>Correspondence: <email>xindeli@seu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>17</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1215</elocation-id><history><date date-type="received"><day>08</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>13</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>14</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Multiview clustering (MVC) is a proven, effective approach to boosting the various downstream tasks given by unlabeled data. In contemporary society, domain-specific multiview data, such as multiphase postoperative liver tumor contrast-enhanced computed tomography (CECT) images, may be vulnerable to exploitation by illicit organizations or may not be comprehensively collected due to patient privacy concerns. Thus, these can be modeled as incomplete multiview clustering (IMVC) problems. Most existing IMVC methods have three issues: (1) most methods rely on paired views, which are often unavailable in clinical practice; (2) directly predicting the features of missing views may omit key features; and (3) recovered views still have subtle differences from the originals. To overcome these challenges, we proposed a novel framework named fuzzy clustering combined with information theory arithmetic based on feature reconstruction (FCITAFR). Specifically, we propose a method for reconstructing the characteristics of prevailing perspectives for each sample. Based on this, we utilized the reconstructed features to predict the missing views. Then, based on the predicted features, we used variational fuzzy c-means clustering (FCM) combined with information theory to learn the mutual information among views. The experimental results indicate the advantages of FCITAFR in comparison to state-of-the-art methods, on both in-house and external datasets, in terms of accuracy (ACC) (77.5%), normalized mutual information (NMI) (37.9%), and adjusted rand index (ARI) (29.5%).</p></abstract><kwd-group><kwd>incomplete multiview clustering</kwd><kwd>feature reconstruction</kwd><kwd>fuzzy clustering</kwd><kwd>information theory</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62233003</award-id><award-id>62073072</award-id></award-group><award-group><funding-source>Key Projects of the Key R&#x00026;D Program of Jiangsu Province</funding-source><award-id>BE2020006</award-id><award-id>BE2020006-1</award-id></award-group><award-group><funding-source>Shenzhen Science and Technology Program</funding-source><award-id>JCYJ20210324132202005</award-id><award-id>JCYJ20220818101206014</award-id></award-group><funding-statement>This work was supported by the National Natural Science Foundation of China under grants 62233003 and 62073072, the Key Projects of the Key R&#x00026;D Program of Jiangsu Province under grants BE2020006 and BE2020006-1, and the Shenzhen Science and Technology Program under grants JCYJ20210324132202005 and JCYJ20220818101206014.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01215"><title>1. Introduction</title><p>Liver tumors represent a significant contributor to mortality and are associated with various diseases, posing a considerable threat to human health [<xref rid="B1-sensors-25-01215" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01215" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01215" ref-type="bibr">3</xref>]. A widely utilized method for the postoperative diagnosis of liver tumors is multiphase contrast-enhanced computed tomography (CECT) [<xref rid="B4-sensors-25-01215" ref-type="bibr">4</xref>]. Compared with single-view methods, in clinical practice, multiphase views provide consistent and complementary information from the same patient [<xref rid="B5-sensors-25-01215" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01215" ref-type="bibr">6</xref>], thus providing more precision regarding postoperative liver tumor type. Liver tumor CECT images of a patient can be divided into four phases: non-contrast (NC), arterial phase (AP), portal venous (PV), and delay phase (DP) [<xref rid="B7-sensors-25-01215" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01215" ref-type="bibr">8</xref>]. Judging tumor type from CECT images is a laborious and time-wasting task for clinicians, meaning that a large number of these images are not &#x0201c;gold standard&#x0201d;. Moreover, some medical workers do not have enough experience. Hence, to further utilize such unlabeled multiview data, it is essential to introduce an unsupervised method, such as multiview clustering (MVC) [<xref rid="B9-sensors-25-01215" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01215" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01215" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01215" ref-type="bibr">12</xref>]. Unfortunately, due to data theft and patient privacy, it is hard to collect complete data during transmission.</p><p>To solve the above-mentioned problems, plenty of clinical scenarios have been pro- posed with the aim of exploring information from incomplete data without labeling, which are set as incomplete multiview clustering [<xref rid="B13-sensors-25-01215" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01215" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01215" ref-type="bibr">15</xref>] (IMVC). IMVC aims to address the issue of heterogeneous features across complete and incomplete views, where the main insight is to predict missing view features. There are two categories of IMVC methods in clinical diagnosis: traditional and deep learning IMVC. Traditional IMVC methods can be categorized as multi-kernel learning [<xref rid="B16-sensors-25-01215" ref-type="bibr">16</xref>], matrix factorization [<xref rid="B17-sensors-25-01215" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01215" ref-type="bibr">18</xref>], tensor factorization [<xref rid="B19-sensors-25-01215" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01215" ref-type="bibr">20</xref>] or graph learning [<xref rid="B21-sensors-25-01215" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01215" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01215" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-01215" ref-type="bibr">24</xref>]. These methods are restricted in obtaining latent features, which increase computationally complex calculations. Other than traditional IMVC methods, deep IMVC methods make rich use of the latent features to achieve missing view reconstruction [<xref rid="B25-sensors-25-01215" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01215" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01215" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-01215" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-01215" ref-type="bibr">29</xref>]. Although these methods present acceptable performances, the following issues remain:<list list-type="bullet"><list-item><p>These methods rely significantly on complete instances (both views exist), which are often unavailable in clinical practice. For instance, in postoperative liver tumor diagnosis, few patients willingly provide complete views, making it hard to obtain complete samples.</p></list-item><list-item><p>Direct feature prediction may not highlight key features; then, easily predicted error features affect the clustering performance [<xref rid="B25-sensors-25-01215" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01215" ref-type="bibr">26</xref>,<xref rid="B30-sensors-25-01215" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-01215" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01215" ref-type="bibr">32</xref>], especially in the absence of paired samples.</p></list-item><list-item><p>Due to noise interference, recovered missing views still have subtle difference from the originals [<xref rid="B33-sensors-25-01215" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-01215" ref-type="bibr">34</xref>]. With clustering tasks without labels, subtle differences may also lead to error clusters in samples.</p></list-item></list></p><p>Thus, it is hard to accomplish multiview clustering with fully incomplete information and in all instances missing at least one view.</p><p>In this study, we introduce a framework called fuzzy clustering combined with information theory arithmetic based on feature reconstruction (FCITAFR), as indicated in <xref rid="sensors-25-01215-f001" ref-type="fig">Figure 1</xref>. The framework primarily comprises two modules: a feature reconstruction prediction module and a fuzzy clustering contrast learning module. More specifically, we firstly reconstruct and correlate features of the existing views for all samples, aiming to optimize the latent features and enhance the relations between them. Based on the reconstructed features of views, we utilize a variational fuzzy c-means clustering (FCM) arithmetic to learn the membership among samples and then combine this with information theory [<xref rid="B31-sensors-25-01215" ref-type="bibr">31</xref>]. We obtained the Euclidean distance among views and combined this with the membership matrix to obtain the similarity between single views. Then, we obtained the fusion similarity between different views combined with information theory, thus ascertaining consistence information among views. The major contributions of our work are as follows:<list list-type="bullet"><list-item><p>Different from existing predicting methods, we reconstruct the features of existing views to enhance the relations among features.</p></list-item><list-item><p>We propose a novel arithmetic that utilizes variational FCM arithmetic combined with information theory to learn the mutual information among views.</p></list-item><list-item><p>The experimental results from two multiphase liver tumor datasets demonstrate that FCITAFR outperforms the state-of-the-art IMVC methods in terms of ACC (77.5%), NMI (37.9%), and ARI (29.5%).</p></list-item></list></p></sec><sec id="sec2-sensors-25-01215"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-01215"><title>2.1. IMVC Methods</title><p>Traditional IMVC methods are categorized as multiply kernel learning, graph learning, and tensor factorization. By using the complete part of the datasets, the multiply kernel IMVC method creates the kernel matrix of the incomplete part. In order to regularize the clustering matrix, Liu et al. [<xref rid="B35-sensors-25-01215" ref-type="bibr">35</xref>] integrated past information and gathered each imperfect base matrix produced by missing views with a clustering matrix. Graph learning-based IMVC methods gather patterns by extracting graph structure information. Chao et al. [<xref rid="B24-sensors-25-01215" ref-type="bibr">24</xref>] proposed instance-level fusion and high-confident guiding to obtain complementary information; then, instance-level contrastive learning was utilized to obtain consistent information. Tensor factorization-based IMVC methods seek to incorporate tensor constraints to characterize the high-order correlations among tensors and elucidate the internal structure associated with cross-view data. Li et al. [<xref rid="B36-sensors-25-01215" ref-type="bibr">36</xref>] proposed an IMVC method based on tensors to disperse multiview block-diagonal structure knowledge among views.</p><p>Deep IMVC methods, of which contrastive learning is one of the most significant, can learn the deep features of views. Xu et al. [<xref rid="B30-sensors-25-01215" ref-type="bibr">30</xref>] proposed an adaptive feature projection-based deep IMVC method, where the auto-encoders were modeled to reconstruct missing data. Lin et al. [<xref rid="B31-sensors-25-01215" ref-type="bibr">31</xref>] proposed a deep contrastive and dual-prediction framework to learn the consistence mutual information among different views via contrastive learning and minimized conditional entropy. Zhang et al. [<xref rid="B29-sensors-25-01215" ref-type="bibr">29</xref>] proposed a self-attention-based encoder network and obtained common information by learning feature sub-vectors.</p></sec><sec id="sec2dot2-sensors-25-01215"><title>2.2. IMVC for Medical Image Analysis</title><p>In clinical practice, the numbers of paired samples are seriously limited in multiview learning. Some methods remove incomplete samples from the dataset [<xref rid="B26-sensors-25-01215" ref-type="bibr">26</xref>]; however, removing data from medical datasets is not optimal, since incomplete samples still possess valuable information. Recently, recovering missing data is the mainstream IMVC method in medical practice. In terms of traditional methods, Wen et al. [<xref rid="B16-sensors-25-01215" ref-type="bibr">16</xref>] proposed a framework for authenticating involuntary multiple muscle tics in children based on multiview (instance) and multiclass (clusters) features. Peng et al. [<xref rid="B17-sensors-25-01215" ref-type="bibr">17</xref>] proposed a group sparse algorithm to combine non-negative matrix factorization and an orthogonal subspace. Different from traditional methods, deep learning methods aim to use encoders to represent the latent features of views, learning the deep relationships between them. Pan et al. [<xref rid="B37-sensors-25-01215" ref-type="bibr">37</xref>] proposed a statement framework based on the space constraint for diagnosing brain diseases from incomplete multiview neuroimages, addressing the lack of positron emission tomography (PET) in patients with only brain MRI.</p><p>Nevertheless, these methods may have more limitations. Although the key idea of other methods has directly predicted the features of missing views, they may ignore some features and make the missing view as a wrong clustering. In order to solve this problem, the motivation is to design an arithmetic to reconstruct the features of instances, which can adequately reconstruct the features that are easy to ignore. Different from the aforementioned arithmetic, FCITAFR reconstructs the features of existing views before predicting the features of missing views. Moreover, we utilize the fuzzy clustering arithmetic combined with the information theory to learn the common features of different views after they are predicted.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-01215"><title>3. Methodology</title><sec id="sec3dot1-sensors-25-01215"><title>3.1. Notations</title><p>To facilitate the discussion, we let the number of views be 2. <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th view with <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> being the number of instances, and each has <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> dimensions. <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are denoted as the <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> view with <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> being the number of samples, and <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> being the dimensions in latent spaces and feature reconstruction, respectively. Here, further notations and descriptions are listed in <xref rid="sensors-25-01215-t001" ref-type="table">Table 1</xref>.</p><p>Based on the aforementioned definitions, we present the objective&#x02019;s comprehensive loss function:<disp-formula id="FD1-sensors-25-01215"><label>(1)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are fuzzy clustering with information loss, within-view reconstruction loss, and feature reconstruction prediction loss, respectively. The parameters <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are balance parameters of loss functions; initially, we simply set them as 0.1.</p></sec><sec id="sec3dot2-sensors-25-01215"><title>3.2. Within-View Reconstruction</title><p>We pass all views through autoencoders to obtain the latent feature representation <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> by<disp-formula id="FD2-sensors-25-01215"><label>(2)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th sample in <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Meanwhile, the latent representation in the <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th view for all instances is given by<disp-formula id="FD3-sensors-25-01215"><label>(3)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec3dot3-sensors-25-01215"><title>3.3. Feature Reconstruction Prediction</title><p>Based on the latent representation <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> obtained by <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, we propose a methodology for reconstructing the characteristics of samples, seeking to take into account the interrelationships between features and among various samples within a singular perspective. The primary objective is to determine the weights of all features in accordance with the specified indicators [<xref rid="B38-sensors-25-01215" ref-type="bibr">38</xref>]. When acquiring latent representations from each perspective, the latent spaces encompass various samples and distinct characteristics. More specific, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> consists of <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> samples, with each sample characterized by <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> features. The <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th sample indicates <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and the <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th feature indicates <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. We employ the entropy weighting method to analyze the obtained latent features, associating them with each unique characteristic of every sample, and subsequently generate a novel latent representation.</p><p>Firstly, for the feature of each sample, we decline the difference of the distribution. We gathered the average of features for all samples, and obtained the distribution matrix <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> as follows:<disp-formula id="FD4-sensors-25-01215"><label>(4)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mstyle><mml:mtext>&#x000a0;</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> donates the average of features for all samples.</p><p>Next, we analyzed the relationships between features across various samples, specifically focusing on the <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th column of <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. We computed the entropy value <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to integrate the different features:<disp-formula id="FD5-sensors-25-01215"><label>(5)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> donates the averages of all samples for all corresponding features.</p><p>In conclusion, it is essential to calculate the entropy coefficient <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for each column and subsequently multiply the aggregate coefficients by the initial latent representation <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The formulas for these calculations are as follows:<disp-formula id="FD6-sensors-25-01215"><label>(6)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>;</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-01215"><label>(7)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>After obtaining the reconstructed latent representation <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, we utilized generators to predict the missing views. Our main task was to connect all features of each sample. In order to predict the features of the other view, the importance is not to ignore the key features. The loss function <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained as follows:<disp-formula id="FD8-sensors-25-01215"><label>(8)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are generators with a multilayer, fully-connected network.</p></sec><sec id="sec3dot4-sensors-25-01215"><title>3.4. Fuzzy Clustering with Information Theory</title><p>To learn more mutual information among views, we used variational fuzzy c-means clustering (FCM) [<xref rid="B39-sensors-25-01215" ref-type="bibr">39</xref>], combined with information theory [<xref rid="B31-sensors-25-01215" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01215" ref-type="bibr">32</xref>], to obtain the relationship among samples and obtain the fusion similarity between views. We used all samples as the clustering centers to determine the degree of membership for the other samples. First, we obtained the similarity matrix <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> of single views, which is formulated as follows:<disp-formula id="FD9-sensors-25-01215"><label>(9)</label><mml:math id="mm57" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mtext>&#x000a0;</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>1,2</mml:mn><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In this function, <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> donates the time complexity matrix of the algorithm, which is calculated by <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for each view. <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> donates the feature matrix of each view after feature reconstruction, which consists of <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mtext>&#x000a0;</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>number of samples and, each sample has <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> features. In order to determine the similarity among all samples in each view, we calculated the feature matrix (<inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) times the transposed feature matrix (<inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). As a result, we can obtain the time complexity matrix. And <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Euclidean distance between the <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th and <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th samples <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are variational samples. <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> donates the membership matrix, which stands for the degree of membership among samples; a larger number means more similarity. Then, the membership matrix among views is calculated as follows:<disp-formula id="FD10-sensors-25-01215"><label>(10)</label><mml:math id="mm71" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the membership factor, which is set as 2 in this study.</p><p>Subsequently, we enhanced the mutual information between <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> by integrating it with information theory. The final layer of the encoder, which is based on the softmax function [<xref rid="B33-sensors-25-01215" ref-type="bibr">33</xref>], can be understood as representing a probability distribution that facilitates clustering. Thus, the cross-view joint probability distribution matrix <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo mathvariant="bold-italic">&#x02032;</mml:mo></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated by <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo mathvariant="bold-italic">&#x02032;</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> become the marginal probability distributions, which denote the <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th row and the <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>-th column on <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The function <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> can be represented as follows:<disp-formula id="FD11-sensors-25-01215"><label>(11)</label><mml:math id="mm82" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo mathvariant="bold-italic">&#x02032;</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b1;</mml:mi><mml:mo mathvariant="bold">+</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msubsup><mml:mo mathvariant="bold">&#x022c5;</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo mathvariant="bold-italic">&#x02032;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b1;</mml:mi><mml:mo mathvariant="bold">+</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> donates the entropy balancing parameter, which is set to 9. The implementation details of FCITAFR are indicated in Algorithm 1.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold> Fuzzy clustering combined with information theory arithmetic based on feature reconstruction (FCITAFR)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Input:</bold> Incomplete dataset <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.<break/><bold>Output:</bold> Total loss <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.<break/>1:&#x000a0;&#x000a0;Initialize overall loss <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, epoch number is <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the number of clusters <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>; the parameters <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.<break/>2:&#x000a0;&#x000a0;<bold>for</bold>
<inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>do</bold><break/>3:&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Compute with-in view reconstruction loss by (2).<break/>4:&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Compute feature reconstruct prediction loss by (8).<break/>5:&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Compute fuzzy clustering with information theory loss by (11).<break/>6:&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Compute total loss by (1) and update balance parameters.<break/>7:&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><break/>8:&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>return</bold>
<inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula><break/>9:&#x000a0;&#x000a0;&#x000a0;&#x000a0;Update parameter <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.<break/>10:&#x000a0;Update parameter <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</td></tr></tbody></array></p></sec></sec><sec id="sec4-sensors-25-01215"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-01215"><title>4.1. Experimental Setting</title><sec id="sec4dot1dot1-sensors-25-01215"><title>4.1.1. Dataset</title><p>We performed experiments using both our internal dataset and external multiview liver tumor CECT dataset, with all volumes obtained from Philips iCT 256 scanners using NC and AP imaging.</p><list list-type="bullet"><list-item><p>Zhongda includes 82 patients with 328 multiphase CT scans obtained from Zhongda Hospital, Southeast University. Each volume has an in-plane dimension of 512 &#x000d7; 512, with spacing varying between 0.601 mm and 0.851 mm. The number of slices in the volumes varies from 36 to 139, with a slice spacing of 2.5 mm.</p></list-item><list-item><p>Zheyi includes 475 patients with 1900 multiphase CT scans obtained from The First Affiliated Hospital of Zhejiang University. Each volume has an in-plane dimension of 512 &#x000d7; 512, with spacing varying between 0.560 mm and 0.847 mm. The number of slices in the volumes varies from 25 to 89, with a slice spacing of 3.0 mm.</p></list-item></list><p>To ensure clustering validity, we retain 55 and 286 instances (for Zhongda and Zheyi, respectively) after removing samples with tiny tumors. These instances are annotated with two levels (M0 and M1) of microvascular invasion (MVI), which is a popular indicator of postoperative liver diagnosis for hepatocellular carcinoma (HCC) [<xref rid="B40-sensors-25-01215" ref-type="bibr">40</xref>]. As shown in <xref rid="sensors-25-01215-f002" ref-type="fig">Figure 2</xref>, for the CECT images that have five instances, and each sample only has one view, which means all patients are missing one view. Here, we introduce the phases of NC and AP as the views for all the methods, with missing rates of <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>10</mml:mn><mml:mo>%</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>30</mml:mn><mml:mo>%</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>50</mml:mn><mml:mo>%</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>70</mml:mn><mml:mo>%</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> used to validate the methods in incomplete multiview scenarios.</p></sec><sec id="sec4dot1dot2-sensors-25-01215"><title>4.1.2. Competitor</title><p>We compare FCITAFR with the state-of-the-art methods as follows:<list list-type="bullet"><list-item><p>SNFR [<xref rid="B29-sensors-25-01215" ref-type="bibr">29</xref>] considers feature reconstruction to learn common information, utilizing information theory to learn the mutual information based on the reconstructed feature.</p></list-item><list-item><p>COMPLETER [<xref rid="B31-sensors-25-01215" ref-type="bibr">31</xref>] learns the common information and constructs a contrastive prediction module to infer missing views.</p></list-item><list-item><p>DCP [<xref rid="B32-sensors-25-01215" ref-type="bibr">32</xref>] extends more than two views based on [<xref rid="B31-sensors-25-01215" ref-type="bibr">31</xref>] and divides these into core-view and complete-view algorithms.</p></list-item><list-item><p>Prolmp [<xref rid="B33-sensors-25-01215" ref-type="bibr">33</xref>] is a dual contrastive learning-based IMVC that learns the relationship between the view-level and instance prototypes.</p></list-item><list-item><p>SURE [<xref rid="B34-sensors-25-01215" ref-type="bibr">34</xref>] considers the view-unaligned and missing problems.</p></list-item></list></p></sec><sec id="sec4dot1dot3-sensors-25-01215"><title>4.1.3. Implementation Details and Metrics</title><p>We implement our approach using PyTorch 1.8.0 on an NVIDIA GeForce RTX 2080Ti GPU (NVIDIA, Santa Clara, CA, USA). The Adam optimizer is employed, with an initial learning rate of 0.0001 applied across all datasets. Initially, we set the batch size to 256 and the training epoch to 500 for all datasets. We set up the two trade-off hyper-parameters <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to 0.1 and 0.1, respectively. The evaluation metrics validate the cluster performances as follows: ACC, NMI, and ARI.</p></sec></sec><sec id="sec4dot2-sensors-25-01215"><title>4.2. Comparison with State-of-the-Art</title><p><xref rid="sensors-25-01215-t002" ref-type="table">Table 2</xref> indicates the clustering results of all methods with the different missing rates <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>10</mml:mn><mml:mo>%</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo>%</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>50</mml:mn><mml:mo>%</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>70</mml:mn><mml:mo>%</mml:mo><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and compares with DCP under a missing rate of 100%. This can explain the clustering performances in different missing rates and highlights better clustering performances in high missing rates compare with competitors. High missing rates are more common in clinical practices, and a robust framework can suit them, where the framework can obtain a better clustering performance in high missing rates, called robust framework. Note that &#x0201c;-&#x0201d; means that the algorithm cannot be operated smoothly due an to out-of-memory error.</p><p>Based on the statistical analysis of the results, the generated observation can be summarized as follows:<list list-type="simple"><list-item><label>(1)</label><p>Compared with other methods, FCITAFR clustering performances can achieve the best results. For instance, in the low missing rates, such as 10%, FCITAFR achieves the best clustering performances on all datasets, where ACC improved 6.1% and 5.7% compared to the second-best method on Zhongda and Zheyi, respectively. Moreover, NMI improved obviously on Zhongda, which improved about 11% compared to the second-best one, and also improved about 3% on Zheyi. Furthermore, ARI improved about 2% and 9% on Zhongda and Zheyi, respectively. On the missing rate of 30%, mostly clustering performances obtained the best one on FCITAFR, especially on Zhongda, which improved about 10% to 20% of all clustering performances compared to the second-best one.</p></list-item><list-item><label>(2)</label><p>FCITAFR has significant advantages. All other deep learning methods, SNFR, COMPLETER, DCP, Prolmp and SURE, directly predict the features of missing views and may predict incorrect features, even with lower missing rates. FCITAFR further reconstructs the features of latent representations of existing views, which obtain the average of all features, and then utilizes the average to recalculate the original features. This informs the obvious efficacy of further reconstruction for the features of latent space.</p></list-item><list-item><label>(3)</label><p>With the increasing missing rate, although the clustering performance of FCITAFR declined, it is still outstanding compared to competitors. For example, on the missing rate of 50%, although some clustering performances obtained the second-best one, ACC improved about 1% on all datasets compared to the second-best method. On the other hand, with the missing rate of 70%, the clustering performances improved about 1% to 2% on Zheyi, and NMI improved about 7.5 on Zhongda compared to the second-best one. Our method suits the case of all instances missing one view. We compared with DCP with the missing rate of 100%, which improved about 2% for all performances compared to DCP.</p></list-item></list></p></sec><sec id="sec4dot3-sensors-25-01215"><title>4.3. Time Cost Comparison</title><p>The running time comparison between the Prolmp [<xref rid="B33-sensors-25-01215" ref-type="bibr">33</xref>] and SURE [<xref rid="B34-sensors-25-01215" ref-type="bibr">34</xref>] are indicated in <xref rid="sensors-25-01215-t003" ref-type="table">Table 3</xref>. As shown in <xref rid="sensors-25-01215-t003" ref-type="table">Table 3</xref>, our method is remarkably efficient compared to Prolmp and SURE on all datasets with the missing rate of 0.5. It may save about 10 s on Zhongda and save 1 min on Zheyi. Note that all methods run on the GPU.</p></sec><sec id="sec4dot4-sensors-25-01215"><title>4.4. Parameter Analysis</title><p>To analyze the impact of trade-off hyper-parameters, we conducted a series of experiments with the missing rate set as 0.3 on Zhongda. <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are set to intervals of <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. As shown in <xref rid="sensors-25-01215-f003" ref-type="fig">Figure 3</xref>, based on the statistical analysis, the following observations can be summarized:
<list list-type="simple"><list-item><label>(1)</label><p>When it is fixed, the performance will be worse when it is too large. For instance, when we set <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> as 0.01, our method can obtain the best performances when <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> fixed as all figures. However, when the parameter <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is increased, the clustering performance significantly declined when it set as 0.1, which declined about 3%, 5% and 6% of ACC, NMI and ARI, respectively. And when <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is set as larger, the clustering performances continually declined. This is because the feature reconstruction prediction loss is used, as predicted by the latent features, and when it is set too large, the structures of features will change more, where original features may be destroyed.</p></list-item><list-item><label>(2)</label><p>When <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is fixed, the performance will decline significantly when <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is too large or small. When we set <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> as 0.1, our method can obtain the best performances when <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> fixed as 0.01 and 0.1. When <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> set as 0.01, although ACC moderately decreased, NMI and ARI decreased about 7% and 8%, respectively. On the other hand, when <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is fixed as 0.01, all clustering performances frequently declined. Moreover, when <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is fixed as other figures, <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> fluctuates in different sets. This is because a small and a large trade-off parameter may destroy the robustness of the framework, which can cause the features to misregister the original features.</p></list-item><list-item><label>(3)</label><p>When both <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are less than 0.1, the clustering performances will be worse when <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is 0.1. On the other hand, when both <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are too large, the clustering performances will significantly decline.</p></list-item></list></p><p>Therefore, after analyzing the different sets of trade-off hyper-parameters, the value of <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are suggested as <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.01</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively.</p></sec><sec id="sec4dot5-sensors-25-01215"><title>4.5. Ablation Studies</title><p>To further confirm the effectiveness of our approach, we performed ablation experiments on Zhongda with a missing rate of 0.3, as indicated in <xref rid="sensors-25-01215-t004" ref-type="table">Table 4</xref>. A series of FCITAFR variants is modeled with different combinations of <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. We can observe that the performance declined sightly when one or two loss functions ablated; all loss functions play significant roles in the arithmetic. The statistical analysis can be shown as follows:
<list list-type="simple"><list-item><label>(1)</label><p>The fuzzy clustering with information theory loss <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> may be the most important loss function in the method. In the first line, with only <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the clustering performances will decline by only about 1%. When this function is ablated, the clustering performance ACC is declined by about 3% to 6%. Such as in line 3, ACC, NMI, and ARI decreased about 6%, 4% and 10%, respectively, when <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is also ablated. This is because it will hardly ignore the significant features for each sample when this loss function is ablated, and <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> also can learn the important features of views.</p></list-item><list-item><label>(2)</label><p>For with-in view reconstruction loss <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, many works demonstrate that autoencoders are widely used in unsupervised learning; autoencoder structure is helpful to avoid the trivial solution. Other loss function also needed to learn the latent representation from encoders by autoencoders. Overall, the aforementioned observations have verified the effectiveness of the proposed loss function in our method. In the third and the fifth line, the clustering performances declined about 2% to 5% when <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was ablated.</p></list-item><list-item><label>(3)</label><p>For feature reconstruction prediction loss <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the feature reconstruction can change the features of views, which may improve the relationship by different features, and avoid ignoring some important features and margin features. Therefore, this loss function may improve the clustering performances. In addition, in the fourth line, ACC declined about 2% when <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> was ablated, and NMI and ARI decrease more than 10% without <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Furthermore, compared with line 4, ACC, NMI, and ARI are increased about 3%, 10%, and 7%, and it can be clearly seen that reconstructing the features of existing views is so significant. Obtaining the average of features about existing views can let the features equalize more, which can close the distance of different features.</p></list-item></list>
</p><p>Overall, the fuzzy clustering with information theory loss <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the most significant loss function in this method, and <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> have mutual promotion and also can cause a minor effect when only <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> exists. And the other two loss functions also play an important role in this algorithm, since they may obtain a better clustering performance when they exist together than only exist alone.</p></sec><sec id="sec4dot6-sensors-25-01215"><title>4.6. Convergence Analysis</title><p>Furthermore, we visualized the latent space via t-SNE on Zheyi with the missing rate of 0.3, as shown in <xref rid="sensors-25-01215-f004" ref-type="fig">Figure 4</xref>. Based on the statistical analysis, the following observations can be summarized: (1) The clustering performance sharply increased during the first 75 epochs; this is because FCITAFR effectively learns the structures of the framework and then the features learn adequately. As a result, the clustering performance levels stay consistent. These findings indicate that the proposed method achieved training convergence. (2) The loss is significantly decreased during the first 75 epochs. This is because FCITAFR utilizes the latent features and reduces the gap between the original features and predicted features.</p><p>To explore the convergence procedure of FCITAFR, we plotted the values of ACC, NMI, and ARI, and the total loss on Zhongda with the missing rate of 0.3 in the training stage, as shown in <xref rid="sensors-25-01215-f005" ref-type="fig">Figure 5</xref>. In comparison with the qualitative results of other approaches, our method shows an improved performance with more epochs, leading to a greater distinction between the features of the two different types.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01215"><title>5. Conclusions</title><p>In this paper, we propose a novel incomplete multiview clustering (IMVC) method with fully incomplete information. First, we built a feature reconstruction arithmetic to reconstruct the features of existing views, utilizing them to predict missing views. Then, we combined information with variational fuzzy c-means (FCM) clustering to learn the mutual information among views. The experimental findings show that FCITAFR outperforms the leading methods on both the internal and external datasets. Also, we analyzed the impact of trade-off hyper-parameters, and analyzed the effect of one parameter and both two parameters. Although our arithmetic obtained an outcome performance compared with one that was state-of-the-art, it only suited the multiview scene of two views, where multiview datasets in clinical experiments included more than two views; our method only can analyze the common and complementary information two-by-two. On the other hand, although our method is a time-saving arithmetic compared with the competitors during the time cost experiment, our method is not suited to the situation of more than two views; it is also waste of time, since the experiments need to analyze the views two-by-two. In the future, we will extend these FCITAFR insights to unaligned samples and extend the framework to the situation of more than two views, thus enhancing the capabilities of FCITAFR in real clinical scenarios.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, S.L. and X.L.; methodology, S.L.; software, S.L.; validation, S.L. and X.L.; formal analysis, S.L.; investigation, S.L.; resources, S.L.; data curation, S.L.; writing&#x02014;original draft preparation, S.L.; writing&#x02014;review and editing, S.L.; visualization, S.L.; supervision, S.L.; project administration, S.L.; funding acquisition, X.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors on request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">CECT</td><td align="left" valign="middle" rowspan="1" colspan="1">Contrast-enhanced computed tomograph</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IMVC</td><td align="left" valign="middle" rowspan="1" colspan="1">Incomplete multiview clustering</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NC</td><td align="left" valign="middle" rowspan="1" colspan="1">Non-contrast</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AP</td><td align="left" valign="middle" rowspan="1" colspan="1">Arterial phase</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PV</td><td align="left" valign="middle" rowspan="1" colspan="1">Portal venous</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">DP</td><td align="left" valign="middle" rowspan="1" colspan="1">Delay-phase</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FCM</td><td align="left" valign="middle" rowspan="1" colspan="1">Fuzzy c-means clustering</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ACC</td><td align="left" valign="middle" rowspan="1" colspan="1">Accuracy</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NMI</td><td align="left" valign="middle" rowspan="1" colspan="1">Normalized mutual information</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ARI</td><td align="left" valign="middle" rowspan="1" colspan="1">Adjusted rand index</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-01215"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ates</surname><given-names>G.C.</given-names></name>
<name><surname>Mohan</surname><given-names>P.</given-names></name>
<name><surname>Celik</surname><given-names>E.</given-names></name>
</person-group><article-title>Dual cross-attention for medical image segmentation</article-title><source>Eng. Appl. Artif. Intell.</source><year>2023</year><volume>126</volume><fpage>107139</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.107139</pub-id></element-citation></ref><ref id="B2-sensors-25-01215"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>G.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Shen</surname><given-names>T.</given-names></name>
<name><surname>Jia</surname><given-names>X.</given-names></name>
<name><surname>Ding</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>P.</given-names></name>
</person-group><article-title>A systemic study on the performance of different quantitative ultrasound imaging techniques for microwave ablation monitoring of liver</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2023</year><volume>72</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1109/TIM.2023.3267375</pub-id><pub-id pub-id-type="pmid">37323850</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01215"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>C.</given-names></name>
<name><surname>Hu</surname><given-names>X.</given-names></name>
<name><surname>Zhu</surname><given-names>Q.</given-names></name>
<name><surname>Tu</surname><given-names>Q.</given-names></name>
<name><surname>Geng</surname><given-names>H.</given-names></name>
<name><surname>Xu</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
</person-group><article-title>Individual Medical Costs Prediction Methods Based On Clinical Notes and DRGs</article-title><source>IEEE J. Radio Freq. Identif.</source><year>2024</year><volume>8</volume><fpage>412</fpage><lpage>418</lpage><pub-id pub-id-type="doi">10.1109/JRFID.2024.3392682</pub-id></element-citation></ref><ref id="B4-sensors-25-01215"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Guo</surname><given-names>L.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Ying</surname><given-names>S.</given-names></name>
<name><surname>Shi</surname><given-names>J.</given-names></name>
</person-group><article-title>Multiview feature transformation based SVM+ for computer-aided diagnosis of liver cancers with ultrasound images</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2023</year><volume>27</volume><fpage>1512</fpage><lpage>1523</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2022.3233717</pub-id><pub-id pub-id-type="pmid">37018255</pub-id>
</element-citation></ref><ref id="B5-sensors-25-01215"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>D.</given-names></name>
<name><surname>Qi</surname><given-names>H.</given-names></name>
<name><surname>Lian</surname><given-names>B.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Song</surname><given-names>H.</given-names></name>
</person-group><article-title>Resilient Decentralized Cooperative Localization for Multi-Source Multi-Robot System</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2023</year><volume>72</volume><fpage>8504713</fpage></element-citation></ref><ref id="B6-sensors-25-01215"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Huang</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Lu</surname><given-names>G.</given-names></name>
<name><surname>Zhou</surname><given-names>Z.</given-names></name>
<name><surname>Pan</surname><given-names>J.</given-names></name>
</person-group><article-title>Attention-Guided and Noise-Resistant Learning for Robust Medical Image Segmentation</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2024</year><volume>73</volume><fpage>4008013</fpage><pub-id pub-id-type="doi">10.1109/TIM.2024.3406804</pub-id></element-citation></ref><ref id="B7-sensors-25-01215"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gorji</surname><given-names>L.</given-names></name>
<name><surname>Brown</surname><given-names>Z.J.</given-names></name>
<name><surname>Limkemann</surname><given-names>A.</given-names></name>
<name><surname>Schenk</surname><given-names>A.D.</given-names></name>
<name><surname>Pawlik</surname><given-names>T.M.</given-names></name>
</person-group><article-title>Liver Transplant as a Treatment of Primary and Secondary Liver Neoplasms</article-title><source>JAMA Surg.</source><year>2024</year><volume>159</volume><fpage>211</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1001/jamasurg.2023.6083</pub-id><pub-id pub-id-type="pmid">38055245</pub-id>
</element-citation></ref><ref id="B8-sensors-25-01215"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Singh</surname><given-names>S.</given-names></name>
<name><surname>Anand</surname><given-names>R.S.</given-names></name>
</person-group><article-title>Multimodal medical image fusion using hybrid layer decomposition with CNN-based feature mapping and structural clustering</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2019</year><volume>69</volume><fpage>3855</fpage><lpage>3865</lpage><pub-id pub-id-type="doi">10.1109/TIM.2019.2933341</pub-id></element-citation></ref><ref id="B9-sensors-25-01215"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>S.</given-names></name>
<name><surname>Zhu</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Guo</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
</person-group><article-title>Multi-modal graph learning for disease prediction</article-title><source>IEEE Trans. Med. Imaging</source><year>2022</year><volume>41</volume><fpage>2207</fpage><lpage>2216</lpage><pub-id pub-id-type="doi">10.1109/TMI.2022.3159264</pub-id><pub-id pub-id-type="pmid">35286257</pub-id>
</element-citation></ref><ref id="B10-sensors-25-01215"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bayram</surname><given-names>H.C.</given-names></name>
<name><surname>Rekik</surname><given-names>I.</given-names></name>
</person-group><article-title>A federated multigraph integration approach for connectional brain template learning</article-title><source>Proceedings of the Multimodal Learning for Clinical Decision Support: 11th International Workshop, ML-CDS 2021, Held in Conjunction with MICCAI 2021</source><conf-loc>Strasbourg, France</conf-loc><conf-date>1 October 2021</conf-date><comment>Proceedings 11</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2021</year><fpage>36</fpage><lpage>47</lpage></element-citation></ref><ref id="B11-sensors-25-01215"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Poornima</surname><given-names>D.</given-names></name>
<name><surname>Shabu</surname><given-names>S.J.</given-names></name>
<name><surname>Aswin</surname><given-names>T.</given-names></name>
<name><surname>Sruthi</surname><given-names>V.</given-names></name>
<name><surname>Sruthi</surname><given-names>K.D.</given-names></name>
</person-group><article-title>A Machine Learning-based Multi-Phase Medical Image Classification for Internet of Medical Things</article-title><source>Proceedings of the 2023 7th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)</source><conf-loc>Kirtipur, Nepal</conf-loc><conf-date>11&#x02013;13 October 2023</conf-date><fpage>178</fpage><lpage>185</lpage></element-citation></ref><ref id="B12-sensors-25-01215"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>L.</given-names></name>
<name><surname>Qi</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>A.</given-names></name>
<name><surname>Deng</surname><given-names>L.</given-names></name>
<name><surname>Jiang</surname><given-names>T.</given-names></name>
</person-group><article-title>TeaBERT: An Efficient Knowledge Infused Cross-Lingual Language Model for Mapping Chinese Medical Entities to the Unified Medical Language System</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2023</year><volume>27</volume><fpage>6029</fpage><lpage>6038</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2023.3315143</pub-id><pub-id pub-id-type="pmid">37703167</pub-id>
</element-citation></ref><ref id="B13-sensors-25-01215"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Xin</surname><given-names>T.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
</person-group><article-title>Partition-a-medical-image: Extracting multiple representative sub-regions for few-shot medical image segmentation</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2024</year><volume>73</volume><fpage>5016312</fpage><pub-id pub-id-type="doi">10.1109/TIM.2024.3381715</pub-id></element-citation></ref><ref id="B14-sensors-25-01215"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>S.Y.</given-names></name>
<name><surname>Jiang</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>Z.H.</given-names></name>
</person-group><article-title>Partial multiview clustering</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Qu&#x000e9;bec City, QC, Canada</conf-loc><conf-date>27&#x02013;31 July 2014</conf-date><volume>Volume 28</volume></element-citation></ref><ref id="B15-sensors-25-01215"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Weiwei</surname><given-names>K.</given-names></name>
<name><surname>Qiguang</surname><given-names>M.</given-names></name>
<name><surname>Yang</surname><given-names>L.</given-names></name>
</person-group><article-title>Multimodal Sensor Medical Image Fusion Based on Local Difference in Non-Subsampled Domain</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2018</year><volume>64</volume><fpage>938</fpage><lpage>951</lpage></element-citation></ref><ref id="B16-sensors-25-01215"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wen</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Rekik</surname><given-names>I.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Peng</surname><given-names>Y.</given-names></name>
<name><surname>He</surname><given-names>H.</given-names></name>
</person-group><article-title>Multi-modal multiple kernel learning for accurate identification of Tourette syndrome children</article-title><source>Pattern Recognit.</source><year>2017</year><volume>63</volume><fpage>601</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2016.09.039</pub-id></element-citation></ref><ref id="B17-sensors-25-01215"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Peng</surname><given-names>P.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Ju</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>K.</given-names></name>
<name><surname>Li</surname><given-names>G.</given-names></name>
<name><surname>Calhoun</surname><given-names>V.D.</given-names></name>
<name><surname>Wang</surname><given-names>Y.P.</given-names></name>
</person-group><article-title>Group sparse joint non-negative matrix factorization on orthogonal subspace for multi-modal imaging genetics data analysis. IEEE/ACM Trans</article-title><source>Comput. Biol. Bioinform.</source><year>2020</year><volume>19</volume><fpage>479</fpage><lpage>490</lpage></element-citation></ref><ref id="B18-sensors-25-01215"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Vivar</surname><given-names>G.</given-names></name>
<name><surname>Zwergal</surname><given-names>A.</given-names></name>
<name><surname>Navab</surname><given-names>N.</given-names></name>
<name><surname>Ahmadi</surname><given-names>S.A.</given-names></name>
</person-group><article-title>Multi-modal disease classification in incomplete datasets using geometric matrix completion</article-title><source>Proceedings of the Graphs in Biomedical Image Analysis and Integrating Medical Imaging and Non-Imaging Modalities: Second International Workshop, GRAIL 2018 and First International Workshop, Beyond MIC 2018, Held in Conjunction with MICCAI 2018</source><conf-loc>Granada, Spain</conf-loc><conf-date>20 September 2018</conf-date><comment>Proceedings 2</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2018</year><fpage>24</fpage><lpage>31</lpage></element-citation></ref><ref id="B19-sensors-25-01215"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Demir</surname><given-names>U.</given-names></name>
<name><surname>Gharsallaoui</surname><given-names>M.A.</given-names></name>
<name><surname>Rekik</surname><given-names>I.</given-names></name>
</person-group><article-title>Clustering-based deep brain multigraph integrator network for learning connectional brain templates</article-title><source>Proceedings of the Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, and Graphs in Biomedical Image Analysis: Second International Workshop, UNSURE 2020, and Third International Workshop, GRAIL 2020, Held in Conjunction with MICCAI 2020</source><conf-loc>Lima, Peru</conf-loc><conf-date>8 October 2020</conf-date><comment>Proceedings 2</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2020</year><fpage>109</fpage><lpage>120</lpage></element-citation></ref><ref id="B20-sensors-25-01215"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>P.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Mu</surname><given-names>J.</given-names></name>
<name><surname>Cheng</surname><given-names>Y.</given-names></name>
</person-group><article-title>Deep embedding based tensor incomplete multiview clustering</article-title><source>Digit. Signal Process.</source><year>2024</year><volume>151</volume><fpage>104534</fpage><pub-id pub-id-type="doi">10.1016/j.dsp.2024.104534</pub-id></element-citation></ref><ref id="B21-sensors-25-01215"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hamilton</surname><given-names>W.</given-names></name>
<name><surname>Ying</surname><given-names>Z.</given-names></name>
<name><surname>Leskovec</surname><given-names>J.</given-names></name>
</person-group><article-title>Inductive representation learning on large graphs</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><pub-id pub-id-type="arxiv">1706.02216</pub-id></element-citation></ref><ref id="B22-sensors-25-01215"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bessadok</surname><given-names>A.</given-names></name>
<name><surname>Mahjoub</surname><given-names>M.A.</given-names></name>
<name><surname>Rekik</surname><given-names>I.</given-names></name>
</person-group><article-title>Topology-aware generative adversarial network for joint prediction of multiple brain graphs from a single brain graph</article-title><source>Proceedings of the Medical Image Computing and Computer Assisted Intervention&#x02013;MICCAI 2020: 23rd International Conference</source><conf-loc>Lima, Peru</conf-loc><conf-date>4&#x02013;8 October 2020</conf-date><comment>Proceedings, Part VII 23</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2020</year><fpage>551</fpage><lpage>561</lpage></element-citation></ref><ref id="B23-sensors-25-01215"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Pu</surname><given-names>J.</given-names></name>
<name><surname>Cui</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>Y.</given-names></name>
<name><surname>Pu</surname><given-names>X.</given-names></name>
<name><surname>Hao</surname><given-names>Z.</given-names></name>
<name><surname>Philip</surname><given-names>S.Y.</given-names></name>
<name><surname>He</surname><given-names>L.</given-names></name>
</person-group><article-title>Adaptive Feature Imputation with Latent Graph for Deep Incomplete Multiview Clustering</article-title><source>Proceedings of the Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>20&#x02013;27 February 2024</conf-date><volume>Volume 38</volume><fpage>14633</fpage><lpage>14641</lpage></element-citation></ref><ref id="B24-sensors-25-01215"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chao</surname><given-names>G.</given-names></name>
<name><surname>Jiang</surname><given-names>Y.</given-names></name>
<name><surname>Chu</surname><given-names>D.</given-names></name>
</person-group><article-title>Incomplete contrastive multiview clustering with high-confidence guiding</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>20&#x02013;27 February2024</conf-date><volume>Volume 38</volume><fpage>11221</fpage><lpage>11229</lpage></element-citation></ref><ref id="B25-sensors-25-01215"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Yue</surname><given-names>L.</given-names></name>
<name><surname>Xiao</surname><given-names>S.</given-names></name>
<name><surname>Yang</surname><given-names>W.</given-names></name>
<name><surname>Shen</surname><given-names>D.</given-names></name>
<name><surname>Liu</surname><given-names>M.</given-names></name>
</person-group><article-title>Assessing clinical progression from subjective cognitive decline to mild cognitive impairment with incomplete multi-modal neuroimages</article-title><source>Med. Image Anal.</source><year>2022</year><volume>75</volume><fpage>102266</fpage><pub-id pub-id-type="doi">10.1016/j.media.2021.102266</pub-id><pub-id pub-id-type="pmid">34700245</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01215"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiao</surname><given-names>J.</given-names></name>
<name><surname>Sun</surname><given-names>H.</given-names></name>
<name><surname>Huang</surname><given-names>Y.</given-names></name>
<name><surname>Xia</surname><given-names>M.</given-names></name>
<name><surname>Qiao</surname><given-names>M.</given-names></name>
<name><surname>Ren</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Guo</surname><given-names>Y.</given-names></name>
</person-group><article-title>GMRLNet: A graph-based manifold regularization learning framework for placental insufficiency diagnosis on incomplete multimodal ultrasound data</article-title><source>IEEE Trans. Med. Imaging</source><year>2023</year><volume>42</volume><fpage>3205</fpage><lpage>3218</lpage><pub-id pub-id-type="doi">10.1109/TMI.2023.3278259</pub-id><pub-id pub-id-type="pmid">37216245</pub-id>
</element-citation></ref><ref id="B27-sensors-25-01215"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Sarawgi</surname><given-names>U.</given-names></name>
</person-group><article-title>Uncertainty-Aware Ensembling in Multi-Modal AI and Its Applications in Digital Health for Neurodegenerative Disorders</article-title><source>Ph.D. Thesis</source><publisher-name>Massachusetts Institute of Technology</publisher-name><publisher-loc>Cambridge, MA, USA</publisher-loc><year>2021</year></element-citation></ref><ref id="B28-sensors-25-01215"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>Q.</given-names></name>
<name><surname>Xu</surname><given-names>B.</given-names></name>
<name><surname>Huang</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Xu</surname><given-names>R.</given-names></name>
<name><surname>Shao</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
</person-group><article-title>Deep multi-modal discriminative and interpretability network for Alzheimer&#x02019;s disease diagnosis</article-title><source>IEEE Trans. Med. Imaging</source><year>2022</year><volume>42</volume><fpage>1472</fpage><lpage>1483</lpage><pub-id pub-id-type="doi">10.1109/TMI.2022.3230750</pub-id><pub-id pub-id-type="pmid">37015464</pub-id>
</element-citation></ref><ref id="B29-sensors-25-01215"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Jiang</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>D.</given-names></name>
<name><surname>Liu</surname><given-names>W.</given-names></name>
</person-group><article-title>Incomplete multiview clustering via self-attention networks and feature reconstruction</article-title><source>Appl. Intell.</source><year>2024</year><volume>54</volume><fpage>2998</fpage><lpage>3016</lpage><pub-id pub-id-type="doi">10.1007/s10489-024-05299-z</pub-id></element-citation></ref><ref id="B30-sensors-25-01215"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Peng</surname><given-names>L.</given-names></name>
<name><surname>Ren</surname><given-names>Y.</given-names></name>
<name><surname>Shi</surname><given-names>X.</given-names></name>
<name><surname>Shen</surname><given-names>H.T.</given-names></name>
<name><surname>Zhu</surname><given-names>X.</given-names></name>
</person-group><article-title>Adaptive feature projection with distribution alignment for deep incomplete multiview clustering</article-title><source>IEEE Trans. Image Process.</source><year>2023</year><volume>32</volume><fpage>1354</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1109/TIP.2023.3243521</pub-id></element-citation></ref><ref id="B31-sensors-25-01215"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>Y.</given-names></name>
<name><surname>Gou</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>B.</given-names></name>
<name><surname>Lv</surname><given-names>J.</given-names></name>
<name><surname>Peng</surname><given-names>X.</given-names></name>
</person-group><article-title>Completer: Incomplete multiview clustering via contrastive prediction</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>17&#x02013;21 June 2021</conf-date><fpage>11174</fpage><lpage>11183</lpage></element-citation></ref><ref id="B32-sensors-25-01215"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>Y.</given-names></name>
<name><surname>Gou</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Bai</surname><given-names>J.</given-names></name>
<name><surname>Lv</surname><given-names>J.</given-names></name>
<name><surname>Peng</surname><given-names>X.</given-names></name>
</person-group><article-title>Dual contrastive prediction for incomplete multiview representation learning</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>45</volume><fpage>4447</fpage><lpage>4461</lpage></element-citation></ref><ref id="B33-sensors-25-01215"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>M.</given-names></name>
<name><surname>Hu</surname><given-names>P.</given-names></name>
<name><surname>Peng</surname><given-names>D.</given-names></name>
<name><surname>Peng</surname><given-names>X.</given-names></name>
</person-group><article-title>Incomplete multiview clustering via prototype-based imputation</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2301.11045</pub-id></element-citation></ref><ref id="B34-sensors-25-01215"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>P.</given-names></name>
<name><surname>Bai</surname><given-names>J.</given-names></name>
<name><surname>Lv</surname><given-names>J.</given-names></name>
<name><surname>Peng</surname><given-names>X.</given-names></name>
</person-group><article-title>Robust multiview clustering with incomplete information</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2022</year><volume>45</volume><fpage>1055</fpage><lpage>1069</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3155499</pub-id><pub-id pub-id-type="pmid">35230947</pub-id>
</element-citation></ref><ref id="B35-sensors-25-01215"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>M.</given-names></name>
<name><surname>Tang</surname><given-names>C.</given-names></name>
<name><surname>Xia</surname><given-names>J.</given-names></name>
<name><surname>Xiong</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>L.</given-names></name>
<name><surname>Kloft</surname><given-names>M.</given-names></name>
<name><surname>Zhu</surname><given-names>E.</given-names></name>
</person-group><article-title>Efficient and effective regularized incomplete multiview clustering</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2020</year><volume>43</volume><fpage>2634</fpage><lpage>2646</lpage></element-citation></ref><ref id="B36-sensors-25-01215"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Tang</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Zheng</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Zhu</surname><given-names>E.</given-names></name>
</person-group><article-title>Tensor-based multiview block-diagonal structure diffusion for clustering incomplete multiview data</article-title><source>Proceedings of the 2021 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Shenzhen, China</conf-loc><conf-date>5&#x02013;9 July 2021</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B37-sensors-25-01215"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pan</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>M.</given-names></name>
<name><surname>Lian</surname><given-names>C.</given-names></name>
<name><surname>Xia</surname><given-names>Y.</given-names></name>
<name><surname>Shen</surname><given-names>D.</given-names></name>
</person-group><article-title>Spatially-constrained fisher representation for brain disease identification with incomplete multi-modal neuroimages</article-title><source>IEEE Trans. Med. Imaging</source><year>2020</year><volume>39</volume><fpage>2965</fpage><lpage>2975</lpage><pub-id pub-id-type="doi">10.1109/TMI.2020.2983085</pub-id><pub-id pub-id-type="pmid">32217472</pub-id>
</element-citation></ref><ref id="B38-sensors-25-01215"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Tian</surname><given-names>D.</given-names></name>
<name><surname>Yan</surname><given-names>F.</given-names></name>
</person-group><article-title>Effectiveness of entropy weight method in decision-making</article-title><source>Math. Probl. Eng.</source><year>2020</year><volume>2020</volume><fpage>3564835</fpage><pub-id pub-id-type="doi">10.1155/2020/3564835</pub-id></element-citation></ref><ref id="B39-sensors-25-01215"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>X.</given-names></name>
<name><surname>Zhu</surname><given-names>T.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
</person-group><article-title>Discriminative Embedded Multiview Fuzzy C-Means Clustering for Feature- redundant and Incomplete Data</article-title><source>Inf. Sci.</source><year>2024</year><volume>677</volume><fpage>120830</fpage><pub-id pub-id-type="doi">10.1016/j.ins.2024.120830</pub-id></element-citation></ref><ref id="B40-sensors-25-01215"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>Z.</given-names></name>
<name><surname>Xia</surname><given-names>T.</given-names></name>
<name><surname>Zhang</surname><given-names>T.</given-names></name>
<name><surname>Du</surname><given-names>M.</given-names></name>
<name><surname>Zhong</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>Y.</given-names></name>
<name><surname>Xuan</surname><given-names>K.</given-names></name>
<name><surname>Xu</surname><given-names>G.</given-names></name>
<name><surname>Wan</surname><given-names>Z.</given-names></name>
<name><surname>Ju</surname><given-names>S.</given-names></name>
<etal/>
</person-group><article-title>Prediction of preoperative microvascular invasion by dynamic radiomic analysis based on contrast-enhanced computed tomography</article-title><source>Abdom. Radiol.</source><year>2024</year><volume>49</volume><fpage>611</fpage><lpage>624</lpage><pub-id pub-id-type="doi">10.1007/s00261-023-04102-w</pub-id><pub-id pub-id-type="pmid">38051358</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01215-f001"><label>Figure 1</label><caption><p>The framework of FCITAFR, where &#x0201c;?&#x0201d; donates the missing views. The within-view reconstruction loss <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> uses encoders to obtain the latent representation <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Then, via feature reconstruct prediction loss <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, in order to reconstruct the latent representation of existing views to obtain new latent representation <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the generators <italic toggle="yes">G</italic> predict the missing views of each sample. Finally, via fuzzy clustering with information theory loss <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, in order to obtain the membership matrix <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> of each view, the fusion similarity matrix among views can be obtained via information theory.</p></caption><graphic xlink:href="sensors-25-01215-g001" position="float"/></fig><fig position="float" id="sensors-25-01215-f002"><label>Figure 2</label><caption><p>Five instances of liver tumor CECT images from the different periods; &#x0201c;?&#x0201d; denotes a missing view and &#x0201c;&#x02192;&#x0201d; denotes mapping the missing view.</p></caption><graphic xlink:href="sensors-25-01215-g002" position="float"/></fig><fig position="float" id="sensors-25-01215-f003"><label>Figure 3</label><caption><p>Parameter evaluation on Zhongda with a missing rate of 0.3.</p></caption><graphic xlink:href="sensors-25-01215-g003" position="float"/></fig><fig position="float" id="sensors-25-01215-f004"><label>Figure 4</label><caption><p>Visualization of latent space via t-SNE on Zheyi with the missing rate of 0.3. With an epoch of 500, compared with other methods, FCITAFR constructed a more discriminative latent space.</p></caption><graphic xlink:href="sensors-25-01215-g004" position="float"/></fig><fig position="float" id="sensors-25-01215-f005"><label>Figure 5</label><caption><p>Convergence evaluation on Zhongda with a missing rate of 0.3.</p></caption><graphic xlink:href="sensors-25-01215-g005" position="float"/></fig><table-wrap position="float" id="sensors-25-01215-t001"><object-id pub-id-type="pii">sensors-25-01215-t001_Table 1</object-id><label>Table 1</label><caption><p>Notations with corresponding descriptions.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Notation</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Descriptions</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the notations utilized for indexing</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the number of samples in the multiview dataset</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the number of views in the multiview dataset</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the data matrix corresponding to the <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th view</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the latent representation of the <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th view</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the new latent representation of the <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th view after feature reconstruction</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm150" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the dimension of <inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the dimension of <inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the trade-off parameters for loss functions</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm156" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the encoder and decoder parameters of the <inline-formula><mml:math id="mm157" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th view</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm158" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the similarity matrix in the <inline-formula><mml:math id="mm159" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th view</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm160" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">the membership matrix of the <inline-formula><mml:math id="mm161" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th view</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01215-t002"><object-id pub-id-type="pii">sensors-25-01215-t002_Table 2</object-id><label>Table 2</label><caption><p>A clustering performance comparison of two liver tumor datasets. The 1st/2nd best results are in <bold>bold</bold> and <underline>underlined</underline>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Zhongda</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Zheyi</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Missing Rates</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACC (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NMI (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARI (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ACC (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NMI (%)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARI (%)</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">10% Missing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNFR (2024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">COMPLETER (2021)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DCP (2022)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prolmp (2023)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>69.5</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>22.3</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>23.2</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SURE (2022)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">63.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>73.9</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>27.9</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>25.8</underline>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FCITAFR (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>75.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>33.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>25.3</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>79.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>31.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>34.6</bold>
</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">30% Missing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNFR (2024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">COMPLETER (2021)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DCP (2022)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prolmp (2023)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>12.5</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>10.8</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>70.2</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>23.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>19.5</underline>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SURE (2022)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>67.3</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FCITAFR (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>77.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>37.9</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>29.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>20.3</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>19.9</bold>
</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">50% Missing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNFR (2024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">57.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">COMPLETER (2021)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DCP (2022)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prolmp (2023)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>76.4</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>29.8</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>30.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>70.7</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>25.2</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>20.9</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SURE (2022)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FCITAFR (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>76.7</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>38.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>28.2</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>22.7</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>15.9</underline>
</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">70% Missing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SNFR (2024)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">COMPLETER (2021)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>25.0</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>62.2</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>7.5</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DCP (2022)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">54.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prolmp (2023)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>73.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>22.5</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>4.6</underline>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SURE (2022)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FCITAFR (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>72.4</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>32.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>20.0</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>63.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>10.3</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>6.3</bold>
</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">100% Missing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DCP (2022)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>56.3</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>1.3</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>0.1</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>53.2</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>0.4</underline>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<underline>0.2</underline>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FCITAFR (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>58.6</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1.8</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>55.0</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>4.4</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>1.6</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01215-t003"><object-id pub-id-type="pii">sensors-25-01215-t003_Table 3</object-id><label>Table 3</label><caption><p>Time cost comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Training Time (seconds)</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Zhongda</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prolmp</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SURE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FCITAFR (Our)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Zheyi</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prolmp</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">210</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SURE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">195</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FCITAFR (Our)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">155</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01215-t004"><object-id pub-id-type="pii">sensors-25-01215-t004_Table 4</object-id><label>Table 4</label><caption><p>Effect of different loss functions on Zhongda with a missing rate of 0.3, &#x0201c;&#x0221a;&#x0201d; donates the loss function is participated in experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm162" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm163" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm164" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ACC (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">NMI (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ARI (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">26.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">27.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.5</td></tr></tbody></table></table-wrap></floats-group></article>