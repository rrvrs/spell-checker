<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40404758</article-id><article-id pub-id-type="pmc">PMC12098988</article-id>
<article-id pub-id-type="publisher-id">2355</article-id><article-id pub-id-type="doi">10.1038/s41598-025-02355-7</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Improved CKD classification based on explainable artificial intelligence with extra trees and BBFS</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Elshewey</surname><given-names>Ahmed M.</given-names></name><address><email>ahmed.elshewey@fci.suezuni.edu.eg</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Selem</surname><given-names>Enas</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Abed</surname><given-names>Amira Hassan</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00ndhrx30</institution-id><institution-id institution-id-type="GRID">grid.430657.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 4699 3087</institution-id><institution>Department of Computer Science, Faculty of Computers and Information, </institution><institution>Suez University, </institution></institution-wrap>P.O.BOX:43221, Suez, Egypt </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00ndhrx30</institution-id><institution-id institution-id-type="GRID">grid.430657.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 4699 3087</institution-id><institution>Department of Information Technology, Faculty of Computers and Information, </institution><institution>Suez University, </institution></institution-wrap>P.O.BOX:43221, Suez, Egypt </aff><aff id="Aff3"><label>3</label>Department of Information Systems, High Institution for Marketing, Commerce &#x00026; Information Systems, Cairo, Egypt </aff></contrib-group><pub-date pub-type="epub"><day>22</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>22</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><elocation-id>17861</elocation-id><history><date date-type="received"><day>2</day><month>4</month><year>2024</year></date><date date-type="accepted"><day>13</day><month>5</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Chronic kidney disease is a persistent ailment marked by the gradual decline of kidney function. Its classification primarily relies on the estimated glomerular filtration rate and the existence of kidney damage. The kidney disease improving global outcomes organization has established a widely accepted system for categorizing chronic kidney disease. explainable artificial intelligence for classification involves creating machine learning models that not only accurately predict outcomes but also offer clear and interpretable explanations for their decisions. Traditional machine learning models often pose difficulties in comprehending the intricate processes behind specific classification choices due to their intricate and obscure nature. In this study, an explainable artificial intelligence-chronic kidney disease model is introduced for the process of classification. The model applies explainable artificial intelligence by utilizing extra trees and shapley additive explanations values. Also, binary breadth-first search algorithm is used to select the most important features for the proposed explainable artificial intelligence-chronic kidney disease model. This methodology is designed to derive valuable insights for enhancing decision-making strategies within the field of classifying chronic kidney diseases. The performance of the proposed model is compared with another machine learning models, namely, random forest, decision tree, bagging classifier, adaptive boosting, and k-nearest neighbor, and the performance of the models is evaluated using accuracy, sensitivity, specificity, F-score, and area under the ROC curve. The experimental results demonstrated that the proposed model achieved the best results with accuracy equals 99.9%.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Chronic kidney disease</kwd><kwd>Explainable artificial intelligence</kwd><kwd>Extra trees classifier</kwd><kwd>SHAP</kwd><kwd>Binary breadth-first search</kwd><kwd>Feature selection</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Kidney diseases</kwd><kwd>Computer science</kwd><kwd>Information technology</kwd></kwd-group><funding-group><award-group><funding-source><institution>Suez University</institution></funding-source></award-group><open-access><p>Open access funding provided by The Science, Technology &#x00026; Innovation Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank (EKB).</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Chronic kidney disease (CKD) represents a pervasive and progressively debilitating health condition that demands early detection and accurate classification for effective intervention and management<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. It is diagnosed by a glomerular filtration rate (GFR) persistently below a certain threshold for over three months or the presence of indicators of kidney damage. Common contributors to CKD encompass diabetes, hypertension, glomerulonephritis, and related conditions. With a global impact affecting millions, CKD imposes a substantial healthcare burden due to its widespread prevalence, costly treatments, and potential life-threatening complications. Timely detection and identification of key risk factors are pivotal in effectively managing CKD and enhancing patient outcomes. Conventional diagnostic approaches have proven inadequate in accurately predicting CKD progression and discerning the most influential factors contributing to its onset. In recent years, the convergence of advanced machine learning techniques and medical research has opened new avenues for improving the precision and interpretability of CKD classification models<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR3">3</xref></sup>. Chronic diseases are types of non-communicable illness and are a major cause of declining physical health and mental well-being. They are among the top causes of illness and death worldwide. While these conditions can be complex, many are preventable if identified and addressed early. To support better clinical decision-making, predictive models have been developed to help doctors and patients. These models analyze various risk factors to assess an individual&#x02019;s likelihood of developing a chronic disease, providing a clearer path for early intervention and prevention<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. This convergence signifies a significant leap forward in addressing the complexities of chronic kidney disease (CKD) classification. Advanced machine learning techniques, fueled by the latest developments in artificial intelligence, are now intricately woven into the fabric of medical research. This integration has ushered in a new era where the precision and interpretability of CKD classification models can be finely tuned, offering a more nuanced and comprehensive understanding of the disease. In the realm of CKD, where early detection and effective risk assessment are paramount, the synergy of advanced machine learning techniques and medical research facilitates a more granular exploration of the intricate patterns and indicators associated with the disease. The precision of these models is enhanced, allowing for a more accurate identification of CKD stages and potential risk factors. Moreover, the focus on interpretability brings transparency to the decision-making processes of these models. This transparency is crucial in gaining the trust of healthcare professionals and end-users who require insights into how predictions are made. The ability to interpret and explain the model&#x02019;s reasoning empowers clinicians to make informed decisions and fosters greater acceptance of these advanced techniques in the medical community. Artificial intelligence (AI), machine learning (ML), and deep learning have advanced rapidly, bringing significant improvements to industries like healthcare. The abundance of medical diagnostic data has made it possible to train powerful algorithms that drive innovation and enhance outcomes<sup><xref ref-type="bibr" rid="CR5">5</xref>&#x02013;<xref ref-type="bibr" rid="CR10">10</xref></sup>. As researchers delve deeper into understanding the multifaceted nature of CKD, the marriage of advanced machine learning and medical research not only refines the accuracy of predictions but also opens avenues for discovering novel biomarkers and subtle nuances that may have previously eluded traditional diagnostic approaches. The majority of machine learning (ML) models are often characterized as &#x0201c;black boxes,&#x0201d; referring to models that are complex enough to defy easy interpretation by humans<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>. When employing such black-box models as diagnostic tools, the challenge arises for healthcare professionals to comprehend the factors that led the model to make a specific prediction<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. This lack of transparency in black-box approaches poses obstacles to medical decision support from both physicians&#x02019; and patients&#x02019; perspectives. Therefore, there is a pressing need to develop diagnostic systems that offer interpretability for ML models. The interpretability of ML models serves as a crucial aspect for creating a diagnostic system that enables physicians to understand and trust the results provided<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. This transparency acts as a safety check on predicted outcomes, fostering confidence among physicians in the system&#x02019;s decision-making process. Consequently, the quest for interpretability has fueled growing interest in the field of explainable artificial intelligence (XAI). Explainable ML models, also referred to as interpretable ML models, belong to the field of explainable artificial intelligence (XAI), defined by Arrieta et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> as follows: &#x0201c;An explainable Artificial Intelligence is one that provides details or reasons to make its functioning clear or easy to understand for a given audience.&#x0201d; Consequently, XAI enables healthcare experts to make informed, data-driven decisions, facilitating the delivery of more personalized and reliable treatments<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. However, a fundamental tension arises between the predictive accuracy and interpretability of ML models, as the best-performing models often lack transparency, while those with clear explanations (e.g., decision trees) may sacrifice accuracy. In the healthcare domain, the limited explainability hinders the broader adoption of ML solutions, as healthcare professionals may find it challenging to trust complex models that require high technical and statistical knowledge. In this paper, we propose an innovative approach that leverages Explainable artificial intelligence (XAI) in conjunction with extra trees, an ensemble learning algorithm, to enhance the classification accuracy and transparency of CKD diagnostic systems. The importance of precise CKD classification appears from the necessity to recognize and interpose in the very early stages of the disease, permitting healthcare providers to tailor treatments and interventions to the certain needs of each patient. The integration of machine learning techniques offers a promising avenue to achieve this goal, with the added advantage of providing transparent insights into the decision-making process of the model. Our approach focuses on the exploitation of Extra Trees, an ensemble learning method famous for its capability to handle complex datasets and provide robust predictions<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. By incorporating a degree of randomness during the construction of decision trees<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, Extra Trees enhances diversity among the individual trees, potentially improving the model&#x02019;s overall performance. However, the intrinsic complexity of such models often raises concerns about interpretability. To address this interpretability challenge, we introduce XAI techniques to our CKD classification model. XAI aims to demystify the decision-making process of machine learning models, offering insights into the factors influencing individual predictions. By employing methods such as local interpretable model-agnostic explanations (LIME)<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> or shapley additive explanations (SHAP)<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, we strive to make our CKD classification model not only accurate but also transparent and understandable to healthcare professionals and patients. In the subsequent sections of this paper, we delve into the methodology employed in integrating Extra Trees and XAI into the CKD classification process. We explore the significance of feature importance analysis, discuss the interpretability of the model through XAI techniques, and present experimental results validating the efficacy of our proposed approach. By fusing the power of Extra Trees with the interpretability of XAI, our research contributes to the ongoing efforts in creating advanced, transparent, and clinically relevant models for CKD classification. The implications of this work extend beyond the realm of kidney disease, providing a blueprint for the development of explainable and accurate diagnostic systems in the broader landscape of medical artificial intelligence.</p><p id="Par3">Despite the advances in chronic kidney disease classification using machine learning, there remains a significant gap in balancing model performance with interpretability. Many high-performance models are considered &#x0201c;black boxes,&#x0201d; limiting their adoption in clinical settings due to a lack of transparency. Furthermore, feature selection methods in existing studies often rely on metaheuristics and statistical filters that may not efficiently capture dependencies in high-dimensional medical data. To address this, we identified the need for a robust and interpretable solution that also efficiently selects the most relevant features. This forms the basis of our proposed approach, which combines binary breadth-first search (BBFS) for feature selection with extra trees ensemble model and shapley additive explanations (SHAP) for interpretability.</p><p id="Par4">The main contribution in this paper is using an explainable artificial intelligence-CKD (XAI-CKD) model for the process of classification. XAI-CKD applies explainable AI (XAI) by utilizing extra trees (ET) and SHAP (shapley additive explanations) values. Also, binary breadth-first search (BBFS) algorithm is used to select the most important features for XAI-CKD model. BBFS is an efficient feature selection algorithm, making it particularly well-suited for CKD datasets, which often have high-dimensional features and interdependencies. BBFS works effectively via traversing the feature space to identify the most relevant features without requiring an exhaustive search, this saves computational resources. Extra Trees is an ensemble learning model used for capturing complex relationships between features through randomized tree construction. By combining BBFS with ET, noise in the dataset is reduced via selecting only the most important features, this helps in improving model performance and interpretability. This integration makes a balance between computational efficiency, accuracy, and clarity the results, which is vital in a medical context like CKD classification. This methodology is designed to derive valuable insights for enhancing decision-making strategies within the field of classifying chronic kidney diseases. The performance of XAI-CKD is compared with another machine learning models, namely, random forest (RF), decision tree (DT), bagging classifier (BC), adaptive boosting (AdaBoost), and k-nearest neighbor (KNN), and the performance of the models is evaluated using accuracy, sensitivity, specificity, F-score, and area under the ROC curve (AUC). The proposed XAI-CKD model achieved the best results with accuracy equals 99.9%. The steps for creating XAI-CKD model for chronic kidney disease classification are:</p><p id="Par5">
<list list-type="bullet"><list-item><p id="Par6">Data collection.</p></list-item><list-item><p id="Par7">Data normalization using Z-score.</p></list-item><list-item><p id="Par8">Using KNN imputer for imputing missing values.</p></list-item><list-item><p id="Par9">Using binary breadth-first search (BBFS) algorithm to identify the most relevant features for the classification task.</p></list-item><list-item><p id="Par10">Select the extra trees (ET) classification model that is inherently explainable and can be easily interpreted.</p></list-item><list-item><p id="Par11">Train the model by splitting the data into training (70%), and testing (30%) sets.</p></list-item><list-item><p id="Par12">Evaluate model performance using evaluation metrics such as accuracy, sensitivity, specificity, F-score, and area under the ROC curve (AUC).</p></list-item><list-item><p id="Par13">Generate explanations for the model predictions. SHAP (shapley additive explanations) is used, which provides insights into how the model arrives at specific predictions.</p></list-item></list>
</p><p id="Par14">The rest of this paper is organized as follows: Sect.&#x000a0;<xref rid="Sec2" ref-type="sec">2</xref> presents related work on CKD classification. Section&#x000a0;<xref rid="Sec3" ref-type="sec">3</xref> describes the materials and methods. Section&#x000a0;<xref rid="Sec18" ref-type="sec">4</xref> discusses experimental results. Section&#x000a0;<xref rid="Sec21" ref-type="sec">5</xref> outlines the limitations of the current study. Section&#x000a0;<xref rid="Sec22" ref-type="sec">6</xref> concludes the paper and suggests future research directions. Section&#x000a0;<xref rid="Sec23" ref-type="sec">7</xref> addresses ethical considerations in AI-based CKD classification.</p></sec><sec id="Sec2"><title>Related work</title><p id="Par15">In recent years, machine learning approaches plays an important role in the medical field<sup><xref ref-type="bibr" rid="CR21">21</xref>&#x02013;<xref ref-type="bibr" rid="CR25">25</xref></sup>. The application of machine learning (ML) models for predicting chronic kidney diseases (CKD) has garnered significant attention. However, the need for transparency and interpretability in these models has led to a surge in research focusing on explainable machine learning (XML) approaches within the realm of CKD prediction. Here is an overview of machine learning methods that support chronic kidney disease (CDK) prediction. Traditional ML Approaches: Early efforts in CKD prediction primarily involved conventional ML techniques, such as logistic regression<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, decision trees<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, support vector machines (SVM) in CKD Prediction<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> and Naive Bayes in CKD Diagnosis<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. While these models exhibited reasonable performance, their interpretability was limited, hindering their adoption in clinical practice. Therefore, Interpretability Challenges in Black-Box Models have been presented. The advent of sophisticated black-box models, including deep neural networks and ensemble methods<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, marked a significant shift towards improved predictive accuracy. However, the opacity of these models posed challenges in understanding the rationale behind predictions, particularly in the context of CKD. For this reason, explainable artificial intelligence (XAI) appeared to cover this problem. explainable artificial intelligence (XAI): Recognizing the need to balance predictive accuracy with interpretability, the research community embraced explainable artificial intelligence (XAI). XAI techniques, such as rule-based models<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, feature importance analysis<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, and shapley additive explanations (SHAP)<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, emerged as pivotal tools for unraveling the decision-making processes of complex ML models. Now we will explore recent work, the exploit Explainable Artificial Intelligence in CKD Prediction. In<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> Two ensemble approaches, namely XGBoost and AdaBoost, have been employed for the detection of chronic kidney disease (CKD). Subsequently, a model leveraging deep neural networks (DNN) is proposed for CKD prognosis, wherein hyperparameters were fine-tuned to enhance performance. Impressively, the DNN outperformed the ensemble methods, achieving an accuracy of 98.8%. To shed light on feature contributions and enhance interpretability, the study incorporated shapley additive explanations (SHAP), a technique gaining prominence in explainable artificial intelligence (XAI). The SHAP analysis aimed to discern the features influencing CKD prediction, confirming alignment with medical explanations. In<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> the proposed system, rooted in the decision tree-based explainable artificial intelligence (DT-EAI), aims to furnish a robust solution for early diagnosis. Employing a Data-driven approach, the system leverages the chronic kidney disease (CKD) dataset, conducting feature selection based on Gini Importance values. Subsequently, it constructs a model utilizing the Decision Tree algorithm and interprets the model using SHAP values. To ensure robustness, the system evaluates and validates its performance through Cross-Validation, undergoing iterative refinement based on feedback from healthcare professionals. This iterative process hones the model for enhanced accuracy and interpretability. Upon achieving satisfactory performance, the system is deployed for practical use in early diagnosis. The system&#x02019;s effectiveness is gauged using evaluation metrics such as the F1 score and fidelity accuracy index (FAI). In<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, the classification of chronic kidney disease (CKD) was carried out using the XGBoost classifier. To optimize the feature set, binary bat optimization (BBO) was employed to reduce the number of features and obtain an optimal subset. The model exhibited remarkable performance with all 24 features, yielding an accuracy, precision, recall, and F1 score of 99.16%, 100%, 98.68%, and 99.33%, respectively. Moreover, utilizing only the 13 features selected by the BBO algorithm maintained strong metrics, achieving an accuracy, precision, recall, and F1 score of 98.33%, 100%, 97.36%, and 98.67%, respectively. The analysis of machine learning models trained on both the original set and the BBO-selected feature subset using shapley additive explanations (SHAP) revealed that hemoglobin and albumin exerted significant influence on the model. Interestingly, the BBO algorithm also identified these attributes, along with a few additional traits, as the most impactful features. The study demonstrated the nuanced impact of each feature on the model&#x02019;s classification of a single sample within a specific class, offering valuable insights for clinicians. The transparency provided by explaining the black-box machine learning model proves advantageous for both clinicians and patients. The proposed system, implementable in any hospital, stands to support less experienced nephrologists in achieving more accurate CKD diagnoses. The authors suggest that future research could explore even more sophisticated explainable artificial intelligence (XAI) methods to further enhance the interpretability of the machine learning model. In<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> introduces the development and assessment of an explainable chronic kidney disease (CKD) prediction model designed to elucidate how various clinical features contribute to the early diagnosis of CKD. The model is crafted within an optimization framework that seeks to strike a balance between classification accuracy and interpretability. The primary contribution of this research lies in its embrace of an explainable data-driven methodology, providing quantifiable insights into the role of specific clinical features in the early detection of CKD. The optimal explainable prediction model is constructed using an extreme gradient boosting classifier, focusing on three key features: hemoglobin, specific gravity, and hypertension. This model achieves an accuracy of 99.2% (standard deviation 0.8) and 97.5% with a 5-fold cross-validation and new, unseen data, respectively. An insightful explain ability analysis underscores the significance of hemoglobin as the most influential feature shaping predictions, closely followed by specific gravity and hypertension. The utilization of this minimal set of features not only enhances the accuracy of early CKD diagnosis but also presents a cost-effective solution, particularly relevant for developing countries. The study in<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> focuses on developing a classifier model aimed at assisting healthcare professionals in the early diagnosis of CKD patients. Employing a comprehensive data pipeline, the researchers conduct an exhaustive search to identify the optimal data mining classifier, considering various parameters within the data preparation stages such as handling missing data and feature selection. Subsequently, Extra Trees emerges as the most effective classifier, achieving remarkable accuracies of 100% with cross-validation and 99% with new, unseen data. Furthermore, the study utilizes the eight selected features to evaluate the explainability of the model&#x02019;s outcomes, providing insights into the relative importance of each feature in influencing the model&#x02019;s predictions. The study in<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> explores the use of explainable artificial intelligence (XAI) techniques to predict chronic kidney disease (CKD) based on clinical characteristics. Clinical data from 491 patients, including 56 with CKD, were analyzed, encompassing various clinical, laboratory, and demographic variables. Five machine learning methods were employed, and the extreme gradient boosting (XGBoost) model emerged as the most effective, achieving an accuracy of 93.29% and an AUC of 0.9689. Feature importance analysis highlighted creatinine, glycosylated hemoglobin type A1C (HgbA1C), and age as the most influential features. SHAP and LIME algorithms were utilized for interpretability, providing insights into individual predictions and enhancing clinicians&#x02019; understanding of the model&#x02019;s rationale. This approach presents an interpretable ML-based strategy for early CKD prediction, facilitating better-informed clinical decision-making. The work in<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, an explainable machine learning model was constructed to forecast chronic kidney disease. The methodology involved building an automated data pipeline utilizing the Random Forest ensemble learning trees model alongside a feature selection algorithm. The model&#x02019;s explainability was evaluated through assessments of feature importance and explainability metrics. Three distinct explainability methods: LIME, SHAP, and SKATER were employed to interpret the model&#x02019;s outcomes. Furthermore, comparisons of explainability results were conducted using interpretability, fidelity, and fidelity-to-interpretability ratio as the key explainability metrics. The work in<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> introduces a novel hybrid approach for diagnosing Chronic Renal Disease, which optimizes an SVM classifier through a hybridized dimensionality reduction technique. It employs a two-step feature selection process: first, a filter-based approach using ReliefF assigns weights and ranks to each feature, followed by dimensionality reduction using PCA. The model also utilizes simultaneous execution on multiple processors for faster processing. Results show that the proposed approach achieves significantly higher prediction accuracies compared to existing methods on both clinical CKD and benchmarked chronic kidney disease datasets.</p></sec><sec id="Sec3"><title>Materials and methods</title><p id="Par16">In this study, an explainable artificial intelligence-CKD (XAI-CKD) model is introduced for the process of classification. XAI-CKD applies explainable AI (XAI) by utilizing extra trees (ET) and SHAP (shapley Additive explanations) values. Also, binary breadth-first search (BBFS) algorithm is used to select the most important features for XAI-CKD model. BBFS is an efficient feature selection algorithm, making it particularly well-suited for CKD datasets, which often have high-dimensional features and interdependencies. BBFS works effectively via traversing the feature space to identify the most relevant features without requiring an exhaustive search, this saves computational resources. Extra Trees is an ensemble learning model used for capturing complex relationships between features through randomized tree construction. According to no free lunch (NFL) theorem<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, which demonstrates that no single classifier works best for every problem. Our study does not claim that extra trees are the best. However, in the specific context of our dataset and problem, focusing on CKD classification and the XAI-CKD model with extra trees provides balance of accuracy and interpretability. By combining BBFS with ET, noise in the dataset is reduced via selecting only the most important features, this helps in improving model performance and interpretability. This integration makes a balance between computational efficiency, accuracy, and clarity the results, which is vital in a medical context like CKD classification. This methodology is designed to derive valuable insights for enhancing decision-making strategies within the field of classifying chronic kidney diseases. The performance of XAI-CKD is compared with another machine learning models, namely, random forest (RF), decision tree (DT), bagging classifier (BC), adaptive boosting (AdaBoost), and k-nearest neighbor (KNN), and the performance of the models is evaluated using accuracy, sensitivity, specificity, F-score, and area under the ROC curve (AUC). The steps for creating XAI-CKD model for chronic kidney disease classification:</p><p id="Par17">
<list list-type="order"><list-item><p id="Par18">Data collection.</p></list-item><list-item><p id="Par19">Data normalization using Z-score.</p></list-item><list-item><p id="Par20">Using KNN imputer for imputing missing values.</p></list-item><list-item><p id="Par21">Using binary breadth-first search (BBFS) algorithm to identify the most relevant features for the classification task.</p></list-item><list-item><p id="Par22">Select the extra trees (ET) classification model that is inherently explainable and can be easily interpreted.</p></list-item><list-item><p id="Par23">Train the model by <bold>s</bold>plitting the data into training (70%), and testing (30%) sets.</p></list-item><list-item><p id="Par24">Evaluate model performance using evaluation metrics such as accuracy, sensitivity, specificity, F-score, and area under the ROC curve (AUC).</p></list-item><list-item><p id="Par25">Generate explanations for the model predictions. SHAP (shapley additive explanations) is used, which provides insights into how the model arrives at specific predictions.</p></list-item></list>
</p><p id="Par26">The framework of the proposed methodology is depicted in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.</p><p id="Par27">
<fig id="Fig1"><label>Fig. 1</label><caption><p>Framework of the proposed methodology.</p></caption><graphic xlink:href="41598_2025_2355_Fig1_HTML" id="d33e452"/></fig>
</p><sec id="Sec4"><title>Dataset</title><p id="Par28">The dataset used in this paper is available at<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The hospital in Karaikudi, Tamilnadu, India provided this data set. There are 25 features in the dataset, 400 patients are employed for training algorithms that use ML to do classification. As a result, the target feature is marked in two classes: CKD and not-CKD. The dataset features include: age, blood pressure, specific gravity, albumin, sugar, red blood cells, pus cell, pus cell clumps, bacteria, blood glucose random, blood urea, serum creatinine, sodium, potassium, hemoglobin, packed cell volume, white blood cell count, red blood cell count, hypertension, diabetes mellitus, coronary artery disease, appetite, Peda edema, anemia, and class (target label: CKD or not-CKD). To handle missing values, we applied k-nearest neighbors (KNN) imputer during preprocessing. Z-score normalization was then used to standardize the dataset. There are medically significant factors in these data records that may be linked to the existence of CKD. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> represents the boxplot visualization for the values of the target feature. Boxplot visualization is crucial lies in its ability to give a brief yet insightful summary of how data is spread out and its variability. Boxplots are valuable in analyzing data because they offer a quick snapshot of important statistical measures like the center, spread, and any outliers present<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. They allow researchers to understand the fundamental characteristics of data distribution without delving into complex numerical analysis, making them extremely useful for exploring and visualizing data<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>.</p><p id="Par29">
<fig id="Fig2"><label>Fig. 2</label><caption><p>Boxplot visualization for the values of the target features.</p></caption><graphic xlink:href="41598_2025_2355_Fig2_HTML" id="d33e481"/></fig>
</p><p id="Par30">Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> provides a summary of important statistical properties for each feature in the dataset:</p><p id="Par31">
<list list-type="order"><list-item><p id="Par32">Feature Name: This column lists the names of the features or variables in the dataset.</p></list-item><list-item><p id="Par33">Count: It shows the number of observations or data points available for each feature. In this case, there are 400 recorded patients for each feature.</p></list-item><list-item><p id="Par34">Mean: This indicates the average value of the feature across all observations. It&#x02019;s calculated by adding up all values and then dividing them by the total number of observations.</p></list-item><list-item><p id="Par35">Std (Standard deviation): This measures the spread or dispersion of the data around the mean. It tells us how much values deviate from the mean. A higher standard deviation suggests greater variability in the dataset.</p></list-item><list-item><p id="Par36">Min (Minimum): It displays the smallest observed value for each feature in the dataset.</p></list-item><list-item><p id="Par37">50% (Median): This value represents the median of the feature. The median is the middle value when observations are sorted in ascending order, dividing the dataset into two equal halves.</p></list-item><list-item><p id="Par38">Max (Maximum): This indicates the largest observed value for each feature in the dataset.</p></list-item></list>
</p><p id="Par39">Figure <xref rid="Fig3" ref-type="fig">3</xref> displays a heatmap analysis of the dataset features. Heatmap analysis is a useful approach for visually illustrating correlations among different features in a dataset. This method is commonly used in exploratory data analysis to help identify patterns and relationships between variables. By observing the heatmap, analysts can understand the strength and direction of correlations between features. This helps in grasping how variables interact and may reveal dependencies within the dataset. In summary, heatmap analysis provides a simple yet effective way to uncover underlying structures and associations in complex datasets. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> showcases the histogram distribution analysis of the dataset features. Examining histogram distributions allows for the visualization of how data points are spread out across different intervals. By studying the shape of the histogram, one can identify trends such as skewness, multimodality, or symmetry within the dataset.</p><p id="Par40">
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Statistical analysis for the dataset features.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Count</th><th align="left">Mean</th><th align="left">Std</th><th align="left">Min</th><th align="left">50%</th><th align="left">Max</th></tr></thead><tbody><tr><td align="left">Age</td><td char="." align="char">400.0</td><td char="." align="char">51.265500</td><td char="." align="char">17.073910</td><td align="left">2.000</td><td align="left">54.000</td><td align="left">90.00</td></tr><tr><td align="left">Blood_pressure</td><td char="." align="char">400.0</td><td char="." align="char">76.295000</td><td char="." align="char">13.566248</td><td align="left">50.00</td><td align="left">80.000</td><td align="left">180.0</td></tr><tr><td align="left">Specific_gravity</td><td char="." align="char">400.0</td><td char="." align="char">1.017142</td><td char="." align="char">0.005470</td><td align="left">1.005</td><td align="left">1.016</td><td align="left">1.025</td></tr><tr><td align="left">Albumin</td><td char="." align="char">400.0</td><td char="." align="char">1.047000</td><td char="." align="char">1.281756</td><td align="left">0.000</td><td align="left">0.600</td><td align="left">5.000</td></tr><tr><td align="left">Sugar</td><td char="." align="char">400.0</td><td char="." align="char">0.465000</td><td char="." align="char">1.045997</td><td align="left">0.000</td><td align="left">0.000</td><td align="left">5.000</td></tr><tr><td align="left">Red_blood_cells</td><td char="." align="char">400.0</td><td char="." align="char">1.262500</td><td char="." align="char">0.655491</td><td align="left">0.000</td><td align="left">1.000</td><td align="left">2.000</td></tr><tr><td align="left">Pus_cell</td><td char="." align="char">400.0</td><td char="." align="char">0.972500</td><td char="." align="char">0.593823</td><td align="left">0.000</td><td align="left">1.000</td><td align="left">2.000</td></tr><tr><td align="left">Pus_cell_clumps</td><td char="." align="char">400.0</td><td char="." align="char">0.125000</td><td char="." align="char">0.360138</td><td align="left">0.000</td><td align="left">0.000</td><td align="left">2.000</td></tr><tr><td align="left">Bacteria</td><td char="." align="char">400.0</td><td char="." align="char">0.075000</td><td char="." align="char">0.299331</td><td align="left">0.000</td><td align="left">0.000</td><td align="left">2.000</td></tr><tr><td align="left">Blood_glucose_random</td><td char="." align="char">400.0</td><td char="." align="char">149.87200</td><td char="." align="char">76.73209</td><td align="left">22.00</td><td align="left">122.0</td><td align="left">490.0</td></tr><tr><td align="left">Blood_urea</td><td char="." align="char">400.0</td><td char="." align="char">57.678000</td><td char="." align="char">49.562082</td><td align="left">1.500</td><td align="left">44.00</td><td align="left">391.0</td></tr><tr><td align="left">Serum_creatinine</td><td char="." align="char">400.0</td><td char="." align="char">3.077375</td><td char="." align="char">5.631741</td><td align="left">0.400</td><td align="left">1.400</td><td align="left">76.00</td></tr><tr><td align="left">Sodium</td><td char="." align="char">400.0</td><td char="." align="char">137.51425</td><td char="." align="char">9.282191</td><td align="left">4.500</td><td align="left">138.0</td><td align="left">163.0</td></tr><tr><td align="left">Potassium</td><td char="." align="char">400.0</td><td char="." align="char">4.574750</td><td char="." align="char">2.826800</td><td align="left">2.500</td><td align="left">4.400</td><td align="left">47.00</td></tr><tr><td align="left">Hemoglobin</td><td char="." align="char">400.0</td><td char="." align="char">12.576650</td><td char="." align="char">2.761359</td><td align="left">3.100</td><td align="left">12.85</td><td align="left">17.80</td></tr><tr><td align="left">Packed_cell_volume</td><td char="." align="char">400.0</td><td char="." align="char">38.965000</td><td char="." align="char">8.364238</td><td align="left">9.000</td><td align="left">40.10</td><td align="left">54.00</td></tr><tr><td align="left">White_blood_cell_count</td><td char="." align="char">400.0</td><td char="." align="char">8482.4500</td><td char="." align="char">2591.071</td><td align="left">2200</td><td align="left">8400</td><td align="left">26,400</td></tr><tr><td align="left">Red_blood_cell_count</td><td char="." align="char">400.0</td><td char="." align="char">4.707250</td><td char="." align="char">0.889911</td><td align="left">2.100</td><td align="left">4.800</td><td align="left">8.000</td></tr><tr><td align="left">Hypertension</td><td char="." align="char">400.0</td><td char="." align="char">0.377500</td><td char="." align="char">0.495588</td><td align="left">0.000</td><td align="left">0.000</td><td align="left">2.000</td></tr><tr><td align="left">Diabetes_mellitus</td><td char="." align="char">400.0</td><td char="." align="char">0.352500</td><td char="." align="char">0.488713</td><td align="left">0.000</td><td align="left">0.000</td><td align="left">2.000</td></tr><tr><td align="left">Coronary_artery_disease</td><td char="." align="char">400.0</td><td char="." align="char">0.095000</td><td char="." align="char">0.310186</td><td align="left">0.000</td><td align="left">0.000</td><td align="left">2.000</td></tr><tr><td align="left">Appetite</td><td char="." align="char">400.0</td><td char="." align="char">0.210000</td><td char="." align="char">0.413918</td><td align="left">0.000</td><td align="left">0.000</td><td align="left">2.000</td></tr><tr><td align="left">Peda_edema</td><td char="." align="char">400.0</td><td char="." align="char">0.195000</td><td char="." align="char">0.402965</td><td align="left">0.000</td><td align="left">0.000</td><td align="left">2.000</td></tr><tr><td align="left">Anemia</td><td char="." align="char">400.0</td><td char="." align="char">0.155000</td><td char="." align="char">0.369210</td><td align="left">0.000</td><td align="left">0.000</td><td align="left">2.000</td></tr><tr><td align="left">Class</td><td char="." align="char">400.0</td><td char="." align="char">0.375000</td><td char="." align="char">0.484729</td><td align="left">0.000</td><td align="left">0.000</td><td align="left">1.000</td></tr></tbody></table></table-wrap>
</p><p id="Par41">
<fig id="Fig3"><label>Fig. 3</label><caption><p>Heatmap analysis for the dataset features.</p></caption><graphic xlink:href="41598_2025_2355_Fig3_HTML" id="d33e952"/></fig>
</p><p id="Par42">
<fig id="Fig4"><label>Fig. 4</label><caption><p>Histogram distribution analysis for the dataset features.</p></caption><graphic xlink:href="41598_2025_2355_Fig4_HTML" id="d33e962"/></fig>
</p></sec><sec id="Sec5"><title>Z-score normalization</title><p id="Par43">Normalization is a widely used preprocessing technique in machine learning. It&#x02019;s employed particularly when the dataset exhibits significant variability. The goal of normalization is to analyze all elements within a consistent framework, allowing for comparisons across different scales. Mathematical functions are utilized to transform data from various scales into a standardized scale. This process involves managing the minimum and maximum values of the data, thereby standardizing the range. The aim is to assign a value of 0 to the smallest data point and a value of 1 to the largest, while evenly distributing all other values between this range of 0 to 1. The application of the minimum-maximum formula, also known as Z-score normalization, is demonstrated in Eq.&#x000a0;(<xref rid="Equ1" ref-type="disp-formula">1</xref>).<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e971">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:z=\frac{x-{\upmu\:}}{\sigma\:}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2355_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par44">where <inline-formula id="IEq1"><alternatives><tex-math id="d33e979">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:z$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq1.gif"/></alternatives></inline-formula> is the transformed data, and <inline-formula id="IEq2"><alternatives><tex-math id="d33e985">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:x$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq2.gif"/></alternatives></inline-formula> represents the input value. <inline-formula id="IEq3"><alternatives><tex-math id="d33e991">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\upmu\:}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq3.gif"/></alternatives></inline-formula> is the mean of the data and <inline-formula id="IEq4"><alternatives><tex-math id="d33e997">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\sigma\:$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq4.gif"/></alternatives></inline-formula> is the standard deviation of the data.</p></sec><sec id="Sec6"><title>Machine learning classifiers models</title><p id="Par45">Our study delved into a range of machine learning classifiers using our training dataset. We explored both linear and non-linear algorithms. Recent studies have highlighted the effectiveness of tree-based bagging and boosting classifiers over traditional models like neural networks. Our analysis included various sub-classifiers such as extra trees (ET), random forest (RF), decision tree (DT), bagging classifier (BC), adaptive boosting (AdaBoost), and k-nearest neighbor (KNN).</p><sec id="Sec7"><title>Extra trees (ET) model</title><p id="Par46">Extra trees (ET), also known as extremely randomized trees, is a tree-based ensemble that is comparable to random forest<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. This model, which is somewhat new within the discipline of machine learning, is similar to an extension of the popular random forest technique. Its purpose is to reduce the likelihood of over fitting<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. Using the traditional top-down method, the extra trees model constructs an assembly of un-pruned decisions or regression tree structures. Its two primary distinctions from previous tree-based ensemble approaches are that it grows the trees using the entire learning sample (instead of a bootstrap replica) and separates nodes by selecting cut-points totally at random. The extra trees procedure trains each base estimator using a selected set of features, much like the random forest. It refrains from choosing a feature in addition to its matching value at random for node splitting. Several decision trees are combined to produce a more robust model using ensemble learning model called extra trees (ET)<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. ET divides the node using a randomly selected set of features for every tree and a randomized threshold value for every feature.</p></sec><sec id="Sec8"><title>Random forest (RF) model</title><p id="Par47">Random forest (RF) model, which can be utilized in batching or non-stream training for problems like regression and classification<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. Several decision trees are grown by the algorithm and then combined into one single model. The ultimate prediction produced by the model for the classification issues is determined by the majority of votes cast by each tree, each of which predicts a class. To prevent overfitting and provide a robust prediction, the basic principle underlying RF is to create a forest of trees with minimal or no connections with one another. There are many distinct trees in the forest that can make accurate predictions, even if some of them are noise-sensitive and inaccurate<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. The overall percentage of the trees&#x02019; votes is less likely to overfit, and the anticipated error is less if the decisions made by the trees have not been correlated. Random feature selection and the process of bagging (bootstrap aggregating) are the two techniques that the model employs while constructing each individual tree in order to reduce the associations between the trees and boost the diversity in the forest<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. To get over a single classifier&#x02019;s constraints in generating the best answer, RF is a resilient ensemble learning (EL) approach made up of many decision tree classifiers. Thus, an RF method that employs the majority vote approach and incorporates several trees as opposed to a single tree is used to determine the final class label.</p></sec><sec id="Sec9"><title>Decision tree (DT) model</title><p id="Par48">Decision tree (DT) is a straightforward to use, non-parametric structure classification technique that can regulate nonlinear relationships between classes and attributes<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. A method for building a model of prediction that makes use of the tree structure to address classification issues is the Inter-DT method. Specifically, DT represents a tree-based structure of instructions and may be characterized as a process that recursively divides the data that is input into progressively smaller subsets<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. An array of thresholds defined for each of the inner nodes of the tree serves as the basis for the splitting procedure. Internal nodes divide input data into sub-nodes from the root node, which is the initial node in the DT, and then further divide sub-nodes into more sub-nodes. The data entered are classified using this incrementally binary subdivision, where the final nodes, referred to as leaf nodes (leaves), represent the final target classes<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. The use of DTs has some issues, where the two most significant ones are overfitting and producing a suboptimal solution<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. Because a DT just uses one tree, it could not yield the best final model. When utilizing DT, overfitting is another prevalent issue that must be considered. The inner nodes (decision nodes) of the data arrangement within the tree are used to test the qualities of the data, while the edges branch the test results, and the leaves indicate the problem&#x02019;s class labels. By choosing the most useful data characteristic for dividing the data into every node, the DT construction algorithm finds ways to partition the training set<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. The construction technique can be employed within the train phase in which it begins execution at the tree&#x02019;s root, which holds the entire dataset.</p></sec><sec id="Sec10"><title>Bagging classifier (BC) model</title><p id="Par49">Bagging classifier is a model for building classifier ensembles that are conceptually similar but fundamentally different<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. Bagging was developed by Breiman<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, which was then expanded to Arcing to support the adaptive incremental generation of the ensemble that forms the basis of the Boosting approach. Using bootstrap sampling to generate random samples from the data set, bagging builds an ensemble of classifiers by creating one classifier for each bootstrap sample. The final decision of classification for an unlabeled data x has been determined through selecting a majority vote among the class labels generated using the L models<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>. The true strength of Bagging is for unstable classifiers like decision trees and neural networks. Small changes in the data set might cause unstable classifiers to become sensitive<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>. As a consequence, two marginally distinct sets of training data used to train the same classifying model might produce very different classifiers. Although the classifiers&#x02019; overall accuracies may be comparable, there will be a natural ensemble variety because of the differences in the parameters (such as the neural network&#x02019;s weights).</p></sec><sec id="Sec11"><title>Adaptive boosting (AdaBoost) model</title><p id="Par50">Adaptive boosting, often known as AdaBoost, is the most widely used boosting algorithm<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. AdaBoost also needs less algorithmic parameter adjusting than other boost algorithm versions. An AdaBoost model for classification starts by fitting a copy of the original dataset using a reproduce of the previous classifier that has been updated to remove error-prone and inaccurate data points. This allows the subsequent classifiers to concentrate on the cases that lead to greater inaccuracy<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. AdaBoost is able to produce a hypothesis based on potential labels. The error in prediction of the weak assumption ought to be smaller than 0.5 during training<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. Choosing the &#x0201c;hard&#x0201d; trained data and samples given to the following iteration is the aim of distribution. By using weighted majority voting to determine the classes from each individual hypothesis&#x02019; prediction, AdaBoost creates an ensemble and a collection of hypotheses. AdaBoost is a kind of iterative computation whose basic idea is to train several classifiers (that is, weak learners) using a preparation set, and then use several different coordination techniques to create an additional robust model.</p></sec><sec id="Sec12"><title>K-Nearest neighbor (KNN) model</title><p id="Par51">K-nearest neighbors (KNN) stand out as a straightforward, yet robust algorithm employed in both classification and regression tasks<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. In classification, it assigns a label to a data point by assessing the majority class among its K closest neighbors within the feature space. Unlike many other algorithms, KNN doesn&#x02019;t undergo an explicit training phase<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>. Instead, it just remembers the training data. When presented with a new data point for classification, KNN identifies its K nearest neighbors based on a chosen distance metric, typically the Euclidean distance. Once the K nearest neighbors is identified, KNN assigns the class label to the new data point by conducting a majority vote among its neighbors. The class that occurs most frequently among the K neighbors becomes the predicted class for the new data point. The selection of the parameter K, representing the number of neighbors to consider, holds significance in KNN. A smaller K value enhances sensitivity to noise and outliers, while a larger K value might lead to smoothing out decision boundaries, potentially resulting in underfitting<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. Determining the optimal K value often entails employing cross-validation or other tuning techniques. The choice of distance metric greatly influences KNN&#x02019;s performance. While the Euclidean distance is widely used, alternative metrics such as Manhattan distance, Minkowski distance, or cosine similarity may better suit the data characteristics.</p></sec></sec><sec id="Sec13"><title>Explainable artificial intelligence</title><p id="Par52">Explainable artificial intelligence (XAI) refers to the set of techniques and methods aimed at making artificial intelligence models and their decisions understandable and interpretable to humans. As AI systems become increasingly prevalent in critical applications such as healthcare, finance, and criminal justice, the need for transparency and accountability in AI decision-making has grown significantly. The main goal of this research is to develop classification pipelines and improve the interpretability of the resulting predictions. The intensity of red in the plot corresponds to the magnitude of the feature value<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>. In this study, we utilized a tool called shapley additive explanations (SHAP) to elucidate various classifiers. SHAP is a widely used method in explainable AI for interpreting game theoretical classifiers<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. It evaluates shapley values for each feature to understand their impact on predictions. The SHAP plot provides a detailed overview of the classifiers&#x02019; insights on a global scale. Each dot on the plot represents an individual data point, while the features are listed along the y-axis in descending order. SHAP values are depicted on the x-axis, and the color gradient indicates the feature values.</p></sec><sec id="Sec14"><title>K-Nearest neighbor (KNN) imputer</title><p id="Par53">The k-nearest neighbor (KNN) imputer is a method utilized to fill in missing values within a dataset by considering the values of its closest neighboring data points<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. The underlying principle is that data points resembling each other tend to possess similar feature values. When encountering a missing value in the dataset, the KNN imputer locates the K nearest neighbors to the data point containing the missing value. These neighbors are determined based on a distance metric like Euclidean distance. After identifying the nearest neighbors, the KNN imputer computes the average (or weighted average) of the feature values associated with the missing value across these neighboring data points. Subsequently, this calculated value fills in the missing value within the dataset<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. The parameter K, representing the number of neighbors considered, plays a critical role in KNN imputation. Opting for a smaller K value may result in imputed values closely resembling those of the nearest neighbors, while a larger K value may yield a smoother imputation, albeit potentially introducing noise from more distant neighbors. KNN imputation extends its applicability to categorical variables by treating them as discrete points within a multidimensional space<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. In this scenario, instead of computing averages, the mode (most frequent value) of the categorical feature across the nearest neighbors is employed for imputation.</p></sec><sec id="Sec15"><title>Binary Breadth-First search (BBFS)</title><p id="Par54">Binary breadth-first search (BBFS) is a feature selection algorithm that is often used in the context of binary classification problems<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Its objective is to identify and select the most relevant features from a given set of features to improve the performance of a machine learning model<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>. The algorithm is based on the principles of breadth-first search (BFS), a graph traversal algorithm, and it efficiently explores the feature space<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>. The mathematical algorithm for binary breadth-first search for feature selection is illustrated in Algorithm <xref ref-type="sec" rid="FPar1">1</xref>.</p><p id="Par420">
<fig position="anchor" id="Figa"><label>Algorithm 1</label><caption><p>Binary Breadth-First Search (BBFS).</p></caption><graphic position="anchor" xlink:href="41598_2025_2355_Figa_HTML" id="d33e1170"/></fig>
</p><p id="Par81">Algorithm 1 systematically explores the feature space in a breadth-first manner, ensuring that the selected subset is chosen based on the performance of the model used. The stopping criterion helps control the search space and prevent exhaustive exploration.</p></sec><sec id="Sec17"><title>Evaluation metrices</title><p id="Par82">In this study, five evaluation metrics: accuracy, sensitivity, specificity, F-score, and area under the ROC curve (AUC) are utilized to assess the performance of the models. These evaluation criteria, outlined from Eq.&#x000a0;<xref rid="Equ1" ref-type="disp-formula">1</xref> to Eq.&#x000a0;5, serve as benchmarks for evaluating the models. During both the training and testing phases of data classification problems, these evaluation metrics are applied<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>. In the training phase, the assessment metric serves as a discriminator, aiding in the selection of the most effective classifier for future assessments. Conversely, during the testing phase, the evaluation metrics act as evaluators, gauging the classifier&#x02019;s performance when confronted with unknown data.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e1186">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\text{Accuracy}}\:{\text{ = }}\frac{{{\text{TP + TN}}}}{{{\text{TP + TN + FP + FN}}}}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2355_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e1192">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{S}\text{e}\text{n}\text{s}\text{i}\text{t}\text{i}\text{v}\text{i}\text{t}\text{y}\:=\frac{TP}{TP+FN}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2355_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e1198">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{S}\text{e}\text{n}\text{s}\text{i}\text{t}\text{i}\text{v}\text{i}\text{t}\text{y}\:=\frac{TP}{TP+FN}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2355_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e1204">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{F}-\text{s}\text{c}\text{o}\text{r}\text{e}=2*\frac{Precision*Recall}{Precision+Recall}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2355_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="d33e1210">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:AUC=\frac{{S}_{p}-{N}_{p}+({N}_{n}+1)/2}{{N}_{p}{N}_{n}}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2355_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par84">where <inline-formula id="IEq10"><alternatives><tex-math id="d33e1218">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:TP$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq10.gif"/></alternatives></inline-formula>, <inline-formula id="IEq11"><alternatives><tex-math id="d33e1224">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:TN$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq11.gif"/></alternatives></inline-formula>, <inline-formula id="IEq12"><alternatives><tex-math id="d33e1230">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:FP$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq12.gif"/></alternatives></inline-formula>, <inline-formula id="IEq13"><alternatives><tex-math id="d33e1236">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:FN$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq13.gif"/></alternatives></inline-formula> represent true positive, true negative, false positive, and false negative values, respectively. <inline-formula id="IEq14"><alternatives><tex-math id="d33e1242">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{S}_{p}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq14.gif"/></alternatives></inline-formula> is the proportion of correctly classified cases belonging to the negative class, and <inline-formula id="IEq15"><alternatives><tex-math id="d33e1249">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{N}_{p}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq15.gif"/></alternatives></inline-formula>
<inline-formula id="IEq16"><alternatives><tex-math id="d33e1255">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\text{a}\text{n}\text{d}\:N}_{n}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2355_Article_IEq16.gif"/></alternatives></inline-formula> are the number of positive and negative classes, respectively.</p></sec></sec><sec id="Sec18"><title>Results and discussion</title><p id="Par85">The experiments were conducted using Jupyter Notebook version 6.4.6, a widely used tool for Python-based data analysis and visualization. Jupyter Notebook provides a robust platform for coding, executing, and documenting processes, along with generating graphical representations of data. This versatile tool supports multiple programming languages, including Python 3.8, and is accessible via a web browser. The experiments were performed on a Microsoft Windows 10 computer equipped with an Intel Core i5 CPU and 16 GB of RAM. In this study, the explainable artificial intelligence-CKD (XAI-CKD) model is introduced for the process of classification. XAI-CKD applies explainable AI (XAI) by utilizing extra trees (ET) and SHAP (shapley additive explanations) values. Also, binary breadth-first search (BBFS) algorithm is used to select the most important features for XAI-CKD model. The selected features using BBFS algorithm are specific gravity, red blood cells, albumin, hypertension, hemoglobin, diabetes mellitus, packed cell volume, appetite, peda edema, pus cell, red blood cell count, blood glucose random, sugar, coronary artery disease, age, pus cell clumps, and bacteria. This methodology is designed to derive valuable insights for enhancing decision-making strategies within the field of classifying chronic kidney diseases. The performance of XAI-CKD is compared with another machine learning models, namely, random forest (RF), decision tree (DT), bagging classifier (BC), adaptive boosting (AdaBoost), and k-nearest neighbor (KNN), and the performance of the models is evaluated using accuracy, sensitivity, specificity, F-score, and area under the ROC curve (AUC).</p><p id="Par86">To ensure the robustness of the results, each experiment was conducted over ten independent runs. In each run, the dataset was randomly split into training (70%) and testing (30%) sets, and the models were retrained and evaluated from scratch. Performance metrics such as accuracy, sensitivity, specificity, F-score, and AUC were averaged across these runs to reduce the effects of any variance due to random data partitioning in the training process.</p><p id="Par87">To ensure a fair comparison among all models, we employed grid search with 5-fold cross-validation for hyperparameter tuning. This approach exhaustively searches through specified hyperparameter ranges to identify the best configuration for each model based on validation performance. Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> presents the hyperparameter tuning process using grid search with 5-fold cross-validation for each machine learning model. For each model, key hyperparameters were explored over a defined search range based. The table includes the search ranges used during tuning and the optimal values selected based on validation performance. ET model was evaluated with different values for n_estimators ranging from 50 to 200, and criterion hyperparameter was tested with &#x02018;gini&#x02019; and &#x02018;entropy&#x02019;. The optimal configuration selected was n_estimators&#x02009;=&#x02009;50 and criterion = &#x02018;gini&#x02019;. Similarly, RF model performed best with n_estimators&#x02009;=&#x02009;150 and criterion = &#x02018;entropy&#x02019;. DT model performed best for a randomized splitting strategy and the entropy criterion. BC model achieved optimal results with 50 estimators and max_samples&#x02009;=&#x02009;1.0. AdaBoost model achieved optimal results with a low learning rate of 0.01 and 100 estimators. KNN model demonstrated optimal results using 30 neighbors and distance weighting.</p><p id="Par88">
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Hyperparameter search ranges and optimal configurations for each model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Hyperparameter</th><th align="left">Search Range</th><th align="left">Best Value</th></tr></thead><tbody><tr><td align="left">ET</td><td align="left">n_estimators</td><td align="left">[50, 100, 150, 200]</td><td align="left">50</td></tr><tr><td align="left"/><td align="left">criterion</td><td align="left">[&#x02018;gini&#x02019;, &#x02018;entropy&#x02019;]</td><td align="left">gini</td></tr><tr><td align="left">RF</td><td align="left">n_estimators</td><td align="left">[100, 150, 200]</td><td align="left">150</td></tr><tr><td align="left"/><td align="left">criterion</td><td align="left">[&#x02018;gini&#x02019;, &#x02018;entropy&#x02019;]</td><td align="left">entropy</td></tr><tr><td align="left">DT</td><td align="left">splitter</td><td align="left">[&#x02018;best&#x02019;, &#x02018;random&#x02019;]</td><td align="left">random</td></tr><tr><td align="left"/><td align="left">criterion</td><td align="left">[&#x02018;gini&#x02019;, &#x02018;entropy&#x02019;]</td><td align="left">entropy</td></tr><tr><td align="left">BC</td><td align="left">n_estimators</td><td align="left">[10, 50, 100]</td><td align="left">50</td></tr><tr><td align="left"/><td align="left">max_samples</td><td align="left">[0.5, 0.7, 1.0]</td><td align="left">1.0</td></tr><tr><td align="left">AdaBoost</td><td align="left">n_estimators</td><td align="left">[50, 100, 150]</td><td align="left">100</td></tr><tr><td align="left"/><td align="left">learning_rate</td><td align="left">[0.01, 0.1, 0.5, 1.0]</td><td align="left">0.01</td></tr><tr><td align="left">KNN</td><td align="left">n_neighbors</td><td align="left">[5, 10, 20, 30, 40]</td><td align="left">30</td></tr><tr><td align="left"/><td align="left">weights</td><td align="left">[&#x02018;uniform&#x02019;, &#x02018;distance&#x02019;]</td><td align="left">distance</td></tr></tbody></table></table-wrap>
</p><p id="Par89">Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> demonstrates the performance of the proposed XAI-CKD model compared to another five traditional machine learning models, namely, random forest (RF), decision tree (DT), bagging classifier (BC), adaptive boosting (AdaBoost), and k-nearest neighbor (KNN) in terms of accuracy, sensitivity, specificity, F-score, and area under the ROC curve (AUC). As depicted in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>, the proposed XAI-CKD model achieved the best results with accuracy of 99.9%, sensitivity of 99.9%, specificity of 99.9%, F-score of 99.9% that reflects a balance between precision and recall, and AUC of 1.0 that indicates excellent discrimination ability. The KNN model achieved the worth results with accuracy of 66.6%, sensitivity of 66.7%, specificity of 66.7%, F-score of 66.6% and AUC of 0.673. The second model achieved the best results after the proposed XAI-CKD model is RF model, its accuracy, sensitivity, specificity, F-score, and AUC are 97.5%, 97.5%, 97.5%, 97.4%, and 0.968. The third model achieved best results after the RF model is DT model, its accuracy, sensitivity, specificity, F-score, and AUC are 95.8%, 95.8%, 95.8%, 95.8%, and 0.955. The fourth model achieved best results after the DT model is BC model, its accuracy, sensitivity, specificity, F-score, and AUC are 94.1%, 94.1%, 94.2%, 94.1%, and 0.936. The fifth model achieved the best results after the BC model is Adaboost model, its accuracy, sensitivity, specificity, F-score, and AUC are 85%, 85%, 85.2%, 85.3%, and 0.862.</p><p id="Par90">
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance of the proposed XAI-CKD model and another traditional machine learning models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Models</th><th align="left">Accuracy</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">F-score</th><th align="left">AUC</th></tr><tr><th align="left">XAI-CKD</th><th align="left">99.9%</th><th align="left">99.9%</th><th align="left">99.9%</th><th align="left">99.9%</th><th align="left">1.000</th></tr></thead><tbody><tr><td align="left">RF</td><td align="left">97.5%</td><td align="left">97.5%</td><td char="." align="char">97.5%</td><td char="." align="char">97.4%</td><td char="." align="char">0.968</td></tr><tr><td align="left">DT</td><td align="left">95.8%</td><td align="left">95.8%</td><td char="." align="char">95.8%</td><td char="." align="char">95.8%</td><td char="." align="char">0.955</td></tr><tr><td align="left">BC</td><td align="left">94.1%</td><td align="left">94.1%</td><td char="." align="char">94.2%</td><td char="." align="char">94.1%</td><td char="." align="char">0.936</td></tr><tr><td align="left">AdaBoost</td><td align="left">85%</td><td align="left">85%</td><td char="." align="char">85.2%</td><td char="." align="char">85.3%</td><td char="." align="char">0.862</td></tr><tr><td align="left">KNN</td><td align="left">66.6%</td><td align="left">66.7%</td><td char="." align="char">66.7%</td><td char="." align="char">66.6%</td><td char="." align="char">0.673</td></tr></tbody></table></table-wrap>
</p><p id="Par91">Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> depicts the performance of the BBFS algorithm against another feature selection algorithms, namely, binary whale optimization algorithm (BWOA), binary particle swarm optimization (BPSO), and binary grey wolf optimizer (BGWO) in terms of accuracy, sensitivity, specificity, F-score and AUC.</p><p id="Par92">
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Comparison between the outcomes of BBFS feature selection algorithm and several feature selection algorithms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Models</th><th align="left">Accuracy</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">F-score</th><th align="left">AUC</th></tr><tr><th align="left">XAI-CKD-BBFS</th><th align="left">99.9%</th><th align="left">99.9%</th><th align="left">99.9%</th><th align="left">99.9%</th><th align="left">1.000</th></tr></thead><tbody><tr><td align="left">XAI-CKD-BWOA</td><td char="." align="char">98.01%</td><td char="." align="char">98.01%</td><td char="." align="char">98.02%</td><td char="." align="char">98.01%</td><td char="." align="char">0.984</td></tr><tr><td align="left">XAI-CKD-BPSO</td><td char="." align="char">97.75%</td><td char="." align="char">97.75%</td><td char="." align="char">97.75%</td><td char="." align="char">97.76%</td><td char="." align="char">0.979</td></tr><tr><td align="left">XAI-CKD-BGWO</td><td char="." align="char">95.84%</td><td char="." align="char">95.85%</td><td char="." align="char">95.84%</td><td char="." align="char">95.84%</td><td char="." align="char">0.957</td></tr></tbody></table></table-wrap>
</p><p id="Par93">From Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>, the best results are obtained by XAI-CKD-BBFS model with accuracy of 99.9%, sensitivity of 99.9%, specificity of 99.9%, F-score of 99.9% that reflects a balance between precision and recall, and AUC of 1.0 that indicates excellent discrimination ability. The XAI-CKD-BGWO model achieved the worth results with accuracy of 95.84%, sensitivity of 95.85%, specificity of 95.84%, F-score of 95.84% and AUC of 0.957.</p><p id="Par94">Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref> demonstrates the configuration of the hyperparameters for the feature selection algorithms used in this study.</p><p id="Par95">
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Hyperparameters for the feature selection algorithms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Algorithm</th><th align="left">Hyperparameters</th><th align="left">Values</th></tr></thead><tbody><tr><td align="left">BBFS</td><td align="left"><p>Iterations</p><p>Number of vertices (<italic>V</italic>)</p><p>Number of edges (<italic>E</italic>)</p></td><td char="." align="char"><p>100</p><p>10</p><p>15</p></td></tr><tr><td align="left">BWOA</td><td align="left"><p>Iterations</p><p>Population size</p></td><td char="." align="char"><p>100</p><p>150</p></td></tr><tr><td align="left">BPSO</td><td align="left"><p>Iterations</p><p>Acceleration constants</p><p>Inertia Wmax, Wmin</p><p>Particles</p></td><td char="." align="char"><p>100</p><p>[8, 8]</p><p>[0.6, 0.9]</p><p>50</p></td></tr><tr><td align="left">BGWO</td><td align="left"><p>Iterations</p><p>Wolves</p></td><td char="." align="char"><p>100</p><p>50</p></td></tr></tbody></table></table-wrap>
</p><p id="Par96">Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref> demonstrates the results of median fitness, mean fitness, best fitness, worst fitness, and standard deviation fitness for four algorithms, namely, BBFS, BWOA, BPSO, and BGWO. BBFS achieved the best results in terms of median fitness, mean fitness, best fitness, worst fitness, and standard deviation fitness.</p><p id="Par97">
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Performance of BBFS model compared to another algorithms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Metrics</th><th align="left">BBFS</th><th align="left">BWOA</th><th align="left">BPSO</th><th align="left">BGWO</th></tr></thead><tbody><tr><td align="left">Median fitness</td><td char="." align="char">0.42515</td><td char="." align="char">0.45538</td><td char="." align="char">0.46962</td><td char="." align="char">0.47364</td></tr><tr><td align="left">Mean fitness</td><td char="." align="char">0.44197</td><td char="." align="char">0.46887</td><td char="." align="char">0.46764</td><td char="." align="char">0.47548</td></tr><tr><td align="left">Best fitness</td><td char="." align="char">0.34377</td><td char="." align="char">0.38917</td><td char="." align="char">0.44794</td><td char="." align="char">0.43958</td></tr><tr><td align="left">Worst fitness</td><td char="." align="char">0.65371</td><td char="." align="char">0.71512</td><td char="." align="char">0.73646</td><td char="." align="char">0.74154</td></tr><tr><td align="left">Standard deviation fitness</td><td char="." align="char">0.22427</td><td char="." align="char">0.27967</td><td char="." align="char">0.27944</td><td char="." align="char">0.28168</td></tr></tbody></table></table-wrap>
</p><p id="Par98">Figures (<xref rid="Fig5" ref-type="fig">5</xref>, <xref rid="Fig6" ref-type="fig">6</xref>, <xref rid="Fig7" ref-type="fig">7</xref>, <xref rid="Fig8" ref-type="fig">8</xref>, <xref rid="Fig9" ref-type="fig">9</xref>,<xref rid="Fig10" ref-type="fig">10</xref>) demonstrate the confusion matrix for the proposed XAI-CKD, random forest (RF), decision tree (DT), bagging classifier (BC), adaptive boosting (AdaBoost), and k-nearest neighbor (KNN) models during the testing phase.</p><p id="Par99">
<fig id="Fig5"><label>Fig. 5</label><caption><p>Confusion matrix for the proposed XAI-CKD model.</p></caption><graphic xlink:href="41598_2025_2355_Fig5_HTML" id="d33e1800"/></fig>
</p><p id="Par100">
<fig id="Fig6"><label>Fig. 6</label><caption><p>Confusion matrix for RF model.</p></caption><graphic xlink:href="41598_2025_2355_Fig6_HTML" id="d33e1810"/></fig>
</p><p id="Par101">
<fig id="Fig7"><label>Fig. 7</label><caption><p>Confusion matrix for DT model.</p></caption><graphic xlink:href="41598_2025_2355_Fig7_HTML" id="d33e1820"/></fig>
</p><p id="Par102">
<fig id="Fig8"><label>Fig. 8</label><caption><p>Confusion matrix for BC model.</p></caption><graphic xlink:href="41598_2025_2355_Fig8_HTML" id="d33e1830"/></fig>
</p><p id="Par103">
<fig id="Fig9"><label>Fig. 9</label><caption><p>Confusion matrix for Adaboost model.</p></caption><graphic xlink:href="41598_2025_2355_Fig9_HTML" id="d33e1840"/></fig>
</p><p id="Par104">
<fig id="Fig10"><label>Fig. 10</label><caption><p>Confusion matrix for KNN model.</p></caption><graphic xlink:href="41598_2025_2355_Fig10_HTML" id="d33e1850"/></fig>
</p><p id="Par105">The area under the ROC curve (AUC), is determined by plotting the receiver operating characteristic (ROC) curve, illustrating how a model&#x02019;s true positive rate (sensitivity) compares to its false positive rate (1-specificity) at different decision thresholds. A higher AUC indicates a more effective model. Figures&#x000a0;(11&#x02013;16) showcase the AUC values for the models XAI-CKD, random forest (RF), decision tree (DT), bagging classifier (BC), adaptive boosting (AdaBoost), and k-nearest neighbor (KNN) models during the testing phase. Notably, the XAI-Med model exhibits an AUC of 1.0, which is considered excellent.</p><p id="Par106">
<fig id="Fig11"><label>Fig. 11</label><caption><p>AUC for the proposed XAI-CKD model.</p></caption><graphic xlink:href="41598_2025_2355_Fig11_HTML" id="d33e1863"/></fig>
</p><p id="Par107">
<fig id="Fig12"><label>Fig. 12</label><caption><p>AUC for RF model.</p></caption><graphic xlink:href="41598_2025_2355_Fig12_HTML" id="d33e1873"/></fig>
</p><p id="Par108">
<fig id="Fig13"><label>Fig. 13</label><caption><p>AUC for DT model.</p></caption><graphic xlink:href="41598_2025_2355_Fig13_HTML" id="d33e1883"/></fig>
</p><p id="Par109">
<fig id="Fig14"><label>Fig. 14</label><caption><p>AUC for BC model.</p></caption><graphic xlink:href="41598_2025_2355_Fig14_HTML" id="d33e1893"/></fig>
</p><p id="Par110">
<fig id="Fig15"><label>Fig. 15</label><caption><p>AUC for Adaboost model.</p></caption><graphic xlink:href="41598_2025_2355_Fig15_HTML" id="d33e1903"/></fig>
</p><p id="Par111">
<fig id="Fig16"><label>Fig. 16</label><caption><p>AUC for KNN model.</p></caption><graphic xlink:href="41598_2025_2355_Fig16_HTML" id="d33e1913"/></fig>
</p><p id="Par112">Table&#x000a0;<xref rid="Tab7" ref-type="table">7</xref> demonstrates the selected features using BBFS algorithm.</p><p id="Par113">
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Selected features using BBFS algorithm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Features names</th></tr></thead><tbody><tr><td align="left">age</td></tr><tr><td align="left">specific_gravity</td></tr><tr><td align="left">albumin</td></tr><tr><td align="left">sugar</td></tr><tr><td align="left">red_blood_cells</td></tr><tr><td align="left">pus_cell</td></tr><tr><td align="left">pus_cell_clumps</td></tr><tr><td align="left">bacteria</td></tr><tr><td align="left">blood_glucose_random</td></tr><tr><td align="left">haemoglobin</td></tr><tr><td align="left">packed_cell_volume</td></tr><tr><td align="left">red_blood_cell_count</td></tr><tr><td align="left">hypertension</td></tr><tr><td align="left">diabetes_mellitus</td></tr><tr><td align="left">coronary_artery_disease</td></tr><tr><td align="left">appetite</td></tr><tr><td align="left">peda_edema</td></tr></tbody></table></table-wrap>
</p><p id="Par114">Figure <xref rid="Fig17" ref-type="fig">17</xref> illustrates the significance of different features in CKD classification, utilizing the XAI-CKD model. We employed the extra tree (ET) model to compute SHAP (shapley additive explanations) values, aiming to provide deeper insights for improved decision-making in the classification process, as depicted in the Fig.&#x000a0;<xref rid="Fig17" ref-type="fig">17</xref>. These SHAP summary plots, generated through the ET approach, offer valuable insights crucial for decision-making processes in CKD classification. They visually represent the contribution of each input feature towards the classification, illustrating both the average contribution and the potential range of contributions for each feature. This visualization method provides a clear understanding of the relationships between input variables and the resulting classification, helping identify the most influential factors. Features are ranked on the y-axis based on their average absolute SHAP values, indicating their importance in the model&#x02019;s predictions. On the x-axis, SHAP values themselves are represented. Positive SHAP values for a feature indicate its presence drives the model&#x02019;s prediction towards a positive outcome (diagnosis of CKD), while negative SHAP values suggest a tendency towards a negative outcome. Red dots represent instances where an individual is diagnosed with CKD (a positive outcome), while blue dots indicate a tendency towards a negative diagnosis.</p><p id="Par115">
<fig id="Fig17"><label>Fig. 17</label><caption><p>Contribution of the features using XAI- CKD model for chronic kidney disease classification.</p></caption><graphic xlink:href="41598_2025_2355_Fig17_HTML" id="d33e2003"/></fig>
</p><p id="Par116">The analysis of the SHAP plots provides a clearer explanation of how specific features influence CKD predictions. Higher blood pressure strongly increases the likelihood of CKD, as shown by the SHAP values. This finding aligns with hypertension is well-known role in damaging kidney function, making it a primary risk factor for CKD. Elevated serum creatinine levels emerged as a significant predictor of CKD. This result reflects its clinical importance as an indicator of the reduced glomerular filtration rate (GFR), and a key criterion for CKD diagnosis. High albumin levels consistently pushed predictions toward a CKD classification. This highlights its role as an early biomarker of kidney damage and emphasizes its diagnostic value. Elevated blood pressure and serum creatinine underline the importance of early detection and management to slow CKD progression. Anemia indicators like hemoglobin and packed cell volume reinforce the need for tailored clinical strategies to manage anemia, a common complication in advanced CKD. Blood glucose and hypertension demonstrate the model&#x02019;s ability to identify systemic conditions, such as diabetes and hypertension, which are leading causes of CKD. This insight can support a lot of comprehensive risk assessments.</p><p id="Par117">Figure <xref rid="Fig18" ref-type="fig">18</xref> demonstrates the convergence curves of feature selection algorithms used in this study.</p><p id="Par118">
<fig id="Fig18"><label>Fig. 18</label><caption><p>Convergence plots of the optimization algorithms.</p></caption><graphic xlink:href="41598_2025_2355_Fig18_HTML" id="d33e2021"/></fig>
</p><p id="Par119">Figure <xref rid="Fig19" ref-type="fig">19</xref> presents the box plot of the objective values across 10 runs for feature selection algorithms used in this study.</p><p id="Par120">
<fig id="Fig19"><label>Fig. 19</label><caption><p>Box plots of objective values for the optimization algorithms.</p></caption><graphic xlink:href="41598_2025_2355_Fig19_HTML" id="d33e2036"/></fig>
</p><sec id="Sec19"><title>SHAP analysis and clinical relevance</title><p id="Par121">The SHAP values to identify the features with the greatest impact on CKD classification. Our findings highlight the following features as the most important: specific gravity, albumin, hemoglobin, packed cell volume, red blood cell count, blood glucose random, and hypertension. These features are highly relevant in predicting CKD and align closely with established clinical knowledge. Specific gravity and albumin are essential markers of kidney function. Specific gravity indicates the kidneys&#x02019; ability to concentrate urine, while albumin reflects proteinuria, a key symptom of CKD. Hemoglobin and packed cell volume are closely linked to anemia, a common complication of CKD caused by reduced erythropoietin production. Blood glucose random and hypertension represent the two leading causes of CKD worldwide, reflecting the model&#x02019;s ability to capture key risk factors. Red blood cell count is a critical indicator, as CKD can impair erythropoiesis, leading to lower red blood cell production. For the alignment with clinical knowledge, specific gravity and albumin are integral to CKD diagnosis and staging. The inclusion of blood glucose random and hypertension reinforces the model&#x02019;s clinical validity by recognizing the most common causes of CKD. Features such as hemoglobin and packed cell volume are vital for managing CKD complications like anemia, improving both patient outcomes and quality of life. For the practical Implications for clinicians, early detection of abnormalities in specific gravity and albumin can prompt timely interventions to prevent CKD progression Monitoring anemia related features such as hemoglobin and packed cell volume can help inform therapeutic strategies, including the use of erythropoiesis-stimulating agents or iron supplements. The model&#x02019;s identification of hypertension and blood glucose random underlines the importance of managing these risk factors to mitigate CKD development. For the enhanced SHAP plot interpretation, positive SHAP values indicate features that increase the likelihood of CKD, while negative values reduce it. For instance, low specific gravity and high albumin levels are strong indicators of CKD, consistent with impaired kidney function. Similarly, low hemoglobin levels significantly drive CKD predictions, highlighting the established link between CKD and anemia.</p></sec><sec id="Sec20"><title>Statistical analysis results</title><p id="Par122">Table&#x000a0;<xref rid="Tab8" ref-type="table">8</xref> demonstrates the descriptive analysis for six models: XAI-CKD, RF, DT, BC, AdaBoost, and KNN. XAI-CKD model has the highest mean value at 80.12, that indicates better performance compared to other models. KNN model achieves the lowest mean value at 53.4546. The highest standard deviation is 44.2294 for the XAI-CKD model, this demonstrates that there is more variation in the model&#x02019;s performance, the smallest standard deviation is 29.505854 for KNN model suggests more consistency in the results. For XAI-CKD model, the maximum value is 99.9, this indicates a good performance for the model. The 95% confidence interval for XAI-CKD is (25.2019, 135.0381), which indicates that the true mean of the model performance lies within the range. The 95% confidence interval for KNN is much smaller (16.8182, 90.0910), which means less uncertainty about the model performance. Table&#x000a0;<xref rid="Tab9" ref-type="table">9</xref> presents the Wilcoxon rank sum test results of the proposed XAI-CKD model against several models. From the data demonstrated in this Table&#x000a0;<xref rid="Tab9" ref-type="table">9</xref>, the proposed XAI-CKD model outperforms the other models. Table&#x000a0;<xref rid="Tab10" ref-type="table">10</xref> presents the ANOVA test results for the proposed XAI-CKD model for CKD prediction. The ANOVA table demonstrates a significant difference in performance, as indicated by the high F-statistic (76.72) and a very small P value (1.15&#x02009;&#x000d7;&#x02009;10&#x0207b;&#x000b9;&#x000b3;).</p><p id="Par123">
<table-wrap id="Tab8"><label>Table 8</label><caption><p>Descriptive analysis results of the proposed XAI-CKD model and another models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Mean</th><th align="left">Standard deviation</th><th align="left">Maximum</th><th align="left">95% Confidence interval</th></tr></thead><tbody><tr><td align="left">XAI-CKD</td><td char="." align="char">80.12</td><td char="." align="char">44.229425</td><td char="." align="char">99.9</td><td char="." align="char">(25.2019, 135.0381)</td></tr><tr><td align="left">RF</td><td char="." align="char">78.1736</td><td char="." align="char">43.159264</td><td char="." align="char">97.5</td><td char="." align="char">(24.5843, 131.7629)</td></tr><tr><td align="left">DT</td><td char="." align="char">76.831</td><td char="." align="char">42.415973</td><td char="." align="char">95.8</td><td char="." align="char">(24.1646, 129.4974)</td></tr><tr><td align="left">BC</td><td char="." align="char">75.4872</td><td char="." align="char">41.675410</td><td char="." align="char">94.2</td><td char="." align="char">(23.7403, 127.2341)</td></tr><tr><td align="left">AdaBoost</td><td char="." align="char">68.2724</td><td char="." align="char">37.683783</td><td char="." align="char">85.3</td><td char="." align="char">(21.4818, 115.0630)</td></tr><tr><td align="left">KNN</td><td char="." align="char">53.4546</td><td char="." align="char">29.505854</td><td char="." align="char">66.7</td><td char="." align="char">(16.8182, 90.0910)</td></tr></tbody></table></table-wrap>
</p><p id="Par124">
<table-wrap id="Tab9"><label>Table 9</label><caption><p>Wilcoxon rank sum test results of the proposed XAI-CKD model and another models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">XAI-CKD</th><th align="left">RF</th><th align="left">DT</th><th align="left">BC</th><th align="left">AdaBoost</th><th align="left">KNN</th></tr></thead><tbody><tr><td align="left">Theoretical Median</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td></tr><tr><td align="left">Actual Median</td><td align="left">99.9</td><td align="left">97.5</td><td align="left">95.8</td><td align="left">94.1</td><td align="left">85.0</td><td align="left">66.6</td></tr><tr><td align="left">Number of Values</td><td align="left">6</td><td align="left">6</td><td align="left">6</td><td align="left">6</td><td align="left">6</td><td align="left">6</td></tr><tr><td align="left">Sum of Signed Ranks (W)</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td></tr><tr><td align="left">Sum of Positive Ranks</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td></tr><tr><td align="left">Sum of Negative Ranks</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td><td align="left">0.0</td></tr><tr><td align="left">P Value (Two-Tailed)</td><td align="left">0.03125</td><td align="left">0.03125</td><td align="left">0.03125</td><td align="left">0.03125</td><td align="left">0.03125</td><td align="left">0.03125</td></tr><tr><td align="left">Exact or Estimate?</td><td align="left">Exact</td><td align="left">Exact</td><td align="left">Exact</td><td align="left">Exact</td><td align="left">Exact</td><td align="left">Exact</td></tr><tr><td align="left">P Value Summary</td><td align="left">**</td><td align="left">**</td><td align="left">**</td><td align="left">**</td><td align="left">**</td><td align="left">**</td></tr><tr><td align="left">Significant (alpha&#x02009;=&#x02009;0.05)?</td><td align="left">Yes</td><td align="left">Yes</td><td align="left">Yes</td><td align="left">Yes</td><td align="left">Yes</td><td align="left">Yes</td></tr><tr><td align="left">Discrepancy</td><td align="left">99.9</td><td align="left">97.5</td><td align="left">95.8</td><td align="left">94.1</td><td align="left">85.0</td><td align="left">66.6</td></tr></tbody></table></table-wrap>
</p><p id="Par125">
<table-wrap id="Tab10"><label>Table 10</label><caption><p>The ANOVA test results for the proposed XAI-CKD model for CKD prediction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">SS</th><th align="left">DF</th><th align="left">MS</th><th align="left">F (DFn, DFd)</th><th align="left"><italic>P</italic> value</th></tr></thead><tbody><tr><td align="left">Treatment (between columns)</td><td char="." align="char">37975.402551</td><td char="." align="char">4</td><td char="." align="char">9493.850638</td><td char="." align="char">76.719301</td><td char="&#x000d7;" align="char">1.15&#x02009;&#x000d7;&#x02009;10&#x0207b;&#x000b9;&#x000b3;</td></tr><tr><td align="left">Residual (within columns)</td><td char="." align="char">3093.696912</td><td char="." align="char">25</td><td char="." align="char">123.747876</td><td align="left"/><td align="left"/></tr><tr><td align="left">Total</td><td char="." align="char">41069.099463</td><td char="." align="char">29</td><td align="left"/><td align="left"/><td align="left"/></tr></tbody></table></table-wrap>
</p></sec></sec><sec id="Sec21"><title>Limitation of the work</title><p id="Par126">The dataset used in this study consists of only 400 samples, which may leads to a potential risk of overfitting, especially when working with complex machine learning models like Extra Trees. The dataset used in this paper originates from a single region, which may restrict the model&#x02019;s applicability to other populations. Factors such as demographics, geography, and socioeconomic conditions can significantly influence CKD prevalence and risk factors. The BBFS algorithm was employed to identify the most important features for CKD classification. However, we understand that feature selection might be influenced by the characteristics of the dataset used. This could lead to the exclusion of certain clinically relevant features. Although the model shows strong performance, its integration into clinical workflows may present practical and ethical challenges. For instance, clinicians may require additional tools to interpret and trust the model&#x02019;s predictions before adopting them in their decision-making processes.</p></sec><sec id="Sec22"><title>Conclusion and future work</title><p id="Par127">Chronic kidney disease (CKD) is a persistent condition characterized by the gradual deterioration of kidney function, typically assessed through the estimated glomerular filtration rate (eGFR) and the presence of kidney damage. The kidney disease improving global outcomes (KDIGO) organization has established a widely accepted system for categorizing CKD based on these factors. However, traditional machine learning models used for classification often lack transparency, making it difficult to understand the reasoning behind their decisions. To address this issue, we introduce the explainable artificial intelligence-CKD (XAI-CKD) model. This approach aims not only to accurately predict outcomes but also to provide clear and interpretable explanations for its decisions. XAI-CKD utilizes extra trees (ET) and shapley additive explanations (SHAP) values, along with the binary breadth-first search (BBFS) algorithm to select the most important features. By doing so, XAI-CKD seeks to offer valuable insights for improving decision-making in CKD classification. We compare the performance of XAI-CKD with several other machine learning models, including random forest (RF), decision tree (DT), bagging classifier (BC), adaptive boosting (AdaBoost), and k-nearest neighbor (KNN). Performance evaluation metrics such as accuracy, sensitivity, specificity, F-score, and area under the ROC curve (AUC) are utilized. Experimental results demonstrate that the proposed XAI-CKD model achieves the highest accuracy of 99.9%, showcasing its effectiveness in classifying CKD while providing transparent and understandable explanations for its decisions. In future work, we can collaborate with healthcare providers to validate the model&#x02019;s performance in real-world clinical settings and integrating it into clinical workflows could facilitate its adoption for aiding healthcare professionals in CKD diagnosis and management, also expanding the XAI framework to other chronic diseases beyond CKD, such as diabetes or cardiovascular diseases, could broaden its applicability and impact in healthcare decision-making. Also, in the future LIME could provide localized insights into individual predictions, offering clinicians a clearer understanding of why specific decisions was made for each patient. By using attention mechanisms in deep learning models, we could highlight the most influential features in each prediction, creating intuitive visualizations that enhance transparency and trust. We also see significant potential in expanding the dataset to include other types of clinical information, such as: genetic information, longitudinal data. By combining these advanced interpretability techniques and additional data sources, future work could address current limitations and better capture the complexities of CKD.</p></sec><sec id="Sec23"><title>Ethical considerations in AI-Based CKD classification</title><p id="Par128">Using patient data in AI models raises critical privacy concerns. Saving health information is essential. To address these concerns, we propose using data anonymization techniques to protect patient identities, ensuring secure storage and processing of data in controlled environments, and designing models with privacy to prevent unauthorized access or data misuse. AI models can sometimes reflect biases present in the training data, which could result in unequal outcomes for different demographic groups. This is particularly relevant for CKD classification, as prevalence and risk factors can vary significantly across populations. To address these concerns, we recommend implementing fairness aware algorithms to minimize bias in predictions. Also, we recognize the potential risks of model misuse, where predictions might be applied incorrectly or out of context, leading to unintended consequences. To prevent these concerns, we emphasize that the model should be used as a decision-support tool, not a standalone diagnostic system, training programs for clinicians should focus on interpreting AI outputs in the context of broader clinical information, monitoring and auditing mechanisms should be implemented to oversee the model&#x02019;s deployment and ensure it is used appropriately. The focus on explainability in this study aligns with ethical principles of transparency. By making the model&#x02019;s predictions interpretable through techniques like SHAP, we aim to help clinicians and patients understand the rationale behind the predictions. Additionally, we suggest incorporating feedback loops into future versions of the model to continuously evaluate and improve its ethical performance.</p></sec><sec id="Sec24" sec-type="supplementary-material"><title>Electronic supplementary material</title><p>Below is the link to the electronic supplementary material.</p><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2025_2355_MOESM1_ESM.docx"><caption><p>Supplementary Material 1</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-025-02355-7.</p></sec><notes notes-type="author-contribution"><title>Author contributions</title><p>All authors have contributed equally.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>Open access funding provided by The Science, Technology &#x00026; Innovation Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank (EKB).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>
<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/mansoordaku/ckdisease/data">https://www.kaggle.com/datasets/mansoordaku/ckdisease/data</ext-link>
</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par131">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>TK</given-names></name><name><surname>Knicely</surname><given-names>DH</given-names></name><name><surname>Grams</surname><given-names>ME</given-names></name></person-group><article-title>Chronic kidney disease diagnosis and management: A review</article-title><source>JAMA</source><year>2019</year><volume>322</volume><issue>13</issue><fpage>1294</fpage><lpage>1304</lpage><pub-id pub-id-type="doi">10.1001/jama.2019.14745</pub-id><pub-id pub-id-type="pmid">31573641</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Chen, T. K., Knicely, D. H. &#x00026; Grams, M. E. Chronic kidney disease diagnosis and management: A review. <italic>JAMA</italic><bold>322</bold> (13), 1294&#x02013;1304 (2019).<pub-id pub-id-type="pmid">31573641</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Arif</surname><given-names>MS</given-names></name><name><surname>Mukheimer</surname><given-names>A</given-names></name><name><surname>Asif</surname><given-names>D</given-names></name></person-group><article-title>Enhancing the early detection of chronic kidney disease: A robust machine learning model</article-title><source>Big Data Cogn. Comput.</source><year>2023</year><volume>7</volume><issue>3</issue><fpage>144</fpage><pub-id pub-id-type="doi">10.3390/bdcc7030144</pub-id></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Arif, M. S., Mukheimer, A. &#x00026; Asif, D. Enhancing the early detection of chronic kidney disease: A robust machine learning model. <italic>Big Data Cogn. Comput.</italic><bold>7</bold> (3), 144 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>JX</given-names></name><etal/></person-group><article-title>Interpretable machine learning for predicting chronic kidney disease progression risk</article-title><source>Digit. Health</source><year>2024</year><volume>10</volume><fpage>20552076231224225</fpage><pub-id pub-id-type="doi">10.1177/20552076231224225</pub-id><pub-id pub-id-type="pmid">38235416</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Zheng, J. X. et al. Interpretable machine learning for predicting chronic kidney disease progression risk. <italic>Digit. Health</italic>. <bold>10</bold>, 20552076231224225 (2024).<pub-id pub-id-type="pmid">38235416</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Yadav</surname><given-names>P</given-names></name><name><surname>Sharma</surname><given-names>SC</given-names></name><name><surname>Mahadeva</surname><given-names>R</given-names></name><name><surname>Patole</surname><given-names>SP</given-names></name></person-group><article-title>Exploring Hyper-Parameters and feature selection for predicting Non-Communicable chronic disease using stacking classifier</article-title><source>IEEE Access.</source><year>2023</year><volume>11</volume><fpage>80030</fpage><lpage>80055</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3299332</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Yadav, P., Sharma, S. C., Mahadeva, R. &#x00026; Patole, S. P. Exploring Hyper-Parameters and feature selection for predicting Non-Communicable chronic disease using stacking classifier. <italic>IEEE Access.</italic><bold>11</bold>, 80030&#x02013;80055 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Elshewey</surname><given-names>AM</given-names></name><name><surname>Osman</surname><given-names>AM</given-names></name></person-group><article-title>Orthopedic disease classification based on breadth-first search algorithm</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>23368</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-73559-6</pub-id><pub-id pub-id-type="pmid">39375370</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Elshewey, A. M. &#x00026; Osman, A. M. Orthopedic disease classification based on breadth-first search algorithm. <italic>Sci. Rep.</italic><bold>14</bold> (1), 23368 (2024).<pub-id pub-id-type="pmid">39375370</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Elkenawy</surname><given-names>ES</given-names></name><name><surname>Alhussan</surname><given-names>AA</given-names></name><name><surname>Khafaga</surname><given-names>DS</given-names></name><name><surname>Tarek</surname><given-names>Z</given-names></name><name><surname>Elshewey</surname><given-names>AM</given-names></name></person-group><article-title>Greylag Goose optimization and multilayer perceptron for enhancing lung cancer classification</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>23784</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-72013-x</pub-id><pub-id pub-id-type="pmid">39390014</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Elkenawy, E. S., Alhussan, A. A., Khafaga, D. S., Tarek, Z. &#x00026; Elshewey, A. M. Greylag Goose optimization and multilayer perceptron for enhancing lung cancer classification. <italic>Sci. Rep.</italic><bold>14</bold> (1), 23784 (2024).<pub-id pub-id-type="pmid">39390014</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Elshewey</surname><given-names>AM</given-names></name><name><surname>Alhussan</surname><given-names>AA</given-names></name><name><surname>Khafaga</surname><given-names>DS</given-names></name><name><surname>Elkenawy</surname><given-names>ES</given-names></name><name><surname>Tarek</surname><given-names>Z</given-names></name></person-group><article-title>EEG-based optimization of eye state classification using modified-BER metaheuristic algorithm</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>24489</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-74475-5</pub-id><pub-id pub-id-type="pmid">39424849</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Elshewey, A. M., Alhussan, A. A., Khafaga, D. S., Elkenawy, E. S. &#x00026; Tarek, Z. EEG-based optimization of eye state classification using modified-BER metaheuristic algorithm. <italic>Sci. Rep.</italic><bold>14</bold> (1), 24489 (2024).<pub-id pub-id-type="pmid">39424849</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">El-Rashidy, N., Tarek, Z., Elshewey, A. M. &#x00026; Shams, M. Y. Multitask multilayer-prediction model for predicting mechanical ventilation and the associated mortality rate. <italic>Neural Comput. Appl.</italic> :1&#x02013;23. (2024).</mixed-citation></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Elshewey</surname><given-names>AM</given-names></name><etal/></person-group><article-title>Enhancing heart disease classification based on Greylag Goose optimization algorithm and long short-term memory</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><issue>1</issue><fpage>1277</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-83592-0</pub-id><pub-id pub-id-type="pmid">39779779</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Elshewey, A. M. et al. Enhancing heart disease classification based on Greylag Goose optimization algorithm and long short-term memory. <italic>Sci. Rep.</italic><bold>15</bold> (1), 1277 (2025).<pub-id pub-id-type="pmid">39779779</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Hosny, K. M., Mohammed, M. A., Salama, R. A. &#x00026; Elshewey, A. M. Explainable ensemble deep learning-based model for brain tumor detection and classification. <italic>Neural Comput. Appl.</italic> :1&#x02013;18. (2024).</mixed-citation></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Hassija</surname><given-names>V</given-names></name><etal/></person-group><article-title>Interpreting black-box models: a review on explainable artificial intelligence</article-title><source>Cogn. Comput.</source><year>2024</year><volume>16</volume><issue>1</issue><fpage>45</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1007/s12559-023-10179-8</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Hassija, V. et al. Interpreting black-box models: a review on explainable artificial intelligence. <italic>Cogn. Comput.</italic><bold>16</bold> (1), 45&#x02013;74 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Baniecki, H. &#x00026; Biecek, P. Adversarial attacks and defenses in explainable artificial intelligence: A survey. <italic>Inform. Fusion</italic> :102303. (2024).</mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Guidotti</surname><given-names>R</given-names></name><etal/></person-group><article-title>A survey of methods for explaining black box models</article-title><source>ACM Comput. Surv. (CSUR)</source><year>2018</year><volume>51</volume><issue>5</issue><fpage>1</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1145/3236009</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Guidotti, R. et al. A survey of methods for explaining black box models. <italic>ACM Comput. Surv. (CSUR)</italic>. <bold>51</bold> (5), 1&#x02013;42 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Tursunalieva</surname><given-names>A</given-names></name><etal/></person-group><article-title>Making sense of machine learning: A review of interpretation techniques and their applications</article-title><source>Appl. Sci.</source><year>2024</year><volume>14</volume><issue>2</issue><fpage>496</fpage><pub-id pub-id-type="doi">10.3390/app14020496</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Tursunalieva, A. et al. Making sense of machine learning: A review of interpretation techniques and their applications. <italic>Appl. Sci.</italic><bold>14</bold> (2), 496 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Arrieta</surname><given-names>AB</given-names></name><etal/></person-group><article-title>Explainable artificial intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI</article-title><source>Inform. Fusion</source><year>2020</year><volume>58</volume><fpage>82</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2019.12.012</pub-id></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Arrieta, A. B. et al. Explainable artificial intelligence (XAI): concepts, taxonomies, opportunities and challenges toward responsible AI. <italic>Inform. Fusion</italic>. <bold>58</bold>, 82&#x02013;115 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Longo, L. et al. Explainable artificial intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions. <italic>Inform. Fusion</italic> :102301. (2024).</mixed-citation></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Jongbo</surname><given-names>OA</given-names></name><name><surname>Adetunmbi</surname><given-names>AO</given-names></name><name><surname>Ogunrinde</surname><given-names>RB</given-names></name><name><surname>Badeji-Ajisafe</surname><given-names>B</given-names></name></person-group><article-title>Development of an ensemble approach to chronic kidney disease diagnosis</article-title><source>Sci. Afr.</source><year>2020</year><volume>8</volume><fpage>e00456</fpage></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Jongbo, O. A., Adetunmbi, A. O., Ogunrinde, R. B. &#x00026; Badeji-Ajisafe, B. Development of an ensemble approach to chronic kidney disease diagnosis. <italic>Sci. Afr.</italic><bold>8</bold>, e00456 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Ilyas</surname><given-names>H</given-names></name><etal/></person-group><article-title>Chronic kidney disease diagnosis using decision tree algorithms</article-title><source>BMC Nephrol.</source><year>2021</year><volume>22</volume><issue>1</issue><fpage>273</fpage><pub-id pub-id-type="doi">10.1186/s12882-021-02474-z</pub-id><pub-id pub-id-type="pmid">34372817</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Ilyas, H. et al. Chronic kidney disease diagnosis using decision tree algorithms. <italic>BMC Nephrol.</italic><bold>22</bold> (1), 273 (2021).<pub-id pub-id-type="pmid">34372817</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><etal/></person-group><article-title>Machine learning algorithm for predict the in-hospital mortality in critically ill patients with congestive heart failure combined with chronic kidney disease</article-title><source>Ren. Fail.</source><year>2024</year><volume>46</volume><issue>1</issue><fpage>2315298</fpage><pub-id pub-id-type="doi">10.1080/0886022X.2024.2315298</pub-id><pub-id pub-id-type="pmid">38357763</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Li, X. et al. Machine learning algorithm for predict the in-hospital mortality in critically ill patients with congestive heart failure combined with chronic kidney disease. <italic>Ren. Fail.</italic><bold>46</bold> (1), 2315298 (2024).<pub-id pub-id-type="pmid">38357763</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Jhumka, K. et al. Explainable Chronic Kidney Disease (CKD) Prediction using Deep Learning and Shapley Additive Explanations (SHAP). InProceedings of the 2023 7th International Conference on Advances in Artificial Intelligence 2023 (pp. 29&#x02013;33).</mixed-citation></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Radomirovic</surname><given-names>B</given-names></name><etal/></person-group><article-title>Optimizing long-short term memory neural networks for electroencephalogram anomaly detection using variable neighborhood search with dynamic strategy change</article-title><source>Complex. Intell. Syst.</source><year>2024</year><volume>10</volume><issue>6</issue><fpage>7987</fpage><lpage>8009</lpage><pub-id pub-id-type="doi">10.1007/s40747-024-01592-z</pub-id></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Radomirovic, B. et al. Optimizing long-short term memory neural networks for electroencephalogram anomaly detection using variable neighborhood search with dynamic strategy change. <italic>Complex. Intell. Syst.</italic><bold>10</bold> (6), 7987&#x02013;8009 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Jovanovic, L. et al. Anomaly detection in ecg using recurrent networks optimized by modified metaheuristic algorithm. In2023 31st telecommunications forum (TELFOR) 2023 (pp. 1&#x02013;4). IEEE.</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Dobrojevic, M. et al. Cyberbullying Sexism Harassment Identification by Metaheurustics-Tuned eXtreme Gradient Boosting. <italic>Mater. Continua</italic> ;<bold>80</bold>(3). (2024).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Jovanovic, L. et al. Utilizing Generative Adversarial Networks for Medical Data Synthesis and Augmentation to Enhance Model Training. InCongress on Control, Robotics, and Mechatronics 2024 (pp. 85&#x02013;98 ). Singapore: Springer Nature Singapore.</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Zivkovic, M. et al. Parkinson&#x02019;s Detection From Gait Time Series Classification Using LSTM Tuned by Modified RSA Algorithm. InInternational Conference on Communication and Computational Technologies 2024 (pp. 119&#x02013;134). Springer, Singapore.</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Al Imran, A., Amin, M. N. &#x00026; Johora, F. T. Classification of chronic kidney disease using logistic regression, feedforward neural network and wide &#x00026; deep learning. In2018 International Conference on Innovation in Engineering and Technology (ICIET) 2018 Dec 27 (pp. 1&#x02013;6). IEEE.</mixed-citation></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Chaudhuri</surname><given-names>AK</given-names></name><name><surname>Sinha</surname><given-names>D</given-names></name><name><surname>Banerjee</surname><given-names>DK</given-names></name><name><surname>Das</surname><given-names>A</given-names></name></person-group><article-title>A novel enhanced decision tree model for detecting chronic kidney disease</article-title><source>Netw. Model. Anal. Health Inf. Bioinf.</source><year>2021</year><volume>10</volume><fpage>1</fpage><lpage>22</lpage></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Chaudhuri, A. K., Sinha, D., Banerjee, D. K. &#x00026; Das, A. A novel enhanced decision tree model for detecting chronic kidney disease. <italic>Netw. Model. Anal. Health Inf. Bioinf.</italic><bold>10</bold>, 1&#x02013;22 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Polat</surname><given-names>H</given-names></name><name><surname>Danaei Mehr</surname><given-names>H</given-names></name><name><surname>Cetin</surname><given-names>A</given-names></name></person-group><article-title>Diagnosis of chronic kidney disease based on support vector machine by feature selection methods</article-title><source>J. Med. Syst.</source><year>2017</year><volume>41</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1007/s10916-017-0703-x</pub-id><pub-id pub-id-type="pmid">27817129</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Polat, H., Danaei Mehr, H. &#x00026; Cetin, A. Diagnosis of chronic kidney disease based on support vector machine by feature selection methods. <italic>J. Med. Syst.</italic><bold>41</bold>, 1&#x02013;11 (2017).<pub-id pub-id-type="pmid">27817129</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Senan, E. M. et al. Diagnosis of chronic kidney disease using effective classification algorithms and recursive feature elimination techniques. <italic>J. Healthcare Eng</italic>. ;2021. (2021).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Iliyas, I. I., Saidu, I. R., Dauda, A. B. &#x00026; Tasiu, S. Prediction of chronic kidney disease using deep neural network. arXiv preprint arXiv:2012.12089. (2020).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Jongbo, O. A., Olowookere, T. A. &#x00026; Adetunmbi, A. O. Performance Evaluation of an Ensemble Method for Diagnosis of Chronic Kidney Disease with Feature Selection Technique. In2020 International Conference on Decision Aid Sciences and Application (DASA) 2020 (pp. 959&#x02013;965). IEEE.</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Manju, V. N. &#x00026; Aparna, N. Decision Tree-Based Explainable AI for Diagnosis of Chronic Kidney Disease. In2023 5th International Conference on Inventive Research in Computing Applications (ICIRCA) 2023 (pp. 947&#x02013;952). IEEE.</mixed-citation></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Raihan</surname><given-names>MJ</given-names></name><name><surname>Khan</surname><given-names>MA</given-names></name><name><surname>Kee</surname><given-names>SH</given-names></name><name><surname>Nahid</surname><given-names>AA</given-names></name></person-group><article-title>Detection of the chronic kidney disease using XGBoost classifier and explaining the influence of the attributes on the model using SHAP</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><issue>1</issue><fpage>6263</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-33525-0</pub-id><pub-id pub-id-type="pmid">37069256</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">Raihan, M. J., Khan, M. A., Kee, S. H. &#x00026; Nahid, A. A. Detection of the chronic kidney disease using XGBoost classifier and explaining the influence of the attributes on the model using SHAP. <italic>Sci. Rep.</italic><bold>13</bold> (1), 6263 (2023).<pub-id pub-id-type="pmid">37069256</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-S&#x000e1;nchez</surname><given-names>PA</given-names></name></person-group><article-title>Data-driven early diagnosis of chronic kidney disease: development and evaluation of an explainable AI model</article-title><source>IEEE Access.</source><year>2023</year><volume>11</volume><fpage>38359</fpage><lpage>38369</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3264270</pub-id></element-citation><mixed-citation id="mc-CR34" publication-type="journal">Moreno-S&#x000e1;nchez, P. A. Data-driven early diagnosis of chronic kidney disease: development and evaluation of an explainable AI model. <italic>IEEE Access.</italic><bold>11</bold>, 38359&#x02013;38369 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Moreno-Sanchez, P. A. An explainable classification model for chronic kidney disease patients. <italic>CoRR</italic> (2021).</mixed-citation></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>SK</given-names></name><name><surname>Khandoker</surname><given-names>AH</given-names></name></person-group><article-title>Investigation on explainable machine learning models to predict chronic kidney diseases</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><issue>1</issue><fpage>3687</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-54375-4</pub-id><pub-id pub-id-type="pmid">38355876</pub-id>
</element-citation><mixed-citation id="mc-CR36" publication-type="journal">Ghosh, S. K. &#x00026; Khandoker, A. H. Investigation on explainable machine learning models to predict chronic kidney diseases. <italic>Sci. Rep.</italic><bold>14</bold> (1), 3687 (2024).<pub-id pub-id-type="pmid">38355876</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Islam, M. A., Nittala, K. &#x00026; Bajwa, G. Adding explainability to machine learning models to detect chronic kidney disease. In2022 IEEE 23rd International Conference on Information Reuse and Integration for Data Science (IRI) 2022 (pp. 297&#x02013;302). IEEE.</mixed-citation></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DH</given-names></name><name><surname>Macready</surname><given-names>WG</given-names></name></person-group><article-title>No free lunch theorems for optimization</article-title><source>IEEE Trans. Evol. Comput.</source><year>1997</year><volume>1</volume><issue>1</issue><fpage>67</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1109/4235.585893</pub-id></element-citation><mixed-citation id="mc-CR38" publication-type="journal">Wolpert, D. H. &#x00026; Macready, W. G. No free lunch theorems for optimization. <italic>IEEE Trans. Evol. Comput.</italic><bold>1</bold> (1), 67&#x02013;82 (1997).</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other"><ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/mansoordaku/ckdisease/data">https://www.kaggle.com/datasets/mansoordaku/ckdisease/data</ext-link></mixed-citation></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>MY</given-names></name><name><surname>Tarek</surname><given-names>Z</given-names></name><name><surname>El-kenawy</surname><given-names>ES</given-names></name><name><surname>Eid</surname><given-names>MM</given-names></name><name><surname>Elshewey</surname><given-names>AM</given-names></name></person-group><article-title>Predicting gross domestic product (GDP) using a PC-LSTM-RNN model in urban profiling areas</article-title><source>Comput. Urban Sci.</source><year>2024</year><volume>4</volume><issue>1</issue><fpage>3</fpage><pub-id pub-id-type="doi">10.1007/s43762-024-00116-2</pub-id></element-citation><mixed-citation id="mc-CR40" publication-type="journal">Shams, M. Y., Tarek, Z., El-kenawy, E. S., Eid, M. M. &#x00026; Elshewey, A. M. Predicting gross domestic product (GDP) using a PC-LSTM-RNN model in urban profiling areas. <italic>Comput. Urban Sci.</italic><bold>4</bold> (1), 3 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Tarek</surname><given-names>Z</given-names></name><etal/></person-group><article-title>An optimized model based on deep learning and gated recurrent unit for COVID-19 death prediction</article-title><source>Biomimetics</source><year>2023</year><volume>8</volume><issue>7</issue><fpage>552</fpage><pub-id pub-id-type="doi">10.3390/biomimetics8070552</pub-id><pub-id pub-id-type="pmid">37999193</pub-id>
</element-citation><mixed-citation id="mc-CR41" publication-type="journal">Tarek, Z. et al. An optimized model based on deep learning and gated recurrent unit for COVID-19 death prediction. <italic>Biomimetics</italic><bold>8</bold> (7), 552 (2023).<pub-id pub-id-type="pmid">37999193</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Alkhammash</surname><given-names>EH</given-names></name><etal/></person-group><article-title>Application of machine learning to predict COVID-19 spread via an optimized BPSO model</article-title><source>Biomimetics</source><year>2023</year><volume>8</volume><issue>6</issue><fpage>457</fpage><pub-id pub-id-type="doi">10.3390/biomimetics8060457</pub-id><pub-id pub-id-type="pmid">37887588</pub-id>
</element-citation><mixed-citation id="mc-CR42" publication-type="journal">Alkhammash, E. H. et al. Application of machine learning to predict COVID-19 spread via an optimized BPSO model. <italic>Biomimetics</italic><bold>8</bold> (6), 457 (2023).<pub-id pub-id-type="pmid">37887588</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Geurts</surname><given-names>P</given-names></name><name><surname>Ernst</surname><given-names>D</given-names></name><name><surname>Wehenkel</surname><given-names>L</given-names></name></person-group><article-title>Extremely randomized trees</article-title><source>Mach. Learn.</source><year>2006</year><volume>63</volume><fpage>3</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1007/s10994-006-6226-1</pub-id></element-citation><mixed-citation id="mc-CR43" publication-type="journal">Geurts, P., Ernst, D. &#x00026; Wehenkel, L. Extremely randomized trees. <italic>Mach. Learn.</italic><bold>63</bold>, 3&#x02013;42 (2006).</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Sharaff, A. &#x00026; Gupta, H. Extra-tree classifier with metaheuristics approach for email classification. InAdvances in Computer Communication and Computational Sciences: Proceedings of IC4S 2018 2019 (pp. 189&#x02013;197). Springer Singapore.</mixed-citation></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>D</given-names></name><name><surname>Kumar</surname><given-names>R</given-names></name><name><surname>Jain</surname><given-names>A</given-names></name></person-group><article-title>Breast cancer prediction based on neural networks and extra tree classifier using feature ensemble learning</article-title><source>Measurement: Sens.</source><year>2022</year><volume>24</volume><fpage>100560</fpage></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Sharma, D., Kumar, R. &#x00026; Jain, A. Breast cancer prediction based on neural networks and extra tree classifier using feature ensemble learning. <italic>Measurement: Sens.</italic><bold>24</bold>, 100560 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Elshewey</surname><given-names>AM</given-names></name><etal/></person-group><article-title>Weight prediction using the hybrid Stacked-LSTM food selection model</article-title><source>Comput. Syst. Sci. Eng.</source><year>2023</year><volume>46</volume><issue>1</issue><fpage>765</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.32604/csse.2023.034324</pub-id></element-citation><mixed-citation id="mc-CR46" publication-type="journal">Elshewey, A. M. et al. Weight prediction using the hybrid Stacked-LSTM food selection model. <italic>Comput. Syst. Sci. Eng.</italic><bold>46</bold> (1), 765&#x02013;781 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Random forests</article-title><source>Mach. Learn.</source><year>2001</year><volume>45</volume><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Breiman, L. Random forests. <italic>Mach. Learn.</italic><bold>45</bold>, 5&#x02013;32 (2001).</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Elshewey, A. M., Tawfeek, S. M., Alhussan, A. A., Radwan, M. &#x00026; Abed, A. H. Optimized deep learning for potato blight detection using the waterwheel plant algorithm and sine cosine algorithm. <italic>Potato Res.</italic> :1&#x02013;25. (2024).</mixed-citation></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name><surname>Friedl</surname><given-names>MA</given-names></name><name><surname>Brodley</surname><given-names>CE</given-names></name></person-group><article-title>Decision tree classification of land cover from remotely sensed data</article-title><source>Remote Sens. Environ.</source><year>1997</year><volume>61</volume><issue>3</issue><fpage>399</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1016/S0034-4257(97)00049-7</pub-id></element-citation><mixed-citation id="mc-CR49" publication-type="journal">Friedl, M. A. &#x00026; Brodley, C. E. Decision tree classification of land cover from remotely sensed data. <italic>Remote Sens. Environ.</italic><bold>61</bold> (3), 399&#x02013;409 (1997).</mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Alkhammash</surname><given-names>EH</given-names></name><name><surname>Hadjouni</surname><given-names>M</given-names></name><name><surname>Elshewey</surname><given-names>AM</given-names></name></person-group><article-title>A hybrid ensemble stacking model for gender voice recognition approach</article-title><source>Electronics</source><year>2022</year><volume>11</volume><issue>11</issue><fpage>1750</fpage><pub-id pub-id-type="doi">10.3390/electronics11111750</pub-id></element-citation><mixed-citation id="mc-CR50" publication-type="journal">Alkhammash, E. H., Hadjouni, M. &#x00026; Elshewey, A. M. A hybrid ensemble stacking model for gender voice recognition approach. <italic>Electronics</italic><bold>11</bold> (11), 1750 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>R</given-names></name><name><surname>Ghosh</surname><given-names>A</given-names></name><name><surname>Joshi</surname><given-names>PK</given-names></name></person-group><article-title>Decision tree approach for classification of remotely sensed satellite data using open source support</article-title><source>J. Earth Syst. Sci.</source><year>2013</year><volume>122</volume><fpage>1237</fpage><lpage>1247</lpage><pub-id pub-id-type="doi">10.1007/s12040-013-0339-2</pub-id></element-citation><mixed-citation id="mc-CR51" publication-type="journal">Sharma, R., Ghosh, A. &#x00026; Joshi, P. K. Decision tree approach for classification of remotely sensed satellite data using open source support. <italic>J. Earth Syst. Sci.</italic><bold>122</bold>, 1237&#x02013;1247 (2013).</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name><surname>Pal</surname><given-names>M</given-names></name><name><surname>Mather</surname><given-names>PM</given-names></name></person-group><article-title>An assessment of the effectiveness of decision tree methods for land cover classification</article-title><source>Remote Sens. Environ.</source><year>2003</year><volume>86</volume><issue>4</issue><fpage>554</fpage><lpage>565</lpage><pub-id pub-id-type="doi">10.1016/S0034-4257(03)00132-9</pub-id></element-citation><mixed-citation id="mc-CR52" publication-type="journal">Pal, M. &#x00026; Mather, P. M. An assessment of the effectiveness of decision tree methods for land cover classification. <italic>Remote Sens. Environ.</italic><bold>86</bold> (4), 554&#x02013;565 (2003).</mixed-citation></citation-alternatives></ref><ref id="CR53"><label>53.</label><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name><surname>Bauer</surname><given-names>E</given-names></name><name><surname>Kohavi</surname><given-names>R</given-names></name></person-group><article-title>An empirical comparison of voting classification algorithms: bagging, boosting, and variants</article-title><source>Mach. Learn.</source><year>1999</year><volume>36</volume><fpage>105</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1023/A:1007515423169</pub-id></element-citation><mixed-citation id="mc-CR53" publication-type="journal">Bauer, E. &#x00026; Kohavi, R. An empirical comparison of voting classification algorithms: bagging, boosting, and variants. <italic>Mach. Learn.</italic><bold>36</bold>, 105&#x02013;139 (1999).</mixed-citation></citation-alternatives></ref><ref id="CR54"><label>54.</label><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Bagging predictors</article-title><source>Mach. Learn.</source><year>1996</year><volume>24</volume><fpage>123</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1007/BF00058655</pub-id></element-citation><mixed-citation id="mc-CR54" publication-type="journal">Breiman, L. Bagging predictors. <italic>Mach. Learn.</italic><bold>24</bold>, 123&#x02013;140 (1996).</mixed-citation></citation-alternatives></ref><ref id="CR55"><label>55.</label><citation-alternatives><element-citation id="ec-CR55" publication-type="journal"><person-group person-group-type="author"><name><surname>Elshewey</surname><given-names>AM</given-names></name><etal/></person-group><article-title>Optimizing HCV disease prediction in Egypt: the HyOPTGB framework</article-title><source>Diagnostics</source><year>2023</year><volume>13</volume><issue>22</issue><fpage>3439</fpage><pub-id pub-id-type="doi">10.3390/diagnostics13223439</pub-id><pub-id pub-id-type="pmid">37998575</pub-id>
</element-citation><mixed-citation id="mc-CR55" publication-type="journal">Elshewey, A. M. et al. Optimizing HCV disease prediction in Egypt: the HyOPTGB framework. <italic>Diagnostics</italic><bold>13</bold> (22), 3439 (2023).<pub-id pub-id-type="pmid">37998575</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Dietterich, T. G. Ensemble methods in machine learning. InInternational workshop on multiple classifier systems 2000 (pp. 1&#x02013;15). Berlin, Heidelberg: Springer Berlin Heidelberg.</mixed-citation></ref><ref id="CR57"><label>57.</label><citation-alternatives><element-citation id="ec-CR57" publication-type="journal"><person-group person-group-type="author"><name><surname>Freund</surname><given-names>Y</given-names></name><name><surname>Schapire</surname><given-names>RE</given-names></name></person-group><article-title>A decision-theoretic generalization of on-line learning and an application to boosting</article-title><source>J. Comput. Syst. Sci.</source><year>1997</year><volume>55</volume><issue>1</issue><fpage>119</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1006/jcss.1997.1504</pub-id></element-citation><mixed-citation id="mc-CR57" publication-type="journal">Freund, Y. &#x00026; Schapire, R. E. A decision-theoretic generalization of on-line learning and an application to boosting. <italic>J. Comput. Syst. Sci.</italic><bold>55</bold> (1), 119&#x02013;139 (1997).</mixed-citation></citation-alternatives></ref><ref id="CR58"><label>58.</label><citation-alternatives><element-citation id="ec-CR58" publication-type="journal"><person-group person-group-type="author"><name><surname>Morra</surname><given-names>JH</given-names></name><etal/></person-group><article-title>Comparison of adaboost and support vector machines for detecting Alzheimer&#x02019;s disease through automated hippocampal segmentation</article-title><source>IEEE Trans. Med. Imaging</source><year>2009</year><volume>29</volume><issue>1</issue><fpage>30</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1109/TMI.2009.2021941</pub-id><pub-id pub-id-type="pmid">19457748</pub-id>
</element-citation><mixed-citation id="mc-CR58" publication-type="journal">Morra, J. H. et al. Comparison of adaboost and support vector machines for detecting Alzheimer&#x02019;s disease through automated hippocampal segmentation. <italic>IEEE Trans. Med. Imaging</italic>. <bold>29</bold> (1), 30&#x02013;43 (2009).<pub-id pub-id-type="pmid">19457748</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Gao, Y., Chen, M. &#x00026; Ma, L. Vehicle detection segmentation based on adaboost and Grabcut. In2010 IEEE International Conference on Progress in Informatics and Computing 2010 (Vol. 2, pp. 896&#x02013;900). IEEE.</mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Wang, H. <italic>Nearest Neighbours Without K: a Classification Formalism Based on Probability</italic> (Faculty of Informatics, University of Ulster, 2002).</mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">Mucherino, A. et al. K-nearest neighbor classification. <italic>Data Min. Agric.</italic> :83&#x02013;106. (2009).</mixed-citation></ref><ref id="CR62"><label>62.</label><citation-alternatives><element-citation id="ec-CR62" publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Ianakiev</surname><given-names>K</given-names></name><name><surname>Govindaraju</surname><given-names>V</given-names></name></person-group><article-title>Improved k-nearest neighbor classification</article-title><source>Pattern Recogn.</source><year>2002</year><volume>35</volume><issue>10</issue><fpage>2311</fpage><lpage>2318</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(01)00132-7</pub-id></element-citation><mixed-citation id="mc-CR62" publication-type="journal">Wu, Y., Ianakiev, K. &#x00026; Govindaraju, V. Improved k-nearest neighbor classification. <italic>Pattern Recogn.</italic><bold>35</bold> (10), 2311&#x02013;2318 (2002).</mixed-citation></citation-alternatives></ref><ref id="CR63"><label>63.</label><citation-alternatives><element-citation id="ec-CR63" publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>MS</given-names></name><etal/></person-group><article-title>Explainable AI: a Neurally-inspired decision stack framework</article-title><source>Biomimetics</source><year>2022</year><volume>7</volume><issue>3</issue><fpage>127</fpage><pub-id pub-id-type="doi">10.3390/biomimetics7030127</pub-id><pub-id pub-id-type="pmid">36134931</pub-id>
</element-citation><mixed-citation id="mc-CR63" publication-type="journal">Khan, M. S. et al. Explainable AI: a Neurally-inspired decision stack framework. <italic>Biomimetics</italic><bold>7</bold> (3), 127 (2022).<pub-id pub-id-type="pmid">36134931</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR64"><label>64.</label><citation-alternatives><element-citation id="ec-CR64" publication-type="journal"><person-group person-group-type="author"><name><surname>Fadlil</surname><given-names>A</given-names></name></person-group><article-title>K nearest neighbor imputation performance on missing value data graduate user satisfaction</article-title><source>Jurnal RESTI (Rekayasa Sistem Dan. Teknologi Informasi)</source><year>2022</year><volume>6</volume><issue>4</issue><fpage>570</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.29207/resti.v6i4.4173</pub-id></element-citation><mixed-citation id="mc-CR64" publication-type="journal">Fadlil, A. K nearest neighbor imputation performance on missing value data graduate user satisfaction. <italic>J. RESTI (Rekayasa Sistem Dan. Teknologi Informasi)</italic>. <bold>6</bold> (4), 570&#x02013;576 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR65"><label>65.</label><citation-alternatives><element-citation id="ec-CR65" publication-type="journal"><person-group person-group-type="author"><name><surname>Alkhammash</surname><given-names>EH</given-names></name><name><surname>Kamel</surname><given-names>AF</given-names></name><name><surname>Al-Fattah</surname><given-names>SM</given-names></name><name><surname>Elshewey</surname><given-names>AM</given-names></name></person-group><article-title>Optimized multivariate adaptive regression splines for predicting crude oil demand in Saudi Arabia</article-title><source>Discrete Dynamics Nat. Soc.</source><year>2022</year><volume>2022</volume><issue>1</issue><fpage>8412895</fpage><pub-id pub-id-type="doi">10.1155/2022/8412895</pub-id></element-citation><mixed-citation id="mc-CR65" publication-type="journal">Alkhammash, E. H., Kamel, A. F., Al-Fattah, S. M. &#x00026; Elshewey, A. M. Optimized multivariate adaptive regression splines for predicting crude oil demand in Saudi Arabia. <italic>Discrete Dynamics Nat. Soc.</italic><bold>2022</bold> (1), 8412895 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR66"><label>66.</label><citation-alternatives><element-citation id="ec-CR66" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name></person-group><article-title>Nearest neighbor selection for iteratively kNN imputation</article-title><source>J. Syst. Softw.</source><year>2012</year><volume>85</volume><issue>11</issue><fpage>2541</fpage><lpage>2552</lpage><pub-id pub-id-type="doi">10.1016/j.jss.2012.05.073</pub-id></element-citation><mixed-citation id="mc-CR66" publication-type="journal">Zhang, S. Nearest neighbor selection for iteratively kNN imputation. <italic>J. Syst. Softw.</italic><bold>85</bold> (11), 2541&#x02013;2552 (2012).</mixed-citation></citation-alternatives></ref><ref id="CR67"><label>67.</label><citation-alternatives><element-citation id="ec-CR67" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>G</given-names></name><name><surname>Song</surname><given-names>J</given-names></name></person-group><article-title>BBFS-STT: an efficient algorithm for number rotation puzzle</article-title><source>Entertainment Comput.</source><year>2016</year><volume>12</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/j.entcom.2015.10.003</pub-id></element-citation><mixed-citation id="mc-CR67" publication-type="journal">Wang, G. &#x00026; Song, J. BBFS-STT: an efficient algorithm for number rotation puzzle. <italic>Entertainment Comput.</italic><bold>12</bold>, 1&#x02013;7 (2016).</mixed-citation></citation-alternatives></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">Fouad, Y., Osman, A. M., Hassan, S. A., El-Bakry, H. M. &#x00026; Elshewey, A. M. Adaptive visual sentiment prediction model based on event concepts and object detection techniques in social media. <italic>Int. J. Adv. Comput. Sci. Appl.</italic> ;<bold>14</bold>(7). (2023).</mixed-citation></ref></ref-list></back></article>