<!--

File produced by pipelineRunner package (for JATS 2 SCJATS with pipeline SCJATS)
At: 2025-04-07T18:28:31.745Z

Version        : 1.16.1
Last update    : 2024-08-27
Modified by    : dunnm

-->
<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id><journal-id journal-id-type="publisher-id">bioinformatics</journal-id><journal-title-group><journal-title>Bioinformatics</journal-title></journal-title-group><issn pub-type="ppub">1367-4803</issn><issn pub-type="epub">1367-4811</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39775454</article-id><article-id pub-id-type="pmc">PMC11758787</article-id>
<article-id pub-id-type="doi">10.1093/bioinformatics/btaf009</article-id><article-id pub-id-type="publisher-id">btaf009</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Paper</subject><subj-group subj-group-type="category-toc-heading"><subject>Phylogenetics</subject></subj-group></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI01060</subject></subj-group></article-categories><title-group><article-title>BetaAlign: a deep learning approach for multiple sequence alignment</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0007-5403-6016</contrib-id><name><surname>Dotan</surname><given-names>Edo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="lead">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation" degree-contribution="equal">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="lead">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="lead">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="lead">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software" degree-contribution="lead">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation" degree-contribution="lead">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization" degree-contribution="lead">Visualization</role><aff>
<institution>The Shmunis School of Biomedicine and Cancer Research, George S. Wise Faculty of Life Sciences, Tel Aviv University</institution>, Tel Aviv 69978, <country country="IL">Israel</country></aff><aff>
<institution>The Henry and Marilyn Taub Faculty of Computer Science, Technion&#x02014;Israel Institute of Technology</institution>, Haifa 3200003, <country country="IL">Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Wygoda</surname><given-names>Elya</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="supporting">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation" degree-contribution="lead">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="supporting">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="supporting">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="supporting">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software" degree-contribution="supporting">Software</role><aff>
<institution>The Shmunis School of Biomedicine and Cancer Research, George S. Wise Faculty of Life Sciences, Tel Aviv University</institution>, Tel Aviv 69978, <country country="IL">Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Ecker</surname><given-names>Noa</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="supporting">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="supporting">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="supporting">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software" degree-contribution="supporting">Software</role><aff>
<institution>The Shmunis School of Biomedicine and Cancer Research, George S. Wise Faculty of Life Sciences, Tel Aviv University</institution>, Tel Aviv 69978, <country country="IL">Israel</country></aff></contrib><contrib contrib-type="author"><name><surname>Alburquerque</surname><given-names>Michael</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation" degree-contribution="supporting">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="supporting">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="supporting">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="supporting">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software" degree-contribution="supporting">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation" degree-contribution="supporting">Validation</role><aff>
<institution>The Shmunis School of Biomedicine and Cancer Research, George S. Wise Faculty of Life Sciences, Tel Aviv University</institution>, Tel Aviv 69978, <country country="IL">Israel</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-1984-2139</contrib-id><name><surname>Avram</surname><given-names>Oren</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="supporting">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation" degree-contribution="supporting">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="supporting">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="supporting">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="supporting">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources" degree-contribution="supporting">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software" degree-contribution="supporting">Software</role><aff>
<institution>The Department of Computer Science, University of California Los Angeles</institution>, Los Angeles, CA 90095, <country country="US">United States</country></aff></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Belinkov</surname><given-names>Yonatan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="lead">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation" degree-contribution="supporting">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="lead">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="supporting">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="lead">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="lead">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration" degree-contribution="equal">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources" degree-contribution="supporting">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software" degree-contribution="supporting">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision" degree-contribution="equal">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation" degree-contribution="supporting">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization" degree-contribution="supporting">Visualization</role><aff>
<institution>The Henry and Marilyn Taub Faculty of Computer Science, Technion&#x02014;Israel Institute of Technology</institution>, Haifa 3200003, <country country="IL">Israel</country></aff><xref rid="btaf009-cor1" ref-type="corresp"/><!--belinkov@technion.ac.il--></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9463-2575</contrib-id><name><surname>Pupko</surname><given-names>Tal</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="lead">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation" degree-contribution="lead">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="lead">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="lead">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="lead">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="lead">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration" degree-contribution="lead">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources" degree-contribution="lead">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software" degree-contribution="supporting">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision" degree-contribution="lead">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation" degree-contribution="lead">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization" degree-contribution="lead">Visualization</role><aff>
<institution>The Shmunis School of Biomedicine and Cancer Research, George S. Wise Faculty of Life Sciences, Tel Aviv University</institution>, Tel Aviv 69978, <country country="IL">Israel</country></aff><xref rid="btaf009-cor1" ref-type="corresp"/><!--talp@tauex.tau.ac.il--></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Schwartz</surname><given-names>Russell</given-names></name><role>Associate Editor</role></contrib></contrib-group><author-notes><corresp id="btaf009-cor1">Corresponding authors. Yonatan Belinkov, The Henry and Marilyn Taub Faculty of Computer Science, Technion&#x02014;Israel Institute of Technol-ogy, Haifa 3200003, Israel. E-mail: <email>belinkov@technion.ac.il</email>; Tal Pupko, The Shmunis School of Biomedicine and Cancer Research, George S. Wise Faculty of Life Sciences, Tel Aviv University, Tel Aviv 69978, Israel. E-mail: <email>talp@tauex.tau.ac.il</email>.</corresp></author-notes><pub-date pub-type="collection"><month>1</month><year>2025</year></pub-date><pub-date pub-type="epub" iso-8601-date="2025-01-08"><day>08</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>08</day><month>1</month><year>2025</year></pub-date><volume>41</volume><issue>1</issue><elocation-id>btaf009</elocation-id><history><date date-type="received"><day>20</day><month>6</month><year>2024</year></date><date date-type="rev-recd"><day>21</day><month>12</month><year>2024</year></date><date date-type="editorial-decision"><day>28</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>07</day><month>1</month><year>2025</year></date><date date-type="corrected-typeset"><day>23</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025. Published by Oxford University Press.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="btaf009.pdf"/><abstract><title>Abstract</title><sec id="s1"><title>Motivation</title><p>Multiple sequence alignments (MSAs) are extensively used in biology, from phylogenetic reconstruction to structure and function prediction. Here, we suggest an out-of-the-box approach for the inference of MSAs, which relies on algorithms developed for processing natural languages. We show that our artificial intelligence (AI)-based methodology can be trained to align sequences by processing alignments that are generated via simulations, and thus different aligners can be easily generated for datasets with specific evolutionary dynamics attributes. We expect that natural language processing (NLP) solutions will replace or augment classic solutions for computing alignments, and more generally, challenging inference tasks in phylogenomics.</p></sec><sec id="s2"><title>Results</title><p>The MSA problem is a fundamental pillar in bioinformatics, comparative genomics, and phylogenetics. Here, we characterize and improve BetaAlign, the first deep learning aligner, which substantially deviates from conventional algorithms of alignment computation. BetaAlign draws on NLP techniques and trains transformers to map a set of unaligned biological sequences to an MSA. We show that our approach is highly accurate, comparable and sometimes better than state-of-the-art alignment tools. We characterize the performance of BetaAlign and the effect of various aspects on accuracy; for example, the size of the training data, the effect of different transformer architectures, and the effect of learning on a subspace of indel-model parameters (subspace learning). We also introduce a new technique that leads to improved performance compared to our previous approach. Our findings further uncover the potential of NLP-based methods for sequence alignment, highlighting that AI-based algorithms can substantially challenge classic approaches in phylogenomics and bioinformatics.</p></sec><sec id="s3"><title>Availability and implementation</title><p>Datasets used in this work are available on HuggingFace (Wolf et al. Transformers: state-of-the-art natural language processing. In: <italic toggle="yes">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</italic>. p.38&#x02013;45. 2020) at: <ext-link xlink:href="https://huggingface.co/dotan1111" ext-link-type="uri">https://huggingface.co/dotan1111</ext-link>. Source code is available at: <ext-link xlink:href="https://github.com/idotan286/SimulateAlignments" ext-link-type="uri">https://github.com/idotan286/SimulateAlignments</ext-link>.</p></sec></abstract><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Israel Science Foundation</institution><institution-id institution-id-type="DOI">10.13039/501100003977</institution-id></institution-wrap>
</funding-source><award-id>448/20</award-id><award-id>2818/21</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Azrieli Foundation Early Career Faculty</institution></institution-wrap>
</funding-source></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Edmond J. Safra Center for Bioinformatics at Tel Aviv University</institution></institution-wrap>
</funding-source></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Edouard Seroussi Chair for Protein Nanobiotechnology</institution></institution-wrap>
</funding-source></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Tel Aviv University</institution><institution-id institution-id-type="DOI">10.13039/501100004375</institution-id></institution-wrap>
</funding-source></award-group></funding-group><counts><page-count count="14"/></counts></article-meta></front><body><sec><title>1 Introduction</title><p>The Needleman&#x02013;Wunsch algorithm was the first to use dynamic programming to efficiently find the best global scoring alignment between two sequences (<xref rid="btaf009-B22" ref-type="bibr">Needleman and Wunsch 1970</xref>). The inference of a multiple sequence alignment (MSA) was later shown to be an NP-complete problem (<xref rid="btaf009-B34" ref-type="bibr">Wang and Jiang 1994</xref>), making the inference task impractical for a large set of sequences. To overcome this hurdle, popular MSA algorithms, such as MAFFT (<xref rid="btaf009-B13" ref-type="bibr">Katoh and Standley 2013</xref>) and PRANK (<xref rid="btaf009-B18" ref-type="bibr">L&#x000f6;ytynoja 2014</xref>), use heuristics to reduce the search space and consequently, the running time.</p><p>There is extensive knowledge regarding the variability of the evolutionary process among different datasets and lineages. For example, amino acid replacement matrices vary between proteins encoded in the nuclear genome, the mitochondria, and plastids (<xref rid="btaf009-B24" ref-type="bibr">Pesole <italic toggle="yes">et al.</italic> 1999</xref>). Indel dynamics also highly vary between datasets and among different phylogenetic groups (<xref rid="btaf009-B36" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic> 2007</xref>, <xref rid="btaf009-B1" ref-type="bibr">Ajawatanawong and Baldauf 2013</xref>, <xref rid="btaf009-B16" ref-type="bibr">Loewenthal <italic toggle="yes">et al.</italic> 2021</xref>). Furthermore, site-specific evolutionary rates vary along the analyzed sequence. For example, amino acid sites that are exposed to the solvent tend to have higher evolutionary rates compared to buried sites (<xref rid="btaf009-B33" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2008</xref>). Alignment algorithms using default configurations implicitly assume that the evolutionary dynamics do not substantially vary among different datasets and within a single dataset. The general inability of MSA inference algorithms to automatically tune their scoring scheme to the specific dataset being analyzed is a shortcoming of present alignment programs. The &#x0201c;one matrix fits all biological datasets&#x0201d; and &#x0201c;one matrix fits all regions within a dataset&#x0201d; assumptions implicitly employed by most current methodologies raise fundamental questions about the correctness of alignments produced by such methods. Although it is possible to modify gap-penalty parameters in some alignment programs, these programs do not provide means to automatically tune the parameters to specific datasets or regions within a dataset, and hence, by and large, all users employ the default settings.</p><p>Alignment algorithms are typically assessed by empirical alignment regions, but these regions are not comprehensive enough to cover the entire range of alignment challenges. These regions are often calculated manually, so their reliability as a &#x0201c;gold standard&#x0201d; is uncertain (<xref rid="btaf009-B12" ref-type="bibr">Iantorno <italic toggle="yes">et al.</italic> 2014</xref>). Differences exist between empirical and simulated datasets, e.g., the latter may not account for evolutionary scenarios such as micro-rearrangements (<xref rid="btaf009-B32" ref-type="bibr">Walker <italic toggle="yes">et al.</italic> 2021</xref>). Thus, when alignment programs are tested with simulated alignments, the results may differ from empirical benchmark outcomes (<xref rid="btaf009-B4" ref-type="bibr">Chang <italic toggle="yes">et al.</italic> 2014</xref>).</p><p>One of the key concepts in learning algorithms, in general, and in deep learning algorithms in particular, is the ability to learn from previously annotated data, i.e., to generalize from previous observations to unseen cases. For the task of alignment inference, a deep learning algorithm should learn from &#x0201c;true&#x0201d; alignments (e.g., simulated sequences for which the correct alignment is known) and apply the obtained knowledge to align novel sequences. In this work, we aimed to harness natural language processing (NLP) learning algorithms to the task of aligning sequences; thus, to better capture the evolutionary dynamics of biological sequences.</p><p>Here, we present an improvement for our previously developed BetaAlign approach (<xref rid="btaf009-B6" ref-type="bibr">Dotan <italic toggle="yes">et al.</italic> 2023</xref>), in which instead of computing a single alignment, we infer multiple alternative alignments and return the one that maximizes the certainty. To further characterize BetaAlign, we conducted the following analyses: (i) evaluating the effect of training time and size; (ii) evaluating the effect of transfer learning; (iii) measuring the performance as a function of the evolutionary dynamics that generated the sequences; and (iv) comparing different transformer architectures. We also introduce the term subspace learning to describe training on a subspace of the indel parameters and investigate its utility for BetaAlign. Lastly, we show that the benefit of our approach is also transferable, that is, the embedding obtained by the model could serve as meaningful features for accurate inference in other learning tasks such as inferring sequence length prediction of ancestral sequences. <xref rid="btaf009-T1" ref-type="table">Table&#x000a0;1</xref> describes the main differences between the previous and current work. For completeness, we start by describing the algorithm.</p><table-wrap position="float" id="btaf009-T1"><label>Table 1.</label><caption><p>The different topics discussed in this research compared to the previous version of BetaAlign.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="left" span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1">Topic</th><th rowspan="1" colspan="1">What is new in this work</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Algorithm: increasing the accuracy by generating alternative alignments for the same set of unaligned sequences and selecting the best one</td><td rowspan="1" colspan="1">We changed our alignment methodology. In the new algorithm, we calculate multiple alternative alignments and return the alignment that maximizes the certainty. Thus, all the results in the current manuscript are new, as they are computed with the novel alignment algorithm</td></tr><tr><td rowspan="1" colspan="1">Analysis: the effect of training time and size</td><td rowspan="1" colspan="1">We investigated the effect of the training phase on BetaAlign&#x02019;s loss and performance</td></tr><tr><td rowspan="1" colspan="1">Analysis: the effect of indel model parameters on BetaAlign performance</td><td rowspan="1" colspan="1">We investigated the effect of indel parameters on BetaAlign&#x02019;s performance</td></tr><tr><td rowspan="1" colspan="1">Analysis: subspace learning</td><td rowspan="1" colspan="1">We introduce the term subspace learning to describe training on a subspace of the indel parameters. We investigate how subspace learning affects BetaAlign&#x02019;s performance</td></tr><tr><td rowspan="1" colspan="1">Algorithm: embedding extraction for downstream tasks</td><td rowspan="1" colspan="1">We introduced a new approach to gather meaningful representations of unaligned and aligned sequences and evaluate its performance</td></tr><tr><td rowspan="1" colspan="1">Analysis: transfer learning</td><td rowspan="1" colspan="1">We investigated the effect of transfer learning on BetaAlign&#x02019;s performance</td></tr><tr><td rowspan="1" colspan="1">Analysis: architecture comparisons</td><td rowspan="1" colspan="1">We investigated the effect of different transformer architectures</td></tr><tr><td rowspan="1" colspan="1">Algorithm: handling invalid alignments and long sequences</td><td rowspan="1" colspan="1">These issues were explained in our previous paper and are hence only shortly described here</td></tr></tbody></table></table-wrap></sec><sec><title>2 Materials and methods</title><sec><title>2.1 New approach</title><p>2.1.1 Outline</p><p>Typically, sequence-to-sequence NLP tasks involve a single sentence (or text) as both input and output, e.g., translating from one language to another or changing a sentence from active to passive (<xref rid="btaf009-B28" ref-type="bibr">Sutskever <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="btaf009-B3" ref-type="bibr">Bahdanau <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btaf009-B27" ref-type="bibr">Shalumov and Haskey 2023</xref>). The learning phase of the algorithm is to map a single input sentence to a single output sentence. When we aim to apply sequence-to-sequence models to the problem of alignment, we are faced with a challenge: the input to the alignment task is several &#x0201c;sentences&#x0201d;, each corresponding to an unaligned sequence. Similarly, the output is a set of related sentences, each corresponding to a row in the resulting alignment. The first task in the BetaAlign algorithm is to transform the set of unaligned sequences to a single &#x0201c;sentence&#x0201d;. Such input- transformation can be done, e.g., by concatenating all the unaligned sequences, adding a special character (we use the pipe character, &#x0201c;|&#x0201d;) to indicate the boundaries between the sequences (<xref rid="btaf009-F1" ref-type="fig">Fig.&#x000a0;1</xref>). For training the algorithm, we also need to provide target sentences. Thus, we also need an output-transformation step, in which we convert resulting alignments to a single target sentence. In BetaAlign, we use the &#x0201c;<italic toggle="yes">spaces</italic>&#x0201d; representation (<xref rid="btaf009-F1" ref-type="fig">Fig.&#x000a0;1</xref>). The above representations allow providing a sequence-to-sequence model with a large set of examples of valid source and target sentences, which are used for model training. The models that we use rely on the transformer architecture (<xref rid="btaf009-B31" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic> 2017</xref>). Once trained, the optimized transformer can process new unseen examples, in our case, it can transform (unseen) unaligned sequences to an alignment.</p><fig position="float" id="btaf009-F1"><label>Figure 1.</label><caption><p>
<bold>Example of aligning three sequences with BetaAlign,</bold> (a): (I) Consider the unaligned sequences &#x0201c;AAG&#x0201d;, &#x0201c;ACGG&#x0201d; and &#x0201c;ACG&#x0201d;; (II) The unaligned sequences are concatenated to a single sentence with a special character &#x0201c;|&#x0201d; between each original sequence; (III) The trained model processes the single input sentence and generates the single output sentence; (IV) The processed output is structured such that the first three nucleotides represent the first column, the next three nucleotides represent the second column, and so on; (V) The output is converted into an MSA. (b) An illustration of the different input (I) and output (II) transformation schemes. (c) Example of handling invalid alignments. When aligning the same sequences, BetaAlign first transformer may mistakenly mutated the character &#x0201c;A&#x0201d; to &#x0201c;G&#x0201d; (I); A different transformer resulted in a different output, may generate a shorter sequence in which the last two characters are missing (II); The third transformer provided a valid alignment as output and can be used as the output of BetalAlign (III).</p></caption><graphic xlink:href="btaf009f1" position="float"/></fig><p>There are several aspects that need to be addressed to fully describe the BetaAlign algorithm and how its performance was evaluated. These include, for example, the generation of training and test data, the transformer architecture and how it was trained, the handling of long sequences and how the generation of invalid alignments was prevented. We aim to provide a more general description as part of the New approach section, while technical details are provided in Materials and Methods.</p></sec><sec><title>2.1.2 Generation of training and test data</title><p>For both training and testing the performance of BetaAlign, many sets (data points) of unaligned sequences and their corresponding &#x0201c;true alignment&#x0201d; were needed. These data points were generated using simulations. Specifically, we use SpartaABC (<xref rid="btaf009-B16" ref-type="bibr">Loewenthal <italic toggle="yes">et al.</italic> 2021</xref>), which allows different length distributions for insertions and deletions. For example, the initial testing and training for the pairwise alignment problem were achieved by generating millions of pairs of two unaligned sequences and their corresponding alignments for the training and testing data. The indel rates, their type (insertion or deletion), and their length distribution were sampled from specific ranges. We note that we do not assume equal rates of insertions and deletions, nor equal length distributions for the two types of events (this is mainly important when simulating along a tree rather than when simulating pairwise alignments).</p></sec><sec><title>2.1.3 Transformer architecture</title><p>Transformers are currently the working horse of NLP and other AI domains. A transformer is a deep learning model designed to handle discrete sequential data. The transformer used in our work is composed of an encoder and a decoder. The encoder embeds each input sequence (and in our case, the source sequence representing multiple unaligned sequences) into a sequence of high-dimensional vector representation. In this vector space, two related sequences should be closer to each other than two less related sequences. This projection from the sequence space to the high-dimensional space, i.e., the embedding process, is not fixed, but rather is learned as part of the training process. Next, the decoder receives those representations and the last generated token and predicts the next token (a token in natural languages is the building block of a sentence, in our case, each token is either a base pair or an amino acid). The encoder and decoder are neural networks with multiple sequential layers, each containing numerous neurons. These neurons act as linear functions with tunable parameters that are adjusted during training. To handle complex data, each layer of the transformer also incorporates non-linear functions (without non-linear functions, the model acts as a function composition of linear only functions which results in a linear function). Although each layer could theoretically attend to all previously computed embeddings (i.e., the input to the layer), previous research has shown that focusing on a specific subset of tokens yields better results. This approach, known as attention, is a foundational aspect of the transformer model (<xref rid="btaf009-B31" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic> 2017</xref>). To create the initialized set of the embeddings, the discrete data (in our case, the DNA and the amino acid sequences) are converted via the tokenizer into a set of ids (<xref rid="btaf009-B7" ref-type="bibr">Dotan <italic toggle="yes">et al.</italic> 2024</xref>), which are then converted to a numerical representation via the embeddings matrix. Transformers may vary in architecture, number of layers, and size. These features are the tunable architectural hyperparameters. When training a transformer, one can also vary the learning hyperparameters, e.g., the parameter &#x0201c;max tokens&#x0201d; determines how much input to process before the model parameters are updated. We have tested several transformer architectures and parameters, implemented using the Fairseq library (<xref rid="btaf009-B23" ref-type="bibr">Ott <italic toggle="yes">et al.</italic> 2019</xref>).</p></sec><sec><title>2.1.4 Transfer learning and subspace learning</title><p>The input and output patterns of the analyzed sequences vary as a function of their number, e.g., the number of pipe characters in the &#x0201c;<italic toggle="yes">concat</italic>&#x0201d; representation. We thus optimized a different transformer for each number of sequences. To this end, when optimizing the transformer for, say, five sequences, we start the parameter optimization step from the set of optimal parameters obtained for the previous transformer that was trained on four sequences, a technique called transfer learning (<xref rid="btaf009-B30" ref-type="bibr">Tan <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="btaf009-B2" ref-type="bibr">Avram <italic toggle="yes">et al.</italic> 2024</xref>).</p><p>We also use transfer learning in order to train a transformer on subregions of the parameter space, i.e., subspace learning (see Section 2.9). For example, we can train a general pairwise alignment transformer as described above and then train a different transformer only for alignments with a high ratio of indels to substitutions. In essence, this allows training several transformers, specialized for subregions of the parameter space.</p></sec><sec><title>2.1.5 Handling invalid alignments</title><p>Transformers have no inherent mechanism that restricts them to generate valid alignments. Thus, in some cases, a trained transformer may produce invalid outputs. For example, when aligning sequences, each output sequence, including gap characters, should be of the same length (<xref rid="btaf009-F1" ref-type="fig">Fig.&#x000a0;1c</xref>). To this end, we trained several different transformers, which differ from each other with respect to their tunable hyperparameters, on the same training dataset (see Section 2.3). If a transformer provided an invalid alignment, we provided the output of an alternative transformer.</p></sec><sec><title>2.1.6 Handling long sequences</title><p>The transformers that we have utilized were designed to process text of natural languages and not biological sequences. As such, they are limited to processing sentences with up to 1024 tokens. When aligning biological sequences, the input and output sentences often exceed this length threshold. Due to memory and run-time constraints, increasing the threshold is infeasible. To overcome this challenge, we introduced a &#x0201c;segmentation&#x0201d; methodology, in which we align segments of the alignments, which are later concatenated to form the entire MSA. This procedure is achieved by training dedicated transformers for this task (<xref rid="btaf009-B6" ref-type="bibr">Dotan <italic toggle="yes">et al.</italic> 2023</xref>).</p></sec><sec><title>2.1.7 Considering alternative input and output transformation schemes</title><p>The transformer architectures we harnessed for the task of aligning sequences are sequence-to-sequence models. One of the key components of our proposed alignment approach is to transpose the multiple input sequences into a single sentence that can be processed by the transformer. Input transformation converts the unaligned sequences into the &#x0201c;input sentence&#x0201d; of the transformer, while output transformation converts the &#x0201c;output sentence&#x0201d; of the transformer into an MSA.</p><p>There are various transformation schemes available for converting unaligned sequences into a single sentence. In <xref rid="btaf009-F1" ref-type="fig">Fig.&#x000a0;1a</xref>, we present the &#x0201c;<italic toggle="yes">concat</italic>&#x0201d; representation: the unaligned sequences are concatenated with a special character &#x0201c;|&#x0201d;. The vocabulary, which encompasses the entire set of possible tokens, of this scheme is {&#x0201c;A&#x0201d;, &#x0201c;C&#x0201d;, &#x0201c;G&#x0201d;, &#x0201c;T&#x0201d; and &#x0201c;|&#x0201d;} for the nucleotide sequences. We used the &#x0201c;<italic toggle="yes">spaces</italic>&#x0201d; representation for output transformation, in which each of the amino acids or nucleotides is considered a separate token. The vocabulary of this scheme for DNA sequences is {&#x0201c;A&#x0201d;, &#x0201c;C&#x0201d;, &#x0201c;G&#x0201d;, &#x0201c;T&#x0201d; and &#x0201c;&#x02013;&#x0201d;}.</p><p>However, alternative transformation schemes for the source sequences can be considered. We previously considered the &#x0201c;<italic toggle="yes">crisscross</italic>&#x0201d; scheme, the tokens of the unaligned sequences are interleaved (<xref rid="btaf009-B6" ref-type="bibr">Dotan <italic toggle="yes">et al.</italic> 2023</xref>). That is, the first token represents the first character from the first unaligned sequence, the second token represents the first token of the second unaligned sequence, and so on. The vocabulary of this scheme is {&#x0201c;A&#x0201d;, &#x0201c;C&#x0201d;, &#x0201c;G&#x0201d;, &#x0201c;T&#x0201d; and &#x0201c;&#x02013;&#x0201d;} for the nucleotide sequences. Of note, the gap character is used to fill the gaps if the sequences are of different lengths (<xref rid="btaf009-F1" ref-type="fig">Fig.&#x000a0;1b</xref>). Similarly, alternative transformation schemes for generating the output sentence are possible.</p><p>In the &#x0201c;pairs&#x0201d; scheme, each token represents the entire column. The vocabulary of this scheme depends on the number of unaligned sequences, for instance, when aligning three DNA sequences, the vocabulary size is 124 tokens: {&#x0201c;AAA&#x0201d;, &#x0201c;AAC&#x0201d;, &#x0201c;AAG&#x0201d;, &#x0201c;AAT&#x0201d;, &#x0201c;AA&#x02013;&#x0201d;, &#x02026;, &#x0201c;TTG&#x0201d;, and &#x0201c;TTT&#x0201d;}. Of note, the token &#x0201c;&#x02013; &#x02013; &#x02013;&#x0201d; (three gap characters) is invalid as such column cannot exist.</p><p>It is important to remember that the transformation schemes are external to the transformer itself. Each transformation methodology creates a different mapping from unaligned sequences to an MSA, which requires training the transformer on these representations. Different considerations come into play when selecting the appropriate scheme (<xref rid="btaf009-B6" ref-type="bibr">Dotan <italic toggle="yes">et al.</italic> 2023</xref>). In the &#x0201c;<italic toggle="yes">pairs</italic>&#x0201d; scheme, the output sequence length is the number of columns, while in the &#x0201c;<italic toggle="yes">spaces</italic>&#x0201d; the length is the number of nucleotides. Because length is a limiting factor when using current transformer architectures, using the &#x0201c;<italic toggle="yes">pairs</italic>&#x0201d; scheme may be advantageous. However, the &#x0201c;<italic toggle="yes">pairs</italic>&#x0201d; scheme restricts the use of transfer learning (see below). When transitioning from pairwise alignment to aligning three sequences, the vocabulary would change (from 24 tokens to 124 tokens) and in general, the number of possible tokens exponentially increases as a function of the number of unaligned sequences. In our previous work, we observed that the &#x0201c;<italic toggle="yes">concat</italic>&#x0201d; and &#x0201c;<italic toggle="yes">spaces</italic>&#x0201d; representations (shown in <xref rid="btaf009-F1" ref-type="fig">Fig.&#x000a0;1a</xref>) performed best (<xref rid="btaf009-B6" ref-type="bibr">Dotan <italic toggle="yes">et al.</italic> 2023</xref>). Thus, all the experiments in this work are done with these representations for the input sequences and output MSA, respectively.</p></sec><sec><title>2.1.8 Increasing the accuracy by generating alternative alignments for the same set of unaligned sequences and selecting the best one</title><p>We present a method for generating multiple alternative MSAs from the same input data. This is done by randomizing the order in which the input unaligned sequences are concatenated. We also show how we select a single MSA from this set using a &#x0201c;majority voting&#x0201d; approach. We show that, on average, this data augmentation followed by majority voting approach provides a more accurate MSA than relying on a randomly sampled MSA from the set of alternative MSAs. The majority voting approach relies on computing for each MSA, the degree of its agreement with all other alternative MSAs and selecting the one that agrees the most (see Section 2.4).</p></sec></sec><sec><title>&#x02003;</title><sec><title>2.2 Generation of training and test data</title><p>We first describe in detail the simulation of nucleotide dataset SND1, in which each data point includes 10 unaligned sequences and their corresponding &#x0201c;true&#x0201d; MSA. We generated 395&#x000a0;000 and 3000 data points for training and testing data, respectively. For each data point, we sampled a random tree using the program ETE 3 (<xref rid="btaf009-B11" ref-type="bibr">Huerta-Cepas <italic toggle="yes">et al.</italic> 2016</xref>), with tree lengths uniformly distributed in the range <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mo>(</mml:mo><mml:mn>0.05</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mn>0.1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>. The sequences along each tree were simulated using SpartaABC (<xref rid="btaf009-B16" ref-type="bibr">Loewenthal <italic toggle="yes">et al.</italic> 2021</xref>). Specifically, indel parameters were sampled from the following ranges: <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mo>(</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mn>0.05</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>, <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mn>1.01</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mn>2.0</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, and root length <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mo>&#x02208;</mml:mo><mml:mo>[</mml:mo><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mn>44</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>. Of note, the insertion (<inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) and deletion (<inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) model parameters were sampled independently allowing a rich-indel model, in which insertions and deletions can have different evolutionary dynamics. The above parameter ranges were found to accurately describe the indel evolution rates along the tree of life (<xref rid="btaf009-B16" ref-type="bibr">Loewenthal <italic toggle="yes">et al.</italic> 2021</xref>). The WAG+G and the GTR+G substitution models were used for the protein and nucleotide datasets, respectively. The GTR+G frequencies were <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mo>(</mml:mo><mml:mn>0.37</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mn>0.166</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mn>0.307</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mn>0.158</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> for the &#x0201c;T&#x0201d;, &#x0201c;C&#x0201d;, &#x0201c;A&#x0201d;, and &#x0201c;G&#x0201d;, respectively. Substitution rates were <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mo>(</mml:mo><mml:mn>0.444</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mn>0.0843</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mn>0.116</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mn>0.107</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mn>0.00027</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula> for the &#x0201c;a&#x0201d;, &#x0201c;b&#x0201d;, &#x0201c;c&#x0201d;, &#x0201c;d&#x0201d;, and &#x0201c;e&#x0201d; rate parameters as defined in <xref rid="btaf009-B37" ref-type="bibr">Yang (1994)</xref>. These frequencies and rate parameters reflect those that characterize the Yeast Intron Database (<xref rid="btaf009-B17" ref-type="bibr">Lopez and S&#x000e9;raphin 2000</xref>). Specific information for the simulation of each dataset is provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>. The datasets are available on HuggingFace (<xref rid="btaf009-B35" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic> 2020</xref>) at: <ext-link xlink:href="https://huggingface.co/dotan1111" ext-link-type="uri">https://huggingface.co/dotan1111</ext-link>.</p></sec><sec><title>2.3 BetaAlign training and architecture</title><p>We applied the &#x0201c;vaswani_wmt_en_de_big&#x0201d; architecture (<xref rid="btaf009-B31" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic> 2017</xref>) with 16 attention heads, embeddings size of 1024 and 6 layers. We also conducted an experiment to evaluate the effect of alternative architectures on performance (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>). We considered a variety of training hyperparameters configurations for the transformer, including different max tokens values, learning rates, and warmup updates and evaluated them on datasets of pairwise alignments (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>). We continued to train two configurations that yielded the best results, which we denote as &#x0201c;original&#x0201d; and &#x0201c;alternative&#x0201d;. The max token parameter values were 4096 and 2048 for the original and alternative transformers, respectively. For both configurations, we used the same learning rate (5E-5) and warmup updates (3000). Model training and evaluations were executed on a Tesla V100-SXM2-32GB GPU machine.</p></sec><sec><title>2.4 Using alternative alignments to increase the accuracy of BetaAlign</title><p>A &#x0201c;column certainty&#x0201d; metric was employed to compute &#x0201c;alignment certainty&#x0201d;. Given an alignment, <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mi>x</mml:mi></mml:math></inline-formula>, and a set of alternative alignments, <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mi>Y</mml:mi></mml:math></inline-formula>, the column certainty of each column in <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mi>x</mml:mi></mml:math></inline-formula> is the number of times the column appears in each alternative alignment <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mi>y</mml:mi><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mi>Y</mml:mi></mml:math></inline-formula> divided by the total number of alignments in <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mi>Y</mml:mi></mml:math></inline-formula>. As a result, column certainty values range between 0 and 1, where a score of 1 indicates high certainty. The alignment certainty is defined as the average of the column certainty values (<xref rid="btaf009-F2" ref-type="fig">Fig.&#x000a0;2a</xref>).</p><fig position="float" id="btaf009-F2"><label>Figure 2.</label><caption><p>
<bold>Quantifying the correlation of alignment certainty and alignment accuracy.</bold> (a) An illustration demonstrating the calculation of alignment certainty. Consider <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mi>x</mml:mi></mml:math></inline-formula> to be a pairwise alignment where &#x0201c;AAGT&#x0201d; is aligned to &#x0201c;ACGT&#x0201d; and <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mi>Y</mml:mi></mml:math></inline-formula> to be the collection of two alternative alignments: (i) where &#x0201c;AAG-T&#x0201d; is aligned to &#x0201c;A-CGT&#x0201d; and (ii) where &#x0201c;AAGT&#x0201d; is aligned to &#x0201c;ACGT&#x0201d;. To determine the certainty for each column in <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mi>x</mml:mi></mml:math></inline-formula>, we count the number of its appearances in the set of alternative alignments <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mi>Y</mml:mi></mml:math></inline-formula> and divide it by the size of the set <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi>Y</mml:mi></mml:math></inline-formula>. For example, the first column, &#x0201c;AA&#x0201d;, appears both in alignments (i) and (ii) and thus its certainty is 2/2. The second column in <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mi>x</mml:mi></mml:math></inline-formula>, &#x0201c;AC&#x0201d; appears only in alignment (ii) and thus its certainty is 1/2. (b) The frequency of the optimal alternative alignment for each certainty rank. For each data point, a total of 20 alternative alignments were considered, each with 10 sequences (SND1 and SPD1 for the nucleotide and protein datasets, respectively). The 20 MSAs were ranked according to their certainty. Next, the most accurate MSA was detected (based on the CS accuracy score) and its rank recorded. Of note, some of the alternative MSAs may be identical. In case the most accurate MSA was ranked multiple times (e.g., the first and second ranks), we consider its rank to be the higher rank (e.g., the first). Shown is the distribution of ranks among 3000 independent data points. In almost all cases, the MSA that had the highest confidence is ranked highest.</p></caption><graphic xlink:href="btaf009f2" position="float"/></fig><p>It is possible to generate alternative MSAs for the same set of sequences. For example, alternative MSAs are generated by GUIDANCE to quantify the reliability of different regions within an MSA (<xref rid="btaf009-B26" ref-type="bibr">Sela <italic toggle="yes">et al.</italic> 2015</xref>). These alternative MSAs are computed by considering alternative guide trees, considering co-optimal solutions of pairwise alignments, and changing the alignment scoring scheme. Alternative MSAs are also computed within the alignment program Muscle (<xref rid="btaf009-B9" ref-type="bibr">Edgar 2022</xref>). The alignment that agrees best with the set of alternative MSAs is then chosen as the inferred MSA. We developed a similar approach for generating alternative MSAs, which is based on the deep learning methodology proposed here. Specifically, we alternate the order of the unaligned sequences given as input to the &#x0201c;<italic toggle="yes">concat</italic>&#x0201d; representation. This results in the inference of different MSAs for the same input. For example, an MSA of three sequences results in six different permutations, thus providing six alternative MSAs and similarly <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mi>k</mml:mi><mml:mo>!</mml:mo></mml:math></inline-formula> alternative alignments for <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mi>k</mml:mi></mml:math></inline-formula> sequences. In addition, as we trained several transformers with different training parameters for each dataset, we can add alternative alignments from two or more transformers by processing the same input using these different transformers (<xref rid="btaf009-B6" ref-type="bibr">Dotan <italic toggle="yes">et al.</italic> 2023</xref>).</p><p>Formally, let <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mi>h</mml:mi></mml:math></inline-formula> be a list of unaligned sequences and an aligner program, respectively. When computing an alignment, the aligner is dependent on a set of parameters, i.e., a configuration, denoted by <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mi>&#x003b1;</mml:mi></mml:math></inline-formula>. Altering <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mi>&#x003b1;</mml:mi></mml:math></inline-formula> would output a different alignment for the same <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mi>x</mml:mi></mml:math></inline-formula> and <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mi>h</mml:mi></mml:math></inline-formula>. Thus, for a list of <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mi>n</mml:mi></mml:math></inline-formula> different configurations: <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, one would receive <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mi>n</mml:mi></mml:math></inline-formula> different alignments: <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>}</mml:mo></mml:math></inline-formula>. Of note, the alignments of different configurations could be the same. Creating the different configurations could be done by changing the scoring scheme for the aligners or by changing the permutation of the unaligned sequences in the case of BetaAlign (see above). For each alignment <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></inline-formula> we calculate the alignment certainty described above, by comparing it to all the other alignments <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mi>Y</mml:mi><mml:mo>\</mml:mo><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>}</mml:mo></mml:math></inline-formula> and computing the average number of shared columns. We return the alignment that maximizes the alignment certainty. Specifically, we have two transformer configurations (&#x0201c;original&#x0201d; and &#x0201c;alternative&#x0201d;) and for each, we generated 10 alternative MSAs. We return the valid alignment with the highest certainty.</p></sec><sec><title>2.5 Calculating the loss</title><p>The training loss is calculated using the cross-entropy loss function (Szegedy <italic toggle="yes">et al.</italic> 2016). Consider a specific position within a pairwise alignment. In the &#x0201c;<italic toggle="yes">spaces</italic>&#x0201d; representation, there are five possible tokens in the nucleotide output (the five characters, &#x0201c;A&#x0201d;, &#x0201c;C&#x0201d;, &#x0201c;G&#x0201d;, &#x0201c;T&#x0201d;, &#x0201c;&#x02013;&#x0201d;. In fact, the pipe character can also appear, as we use the same dictionary as the input). Our aligner predicts (accounting for the proceeding predictions in the alignment) a probability for each token. Let <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>&#x000a0;</mml:mi></mml:math></inline-formula>be the probability for the token in which the next character in the output alignment is <italic toggle="yes">i</italic>. Assume, for example that the correct class (the next character in the correct alignment) is &#x0201c;C&#x0201d;. In this case, the loss for this position is simply <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The loss over the entire alignment string is the average loss over all positions in the alignment. If all positions are predicted correctly (i.e., with a <inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>&#x000a0;</mml:mi><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mo>&#x0223c;</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:math></inline-formula>), the loss is close to zero. The higher the loss, the less accurate the prediction is. When the loss function is computed on the training data, we call it &#x0201c;training loss&#x0201d;, while when it is computed on the validation data, we call it &#x0201c;validation loss&#x0201d;.</p></sec><sec><title>2.6 Evaluating accuracy and coverage</title><p>We evaluated the performance of BetaAlign using two metrics: (i) column score (CS), which identifies how many columns are shared between the inferred and the true alignment. Of note, a shared column requires the same characters with the same positions of each character (<xref rid="btaf009-B26" ref-type="bibr">Sela <italic toggle="yes">et al.</italic> 2015</xref>). The CS is the number of shared columns divided by the number of columns and thus the score is in the range [0,1]. The CS-error is the complementary of the CS to 1. (ii) We use the term coverage to denote the percentage of valid alignments out of the total number of MSAs generated by the transformer. Examples of invalid alignments are illustrated in <xref rid="btaf009-F1" ref-type="fig">Fig.&#x000a0;1c</xref>.</p></sec><sec><title>2.7 Evaluating the effect of training time and size</title><p>We generated datasets containing 50&#x000a0;000, 100&#x000a0;000, and 200&#x000a0;000 alignments. Next, we trained transformers on each of the datasets for 60 epochs with the original transformer training parameters. We evaluated the performance of the transformers at the end of each epoch, with respect to the following metrics: (i) training loss, (ii) validation loss, (iii) fraction of invalid alignments (i.e., 1&#x02014;coverage), and (iv) CS-error. The validation data contained 2000 alignments, which were used to measure the validation loss. The test data contained 3000 alignments, which were used to measure the fraction of invalid alignments and CS-error. Of note, in each of the three experiments we initialized the model with random weights, and thus, transfer learning did not affect these results.</p></sec><sec><title>2.8 Evaluating the effect of indel parameters on alignment inference accuracy</title><p>To quantify the effect of the evolutionary parameters on alignment inference accuracy, we generated training and test data using the same random topology and branch lengths as were used in PD14 (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). The range of indel evolutionary parameters was binned: For <inline-formula id="IE56"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE57"><mml:math id="IM57" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that dictate indel-length distribution for insertions and deletions, respectively, the following ten bins were considered for each parameter: (1.0, 1.1), (1.1, 1.2) &#x02026; (1.9, 2.0). For <inline-formula id="IE58"><mml:math id="IM58" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE59"><mml:math id="IM59" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that dictate indel rates relative to substitutions for insertions and deletions, respectively, the following ten bins were considered for each parameter: (0.000, 0.005), (0.005, 0.01) &#x02026; (0.045, 0.05). We thus considered 100 bins for the pair (<inline-formula id="IE60"><mml:math id="IM60" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> and similarly for the pair (<inline-formula id="IE61"><mml:math id="IM61" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE62"><mml:math id="IM62" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>). When analyzing the effect of <inline-formula id="IE63"><mml:math id="IM63" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE64"><mml:math id="IM64" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, for each of the 100 (<inline-formula id="IE65"><mml:math id="IM65" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> bins, 100 alignments were generated, in which the <inline-formula id="IE66"><mml:math id="IM66" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE67"><mml:math id="IM67" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> values were sampled randomly from the range (0.00, 0.05). Thus, in total 10&#x000a0;000 MSAs were considered when studying the effect of the <inline-formula id="IE68"><mml:math id="IM68" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE69"><mml:math id="IM69" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> parameters. Similarly, 10&#x000a0;000 MSAs were considered when studying the effect of the <inline-formula id="IE70"><mml:math id="IM70" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE71"><mml:math id="IM71" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> parameters, and in this case, in each MSA the <inline-formula id="IE72"><mml:math id="IM72" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE73"><mml:math id="IM73" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> parameters were sampled from the range <inline-formula id="IE74"><mml:math id="IM74" display="inline" overflow="scroll"><mml:mo>(</mml:mo><mml:mn>1.0</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x000a0;</mml:mi><mml:mn>2.0</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>. The score for each bin is the average over the scores of the 100 alignments in each bin.</p></sec><sec><title>2.9 Subspace learning evaluation</title><p>The MSA in the training data for BetaAlign is generated by evolving sequences along a specific phylogenetic tree and different MSAs are generated with different trees and with different evolutionary models. The substitution and indel dynamics are dictated in this simulation by an evolutionary model (a continuous-time Markov process). Let <inline-formula id="IE75"><mml:math id="IM75" display="inline" overflow="scroll"><mml:mi>g</mml:mi></mml:math></inline-formula> be the set of evolutionary models and trees used to generate the data. Clearly, a trained aligner, <italic toggle="yes">h</italic>, depends on <inline-formula id="IE76"><mml:math id="IM76" display="inline" overflow="scroll"><mml:mi>g</mml:mi></mml:math></inline-formula>. In other words, our aligner learns to align sequences generated by the set of evolutionary models <inline-formula id="IE77"><mml:math id="IM77" display="inline" overflow="scroll"><mml:mi>g</mml:mi></mml:math></inline-formula> that generated the training data. Thus, we can easily create aligners that will best suit a specific subspace of model parameters and trees, e.g., aligners for a specific phylogenetic tree, and similarly aligners for species or proteins with a specific indel or substitution dynamics. In subspace learning, the transformer is optimized on a subspace of the alignment parameters space. To test how subspace learning affects performance, we generated three nucleotide datasets, each one with a narrower range of model parameters, i.e., <inline-formula id="IE78"><mml:math id="IM78" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE79"><mml:math id="IM79" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula id="IE80"><mml:math id="IM80" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE81"><mml:math id="IM81" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, branch lengths and root lengths (ND10, ND11, and ND12). We trained BetaAlign starting with the dataset of the widest parameter range (ND10), which we named &#x0201c;general&#x0201d;. Then, the optimized transformers were used as the starting point for additional training on the next dataset, ND11, whose model parameters are a subset of those of ND10. We named this dataset &#x0201c;specific&#x0201d;. The optimized transformers from ND11 were then further trained on the next dataset (ND12) &#x0201c;ultra specific&#x0201d;. Each of the three transformers was evaluated on each of the three test datasets.</p></sec><sec><title>2.10 Embedding of MSAs in a high-dimensional space</title><p>The deep learning approach presented here enables embedding the information within the sequences in a high-dimensional space, i.e., it allows automatic feature extraction, which could be utilized for downstream analyses. The high-dimensional vector is created within the encoding process from a set of unaligned sequences. To obtain the embedded vector, the unaligned sequences were given as an input to the trained transformer. The vector is internally created by the encoder part of the transformer, and we have modified the code of the transformer to extract it (to reduce running time, we skipped the decoder step). This high-dimensional vector contains &#x0223c;1024 <inline-formula id="IE82"><mml:math id="IM82" display="inline" overflow="scroll"><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>l</mml:mi></mml:math></inline-formula> entries, where <italic toggle="yes">n</italic> is the number of input sequences and <italic toggle="yes">l</italic> is the average length of unaligned sequences. A representation of this vector, for three sequences, is given in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2a</xref>.</p><p>For various downstream tasks, it is often desirable to compress this vector to a fixed size, i.e., a size that does not depend on the sequence length (the compressed vector size does depend on the number of sequences). For the compression example shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2</xref>, the uncompressed vector is of size 1024&#x000d7;15 and the size of the compressed vector is 1024&#x000d7;5. Each of the unaligned sequences is represented by 1024 entries in the compressed vector by row-wise averaging of the corresponding tokens in the input sequences. In addition, we use the representations of the pipe character in the compressed vector. Thus, the compressed vector corresponds to a vector of a fixed size of 1024 <inline-formula id="IE83"><mml:math id="IM83" display="inline" overflow="scroll"><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mo>&#x02013;</mml:mo><mml:mi mathvariant="normal">&#x000a0;</mml:mi><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>.</p></sec><sec><title>2.11 Evaluating and implementing transfer learning</title><p>In our work, transfer learning was repeatedly used for training the transformers. The first protein transformer was trained on a simple dataset of pairwise amino acid sequences (we denote this dataset PD1, for protein dataset 1). Its weights were randomly sampled with default values of the Fairseq library (<xref rid="btaf009-B23" ref-type="bibr">Ott <italic toggle="yes">et al.</italic> 2019</xref>). The resulting trained transformer is termed as &#x0201c;PT1&#x0201d;, for protein transformer 1. PT1 was next trained on PD2, resulting in PT2, etc. The term transfer learning is used to denote the fact that in order to obtain PT2, the transformer trained on PD2 was initialized with weights transferred from PT1, rather than random initialization. A similar process was used to train the nucleotide-based transformers (NT1, NT2, etc.) on nucleotide datasets (ND1, ND2, etc.). Of note, transfer learning was applied across this study only between models that processed data with the same representation, i.e., they share the same dictionaries.</p><p>We aimed to evaluate the contribution of transfer learning. To this end, we compared three different scenarios (illustrated in <xref rid="btaf009-F3" ref-type="fig">Fig.&#x000a0;3</xref>). In Scenario 1, we evaluate a transformer that first encounters protein data PD5 (three protein sequences). This transformer was trained before on simpler datasets. In Scenario 2, the trained transformer from Scenario 1 was retrained on PD5, without experiencing more complex datasets. In Scenario 3, the trained transformer from Scenario 1 was trained on additional more complex datasets (PD6, PD7, PD8, PD9, PD10, PD11, PD12, PD13, PD14, and PD15) and was then retrained on PD5.</p><fig position="float" id="btaf009-F3"><label>Figure 3.</label><caption><p>
<bold>Quantifying the contribution of transfer learning to performance.</bold> (a) The transfer learning path. Scenario 1 includes training on &#x0201c;D1&#x0201d;, &#x0201c;D2&#x0201d;, and &#x0201c;D3&#x0201d;. Scenario 2 is the same as Scenario 1, but the transformer was trained twice on &#x0201c;D3&#x0201d;. Scenario 3 includes training on &#x0201c;D1&#x0201d;, &#x0201c;D2&#x0201d;, &#x0201c;D3&#x0201d;, &#x0201c;D4&#x0201d;, &#x0201c;D5&#x0201d; and then again on &#x0201c;D3&#x0201d;. &#x0201c;D1&#x0201d; and &#x0201c;D2&#x0201d; represent simpler datasets. &#x0201c;D3&#x0201d; is the target dataset, composed of MSAs of three DNA or amino acid sequences, on which the performance was evaluated. &#x0201c;D4&#x0201d; and &#x0201c;D5&#x0201d; represent more complex datasets. Arrows between datasets represent the transfer learning path, i.e., the transformer optimized on a dataset was used as a base transformer for the next dataset. (b) The effect of transfer learning on the performance.</p></caption><graphic xlink:href="btaf009f3" position="float"/></fig><p>A similar evaluation was done on nucleotide transformers. Here instead of PD5, the base-dataset was ND4, comprised of alignments of three sequences. In Scenario 3, the additional more complex datasets are: ND5, ND6, ND7, ND8, ND9, ND10, ND11, ND12, ND13, and ND14.</p></sec><sec><title>2.12 Comparing against other alignment programs</title><p>The performance of BetaAlign was compared to the following programs used with default parameters: MUSCLE v3.8.1551 (<xref rid="btaf009-B8" ref-type="bibr">Edgar 2004</xref>), MAFFT v7.475 (<xref rid="btaf009-B13" ref-type="bibr">Katoh and Standley 2013</xref>), PRANK v.150803 (<xref rid="btaf009-B19" ref-type="bibr">L&#x000f6;ytynoja and Goldman 2008</xref>), ClustalW 2.1 (<xref rid="btaf009-B14" ref-type="bibr">Larkin <italic toggle="yes">et al.</italic> 2007</xref>), and DIALIGN dialign2-2 (<xref rid="btaf009-B21" ref-type="bibr">Morgenstern 2004</xref>). Specific commands used for evaluation are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p></sec></sec><sec><title>3 Results</title><sec><title>3.1 Effect of training time and size</title><p>We tested how the number of epochs (a single pass on the whole training set) and training size affect the accuracy and coverage of BetaAlign. We compared the model&#x02019;s performance when trained on three training data sizes: 50&#x000a0;000, 100&#x000a0;000, and 200&#x000a0;000 protein alignments. Our results clearly indicate that for all datasets, the training loss (see Section 2.5) decreases as the number of epochs increases, reaching almost a plateau when the data size is 200&#x000a0;000 alignments (<xref rid="btaf009-F4" ref-type="fig">Fig.&#x000a0;4</xref>). For each training data size, the validation loss follows the decrease in the training loss, suggesting that there is no overfitting for the transformer. The coverage (fraction of resulting alignments that are valid) also continuously increases, e.g., after 20 epochs the coverage was &#x0223c;40%, while after 60 epochs, the coverage was already &#x0223c;80%.</p><fig position="float" id="btaf009-F4"><label>Figure 4.</label><caption><p>
<bold>Effect of increasing the training time (number of epochs) and size (number of different MSAs) on the fraction of invalid alignments (blue dots), CS-error (orange dots), validation loss (red dots), and training loss (green dots).</bold> All alignments were of three protein sequences, dataset SPD2. Note that the figure contains the four metrics together for comparing the correlation between the metrics. Each metric has a different range, and thus, there are multiple <italic toggle="yes">y</italic>-axes. Also note that the errors and coverage in this graph are based on a single alternative alignment, while in practice both the accuracy and coverage are substantially improved by considering a set of alternative MSAs (see text for details). The training loss and the validation loss clearly decrease with the number of epochs and the number of alignments used for training. In contrast, the CS-error and the fraction of invalid alignments show mediocre correlations with the training loss, and do not significantly decrease with the number of alignments used for training.</p></caption><graphic xlink:href="btaf009f4" position="float"/></fig><p>Inference accuracy is measured using the column score (CS), which quantifies the number of columns that are shared between the inferred and the &#x0201c;true&#x0201d; MSA (see Section 2.6). The CS-error (one minus the CS) seems to substantially fluctuate even after 30 epochs (we note that the CS-error quantifies the error on valid alignments only, while the loss function quantifies the error on all alignments). The correlation between the loss on the validation data and the CS-error on the dataset of 100&#x000a0;000 alignments, between epochs 20 and 60, was <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> = 0.467 (<italic toggle="yes">p</italic>&#x02009;=&#x02009;0.0023). This correlation suggests that reducing the loss also reduces the CS-error, despite the clear differences between these two functions.</p><p>Comparing the training and validation loss between the different training size datasets indicated that increasing the training size decreases the loss as expected (training loss at epoch 60: 0.989, 0.985, 0.977, for datasets of 50&#x000a0;000, 100&#x000a0;000, 200&#x000a0;000, respectively). This gain in accuracy as reflected in the loss function was not evident when the performance is measured by the CS-error, reflecting lack of strong correlation between these two scores.</p></sec><sec><title>3.2 Effect of indel model parameters on BetaAlign performance</title><p>We next studied the effect of the different indel parameters (of the assumed indel model that generated the simulated data) on the performance. To this end, we divided the alignments into bins by their evolutionary parameters: the insertion and deletion rate parameters (<inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, respectively) and the parameters that determine the distribution of indel lengths (<inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for the insertion and deletion distributions, respectively). As expected, increasing the indel rate parameters <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> substantially decreases accuracy (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1a</xref>). The size distribution of the indels had little effect on accuracy (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1b</xref>).</p></sec><sec><title>3.3 Subspace learning</title><p>As stated above, we can train a transformer on a set of MSAs that share specific features, e.g., training them on MSAs with a high deletion rate and a low insertion rate. Deep learning models have a large number of free parameters, allowing learning complex patterns. In subspace learning, we optimize these free parameters again on a subset of the dataset. The starting weights of the parameters are the weights obtained for the entire range. Nevertheless, we note that as the architecture is fixed, the number of free parameters is fixed as well. To determine if such a subspace-learning approach increases accuracy, we simulated three nucleotide datasets of five sequences per sample (see Section 2.9). The first dataset, &#x0201c;general&#x0201d; (ND10), was simulated with a wide range of indel model parameters. The second dataset, &#x0201c;specific&#x0201d; (ND11), was simulated on a subspace of the indel model parameter space, i.e., the generated MSAs resemble each other in terms of indel dynamics. Finally, the third dataset, &#x0201c;ultra specific&#x0201d; (ND12), is even more restrictive in terms of the allowed indel dynamics (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>). Our results suggest that subspace learning can improve both coverage and accuracy (<xref rid="btaf009-F5" ref-type="fig">Fig.&#x000a0;5</xref>), with a more substantial effect on coverage. This highlights the importance of fitting the correct configuration of the alignment program (and in our case the training of the transformer) to the specific data. These results demonstrate that subspace learning has the potential to improve the accuracy of BetaAlign.</p><fig position="float" id="btaf009-F5"><label>Figure 5.</label><caption><p>
<bold>Effect of subspace learning on the CS-error (a) and the fraction of invalid alignments (b).</bold> The three transformers: &#x0201c;general&#x0201d;, &#x0201c;specific&#x0201d;, and &#x0201c;ultra specific&#x0201d; were trained on the &#x0201c;general&#x0201d;, &#x0201c;specific&#x0201d;, and &#x0201c;ultra specific&#x0201d;, datasets, respectively. The &#x0201c;ultra specific&#x0201d; dataset (ND12) parameters (e.g., the indel rates) are a subset of the &#x0201c;specific&#x0201d; dataset (ND11) parameters, which are a subset of the &#x0201c;general&#x0201d; dataset (ND10) parameters. The difference between the accuracy of &#x0201c;general&#x0201d; and &#x0201c;ultra specific&#x0201d; transformers on the &#x0201c;ultra specific&#x0201d; dataset is significant (paired <italic toggle="yes">t</italic>-test; <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>).</p></caption><graphic xlink:href="btaf009f5" position="float"/></fig></sec><sec><title>3.4 Embedding extraction for downstream tasks</title><p>Transformers are composed of two parts, the encoder and the decoder. The encoder creates high-dimensional vector representations of the source sentence, i.e., the unaligned sequences, which are passed to the decoder to create the translated sentence, i.e., the aligned sequences. This high-dimensional vector embeds the information in sequences as a numeric representation. We compressed this vector to a vector of a size that does not depend on the number of positions. In the case of <italic toggle="yes">n</italic> sequences, the dimension of the vector is <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mn>1024</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> (see Section 2.10). To exemplify the utility of such a representation, we used this vector representation as input for a different machine-learning task, which is to estimate for each MSA the length of the root sequence, from which the resulting sequences diverged. To this end, we trained a linear regression model that takes the coordinates of the compressed high-dimensional vector as input. The training set includes 90&#x000a0;000 nucleotide MSAs, each with five sequences (ND10). The accuracy of the linear-regression model using these features was evaluated on test data comprising 10&#x000a0;000 MSAs (<xref rid="btaf009-F6" ref-type="fig">Fig.&#x000a0;6</xref>). The significant correlation between the true and inferred root lengths [<inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.91</mml:mn></mml:math></inline-formula> and 2.003 base pairs mean squared error (MSE)] suggests that our approach can be used to compactly code sequences, as a preliminary step for downstream machine-learning tasks.</p><fig position="float" id="btaf009-F6"><label>Figure 6.</label><caption><p>
<bold>Results of the linear regressor trained to predict the root length from the embedding of the unaligned sequences, with an</bold> &#x000a0;<inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow><mml:mn mathvariant="bold">2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> &#x000a0;<bold>of</bold> &#x000a0;<inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mn mathvariant="bold">0</mml:mn><mml:mo mathvariant="bold">.</mml:mo><mml:mn mathvariant="bold">91</mml:mn></mml:math></inline-formula> &#x000a0;<bold>and</bold> &#x000a0;<inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mi mathvariant="bold">MSE</mml:mi></mml:math></inline-formula> &#x000a0;<bold>of 2.003 base pairs.</bold> The solid (orange) line is the regression line and the dashed (red) line reflects the <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi></mml:math></inline-formula> function. The embeddings are of the ND10 dataset sequences.</p></caption><graphic xlink:href="btaf009f6" position="float"/></fig></sec><sec><title>3.5 Transfer learning</title><p>Our approach heavily depends on transfer learning. Except for the first transformers, for which the weights were randomly initialized, all other transformers used initial weights that were optimized on a previous dataset. The transformers of the nucleotide datasets have a different path of training from the transformers of the amino acid datasets. In addition, each transformer is optimized based on the previous transformer with the same configuration (as we trained two different transformers for each dataset). To evaluate the contribution of transfer learning to performance, we tested three alternative scenarios (<xref rid="btaf009-F3" ref-type="fig">Fig.&#x000a0;3a</xref>, see Section 2.11). Briefly, the transformer in Scenario 1 (Transformer 1) is trained once on a target dataset. Transformer 2 (Scenario 2) started from the end point of Transformer 1 and was retrained on the same target dataset. Transformer 3 (Scenario 3) started from the end point of Transformer 1 and was trained on various other datasets, and then retrained on the same target dataset. Our results demonstrated the benefit of transfer learning (<xref rid="btaf009-F3" ref-type="fig">Fig.&#x000a0;3b</xref>). Transformer 3 outperformed Transformer 1, both for protein and DNA sequences, with error reduction of 37.3% and 33.3%, respectively (paired <italic toggle="yes">t</italic>-test; <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.005). It may be that the increased accuracy resulted from the fact that Transformer 3 was trained twice on the target dataset and not due to the additional training. To test this hypothesis, we compared it to Transformer 2. Our analysis suggests that some of the improved accuracy is indeed due to the extra training (comparing Transformers 1 and 2). Nevertheless, it also shows that transfer learning substantially contributes to performance (comparing Transformers 2 and 3), resulting in 16% and 25% error reductions for protein and DNA, respectively (paired <italic toggle="yes">t</italic>-test; <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.005).</p></sec><sec><title>3.6 Correlation of certainty and the alignment accuracy</title><p>We found a strong dependence between the alignment certainty and the CS-score (<xref rid="btaf009-F2" ref-type="fig">Fig.&#x000a0;2</xref>). As the certainty of alignments can be calculated by creating multiple alternative alignments for the same set of unaligned sequences (see Section 2.4), we could utilize this dependence to infer the most accurate alignment, similar to a previous approach (<xref rid="btaf009-B9" ref-type="bibr">Edgar 2022</xref>).</p><p>Having observed that the alignment with the highest (alignment) certainty is ranked higher than expected (among the set of alternative alignments from a specific dataset), we next directly compared performance between choosing the alignment alternative with the highest certainty and selecting the first alternative alignment. We tested this approach on 10-sequences data points (SND1 and SPD1) and observed a significant CS-error reduction of 9.8% and 20.9% for DNA and protein alignments, respectively (paired <italic toggle="yes">t</italic>-test; <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mi>p</mml:mi></mml:math></inline-formula> = 0.002).</p></sec><sec><title>3.7 Comparing performance</title><p>We compared the performance of BetaAlign after selecting the MSA with the highest certainty against other commonly used alignment programs, both for DNA and protein sequences (<xref rid="btaf009-F7" ref-type="fig">Fig.&#x000a0;7</xref>). For DNA sequences, regardless of the number of sequences analyzed, BetaAlign was the most accurate (paired <italic toggle="yes">t</italic>-test; <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>), with a minimal error reduction of 12.7%. The second most accurate alignment program was MUSCLE for 4&#x02013;7 sequences and PRANK for 8&#x02013;10 sequences. For 10 sequences, for example, BetaAlign had an 13.7% error reduction compared to PRANK (paired <italic toggle="yes">t</italic>-test; <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>) and similar results were obtained for other number of sequences. MAFFT, DIALIGN, and ClustalW had a significantly lower performance, with MAFFT outperforming the two other alignment programs. Notably, for protein sequences, BetaAlign was typically the second most accurate. For 10 sequences, the error reduction of PRANK was 5.1% relative to BetaAlign. We speculate that the higher accuracy of protein MSAs compared to DNA-based MSAs, which is observed across all methods, stems from the higher alphabet size of protein sequences, which makes it easier to find anchors to guide MSA inference.</p><fig position="float" id="btaf009-F7"><label>Figure 7.</label><caption><p>
<bold>Comparing the results of BetaAlign with different aligners on SND1 (a) and SPD1 (b).</bold> The <italic toggle="yes">y</italic>-axis represents the performance of sequence alignment programs. The lower the CS-error, the better the performance.</p></caption><graphic xlink:href="btaf009f7" position="float"/></fig></sec></sec><sec><title>4 Discussion</title><p>The weights that are learned by the encoder can be used as a starting point for other machine-learning tasks, i.e., the sequences are embedded as meaningful vectors that hold contextual information. In this work, we demonstrated using such embedding for predicting the length of ancestral sequences, without computing the MSA. A similar approach can be used for other machine-learning tasks, e.g., secondary structure prediction, predicting the stability of proteins, and ancestral sequence reconstruction. In NLP, transferring representations from one task to another is highly common, and encoder&#x02013;decoder models are commonly used for this purpose (<xref rid="btaf009-B20" ref-type="bibr">McCann <italic toggle="yes">et al.</italic> 2017</xref>).</p><p>There are limitations when using NLP approaches for sequence alignment, one of which arises from the maximum sequence length that can be inserted into an attention-based model. This limitation stems from computing attention matrices, in which the memory requirement increases quadratically with the total length of the input and output sequences. The data size that can be processed depends on multiple factors that dictate the size of the obtained alignment. These factors include the number of sequences, the root length, the sequence divergence, which is dictated by the edges of the phylogenetic tree, and the indel rates and indel-length distributions. In general, the memory limitation is computer-specific, and on the current GPUs that we have used, we could analyze 1024 tokens (e.g., &#x0223c;10 sequences of 100 aligned columns, see Section 2). To overcome this issue, we have developed a novel approach that involves splitting and merging the alignment while training the transformer on a slightly different task (<xref rid="btaf009-B6" ref-type="bibr">Dotan <italic toggle="yes">et al.</italic> 2023</xref>). It is possible to apply different techniques to increase the limit on the sizes of the sequences. For example, a different tokenization technique allows multiple amino acids or nucleotides to be considered as a single token, and thus reduces the number of tokens for the entire sequence (<xref rid="btaf009-B7" ref-type="bibr">Dotan <italic toggle="yes">et al.</italic> 2024</xref>). Another option would be to employ state-space models instead of transformers (<xref rid="btaf009-B10" ref-type="bibr">Gu <italic toggle="yes">et al.</italic> 2022</xref>).</p><p>Our proposed method introduces a paradigm shift: it redirects the focus from the traditionally labor-intensive task of developing new sequence aligners to the more manageable process of creating simulations that replicate the evolutionary dynamics observed in empirical data. This approach is particularly beneficial for incorporating additional types of evolutionary events. For example, developing an aligner capable of detecting inversions in unaligned sequences would be complex and likely increase the algorithm&#x02019;s complexity. In contrast, BetaAlign can be easily trained on simulated data that include inversions, enabling it to effectively align sequences which experience inversions.</p><p>Generating multiple alternative alignments can be important for various applications, including the inference of alignment reliability (<xref rid="btaf009-B26" ref-type="bibr">Sela <italic toggle="yes">et al.</italic> 2015</xref>). MergeAlign (<xref rid="btaf009-B5" ref-type="bibr">Collingridge and Kelly 2012</xref>) combines alternative alignments into a single consensus, offering a promising method for enhancing the accuracy and reliability of MSAs. Multiple alternative MSAs are also accounted for in Bayesian alignment strategies, such as Bali-Phy (<xref rid="btaf009-B25" ref-type="bibr">Redelings 2021</xref>). However, Bayesian methods rely on a predefined prior and stochastic evolutionary model to guide alignment calculations, while in BetaAlign, the stochastic method is used for generating the training data, and not for the alignment inference.</p><p>We have coupled the NLP domain and the MSA problem by using transformers that were originally designed for natural languages. Future improvements in the NLP field are likely to directly impact future alignment methodologies. We expect that transformers that are dedicated to the task of sequence alignment, together with other breakthroughs in machine learning, will lead to alignment algorithms that account for the specific grammar rules of each set of analyzed sequences and will substantially outperform existing aligners.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>btaf009_Supplementary_Data</label><media xlink:href="btaf009_supplementary_data.zip"/></supplementary-material></sec></body><back><sec><title>Author contributions</title><p>Edo Dotan (Conceptualization, Formal analysis, Investigation, Methodology, Software, Validation, Visualization [lead], Data curation, Writing&#x02014;original draft, Writing&#x02014;review &#x00026; editing [equal]), Elya Wygoda (Conceptualization, Formal analysis, Investigation, Methodology, Software, Writing&#x02014;review &#x00026; editing [supporting], Data curation [lead]), Noa Ecker (Conceptualization, Investigation, Methodology, Software [supporting], Writing&#x02014;review &#x00026; editing [equal]), Michael Alburquerque (Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Writing&#x02014;review &#x00026; editing [supporting]), Oren Avram (Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Resources, Software [supporting], Writing&#x02014;review &#x00026; editing [equal]), Yonatan Belinkov (Conceptualization, Formal analysis, Investigation, Methodology, Writing&#x02014;original draft [lead], Data curation, Funding acquisition, Resources, Software, Validation, Visualization, Writing&#x02014;review &#x00026; editing [supporting], Project administration, Supervision [equal]), and Tal Pupko (Conceptualization, Data curation, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Supervision, Validation, Visualization, Writing&#x02014;original draft, Writing&#x02014;review &#x00026; editing [lead], Software [supporting])</p></sec><sec><title>Supplementary data</title><p>
<xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p><p>Conflict of interest: None declared.</p></sec><sec><title>Funding</title><p>Y.B. and T.P. have received funding from the Israel Science Foundation (grants 448/20 and 2818/21, respectively). Y.B. was partly supported by an Azrieli Foundation Early Career Faculty Fellowship. E.D., E.W., N.E., and M.A. were supported by the Edmond J. Safra Center for Bioinformatics at Tel Aviv University Fellowship. T.P.&#x02019;s research is supported in part by the Edouard Seroussi Chair for Protein Nanobiotechnology, Tel Aviv University.</p></sec><ref-list id="ref1"><title>References</title><ref id="btaf009-B1"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ajawatanawong</surname>
<given-names>P</given-names>
</string-name>, <string-name><surname>Baldauf</surname><given-names>SL.</given-names></string-name></person-group> &#x000a0;<article-title>Evolution of protein indels in plants, animals and fungi</article-title>. <source>BMC Evol Biol</source> &#x000a0;<year>2013</year>;<volume>13</volume>:<fpage>140</fpage>.<pub-id pub-id-type="pmid">23826714</pub-id>
</mixed-citation></ref><ref id="btaf009-B2"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Avram</surname>
<given-names>O</given-names>
</string-name>, <string-name><surname>Durmus</surname><given-names>B</given-names></string-name>, <string-name><surname>Rakocz</surname><given-names>N</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>SLIViT: a general AI framework for accurate clinical-feature diagnosis from limited 3D medical-imaging data</article-title>. <source>Invest Ophthalmol Vis Sci</source> &#x000a0;<year>2024</year>;<volume>65</volume>:<fpage>1614</fpage>.</mixed-citation></ref><ref id="btaf009-B3"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Bahdanau</surname>
<given-names>D</given-names>
</string-name>, <string-name><surname>Cho</surname><given-names>K</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group> Neural machine translation by jointly learning to align and translate. In: International Conference on Learning Representations (ICLR), San Diego, CA, USA, 2015.</mixed-citation></ref><ref id="btaf009-B4"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chang</surname>
<given-names>J-M</given-names>
</string-name>, <string-name><surname>Di Tommaso</surname><given-names>P</given-names></string-name>, <string-name><surname>Notredame</surname><given-names>C.</given-names></string-name></person-group> &#x000a0;<article-title>TCS: a new multiple sequence alignment reliability measure to estimate alignment accuracy and improve phylogenetic tree reconstruction</article-title>. <source>Mol Biol Evol</source> &#x000a0;<year>2014</year>;<volume>31</volume>:<fpage>1625</fpage>&#x02013;<lpage>37</lpage>.<pub-id pub-id-type="pmid">24694831</pub-id>
</mixed-citation></ref><ref id="btaf009-B5"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Collingridge</surname>
<given-names>PW</given-names>
</string-name>, <string-name><surname>Kelly</surname><given-names>S.</given-names></string-name></person-group> &#x000a0;<article-title>MergeAlign: improving multiple sequence alignment performance by dynamic reconstruction of consensus multiple sequence alignments</article-title>. <source>BMC Bioinformatics</source> &#x000a0;<year>2012</year>;<volume>13</volume>:<fpage>117</fpage>.<pub-id pub-id-type="pmid">22646090</pub-id>
</mixed-citation></ref><ref id="btaf009-B6"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Dotan</surname>
<given-names>E</given-names>
</string-name>, <string-name><surname>Belinkov</surname><given-names>Y</given-names></string-name>, <string-name><surname>Avram</surname><given-names>O</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> Multiple sequence alignment as a sequence-to-sequence learning problem. In: International Conference on Learning Representations (ICLR), Kigali, Rwanda, 2023.</mixed-citation></ref><ref id="btaf009-B7"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dotan</surname>
<given-names>E</given-names>
</string-name>, <string-name><surname>Jaschek</surname><given-names>G</given-names></string-name>, <string-name><surname>Pupko</surname><given-names>T</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>Effect of tokenization on transformers for biological sequences</article-title>. <source>Bioinformatics</source> &#x000a0;<year>2024</year>;<volume>40</volume>:<fpage>4</fpage>:btae196.</mixed-citation></ref><ref id="btaf009-B8"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Edgar</surname>
<given-names>RC.</given-names>
</string-name>
</person-group> &#x000a0;<article-title>MUSCLE: multiple sequence alignment with high accuracy and high throughput</article-title>. <source>Nucleic Acids Res</source> &#x000a0;<year>2004</year>;<volume>32</volume>:<fpage>1792</fpage>&#x02013;<lpage>7</lpage>.<pub-id pub-id-type="pmid">15034147</pub-id>
</mixed-citation></ref><ref id="btaf009-B9"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Edgar</surname>
<given-names>RC.</given-names>
</string-name>
</person-group> &#x000a0;<article-title>Muscle5: high-accuracy alignment ensembles enable unbiased assessments of sequence homology and phylogeny</article-title>. <source>Nat Commun</source> &#x000a0;<year>2022</year>;<volume>13</volume>:<fpage>6968</fpage>.<pub-id pub-id-type="pmid">36379955</pub-id>
</mixed-citation></ref><ref id="btaf009-B10"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Gu</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Goel</surname><given-names>K</given-names></string-name>, <string-name><surname>R&#x000e9;</surname><given-names>C.</given-names></string-name></person-group> Efficiently modeling long sequences with structured state. In: <italic toggle="yes">International Conference on Learning Representations (ICLR)</italic>, Virtual Event, <year>2022</year>.</mixed-citation></ref><ref id="btaf009-B11"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Huerta-Cepas</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Serra</surname><given-names>F</given-names></string-name>, <string-name><surname>Bork</surname><given-names>P.</given-names></string-name></person-group> &#x000a0;<article-title>ETE 3: reconstruction, analysis, and visualization of phylogenomic data</article-title>. <source>Mol Biol Evol</source> &#x000a0;<year>2016</year>;<volume>33</volume>:<fpage>1635</fpage>&#x02013;<lpage>8</lpage>.<pub-id pub-id-type="pmid">26921390</pub-id>
</mixed-citation></ref><ref id="btaf009-B12"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Iantorno</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Gori</surname><given-names>K</given-names></string-name>, <string-name><surname>Goldman</surname><given-names>N</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<part-title>Who watches the watchmen? An appraisal of benchmarks for multiple sequence alignment</part-title>. <source>Methods Mol Biol</source>. <year>2014</year>;<volume>1079</volume>:<fpage>59</fpage>&#x02013;<lpage>73</lpage>.<pub-id pub-id-type="pmid">24170395</pub-id>
</mixed-citation></ref><ref id="btaf009-B13"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Katoh</surname>
<given-names>K</given-names>
</string-name>, <string-name><surname>Standley</surname><given-names>DM.</given-names></string-name></person-group> &#x000a0;<article-title>MAFFT multiple sequence alignment software version 7: improvements in performance and usability</article-title>. <source>Mol Biol Evol</source> &#x000a0;<year>2013</year>;<volume>30</volume>:<fpage>772</fpage>&#x02013;<lpage>80</lpage>.<pub-id pub-id-type="pmid">23329690</pub-id>
</mixed-citation></ref><ref id="btaf009-B14"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Larkin</surname>
<given-names>MA</given-names>
</string-name>, <string-name><surname>Blackshields</surname><given-names>G</given-names></string-name>, <string-name><surname>Brown</surname><given-names>NP</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>Clustal W and clustal X version 2.0</article-title>. <source>Bioinformatics</source> &#x000a0;<year>2007</year>;<volume>23</volume>:<fpage>2947</fpage>&#x02013;<lpage>8</lpage>.<pub-id pub-id-type="pmid">17846036</pub-id>
</mixed-citation></ref><ref id="btaf009-B16"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Loewenthal</surname>
<given-names>G</given-names>
</string-name>, <string-name><surname>Rapoport</surname><given-names>D</given-names></string-name>, <string-name><surname>Avram</surname><given-names>O</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>A probabilistic model for indel evolution: differentiating insertions from deletions</article-title>. <source>Mol Biol Evol</source> &#x000a0;<year>2021</year>;<volume>38</volume>:<fpage>5769</fpage>&#x02013;<lpage>81</lpage>.<pub-id pub-id-type="pmid">34469521</pub-id>
</mixed-citation></ref><ref id="btaf009-B17"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lopez</surname>
<given-names>PJ</given-names>
</string-name>, <string-name><surname>S&#x000e9;raphin</surname><given-names>B.</given-names></string-name></person-group> &#x000a0;<article-title>YIDB: the yeast intron DataBase</article-title>. <source>Nucleic Acids Res</source> &#x000a0;<year>2000</year>;<volume>28</volume>:<fpage>85</fpage>&#x02013;<lpage>6</lpage>.<pub-id pub-id-type="pmid">10592188</pub-id>
</mixed-citation></ref><ref id="btaf009-B18"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>L&#x000f6;ytynoja</surname>
<given-names>A.</given-names>
</string-name>
</person-group> &#x000a0;<article-title>Phylogeny-aware alignment with PRANK</article-title>. <source>Methods Mol Biol</source> &#x000a0;<year>2014</year>;<volume>1079</volume>:<fpage>155</fpage>&#x02013;<lpage>70</lpage>.<pub-id pub-id-type="pmid">24170401</pub-id>
</mixed-citation></ref><ref id="btaf009-B19"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>L&#x000f6;ytynoja</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Goldman</surname><given-names>N.</given-names></string-name></person-group> &#x000a0;<article-title>Phylogeny-aware gap placement prevents errors in sequence alignment and evolutionary analysis</article-title>. <source>Science</source> &#x000a0;<year>2008</year>;<volume>320</volume>:<fpage>1632</fpage>&#x02013;<lpage>5</lpage>.<pub-id pub-id-type="pmid">18566285</pub-id>
</mixed-citation></ref><ref id="btaf009-B20"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>McCann</surname>
<given-names>B</given-names>
</string-name>, <string-name><surname>Bradbury</surname><given-names>J</given-names></string-name>, <string-name><surname>Xiong</surname><given-names>C</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>Learned in translation: contextualized word vectors</article-title>. <source>Adv Neural Inf Process Syst</source> &#x000a0;<year>2017</year>;<volume>30</volume>:<fpage>6297</fpage>&#x02013;<lpage>308</lpage>.</mixed-citation></ref><ref id="btaf009-B21"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Morgenstern</surname>
<given-names>B.</given-names>
</string-name>
</person-group> &#x000a0;<article-title>DIALIGN: multiple DNA and protein sequence alignment at BiBiServ</article-title>. <source>Nucleic Acids Res</source> &#x000a0;<year>2004</year>;<volume>32</volume>:<fpage>W33</fpage>&#x02013;<lpage>6</lpage>.<pub-id pub-id-type="pmid">15215344</pub-id>
</mixed-citation></ref><ref id="btaf009-B22"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Needleman</surname>
<given-names>SB</given-names>
</string-name>, <string-name><surname>Wunsch</surname><given-names>CD.</given-names></string-name></person-group> &#x000a0;<article-title>A general method applicable to the search for similarities in the amino acid sequence of two proteins</article-title>. <source>J Mol Biol</source> &#x000a0;<year>1970</year>;<volume>48</volume>:<fpage>443</fpage>&#x02013;<lpage>53</lpage>.<pub-id pub-id-type="pmid">5420325</pub-id>
</mixed-citation></ref><ref id="btaf009-B23"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Ott</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Edunov</surname><given-names>S</given-names></string-name>, <string-name><surname>Baevski</surname><given-names>A</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> fairseq: a fast, extensible toolkit for sequence modeling. In: <italic toggle="yes">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Minneapolis, Minnesota</italic>. p.<fpage>48</fpage>&#x02013;<lpage>53</lpage>. Association for Computational Linguistics, <year>2019</year>. <ext-link xlink:href="https://aclanthology.org/N19-1000/" ext-link-type="uri">https://aclanthology.org/N19-1000/</ext-link></mixed-citation></ref><ref id="btaf009-B24"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pesole</surname>
<given-names>G</given-names>
</string-name>, <string-name><surname>Gissi</surname><given-names>C</given-names></string-name>, <string-name><surname>De Chirico</surname><given-names>A</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>Nucleotide substitution rate of mammalian mitochondrial genomes</article-title>. <source>J Mol Evol</source> &#x000a0;<year>1999</year>;<volume>48</volume>:<fpage>427</fpage>&#x02013;<lpage>34</lpage>.<pub-id pub-id-type="pmid">10079281</pub-id>
</mixed-citation></ref><ref id="btaf009-B25"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Redelings</surname>
<given-names>BD.</given-names>
</string-name>
</person-group> &#x000a0;<article-title>BAli-Phy version 3: model-based co-estimation of alignment and phylogeny</article-title>. <source>Bioinformatics</source> &#x000a0;<year>2021</year>;<volume>37</volume>:<fpage>3032</fpage>&#x02013;<lpage>4</lpage>.<pub-id pub-id-type="pmid">33677478</pub-id>
</mixed-citation></ref><ref id="btaf009-B26"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sela</surname>
<given-names>I</given-names>
</string-name>, <string-name><surname>Ashkenazy</surname><given-names>H</given-names></string-name>, <string-name><surname>Katoh</surname><given-names>K</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>GUIDANCE2: accurate detection of unreliable alignment regions accounting for the uncertainty of multiple parameters</article-title>. <source>Nucleic Acids Res</source> &#x000a0;<year>2015</year>;<volume>43</volume>:<fpage>W7</fpage>&#x02013;<lpage>14</lpage>.<pub-id pub-id-type="pmid">25883146</pub-id>
</mixed-citation></ref><ref id="btaf009-B27"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Shalumov</surname>
<given-names>V</given-names>
</string-name>, <string-name><surname>Haskey</surname><given-names>H.</given-names></string-name></person-group> HeRo: RoBERTa and Longformer Hebrew language models. arXiv, <ext-link xlink:href="https://arxiv.org/abs/2304.11077" ext-link-type="uri">https://arxiv.org/abs/2304.11077</ext-link>, <year>2023</year>, preprint: not peer reviewed.</mixed-citation></ref><ref id="btaf009-B28"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sutskever</surname>
<given-names>I</given-names>
</string-name>, <string-name><surname>Vinyals</surname><given-names>O</given-names></string-name>, <string-name><surname>Le</surname><given-names>QV.</given-names></string-name></person-group> &#x000a0;<article-title>Sequence to sequence learning with neural networks</article-title>. <source>Adv Neural Inf Process Syst</source> &#x000a0;<year>2014</year>;<volume>27</volume>:<fpage>3104</fpage>&#x02013;<lpage>12</lpage>.</mixed-citation></ref><ref id="btaf009-B29"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Szegedy</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Vanhoucke</surname><given-names>V</given-names></string-name>, <string-name><surname>Ioffe</surname><given-names>S</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> Rethinking the Inception Architecture for Computer Vision. In: <italic toggle="yes">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 2818&#x02013;2826</italic>. <year>2016</year>.</mixed-citation></ref><ref id="btaf009-B30"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Tan</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Sun</surname><given-names>F</given-names></string-name>, <string-name><surname>Kong</surname><given-names>T</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> A survey on deep transfer learning. In: <italic toggle="yes">Artificial Neural Networks and Machine Learning&#x02014;ICANN 2018, Rhodes, Greece,</italic> &#x000a0;<year>2018</year>.</mixed-citation></ref><ref id="btaf009-B31"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Vaswani</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Shazeer</surname><given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> Attention is all you need. In: <italic toggle="yes">31st Conference on Neural Information Processing Systems (NIPS), Long Beach, CA, USA</italic>, <year>2017</year>.</mixed-citation></ref><ref id="btaf009-B32"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Walker</surname>
<given-names>CR</given-names>
</string-name>, <string-name><surname>Scally</surname><given-names>A</given-names></string-name>, <string-name><surname>Maio</surname><given-names>ND</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>Short-range template switching in great ape genomes explored using pair hidden markov models</article-title>. <source>PLoS Genet</source> &#x000a0;<year>2021</year>;<volume>17</volume>:<fpage>e1009221</fpage>.<pub-id pub-id-type="pmid">33651813</pub-id>
</mixed-citation></ref><ref id="btaf009-B33"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>H-C</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>K</given-names></string-name>, <string-name><surname>Susko</surname><given-names>E</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>A class frequency mixture model that adjusts for site-specific amino acid frequencies and improves inference of protein phylogeny</article-title>. <source>BMC Evol Biol</source> &#x000a0;<year>2008</year>;<volume>8</volume>:<fpage>331</fpage>.<pub-id pub-id-type="pmid">19087270</pub-id>
</mixed-citation></ref><ref id="btaf009-B34"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Jiang</surname><given-names>T.</given-names></string-name></person-group> &#x000a0;<article-title>On the complexity of multiple sequence alignment</article-title>. <source>J Comput Biol</source> &#x000a0;<year>1994</year>;<volume>1</volume>:<fpage>337</fpage>&#x02013;<lpage>48</lpage>.<pub-id pub-id-type="pmid">8790475</pub-id>
</mixed-citation></ref><ref id="btaf009-B35"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Wolf</surname>
<given-names>T</given-names>
</string-name>, <string-name><surname>Debut</surname><given-names>L</given-names></string-name>, <string-name><surname>Sanh</surname><given-names>V</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> Transformers: state-of-the-art natural language processing. In: <italic toggle="yes">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</italic>. p.<fpage>38</fpage>&#x02013;<lpage>45, Virtual Event</lpage>, <year>2020</year>.</mixed-citation></ref><ref id="btaf009-B36"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wolf</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Madej</surname><given-names>T</given-names></string-name>, <string-name><surname>Babenko</surname><given-names>V</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>Long-term trends in evolution of indels in protein sequences</article-title>. <source>BMC Evol Biol</source> &#x000a0;<year>2007</year>;<volume>7</volume>:<fpage>19</fpage>.<pub-id pub-id-type="pmid">17298668</pub-id>
</mixed-citation></ref><ref id="btaf009-B37"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Yang</surname>
<given-names>Z.</given-names>
</string-name>
</person-group> &#x000a0;<article-title>Estimating the pattern of nucleotide substitution</article-title>. <source>J Mol Evol</source> &#x000a0;<year>1994</year>;<volume>39</volume>:<fpage>105</fpage>&#x02013;<lpage>11</lpage>.<pub-id pub-id-type="pmid">8064867</pub-id>
</mixed-citation></ref></ref-list></back></article>