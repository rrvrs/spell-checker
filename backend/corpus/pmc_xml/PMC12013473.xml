<!--

File produced by pipelineRunner package (for JATS 2 SCJATS with pipeline SCJATS)
At: 2025-05-22T15:12:31.972Z

Version        : 1.16.1
Last update    : 2024-08-27
Modified by    : dunnm

-->
<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Database (Oxford)</journal-id><journal-id journal-id-type="iso-abbrev">Database (Oxford)</journal-id><journal-id journal-id-type="publisher-id">databa</journal-id><journal-title-group><journal-title>Database: The Journal of Biological Databases and Curation</journal-title></journal-title-group><issn pub-type="epub">1758-0463</issn><publisher><publisher-name>Oxford University Press</publisher-name><publisher-loc>UK</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40261733</article-id><article-id pub-id-type="pmc">PMC12013473</article-id>
<article-id pub-id-type="doi">10.1093/database/baaf031</article-id><article-id pub-id-type="publisher-id">baaf031</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Article</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI00960</subject></subj-group></article-categories><title-group><article-title>CPDMS: a database system for crop physiological disorder management</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-1100-6953</contrib-id><name><surname>Oh</surname><given-names>Jae-Hyeon</given-names></name><aff>
<institution>National institute of Agricultural Sciences</institution>, Rural Development Administration, 370, Jeonju-si, Jeollabuk-do 54874, <country country="KR">Republic of Korea</country></aff></contrib><contrib contrib-type="author"><name><surname>Jeong</surname><given-names>Hwang-Weon</given-names></name><aff>
<institution>National institute of Agricultural Sciences</institution>, Rural Development Administration, 370, Jeonju-si, Jeollabuk-do 54874, <country country="KR">Republic of Korea</country></aff></contrib><contrib contrib-type="author"><name><surname>Ahn</surname><given-names>Il Pyung</given-names></name><aff>
<institution>National institute of Agricultural Sciences</institution>, Rural Development Administration, 370, Jeonju-si, Jeollabuk-do 54874, <country country="KR">Republic of Korea</country></aff></contrib><contrib contrib-type="author"><name><surname>Bae</surname><given-names>Seon-Hwa</given-names></name><aff>
<institution>National Institute of Horticultural and Herbal Science</institution>, Rural Development Administration, 100, Wanju-gun, Jeollabuk-do 55365, <country country="KR">Republic of Korea</country></aff></contrib><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Sung Mi</given-names></name><aff>
<institution>National institute of Agricultural Sciences</institution>, Rural Development Administration, 370, Jeonju-si, Jeollabuk-do 54874, <country country="KR">Republic of Korea</country></aff></contrib><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Eunhee</given-names></name><aff>
<institution>National institute of Agricultural Sciences</institution>, Rural Development Administration, 370, Jeonju-si, Jeollabuk-do 54874, <country country="KR">Republic of Korea</country></aff></contrib><contrib contrib-type="author"><name><surname>Ra</surname><given-names>Su Jung</given-names></name><aff>
<institution>National institute of Agricultural Sciences</institution>, Rural Development Administration, 370, Jeonju-si, Jeollabuk-do 54874, <country country="KR">Republic of Korea</country></aff></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Jinjeong</given-names></name><aff>
<institution>National institute of Agricultural Sciences</institution>, Rural Development Administration, 370, Jeonju-si, Jeollabuk-do 54874, <country country="KR">Republic of Korea</country></aff></contrib><contrib contrib-type="author"><name><surname>Choi</surname><given-names>Hye Yeon</given-names></name><aff>
<institution>Phyzen Genomics Institute, 605, Baekgung Plaza 1st Building</institution>, 13, Seongnam-daero 331-gil, Bundang-gu, Seongnam-si, Gyeonggi-do 13558, <country country="KR">Republic of Korea</country></aff></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Seol</surname><given-names>Young-Joo</given-names></name><!--jhoh8288@korea.kr--><aff>
<institution>Technology Cooperation Bureau</institution>, Rural Development Administration, 300, Jeonju-si, Jeollabuk-do 54874, <country country="KR">Republic of Korea</country></aff><xref rid="COR0001" ref-type="corresp"/></contrib></contrib-group><author-notes><corresp id="COR0001">*Corresponding author. Technology Cooperation Bureau, Rural Development Administration, 300, Jeonju-si, Jeollabuk-do 54874, Republic of Korea. E-mail: <email xlink:href="jhoh8288@korea.kr">jhoh8288@korea.kr</email></corresp></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub" iso-8601-date="2025-04-22"><day>22</day><month>4</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>22</day><month>4</month><year>2025</year></pub-date><volume>2025</volume><elocation-id>baaf031</elocation-id><history><date date-type="received"><day>10</day><month>10</month><year>2024</year></date><date date-type="rev-recd"><day>17</day><month>3</month><year>2025</year></date><date date-type="accepted"><day>01</day><month>4</month><year>2025</year></date><date date-type="editorial-decision"><day>29</day><month>3</month><year>2025</year></date><date date-type="corrected-typeset"><day>22</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025. Published by Oxford University Press.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="baaf031.pdf"/><abstract><title>Abstract</title><p>As the importance of precision agriculture grows, scalable and efficient methods for real-time data collection and analysis have become essential. In this study, we developed a system to collect real-time crop images, focusing on physiological disorders in tomatoes. This system systematically collects crop images and related data, with the potential to evolve into a valuable tool for researchers and agricultural practitioners. A total of 58&#x02009;479 images were produced under stress conditions, including bacterial wilt (BW), <italic toggle="yes">Tomato Yellow Leaf Curl Virus</italic> (TYLCV), <italic toggle="yes">Tomato Spotted Wilt Virus</italic> (TSWV), drought, and salinity, across seven tomato varieties. The images include front views at 0 degrees, 120 degrees, 240 degrees, and top views and petiole images. Of these, 43&#x02009;894 images were suitable for labeling. Based on this, 24&#x02009;000 images were used for AI model training, and 13&#x02009;037 images for model testing. By training a deep learning model, we achieved a mean Average Precision (mAP) of 0.46 and a recall rate of 0.60. Additionally, we discussed data augmentation and hyperparameter tuning strategies to improve AI model performance and explored the potential for generalizing the system across various agricultural environments. The database constructed in this study will serve as a crucial resource for the future development of agricultural AI.</p><p>
<bold>Database URL</bold>: <ext-link xlink:href="https://crops.phyzen.com/" ext-link-type="uri">https://crops.phyzen.com/</ext-link></p></abstract><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>National Institute of Agricultural Sciences</institution><institution-id institution-id-type="DOI">10.13039/501100007270</institution-id></institution-wrap>
</funding-source><award-id>PJ01740601</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Rural Development Administration, Republic of Korea</institution></institution-wrap>
</funding-source></award-group></funding-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>National Institute of Agricultural Sciences</institution><institution-id institution-id-type="DOI">10.13039/501100007270</institution-id></institution-wrap>
</funding-source><award-id>PJ01740601</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Rural Development Administration, Republic of Korea</institution></institution-wrap>
</funding-source></award-group></funding-group><counts><page-count count="9"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>The agricultural sector is undergoing a transformation driven by the integration of digital technologies, particularly artificial intelligence (AI), machine learning, and big data analytics [<xref rid="R1" ref-type="bibr">1</xref>]. As global food demand continues to rise, there is an increasing need for precise and efficient crop management methods that can enhance productivity while minimizing resource use [<xref rid="R2" ref-type="bibr">2</xref>]. Traditional crop disease diagnosis methods have primarily relied on manual inspections and expert knowledge, which are time-consuming and prone to human error and variability [<xref rid="R3" ref-type="bibr">3</xref>]. These limitations are particularly problematic in large-scale agricultural operations, where timely and accurate disease detection is critical to maintaining crop health and maximizing yield [<xref rid="R4" ref-type="bibr">4</xref>].</p><p>Recent advancements in AI, particularly deep learning, have demonstrated the potential for automating plant disease diagnosis through image analysis [<xref rid="R5" ref-type="bibr">5</xref>]. For instance, Mohanty <italic toggle="yes">et&#x000a0;al</italic>. (2016) [<xref rid="R6" ref-type="bibr">6</xref>] demonstrated that deep learning models can achieve expert-level accuracy in identifying plant diseases from images. However, the success of these AI models heavily depends on the availability of large annotated datasets that include diverse crop types, disease conditions, and environmental variables [<xref rid="R7" ref-type="bibr">7</xref>]. Smartphones equipped with high-resolution cameras and internet connectivity offer an unparalleled opportunity to collect vast datasets directly from agricultural fields. These devices can be essential tools for real-time data collection and systematic management in the field. When combined with robust database systems and sophisticated AI algorithms, real-time data collection via smartphones can significantly enhance the scalability and accessibility of precision agriculture.</p><p>This study presents the development of a smartphone-based crop image collection, annotation, and management system, focusing on physiological disorders such as BW, TYLCV, TSWV, drought, and salinity in tomatoes. This system integrates advanced database management with AI-based analysis to support both research and practical applications. By building a large-scale annotated image database, this study aims to improve crop management practices and contribute to the advancement of digital agriculture.</p><p>However, several challenges remain in applying AI to agriculture. One of the major issues is the variability of environmental conditions, which can significantly affect the appearance of plant diseases and the performance of AI models trained in controlled environments [<xref rid="R8" ref-type="bibr">8</xref>]. To address these challenges, this study focuses on developing AI-based diagnostic tools that leverage diverse and extensive datasets, ensuring adaptability across various agricultural environments. The ultimate goal of this research is to provide a scalable and practical solution for improving crop disease diagnosis, contributing to enhanced agricultural productivity and sustainability.</p></sec><sec id="s2"><title>Materials and methods</title><sec id="s2-s1"><title>Plant materials and pathogen/stress treatments</title><p>The tomato varieties used in the study were Hawaii7996, Seogwang, AVTO0101, Super Doterang, Tori, Lucky, and Ten-Ten, making a total of seven varieties. These varieties were subjected to infection by BW, TYLCV, and TSWV. For BW infection, the <italic toggle="yes">Ralstonia solanacearum</italic> strain (18&#x02009;644, WR-1) obtained from the Korean Agricultural Culture Collection (KACC) of the Rural Development Administration was used to inoculate the tomato plants. TYLCV and TSWV infections were induced using mechanical inoculation with <italic toggle="yes">Agrobacterium tumefaciens</italic>, and the infected plants were maintained under controlled laboratory conditions (<xref rid="T1" ref-type="table">Table&#x000a0;1</xref>).</p><table-wrap position="float" id="T1"><label>Table&#x000a0;1.</label><caption><p>Information on tomato disease symptoms</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th valign="bottom" align="left" rowspan="1" colspan="1">Name of the disease</th><th valign="bottom" align="left" rowspan="1" colspan="1">Pathogen nucleic acid type</th><th valign="bottom" align="left" rowspan="1" colspan="1">Pathogen</th><th valign="bottom" align="left" rowspan="1" colspan="1">Specific amplification region for each pathogen</th><th valign="bottom" align="left" rowspan="1" colspan="1">Primer information</th></tr></thead><tbody><tr><td rowspan="2" align="left" colspan="1">Bacterial Wilt</td><td rowspan="4" align="left" colspan="1">DNA</td><td rowspan="2" align="left" colspan="1">
<italic toggle="yes">Ralstonia solanacearum</italic> Species Complex (RSSC)</td><td align="left" rowspan="1" colspan="1">lpxC(RSc2836 or RS06240)</td><td align="left" rowspan="1" colspan="1">F: 5&#x002b9;-ATGTCGAGCGGTAGCATC-3&#x02019;</td></tr><tr><td align="left" rowspan="1" colspan="1">intergenic region upstream of RSc2836</td><td align="left" rowspan="1" colspan="1">R: 5&#x002b9;-TGCGATCGTCGATCGATC-3&#x002b9;<break/><bold>*</bold> Product size (bp) : 150</td></tr><tr><td rowspan="2" align="left" colspan="1">TYLCV disease</td><td rowspan="2" align="left" colspan="1">TYLCV</td><td align="left" rowspan="1" colspan="1">V1 protein gene</td><td align="left" rowspan="1" colspan="1">F: 5&#x002b9;-CGATCGATCGATCGATCG-3&#x02019;</td></tr><tr><td align="left" rowspan="1" colspan="1">TYLCV coat protein gene region</td><td align="left" rowspan="1" colspan="1">R: 5&#x002b9;-GCTAGCTAGCTAGCTAGC-3&#x002b9;<break/><bold>*</bold> Product size (bp) : 200</td></tr><tr><td rowspan="2" align="left" colspan="1">TSWV disease</td><td rowspan="2" align="left" colspan="1">RNA</td><td rowspan="2" align="left" colspan="1">TSWV</td><td align="left" rowspan="1" colspan="1">NSm</td><td align="left" rowspan="1" colspan="1">F: 5&#x002b9;-CAGCAACACGGTCAAGACAA-3&#x02019;</td></tr><tr><td align="left" rowspan="1" colspan="1">The movement protein gene, involved in the movement of TSWV</td><td align="left" rowspan="1" colspan="1">R: 5&#x002b9;-TGGTCCAGGTGTTGATGATG-3&#x002b9;<break/><bold>*</bold> Product size (bp) : 120</td></tr></tbody></table></table-wrap></sec><sec id="s2-s2"><title>Plant growth and management practices</title><p>Plants were grown in 15&#x02009;cm diameter pots filled with a mixture of peat and perlite in a 3:1 ratio. Each cultivar was represented by 15 plants (three biological replicates, five plants per replicate), and all plants were maintained under controlled environmental conditions: temperature was maintained at 25&#x000b0;C&#x02009;&#x000b1;&#x02009;2&#x000b0;C, relative humidity at 60%&#x02009;&#x000b1;&#x02009;5%, and light intensity at 200&#x02009;&#x000b5;mol/m<sup>2</sup>/s with a 16-h/8-h light/dark cycle. A balanced NPK fertilizer was applied weekly to ensure optimal growth conditions. Experiments were conducted in a controlled environment chamber with consistent ventilation and CO<sub>2</sub> levels.</p></sec><sec id="s2-s3"><title>Disease inoculation and image acquisition</title><p>For disease inoculation, plants were infected at the four-leaf stage. Images were acquired 7&#x02009;days post-inoculation to capture disease progression. For drought and salinity treatments, plants were subjected to stress at the six-leaf stage. Drought stress was induced by water deprivation, while salinity stress was applied using a 200&#x02009;mM NaCl solution. Both treatments lasted for 14&#x02009;days, and images were acquired at the end of the treatment period.</p><p>In addition, drought stress (water deprivation) and salinity stress (200&#x02009;mM NaCl treatment) were applied to the seven tomato varieties, starting from 28&#x02009;days after germination. The resulting stress responses were captured as image data.</p></sec><sec id="s2-s4"><title>Data collection and annotation</title><p>Images of each tomato variety were collected at various growth stages and positions (front views at 0&#x000b0;, 120&#x000b0;, 240&#x000b0;, top view, and petiole views of upper and lower leaves) under conditions of BW, TYLCV, and TSWV infection, as well as environmental stress (salinity and drought) (<xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref>). The collected images were annotated with information about the specific plant parts affected by physiological disorders and the severity of the symptoms. The annotations were performed manually using the LabelImg tool (<ext-link xlink:href="https://github.com/tzutalin/labelImg" ext-link-type="uri">https://github.com/tzutalin/labelImg</ext-link>), an open-source image annotation tool. Each image was annotated for disease symptoms, stress types, and plant growth stages. The annotated data were stored in a relational database for systematic management. This high-quality dataset served as the foundation for AI model training.</p><fig position="float" id="F1" fig-type="figure"><label>Figure&#x000a0;1.</label><caption><p>Collection of healthy and diseased plant images from various angles. (a). Front view image (0&#x000b0;), (b) front view image (120&#x000b0;), (c) front view image (240&#x000b0;), (d) top view image, (e) petiole image (upper leaf), (f) petiole image (lower leaf).</p></caption><graphic xlink:href="baaf031f1" position="float"/></fig></sec><sec id="s2-s5"><title>System development</title><p>The system developed in this study is a web-based platform that provides a responsive web application optimized for both Android and Apple devices. This system is designed to allow users to access it through a mobile web browser without the need to install a separate app. Instead of using React Native [<xref rid="R9" ref-type="bibr">9</xref>], HTML5 [<xref rid="R10" ref-type="bibr">10</xref>], and CSS3 [<xref rid="R11" ref-type="bibr">11</xref>] were employed to maximize compatibility and responsiveness across various browser environments. The system integrates data collection and database management functions to ensure efficient operation, with the collected data being securely stored in a central database in real time (<xref rid="F2" ref-type="fig">Fig.&#x000a0;2</xref>). Currently, the annotation process is handled using an external tool, with future plans to fully integrate this feature into the system.</p><fig position="float" id="F2" fig-type="figure"><label>Figure&#x000a0;2.</label><caption><p>A snapshot of the CPDMS web database. The website is organized into an introduction and a physiological disorder collecting flow chart.</p></caption><graphic xlink:href="baaf031f2" position="float"/></fig></sec><sec id="s2-s6"><title>Database design and integration</title><p>To efficiently manage the large volume of collected image data, a relational database structure was adopted. This database is managed on an on-premises server, providing stable access and regular updates, allowing users to quickly search and access data based on specific conditions. The database is designed with compatibility for deep learning frameworks such as TensorFlow [<xref rid="R12" ref-type="bibr">12</xref>] and PyTorch [<xref rid="R13" ref-type="bibr">13</xref>], enabling annotated datasets to be exported in formats suitable for use in these frameworks.</p></sec><sec id="s2-s7"><title>AI model training and validation</title><p>To diagnose tomato diseases, the YOLOv5 [<xref rid="R14" ref-type="bibr">14</xref>] framework was used to train a deep learning model. This framework was chosen for its efficiency in real-time object detection. The dataset was divided into training (70%), validation (20%), and testing (10%) sets, with a total of 24&#x02009;000 images used for training, 6857 for validation, and 13&#x02009;037 for testing. The images were collected under various environmental conditions, which refer to different stress conditions (biotic and abiotic) applied within the same controlled laboratory environment. For example, drought, salinity stress, and pathogen infection (BW, TYLCV, and TSWV) were considered different environmental conditions. The model&#x02019;s performance was evaluated using metrics such as precision, recall, and mean Average Precision (mAP).</p><p>To enhance the model&#x02019;s performance, data augmentation techniques (e.g. random rotation, zoom, and lighting adjustments) were applied, and hyperparameter tuning was conducted to find the optimal model configuration. In the future, the model will be expanded to generalize across various environments and crops, with plans for additional data collection and model retraining.</p></sec><sec id="s2-s8"><title>Pathogen quantification using qRT-PCR</title><p>Quantitative reverse transcription polymerase chain reaction (qRT-PCR) was used to quantitatively assess pathogen proliferation. Infected tissues were collected from susceptible varieties (AVTO0101, Hawaii7996, Seogwang) and resistant varieties (Lucky, Tori) infected with Bacterial Wilt (BW), followed by RNA extraction and conversion to cDNA for qRT-PCR analysis. Each experiment was performed with three biological replicates, and samples were collected from five plants per replicate. The primer sequences used for qRT-PCR are listed in <xref rid="T1" ref-type="table">Table&#x000a0;1</xref>. This method enabled the analysis of the correlation between visible disease symptoms and actual pathogen proliferation, allowing for a quantitative evaluation of disease progression. The same method was applied to plants infected with TYLCV, where RNA was extracted and viral proliferation was quantified through qRT-PCR.</p></sec></sec><sec id="s3"><title>Results</title><sec id="s3-s1"><title>Field performance of the web application</title><p>The developed web-based application was tested under various environmental conditions, allowing users to download images and annotate them in the web environment. The application provides a user-friendly interface and responsive performance, enabling agricultural workers to collect data in real time. Laboratory tests demonstrated that the system performed reliably across different stress conditions and crop growth stages within a controlled environment, allowing users to consistently capture disease symptoms (<xref rid="F3" ref-type="fig">Fig.&#x000a0;3</xref>).</p><fig position="float" id="F3" fig-type="figure"><label>Figure&#x000a0;3.</label><caption><p>Application of physiological disorder collecting images using a smartphone. (a) Application main screen snap. (b) Major functions of application. (c) QR code-based management of object meta-information. (D) Upload of image. (E) My account includes user information and collected information such as image.</p></caption><graphic xlink:href="baaf031f3" position="float"/></fig></sec><sec id="s3-s2"><title>Image collection and annotation</title><p>Each variety of tomato was observed at various growth stages following infection with BW, TYLCV, and TSWV, with 10&#x02009;732 images, 22&#x02009;343 images, and 3027 images collected, respectively. Additionally, each tomato variety was exposed to salinity and drought stress, with a total of 22&#x02009;377 images collected at various growth stages (<xref rid="T2" ref-type="table">Table&#x000a0;2</xref>). Detailed annotations of the collected images enabled the identification of affected plant parts and the severity of physiological disorders (<xref rid="F4" ref-type="fig">Fig.&#x000a0;4</xref>). The systematic organization of these data in a relational database facilitated efficient training of AI models, as detailed in <xref rid="s6" ref-type="sec">Supplementary Fig. S1</xref>.
</p><fig position="float" id="F4" fig-type="figure"><label>Figure&#x000a0;4.</label><caption><p>Plant disease symptom image annotation. (a) FW(Flower), (b) LF(Leaf), (c) TOP(Top view), (d) FR(Fruit), and (e) SV(Side view).</p></caption><graphic xlink:href="baaf031f4" position="float"/></fig><table-wrap position="float" id="T2"><label>Table&#x000a0;2.</label><caption><p>Number of images collected for each tomato variety under different conditions</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th valign="bottom" align="left" rowspan="1" colspan="1">Disease symptom</th><th valign="bottom" align="left" rowspan="1" colspan="1">BW</th><th valign="bottom" align="left" rowspan="1" colspan="1">TYLCV</th><th valign="bottom" align="left" rowspan="1" colspan="1">TSWV</th><th valign="bottom" align="left" rowspan="1" colspan="1">Physiological disorder</th><th valign="bottom" align="left" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Image yield</td><td align="left" rowspan="1" colspan="1">10&#x02009;732</td><td align="left" rowspan="1" colspan="1">22&#x02009;343</td><td align="left" rowspan="1" colspan="1">3027</td><td align="left" rowspan="1" colspan="1">22&#x02009;377</td><td align="left" rowspan="1" colspan="1">58&#x02009;479</td></tr></tbody></table></table-wrap></sec><sec id="s3-s3"><title>Database functionality and accessibility</title><p>The relational database efficiently manages large-scale image data and is designed to allow users to quickly filter and access specific data (<xref rid="F5" ref-type="fig">Fig.&#x000a0;5</xref>). The database is compatible with TensorFlow and PyTorch, enabling the export of annotated datasets for AI model training. The current system is operated on an on-premises server, supporting stable data access and regular updates, which enhances research productivity.</p><fig position="float" id="F5" fig-type="figure"><label>Figure&#x000a0;5.</label><caption><p>Physiological disorder image database system. (a) Main screen interface of crop physiological disorder image management system (b) Crop stress and crop information included in the informational list category (c) Project creation list. (d) The database can be aligned in a list form of images taken by the smartphone.</p></caption><graphic xlink:href="baaf031f5" position="float"/></fig></sec><sec id="s3-s4"><title>AI model accuracy and utility</title><p>To develop the AI training model, the YOLOv5 framework was employed to detect and classify physiological disorders in tomatoes. Data folders were organized, and training parameters were configured before initiating the model training process. The trained model achieved a mAP of 0.46 and a recall of 0.60 on the test dataset (<xref rid="F6" ref-type="fig">Fig.&#x000a0;6</xref>), demonstrating reliable detection accuracy across various disease conditions (<xref rid="s6" ref-type="sec">Supplementary Fig S2</xref>). The model&#x02019;s performance was assessed across different stages of TYLCV, TSWV, and BW (Bacterial Wilt). For BW, early stages were challenging to distinguish from healthy plants due to similar phenotypic characteristics, resulting in lower recognition accuracy. However, for TYLCV and TSWV, recognition rates improved as the diseases progressed, achieving an overall accuracy exceeding 95% (<xref rid="s6" ref-type="sec">Supplementary Fig S2</xref>). Additionally, the model maintained consistent performance under common stress conditions, such as drought and salinity, highlighting its robustness across diverse agricultural environments.</p><fig position="float" id="F6" fig-type="figure"><label>Figure&#x000a0;6.</label><caption><p>Artificial intelligence training process.</p></caption><graphic xlink:href="baaf031f6" position="float"/></fig></sec><sec id="s3-s5"><title>Pathogen quantification using qRT-PCR</title><p>qRT-PCR analysis revealed significant differences in pathogen proliferation between susceptible and resistant varieties. In susceptible varieties infected with BW, pathogen proliferation increased rapidly, with Hawaii7996 showing the lowest proliferation rate, indicating high resistance to the pathogen. In contrast, pathogen proliferation was limited in resistant varieties (<xref rid="F7" ref-type="fig">Fig.&#x000a0;7</xref>). In the TYLCV and TSWV infection experiments, viral proliferation was active in susceptible varieties, whereas it was suppressed in resistant varieties (<xref rid="s6" ref-type="sec">Supplementary Fig S3 and S4</xref>). These results clearly demonstrate the correlation between visible disease symptoms and actual pathogen proliferation.</p><fig position="float" id="F7" fig-type="figure"><label>Figure&#x000a0;7.</label><caption><p>Correlation between <italic toggle="yes">Ralstonia solanacearum</italic> cell count and CP value, and pathogen content in resistant and susceptible tomato tissues. (a) Correlation between the number of <italic toggle="yes">R. solanacearum</italic> (Bacterial Wilt pathogen) cells and CP value. (b) Measurement of Bacterial Wilt pathogen content in infected tissues of resistant and susceptible tomato plants.</p></caption><graphic xlink:href="baaf031f7" position="float"/></fig></sec></sec><sec id="s4"><title>Discussion</title><p>This study developed a smartphone-based crop image collection and management system, providing an efficient and scalable solution for real-time data collection and management in agricultural fields. The system is designed to rapidly diagnose major diseases, including physiological disorders in tomatoes, across various environments, and store the data in a central database for future analysis and research. Notably, through the web-based platform, users could easily access the system via mobile web browsers without installing an application, and the data were securely stored in real-time in the central database. This approach significantly enhanced user accessibility, increasing the system&#x02019;s practicality and boosting the efficiency of field operations.</p><p>The deep learning model trained using the YOLOv5 framework produced reliable results under various disease conditions, with particularly high accuracy for specific diseases. However, the relatively low mAP suggests that further improvements are needed through additional data augmentation and hyperparameter tuning. Future research aims to enhance the model&#x02019;s generalization performance by collecting data from more diverse environments and crops, ultimately achieving higher accuracy and recall rates.</p><p>While significant progress has been made, several challenges persist in applying AI to agriculture. One critical issue is the variability of environmental conditions, which can influence AI model performance and complicate accurate disease diagnosis. For instance, symptoms of the same disease may vary depending on environmental factors, necessitating model training strategies that account for such variables. This study demonstrates that collecting data from diverse environments and incorporating these variables into model training significantly improves diagnostic accuracy. Furthermore, ensuring user accessibility, enhancing data security, and addressing the long-term sustainability of AI systems are essential to facilitate practical adoption in the field. The development of intuitive user interfaces and system stability was also identified as key factors for improving user experience and operational efficiency.</p><p>The system developed in this study focuses on specific crops (tomatoes) and diseases (physiological disorders), and its applicability to other crops and diseases may be limited. Therefore, future research should aim to develop an expanded system that includes data from a variety of crops and diseases. Additionally, advanced techniques such as transfer learning or ensemble learning could be applied to further improve AI model performance. In conclusion, this study aims to bridge the gap between cutting-edge AI technology and practical agricultural applications by providing a scalable, efficient, and accessible crop management solution. Through the development of a comprehensive smartphone-based system, this study seeks to enhance the accuracy and effectiveness of crop disease diagnosis, ultimately contributing to improved agricultural productivity and sustainability. This study has laid an important foundation for the advancement of precision agriculture, opening up the potential for better agricultural management solutions through the integration of AI and digital technologies. With continuous research and system improvements, this system has the potential to become a powerful tool for enhancing agricultural productivity and promoting sustainable farming practices.</p></sec><sec id="s5"><title>Conclusions</title><p>The smartphone-based crop image collection and management system developed in this study is a web-based platform accessible through a mobile web browser, providing optimized displays for both Android and Apple devices. This system allows agricultural workers to efficiently collect high-quality crop images in real time from the field, with the collected data securely stored and managed in a central database. Additionally, the system integrates advanced database management with AI-based analysis functions to support both research and practical applications, with plans to incorporate AI analysis functions in the future. Laboratory tests have demonstrated that the system performs reliably and consistently across diverse stress conditions, and users highly value its intuitive interface and dependable performance. In particular, deep learning-based AI analysis using the YOLOv5 model has produced reliable results in detecting and classifying physiological disorders in tomatoes. This system is poised to become a powerful tool for advancing digital agriculture and supporting sustainable farming practices, with scalability for application to various crops and environments in the future.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>baaf031_Supp</label><media xlink:href="baaf031_supp.zip"/></supplementary-material></sec></body><back><ack id="ack1"><title>Acknowledgements</title><p>None declared.</p></ack><sec id="s6"><title>Supplementary data</title><p>
<xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> is available at Database online.</p></sec><sec sec-type="COI-statement" id="s7"><title>Conflict of interest:</title><p>None declared.</p></sec><sec id="s8"><title>Funding</title><p>This work was supported by a grant from the National Institute of Agricultural Sciences Program (ProjectNo. PJ01600101 &#x00026; PJ01740601), Rural Development Administration, Republic of Korea.</p></sec><sec sec-type="data-availability" id="s9"><title>Data availability</title><p>All data used in this study are publicly available at <ext-link xlink:href="https://crops.phyzen.com/" ext-link-type="uri">https://crops.phyzen.com/</ext-link> and <ext-link xlink:href="https://crops.phyzen.com/app" ext-link-type="uri">https://crops.phyzen.com/app</ext-link></p></sec><sec id="s10"><title>Author contributions</title><p>J.H.O. was responsible for the conceptualization of the study.</p><p>Y.J.S. developed the methodology for the research.</p><p>Y.J.S., H.W.J., and H.Y.C. contributed to the software development.</p><p>I.P.A. and S.H.B. conducted formal analysis.</p><p>J.L., E.K., S.M.K., and S.J.R. were involved in the investigation process.</p><p>J.L., E.K., S.M.K., and S.J.R. provided necessary resources for the study.</p><p>J.H.O. and S.H.B. curated the data used in the research; took the lead in preparing the original draft of the manuscript.</p><p>J.H.O. participated in the writing, review, editing of the paper, and secured funding for the study.</p><p>J.H.O. and Y.J.S. provided supervision throughout the study.</p><p>J.H.O. handled project administration.</p><p>All authors have reviewed and agreed upon the final version of the manuscript.</p></sec><ref-list id="ref1"><title>References</title><ref id="R1"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wolfert</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Ge</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Verdouw</surname> &#x000a0;<given-names>C</given-names></string-name></person-group>, <etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Big data in smart farming &#x02013; a review</article-title>. <source><italic toggle="yes">Agric Syst</italic></source> &#x000a0;<year>2017</year>;<volume>153</volume>:<fpage>69</fpage>&#x02013;<lpage>80</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.agsy.2017.01.023</pub-id></mixed-citation></ref><ref id="R2"><label>2.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ray</surname> &#x000a0;<given-names>DK</given-names></string-name>, <string-name><surname>Mueller</surname> &#x000a0;<given-names>ND</given-names></string-name>, <string-name><surname>West</surname> &#x000a0;<given-names>PC</given-names></string-name></person-group>, <etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Yield trends are insufficient to double global crop production by 2050</article-title>. <source><italic toggle="yes">PLOS ONE</italic></source> &#x000a0;<year>2013</year>;<volume>8</volume>:<page-range>e66428</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0066428</pub-id></mixed-citation></ref><ref id="R3"><label>3.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sankaran</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Mishra</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Ehsani</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>, <etal>et&#x000a0;al.</etal> &#x000a0;<article-title>A review of advanced techniques for detecting plant diseases</article-title>. <source><italic toggle="yes">Comput Electron Agric</italic></source> &#x000a0;<year>2010</year>;<volume>72</volume>:<fpage>1</fpage>&#x02013;<lpage>13</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.compag.2010.02.007</pub-id></mixed-citation></ref><ref id="R4"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mahlein</surname> &#x000a0;<given-names>AK</given-names></string-name>
</person-group> &#x000a0;<article-title>Plant disease detection by imaging sensors &#x02013; parallels and specific demands for precision agriculture and plant phenotyping</article-title>. <source><italic toggle="yes">Plant Dis</italic></source> &#x000a0;<year>2016</year>;<volume>100</volume>:<fpage>241</fpage>&#x02013;<lpage>51</lpage>. doi: <pub-id pub-id-type="doi">10.1094/PDIS-03-15-0340-FE</pub-id><pub-id pub-id-type="pmid">30694129</pub-id>
</mixed-citation></ref><ref id="R5"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>LeCun</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Bengio</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Hinton</surname> &#x000a0;<given-names>G</given-names></string-name></person-group> &#x000a0;<article-title>Deep learning</article-title>. <source><italic toggle="yes">Nature</italic></source> &#x000a0;<year>2015</year>;<volume>521</volume>:<fpage>436</fpage>&#x02013;<lpage>44</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id>
</mixed-citation></ref><ref id="R6"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mohanty</surname> &#x000a0;<given-names>SP</given-names></string-name>, <string-name><surname>Hughes</surname> &#x000a0;<given-names>DP</given-names></string-name>, <string-name><surname>Salath&#x000e9;</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<article-title>Using deep learning for image-based plant disease detection</article-title>. <source><italic toggle="yes">Front Plant Sci</italic></source> &#x000a0;<year>2016</year>;<volume>7</volume>:<page-range>1419</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fpls.2016.01419</pub-id></mixed-citation></ref><ref id="R7"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Barbedo</surname> &#x000a0;<given-names>JGA</given-names></string-name>
</person-group> &#x000a0;<article-title>Factors influencing the use of deep learning for plant disease recognition</article-title>. <source><italic toggle="yes">Biosyst Eng</italic></source> &#x000a0;<year>2018</year>;<volume>172</volume>:<fpage>84</fpage>&#x02013;<lpage>91</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2018.05.013</pub-id></mixed-citation></ref><ref id="R8"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Singh</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Ganapathysubramanian</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Singh</surname> &#x000a0;<given-names>AK</given-names></string-name></person-group>, <etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Machine learning for high-throughput stress phenotyping in plants</article-title>. <source><italic toggle="yes">Trends Plant Sci</italic></source> &#x000a0;<year>2016</year>;<volume>21</volume>:<fpage>110</fpage>&#x02013;<lpage>24</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tplants.2015.10.015</pub-id><pub-id pub-id-type="pmid">26651918</pub-id>
</mixed-citation></ref><ref id="R9"><label>9.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Braid</surname> &#x000a0;<given-names>A</given-names></string-name>
</person-group>. <article-title>React native: bringing modern web techniques to mobile</article-title>. <source><italic toggle="yes">IEEE Softw</italic></source> &#x000a0;<year>2015</year>;<volume>32</volume>:<fpage>14</fpage>&#x02013;<lpage>16</lpage>. doi: <pub-id pub-id-type="doi">10.1109/MS.2015.77</pub-id></mixed-citation></ref><ref id="R10"><label>10.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Pilgrim</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Quin</surname> &#x000a0;<given-names>H</given-names></string-name></person-group> &#x000a0;<source><italic toggle="yes">HTML5: Up and Running</italic></source>. <publisher-loc>Sebastopol, CA</publisher-loc>: <publisher-name>O&#x02019;Reilly Media</publisher-name>, <year>2010</year>.</mixed-citation></ref><ref id="R11"><label>11.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Meyer</surname> &#x000a0;<given-names>E</given-names></string-name>
</person-group> &#x000a0;<source><italic toggle="yes">CSS: The Definitive Guide</italic></source>. <edition>4th ed</edition>. <publisher-loc>Sebastopol, CA</publisher-loc>: <publisher-name>O&#x02019;Reilly Media</publisher-name>, <year>2017</year>.</mixed-citation></ref><ref id="R12"><label>12.</label><mixed-citation publication-type="confproc">
<person-group person-group-type="author">
<string-name>
<surname>Abadi</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Barham</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Chen</surname> &#x000a0;<given-names>J</given-names></string-name></person-group>, <etal>et&#x000a0;al.</etal> &#x000a0;<article-title>TensorFlow: A System for Large-Scale Machine Learning</article-title>. <italic toggle="yes">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</italic>, <conf-loc>Savannah, GA, USA</conf-loc>, <year>2016</year>, pp. <fpage>265</fpage>&#x02013;<lpage>83</lpage>.</mixed-citation></ref><ref id="R13"><label>13.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Paszke</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Gross</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Massa</surname> &#x000a0;<given-names>F</given-names></string-name></person-group>, <etal>et&#x000a0;al.</etal> &#x000a0;<article-title>PyTorch: an imperative style, high-performance deep learning library</article-title>. <source><italic toggle="yes">Advances in Neural Information Processing Systems(NeurIPS)</italic></source> &#x000a0;<year>2019</year>;32:<fpage>8024</fpage>&#x02013;<lpage>35</lpage>.</mixed-citation></ref><ref id="R14"><label>14.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Jocher</surname> &#x000a0;<given-names>G</given-names></string-name>
</person-group> &#x000a0;<article-title>YOLOv5 by Ultralytics</article-title>. <source><italic toggle="yes">GitHub Repository</italic></source>, <year>2020</year>. <ext-link xlink:href="https://github.com/ultralytics/yolov5" ext-link-type="uri">https://github.com/ultralytics/yolov5</ext-link> ( <comment>14 October 2024</comment>, date last accessed).</mixed-citation></ref></ref-list></back></article>