<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id><journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id><journal-title-group><journal-title>Computational and Structural Biotechnology Journal</journal-title></journal-title-group><issn pub-type="epub">2001-0370</issn><publisher><publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11786909</article-id><article-id pub-id-type="pii">S2001-0370(24)00454-9</article-id><article-id pub-id-type="doi">10.1016/j.csbj.2024.12.033</article-id><article-categories><subj-group subj-group-type="heading"><subject>Mini-Review</subject></subj-group></article-categories><title-group><article-title>Machine learning methods for histopathological image analysis: Updates in 2024</article-title></title-group><contrib-group><contrib contrib-type="author" id="au0005"><name><surname>Komura</surname><given-names>Daisuke</given-names></name></contrib><contrib contrib-type="author" id="au0010"><name><surname>Ochi</surname><given-names>Mieko</given-names></name></contrib><contrib contrib-type="author" id="au0015"><name><surname>Ishikawa</surname><given-names>Shumpei</given-names></name><email>ishum-prm@m.u-tokyo.ac.jp</email><xref rid="cor1" ref-type="corresp">&#x0204e;</xref></contrib><aff id="aff0005">Department of Preventive Medicine, Graduate School of Medicine, The University of Tokyo, Tokyo, Japan</aff></contrib-group><author-notes><corresp id="cor1"><label>&#x0204e;</label>Corresponding author. <email>ishum-prm@m.u-tokyo.ac.jp</email></corresp></author-notes><pub-date pub-type="pmc-release"><day>30</day><month>12</month><year>2024</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub"><day>30</day><month>12</month><year>2024</year></pub-date><volume>27</volume><fpage>383</fpage><lpage>400</lpage><history><date date-type="received"><day>30</day><month>9</month><year>2024</year></date><date date-type="rev-recd"><day>23</day><month>12</month><year>2024</year></date><date date-type="accepted"><day>26</day><month>12</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2025 The Authors</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p></license></permissions><abstract id="ab0010"><p>The combination of artificial intelligence and digital pathology has emerged as a transformative force in healthcare and biomedical research. As an update to our 2018 review, this review presents comprehensive analysis of machine learning applications in histopathological image analysis, with focus on the developments since 2018. We highlight significant advances that have expanded the technical capabilities and practical applications of computational pathology. The review examines progress in addressing key challenges in the field as follows: processing of gigapixel whole slide images, insufficient labeled data, multidimensional analysis, domain shifts across institutions, and interpretability of machine learning models. We evaluate emerging trends, such as foundation models and multimodal integration, that are reshaping the field. Overall, our review highlights the potential of machine learning in enhancing both routine pathological analysis and scientific discovery in pathology. By providing this comprehensive overview, this review aims to guide researchers and clinicians in understanding the current state of the pathology image analysis field and its future trajectory.</p></abstract><kwd-group id="keys0005"><title>Keywords</title><kwd>Histopathology</kwd><kwd>Deep learning</kwd><kwd>Machine learning</kwd><kwd>Whole slide image</kwd><kwd>Computer-assisted diagnosis</kwd><kwd>Digital image analysis</kwd><kwd>Foundation model</kwd></kwd-group></article-meta></front><body><sec id="sec0005"><label>1</label><title>Introduction</title><p id="p0010">Histopathological examination is crucial in modern healthcare, particularly for cancer diagnosis. Traditionally dependent on pathologists&#x02019; expertise with microscopes, the field of pathology image analysis is now gradually incorporating digital pathology and advanced computational techniques to enhance the accuracy, efficiency, and reproducibility of pathological diagnosis <xref rid="bib1" ref-type="bibr">[1]</xref>, <xref rid="bib2" ref-type="bibr">[2]</xref>, <xref rid="bib3" ref-type="bibr">[3]</xref>, <xref rid="bib4" ref-type="bibr">[4]</xref>, <xref rid="bib5" ref-type="bibr">[5]</xref>. Moreover, the computational analysis of vast numbers of histological images enables the discovery of biologically or clinically relevant knowledge <xref rid="bib6" ref-type="bibr">[6]</xref>, <xref rid="bib7" ref-type="bibr">[7]</xref>, <xref rid="bib8" ref-type="bibr">[8]</xref>.</p><p id="p0015">In 2018, we published a mini-review that provided an overview of digital pathology image analysis using machine learning techniques and important challenges to be addressed <xref rid="bib9" ref-type="bibr">[9]</xref>. Since then, the field has witnessed remarkable progress, with numerous studies advancing both the accuracy and capabilities of pathological image analysis. The number of published articles on deep learning in histopathology has increased from 301 in 2018&#x02013;2535 in 2024, i.e., a more than eightfold increase in just 6 years (<xref rid="fig0005" ref-type="fig">Fig. 1</xref>). This surge in research activity has not only significantly improved the existing applications but also paved the way for novel applications that were not addressed in our previous review.<fig id="fig0005"><label>Fig. 1</label><caption><p>Number of research articles in PubMed with the keywords &#x0201c;deep learning&#x0201d; and &#x0201c;histopathology&#x0201d; between 2014 and 2024.</p></caption><alt-text id="at0005">Fig. 1</alt-text><graphic xlink:href="gr1" id="lk0005"/></fig></p><p id="p0020">Despite these advances, several challenges persist, some of which were noted in our previous review. In this review, we present a comprehensive summary of computational pathology, with emphasis on the most impactful and novel approaches in these fields. We focus on challenges that are unique to or especially significant in pathological image analysis in order to provide valuable insights for pathologists and machine learning researchers new to this field.</p><p id="p0025">In the following sections, we first discuss the key clinically and biologically important applications in computational pathology. We then explore specific challenges in this field and the solutions being developed to address these challenges.</p></sec><sec id="sec0010"><label>2</label><title>Basic workflow of machine learning-based histopathological image analysis</title><p id="p0030">Since 2018, deep learning approaches have become increasingly prevalent across all stages of pathological image analysis. Although deep learning methods were prevalent in 2018, there has been a continued shift toward more sophisticated deep neural network architectures for both feature extraction and downstream tasks.</p><p id="p0035">The basic workflow of pathological image analysis has remained largely consistent since 2018, primarily involving the extraction of fixed-size patches from whole tissue regions or regions of interest in whole slide images (WSIs) (<xref rid="sec0040" ref-type="sec">4.1</xref>), followed by feature extraction from these patches (<xref rid="fig0010" ref-type="fig">Fig. 2</xref>). Some studies have explored alternative approaches, such as the use of superpixels <xref rid="bib10" ref-type="bibr">[10]</xref>, <xref rid="bib11" ref-type="bibr">[11]</xref> or constructing graphs <xref rid="bib12" ref-type="bibr">[12]</xref>, <xref rid="bib13" ref-type="bibr">[13]</xref>, <xref rid="bib14" ref-type="bibr">[14]</xref>, based on the detected tissue components. For tissue region detection, Otsu&#x02019;s thresholding or adaptive thresholding method <xref rid="bib15" ref-type="bibr">[15]</xref>, <xref rid="bib16" ref-type="bibr">[16]</xref> is commonly used to separate tissue from background. Regions of interest, such as tumor regions, are typically defined either by pathologists using specialized annotation tools <xref rid="bib17" ref-type="bibr">[17]</xref>, <xref rid="bib18" ref-type="bibr">[18]</xref> or automatically through deep learning-based segmentation and classification methods <xref rid="bib19" ref-type="bibr">[19]</xref>, <xref rid="bib20" ref-type="bibr">[20]</xref>. Various filtering techniques are applied to exclude patches containing blur or artifacts <xref rid="bib21" ref-type="bibr">[21]</xref>, <xref rid="bib22" ref-type="bibr">[22]</xref>, ensuring the quality of the extracted patches (<xref rid="sec0085" ref-type="sec">4.4</xref>).</p><p id="p0040">In the domain of feature extraction from these sampled patches, the use of vision transformers (ViTs) <xref rid="bib23" ref-type="bibr">[23]</xref>, <xref rid="bib24" ref-type="bibr">[24]</xref> alongside conventional convolutional neural networks has increased due to their enhanced ability to capture contextual information. Methods to aggregate patch-level features into slide-level features have gained popularity <xref rid="bib25" ref-type="bibr">[25]</xref>, <xref rid="bib26" ref-type="bibr">[26]</xref>, <xref rid="bib27" ref-type="bibr">[27]</xref>, <xref rid="bib28" ref-type="bibr">[28]</xref>, <xref rid="bib29" ref-type="bibr">[29]</xref> for slide-level tasks, such as prognosis prediction. Some approaches employ detection or segmentation techniques to identify and classify tissues, cells, or nuclei, followed by the creation of interpretable hand-crafted features, such as cell distribution, nuclear shape, and texture <xref rid="bib30" ref-type="bibr">[30]</xref>, <xref rid="bib31" ref-type="bibr">[31]</xref>. These methods often involve constructing network structures with cells as nodes that are then used for classification and other analytical tasks.</p><p id="p0045">The emergence of large-scale pretrained models, also known as foundation models pretrained on vast and diverse datasets of pathological images, has markedly advanced pathological image analysis <xref rid="bib24" ref-type="bibr">[24]</xref>, <xref rid="bib27" ref-type="bibr">[27]</xref>, <xref rid="bib29" ref-type="bibr">[29]</xref>, <xref rid="bib32" ref-type="bibr">[32]</xref> (<xref rid="sec0055" ref-type="sec">4.2.2</xref>). These models are used to extract patch- or slide-level features. After appropriate feature extraction, various analytical tasks common to image analysis are performed, such as supervised learning, unsupervised learning, multiple instance learning (also known as weakly supervised learning), and contrastive learning. In some applications, features from various modalities, such as text and transcriptome data, are integrated to learn the correlations between modalities or predict clinical information using multiple modalities (<xref rid="sec0060" ref-type="sec">4.3</xref>). These tasks can be performed either as a separate step after feature extraction or integrated into an end-to-end training process with the feature extractor. Finally, the output might be visualized in an interpretable way (<xref rid="sec0090" ref-type="sec">4.5</xref>), and a clinical decision may be made based on the result in clinical settings (<xref rid="sec0095" ref-type="sec">4.6</xref>). <xref rid="fig0010" ref-type="fig">Fig. 2</xref>.<fig id="fig0010"><label>Fig. 2</label><caption><p>Workflow of histopathological image analysis with machine learning. Related sections in this review are shown in each process. Created in BioRender. Komura <xref rid="bib33" ref-type="bibr">[33]</xref><ext-link ext-link-type="uri" xlink:href="https://app.biorender.com/citation/6778ba7098af19cb5641ccdc" id="ir0005">https://BioRender.com/g69j665</ext-link>.</p></caption><alt-text id="at0010">Fig. 2</alt-text><graphic xlink:href="gr2" id="lk0010"/></fig></p></sec><sec id="sec0015"><label>3</label><title>Machine learning application in digital pathology</title><p id="p0050">Machine learning techniques have enabled diverse applications in pathological image analysis, ranging from diagnostic support to novel biological discoveries. This section highlights the key applications that have demonstrated practical impact.</p><sec id="sec0020"><label>3.1</label><title>Computer-assisted diagnosis (CAD)</title><p id="p0055">CAD systems use machine learning algorithms to assist pathologists in interpreting histopathological images. These systems primarily employ supervised learning approaches, which can be categorized into three main tasks as follows: classification, detection, and segmentation. In pathology, classification tasks involve several key applications, such as tissue categorization, including cancer subtypes <xref rid="bib2" ref-type="bibr">[2]</xref>, <xref rid="bib3" ref-type="bibr">[3]</xref>, <xref rid="bib4" ref-type="bibr">[4]</xref>, <xref rid="bib5" ref-type="bibr">[5]</xref>, <xref rid="bib34" ref-type="bibr">[34]</xref>, <xref rid="bib35" ref-type="bibr">[35]</xref>, <xref rid="bib36" ref-type="bibr">[36]</xref>, tumor grade assessment <xref rid="bib2" ref-type="bibr">[2]</xref>, <xref rid="bib37" ref-type="bibr">[37]</xref>, assessment of pathologic response to chemotherapy&#x000a0;<xref rid="bib38" ref-type="bibr">[38]</xref>, and identification of primary cancer sites in cases of unknown origin <xref rid="bib39" ref-type="bibr">[39]</xref>. Detection or segmentation tasks involve localizing specific pathological structures, such as metastatic foci in the lymph nodes <xref rid="bib40" ref-type="bibr">[40]</xref>, <xref rid="bib41" ref-type="bibr">[41]</xref>.</p><p id="p0060">Although the basic concept of CAD has remained unchanged since 2018, recent studies have expanded significantly in scale, analyzing thousands of patients across multiple institutions <xref rid="bib42" ref-type="bibr">[42]</xref>, <xref rid="bib43" ref-type="bibr">[43]</xref>, <xref rid="bib44" ref-type="bibr">[44]</xref>, <xref rid="bib45" ref-type="bibr">[45]</xref>, <xref rid="bib46" ref-type="bibr">[46]</xref>, <xref rid="bib47" ref-type="bibr">[47]</xref>, <xref rid="bib48" ref-type="bibr">[48]</xref>. These studies have not only facilitated more accurate predictions but also provided crucial validation of the robustness of CAD systems in settings that more closely resemble real-world clinical settings. Building upon these advances, a significant milestone in CAD is the FDA approval of clinically approved systems <xref rid="bib49" ref-type="bibr">[49]</xref>, <xref rid="bib50" ref-type="bibr">[50]</xref>, signaling their readiness for real-world healthcare integration.</p><p id="p0065">Content-based image retrieval (CBIR) is a CAD technique that enables searching for images based on their visual content. At its core, CBIR operates by finding similar images through a nearest neighbor approach, comparing image features to identify matches with existing cases <xref rid="bib51" ref-type="bibr">[51]</xref>. CBIR systems are especially valuable for rare cases <xref rid="bib27" ref-type="bibr">[27]</xref>. When pathologists encounter rare tumors, they typically consult other pathologists or reference materials, such as books published by the World Health Organization. Instead, CBIR systems offer a powerful alternative, providing quick access to visually similar cases and associated diagnoses, assisting decision-making by pathologists. In patch-based CBIR, which was predominant until 2018, pathologists select diagnostically important regions within an image to use as queries <xref rid="bib51" ref-type="bibr">[51]</xref>, <xref rid="bib52" ref-type="bibr">[52]</xref>, <xref rid="bib53" ref-type="bibr">[53]</xref>. This method allows for focused searches based on specific histological features or patterns that the pathologist deems crucial for diagnosis, leveraging their expertise in the search process. Patch-based systems are lightweight and versatile, making them applicable even to smartphone-captured images <xref rid="bib51" ref-type="bibr">[51]</xref>. By contrast, WSI-based CBIR, a method that uses WSIs as queries, has recently emerged <xref rid="bib27" ref-type="bibr">[27]</xref>, <xref rid="bib54" ref-type="bibr">[54]</xref>, <xref rid="bib55" ref-type="bibr">[55]</xref>, <xref rid="bib56" ref-type="bibr">[56]</xref>, <xref rid="bib57" ref-type="bibr">[57]</xref>. This process typically involves extracting slide-level features that capture global tissue characteristics and overall diagnostic information, enabling efficient retrieval of similar cases at the slide level. Both approaches have their merits. Although patch-based CBIR offers precision and efficiency, WSI-based CBIR provides a more holistic view of the tissue sample. The choice between these methods often depends on the specific diagnostic needs, available resources, and the nature of the pathological specimens under examination.</p><p id="p0070">Pathologists observe pathological specimens to make diagnoses and then write reports. These reports contain information that could be considered the pathologist&#x02019;s diagnosis itself, including histological findings related to tumor diagnosis, diagnostic basis, and results of immunohistochemical staining. Recently, attempts have begun to make artificial intelligence (AI) perform this task. This has been made possible largely due to the development of high-performance vision&#x02013;language encoders and language decoders <xref rid="bib27" ref-type="bibr">[27]</xref>, <xref rid="bib28" ref-type="bibr">[28]</xref>, <xref rid="bib58" ref-type="bibr">[58]</xref>, <xref rid="bib59" ref-type="bibr">[59]</xref>, <xref rid="bib60" ref-type="bibr">[60]</xref>.</p></sec><sec id="sec0025"><label>3.2</label><title>Predicting or discovering clinicopathological correlations</title><p id="p0075">Pathological image analysis has evolved from basic diagnostic support to enabling precision medicine, particularly in oncology <xref rid="bib61" ref-type="bibr">[61]</xref>. Since 2018, the field has seen rapid growth in both the sophistication of machine learning approaches and the scope of their applications, utilizing supervised learning methods to uncover novel correlations between histological patterns and clinical variables, such as prognosis, treatment efficacy, and genetic mutations from histological images <xref rid="bib51" ref-type="bibr">[51]</xref>, <xref rid="bib62" ref-type="bibr">[62]</xref>, <xref rid="bib63" ref-type="bibr">[63]</xref>, <xref rid="bib64" ref-type="bibr">[64]</xref>, <xref rid="bib65" ref-type="bibr">[65]</xref>, <xref rid="bib66" ref-type="bibr">[66]</xref>.</p><p id="p0080">These analyses have direct practical applications in clinical settings. For example, predicting mutations from routine histological images can identify patients negative for mutations without expensive genetic testing, reducing healthcare costs <xref rid="bib67" ref-type="bibr">[67]</xref>, <xref rid="bib68" ref-type="bibr">[68]</xref>. In prognostic prediction, incorporating model outputs as covariates may reduce the required number of enrolled patients in new anticancer drug development <xref rid="bib69" ref-type="bibr">[69]</xref>. Treatment response prediction can help ensure that patients receive appropriate treatments, reducing the risk of suffering from side effects of ineffective treatments while enabling quicker transitions to more effective drugs <xref rid="bib70" ref-type="bibr">[70]</xref>, <xref rid="bib71" ref-type="bibr">[71]</xref>, <xref rid="bib72" ref-type="bibr">[72]</xref>. For expensive treatments, such as immune checkpoint inhibitors, this approach could contribute to healthcare cost reduction by minimizing the wasteful use of costly medications <xref rid="bib73" ref-type="bibr">[73]</xref>, <xref rid="bib74" ref-type="bibr">[74]</xref>.</p><p id="p0085">Beyond clinical applications, machine learning analyses of pathological images provide insights into cancer mechanisms and biology <xref rid="bib6" ref-type="bibr">[6]</xref>, <xref rid="bib7" ref-type="bibr">[7]</xref>, <xref rid="bib8" ref-type="bibr">[8]</xref>. Developing interpretable models to translate computational findings into meaningful pathological understanding remains a key challenge, which we discuss in detail in <xref rid="sec0085" ref-type="sec">4.4</xref>. For example, Wulczyn <italic>et al</italic>. <xref rid="bib6" ref-type="bibr">[6]</xref> developed a deep learning system to predict survival in colorectal cancer from histological images and introduced a clustering-based method to generate interpretable histological features that explain their model&#x02019;s predictions. The authors identified a feature characterized by poorly differentiated tumor cell clusters adjacent to adipose tissue, which was associated with poor prognosis. Consequently, this study highlights the need for interpretable AI models to gain clinically relevant pathological insights.</p></sec><sec id="sec0030"><label>3.3</label><title>Virtual staining</title><p id="p0090">Traditional tissue staining methods, although essential for pathological diagnosis, can be time-consuming, costly, and require precious tissue samples. Virtual staining, which emerged around 2020, addresses these limitations by generating immunohistochemical or special stains from hematoxylin and eosin (H&#x00026;E)-stained images <xref rid="bib75" ref-type="bibr">[75]</xref>, <xref rid="bib76" ref-type="bibr">[76]</xref>, <xref rid="bib77" ref-type="bibr">[77]</xref> or predicting H&#x00026;E or special stains from unstained specimens <xref rid="bib78" ref-type="bibr">[78]</xref>, <xref rid="bib79" ref-type="bibr">[79]</xref>, <xref rid="bib80" ref-type="bibr">[80]</xref>, <xref rid="bib81" ref-type="bibr">[81]</xref>. This technology offers several practical benefits. It can reduce staining costs, preserve valuable specimens, and mitigate domain shift caused by staining variability. Additionally, it enables multiple staining patterns to be generated from a single tissue section, which is particularly valuable for small structures, such as glomeruli, where consecutive sections may not capture the same features. Another promising application is intraoperative rapid diagnosis, where virtual staining can enhance the quality of frozen section images by converting them to appear more like standard formalin-fixed paraffin-embedded sections <xref rid="bib82" ref-type="bibr">[82]</xref>. Another interesting usage of virtual staining is that incorporation of virtually stained sections could improve the performance of downstream tasks, such as survival prediction <xref rid="bib83" ref-type="bibr">[83]</xref>.</p><p id="p0095">From a machine learning perspective, virtual staining is implemented as a pixel-wise regression or image-to-image translation task. The field initially relied on supervised learning techniques but has increasingly adopted generative models, such as generative adversarial networks (GANs) <xref rid="bib75" ref-type="bibr">[75]</xref>, <xref rid="bib84" ref-type="bibr">[84]</xref> and diffusion models <xref rid="bib85" ref-type="bibr">[85]</xref>, <xref rid="bib86" ref-type="bibr">[86]</xref>, which have shown remarkable progress since the early 2020&#x02009;s. For example, de Haan <italic>et al.</italic> demonstrated the use of GANs for transforming H&#x00026;E-stained kidney biopsy images into virtual special stains (Masson&#x02019;s trichrome, periodic acid-Schiff, and Jones silver stain) <xref rid="bib75" ref-type="bibr">[75]</xref>, improving the preliminary diagnoses of non-neoplastic kidney diseases.</p><p id="p0100">Despite these advances, it is crucial to understand the fundamental difference between actual staining methods and deep learning-based virtual staining. Although traditional staining relies on chemical reactions, deep learning-based virtual staining derives patterns from tissue morphology and texture. These distinct principles can lead to significant discrepancies between virtual and actual staining results <xref rid="bib81" ref-type="bibr">[81]</xref>, <xref rid="bib87" ref-type="bibr">[87]</xref>, <xref rid="bib88" ref-type="bibr">[88]</xref>, requiring careful validation in clinical applications.</p></sec></sec><sec id="sec0035"><label>4</label><title>Problems in histopathological image analysis</title><p id="p0105">Histopathological image analysis presents unique technical challenges that distinguish it from other medical imaging domains. Since our 2018 review, some challenges have been partially addressed, whereas others have emerged or persisted. This section examines five critical areas as follows: very large image size, insufficient labeled images, multidimensional analysis, domain shifts, and interpretability. For each area, we discuss recent advances, current limitations, and promising research directions for the machine learning community.</p><sec id="sec0040"><label>4.1</label><title>Very large image size</title><p id="p0110">The analysis of WSIs presents a fundamental computational challenge. Although a pathology specimen on a glass slide typically measures up to a few cm<sup>2</sup>, high-resolution slide scanners produce WSIs that can reach several gigapixels (<xref rid="fig0005" ref-type="fig">Fig. 1</xref>a). To manage the massive images that exceed conventional memory capacity, researchers typically divide WSIs into smaller patches for analysis.</p><p id="p0115">Since 2018, significant advances have been made in addressing the critical challenge of effectively combining information from individual patches to make slide-level decisions. Prior to these advances, early approaches used simple aggregation methods, such as majority voting and pooling operations <xref rid="bib64" ref-type="bibr">[64]</xref>, <xref rid="bib89" ref-type="bibr">[89]</xref>, <xref rid="bib90" ref-type="bibr">[90]</xref>, <xref rid="bib91" ref-type="bibr">[91]</xref>. However, these methods had significant limitations&#x02014;max pooling considers only the most prominent signals, potentially missing important contextual information, whereas average pooling dilutes important features by distributing equal weight to all regions, including nonrelevant areas, such as normal tissue, in cancer classification tasks.</p><p id="p0120">The limitations of these simple approaches drove the development of more sophisticated aggregation methods, such as recurrent neural networks (RNNs) <xref rid="bib2" ref-type="bibr">[2]</xref> and attention pooling <xref rid="bib92" ref-type="bibr">[92]</xref>, <xref rid="bib93" ref-type="bibr">[93]</xref>, <xref rid="bib94" ref-type="bibr">[94]</xref>, <xref rid="bib95" ref-type="bibr">[95]</xref>, <xref rid="bib96" ref-type="bibr">[96]</xref>, <xref rid="bib97" ref-type="bibr">[97]</xref>. RNNs address the challenge of maintaining context while processing patches sequentially. The network builds understanding incrementally, updating its hidden state with each new patch while retaining relevant information from previous patches. However, RNNs face significant practical limitations&#x02014;training difficulties due to vanishing or exploding gradients, limited long-term memory, and slow processing due to their sequential nature.</p><p id="p0125">Attention pooling overcame several of these limitations by enabling parallel processing and direct modeling of the correlations between patches. This approach dynamically learns to identify and focus on the most diagnostically relevant regions while suppressing the less informative areas. However, its computational demands grow quadratically with the number of patches, creating challenges for large WSIs.</p><p id="p0130">To address this, various strategies have been proposed that include creating multiresolution hierarchical structures <xref rid="bib27" ref-type="bibr">[27]</xref>, <xref rid="bib98" ref-type="bibr">[98]</xref>, <xref rid="bib99" ref-type="bibr">[99]</xref>, <xref rid="bib100" ref-type="bibr">[100]</xref>. Hierarchical representation is a natural approach because pathologists typically begin with a low-magnification WSI to identify global structures and then transition to higher magnification images to identify fine-grained structures. For example, hierarchical image pyramid transformer <xref rid="bib100" ref-type="bibr">[100]</xref> starts with 16&#x02009;&#x000d7;&#x02009;16 pixel images at 20&#x02009;&#x000d7;&#x02009;magnification to capture cellular features. These cellular features were aggregated into 256&#x02009;&#x000d7;&#x02009;256-pixel regions representing cellular organization, with further aggregation into 4096&#x02009;&#x000d7;&#x02009;4096 tissue phenotypes and finally into WSIs. Feature aggregation is accomplished with the same mechanism using a per-instance multilayer perceptron, followed by a ViT and global pooling. LongViT <xref rid="bib29" ref-type="bibr">[29]</xref> employs efficient dilated attention to capture correlations between distant patches. The advantage of this approach is a simple architecture that can be trained with efficiency in computation and memory while capturing both short- and long-range dependencies. The computation complexity of LongViT is <inline-formula><mml:math id="M1" altimg="si0001.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">Nd</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which is much smaller than vanilla attention with <inline-formula><mml:math id="M2" altimg="si0002.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M3" altimg="si0003.svg"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of patches and <inline-formula><mml:math id="M4" altimg="si0004.svg"><mml:mi>d</mml:mi></mml:math></inline-formula> is the hidden dimension.</p><p id="p0135">Recently, state-space models, such as Mamba-2 <xref rid="bib101" ref-type="bibr">[101]</xref> and Vision Mamba <xref rid="bib102" ref-type="bibr">[102]</xref>, <xref rid="bib103" ref-type="bibr">[103]</xref>, <xref rid="bib104" ref-type="bibr">[104]</xref>, have gained significant attention in the field due to their ability to capture longer-range interactions. These models can compress and retain important information similar to RNNs, but they operate selectively in a data-dependent manner. This selective nature means they can efficiently identify and retain only the most relevant information from the input data, which results in longer memorization. Additionally, the computation can be parallelized, which makes state-space models more computationally efficient than RNNs. Although the original Mamba architecture results in spatial discrepancies due to the limitations of 1D sequence processing, a Mamba derivative that processes a WSI in a 2D manner has been developed recently <xref rid="bib105" ref-type="bibr">[105]</xref>.</p><p id="p0140">It is pertinent to mention that considering a wide range in some cases could be important not only for classification but also for detection and segmentation tasks <xref rid="bib106" ref-type="bibr">[106]</xref>, <xref rid="bib107" ref-type="bibr">[107]</xref>, <xref rid="bib108" ref-type="bibr">[108]</xref>, <xref rid="bib109" ref-type="bibr">[109]</xref>, <xref rid="bib110" ref-type="bibr">[110]</xref>, although this direction remains underexplored. For example, when pathologists encounter unfamiliar color tones or appearances, they may need to examine overall distribution to recognize cell morphology prototypes. Additionally, some structures cannot be classified based solely on their immediate surroundings, such as germinal centers in the lymph nodes, epithelioid macrophages, or well-differentiated hepatocellular carcinoma. This is because these structures may only become discernible when contrasted with other clear structures or surrounding tissues over a broader area. The importance of wide-range examination is also evident in tumor identification, where similarities between clearly defined tumor regions and other areas may inform diagnosis. This is particularly relevant because tumors are clonal, which means that tumor cells tend to have similar appearances to one another. Consequently, by examining a wider area, pathologists can identify patterns and similarities that may not be apparent when focusing on a small region, thus improving the accuracy of tumor diagnosis and classification.</p></sec><sec id="sec0045"><label>4.2</label><title>Insufficiently labeled images</title><p id="p0145">The scarcity of labeled histopathological images presents a fundamental challenge in the development of reliable machine learning models. This limitation stems from two key factors: 1) the time-intensive nature of expert annotation and 2) the privacy concerns surrounding medical data. To address this challenge, researchers have pursued two complementary approaches: 1) building comprehensive public datasets and 2) developing methods that can learn effectively from limited labeled data. In this section, we examine recent progress in both directions.</p><sec id="sec0050"><label>4.2.1</label><title>Dataset</title><p id="p0150">The development of robust pathological AI systems requires large-scale, diverse, and well-annotated datasets that capture the complexity of clinical diagnosis. Since 2018, the field has seen substantial growth in both the size and variety of public datasets (<xref rid="tbl0005" ref-type="table">Table 1</xref>). The Cancer Genome Atlas continues to serve as the cornerstone resource for pathological image analysis, offering an extensive collection of WSIs across 32 cancer types. Its value stems from its comprehensive annotations, including diagnostic grades, prognostic information, and molecular data, such as somatic and germline mutations. This rich combination of imaging and molecular data enables researchers to investigate the correlations between morphological features and clinical outcomes. The versatility of The Cancer Genome Atlas has spawned numerous derivative datasets <xref rid="bib51" ref-type="bibr">[51]</xref>, <xref rid="bib111" ref-type="bibr">[111]</xref>, <xref rid="bib112" ref-type="bibr">[112]</xref>, where researchers have enhanced original data with additional annotations and preprocessing steps to address specific research questions in cancer pathology. Clinical Proteomic Tumor Analysis Consortium (CPTAC) provides another comprehensive multimodal dataset <xref rid="bib113" ref-type="bibr">[113]</xref>. Recent years have seen the emergence of specialized large-scale datasets focused on specific cancer types. For example, IMP-CRS <xref rid="bib114" ref-type="bibr">[114]</xref> offers &#x0003e;&#x02009;5000 annotated slides for colorectal cancer classification, whereas PANDA <xref rid="bib37" ref-type="bibr">[37]</xref> provides &#x0003e;&#x02009;12,000 prostate cancer images with detailed Gleason grade annotations. These specialized datasets enable the development of highly accurate AI models for specific diagnostic tasks.<table-wrap position="float" id="tbl0005"><label>Table 1</label><caption><p>Publicly available large-scale histopathology datasets.</p></caption><alt-text id="at0035">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Dataset name</th><th># Samples</th><th>Annotation or additional modality</th><th>Disease or tissue</th></tr></thead><tbody><tr><td><italic>Classification</italic></td><td/><td/><td/></tr><tr><td>TCGA <xref rid="bib115" ref-type="bibr">[115]</xref>, <xref rid="bib116" ref-type="bibr">[116]</xref></td><td>&#x0003e;&#x02009;18,000 slides</td><td>Various clinical information on genome/transcriptome/epigenome pathology reports</td><td>32 cancer types</td></tr><tr><td>TCIA <xref rid="bib117" ref-type="bibr">[117]</xref></td><td>&#x0003e;&#x02009;1400 slides (except for CPTAC)</td><td>Various clinical information</td><td/></tr><tr><td>CPTAC <xref rid="bib113" ref-type="bibr">[113]</xref></td><td>2059 slides</td><td>Various clinical information on genome/proteome/transcriptome</td><td>14 cancer types</td></tr><tr><td>PANDA <xref rid="bib37" ref-type="bibr">[37]</xref></td><td>12,625 slides</td><td>Gleason grade</td><td>Prostate cancer</td></tr><tr><td>IMP-CRS <xref rid="bib114" ref-type="bibr">[114]</xref></td><td>5333 slides</td><td>Grade (non-neoplastic, low/high grade lesions)</td><td>Colon tumor</td></tr><tr><td>DHMC <xref rid="bib118" ref-type="bibr">[118]</xref></td><td>563 slides</td><td>Renal cell carcinoma (RCC) subtypes</td><td>RCC</td></tr><tr><td>GTEx <xref rid="bib119" ref-type="bibr">[119]</xref>, <xref rid="bib120" ref-type="bibr">[120]</xref></td><td>25,380 slides</td><td>Tissue<break/>Bulk transcriptome</td><td>Normal</td></tr><tr><td><italic>Detection/segmentation</italic></td><td/><td/><td/></tr><tr><td>Camelyon <xref rid="bib121" ref-type="bibr">[121]</xref></td><td>1399 slides</td><td>Segmentation mask for the cancer region</td><td>Lymph node metastasis in breast cancer</td></tr><tr><td>BCSS</td><td><list list-type="simple" id="li0005"><list-item id="u0005"><label>151.</label><p id="p0005">slides</p></list-item></list></td><td>&#x0003e;&#x02009;20,000 segmentation annotations of tissue region by human (five tissue types)</td><td>Breast cancer</td></tr><tr><td>NuCLS <xref rid="bib111" ref-type="bibr">[111]</xref></td><td>125 slides</td><td>&#x0003e;&#x02009;220,000 nuclei labels by humans (12 cell types)</td><td>Breast cancer</td></tr><tr><td>Lizard <xref rid="bib122" ref-type="bibr">[122]</xref></td><td>291 images</td><td>495,179 nuclei labels by humans (six cell types)</td><td>Colon cancer</td></tr><tr><td>MoNuSAC <xref rid="bib123" ref-type="bibr">[123]</xref></td><td>71 patients</td><td>&#x0003e;&#x02009;46,909 nuclei labels by humans (four cell types)</td><td>Lung, prostate, kidney, and breast cancer</td></tr><tr><td>MIDOG+&#x02009;+ <xref rid="bib124" ref-type="bibr">[124]</xref></td><td>503 cases</td><td>&#x0223c;12,000 cell labels for mitosis by humans</td><td>Seven cancer types</td></tr><tr><td>SegPath <xref rid="bib125" ref-type="bibr">[125]</xref></td><td>1583 patients (tissue microarray)</td><td>-Tissue segmentation labels for &#x0003e;&#x02009;80,000 patches based on immunofluorescence (IF) images (three tissue types)<break/>- &#x0003e;&#x02009;1500,000 nuclei labels and based on IF images (five cell types)</td><td>Tumors from 18 organs</td></tr><tr><td><italic>others</italic></td><td/><td/><td/></tr><tr><td>PathVQA <xref rid="bib126" ref-type="bibr">[126]</xref></td><td>5004 images</td><td>32,795 questions and answers for the image</td><td>Various diseases</td></tr><tr><td>Quilt-1M <xref rid="bib127" ref-type="bibr">[127]</xref></td><td>1000,000 images</td><td>Paired text</td><td>Various diseases</td></tr><tr><td>PLISM <xref rid="bib128" ref-type="bibr">[128]</xref></td><td>368,849 images from 46 WSIs</td><td>13H&#x00026;E staining conditions with 13 imaging media (7 WSI scanners and 6 smarpthones)</td><td>48 tissue types</td></tr><tr><td>HEST&#x02212;1k <xref rid="bib129" ref-type="bibr">[129]</xref></td><td>1108 WSIs</td><td>Paired spatial transcriptome and 60 million detected nuclei</td><td>25 organs</td></tr></tbody></table><table-wrap-foot><fn><p>*All datasets contain formalin-fixed paraffin-embedded hematoxylin and eosin-stained images</p></fn></table-wrap-foot></table-wrap></p><p id="p0155">The evolving needs of computational pathology have driven the creation of diverse specialized datasets that serve different purposes. PathVQA <xref rid="bib126" ref-type="bibr">[126]</xref> is a visual question&#x02013;answering dataset that is semiautomatically generated from textbooks and online digital libraries, comprising &#x0003e;&#x02009;30,000 pathology images paired with relevant questions and answers. Quilt-1M <xref rid="bib127" ref-type="bibr">[127]</xref> contains 1 million image&#x02013;text pairs curated from various sources, including YouTube videos. These datasets can be used to create vision&#x02013;language models for pathology (see <xref rid="sec0070" ref-type="sec">4.3.2</xref> for details).</p><p id="p0160">Another crucial development addresses the challenge of method generalization across different imaging conditions. The PLISM dataset <xref rid="bib128" ref-type="bibr">[128]</xref> specifically tackles this issue by providing images of identical specimens captured under various staining methods and imaging devices&#x02014;from professional slide scanners to smartphones. This resource enables researchers to develop and validate robust analysis methods that can perform consistently across various technical conditions&#x02014;a critical requirement for real-world clinical applications.</p><p id="p0165">The growing sophistication of AI applications in pathology has created a demand for more granular annotations. New datasets now provide detailed cell- and region-level annotations <xref rid="bib111" ref-type="bibr">[111]</xref>, <xref rid="bib130" ref-type="bibr">[130]</xref>, enabling the development of precise detection and segmentation models. These datasets mark a significant advance from earlier resources that provided only slide-level labels, allowing AI systems to identify specific structures and cellular features that pathologists use for diagnosis.</p><p id="p0170">In addition to normal tissue features, researchers have recognized the importance of handling technical artifacts in pathological images. New specialized datasets document common imaging artifacts and preparation issues <xref rid="bib131" ref-type="bibr">[131]</xref>, providing essential training data for developing robust AI systems that can maintain performance despite image quality variations.</p><p id="p0175">Dataset creation has evolved from purely manual annotation to a more efficient, multifaceted approach. One major advancement is the use of AI-assisted annotation, where deep learning models provide initial labels for expert refinement <xref rid="bib111" ref-type="bibr">[111]</xref>, <xref rid="bib132" ref-type="bibr">[132]</xref>, <xref rid="bib133" ref-type="bibr">[133]</xref>, <xref rid="bib134" ref-type="bibr">[134]</xref>. This semiautomated approach, combined with crowdsourcing techniques and the involvement of medical students, along with pathologist verification, has greatly reduced the time and effort required for manual annotation while creating educational opportunities. Advances in annotation techniques that rely less on manual input have emerged. Immunohistochemistry or immunofluorescence staining highlights specific cell types or proteins, enabling the creation of more accurate and objective annotations with minimal manual labor <xref rid="bib125" ref-type="bibr">[125]</xref>, <xref rid="bib135" ref-type="bibr">[135]</xref>. SegPath demonstrates the potential of automated annotation approaches for the semantic segmentation of H&#x00026;E images. It uses immunofluorescence retaining as the ground truth to identify tissue and cell types, including epithelial tissues, various immune cells, smooth muscle tissues, and red blood cells, across 18 organ types <xref rid="bib125" ref-type="bibr">[125]</xref>. The development of SegPath revealed specific limitations in manual annotation practices. Pathologists were found to report systematic biases in their annotations, such as overlooking plasma cells that lack typical cartwheel-shaped nuclei. The use of immunofluorescence data as a reference enables more consistent annotations that capture cellular diversity beyond the standard morphological criteria.</p><p id="p0180">Multimodal datasets that combine different types of biological data have emerged as valuable resources. HEST-1k <xref rid="bib129" ref-type="bibr">[129]</xref>, which pairs pathological images with spatial transcriptomics data, has opened new avenues for integrating morphological- and molecular-level information in the analysis (see <xref rid="sec0075" ref-type="sec">4.3.3</xref> for details). Such an approach not only offers high throughput but is also effective for annotating cells that even pathologists find challenging to label definitively <xref rid="bib125" ref-type="bibr">[125]</xref>.</p><p id="p0185">Despite these advances, significant challenges persist in the field. Many datasets remain inaccessible or require direct requests to authors, often due to concerns over patient privacy. This limited access can subsequently impede the rapid progression of research. To address the challenge of data privacy while enabling collaboration, researchers have developed distributed learning approaches, such as federated and swarm learning <xref rid="bib46" ref-type="bibr">[46]</xref>, <xref rid="bib47" ref-type="bibr">[47]</xref>, <xref rid="bib48" ref-type="bibr">[48]</xref>. Federated learning allows multiple institutions to train AI models collaboratively while keeping data local; only model updates are shared with a central server. Swarm learning achieves the same goal using blockchain technology without a central server. These approaches could enable institutions to contribute to public datasets while preserving patient privacy, because sensitive data never leaves its original location. The ultimate goal would be to leverage these technologies to create comprehensive public datasets and models that benefit research while respecting privacy regulations and patient confidentiality. This would allow institutions to contribute valuable data to advance research without compromising their patients&#x02019; privacy or violating data protection laws.</p><p id="p0190">Changes in the diagnostic criteria over time present a unique challenge for pathology datasets. Although most machine learning approaches assume stable ground truth labels, medical knowledge and diagnostic standards evolve continuously. A clear example is the diagnosis of glioma, which has transitioned from purely morphological assessment to genetic mutation-based classification <xref rid="bib136" ref-type="bibr">[136]</xref>. This evolution in diagnostic criteria creates practical challenges for dataset curation. The same histological image may receive different diagnoses at different time points, not due to errors but because of advances in diagnostic standards. To address this challenge, datasets must include both historical diagnostic context and updated classifications, allowing AI systems to understand and adapt to evolving medical knowledge.</p></sec><sec id="sec0055"><label>4.2.2</label><title>Minimal supervision and foundation models</title><p id="p0195">In 2018, the significance of using pretrained models for constructing high-accuracy prediction models with limited labeled training data was highlighted. At that time, the use of ImageNet pretrained models was commonplace. Although such models provided useful feature extractors, they were not optimized for the unique characteristics of pathology images. Self-supervised learning (SSL) emerged as a more effective approach, allowing models to learn directly from large collections of unlabeled pathology images without requiring extensive manual annotations.</p><p id="p0200">SSL in computer vision is a learning paradigm where the model learns meaningful visual representations from unlabeled images by solving pretext tasks that can be automatically generated from the input data itself. SSL is effective for pathology images because of the scarcity of expert annotations for extremely large WSIs. Two main approaches have proven effective: masked image modeling and contrastive learning. In contrastive learning, the model learns to recognize that different views of the same image should have similar representations while maintaining distinct representations for different images. To create these different views, researchers have applied various data augmentation methods as follows: geometric transformations, such as rotation and flipping, and pathology-specific modifications, such as color jittering and stain normalization <xref rid="bib137" ref-type="bibr">[137]</xref>. These color augmentations are important in pathology because they help the model learn representations that are robust to stain variations across laboratories and scanning protocols. Masked image modeling reconstructs randomly masked portions of images, typically masking patches and using an encoder with a deep decoder for reconstruction. This approach has proven effective in learning rich visual representations. Recent approaches such as iBOT <xref rid="bib138" ref-type="bibr">[138]</xref> and DINOv2 <xref rid="bib139" ref-type="bibr">[139]</xref> take advantage of both contrastive learning and masked image modeling. By combining these methods, the models can learn local tissue features and global structural patterns. This combined approach has proven effective for pathology images, where both detailed cellular features and overall tissue architecture are important for diagnosis.</p><p id="p0205">The availability of large pathology image collections, sometimes exceeding one million WSIs <xref rid="bib140" ref-type="bibr">[140]</xref>, has enabled the development of foundation models. These models leverage SSL techniques to learn from vast amounts of unlabeled data <xref rid="bib24" ref-type="bibr">[24]</xref>, <xref rid="bib140" ref-type="bibr">[140]</xref>, <xref rid="bib141" ref-type="bibr">[141]</xref>, <xref rid="bib142" ref-type="bibr">[142]</xref>, <xref rid="bib143" ref-type="bibr">[143]</xref>, <xref rid="bib144" ref-type="bibr">[144]</xref>, <xref rid="bib145" ref-type="bibr">[145]</xref>, <xref rid="bib146" ref-type="bibr">[146]</xref>. The emergence of high-capacity architectures, particularly ViTs, has made it possible to utilize these large-scale datasets for model training. For example, UNI <xref rid="bib24" ref-type="bibr">[24]</xref> was trained on &#x0003e;&#x02009;100 million patches from 100,000H&#x00026;E-stained WSIs using ViT-Large architecture. The more recent model Virchow2G <xref rid="bib145" ref-type="bibr">[145]</xref> expanded the scale significantly, training on 3.1 million WSIs, including both H&#x00026;E and immunohistochemistry images, using an even larger ViT architecture. Both UNI and Virchow2G were trained with DINOv2. These models offer flexibility in their application. Researchers can use them as feature extractors with simple linear classifiers as zero-shot learners using k-NN classification or as pretrained models for task-specific fine-tuning. This versatility allows adaptation to various pathology tasks while reducing the need for large amounts of labeled data.</p><p id="p0210">Although most foundation models focus on analyzing small tissue patches, the need to understand entire slides has led to more comprehensive approaches. Prov-Gigapath <xref rid="bib29" ref-type="bibr">[29]</xref>, PRISM <xref rid="bib28" ref-type="bibr">[28]</xref>, and TITAN <xref rid="bib27" ref-type="bibr">[27]</xref> are pioneered slide-level foundation models. Each model uses a different approach to combining information across an entire slide. Prov-Gigapath utilizes LongNet with dilated attention <xref rid="bib147" ref-type="bibr">[147]</xref> to aggregate patch-level features into slide-level features, PRISM adopts Perceiver network <xref rid="bib148" ref-type="bibr">[148]</xref>, which can be considered a variant of RNN, for aggregation, and TITAN builds hierarchical representation <xref rid="bib27" ref-type="bibr">[27]</xref>. These models can be valuable because many recent tasks involve slide-level predictions, including tumor subtype classification and genomic mutation prediction.</p><p id="p0215">The growing availability of labeled pathology data has enabled a new generation of foundation models based on supervised learning. Because segmentation of tissue and cells is important in pathology, foundation models targeting segmentation, such as BiomedParse <xref rid="bib149" ref-type="bibr">[149]</xref>, SegAnyPath <xref rid="bib150" ref-type="bibr">[150]</xref>, and PAGET <xref rid="bib33" ref-type="bibr">[33]</xref>, have been recently developed, which can be used for diverse tissues and modalities. Some foundation models take advantage of related diagnostic tasks through multitask learning <xref rid="bib151" ref-type="bibr">[151]</xref>, <xref rid="bib152" ref-type="bibr">[152]</xref>, <xref rid="bib153" ref-type="bibr">[153]</xref>, <xref rid="bib154" ref-type="bibr">[154]</xref>. For example, CHIEF <xref rid="bib154" ref-type="bibr">[154]</xref> is trained for various slide-level prediction tasks, including cancer detection, tumor origin identification, biomarker prediction, and prognosis prediction, for various cancer types. The advantages of these models is that they can be used as feature extractors as well as potentially used for specific tasks, such as cancer detection and nucleus segmentation without fine-tuning.</p><p id="p0220">Different foundation models often excel at capturing different aspects of pathological images. Research has shown that combining these complementary models through ensemble approaches can improve performance across various diagnostic tasks <xref rid="bib146" ref-type="bibr">[146]</xref>. Building on this insight, COBRA <xref rid="bib155" ref-type="bibr">[155]</xref> takes ensemble learning further by integrating features from multiple foundation models to create comprehensive slide-level representations. This approach combines the strengths of different models while maintaining computational efficiency through the Mamba-2 architecture <xref rid="bib101" ref-type="bibr">[101]</xref>.</p></sec></sec><sec id="sec0060"><label>4.3</label><title>Multidimensional analysis</title><p id="p0225">Pathological analysis increasingly incorporates multiple dimensions of data&#x02014;temporal changes over disease progression, spatial correlations within tissues, and information from different analytical modalities (<xref rid="fig0015" ref-type="fig">Fig. 3</xref>). Although analyzing temporal and spatial patterns within pathology images could build on established methods, combining data from different modalities, such as genomics, radiology, and pathology reports, requires new analytical approaches.<fig id="fig0015"><label>Fig. 3</label><caption><p>Various dimensions enhancing pathology image analysis. Temporal sequences, 3D histology, and multimodal integration for comprehensive diagnostic insights. Created in BioRender. Komura <xref rid="bib33" ref-type="bibr">[33]</xref><ext-link ext-link-type="uri" xlink:href="https://app.biorender.com/citation/6778ba7098af19cb5641ccdc" id="ir0010">https://BioRender.com/g69j665</ext-link>.</p></caption><alt-text id="at0015">Fig. 3</alt-text><graphic xlink:href="gr3" id="lk0015"/></fig></p><p id="p0230">A combination of different types of biological data can enhance our understanding of disease processes. Each modality captures distinct aspects of pathology&#x02014;images reveal tissue structure, genomics or transcriptomics identify molecular changes, and pathology reports provide diagnostic interpretations.</p><p id="p0235">Multimodal data integration in pathology follows two main approaches. The first approach uses contrastive learning to align different modalities in a shared feature space. Each modality has its own encoder, but the encoders are trained to map similar content from different modalities to nearby points in the feature space. This alignment enables various applications, such as predicting molecular features from images <xref rid="bib156" ref-type="bibr">[156]</xref>, <xref rid="bib157" ref-type="bibr">[157]</xref>.</p><p id="p0240">The second approach directly combines multiple modalities to predict clinical outcomes. This method requires careful consideration of how and when to merge different types of information <xref rid="bib158" ref-type="bibr">[158]</xref>, <xref rid="bib159" ref-type="bibr">[159]</xref>, <xref rid="bib160" ref-type="bibr">[160]</xref>, <xref rid="bib161" ref-type="bibr">[161]</xref>. Early fusion strategies combine raw data immediately, whereas late fusion maintains separate processing paths until the final prediction stage. To manage the complexity of crossmodal interactions, models can employ specialized attention mechanisms that focus on specific modality pairs or process modalities hierarchically.</p><p id="p0245">In the following sections, we will evaluate modality-specific applications and their associated challenges in pathological image analysis.</p><sec id="sec0065"><label>4.3.1</label><title>Time and space</title><p id="p0250">Temporal analysis in pathology aims to understand how diseases evolve, particularly in response to treatment. This understanding is crucial for several key clinical applications as follows: monitoring tumor response to therapy through changes in cellular morphology and immune cell infiltration; tracking the progression of chronic diseases, such as liver fibrosis; and evaluating transplant rejection to guide immunosuppression therapy. Another important application is the comparative analysis between primary tumors and their metastatic lesions, which can help verify metastatic origin and provide insights into tumor evolution and heterogeneity. However, this comparison presents unique challenges as the stromal components differ significantly between organs, necessitating methods to isolate and analyze tumor cell characteristics independent of the surrounding tissue microenvironment. Computational approaches like SegRep&#x000a0;<xref rid="bib162" ref-type="bibr">[162]</xref>, which can extract features specifically from tumor cells while excluding stromal influences, may prove valuable for such analyses.</p><p id="p0255">However, obtaining temporal data in pathology presents unique challenges. Although other medical imaging methods, such as CT, can easily track changes through repeated scans <xref rid="bib163" ref-type="bibr">[163]</xref>, <xref rid="bib164" ref-type="bibr">[164]</xref>, <xref rid="bib165" ref-type="bibr">[165]</xref>, pathological analysis requires tissue sampling, making frequent temporal measurements impractical and potentially harmful to patients. Once the tissue is removed for examination, that exact region can no longer be monitored over time. Instead, researchers must analyze different tissue samples from nearby regions, introducing variability that complicates temporal comparisons. These inherent constraints have resulted in limited research on the temporal analysis of pathological changes compared with other imaging modalities. As analytical methods and AI technologies advance, this field may see significant developments, particularly in techniques for comparing samples from different time points and locations.</p><p id="p0260">Traditional pathology analysis relies on 2D tissue sections, which provide only a limited view of the complex tissue structures, and 3D analysis offers a more complete understanding of tissue architecture <xref rid="bib166" ref-type="bibr">[166]</xref>, <xref rid="bib167" ref-type="bibr">[167]</xref>, <xref rid="bib168" ref-type="bibr">[168]</xref>, <xref rid="bib169" ref-type="bibr">[169]</xref>, which is important for studying vessel networks, tumor invasion patterns, and cellular spatial correlations. Several technologies enable 3D tissue imaging, each with distinct capabilities. Serial sectioning reconstructs the 3D structure from multiple thin slices <xref rid="bib170" ref-type="bibr">[170]</xref>, <xref rid="bib171" ref-type="bibr">[171]</xref>, <xref rid="bib172" ref-type="bibr">[172]</xref>, microCT <xref rid="bib173" ref-type="bibr">[173]</xref> provides detailed internal structure visualization, and 3D light-sheet microscopy with tissue clearing <xref rid="bib174" ref-type="bibr">[174]</xref>, <xref rid="bib175" ref-type="bibr">[175]</xref> enables deep tissue imaging while preserving cellular detail. Song <italic>et al</italic>. performed pioneering work that analyzed 3D histological images using a deep learning framework specialized for 3D images <xref rid="bib166" ref-type="bibr">[166]</xref>. They proved that patient prognostication using 3D tissue volumes outperforms 2D slice-based approaches in prostate cancer. Although these initial results are promising, the full potential of the 3D analysis methods will depend on accumulating large-scale 3D pathology datasets. As more institutions adopt 3D imaging technologies and contribute to data collection, we can expect further improvements in the accuracy and reliability of 3D pathology analysis.</p></sec><sec id="sec0070"><label>4.3.2</label><title>Language</title><p id="p0265">Pathologists have developed a sophisticated language for describing tissue appearances over decades of practice. Recent AI models aim to leverage this expertise by learning from the text descriptions of pathological images. Several foundation models now use textual information from sources such as research articles and clinical reports to improve their understanding of pathological patterns. Notable examples are PLIP <xref rid="bib157" ref-type="bibr">[157]</xref>, CHIEF <xref rid="bib154" ref-type="bibr">[154]</xref>, CONCH <xref rid="bib32" ref-type="bibr">[32]</xref>, and PRISM <xref rid="bib28" ref-type="bibr">[28]</xref>, which fine-tune an SSL encoder using textual information, such as Twitter, PubMed research articles, and pathology diagnostic reports. PLIP and CHIEF adopt the CLIP framework <xref rid="bib176" ref-type="bibr">[176]</xref>, which uses dual encoders to project both images and text into a shared embedding space, and trains the model using image&#x02013;text pairs to maximize similarity between correct image&#x02013;text pairs while minimizing similarity for incorrect pairs. CONCH and PRISM adopt the Contrastive Captioner (CoCa) framework <xref rid="bib177" ref-type="bibr">[177]</xref>, which combines CLIP-style contrastive loss with a captioning loss using a vision&#x02013;language decoder. A recent benchmarking study revealed that the CONCH image encoder demonstrates superior performance across multiple tasks compared with models trained exclusively on pathological images <xref rid="bib178" ref-type="bibr">[178]</xref>. The success of these models may stem from their ability to focus on clinically relevant features while filtering out irrelevant image variations, similar to how pathologists learn to focus on diagnostic features. Traditional vision-based learning relies on data augmentation to teach models which features to ignore, such as color variations and rotations. Although this approach helps models become robust to technical variations, it provides no guidance about which features are clinically important. Leveraging textual information, which pathologists have developed a rich linguistic representation for describing pathological images over time, as supervisory information can potentially lead to improved feature extraction.</p></sec><sec id="sec0075"><label>4.3.3</label><title>Transcriptome</title><p id="p0270">Transcriptomics data offer a new method to understand the correlation between tissue appearance and molecular function <xref rid="bib179" ref-type="bibr">[179]</xref>, <xref rid="bib180" ref-type="bibr">[180]</xref>, <xref rid="bib181" ref-type="bibr">[181]</xref>, <xref rid="bib182" ref-type="bibr">[182]</xref>. Gene expression data provide detailed information regarding the cell types present in a region as well as their activation states and signaling pathways in a tissue. These molecular changes may be reflected, either directly or indirectly, in cellular and tissue morphology <xref rid="bib51" ref-type="bibr">[51]</xref>, <xref rid="bib183" ref-type="bibr">[183]</xref>. This comprehensive molecular data may help capture subtle patterns and correlations in histology that may not be apparent through traditional human observation, enabling deep learning models to extract more nuanced and informative features from a biological system.</p><p id="p0275">Transcriptome data can be obtained in three ways, each offering different insights into tissue biology (<xref rid="fig0020" ref-type="fig">Fig. 4</xref>). Bulk transcriptomics measures average gene expression across an entire tissue sample, providing a comprehensive but spatially averaged view of cellular activity. Single-cell transcriptomics measures gene expression in individual cells, but spatial information is lost. By contrast, spatial transcriptomics preserves information about where genes are expressed within a tissue, enabling researchers to connect molecular activities with specific tissue regions. Different spatial transcriptome platforms offer varying spatial resolution capabilities: The original Visium <xref rid="bib184" ref-type="bibr">[184]</xref> provides a moderate resolution of approximately 55 &#x003bc;m per spot, whereas the newer Visium HD <xref rid="bib185" ref-type="bibr">[185]</xref> improves this to 2 &#x003bc;m per spot, although 10&#x02009;&#x000d7;&#x02009;Genomics recommends using 8-&#x003bc;m spacing for optimal data quality and capture efficiency. Achieving even higher resolution, platforms such as CosMx <xref rid="bib186" ref-type="bibr">[186]</xref>, MERFISH <xref rid="bib187" ref-type="bibr">[187]</xref>, and Xenium <xref rid="bib188" ref-type="bibr">[188]</xref> reach subcellular resolution (&#x0223c;1 &#x003bc;m), enabling the precise localization of individual RNA molecules at single-cell resolution within the tissue context.<fig id="fig0020"><label>Fig. 4</label><caption><p>Various Transcriptome Technologies. Created in BioRender. Komura <xref rid="bib33" ref-type="bibr">[33]</xref><ext-link ext-link-type="uri" xlink:href="https://app.biorender.com/citation/6778ba7098af19cb5641ccdc" id="ir0015">https://BioRender.com/g69j665</ext-link>.</p></caption><alt-text id="at0020">Fig. 4</alt-text><graphic xlink:href="gr4" id="lk0020"/></fig></p><p id="p0280">Analyzing the correlation between histology and transcriptome data presents different challenges depending on the type of transcriptome data available. For bulk transcriptomics, the primary challenge is the loss of spatial information, making it difficult to connect tissue-wide gene expression patterns with histological features. Although the tissue samples come from the same patient, they often represent different regions, requiring methods that can identify general patterns rather than exact correspondence.</p><p id="p0285">Spatial transcriptomics offers a more precise mapping between gene expression and tissue structure, but similar issues persist. Researchers can either analyze consecutive tissue sections (one for H&#x00026;E staining and another for transcriptomics) or perform both analyses on the same section. Consecutive sections may show different cellular patterns due to tissue heterogeneity, whereas using the same section can compromise staining quality due to the transcriptome analysis process. Recent platforms, such as Visium HD, have addressed these limitations by enabling transcriptome analysis after standard H&#x00026;E staining, providing better alignment between molecular and visual data.</p><p id="p0290">Transcriptome-based models serve two distinct purposes in pathology image analysis. The first aims to predict gene expression patterns directly from H&#x00026;E images, potentially reducing the need for costly molecular testing. The second uses gene expression data as a form of supervision to help models learn more biologically meaningful features from images, similar to how molecular analysis guides pathologists&#x02019; understanding of tissue patterns.</p><p id="p0295">These approaches have led to several modeling strategies. Some models use contrastive learning to align image and gene expression features in a shared mathematical space, enabling direct comparison between visual and molecular patterns. Others directly predict gene expression levels from images, either focusing on broad expression patterns or specific genes of interest. Each strategy has shown promise in different applications&#x02014;contrastive methods excel at detecting general correlations between morphology and molecular states, whereas direct prediction methods are valuable for specific diagnostic tasks.</p><p id="p0300">HE2RNA <xref rid="bib189" ref-type="bibr">[189]</xref> and DeepPT <xref rid="bib190" ref-type="bibr">[190]</xref> aim to predict bulk gene expression profiles from H&#x00026;E-stained images, whereas the final aim of DeepPT is to predict cancer treatment response. Several methods predict the spatial transcriptome <xref rid="bib180" ref-type="bibr">[180]</xref>, <xref rid="bib182" ref-type="bibr">[182]</xref>, <xref rid="bib191" ref-type="bibr">[191]</xref>, <xref rid="bib192" ref-type="bibr">[192]</xref>, <xref rid="bib193" ref-type="bibr">[193]</xref>. The development of spatial transcriptomics has led to more sophisticated approaches. HistoSPACE <xref rid="bib180" ref-type="bibr">[180]</xref> trains an autoencoder for histology images, and its latent representation is attached to convolution and fully connected layers to infer spatial gene expression profiles. By contrast, TANGLE <xref rid="bib179" ref-type="bibr">[179]</xref> performs crossmodal contrastive learning using bulk gene expression embeddings obtained through multilayer perceptron-based gene expression encoders. ConGcR and ConGaR <xref rid="bib194" ref-type="bibr">[194]</xref> are contrastive learning-based models for integrating gene expression, spatial location, and tissue morphology for data representation and spatial tissue architecture identification utilizing graph convolution for gene expression and convolutional neural network for histological images. mclSTExp <xref rid="bib195" ref-type="bibr">[195]</xref> and BLEEP <xref rid="bib196" ref-type="bibr">[196]</xref> are intermediate approaches that perform crossmodal contrastive learning, and the models are also used for spatial transcriptome prediction.</p></sec><sec id="sec0080"><label>4.3.4</label><title>Other modalities</title><p id="p0305">Although less commonly used compared with text and transcriptome data, integrating pathology images with other modalities can provide a comprehensive understanding of disease processes. This includes combining pathology images with (multiple) immunohistochemistry <xref rid="bib197" ref-type="bibr">[197]</xref>, <xref rid="bib198" ref-type="bibr">[198]</xref>, cytology <xref rid="bib199" ref-type="bibr">[199]</xref>, radiology images <xref rid="bib200" ref-type="bibr">[200]</xref>, bulk genomic data <xref rid="bib201" ref-type="bibr">[201]</xref>, <xref rid="bib202" ref-type="bibr">[202]</xref>, or epigenomic information <xref rid="bib200" ref-type="bibr">[200]</xref>. IHC can identify specific proteins and markers, enhancing tissue analysis and disease classification. Cytology, particularly from biopsies, complements pathology by offering additional diagnostic insights. Radiology images, such as CT and MRI scans, are invaluable for noninvasive visualization of internal structures and can help correlate histological findings with anatomical changes, especially in cancer. Bulk genomic data contribute to the understanding of the genetic basis of diseases, and epigenomic data adds further depth by revealing gene regulation mechanisms. Recently, foundation models for various modalities have been developed, such as DNA <xref rid="bib203" ref-type="bibr">[203]</xref>, <xref rid="bib204" ref-type="bibr">[204]</xref>, protein <xref rid="bib205" ref-type="bibr">[205]</xref>, <xref rid="bib206" ref-type="bibr">[206]</xref>, CT <xref rid="bib207" ref-type="bibr">[207]</xref>, <xref rid="bib208" ref-type="bibr">[208]</xref>, and MRI <xref rid="bib209" ref-type="bibr">[209]</xref>. These models could be integrated within multimodal models and improve disease diagnosis, predict progression, and personalize treatment, moving toward more tailored and effective healthcare strategies.</p></sec></sec><sec id="sec0085"><label>4.4</label><title>Domain shifts and artifacts</title><p id="p0310">The journey from the tissue sample to the digital image involves multiple stages, each of which may contribute to the variations observed <xref rid="bib210" ref-type="bibr">[210]</xref>, <xref rid="bib211" ref-type="bibr">[211]</xref>. Preanalytical factors, such as time to fixation, specimen size, sectioning, formalin concentration, and fixation duration, impact tissue quality, affecting image color and texture, and producing artifacts (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>). Section thickness during processing directly influences image appearance <xref rid="bib212" ref-type="bibr">[212]</xref>. Even with standardized staining protocols, differences in stain manufacturers, lot, reagent age, and specific protocols may result in variations <xref rid="bib128" ref-type="bibr">[128]</xref>. In addition, different scanner models and settings contribute to variations in the final digital image <xref rid="bib128" ref-type="bibr">[128]</xref>, <xref rid="bib130" ref-type="bibr">[130]</xref>. These sources of variation present a significant challenge in pathology image analysis, especially when developing algorithms designed to generalize across institutions. The extracted features must reflect true biological differences rather than artifacts of image acquisition and processing.<fig id="fig0025"><label>Fig. 5</label><caption><p>Variation in pathology images and artifacts. A) Various factors that affect image quality. B) Examples of images produced by factors such as tissue thickness, staining solution and protocol, and slide scanners. C) Tertiary lymphoid structures in gastric cancer slides from various institutions. D) Tissue fold artifact. Created in BioRender. Komura <xref rid="bib33" ref-type="bibr">[33]</xref><ext-link ext-link-type="uri" xlink:href="https://app.biorender.com/citation/6778ba7098af19cb5641ccdc" id="ir0020">https://BioRender.com/g69j665</ext-link>.</p></caption><alt-text id="at0025">Fig. 5</alt-text><graphic xlink:href="gr5" id="lk0025"/></fig></p><p id="p0315">The challenge of interinstitutional variability in pathology images aligns with the machine learning concepts of domain adaptation and generalization. Since 2018, several strategies have been proposed to address these issues. Data transformation, particularly stain normalization, remains the most common approach to standardize color distributions across images. The <italic>de facto</italic> standard methods transfer the estimated stain vector (H&#x00026;E) of the source image to that of the target images using principle component analysis or non-negative matrix factorization <xref rid="bib213" ref-type="bibr">[213]</xref>, <xref rid="bib214" ref-type="bibr">[214]</xref>. Although these core techniques remain unchanged, recent developments have focused on fast implementations, especially GPU-based libraries <xref rid="bib215" ref-type="bibr">[215]</xref>, <xref rid="bib216" ref-type="bibr">[216]</xref>, to improve processing efficiency.</p><p id="p0320">Stain augmentation involves creating synthetic variations of existing images to expand the training dataset and improve model robustness. However, the degree of color variation often becomes an important issue. Therefore, setting an appropriate augmentation range is crucial. Excessive changes can undermine performance, whereas insufficient alterations may limit effectiveness <xref rid="bib217" ref-type="bibr">[217]</xref>. As a result, advanced techniques have been proposed to address this issue, including meta-learning for the automatic optimization of augmentation parameters <xref rid="bib218" ref-type="bibr">[218]</xref> and the selection of reference images from the distribution of other datasets <xref rid="bib219" ref-type="bibr">[219]</xref>, which aim to balance the creation of diverse, realistic variations with the preservation of the characteristics of the original image.</p><p id="p0325">However, because color is not the only variable that requires normalization, data augmentation in the embedding space, rather than pixel space, may be one of the solutions <xref rid="bib220" ref-type="bibr">[220]</xref>. Advanced techniques, such as cycle-consistent GANs (CycleGANs)&#x000a0;<xref rid="bib221" ref-type="bibr">[221]</xref> and diffusion models <xref rid="bib222" ref-type="bibr">[222]</xref>, have shown promise in translating images between domains. These techniques can be used to generate different images to address issues, such as fairness <xref rid="bib223" ref-type="bibr">[223]</xref> and class imbalance <xref rid="bib224" ref-type="bibr">[224]</xref>. Although these methods show promise, they have potential pitfalls. Image or feature translation techniques, for instance, run the risk of introducing hallucinations or artifactual features. This is particularly concerning in pathology, where subtle visual cues can have significant diagnostic implications. The challenge lies in distinguishing between variations due to different image acquisition protocols and those stemming from genuine biological differences, such as cell type variations or activation states. Incorrect modification of biologically relevant features could lead to decreased performance in certain applications.</p><p id="p0330">Various methods have been developed to mitigate the generation of artifacts, such as detection and removal <xref rid="bib21" ref-type="bibr">[21]</xref>, <xref rid="bib225" ref-type="bibr">[225]</xref>, injection through data augmentation to enhance model robustness <xref rid="bib226" ref-type="bibr">[226]</xref>, and restoration using generative models <xref rid="bib227" ref-type="bibr">[227]</xref>. In these approaches, the models and diverse training datasets containing a wide range of artifacts crucial are crucial. However, publicly available models and datasets are limited <xref rid="bib21" ref-type="bibr">[21]</xref>, <xref rid="bib225" ref-type="bibr">[225]</xref>, rendering their release desirable.</p><p id="p0335">Although studies into adding these new dimensions are important, it is crucial to note that domain shift can be more pronounced in this type of analysis. In addition to the diversity in WSIs, a domain shift exists in clinical data, cytology, and radiological images, such as CT and MRI. The types of mutations and genes that can be recognized differ between whole genome and targeted sequencing. The diversity in spatial transcriptome measurement devices and protocols is likely greater than that in pathological images, resulting in significant variations in spatial resolution, detectable gene types, and patterns of noise and errors across platforms. A key challenge will be to determine how to robustly integrate this information. Additionally, as the number of modalities increases, the complexity of information grows, necessitating a large number of cases for the development of good machine learning models. However, because the cost and resources required to collect data for a single case increase simultaneously, the development of appropriate methods that could address these challenges will also be necessary.</p></sec><sec id="sec0090"><label>4.5</label><title>Interpretability</title><p id="p0340">Interpretability in pathological image analysis is important for several reasons. First, it enables pathologists or researchers to verify that the model captures the correct pathological phenomena, which is crucial for ensuring the reliability of automated diagnostic systems. Second, interpretable models can uncover new pathological indicators that might be useful for diagnosis. These discoveries could be incorporated into routine clinical practice by pathologists, expanding their diagnostic toolkit.</p><p id="p0345">Although validating a model&#x02019;s explanatory accuracy is relatively straightforward for diagnoses with known pathological features, discovering new diagnostic features without prior information presents significant challenges due to several factors. For example, pathological changes can occur at various scales, including individual cell types, cellular morphology, multicellular structures, spatial correlations between cells and structures, and alterations in noncellular components. Typically, it is not merely the individual features but their combinations that are diagnostically significant, which in turn increase the complexity of interpretation. Therefore, it is a significant challenge to present this multifaceted, complex information in a way that is understandable and actionable for human pathologists.</p><p id="p0350">Since 2018, there has been an increasing trend in tasks where discriminative features are not well-known, such as when predicting genetic mutations. However, the development of explainable AI models has not advanced sufficiently to accommodate the trend.</p><p id="p0355">The current approaches to interpretability in pathological image analysis can be broadly categorized into several types (<xref rid="fig0030" ref-type="fig">Fig. 6</xref>). The first category includes post hoc methods, such as GradCAM <xref rid="bib228" ref-type="bibr">[228]</xref>, <xref rid="bib229" ref-type="bibr">[229]</xref>, integrated gradients <xref rid="bib230" ref-type="bibr">[230]</xref>, and attention, which highlight the regions that contributed significantly to the model&#x02019;s decision. Although these methods can work with high-accuracy models, they often struggle to adequately represent complex pathological phenomena. To tackle this problem, MRAN <xref rid="bib231" ref-type="bibr">[231]</xref> and E<sup>2</sup>-MIL <xref rid="bib232" ref-type="bibr">[232]</xref>, for instance, proposed a multilevel attention mechanism to identify both significant local and global structures in the WSI classification.<fig id="fig0030"><label>Fig. 6</label><caption><p>Approaches to interpretable artificial intelligence models.</p></caption><alt-text id="at0030">Fig. 6</alt-text><graphic xlink:href="gr6" id="lk0030"/></fig></p><p id="p0360">The second category involves inherently interpretable models, in which interpretable features are designed from the outset and used to build the classification model. The contributing features are then presented as the interpretation <xref rid="bib233" ref-type="bibr">[233]</xref>, <xref rid="bib234" ref-type="bibr">[234]</xref>. Some of these approaches rely on accurate tissue or nucleus detection or segmentation models <xref rid="bib235" ref-type="bibr">[235]</xref>, <xref rid="bib236" ref-type="bibr">[236]</xref> because they depend on information pertaining to tissue distribution, cell position, shape, and texture. Therefore, it is essential to introduce higher precision techniques that can accurately detect various components of tumor tissue. Another challenge in this approach lies in the appropriate design of features that can represent the full complexity of the pathological phenomena.</p><p id="p0365">The third category involves image generation techniques. These include methods such as activation maximization <xref rid="bib237" ref-type="bibr">[237]</xref>, which generates tissue images that maximize the prediction probability for a given class, or approaches using generative models, such as CycleGAN or diffusion modes, to generate counterfactual images <xref rid="bib238" ref-type="bibr">[238]</xref>, <xref rid="bib239" ref-type="bibr">[239]</xref>, <xref rid="bib240" ref-type="bibr">[240]</xref>. For example, &#x0017d;igutyt&#x00117; <italic>et al</italic>. developed a method using a diffusion autoencoder to generate counterfactual pathology images across different types of cancer, including colorectal, liver, lung, and breast. Their method can generate images that show varying combinations of two classifications&#x02014;for example, blending normal and cancerous tissue, squamous cell carcinoma with adenocarcinoma, or microsatellite instability-high with non-microsatellite instability-high cases <xref rid="bib240" ref-type="bibr">[240]</xref>. Unlike these approaches, HIPPO <xref rid="bib241" ref-type="bibr">[241]</xref> iteratively exchanges patches to generate a counterfactual patch set based on the classifiers output in order to capture important regions for classification.</p><p id="p0370">Although these methods do not directly explain the decision-making process and rely heavily on the interpretive skills of pathologists, they can extract findings that go beyond specific image regions or verbalized features.</p><p id="p0375">Another emerging and promising approach in this field is the use of vision&#x02013;language models, such as PathAsst <xref rid="bib242" ref-type="bibr">[242]</xref> and PathChat <xref rid="bib60" ref-type="bibr">[60]</xref>, which represent images through natural language descriptions. This method leverages the power of large language models to provide detailed, contextual descriptions of pathological images. By translating visual information into textual form, these models can bridge the gap between complex image data and human-interpretable insights. This approach depends on how pathologists traditionally communicate their findings and could therefore facilitate a more intuitive interaction with AI-assisted diagnostic systems. Notably, the ability of these models to identify novel features or patterns that have been unrecognized by pathologists remains to be validated. Moreover, these models can potentially generate hallucinations, producing plausible but inaccurate descriptions of pathological features. Although its potential is significant, careful evaluation is required to ensure both the reliability of these language-based interpretations and their ability to extend beyond known pathological knowledge, potentially contributing to new discoveries in the field.</p></sec><sec id="sec0095"><label>4.6</label><title>Decision-making in pathological diagnosis</title><p id="p0380">Most pathology AI technologies aim to make diagnoses or predictions from pathological images. However, the actual process of pathological diagnosis is significantly more complex. Additional tests, such as immunohistochemistry or FISH, may be necessary to narrow down the diagnosis. In some cases, it is important to review past specimens to track changes over time. Given this context, there is a growing need for AI technologies that can indicate additional tests or highlight important information to confirm diagnoses <xref rid="bib60" ref-type="bibr">[60]</xref>. Such decision-making processes require not only histological knowledge but also consideration of a hospital&#x02019;s medical resources and related costs as well as past clinical tests and medical records. In addition, it is crucial to engage in thorough discussions with other clinicians to gather diverse perspectives and expertise, ensuring a comprehensive approach to patient care and treatment planning. Therefore, a wide range of medical knowledge beyond diagnostic pathology is necessary <xref rid="bib243" ref-type="bibr">[243]</xref>. Future research should focus on seamlessly integrating AI-assisted analysis tools into clinical pathology workflows to complement and enhance, rather than replace, human expertise. AI agents that can access various data sources and tools to automatically perform tasks based on text instructions are becoming increasingly popular, including in the field of pathology, as exemplified by systems such as Judith <xref rid="bib244" ref-type="bibr">[244]</xref>. By addressing these challenges and exploring new directions, digital pathology image analysis can significantly impact clinical practice, enhance our understanding of disease mechanisms, and ultimately improve patient care.</p><p id="p0385">In actual clinical settings, treatment decisions and other choices are made by integrating data from multiple modalities. In the future, it is desirable to develop decision support systems that integrate multimodal information to assist in decision making.</p></sec></sec><sec id="sec0100"><label>5</label><title>Summary</title><p id="p0390">This review discusses various challenges and approaches in pathological image analysis. Since 2018, numerous methodologies have been developed, showing clear gradual improvement in the field. However, the emergence of new modalities, such as spatial transcriptomics, has expanded the applications of pathological image analysis, consequently bringing new technical challenges to the forefront.</p><p id="p0395">Although research in 2018 primarily focused on basic research aspects, recent years have seen an increase in studies oriented toward clinical applications. This shift has raised the bar for the required accuracy levels. Although some challenges remain unresolved, the technology continues to evolve.</p><p id="p0400">The review concludes that the contribution of pathological image analysis technologies to diagnosis, prediction, and treatment is expected to increase.</p></sec><sec id="sec0105"><title>Funding</title><p id="p0405">This study was supported by <funding-source id="spnsr1">JSPS KAKENHI</funding-source> Grant-in-Aid for Scientific Research (B) under Grant Number <award-id award-type="grant" rid="spnsr1">21H03836</award-id> to D.K, <funding-source id="spnsr2">The AMED Practical Research for Innovative Cancer Control</funding-source> under grant number <award-id award-type="grant" rid="spnsr2">JP 24ck0106873</award-id>, and The AMED Practical Research for Innovative Cancer Control under grant number JP 24ck0106904 to S.I.</p></sec><sec id="sec0110"><title>CRediT authorship contribution statement</title><p id="p0410"><bold>Mieko Ochi:</bold> Writing &#x02013; review &#x00026; editing, Visualization. <bold>Shumpei Ishikawa:</bold> Writing &#x02013; review &#x00026; editing, Supervision. <bold>Daisuke Komura:</bold> Writing &#x02013; original draft, Visualization, Investigation, Conceptualization.</p></sec><sec id="sec0115"><title>Author contributions</title><p id="p0415">Conceptualization: DK; Investigation: DK; Supervision: SI; Visualization: DK, MO; Writing-original draft: DK; Writing-review &#x00026; editing: MO, SI.</p></sec><sec id="sec0120"><title>Declaration of Generative AI and AI-assisted technologies in the writing process</title><p id="p0420">During the preparation of this work, the authors used Claude AI to improve language. After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication.</p></sec><sec sec-type="COI-statement" id="coi0005"><title>Declaration of Competing Interest</title><p id="p0425">The authors declare no competing interests.</p></sec></body><back><ref-list id="bibliog0005"><title>References</title><ref id="bib1"><label>1</label><element-citation publication-type="journal" id="sbref1"><person-group person-group-type="author"><name><surname>Ehteshami Bejnordi</surname><given-names>B.</given-names></name><name><surname>Veta</surname><given-names>M.</given-names></name><name><surname>Johannes van Diest</surname><given-names>P.</given-names></name><name><surname>van Ginneken</surname><given-names>B.</given-names></name><name><surname>Karssemeijer</surname><given-names>N.</given-names></name><name><surname>Litjens</surname><given-names>G.</given-names></name><etal/></person-group><article-title>Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</article-title><source>JAMA</source><volume>318</volume><year>2017</year><fpage>2199</fpage><lpage>2210</lpage><pub-id pub-id-type="doi">10.1001/jama.2017.14585</pub-id><pub-id pub-id-type="pmid">29234806</pub-id>
</element-citation></ref><ref id="bib2"><label>2</label><element-citation publication-type="journal" id="sbref2"><person-group person-group-type="author"><name><surname>Campanella</surname><given-names>G.</given-names></name><name><surname>Hanna</surname><given-names>M.G.</given-names></name><name><surname>Geneslaw</surname><given-names>L.</given-names></name><name><surname>Miraflor</surname><given-names>A.</given-names></name><name><surname>Werneck Krauss Silva</surname><given-names>V.</given-names></name><name><surname>Busam</surname><given-names>K.J.</given-names></name><etal/></person-group><article-title>Clinical-grade computational pathology using weakly supervised deep learning on whole slide images</article-title><source>Nat Med</source><volume>25</volume><year>2019</year><fpage>1301</fpage><lpage>1309</lpage><pub-id pub-id-type="doi">10.1038/s41591-019-0508-1</pub-id><pub-id pub-id-type="pmid">31308507</pub-id>
</element-citation></ref><ref id="bib3"><label>3</label><element-citation publication-type="journal" id="sbref3"><person-group person-group-type="author"><name><surname>Hewitt</surname><given-names>K.J.</given-names></name><name><surname>L&#x000f6;ffler</surname><given-names>C.M.L.</given-names></name><name><surname>Muti</surname><given-names>H.S.</given-names></name><name><surname>Berghoff</surname><given-names>A.S.</given-names></name><name><surname>Eisenl&#x000f6;ffel</surname><given-names>C.</given-names></name><name><surname>van Treeck</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Direct image to subtype prediction for brain tumors using deep learning</article-title><source>Neuro-Oncol Adv</source><volume>5</volume><year>2023</year><object-id pub-id-type="publisher-id">vdad139</object-id><pub-id pub-id-type="doi">10.1093/noajnl/vdad139</pub-id></element-citation></ref><ref id="bib4"><label>4</label><element-citation publication-type="journal" id="sbref4"><person-group person-group-type="author"><name><surname>Li</surname><given-names>D.</given-names></name><name><surname>Bledsoe</surname><given-names>J.R.</given-names></name><name><surname>Zeng</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Hu</surname><given-names>Y.</given-names></name><name><surname>Bi</surname><given-names>K.</given-names></name><etal/></person-group><article-title>A deep learning diagnostic platform for diffuse large B-cell lymphoma with high accuracy across multiple hospitals</article-title><source>Nat Commun</source><volume>11</volume><year>2020</year><fpage>6004</fpage><pub-id pub-id-type="doi">10.1038/s41467-020-19817-3</pub-id><pub-id pub-id-type="pmid">33244018</pub-id>
</element-citation></ref><ref id="bib5"><label>5</label><element-citation publication-type="journal" id="sbref5"><person-group person-group-type="author"><name><surname>Steinbuss</surname><given-names>G.</given-names></name><name><surname>Kriegsmann</surname><given-names>M.</given-names></name><name><surname>Zgorzelski</surname><given-names>C.</given-names></name><name><surname>Brobeil</surname><given-names>A.</given-names></name><name><surname>Goeppert</surname><given-names>B.</given-names></name><name><surname>Dietrich</surname><given-names>S.</given-names></name><etal/></person-group><article-title>Deep learning for the classification of non-hodgkin lymphoma on histopathological images</article-title><source>Cancers</source><volume>13</volume><year>2021</year><fpage>2419</fpage><pub-id pub-id-type="doi">10.3390/cancers13102419</pub-id><pub-id pub-id-type="pmid">34067726</pub-id>
</element-citation></ref><ref id="bib6"><label>6</label><element-citation publication-type="journal" id="sbref6"><person-group person-group-type="author"><name><surname>Wulczyn</surname><given-names>E.</given-names></name><name><surname>Steiner</surname><given-names>D.F.</given-names></name><name><surname>Moran</surname><given-names>M.</given-names></name><name><surname>Plass</surname><given-names>M.</given-names></name><name><surname>Reihs</surname><given-names>R.</given-names></name><name><surname>Tan</surname><given-names>F.</given-names></name><etal/></person-group><article-title>Interpretable survival prediction for colorectal cancer using deep learning</article-title><source>Npj Digit Med</source><volume>4</volume><year>2021</year><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41746-021-00427-2</pub-id><pub-id pub-id-type="pmid">33398041</pub-id>
</element-citation></ref><ref id="bib7"><label>7</label><element-citation publication-type="journal" id="sbref7"><person-group person-group-type="author"><name><surname>Yamamoto</surname><given-names>Y.</given-names></name><name><surname>Tsuzuki</surname><given-names>T.</given-names></name><name><surname>Akatsuka</surname><given-names>J.</given-names></name><name><surname>Ueki</surname><given-names>M.</given-names></name><name><surname>Morikawa</surname><given-names>H.</given-names></name><name><surname>Numata</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>Automated acquisition of explainable knowledge from unannotated histopathology images</article-title><source>Nat Commun</source><volume>10</volume><year>2019</year><fpage>5642</fpage><pub-id pub-id-type="doi">10.1038/s41467-019-13647-8</pub-id><pub-id pub-id-type="pmid">31852890</pub-id>
</element-citation></ref><ref id="bib8"><label>8</label><mixed-citation publication-type="other" id="othref0005">Yu K.-H., Zhang C., Berry G.J., Altman R.B., R&#x000e9; C., Rubin D.L., et al. Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features. Nat Commun 2016;7:12474. https://doi.org/10.1038/ncomms12474.</mixed-citation></ref><ref id="bib9"><label>9</label><element-citation publication-type="journal" id="sbref8"><person-group person-group-type="author"><name><surname>Komura</surname><given-names>D.</given-names></name><name><surname>Ishikawa</surname><given-names>S.</given-names></name></person-group><article-title>Machine learning methods for histopathological image analysis</article-title><source>Comput Struct Biotechnol J</source><volume>16</volume><year>2018</year><fpage>34</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1016/j.csbj.2018.01.001</pub-id><pub-id pub-id-type="pmid">30275936</pub-id>
</element-citation></ref><ref id="bib10"><label>10</label><element-citation publication-type="journal" id="sbref9"><person-group person-group-type="author"><name><surname>Hacking</surname><given-names>S.M.</given-names></name><name><surname>Wu</surname><given-names>D.</given-names></name><name><surname>Alexis</surname><given-names>C.</given-names></name><name><surname>Nasim</surname><given-names>M.</given-names></name></person-group><article-title>A novel superpixel approach to the tumoral microenvironment in colorectal cancer</article-title><source>J Pathol Inf</source><volume>13</volume><year>2022</year><object-id pub-id-type="publisher-id">100009</object-id><pub-id pub-id-type="doi">10.1016/j.jpi.2022.100009</pub-id></element-citation></ref><ref id="bib11"><label>11</label><element-citation publication-type="journal" id="sbref10"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>S.</given-names></name><name><surname>Amgad</surname><given-names>M.</given-names></name><name><surname>Mobadersany</surname><given-names>P.</given-names></name><name><surname>McCormick</surname><given-names>M.</given-names></name><name><surname>Pollack</surname><given-names>B.P.</given-names></name><name><surname>Elfandy</surname><given-names>H.</given-names></name><etal/></person-group><article-title>Interactive classification of whole-slide imaging data for cancer researchers</article-title><source>Cancer Res</source><volume>81</volume><year>2021</year><fpage>1171</fpage><lpage>1177</lpage><pub-id pub-id-type="doi">10.1158/0008-5472.CAN-20-0668</pub-id><pub-id pub-id-type="pmid">33355190</pub-id>
</element-citation></ref><ref id="bib12"><label>12</label><element-citation publication-type="journal" id="sbref11"><person-group person-group-type="author"><name><surname>Vanea</surname><given-names>C.</given-names></name><name><surname>D&#x0017e;igurski</surname><given-names>J.</given-names></name><name><surname>Rukins</surname><given-names>V.</given-names></name><name><surname>Dodi</surname><given-names>O.</given-names></name><name><surname>Siigur</surname><given-names>S.</given-names></name><name><surname>Salum&#x000e4;e</surname><given-names>L.</given-names></name><etal/></person-group><article-title>Mapping cell-to-tissue graphs across human placenta histology whole slide images using deep learning with HAPPY</article-title><source>Nat Commun</source><volume>15</volume><year>2024</year><fpage>2710</fpage><pub-id pub-id-type="doi">10.1038/s41467-024-46986-2</pub-id><pub-id pub-id-type="pmid">38548713</pub-id>
</element-citation></ref><ref id="bib13"><label>13</label><element-citation publication-type="book" id="sbref12"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>T.H.</given-names></name><name><surname>Cendra</surname><given-names>F.J.</given-names></name><name><surname>Ma</surname><given-names>L.</given-names></name><name><surname>Yin</surname><given-names>G.</given-names></name><name><surname>Yu</surname><given-names>L.</given-names></name></person-group><part-title>Histopathology Whole Slide Image Analysis with Heterogeneous Graph Representation Learning</part-title><series>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</series><year>2023</year><publisher-name>IEEE</publisher-name><publisher-loc>Vancouver, BC, Canada</publisher-loc><fpage>15661</fpage><lpage>15670</lpage><pub-id pub-id-type="doi">10.1109/CVPR52729.2023.01503</pub-id></element-citation></ref><ref id="bib14"><label>14</label><element-citation publication-type="journal" id="sbref13"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Y.</given-names></name><name><surname>Park</surname><given-names>J.H.</given-names></name><name><surname>Oh</surname><given-names>S.</given-names></name><name><surname>Shin</surname><given-names>K.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name><name><surname>Jung</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Derivation of prognostic contextual histopathological features from whole-slide images of tumours via graph deep learning</article-title><source>Nat Biomed Eng</source><year>2022</year><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1038/s41551-022-00923-0</pub-id><pub-id pub-id-type="pmid">35064246</pub-id>
</element-citation></ref><ref id="bib15"><label>15</label><element-citation publication-type="journal" id="sbref14"><person-group person-group-type="author"><name><surname>Otsu</surname><given-names>N.</given-names></name></person-group><article-title>A threshold selection method from gray-level histograms</article-title><source>IEEE Trans Syst Man Cyber</source><volume>9</volume><year>1979</year><fpage>62</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id></element-citation></ref><ref id="bib16"><label>16</label><element-citation publication-type="journal" id="sbref15"><person-group person-group-type="author"><name><surname>Mu&#x000f1;oz-Aguirre</surname><given-names>M.</given-names></name><name><surname>Ntasis</surname><given-names>V.F.</given-names></name><name><surname>Rojas</surname><given-names>S.</given-names></name><name><surname>Guig&#x000f3;</surname><given-names>R.</given-names></name></person-group><article-title>PyHIST: a histological image segmentation tool</article-title><source>PLOS Comput Biol</source><volume>16</volume><year>2020</year><object-id pub-id-type="publisher-id">e1008349</object-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008349</pub-id></element-citation></ref><ref id="bib17"><label>17</label><element-citation publication-type="journal" id="sbref16"><person-group person-group-type="author"><name><surname>Bankhead</surname><given-names>P.</given-names></name><name><surname>Loughrey</surname><given-names>M.B.</given-names></name><name><surname>Fern&#x000e1;ndez</surname><given-names>J.A.</given-names></name><name><surname>Dombrowski</surname><given-names>Y.</given-names></name><name><surname>McArt</surname><given-names>D.G.</given-names></name><name><surname>Dunne</surname><given-names>P.D.</given-names></name><etal/></person-group><article-title>QuPath: open source software for digital pathology image analysis</article-title><source>Sci Rep</source><volume>7</volume><year>2017</year><object-id pub-id-type="publisher-id">16878</object-id><pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id></element-citation></ref><ref id="bib18"><label>18</label><element-citation publication-type="journal" id="sbref17"><person-group person-group-type="author"><name><surname>Rubens</surname><given-names>U.</given-names></name><name><surname>Hoyoux</surname><given-names>R.</given-names></name><name><surname>Vanosmael</surname><given-names>L.</given-names></name><name><surname>Ouras</surname><given-names>M.</given-names></name><name><surname>Tasset</surname><given-names>M.</given-names></name><name><surname>Hamilton</surname><given-names>C.</given-names></name><name><surname>Longuesp&#x000e9;e</surname><given-names>R.</given-names></name><name><surname>Mar&#x000e9;e</surname><given-names>R.</given-names></name></person-group><article-title>Cytomine: Toward an Open and Collaborative Software Platform for Digital Pathology Bridged to Molecular Investigations</article-title><source>Proteomics Clin Appl.</source><volume>13</volume><issue>1</issue><year>2019</year><object-id pub-id-type="publisher-id">e1800057</object-id><pub-id pub-id-type="doi">10.1002/prca.201800057</pub-id><comment>Epub 2018 Dec 19. PMID: 30520559.</comment></element-citation></ref><ref id="bib19"><label>19</label><element-citation publication-type="journal" id="sbref18"><person-group person-group-type="author"><name><surname>Yamashita</surname><given-names>R.</given-names></name><name><surname>Long</surname><given-names>J.</given-names></name><name><surname>Longacre</surname><given-names>T.</given-names></name><name><surname>Peng</surname><given-names>L.</given-names></name><name><surname>Berry</surname><given-names>G.</given-names></name><name><surname>Martin</surname><given-names>B.</given-names></name><etal/></person-group><article-title>Deep learning model for the prediction of microsatellite instability in colorectal cancer: a diagnostic study</article-title><source>Lancet Oncol</source><volume>22</volume><year>2021</year><fpage>132</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1016/S1470-2045(20)30535-0</pub-id><pub-id pub-id-type="pmid">33387492</pub-id>
</element-citation></ref><ref id="bib20"><label>20</label><element-citation publication-type="journal" id="sbref19"><person-group person-group-type="author"><name><surname>Kather</surname><given-names>J.N.</given-names></name><name><surname>Pearson</surname><given-names>A.T.</given-names></name><name><surname>Halama</surname><given-names>N.</given-names></name><name><surname>J&#x000e4;ger</surname><given-names>D.</given-names></name><name><surname>Krause</surname><given-names>J.</given-names></name><name><surname>Loosen</surname><given-names>S.H.</given-names></name><etal/></person-group><article-title>Deep learning can predict microsatellite instability directly from histology in gastrointestinal cancer</article-title><source>Nat Med</source><volume>25</volume><year>2019</year><fpage>1054</fpage><lpage>1056</lpage><pub-id pub-id-type="doi">10.1038/s41591-019-0462-y</pub-id><pub-id pub-id-type="pmid">31160815</pub-id>
</element-citation></ref><ref id="bib21"><label>21</label><element-citation publication-type="journal" id="sbref20"><person-group person-group-type="author"><name><surname>Kanwal</surname><given-names>N.</given-names></name><name><surname>L&#x000f3;pez-P&#x000e9;rez</surname><given-names>M.</given-names></name><name><surname>Kiraz</surname><given-names>U.</given-names></name><name><surname>Zuiverloon</surname><given-names>T.C.M.</given-names></name><name><surname>Molina</surname><given-names>R.</given-names></name><name><surname>Engan</surname><given-names>K.</given-names></name></person-group><article-title>Are you sure it&#x02019;s an artifact? Artifact detection and uncertainty quantification in histological images</article-title><source>Comput Med Imaging Graph</source><volume>112</volume><year>2024</year><object-id pub-id-type="publisher-id">102321</object-id><pub-id pub-id-type="doi">10.1016/j.compmedimag.2023.102321</pub-id></element-citation></ref><ref id="bib22"><label>22</label><mixed-citation publication-type="other" id="othref0010">Wu H., Phan J.H., Bhatia A.K., Shehata B., Wang M.D. Detection of Blur Artifacts in Histopathological Whole-Slide Images of Endomyocardial Biopsies. Conf Proc Annu Int Conf IEEE Eng Med Biol Soc IEEE Eng Med Biol Soc Annu Conf 2015;2015:727&#x02013;730. <pub-id pub-id-type="doi">10.1109/EMBC.2015.7318465</pub-id>.</mixed-citation></ref><ref id="bib23"><label>23</label><element-citation publication-type="journal" id="sbref21"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>H.</given-names></name><name><surname>Xu</surname><given-names>Q.</given-names></name><name><surname>Cong</surname><given-names>F.</given-names></name><name><surname>Kang</surname><given-names>J.</given-names></name><name><surname>Han</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>Z.</given-names></name><etal/></person-group><article-title>Vision transformers for computational histopathology</article-title><source>IEEE Rev Biomed Eng</source><volume>17</volume><year>2024</year><fpage>63</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1109/RBME.2023.3297604</pub-id><pub-id pub-id-type="pmid">37478035</pub-id>
</element-citation></ref><ref id="bib24"><label>24</label><element-citation publication-type="journal" id="sbref22"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Ding</surname><given-names>T.</given-names></name><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Jaume</surname><given-names>G.</given-names></name><name><surname>Song</surname><given-names>A.H.</given-names></name><etal/></person-group><article-title>Towards a general-purpose foundation model for computational pathology</article-title><source>Nat Med</source><volume>30</volume><year>2024</year><fpage>850</fpage><lpage>862</lpage><pub-id pub-id-type="doi">10.1038/s41591-024-02857-3</pub-id><pub-id pub-id-type="pmid">38504018</pub-id>
</element-citation></ref><ref id="bib25"><label>25</label><element-citation publication-type="journal" id="sbref23"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Cong</surname><given-names>Y.</given-names></name><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Qi</surname><given-names>J.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name><name><surname>Yan</surname><given-names>T.</given-names></name><etal/></person-group><article-title>Vision transformer-based weakly supervised histopathological image analysis of primary brain tumors</article-title><source>iScience</source><volume>26</volume><year>2023</year><pub-id pub-id-type="doi">10.1016/j.isci.2022.105872</pub-id></element-citation></ref><ref id="bib26"><label>26</label><mixed-citation publication-type="other" id="othref0015">Shao Z., Bian H., Chen Y., Wang Y., Zhang J., Ji X., et al. TransMIL: transformer based correlated multiple instance learning for whole slide image classification. Proc. 35th Int. Conf. Neural Inf. Process. Syst., Red Hook, NY, USA: Curran Associates Inc.; 2024, p. 2136&#x02013;2147.</mixed-citation></ref><ref id="bib27"><label>27</label><element-citation publication-type="journal" id="sbref24"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>T.</given-names></name><name><surname>Wagner</surname><given-names>S.J.</given-names></name><name><surname>Song</surname><given-names>A.H.</given-names></name><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Zhang</surname><given-names>A.</given-names></name><name><surname>Vaidya</surname><given-names>A.J.</given-names></name><name><surname>Jaume</surname><given-names>G.</given-names></name><name><surname>Shaban</surname><given-names>M.</given-names></name><name><surname>Kim</surname><given-names>A.</given-names></name><name><surname>Williamson</surname><given-names>D.F.</given-names></name></person-group><article-title>Multimodal whole slide foundation model for pathology</article-title><source>arXiv preprint arXiv</source><year>2024</year><object-id pub-id-type="publisher-id">2411.19666</object-id></element-citation></ref><ref id="bib28"><label>28</label><element-citation publication-type="journal" id="sbref25"><person-group person-group-type="author"><name><surname>Shaikovski</surname><given-names>G.</given-names></name><name><surname>Casson</surname><given-names>A.</given-names></name><name><surname>Severson</surname><given-names>K.</given-names></name><name><surname>Zimmermann</surname><given-names>E.</given-names></name><name><surname>Wang</surname><given-names>Y.K.</given-names></name><name><surname>Kunz</surname><given-names>J.D.</given-names></name><name><surname>Retamero</surname><given-names>J.A.</given-names></name><name><surname>Oakley</surname><given-names>G.</given-names></name><name><surname>Klimstra</surname><given-names>D.</given-names></name><name><surname>Kanan</surname><given-names>C.</given-names></name><name><surname>Hanna</surname><given-names>M.</given-names></name></person-group><article-title>PRISM: A Multi-Modal Generative Foundation Model for Slide-Level Histopathology</article-title><source>arXiv preprint arXiv</source><year>2024</year><object-id pub-id-type="publisher-id">2405.10254</object-id></element-citation></ref><ref id="bib29"><label>29</label><element-citation publication-type="journal" id="sbref26"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>H.</given-names></name><name><surname>Usuyama</surname><given-names>N.</given-names></name><name><surname>Bagga</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>S.</given-names></name><name><surname>Rao</surname><given-names>R.</given-names></name><name><surname>Naumann</surname><given-names>T.</given-names></name><etal/></person-group><article-title>A whole-slide foundation model for digital pathology from real-world data</article-title><source>Nature</source><volume>630</volume><year>2024</year><fpage>181</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-07441-w</pub-id><pub-id pub-id-type="pmid">38778098</pub-id>
</element-citation></ref><ref id="bib30"><label>30</label><mixed-citation publication-type="other" id="othref0020">Li Z., Jiang Y., Liu L., Xia Y., Li R. Single-Cell Spatial Analysis of Histopathology Images for Survival Prediction via Graph Attention Network. In: Wu S., Shabestari B., Xing L., editors. Appl. Med. Artif. Intell., Cham: Springer Nature Switzerland; 2024, p. 114&#x02013;124. https://doi.org/10.1007/978-3-031-47076-9_12.</mixed-citation></ref><ref id="bib31"><label>31</label><element-citation publication-type="journal" id="sbref27"><person-group person-group-type="author"><name><surname>Abbas</surname><given-names>S.F.</given-names></name><name><surname>Vuong</surname><given-names>T.T.L.</given-names></name><name><surname>Kim</surname><given-names>K.</given-names></name><name><surname>Song</surname><given-names>B.</given-names></name><name><surname>Kwak</surname><given-names>J.T.</given-names></name></person-group><article-title>Multi-cell type and multi-level graph aggregation network for cancer grading in pathology images</article-title><source>Med Image Anal</source><volume>90</volume><year>2023</year><object-id pub-id-type="publisher-id">102936</object-id><pub-id pub-id-type="doi">10.1016/j.media.2023.102936</pub-id></element-citation></ref><ref id="bib32"><label>32</label><element-citation publication-type="journal" id="sbref28"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Liang</surname><given-names>I.</given-names></name><name><surname>Ding</surname><given-names>T.</given-names></name><etal/></person-group><article-title>A visual-language foundation model for computational pathology</article-title><source>Nat Med</source><volume>30</volume><year>2024</year><fpage>863</fpage><lpage>874</lpage><pub-id pub-id-type="doi">10.1038/s41591-024-02856-4</pub-id><pub-id pub-id-type="pmid">38504017</pub-id>
</element-citation></ref><ref id="bib33"><label>33</label><mixed-citation publication-type="other" id="othref0025">Komura, D., Takao, M., Ochi, M., Onoyama, T., Katoh, H., Abe, H., Sano, H., Konishi, T., Kumasaka, T., Yokose, T., Miyagi, Y., Ushiku, T., &#x00026; Ishikawa, S. (2025). Comprehensive Pathological Image Segmentation via Teacher Aggregation for Tumor Microenvironment Analysis. arXiv:2501.02909.</mixed-citation></ref><ref id="bib34"><label>34</label><element-citation publication-type="journal" id="sbref29"><person-group person-group-type="author"><name><surname>Foersch</surname><given-names>S.</given-names></name><name><surname>Eckstein</surname><given-names>M.</given-names></name><name><surname>Wagner</surname><given-names>D.-C.</given-names></name><name><surname>Gach</surname><given-names>F.</given-names></name><name><surname>Woerl</surname><given-names>A.-C.</given-names></name><name><surname>Geiger</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Deep learning for diagnosis and survival prediction in soft tissue sarcoma</article-title><source>Ann Oncol J Eur Soc Med Oncol</source><volume>32</volume><year>2021</year><fpage>1178</fpage><lpage>1187</lpage><pub-id pub-id-type="doi">10.1016/j.annonc.2021.06.007</pub-id></element-citation></ref><ref id="bib35"><label>35</label><element-citation publication-type="journal" id="sbref30"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>S.</given-names></name><name><surname>Rudzinski</surname><given-names>E.R.</given-names></name><name><surname>Agarwal</surname><given-names>S.</given-names></name><name><surname>Rong</surname><given-names>R.</given-names></name><name><surname>Barkauskas</surname><given-names>D.A.</given-names></name><etal/></person-group><article-title>Deep learning of rhabdomyosarcoma pathology images for classification and survival outcome prediction</article-title><source>Am J Pathol</source><volume>192</volume><year>2022</year><fpage>917</fpage><lpage>925</lpage><pub-id pub-id-type="doi">10.1016/j.ajpath.2022.03.011</pub-id><pub-id pub-id-type="pmid">35390316</pub-id>
</element-citation></ref><ref id="bib36"><label>36</label><element-citation publication-type="journal" id="sbref31"><person-group person-group-type="author"><name><surname>Hoang</surname><given-names>D.-T.</given-names></name><name><surname>Shulman</surname><given-names>E.D.</given-names></name><name><surname>Turakulov</surname><given-names>R.</given-names></name><name><surname>Abdullaev</surname><given-names>Z.</given-names></name><name><surname>Singh</surname><given-names>O.</given-names></name><name><surname>Campagnolo</surname><given-names>E.M.</given-names></name><etal/></person-group><article-title>Prediction of DNA methylation-based tumor types from histopathology in central nervous system tumors with deep learning</article-title><source>Nat Med</source><volume>30</volume><year>2024</year><fpage>1952</fpage><lpage>1961</lpage><pub-id pub-id-type="doi">10.1038/s41591-024-02995-8</pub-id><pub-id pub-id-type="pmid">38760587</pub-id>
</element-citation></ref><ref id="bib37"><label>37</label><mixed-citation publication-type="other" id="othref0030">Bulten W., Kartasalo K., Chen P.-H.C., Str&#x000f6;m P., Pinckaers H., Nagpal K., et al. Artificial intelligence for diagnosis and Gleason grading of prostate cancer: the PANDA challenge. Nat Med 2022;28:154&#x02013;163. https://doi.org/10.1038/s41591-021-01620-2.</mixed-citation></ref><ref id="bib38"><label>38</label><element-citation publication-type="journal" id="sbref32"><person-group person-group-type="author"><name><surname>Dacic</surname><given-names>S.</given-names></name><name><surname>Travis</surname><given-names>W.D.</given-names></name><name><surname>Giltnane</surname><given-names>J.M.</given-names></name><name><surname>Kos</surname><given-names>F.</given-names></name><name><surname>Abel</surname><given-names>J.</given-names></name><name><surname>Hilz</surname><given-names>S.</given-names></name><etal/></person-group><article-title>Artificial intelligence&#x02013;powered assessment of pathologic response to neoadjuvant atezolizumab in patients with NSCLC: results from the LCMC3 study</article-title><source>J Thorac Oncol</source><volume>19</volume><year>2024</year><fpage>719</fpage><lpage>731</lpage><pub-id pub-id-type="doi">10.1016/j.jtho.2023.12.010</pub-id><pub-id pub-id-type="pmid">38070597</pub-id>
</element-citation></ref><ref id="bib39"><label>39</label><element-citation publication-type="journal" id="sbref33"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Chen</surname><given-names>T.Y.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Zhao</surname><given-names>M.</given-names></name><name><surname>Shady</surname><given-names>M.</given-names></name><name><surname>Lipkova</surname><given-names>J.</given-names></name><etal/></person-group><article-title>AI-based pathology predicts origins for cancers of unknown primary</article-title><source>Nature</source><volume>594</volume><year>2021</year><fpage>106</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03512-4</pub-id><pub-id pub-id-type="pmid">33953404</pub-id>
</element-citation></ref><ref id="bib40"><label>40</label><element-citation publication-type="journal" id="sbref34"><person-group person-group-type="author"><name><surname>Bejnordi</surname><given-names>B.E.</given-names></name><name><surname>Veta</surname><given-names>M.</given-names></name><name><surname>Diest</surname><given-names>P.J. van</given-names></name><name><surname>Ginneken</surname><given-names>B. van</given-names></name><name><surname>Karssemeijer</surname><given-names>N.</given-names></name><name><surname>Litjens</surname><given-names>G.</given-names></name><etal/></person-group><article-title>Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</article-title><source>JAMA</source><volume>318</volume><year>2017</year><fpage>2199</fpage><lpage>2210</lpage><pub-id pub-id-type="doi">10.1001/jama.2017.14585</pub-id><pub-id pub-id-type="pmid">29234806</pub-id>
</element-citation></ref><ref id="bib41"><label>41</label><element-citation publication-type="journal" id="sbref35"><person-group person-group-type="author"><name><surname>Bandi</surname><given-names>P.</given-names></name><name><surname>Geessink</surname><given-names>O.</given-names></name><name><surname>Manson</surname><given-names>Q.</given-names></name><name><surname>Van Dijk</surname><given-names>M.</given-names></name><name><surname>Balkenhol</surname><given-names>M.</given-names></name><name><surname>Hermsen</surname><given-names>M.</given-names></name><etal/></person-group><article-title>From detection of individual metastases to classification of lymph node status at the patient level: the CAMELYON17 challenge</article-title><source>IEEE Trans Med Imaging</source><volume>38</volume><year>2019</year><fpage>550</fpage><lpage>560</lpage><pub-id pub-id-type="doi">10.1109/TMI.2018.2867350</pub-id><pub-id pub-id-type="pmid">30716025</pub-id>
</element-citation></ref><ref id="bib42"><label>42</label><element-citation publication-type="journal" id="sbref36"><person-group person-group-type="author"><name><surname>Bilal</surname><given-names>M.</given-names></name><name><surname>Tsang</surname><given-names>Y.W.</given-names></name><name><surname>Ali</surname><given-names>M.</given-names></name><name><surname>Graham</surname><given-names>S.</given-names></name><name><surname>Hero</surname><given-names>E.</given-names></name><name><surname>Wahab</surname><given-names>N.</given-names></name><etal/></person-group><article-title>Development and validation of artificial intelligence-based prescreening of large-bowel biopsies taken in the UK and Portugal: a retrospective cohort study</article-title><source>Lancet Digit Health</source><volume>5</volume><year>2023</year><fpage>e786</fpage><lpage>e797</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(23)00148-6</pub-id><pub-id pub-id-type="pmid">37890902</pub-id>
</element-citation></ref><ref id="bib43"><label>43</label><element-citation publication-type="journal" id="sbref37"><person-group person-group-type="author"><name><surname>Tolkach</surname><given-names>Y.</given-names></name><name><surname>Wolgast</surname><given-names>L.M.</given-names></name><name><surname>Damanakis</surname><given-names>A.</given-names></name><name><surname>Pryalukhin</surname><given-names>A.</given-names></name><name><surname>Schallenberg</surname><given-names>S.</given-names></name><name><surname>Hulla</surname><given-names>W.</given-names></name><etal/></person-group><article-title>Artificial intelligence for tumour tissue detection and histological regression grading in oesophageal adenocarcinomas: a retrospective algorithm development and validation study</article-title><source>Lancet Digit Health</source><volume>5</volume><year>2023</year><fpage>e265</fpage><lpage>e275</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(23)00027-4</pub-id><pub-id pub-id-type="pmid">37100542</pub-id>
</element-citation></ref><ref id="bib44"><label>44</label><element-citation publication-type="journal" id="sbref38"><person-group person-group-type="author"><name><surname>Kers</surname><given-names>J.</given-names></name><name><surname>B&#x000fc;low</surname><given-names>R.D.</given-names></name><name><surname>Klinkhammer</surname><given-names>B.M.</given-names></name><name><surname>Breimer</surname><given-names>G.E.</given-names></name><name><surname>Fontana</surname><given-names>F.</given-names></name><name><surname>Abiola</surname><given-names>A.A.</given-names></name><etal/></person-group><article-title>Deep learning-based classification of kidney transplant pathology: a retrospective, multicentre, proof-of-concept study</article-title><source>Lancet Digit Health</source><volume>4</volume><year>2022</year><fpage>e18</fpage><lpage>e26</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(21)00211-9</pub-id><pub-id pub-id-type="pmid">34794930</pub-id>
</element-citation></ref><ref id="bib45"><label>45</label><element-citation publication-type="journal" id="sbref39"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Zhao</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>R.</given-names></name><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Zheng</surname><given-names>X.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><etal/></person-group><article-title>A multicenter proof-of-concept study on deep learning-based intraoperative discrimination of primary central nervous system lymphoma</article-title><source>Nat Commun</source><volume>15</volume><year>2024</year><fpage>3768</fpage><pub-id pub-id-type="doi">10.1038/s41467-024-48171-x</pub-id><pub-id pub-id-type="pmid">38704409</pub-id>
</element-citation></ref><ref id="bib46"><label>46</label><element-citation publication-type="journal" id="sbref40"><person-group person-group-type="author"><name><surname>Saldanha</surname><given-names>O.L.</given-names></name><name><surname>Quirke</surname><given-names>P.</given-names></name><name><surname>West</surname><given-names>N.P.</given-names></name><name><surname>James</surname><given-names>J.A.</given-names></name><name><surname>Loughrey</surname><given-names>M.B.</given-names></name><name><surname>Grabsch</surname><given-names>H.I.</given-names></name><etal/></person-group><article-title>Swarm learning for decentralized artificial intelligence in cancer histopathology</article-title><source>Nat Med</source><volume>28</volume><year>2022</year><fpage>1232</fpage><lpage>1239</lpage><pub-id pub-id-type="doi">10.1038/s41591-022-01768-5</pub-id><pub-id pub-id-type="pmid">35469069</pub-id>
</element-citation></ref><ref id="bib47"><label>47</label><element-citation publication-type="journal" id="sbref41"><person-group person-group-type="author"><name><surname>Haggenm&#x000fc;ller</surname><given-names>S.</given-names></name><name><surname>Schmitt</surname><given-names>M.</given-names></name><name><surname>Krieghoff-Henning</surname><given-names>E.</given-names></name><name><surname>Hekler</surname><given-names>A.</given-names></name><name><surname>Maron</surname><given-names>R.C.</given-names></name><name><surname>Wies</surname><given-names>C.</given-names></name><etal/></person-group><article-title>Federated learning for decentralized artificial intelligence in melanoma diagnostics</article-title><source>JAMA Dermatol</source><volume>160</volume><year>2024</year><fpage>303</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1001/jamadermatol.2023.5550</pub-id><pub-id pub-id-type="pmid">38324293</pub-id>
</element-citation></ref><ref id="bib48"><label>48</label><element-citation publication-type="journal" id="sbref42"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Kong</surname><given-names>D.</given-names></name><name><surname>Lipkova</surname><given-names>J.</given-names></name><name><surname>Singh</surname><given-names>R.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><etal/></person-group><article-title>Federated learning for computational pathology on gigapixel whole slide images</article-title><source>Med Image Anal</source><volume>76</volume><year>2022</year><object-id pub-id-type="publisher-id">102298</object-id><pub-id pub-id-type="doi">10.1016/j.media.2021.102298</pub-id></element-citation></ref><ref id="bib49"><label>49</label><element-citation publication-type="journal" id="sbref43"><person-group person-group-type="author"><name><surname>da Silva</surname><given-names>L.M.</given-names></name><name><surname>Pereira</surname><given-names>E.M.</given-names></name><name><surname>Salles</surname><given-names>P.G.</given-names></name><name><surname>Godrich</surname><given-names>R.</given-names></name><name><surname>Ceballos</surname><given-names>R.</given-names></name><name><surname>Kunz</surname><given-names>J.D.</given-names></name><etal/></person-group><article-title>Independent real-world application of a clinical-grade automated prostate cancer detection system</article-title><source>J Pathol</source><volume>254</volume><year>2021</year><fpage>147</fpage><lpage>158</lpage><pub-id pub-id-type="doi">10.1002/path.5662</pub-id><pub-id pub-id-type="pmid">33904171</pub-id>
</element-citation></ref><ref id="bib50"><label>50</label><element-citation publication-type="journal" id="sbref44"><person-group person-group-type="author"><name><surname>Perincheri</surname><given-names>S.</given-names></name><name><surname>Levi</surname><given-names>A.W.</given-names></name><name><surname>Celli</surname><given-names>R.</given-names></name><name><surname>Gershkovich</surname><given-names>P.</given-names></name><name><surname>Rimm</surname><given-names>D.</given-names></name><name><surname>Morrow</surname><given-names>J.S.</given-names></name><etal/></person-group><article-title>An independent assessment of an artificial intelligence system for prostate cancer detection shows strong diagnostic accuracy</article-title><source>Mod Pathol</source><volume>34</volume><year>2021</year><fpage>1588</fpage><lpage>1595</lpage><pub-id pub-id-type="doi">10.1038/s41379-021-00794-x</pub-id><pub-id pub-id-type="pmid">33782551</pub-id>
</element-citation></ref><ref id="bib51"><label>51</label><element-citation publication-type="journal" id="sbref45"><person-group person-group-type="author"><name><surname>Komura</surname><given-names>D.</given-names></name><name><surname>Kawabe</surname><given-names>A.</given-names></name><name><surname>Fukuta</surname><given-names>K.</given-names></name><name><surname>Sano</surname><given-names>K.</given-names></name><name><surname>Umezaki</surname><given-names>T.</given-names></name><name><surname>Koda</surname><given-names>H.</given-names></name><etal/></person-group><article-title>Universal encoding of pan-cancer histology by deep texture representations</article-title><source>Cell Rep</source><volume>38</volume><year>2022</year><pub-id pub-id-type="doi">10.1016/j.celrep.2022.110424</pub-id></element-citation></ref><ref id="bib52"><label>52</label><element-citation publication-type="journal" id="sbref46"><person-group person-group-type="author"><name><surname>Herdiantoputri</surname><given-names>R.R.</given-names></name><name><surname>Komura</surname><given-names>D.</given-names></name><name><surname>Ochi</surname><given-names>M.</given-names></name><name><surname>Fukawa</surname><given-names>Y.</given-names></name><name><surname>Kayamori</surname><given-names>K.</given-names></name><name><surname>Tsuchiya</surname><given-names>M.</given-names></name><name><surname>Kikuchi</surname><given-names>Y.</given-names></name><name><surname>Ushiku</surname><given-names>T.</given-names></name><name><surname>Ikeda</surname><given-names>T.</given-names></name><name><surname>Ishikawa</surname><given-names>S.</given-names></name></person-group><article-title>Benchmarking Deep Learning-Based Image Retrieval of Oral Tumor Histology</article-title><source>Cureus.</source><volume>16</volume><issue>6</issue><year>2024</year><object-id pub-id-type="publisher-id">e62264</object-id><pub-id pub-id-type="doi">10.7759/cureus.62264</pub-id><comment>PMID: 39011227; PMCID: PMC11247249.</comment></element-citation></ref><ref id="bib53"><label>53</label><element-citation publication-type="journal" id="sbref47"><person-group person-group-type="author"><name><surname>Shafique</surname><given-names>A.</given-names></name><name><surname>Gonzalez</surname><given-names>R.</given-names></name><name><surname>Pantanowitz</surname><given-names>L.</given-names></name><name><surname>Tan</surname><given-names>P.H.</given-names></name><name><surname>Machado</surname><given-names>A.</given-names></name><name><surname>Cree</surname><given-names>I.A.</given-names></name><name><surname>Tizhoosh</surname><given-names>H.R.</given-names></name></person-group><article-title>A Preliminary Investigation into Search and Matching for Tumor Discrimination in World Health Organization Breast Taxonomy Using Deep Networks</article-title><source>Mod Pathol.</source><volume>37</volume><issue>2</issue><year>2024</year><object-id pub-id-type="publisher-id">100381</object-id><pub-id pub-id-type="doi">10.1016/j.modpat.2023.100381</pub-id><comment>Epub 2023 Nov 7. PMID: 37939901; PMCID: PMC10891482.</comment></element-citation></ref><ref id="bib54"><label>54</label><element-citation publication-type="journal" id="sbref48"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C.</given-names></name><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Chen</surname><given-names>T.Y.</given-names></name><name><surname>Schaumberg</surname><given-names>A.J.</given-names></name><name><surname>Mahmood</surname><given-names>F.</given-names></name></person-group><article-title>Fast and scalable search of whole-slide images via self-supervised deep learning</article-title><source>Nat Biomed Eng</source><volume>6</volume><year>2022</year><fpage>1420</fpage><lpage>1434</lpage><pub-id pub-id-type="doi">10.1038/s41551-022-00929-8</pub-id><pub-id pub-id-type="pmid">36217022</pub-id>
</element-citation></ref><ref id="bib55"><label>55</label><element-citation publication-type="journal" id="sbref49"><person-group person-group-type="author"><name><surname>Kalra</surname><given-names>S.</given-names></name><name><surname>Tizhoosh</surname><given-names>H.R.</given-names></name><name><surname>Choi</surname><given-names>C.</given-names></name><name><surname>Shah</surname><given-names>S.</given-names></name><name><surname>Diamandis</surname><given-names>P.</given-names></name><name><surname>Campbell</surname><given-names>C.J.</given-names></name><name><surname>Pantanowitz</surname><given-names>L.</given-names></name></person-group><article-title>Yottixel&#x02013;an image search engine for large archives of histopathology whole slide images</article-title><source>Medical Image Analysis</source><volume>65</volume><year>2020</year><object-id pub-id-type="publisher-id">101757</object-id></element-citation></ref><ref id="bib56"><label>56</label><element-citation publication-type="journal" id="sbref50"><person-group person-group-type="author"><name><surname>Shang</surname><given-names>H.H.</given-names></name><name><surname>Nasr</surname><given-names>M.S.</given-names></name><name><surname>Veerla</surname><given-names>J.P.</given-names></name><name><surname>Saurav</surname><given-names>J.R.</given-names></name><name><surname>Hajighasemi</surname><given-names>A.</given-names></name><name><surname>Malidarreh</surname><given-names>P.</given-names></name><etal/></person-group><article-title>Histopathology slide indexing and search &#x02014; are we there yet?</article-title><source>NEJM AI</source><volume>1</volume><year>2024</year><object-id pub-id-type="publisher-id">AIcs2300019</object-id><pub-id pub-id-type="doi">10.1056/AIcs2300019</pub-id></element-citation></ref><ref id="bib57"><label>57</label><element-citation publication-type="journal" id="sbref51"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Du</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>S.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><etal/></person-group><article-title>RetCCL: Clustering-guided contrastive learning for whole-slide image retrieval</article-title><source>Med Image Anal</source><volume>83</volume><year>2023</year><object-id pub-id-type="publisher-id">102645</object-id><pub-id pub-id-type="doi">10.1016/j.media.2022.102645</pub-id></element-citation></ref><ref id="bib58"><label>58</label><element-citation publication-type="book" id="sbref52"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>J.W.</given-names></name><name><surname>Kim</surname><given-names>S.</given-names></name><name><surname>Kim</surname><given-names>E.</given-names></name><name><surname>Lee</surname><given-names>S.H.</given-names></name><name><surname>Ahn</surname><given-names>S.</given-names></name><name><surname>Jeong</surname><given-names>W.K.</given-names></name></person-group><part-title>Clinical-Grade Multi-organ Pathology Report Generation for Multi-scale Whole Slide Images via a Semantically Guided Medical Text Foundation Model</part-title><series>International Conference on Medical Image Computing and Computer-Assisted Intervention</series><year>2024</year><publisher-name>Springer Nature</publisher-name><publisher-loc>Switzerland</publisher-loc><fpage>25</fpage><lpage>35</lpage></element-citation></ref><ref id="bib59"><label>59</label><element-citation publication-type="book" id="sbref53"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>P.</given-names></name><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Zhu</surname><given-names>C.</given-names></name><name><surname>Zheng</surname><given-names>S.</given-names></name><name><surname>Shui</surname><given-names>Z.</given-names></name><name><surname>Yang</surname><given-names>L.</given-names></name></person-group><part-title>Wsicaption: Multiple instance generation of pathology reports for gigapixel whole-slide images</part-title><series>International Conference on Medical Image Computing and Computer-Assisted Intervention</series><year>2024</year><publisher-name>Springer Nature</publisher-name><publisher-loc>Switzerland</publisher-loc><fpage>546</fpage><lpage>556</lpage></element-citation></ref><ref id="bib60"><label>60</label><element-citation publication-type="journal" id="sbref54"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Zhao</surname><given-names>M.</given-names></name><name><surname>Chow</surname><given-names>A.K.</given-names></name><etal/></person-group><article-title>A multimodal generative AI copilot for human pathology</article-title><source>Nature</source><year>2024</year><fpage>1</fpage><lpage>3</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-07618-3</pub-id></element-citation></ref><ref id="bib61"><label>61</label><element-citation publication-type="journal" id="sbref55"><person-group person-group-type="author"><name><surname>Echle</surname><given-names>A.</given-names></name><name><surname>Rindtorff</surname><given-names>N.T.</given-names></name><name><surname>Brinker</surname><given-names>T.J.</given-names></name><name><surname>Luedde</surname><given-names>T.</given-names></name><name><surname>Pearson</surname><given-names>A.T.</given-names></name><name><surname>Kather</surname><given-names>J.N.</given-names></name></person-group><article-title>Deep learning in cancer pathology: a new generation of clinical biomarkers</article-title><source>Br J Cancer</source><volume>124</volume><year>2021</year><fpage>686</fpage><lpage>696</lpage><pub-id pub-id-type="doi">10.1038/s41416-020-01122-x</pub-id><pub-id pub-id-type="pmid">33204028</pub-id>
</element-citation></ref><ref id="bib62"><label>62</label><element-citation publication-type="journal" id="sbref56"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>X.</given-names></name><name><surname>Hoffmeister</surname><given-names>M.</given-names></name><name><surname>Brenner</surname><given-names>H.</given-names></name><name><surname>Muti</surname><given-names>H.S.</given-names></name><name><surname>Yuan</surname><given-names>T.</given-names></name><name><surname>Foersch</surname><given-names>S.</given-names></name><etal/></person-group><article-title>End-to-end prognostication in colorectal cancer by deep learning: a retrospective, multicentre study</article-title><source>Lancet Digit Health</source><volume>6</volume><year>2024</year><fpage>e33</fpage><lpage>e43</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(23)00208-X</pub-id><pub-id pub-id-type="pmid">38123254</pub-id>
</element-citation></ref><ref id="bib63"><label>63</label><element-citation publication-type="journal" id="sbref57"><person-group person-group-type="author"><name><surname>Bilal</surname><given-names>M.</given-names></name><name><surname>Raza</surname><given-names>S.E.A.</given-names></name><name><surname>Azam</surname><given-names>A.</given-names></name><name><surname>Graham</surname><given-names>S.</given-names></name><name><surname>Ilyas</surname><given-names>M.</given-names></name><name><surname>Cree</surname><given-names>I.A.</given-names></name><etal/></person-group><article-title>Development and validation of a weakly supervised deep learning framework to predict the status of molecular pathways and key mutations in colorectal cancer from routine histology images: a retrospective study</article-title><source>Lancet Digit Health</source><volume>3</volume><year>2021</year><fpage>e763</fpage><lpage>e772</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(21)00180-1</pub-id><pub-id pub-id-type="pmid">34686474</pub-id>
</element-citation></ref><ref id="bib64"><label>64</label><element-citation publication-type="journal" id="sbref58"><person-group person-group-type="author"><name><surname>Coudray</surname><given-names>N.</given-names></name><name><surname>Ocampo</surname><given-names>P.S.</given-names></name><name><surname>Sakellaropoulos</surname><given-names>T.</given-names></name><name><surname>Narula</surname><given-names>N.</given-names></name><name><surname>Snuderl</surname><given-names>M.</given-names></name><name><surname>Feny&#x000f6;</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Classification and mutation prediction from non&#x02013;small cell lung cancer histopathology images using deep learning</article-title><source>Nat Med</source><volume>24</volume><year>2018</year><fpage>1559</fpage><pub-id pub-id-type="doi">10.1038/s41591-018-0177-5</pub-id><pub-id pub-id-type="pmid">30224757</pub-id>
</element-citation></ref><ref id="bib65"><label>65</label><element-citation publication-type="journal" id="sbref59"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Chen</surname><given-names>R.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Dong</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Histopathology images-based deep learning prediction of prognosis and therapeutic response in small cell lung cancer</article-title><source>Npj Digit Med</source><volume>7</volume><year>2024</year><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41746-024-01003-0</pub-id><pub-id pub-id-type="pmid">38172429</pub-id>
</element-citation></ref><ref id="bib66"><label>66</label><element-citation publication-type="journal" id="sbref60"><person-group person-group-type="author"><name><surname>Kather</surname><given-names>J.N.</given-names></name><name><surname>Krisam</surname><given-names>J.</given-names></name><name><surname>Charoentong</surname><given-names>P.</given-names></name><name><surname>Luedde</surname><given-names>T.</given-names></name><name><surname>Herpel</surname><given-names>E.</given-names></name><name><surname>Weis</surname><given-names>C.-A.</given-names></name><etal/></person-group><article-title>Predicting survival from colorectal cancer histology slides using deep learning: a retrospective multicenter study</article-title><source>PLOS Med</source><volume>16</volume><year>2019</year><object-id pub-id-type="publisher-id">e1002730</object-id><pub-id pub-id-type="doi">10.1371/journal.pmed.1002730</pub-id></element-citation></ref><ref id="bib67"><label>67</label><element-citation publication-type="journal" id="sbref61"><person-group person-group-type="author"><name><surname>Juan Ramon</surname><given-names>A.</given-names></name><name><surname>Parmar</surname><given-names>C.</given-names></name><name><surname>Carrasco-Zevallos</surname><given-names>O.M.</given-names></name><name><surname>Csiszer</surname><given-names>C.</given-names></name><name><surname>Yip</surname><given-names>S.S.F.</given-names></name><name><surname>Raciti</surname><given-names>P.</given-names></name><etal/></person-group><article-title>Development and deployment of a histopathology-based deep learning algorithm for patient prescreening in a clinical trial</article-title><source>Nat Commun</source><volume>15</volume><year>2024</year><fpage>4690</fpage><pub-id pub-id-type="doi">10.1038/s41467-024-49153-9</pub-id><pub-id pub-id-type="pmid">38824132</pub-id>
</element-citation></ref><ref id="bib68"><label>68</label><element-citation publication-type="journal" id="sbref62"><person-group person-group-type="author"><name><surname>Saillard</surname><given-names>C.</given-names></name><name><surname>Dubois</surname><given-names>R.</given-names></name><name><surname>Tchita</surname><given-names>O.</given-names></name><name><surname>Loiseau</surname><given-names>N.</given-names></name><name><surname>Garcia</surname><given-names>T.</given-names></name><name><surname>Adriansen</surname><given-names>A.</given-names></name><etal/></person-group><article-title>Validation of MSIntuit as an AI-based pre-screening tool for MSI detection from colorectal cancer histology slides</article-title><source>Nat Commun</source><volume>14</volume><year>2023</year><fpage>6695</fpage><pub-id pub-id-type="doi">10.1038/s41467-023-42453-6</pub-id><pub-id pub-id-type="pmid">37932267</pub-id>
</element-citation></ref><ref id="bib69"><label>69</label><mixed-citation publication-type="other" id="othref0035">Power and significance: clinical trial benefits with advanced covariate adjustment. <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/d42473-023-00245-y" id="ir0035">&#x02329;https://www.nature.com/articles/d42473-023-00245-y&#x0232a;</ext-link>.</mixed-citation></ref><ref id="bib70"><label>70</label><element-citation publication-type="journal" id="sbref63"><person-group person-group-type="author"><name><surname>Hoang</surname><given-names>D.-T.</given-names></name><name><surname>Dinstag</surname><given-names>G.</given-names></name><name><surname>Shulman</surname><given-names>E.D.</given-names></name><name><surname>Hermida</surname><given-names>L.C.</given-names></name><name><surname>Ben-Zvi</surname><given-names>D.S.</given-names></name><name><surname>Elis</surname><given-names>E.</given-names></name><etal/></person-group><article-title>A deep-learning framework to predict cancer treatment response from histopathology images through imputed transcriptomics</article-title><source>Nat Cancer</source><volume>5</volume><year>2024</year><fpage>1305</fpage><lpage>1317</lpage><pub-id pub-id-type="doi">10.1038/s43018-024-00793-2</pub-id><pub-id pub-id-type="pmid">38961276</pub-id>
</element-citation></ref><ref id="bib71"><label>71</label><element-citation publication-type="journal" id="sbref64"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q.</given-names></name><name><surname>Teodoro</surname><given-names>G.</given-names></name><name><surname>Jiang</surname><given-names>Y.</given-names></name><name><surname>Kong</surname><given-names>J.</given-names></name></person-group><article-title>NACNet: A histology context-aware transformer graph convolution network for predicting treatment response to neoadjuvant chemotherapy in Triple Negative Breast Cancer</article-title><source>Comput. Med. Imaging Graph.</source><volume>118</volume><year>2024</year><fpage>102467</fpage><pub-id pub-id-type="pmid">39591709</pub-id>
</element-citation></ref><ref id="bib72"><label>72</label><element-citation publication-type="journal" id="sbref65"><person-group person-group-type="author"><name><surname>Nimgaonkar</surname><given-names>V.</given-names></name><name><surname>Krishna</surname><given-names>V.</given-names></name><name><surname>Krishna</surname><given-names>V.</given-names></name><name><surname>Tiu</surname><given-names>E.</given-names></name><name><surname>Joshi</surname><given-names>A.</given-names></name><name><surname>Vrabac</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Development of an artificial intelligence-derived histologic signature associated with adjuvant gemcitabine treatment outcomes in pancreatic cancer</article-title><source>Cell Rep Med</source><volume>4</volume><year>2023</year><object-id pub-id-type="publisher-id">101013</object-id><pub-id pub-id-type="doi">10.1016/j.xcrm.2023.101013</pub-id></element-citation></ref><ref id="bib73"><label>73</label><element-citation publication-type="journal" id="sbref66"><person-group person-group-type="author"><name><surname>Shamai</surname><given-names>G.</given-names></name><name><surname>Livne</surname><given-names>A.</given-names></name><name><surname>Pol&#x000f3;nia</surname><given-names>A.</given-names></name><name><surname>Sabo</surname><given-names>E.</given-names></name><name><surname>Cretu</surname><given-names>A.</given-names></name><name><surname>Bar-Sela</surname><given-names>G.</given-names></name><etal/></person-group><article-title>Deep learning-based image analysis predicts PD-L1 status from H&#x00026;E-stained histopathology images in breast cancer</article-title><source>Nat Commun</source><volume>13</volume><year>2022</year><fpage>6753</fpage><pub-id pub-id-type="doi">10.1038/s41467-022-34275-9</pub-id><pub-id pub-id-type="pmid">36347854</pub-id>
</element-citation></ref><ref id="bib74"><label>74</label><element-citation publication-type="journal" id="sbref67"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>U.</given-names></name><name><surname>Im</surname><given-names>S.</given-names></name><name><surname>Park</surname><given-names>H.S.</given-names></name></person-group><article-title>Exploring histological predictive biomarkers for immune checkpoint inhibitor therapy response in non&#x02013;small cell lung cancer</article-title><source>J Pathol Transl Med</source><volume>58</volume><year>2024</year><fpage>49</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.4132/jptm.2024.01.31</pub-id><pub-id pub-id-type="pmid">38389279</pub-id>
</element-citation></ref><ref id="bib75"><label>75</label><element-citation publication-type="journal" id="sbref68"><person-group person-group-type="author"><name><surname>de Haan</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Zuckerman</surname><given-names>J.E.</given-names></name><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>Sisk</surname><given-names>A.E.</given-names></name><name><surname>Diaz</surname><given-names>M.F.P.</given-names></name><etal/></person-group><article-title>Deep learning-based transformation of H&#x00026;E stained tissues into special stains</article-title><source>Nat Commun</source><volume>12</volume><year>2021</year><fpage>4884</fpage><pub-id pub-id-type="doi">10.1038/s41467-021-25221-2</pub-id><pub-id pub-id-type="pmid">34385460</pub-id>
</element-citation></ref><ref id="bib76"><label>76</label><element-citation publication-type="book" id="sbref69"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Dong</surname><given-names>J.</given-names></name><name><surname>Huang</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Jiang</surname><given-names>J.</given-names></name><name><surname>Fan</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><part-title>Virtual Immunohistochemistry Staining for Histological Images Assisted by Weakly-supervised Learning</part-title><series>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</series><year>2024</year><publisher-name>IEEE</publisher-name><fpage>11259</fpage><lpage>11268</lpage></element-citation></ref><ref id="bib77"><label>77</label><element-citation publication-type="book" id="sbref70"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>F.</given-names></name><name><surname>Zhang</surname><given-names>R.</given-names></name><name><surname>Zheng</surname><given-names>B.</given-names></name><name><surname>Sun</surname><given-names>Y.</given-names></name><name><surname>He</surname><given-names>J.</given-names></name><name><surname>Qin</surname><given-names>W.</given-names></name></person-group><part-title>Pathological semantics-preserving learning for H&#x00026;E-to-IHC virtual staining</part-title><series>International Conference on Medical Image Computing and Computer-Assisted Intervention</series><year>2024</year><publisher-name>Springer Nature</publisher-name><publisher-loc>Switzerland</publisher-loc><fpage>384</fpage><lpage>394</lpage></element-citation></ref><ref id="bib78"><label>78</label><element-citation publication-type="journal" id="sbref71"><person-group person-group-type="author"><name><surname>Koivukoski</surname><given-names>S.</given-names></name><name><surname>Khan</surname><given-names>U.</given-names></name><name><surname>Ruusuvuori</surname><given-names>P.</given-names></name><name><surname>Latonen</surname><given-names>L.</given-names></name></person-group><article-title>Unstained tissue imaging and virtual hematoxylin and eosin staining of histologic whole slide images</article-title><source>Lab Invest</source><volume>103</volume><year>2023</year><object-id pub-id-type="publisher-id">100070</object-id><pub-id pub-id-type="doi">10.1016/j.labinv.2023.100070</pub-id></element-citation></ref><ref id="bib79"><label>79</label><element-citation publication-type="journal" id="sbref72"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>B.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>de Haan</surname><given-names>K.</given-names></name><name><surname>Colonnese</surname><given-names>F.</given-names></name><name><surname>Wan</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>Label-free virtual HER2 immunohistochemical staining of breast tissue using deep learning</article-title><source>BME Front</source><volume>2022</volume><year>2022</year><object-id pub-id-type="publisher-id">9786242</object-id><pub-id pub-id-type="doi">10.34133/2022/9786242</pub-id></element-citation></ref><ref id="bib80"><label>80</label><element-citation publication-type="journal" id="sbref73"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Huang</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>Cheng</surname><given-names>K.</given-names></name><name><surname>Kevin de Haan</surname></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Bai</surname><given-names>B.</given-names></name><name><surname>Ozcan</surname><given-names>A.</given-names></name></person-group><article-title>Virtual staining of defocused autofluorescence images of unlabeled tissue using deep neural networks</article-title><source>Intell.&#x000a0;Comput.</source><year>2022</year></element-citation></ref><ref id="bib81"><label>81</label><element-citation publication-type="journal" id="sbref74"><person-group person-group-type="author"><name><surname>Rivenson</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Wei</surname><given-names>Z.</given-names></name><name><surname>Kevin de Haan</surname></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Wu</surname><given-names>Y.</given-names></name><name><surname>G&#x000fc;nayd&#x00131;n</surname><given-names>H.</given-names></name><etal/></person-group><article-title>Virtual histological staining of unlabelled tissue-autofluorescence images via deep learning</article-title><source>Nat.&#x000a0;Biomed.&#x000a0;Eng.</source><volume>3</volume><issue>6</issue><year>2019</year><fpage>466</fpage><lpage>477</lpage><pub-id pub-id-type="pmid">31142829</pub-id>
</element-citation></ref><ref id="bib82"><label>82</label><element-citation publication-type="journal" id="sbref75"><person-group person-group-type="author"><name><surname>Ozyoruk</surname><given-names>K.B.</given-names></name><name><surname>Can</surname><given-names>S.</given-names></name><name><surname>Darbaz</surname><given-names>B.</given-names></name><name><surname>Ba&#x0015f;ak</surname><given-names>K.</given-names></name><name><surname>Demir</surname><given-names>D.</given-names></name><name><surname>Gokceler</surname><given-names>G.I.</given-names></name><etal/></person-group><article-title>A deep-learning model for transforming the style of tissue images from cryosectioned to formalin-fixed and paraffin-embedded</article-title><source>Nat Biomed Eng</source><volume>6</volume><year>2022</year><fpage>1407</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1038/s41551-022-00952-9</pub-id><pub-id pub-id-type="pmid">36564629</pub-id>
</element-citation></ref><ref id="bib83"><label>83</label><element-citation publication-type="journal" id="sbref76"><person-group person-group-type="author"><name><surname>Pati</surname><given-names>P.</given-names></name><name><surname>Karkampouna</surname><given-names>S.</given-names></name><name><surname>Bonollo</surname><given-names>F.</given-names></name><name><surname>Comp&#x000e9;rat</surname><given-names>E.</given-names></name><name><surname>Radi&#x00107;</surname><given-names>M.</given-names></name><name><surname>Spahn</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Accelerating histopathology workflows with generative AI-based virtually multiplexed tumour profiling</article-title><source>Nat Mach Intell</source><volume>6</volume><year>2024</year><fpage>1077</fpage><lpage>1093</lpage><pub-id pub-id-type="doi">10.1038/s42256-024-00889-5</pub-id><pub-id pub-id-type="pmid">39309216</pub-id>
</element-citation></ref><ref id="bib84"><label>84</label><element-citation publication-type="journal" id="sbref77"><person-group person-group-type="author"><name><surname>Alajaji</surname><given-names>S.A.</given-names></name><name><surname>Khoury</surname><given-names>Z.H.</given-names></name><name><surname>Elgharib</surname><given-names>M.</given-names></name><name><surname>Saeed</surname><given-names>M.</given-names></name><name><surname>Ahmed</surname><given-names>A.R.H.</given-names></name><name><surname>Khan</surname><given-names>M.B.</given-names></name><etal/></person-group><article-title>Generative adversarial networks in digital histopathology: current applications, limitations, ethical considerations, and future directions</article-title><source>Mod Pathol</source><volume>37</volume><year>2024</year><pub-id pub-id-type="doi">10.1016/j.modpat.2023.100369</pub-id></element-citation></ref><ref id="bib85"><label>85</label><mixed-citation publication-type="other" id="othref0040">Kataria, Tushar, Beatrice Knudsen, and Shireen Y. Elhabian. "StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining." <italic>arXiv preprint arXiv:2403.11340</italic> (2024).</mixed-citation></ref><ref id="bib86"><label>86</label><element-citation publication-type="journal" id="sbref78"><person-group person-group-type="author"><name><surname>He</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>Z.</given-names></name><name><surname>Qi</surname><given-names>M.</given-names></name><name><surname>Ding</surname><given-names>S.</given-names></name><name><surname>Zhang</surname><given-names>P.</given-names></name><name><surname>Song</surname><given-names>F.</given-names></name><etal/></person-group><article-title>PST-Diff: achieving high-consistency stain transfer by diffusion models with pathological and structural constraints</article-title><comment>1&#x02013;1</comment><source>IEEE Trans Med Imaging</source><year>2024</year><pub-id pub-id-type="doi">10.1109/TMI.2024.3430825</pub-id></element-citation></ref><ref id="bib87"><label>87</label><element-citation publication-type="journal" id="sbref79"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>U.</given-names></name><name><surname>Koivukoski</surname><given-names>S.</given-names></name><name><surname>Valkonen</surname><given-names>M.</given-names></name><name><surname>Latonen</surname><given-names>L.</given-names></name><name><surname>Ruusuvuori</surname><given-names>P.</given-names></name></person-group><article-title>The effect of neural network architecture on virtual H&#x00026;E staining: systematic assessment of histological feasibility</article-title><source>Patterns</source><volume>4</volume><year>2023</year><pub-id pub-id-type="doi">10.1016/j.patter.2023.100725</pub-id></element-citation></ref><ref id="bib88"><label>88</label><mixed-citation publication-type="other" id="othref0045">Ounissi, Mehdi, Ilias Sarbout, Jean-Pierre Hugot, Christine Martinez-Vinson, Dominique Berrebi, and Daniel Racoceanu. "Scalable, Trustworthy Generative Model for Virtual Multi-Staining from H&#x00026;E Whole Slide Images." <italic>arXiv preprint arXiv:2407.00098</italic> (2024).</mixed-citation></ref><ref id="bib89"><label>89</label><element-citation publication-type="journal" id="sbref80"><person-group person-group-type="author"><name><surname>Mobadersany</surname><given-names>P.</given-names></name><name><surname>Yousefi</surname><given-names>S.</given-names></name><name><surname>Amgad</surname><given-names>M.</given-names></name><name><surname>Gutman</surname><given-names>D.A.</given-names></name><name><surname>Barnholtz-Sloan</surname><given-names>J.S.</given-names></name><name><surname>Vel&#x000e1;zquez Vega</surname><given-names>J.E.</given-names></name><etal/></person-group><article-title>Predicting cancer outcomes from histology and genomics using convolutional networks</article-title><source>Proc Natl Acad Sci USA</source><volume>115</volume><year>2018</year><fpage>E2970</fpage><lpage>E2979</lpage><pub-id pub-id-type="doi">10.1073/pnas.1717139115</pub-id><pub-id pub-id-type="pmid">29531073</pub-id>
</element-citation></ref><ref id="bib90"><label>90</label><element-citation publication-type="book" id="sbref81"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X.</given-names></name><name><surname>Yao</surname><given-names>J.</given-names></name><name><surname>Junzhou</surname><given-names>H.</given-names></name></person-group><source>Deep convolutional neural network for survival analysis with pathological images</source><year>2016</year><publisher-name>IEEE</publisher-name><fpage>544</fpage><lpage>547</lpage></element-citation></ref><ref id="bib91"><label>91</label><element-citation publication-type="journal" id="sbref82"><person-group person-group-type="author"><name><surname>Su</surname><given-names>F.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Zhao</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>B.</given-names></name><name><surname>Hu</surname><given-names>Y.</given-names></name><name><surname>Sun</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>Interpretable tumor differentiation grade and microsatellite instability recognition in gastric cancer using deep learning</article-title><source>Lab Invest</source><volume>102</volume><year>2022</year><fpage>641</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1038/s41374-022-00742-6</pub-id><pub-id pub-id-type="pmid">35177797</pub-id>
</element-citation></ref><ref id="bib92"><label>92</label><element-citation publication-type="book" id="sbref83"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>Y.</given-names></name><name><surname>Shrivastava</surname><given-names>A.</given-names></name><name><surname>Ehsan</surname><given-names>L.</given-names></name><name><surname>Moskaluk</surname><given-names>C.A.</given-names></name><name><surname>Syed</surname><given-names>S.</given-names></name><name><surname>Brown</surname><given-names>D.</given-names></name></person-group><part-title>Cluster-to-conquer: A framework for end-to-end multi-instance learning for whole slide image classification</part-title><source>Proc. Fourth Conf. Med. Imaging Deep Learn.</source><year>2021</year><publisher-name>PMLR</publisher-name><fpage>682</fpage><lpage>698</lpage></element-citation></ref><ref id="bib93"><label>93</label><element-citation publication-type="journal" id="sbref84"><person-group person-group-type="author"><name><surname>Javed</surname><given-names>S.A.</given-names></name><name><surname>Juyal</surname><given-names>D.</given-names></name><name><surname>Padigela</surname><given-names>H.</given-names></name><name><surname>Taylor-Weiner</surname><given-names>A.</given-names></name><name><surname>Yu</surname><given-names>L.</given-names></name><name><surname>Prakash</surname><given-names>A.</given-names></name></person-group><article-title>Additive MIL: intrinsically interpretable multiple instance learning for pathology</article-title><source>Adv Neural Inf Process Syst</source><volume>35</volume><year>2022</year><fpage>20689</fpage><lpage>20702</lpage></element-citation></ref><ref id="bib94"><label>94</label><mixed-citation publication-type="other" id="othref0050">Rymarczyk D., Pardyl A., Kraus J., Kaczy&#x00144;ska A., Skomorowski M., Zieli&#x00144;ski B. ProtoMIL: Multiple Instance Learning with Prototypical Parts for Whole-Slide Image Classification. In: Amini M.-R., Canu S., Fischer A., Guns T., Kralj Novak P., Tsoumakas G., editors. Mach. Learn. Knowl. Discov. Databases, Cham: Springer International Publishing; 2023, p. 421&#x02013;436. https://doi.org/10.1007/978-3-031-26387-3_26.</mixed-citation></ref><ref id="bib95"><label>95</label><element-citation publication-type="book" id="sbref85"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Meng</surname><given-names>Y.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><name><surname>Qiao</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>X.</given-names></name><name><surname>Coupland</surname><given-names>S.E.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name></person-group><part-title>Dtfd-mil: Double-tier feature distillation multiple instance learning for histopathology whole slide image classification</part-title><series>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</series><year>2022</year><publisher-name>IEEE</publisher-name><fpage>18802</fpage><lpage>18812</lpage></element-citation></ref><ref id="bib96"><label>96</label><element-citation publication-type="journal" id="sbref86"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Chen</surname><given-names>T.Y.</given-names></name><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Barbieri</surname><given-names>M.</given-names></name><name><surname>Mahmood</surname><given-names>F.</given-names></name></person-group><article-title>Data-efficient and weakly supervised computational pathology on whole-slide images</article-title><source>Nat Biomed Eng</source><volume>5</volume><year>2021</year><fpage>555</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1038/s41551-020-00682-w</pub-id><pub-id pub-id-type="pmid">33649564</pub-id>
</element-citation></ref><ref id="bib97"><label>97</label><mixed-citation publication-type="other" id="othref0055">Using Multi-Scale Convolutional Neural Network Based on Multi-Instance Learning to Predict the Efficacy of Neoadjuvant Chemoradiotherapy for Rectal Cancer. IEEE J Transl Eng Health Med 2022;10:4300108. <pub-id pub-id-type="doi">10.1109/JTEHM.2022.3156851</pub-id>.</mixed-citation></ref><ref id="bib98"><label>98</label><element-citation publication-type="journal" id="sbref87"><person-group person-group-type="author"><name><surname>Xiang</surname><given-names>H.</given-names></name><name><surname>Shen</surname><given-names>J.</given-names></name><name><surname>Yan</surname><given-names>Q.</given-names></name><name><surname>Xu</surname><given-names>M.</given-names></name><name><surname>Shi</surname><given-names>X.</given-names></name><name><surname>Zhu</surname><given-names>X.</given-names></name></person-group><article-title>Multi-scale representation attention based deep multiple instance learning for gigapixel whole slide image analysis</article-title><source>Med Image Anal</source><volume>89</volume><year>2023</year><object-id pub-id-type="publisher-id">102890</object-id><pub-id pub-id-type="doi">10.1016/j.media.2023.102890</pub-id></element-citation></ref><ref id="bib99"><label>99</label><element-citation publication-type="journal" id="sbref88"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name><name><surname>Sisk</surname><given-names>A.</given-names></name><name><surname>Ye</surname><given-names>H.</given-names></name><name><surname>Wallace</surname><given-names>W.D.</given-names></name><name><surname>Speier</surname><given-names>W.</given-names></name><etal/></person-group><article-title>A multi-resolution model for histopathology image classification and localization with multiple instance learning</article-title><source>Comput Biol Med</source><volume>131</volume><year>2021</year><object-id pub-id-type="publisher-id">104253</object-id><pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.104253</pub-id></element-citation></ref><ref id="bib100"><label>100</label><mixed-citation publication-type="other" id="othref0060">Chen R.J., Chen C., Li Y., Chen T.Y., Trister A.D., Krishnan R.G., et al. Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning. 2022 IEEECVF Conf. Comput. Vis. Pattern Recognit. CVPR, 2022, p. 16123&#x02013;16134. <pub-id pub-id-type="doi">10.1109/CVPR52688.2022.01567</pub-id>.</mixed-citation></ref><ref id="bib101"><label>101</label><mixed-citation publication-type="other" id="othref0065">Dao, Tri, and Albert Gu. "Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality." <italic>arXiv preprint arXiv:2405.21060</italic> (2024).</mixed-citation></ref><ref id="bib102"><label>102</label><mixed-citation publication-type="other" id="othref0070">Fillioux L., Boyd J., Vakalopoulou M., Courn&#x000e8;de P., Christodoulidis S. Structured State Space Models for Multiple Instance Learning in Digital Pathology. In: Greenspan H., Madabhushi A., Mousavi P., Salcudean S., Duncan J., Syeda-Mahmood T., et al., editors. Med. Image Comput. Comput. Assist. Interv. &#x02013; MICCAI 2023, Cham: Springer Nature Switzerland; 2023, p. 594&#x02013;604. https://doi.org/10.1007/978-3-031-43907-0_57.</mixed-citation></ref><ref id="bib103"><label>103</label><element-citation publication-type="journal" id="sbref89"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Peng</surname><given-names>T.</given-names></name><name><surname>Tong</surname><given-names>C.</given-names></name></person-group><article-title>Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology</article-title><source>arXiv preprint arXiv</source><year>2024</year><object-id pub-id-type="publisher-id">2408.15032</object-id></element-citation></ref><ref id="bib104"><label>104</label><element-citation publication-type="book" id="sbref90"><person-group person-group-type="author"><name><surname>Ali</surname><given-names>N.-S.</given-names></name><name><surname>Trinh</surname><given-names>V.Q.-H.</given-names></name><name><surname>Rivaz</surname><given-names>H.</given-names></name><name><surname>Hosseini</surname><given-names>M.S.</given-names></name></person-group><part-title>Vim4Path: Self-Supervised Vision Mamba for Histopathology Images</part-title><series>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</series><year>2024</year><publisher-name>IEEE</publisher-name><fpage>6894</fpage><lpage>6903</lpage></element-citation></ref><ref id="bib105"><label>105</label><element-citation publication-type="journal" id="sbref91"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Nguyen</surname><given-names>A.T.</given-names></name><name><surname>Han</surname><given-names>X.</given-names></name><name><surname>Trinh</surname><given-names>V.Q.-H.</given-names></name><name><surname>Qin</surname><given-names>H.</given-names></name><name><surname>Samaras</surname><given-names>D.</given-names></name><name><surname>Hosseini</surname><given-names>M.S.</given-names></name></person-group><article-title>2DMamba: Efficient State Space Model for Image Representation with Applications on Giga-Pixel Whole Slide Image Classification</article-title><source>arXiv preprint arXiv</source><year>2024</year><object-id pub-id-type="publisher-id">2412.00678</object-id></element-citation></ref><ref id="bib106"><label>106</label><element-citation publication-type="book" id="sbref92"><person-group person-group-type="author"><name><surname>Ryu</surname><given-names>J.</given-names></name><name><surname>Puche</surname><given-names>A.V.</given-names></name><name><surname>Shin</surname><given-names>J.</given-names></name><name><surname>Park</surname><given-names>S.</given-names></name><name><surname>Brattoli</surname><given-names>B.</given-names></name><name><surname>Lee</surname><given-names>J.</given-names></name><name><surname>Jung</surname><given-names>W.</given-names></name><etal/></person-group><part-title>OCELOT: overlapped cell on tissue dataset for histopathology</part-title><series>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</series><year>2023</year><publisher-name>IEEE</publisher-name><fpage>23902</fpage><lpage>23912</lpage></element-citation></ref><ref id="bib107"><label>107</label><element-citation publication-type="book" id="sbref93"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>D.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Chang</surname><given-names>H.</given-names></name><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Braet</surname><given-names>F.</given-names></name><name><surname>Chen</surname><given-names>M.</given-names></name><name><surname>Weidong</surname><given-names>C.</given-names></name></person-group><source>Revisiting adaptive cellular recognition under domain shifts: A contextual correspondence view</source><year>2025</year><publisher-name>Springer</publisher-name><publisher-loc>Cham</publisher-loc><fpage>275</fpage><lpage>292</lpage></element-citation></ref><ref id="bib108"><label>108</label><mixed-citation publication-type="other" id="othref0075">Li Z., Li W., Mai H., Zhang T., Xiong Z. Enhancing Cell Detection in Histopathology Images: A ViT-Based U-Net Approach. In: Ahmadi S.-A., Pereira S., editors. Graphs Biomed. Image Anal. Overlapped Cell Tissue Dataset Histopathol., Cham: Springer Nature Switzerland; 2024, p. 150&#x02013;160. https://doi.org/10.1007/978-3-031-55088-1_14.</mixed-citation></ref><ref id="bib109"><label>109</label><mixed-citation publication-type="other" id="othref0080">Schoenpflug L.A., Koelzer V.H. SoftCTM: Cell Detection by Soft Instance Segmentation and Consideration of Cell-Tissue Interaction. In: Ahmadi S.-A., Pereira S., editors. Graphs Biomed. Image Anal. Overlapped Cell Tissue Dataset Histopathol., Cham: Springer Nature Switzerland; 2024, p. 109&#x02013;122. https://doi.org/10.1007/978-3-031-55088-1_10.</mixed-citation></ref><ref id="bib110"><label>110</label><mixed-citation publication-type="other" id="othref0085">Millward J., He Z., Nibali A. Dense Prediction of Cell Centroids Using Tissue Context and Cell Refinement. In: Ahmadi S.-A., Pereira S., editors. Graphs Biomed. Image Anal. Overlapped Cell Tissue Dataset Histopathol., Cham: Springer Nature Switzerland; 2024, p. 138&#x02013;149. https://doi.org/10.1007/978-3-031-55088-1_13.</mixed-citation></ref><ref id="bib111"><label>111</label><element-citation publication-type="journal" id="sbref94"><person-group person-group-type="author"><name><surname>Amgad</surname><given-names>M.</given-names></name><name><surname>Atteya</surname><given-names>L.A.</given-names></name><name><surname>Hussein</surname><given-names>H.</given-names></name><name><surname>Mohammed</surname><given-names>K.H.</given-names></name><name><surname>Hafiz</surname><given-names>E.</given-names></name><name><surname>Elsebaie</surname><given-names>M.A.T.</given-names></name><name><surname>Alhusseiny</surname><given-names>A.M.</given-names></name><name><surname>AlMoslemany</surname><given-names>M.A.</given-names></name><name><surname>Elmatboly</surname><given-names>A.M.</given-names></name><name><surname>Pappalardo</surname><given-names>P.A.</given-names></name></person-group><article-title>NuCLS: A scalable crowdsourcing, deep learning approach and dataset for nucleus classification, localization and segmentation</article-title><source>arXiv preprint arXiv</source><year>2021</year><object-id pub-id-type="publisher-id">2102.09099</object-id></element-citation></ref><ref id="bib112"><label>112</label><element-citation publication-type="journal" id="sbref95"><person-group person-group-type="author"><name><surname>Amgad</surname><given-names>M.</given-names></name><name><surname>Elfandy</surname><given-names>H.</given-names></name><name><surname>Hussein</surname><given-names>H.</given-names></name><name><surname>Atteya</surname><given-names>L.A.</given-names></name><name><surname>Elsebaie</surname><given-names>M.A.T.</given-names></name><name><surname>Abo Elnasr</surname><given-names>L.S.</given-names></name><etal/></person-group><article-title>Structured crowdsourcing enables convolutional segmentation of histology images</article-title><source>Bioinforma Oxf Engl</source><volume>35</volume><year>2019</year><fpage>3461</fpage><lpage>3467</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btz083</pub-id></element-citation></ref><ref id="bib113"><label>113</label><element-citation publication-type="journal" id="sbref96"><person-group person-group-type="author"><name><surname>Edwards</surname><given-names>N.J.</given-names></name><name><surname>Oberti</surname><given-names>M.</given-names></name><name><surname>Thangudu</surname><given-names>R.R.</given-names></name><name><surname>Cai</surname><given-names>S.</given-names></name><name><surname>McGarvey</surname><given-names>P.B.</given-names></name><name><surname>Jacob</surname><given-names>S.</given-names></name><etal/></person-group><article-title>The CPTAC data portal: a resource for cancer proteomics research</article-title><source>J Proteome Res</source><volume>14</volume><year>2015</year><fpage>2707</fpage><lpage>2713</lpage><pub-id pub-id-type="doi">10.1021/pr501254j</pub-id><pub-id pub-id-type="pmid">25873244</pub-id>
</element-citation></ref><ref id="bib114"><label>114</label><mixed-citation publication-type="other" id="othref0090">IMP Whole-Slide Images of Colorectal Samples 2024 - CKAN n.d. <ext-link ext-link-type="uri" xlink:href="https://rdm.inesctec.pt/dataset/nis-2023-008" id="ir0050">&#x02329;https://rdm.inesctec.pt/dataset/nis-2023-008&#x0232a;</ext-link> (accessed September 16, 2024).</mixed-citation></ref><ref id="bib115"><label>115</label><element-citation publication-type="journal" id="sbref97"><person-group person-group-type="author"><name><surname>Weinstein</surname><given-names>J.N.</given-names></name><name><surname>Collisson</surname><given-names>E.A.</given-names></name><name><surname>Mills</surname><given-names>G.B.</given-names></name><name><surname>Shaw</surname><given-names>K.R.M.</given-names></name><name><surname>Ozenberger</surname><given-names>B.A.</given-names></name><name><surname>Ellrott</surname><given-names>K.</given-names></name><etal/></person-group><article-title>The cancer genome atlas pan-cancer analysis project</article-title><source>Nat Genet</source><volume>45</volume><year>2013</year><fpage>1113</fpage><lpage>1120</lpage><pub-id pub-id-type="pmid">24071849</pub-id>
</element-citation></ref><ref id="bib116"><label>116</label><mixed-citation publication-type="other" id="othref0095">https://portal.gdc.cancer.gov/legacy-archive n.d..</mixed-citation></ref><ref id="bib117"><label>117</label><mixed-citation publication-type="other" id="othref0100">Eaglescope n.d. <ext-link ext-link-type="uri" xlink:href="https://pathdb.cancerimagingarchive.net/eaglescope/dist/?configurl=%2Fsystem%2Ffiles%2Fcollectionmetadat%2F202210%2Feaglescope_main_page_current.json" id="ir0055">&#x02329;https://pathdb.cancerimagingarchive.net/eaglescope/dist/?configurl=&#x02009;%2Fsystem%2Ffiles%2Fcollectionmetadat%2F202210%2Feaglescope_main_page_current.json&#x0232a;</ext-link> (accessed September 16, 2024).</mixed-citation></ref><ref id="bib118"><label>118</label><element-citation publication-type="journal" id="sbref98"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>M.</given-names></name><name><surname>Ren</surname><given-names>B.</given-names></name><name><surname>Richards</surname><given-names>R.</given-names></name><name><surname>Suriawinata</surname><given-names>M.</given-names></name><name><surname>Tomita</surname><given-names>N.</given-names></name><name><surname>Hassanpour</surname><given-names>S.</given-names></name></person-group><article-title>Development and evaluation of a deep neural network for histologic classification of renal cell carcinoma on biopsy and surgical resection slides</article-title><source>Sci Rep</source><volume>11</volume><year>2021</year><fpage>7080</fpage><pub-id pub-id-type="doi">10.1038/s41598-021-86540-4</pub-id><pub-id pub-id-type="pmid">33782535</pub-id>
</element-citation></ref><ref id="bib119"><label>119</label><element-citation publication-type="journal" id="sbref99"><person-group person-group-type="author"><name><surname>Lonsdale</surname><given-names>J.</given-names></name><name><surname>Salvatore</surname><given-names>M.</given-names></name><name><surname>Salvatore</surname><given-names>M.</given-names></name><name><surname>Phillips</surname><given-names>R.</given-names></name><name><surname>Edmund Lo</surname></name><name><surname>Saboor Shad</surname></name><name><surname>Hasz</surname><given-names>R.</given-names></name></person-group><article-title>The genotype-tissue expression (GTEx) projecta</article-title><source>Nature genetics</source><volume>45</volume><issue>6</issue><year>2013</year><fpage>580</fpage><lpage>585</lpage><pub-id pub-id-type="pmid">23715323</pub-id>
</element-citation></ref><ref id="bib120"><label>120</label><mixed-citation publication-type="other" id="othref0105">Biospecimen Research Database n.d. <ext-link ext-link-type="uri" xlink:href="https://brd.nci.nih.gov/brd/image-search/searchhome" id="ir0060">&#x02329;https://brd.nci.nih.gov/brd/image-search/searchhome&#x0232a;</ext-link> (accessed August 30, 2017).</mixed-citation></ref><ref id="bib121"><label>121</label><mixed-citation publication-type="other" id="othref0110">CAMELYON17 n.d. <ext-link ext-link-type="uri" xlink:href="https://camelyon17.grand-challenge.org/" id="ir0065">&#x02329;https://camelyon17.grand-challenge.org/&#x0232a;</ext-link> (accessed August 21, 2017).</mixed-citation></ref><ref id="bib122"><label>122</label><mixed-citation publication-type="other" id="othref0115">Graham S., Jahanifar M., Azam A., Nimir M., Tsang Y.-W., Dodd K., et al. Lizard: A Large-Scale Dataset for Colonic Nuclear Instance Segmentation and Classification. 2021 IEEECVF Int. Conf. Comput. Vis. Workshop ICCVW, 2021, p. 684&#x02013;693. <pub-id pub-id-type="doi">10.1109/ICCVW54120.2021.00082</pub-id>.</mixed-citation></ref><ref id="bib123"><label>123</label><element-citation publication-type="journal" id="sbref100"><person-group person-group-type="author"><name><surname>Verma</surname><given-names>R.</given-names></name><name><surname>Kumar</surname><given-names>N.</given-names></name><name><surname>Patil</surname><given-names>A.</given-names></name><name><surname>Kurian</surname><given-names>N.C.</given-names></name><name><surname>Rane</surname><given-names>S.</given-names></name><name><surname>Graham</surname><given-names>S.</given-names></name><etal/></person-group><article-title>MoNuSAC2020: a multi-organ nuclei segmentation and classification challenge</article-title><source>IEEE Trans Med Imaging</source><volume>40</volume><year>2021</year><fpage>3413</fpage><lpage>3423</lpage><pub-id pub-id-type="doi">10.1109/TMI.2021.3085712</pub-id><pub-id pub-id-type="pmid">34086562</pub-id>
</element-citation></ref><ref id="bib124"><label>124</label><element-citation publication-type="journal" id="sbref101"><person-group person-group-type="author"><name><surname>Aubreville</surname><given-names>M.</given-names></name><name><surname>Wilm</surname><given-names>F.</given-names></name><name><surname>Stathonikos</surname><given-names>N.</given-names></name><name><surname>Breininger</surname><given-names>K.</given-names></name><name><surname>Donovan</surname><given-names>T.A.</given-names></name><name><surname>Jabari</surname><given-names>S.</given-names></name><etal/></person-group><article-title>A comprehensive multi-domain dataset for mitotic figure detection</article-title><source>Sci Data</source><volume>10</volume><year>2023</year><fpage>484</fpage><pub-id pub-id-type="doi">10.1038/s41597-023-02327-4</pub-id><pub-id pub-id-type="pmid">37491536</pub-id>
</element-citation></ref><ref id="bib125"><label>125</label><element-citation publication-type="journal" id="sbref102"><person-group person-group-type="author"><name><surname>Komura</surname><given-names>D.</given-names></name><name><surname>Onoyama</surname><given-names>T.</given-names></name><name><surname>Shinbo</surname><given-names>K.</given-names></name><name><surname>Odaka</surname><given-names>H.</given-names></name><name><surname>Hayakawa</surname><given-names>M.</given-names></name><name><surname>Ochi</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Restaining-based annotation for cancer histology segmentation to overcome annotation-related limitations among pathologists</article-title><source>Patterns</source><volume>4</volume><year>2023</year><pub-id pub-id-type="doi">10.1016/j.patter.2023.100688</pub-id></element-citation></ref><ref id="bib126"><label>126</label><element-citation publication-type="journal" id="sbref103"><person-group person-group-type="author"><name><surname>He,</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Mou</surname><given-names>L.</given-names></name><name><surname>Xing</surname><given-names>E.</given-names></name><name><surname>Xie</surname><given-names>P.</given-names></name></person-group><article-title>Pathvqa: 30000+ questions for medical visual question answering</article-title><source>arXiv preprint arXiv</source><year>2020</year><object-id pub-id-type="publisher-id">2003.10286</object-id></element-citation></ref><ref id="bib127"><label>127</label><element-citation publication-type="journal" id="sbref104"><person-group person-group-type="author"><name><surname>Ikezogwo</surname><given-names>W.</given-names></name><name><surname>Saygin Seyfioglu</surname></name><name><surname>Fatemeh Ghezloo</surname></name><name><surname>Dylan Geva</surname></name><name><surname>Fatwir Sheikh Mohammed</surname></name><name><surname>Pavan Kumar Anand</surname></name><name><surname>Ranjay Krishna</surname></name><name><surname>Linda</surname><given-names>S.</given-names></name></person-group><article-title>Quilt-1m: One million image-text pairs for histopathology</article-title><source>Adv. Neural Inf. Process. Syst.</source><volume>36</volume><year>2024</year></element-citation></ref><ref id="bib128"><label>128</label><element-citation publication-type="journal" id="sbref105"><person-group person-group-type="author"><name><surname>Ochi</surname><given-names>M.</given-names></name><name><surname>Daisuke Komura</surname></name><name><surname>Takumi Onoyama</surname></name><name><surname>Koki Shinbo</surname></name><name><surname>Haruya Endo</surname></name><name><surname>Odaka</surname><given-names>H.</given-names></name><name><surname>Miwako Kakiuchi</surname></name><name><surname>Katoh</surname><given-names>H.</given-names></name><name><surname>Tetsuo Ushiku</surname></name><name><surname>Shumpei</surname><given-names>I.</given-names></name></person-group><article-title>Registered multi-device/staining histology image dataset for domain-agnostic machine learning models</article-title><source>Scientific Data</source><volume>11</volume><issue>1</issue><year>2024</year><fpage>330</fpage><pub-id pub-id-type="pmid">38570515</pub-id>
</element-citation></ref><ref id="bib129"><label>129</label><element-citation publication-type="journal" id="sbref106"><person-group person-group-type="author"><name><surname>Jaume</surname><given-names>G.</given-names></name><name><surname>Doucet</surname><given-names>P.</given-names></name><name><surname>Song</surname><given-names>A.H.</given-names></name><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Almagro-P&#x000e9;rez</surname><given-names>C.</given-names></name><name><surname>Wagner</surname><given-names>S.J.</given-names></name><name><surname>Vaidya</surname><given-names>A.J.</given-names></name><etal/></person-group><article-title>Hest-1k: A dataset for spatial transcriptomics and histology image analysis</article-title><source>arXiv preprint arXiv</source><year>2024</year><object-id pub-id-type="publisher-id">2406.16192</object-id></element-citation></ref><ref id="bib130"><label>130</label><element-citation publication-type="journal" id="sbref107"><person-group person-group-type="author"><name><surname>Aubreville</surname><given-names>M.</given-names></name><name><surname>Wilm</surname><given-names>F.</given-names></name><name><surname>Stathonikos</surname><given-names>N.</given-names></name><name><surname>Breininger</surname><given-names>K.</given-names></name><name><surname>Donovan</surname><given-names>T.A.</given-names></name><name><surname>Jabari</surname><given-names>S.</given-names></name><etal/></person-group><article-title>A comprehensive multi-domain dataset for mitotic figure detection</article-title><source>Sci Data</source><volume>10</volume><year>2023</year><fpage>484</fpage><pub-id pub-id-type="doi">10.1038/s41597-023-02327-4</pub-id><pub-id pub-id-type="pmid">37491536</pub-id>
</element-citation></ref><ref id="bib131"><label>131</label><element-citation publication-type="journal" id="sbref108"><person-group person-group-type="author"><name><surname>Kanwal</surname><given-names>N.</given-names></name><name><surname>Khoraminia</surname><given-names>F.</given-names></name><name><surname>Kiraz</surname><given-names>U.</given-names></name><name><surname>Mosquera-Zamudio</surname><given-names>A.</given-names></name><name><surname>Monteagudo</surname><given-names>C.</given-names></name><name><surname>Janssen</surname><given-names>E.A.M.</given-names></name><name><surname>Zuiverloon</surname><given-names>T.C.M.</given-names></name><name><surname>Rong</surname><given-names>C.</given-names></name><name><surname>Engan</surname><given-names>K.</given-names></name></person-group><article-title>Equipping computational pathology systems with artifact processing pipelines: a showcase for computation and performance trade-offs</article-title><source>BMC Medical Informatics and Decision Making 24</source><volume>1</volume><year>2024</year><fpage>288</fpage></element-citation></ref><ref id="bib132"><label>132</label><element-citation publication-type="journal" id="sbref109"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>C.</given-names></name><name><surname>Talawalla</surname><given-names>T.</given-names></name><name><surname>Toth</surname><given-names>R.</given-names></name><name><surname>Ambekar</surname><given-names>A.</given-names></name><name><surname>Rea</surname><given-names>K.</given-names></name><name><surname>Chamian</surname><given-names>O.</given-names></name><etal/></person-group><article-title>PatchSorter: a high throughput deep learning digital pathology tool for object labeling</article-title><source>Npj Digit Med</source><volume>7</volume><year>2024</year><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/s41746-024-01150-4</pub-id><pub-id pub-id-type="pmid">38172429</pub-id>
</element-citation></ref><ref id="bib133"><label>133</label><element-citation publication-type="journal" id="sbref110"><person-group person-group-type="author"><name><surname>Das</surname><given-names>A.</given-names></name><name><surname>Nair</surname><given-names>M.S.</given-names></name><name><surname>Peter</surname><given-names>D.S.</given-names></name></person-group><article-title>Batch mode active learning on the riemannian manifold for automated scoring of nuclear pleomorphism in breast cancer</article-title><source>Artif Intell Med</source><volume>103</volume><year>2020</year><object-id pub-id-type="publisher-id">101805</object-id><pub-id pub-id-type="doi">10.1016/j.artmed.2020.101805</pub-id></element-citation></ref><ref id="bib134"><label>134</label><element-citation publication-type="journal" id="sbref111"><person-group person-group-type="author"><name><surname>Lutnick</surname><given-names>B.</given-names></name><name><surname>Ginley</surname><given-names>B.</given-names></name><name><surname>Govind</surname><given-names>D.</given-names></name><name><surname>McGarry</surname><given-names>S.D.</given-names></name><name><surname>LaViolette</surname><given-names>P.S.</given-names></name><name><surname>Yacoub</surname><given-names>R.</given-names></name><etal/></person-group><article-title>An integrated iterative annotation technique for easing neural network training in medical image analysis</article-title><source>Nat Mach Intell</source><volume>1</volume><year>2019</year><fpage>112</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1038/s42256-019-0018-3</pub-id><pub-id pub-id-type="pmid">31187088</pub-id>
</element-citation></ref><ref id="bib135"><label>135</label><element-citation publication-type="journal" id="sbref112"><person-group person-group-type="author"><name><surname>Simard</surname><given-names>M.</given-names></name><name><surname>Shen</surname><given-names>Z.</given-names></name><name><surname>Hawkins</surname><given-names>M.A.</given-names></name><name><surname>Collins-Fekete</surname><given-names>C.-A.</given-names></name></person-group><article-title>Immunocto: a massive immune cell database auto-generated for histopathology</article-title><source>arXiv preprint arXiv</source><year>2024</year><object-id pub-id-type="publisher-id">2406.02618</object-id></element-citation></ref><ref id="bib136"><label>136</label><element-citation publication-type="journal" id="sbref113"><person-group person-group-type="author"><name><surname>Louis</surname><given-names>D.N.</given-names></name><name><surname>Perry</surname><given-names>A.</given-names></name><name><surname>Wesseling</surname><given-names>P.</given-names></name><name><surname>Brat</surname><given-names>D.J.</given-names></name><name><surname>Cree</surname><given-names>I.A.</given-names></name><name><surname>Figarella-Branger</surname><given-names>D.</given-names></name><etal/></person-group><article-title>The 2021 WHO classification of tumors of the central nervous system: a summary</article-title><source>Neuro-Oncol</source><volume>23</volume><year>2021</year><fpage>1231</fpage><lpage>1251</lpage><pub-id pub-id-type="doi">10.1093/neuonc/noab106</pub-id><pub-id pub-id-type="pmid">34185076</pub-id>
</element-citation></ref><ref id="bib137"><label>137</label><element-citation publication-type="journal" id="sbref114"><person-group person-group-type="author"><name><surname>Tiard</surname><given-names>A.</given-names></name><name><surname>Wong</surname><given-names>A.</given-names></name><name><surname>Ho</surname><given-names>D.J.</given-names></name><name><surname>Wu</surname><given-names>Y.</given-names></name><name><surname>Nof</surname><given-names>E.</given-names></name><name><surname>Goh</surname><given-names>A.C.</given-names></name><name><surname>Soatto</surname><given-names>S.</given-names></name><name><surname>Nadeem</surname><given-names>S.</given-names></name></person-group><article-title>Stain-invariant self supervised learning for histopathology image analysis</article-title><source>arXiv preprint arXiv</source><year>2022</year><object-id pub-id-type="publisher-id">2211.07590</object-id></element-citation></ref><ref id="bib138"><label>138</label><element-citation publication-type="journal" id="sbref115"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J.</given-names></name><name><surname>Wei</surname><given-names>C.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Shen</surname><given-names>W.</given-names></name><name><surname>Xie</surname><given-names>C.</given-names></name><name><surname>Yuille</surname><given-names>A.</given-names></name><etal/></person-group><source>iBOT: Image BERT Pre-Train Online Tokenizer</source><year>2022</year><pub-id pub-id-type="doi">10.48550/arXiv.2111.07832</pub-id></element-citation></ref><ref id="bib139"><label>139</label><mixed-citation publication-type="other" id="othref0120">Oquab M., Darcet T., Moutakanni T., Vo H., Szafraniec M., Khalidov V., et al. DINOv2: Learning Robust Visual Features without Supervision 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2304.07193</pub-id>.</mixed-citation></ref><ref id="bib140"><label>140</label><element-citation publication-type="journal" id="sbref116"><person-group person-group-type="author"><name><surname>Vorontsov</surname><given-names>E.</given-names></name><name><surname>Bozkurt</surname><given-names>A.</given-names></name><name><surname>Casson</surname><given-names>A.</given-names></name><name><surname>Shaikovski</surname><given-names>G.</given-names></name><name><surname>Zelechowski</surname><given-names>M.</given-names></name><name><surname>Severson</surname><given-names>K.</given-names></name><etal/></person-group><article-title>A foundation model for clinical-grade computational pathology and rare cancers detection</article-title><source>Nat Med</source><year>2024</year><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41591-024-03141-0</pub-id><pub-id pub-id-type="pmid">38242978</pub-id>
</element-citation></ref><ref id="bib141"><label>141</label><element-citation publication-type="journal" id="sbref117"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>M.</given-names></name><name><surname>Song</surname><given-names>H.</given-names></name><name><surname>Park</surname><given-names>S.</given-names></name><name><surname>Yoo</surname><given-names>D.</given-names></name><name><surname>Pereira</surname><given-names>S.</given-names></name></person-group><article-title>Benchmarking Self-Supervised</article-title><source>Learn Divers Pathol Datasets</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2212.04690</pub-id></element-citation></ref><ref id="bib142"><label>142</label><element-citation publication-type="journal" id="sbref118"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>S.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Yang</surname><given-names>W.</given-names></name><etal/></person-group><article-title>Transformer-based unsupervised contrastive learning for histopathological image classification</article-title><source>Med Image Anal</source><volume>81</volume><year>2022</year><object-id pub-id-type="publisher-id">102559</object-id><pub-id pub-id-type="doi">10.1016/j.media.2022.102559</pub-id></element-citation></ref><ref id="bib143"><label>143</label><mixed-citation publication-type="other" id="othref0125">Dippel J., Feulner B., Winterhoff T., Milbich T., Tietz S., Schallenberg S., et al. RudolfV: A Foundation Model by Pathologists for Pathologists 2024. https://doi.org/10.48550/arXiv.2401.04079.</mixed-citation></ref><ref id="bib144"><label>144</label><mixed-citation publication-type="other" id="othref0130">Filiot A., Jacob P., Kain A.M., Saillard C. Phikon-v2, A large and public feature extractor for biomarker prediction 2024. https://doi.org/10.48550/arXiv.2409.09173.</mixed-citation></ref><ref id="bib145"><label>145</label><mixed-citation publication-type="other" id="othref0135">Zimmermann E., Vorontsov E., Viret J., Casson A., Zelechowski M., Shaikovski G., et al. Virchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology 2024. https://doi.org/10.48550/arXiv.2408.00738.</mixed-citation></ref><ref id="bib146"><label>146</label><mixed-citation publication-type="other" id="othref0140">Neidlinger P., Nahhas O.S.M.E., Muti H.S., Lenz T., Hoffmeister M., Brenner H., et al. Benchmarking foundation models as feature extractors for weakly-supervised computational pathology 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2408.15823</pub-id>.</mixed-citation></ref><ref id="bib147"><label>147</label><mixed-citation publication-type="other" id="othref0145">Ding J., Ma S., Dong L., Zhang X., Huang S., Wang W., et al. LongNet: Scaling Transformers to 1,000,000,000 Tokens 2023. <pub-id pub-id-type="doi">10.48550/arXiv.2307.02486</pub-id>.</mixed-citation></ref><ref id="bib148"><label>148</label><mixed-citation publication-type="other" id="othref0150">Jaegle A., Gimeno F., Brock A., Zisserman A., Vinyals O., Carreira J. Perceiver: General Perception with Iterative Attention 2021. <pub-id pub-id-type="doi">10.48550/arXiv.2103.03206</pub-id>.</mixed-citation></ref><ref id="bib149"><label>149</label><element-citation publication-type="journal" id="sbref119"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Gu</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>Usuyama</surname><given-names>N.</given-names></name><name><surname>Lee</surname><given-names>H.H.</given-names></name><name><surname>Kiblawi</surname><given-names>S.</given-names></name><etal/></person-group><article-title>A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities</article-title><source>Nat Methods</source><year>2024</year><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41592-024-02499-w</pub-id><pub-id pub-id-type="pmid">38212549</pub-id>
</element-citation></ref><ref id="bib150"><label>150</label><element-citation publication-type="journal" id="sbref120"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Wan</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Qu</surname><given-names>K.</given-names></name><name><surname>Zhou</surname><given-names>X.</given-names></name><name><surname>He</surname><given-names>J.</given-names></name><etal/></person-group><article-title>SegAnyPath: a foundation model for multi-resolution stain-variant and multi-task pathology image segmentation</article-title><comment>1&#x02013;1</comment><source>IEEE Trans Med Imaging</source><year>2024</year><pub-id pub-id-type="doi">10.1109/TMI.2024.3501352</pub-id></element-citation></ref><ref id="bib151"><label>151</label><element-citation publication-type="journal" id="sbref121"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>S.</given-names></name><name><surname>Vu</surname><given-names>Q.D.</given-names></name><name><surname>Jahanifar</surname><given-names>M.</given-names></name><name><surname>Raza</surname><given-names>S.E.A.</given-names></name><name><surname>Minhas</surname><given-names>F.</given-names></name><name><surname>Snead</surname><given-names>D.</given-names></name><etal/></person-group><article-title>One model is all you need: Multi-task learning enables simultaneous histology image segmentation and classification</article-title><source>Med Image Anal</source><volume>83</volume><year>2023</year><object-id pub-id-type="publisher-id">102685</object-id><pub-id pub-id-type="doi">10.1016/j.media.2022.102685</pub-id></element-citation></ref><ref id="bib152"><label>152</label><mixed-citation publication-type="other" id="othref0155">Tellez D., H&#x000f6;ppener D., Verhoef C., Gr&#x000fc;nhagen D., Nierop P., Drozdzal M., et al. Extending Unsupervised Neural Image Compression With Supervised Multitask Learning. Proc. Third Conf. Med. Imaging Deep Learn., PMLR; 2020, p. 770&#x02013;783.</mixed-citation></ref><ref id="bib153"><label>153</label><element-citation publication-type="journal" id="sbref122"><person-group person-group-type="author"><name><surname>Mormont</surname><given-names>R.</given-names></name><name><surname>Pierre Geurts</surname></name><name><surname>Rapha&#x000eb;l</surname><given-names>M.</given-names></name></person-group><article-title>Multi-task pre-training of deep neural networks for digital pathology</article-title><source>IEEE journal of biomedical and health informatics</source><volume>25</volume><issue>2</issue><year>2020</year><fpage>412</fpage><lpage>421</lpage></element-citation></ref><ref id="bib154"><label>154</label><element-citation publication-type="journal" id="sbref123"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Zhao</surname><given-names>J.</given-names></name><name><surname>Marostica</surname><given-names>E.</given-names></name><name><surname>Yuan</surname><given-names>W.</given-names></name><name><surname>Jin</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><etal/></person-group><article-title>A pathology foundation model for cancer diagnosis and prognosis prediction</article-title><source>Nature</source><year>2024</year><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41586-024-07894-z</pub-id></element-citation></ref><ref id="bib155"><label>155</label><mixed-citation publication-type="other" id="othref0160">Lenz T., Neidlinger P., Ligero M., W&#x000f6;lflein G., Treeck M. van, Kather J.N. Unsupervised Foundation Model-Agnostic Slide-Level Representation Learning 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2411.13623</pub-id>.</mixed-citation></ref><ref id="bib156"><label>156</label><mixed-citation publication-type="other" id="othref0165">Min W., Shi Z., Zhang J., Wan J., Wang C. Multimodal contrastive learning for spatial gene expression prediction using histology images 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2407.08216</pub-id>.</mixed-citation></ref><ref id="bib157"><label>157</label><element-citation publication-type="journal" id="sbref124"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Z.</given-names></name><name><surname>Bianchi</surname><given-names>F.</given-names></name><name><surname>Yuksekgonul</surname><given-names>M.</given-names></name><name><surname>Montine</surname><given-names>T.J.</given-names></name><name><surname>Zou</surname><given-names>J.</given-names></name></person-group><article-title>A visual&#x02013;language foundation model for pathology image analysis using medical Twitter</article-title><source>Nat Med</source><volume>29</volume><year>2023</year><fpage>2307</fpage><lpage>2316</lpage><pub-id pub-id-type="doi">10.1038/s41591-023-02504-3</pub-id><pub-id pub-id-type="pmid">37592105</pub-id>
</element-citation></ref><ref id="bib158"><label>158</label><element-citation publication-type="journal" id="sbref125"><person-group person-group-type="author"><name><surname>Steyaert</surname><given-names>S.</given-names></name><name><surname>Pizurica</surname><given-names>M.</given-names></name><name><surname>Nagaraj</surname><given-names>D.</given-names></name><name><surname>Khandelwal</surname><given-names>P.</given-names></name><name><surname>Hernandez-Boussard</surname><given-names>T.</given-names></name><name><surname>Gentles</surname><given-names>A.J.</given-names></name><etal/></person-group><article-title>Multimodal data fusion for cancer biomarker discovery with deep learning</article-title><source>Nat Mach Intell</source><volume>5</volume><year>2023</year><fpage>351</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/s42256-023-00633-5</pub-id><pub-id pub-id-type="pmid">37693852</pub-id>
</element-citation></ref><ref id="bib159"><label>159</label><element-citation publication-type="journal" id="sbref126"><person-group person-group-type="author"><name><surname>Steyaert</surname><given-names>S.</given-names></name><name><surname>Qiu</surname><given-names>Y.L.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><name><surname>Mukherjee</surname><given-names>P.</given-names></name><name><surname>Vogel</surname><given-names>H.</given-names></name><name><surname>Gevaert</surname><given-names>O.</given-names></name></person-group><article-title>Multimodal deep learning to predict prognosis in adult and pediatric brain tumors</article-title><source>Commun Med</source><volume>3</volume><year>2023</year><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1038/s43856-023-00276-y</pub-id><pub-id pub-id-type="pmid">36596859</pub-id>
</element-citation></ref><ref id="bib160"><label>160</label><element-citation publication-type="journal" id="sbref127"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Chen</surname><given-names>T.Y.</given-names></name><name><surname>Lipkova</surname><given-names>J.</given-names></name><name><surname>Noor</surname><given-names>Z.</given-names></name><etal/></person-group><article-title>Pan-cancer integrative histology-genomic analysis via multimodal deep learning</article-title><comment>e6</comment><source>Cancer Cell</source><volume>40</volume><year>2022</year><fpage>865</fpage><lpage>878</lpage><pub-id pub-id-type="doi">10.1016/j.ccell.2022.07.004</pub-id><pub-id pub-id-type="pmid">35944502</pub-id>
</element-citation></ref><ref id="bib161"><label>161</label><element-citation publication-type="journal" id="sbref128"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Rodig</surname><given-names>S.J.</given-names></name><name><surname>Lindeman</surname><given-names>N.I.</given-names></name><etal/></person-group><article-title>Pathomic fusion: an integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis</article-title><source>IEEE Trans Med Imaging</source><volume>41</volume><year>2022</year><fpage>757</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1109/TMI.2020.3021387</pub-id><pub-id pub-id-type="pmid">32881682</pub-id>
</element-citation></ref><ref id="bib162"><label>162</label><element-citation publication-type="journal" id="sbref129"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>C.</given-names></name><name><surname>Daisuke Komura</surname></name><name><surname>Mieko Ochi</surname></name><name><surname>Miwako Kakiuchi</surname></name><name><surname>Katoh</surname><given-names>H.</given-names></name><name><surname>Tetsuo Ushiku</surname></name><name><surname>Shumpei</surname><given-names>I.</given-names></name></person-group><article-title>SegRep: Mask-Supervised Learning for Segment Representation in Pathology Images</article-title><source>IEEE Access</source><year>2024</year></element-citation></ref><ref id="bib163"><label>163</label><element-citation publication-type="journal" id="sbref130"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>Y.</given-names></name><name><surname>Zhou</surname><given-names>J.</given-names></name><name><surname>Xun</surname><given-names>X.</given-names></name><name><surname>Johnston</surname><given-names>L.</given-names></name><name><surname>Wei</surname><given-names>T.</given-names></name><name><surname>Gao</surname><given-names>R.</given-names></name><etal/></person-group><article-title>Deep learning for oncologic treatment outcomes and endpoints evaluation from CT scans in liver cancer</article-title><source>Npj Precis Oncol</source><volume>8</volume><year>2024</year><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1038/s41698-024-00754-z</pub-id><pub-id pub-id-type="pmid">38167869</pub-id>
</element-citation></ref><ref id="bib164"><label>164</label><element-citation publication-type="journal" id="sbref131"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>Y.</given-names></name><name><surname>Zhou</surname><given-names>J.</given-names></name><name><surname>Xun</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Wei</surname><given-names>T.</given-names></name><name><surname>Gao</surname><given-names>R.</given-names></name><etal/></person-group><article-title>CT-based multimodal deep learning for non-invasive overall survival prediction in advanced hepatocellular carcinoma patients treated with immunotherapy</article-title><source>Insights Imaging</source><volume>15</volume><year>2024</year><fpage>214</fpage><pub-id pub-id-type="doi">10.1186/s13244-024-01784-8</pub-id><pub-id pub-id-type="pmid">39186192</pub-id>
</element-citation></ref><ref id="bib165"><label>165</label><element-citation publication-type="journal" id="sbref132"><person-group person-group-type="author"><name><surname>Van Sloun</surname></name><name><surname>Ruud</surname><given-names>J.G.</given-names></name><name><surname>Regev Cohen</surname></name><name><surname>Yonina</surname><given-names>C.E.</given-names></name></person-group><article-title>Deep learning in ultrasound imaging</article-title><source>Proceedings of the IEEE</source><volume>108</volume><issue>1</issue><year>2019</year><fpage>11</fpage><lpage>29</lpage></element-citation></ref><ref id="bib166"><label>166</label><element-citation publication-type="journal" id="sbref133"><person-group person-group-type="author"><name><surname>Song</surname><given-names>A.H.</given-names></name><name><surname>Williams</surname><given-names>M.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Chow</surname><given-names>S.S.L.</given-names></name><name><surname>Jaume</surname><given-names>G.</given-names></name><name><surname>Gao</surname><given-names>G.</given-names></name><etal/></person-group><article-title>Analysis of 3D pathology samples using weakly supervised AI</article-title><source>Cell</source><volume>187</volume><year>2024</year><fpage>2502</fpage><lpage>2520.e17</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2024.03.035</pub-id><pub-id pub-id-type="pmid">38729110</pub-id>
</element-citation></ref><ref id="bib167"><label>167</label><element-citation publication-type="journal" id="sbref134"><person-group person-group-type="author"><name><surname>Ert&#x000fc;rk</surname><given-names>A.</given-names></name></person-group><article-title>Deep 3D histology powered by tissue clearing, omics and AI</article-title><source>Nat Methods</source><volume>21</volume><year>2024</year><fpage>1153</fpage><lpage>1165</lpage><pub-id pub-id-type="doi">10.1038/s41592-024-02327-1</pub-id><pub-id pub-id-type="pmid">38997593</pub-id>
</element-citation></ref><ref id="bib168"><label>168</label><element-citation publication-type="journal" id="sbref135"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Chauhan</surname><given-names>S.</given-names></name><name><surname>Gong</surname><given-names>C.</given-names></name><name><surname>Dayton</surname><given-names>H.</given-names></name><name><surname>Xu</surname><given-names>C.</given-names></name><name><surname>De La Cruz</surname><given-names>E.D.</given-names></name><etal/></person-group><article-title>Low-cost and scalable projected light-sheet microscopy for the high-resolution imaging of cleared tissue and living samples</article-title><source>Nat Biomed Eng</source><year>2024</year><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1038/s41551-024-01249-9</pub-id><pub-id pub-id-type="pmid">38253670</pub-id>
</element-citation></ref><ref id="bib169"><label>169</label><element-citation publication-type="book" id="sbref136"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>G.</given-names></name><name><surname>Song</surname><given-names>A.H.</given-names></name><name><surname>Wang</surname><given-names>F.</given-names></name><name><surname>Brenes</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>R.</given-names></name><name><surname>Chow</surname><given-names>S.S.L.</given-names></name><name><surname>Bishop</surname><given-names>K.W.</given-names></name><name><surname>True</surname><given-names>L.D.</given-names></name><name><surname>Mahmood</surname><given-names>F.</given-names></name><name><surname>Liu</surname><given-names>J.T.C.</given-names></name></person-group><part-title>Triage of 3D pathology data via 2.5 D multiple-instance learning to guide pathologist assessments</part-title><series>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</series><year>2024</year><publisher-name>IEEE</publisher-name><fpage>6955</fpage><lpage>6965</lpage></element-citation></ref><ref id="bib170"><label>170</label><element-citation publication-type="journal" id="sbref137"><person-group person-group-type="author"><name><surname>Kurz</surname><given-names>A.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>H.</given-names></name><name><surname>Kather</surname><given-names>J.N.</given-names></name><name><surname>Schneider</surname><given-names>L.</given-names></name><name><surname>Bucher</surname><given-names>T.C.</given-names></name><name><surname>Brinker</surname><given-names>T.J.</given-names></name></person-group><article-title>3-dimensional reconstruction from histopathological sections: a systematic review</article-title><source>Lab Invest</source><volume>104</volume><year>2024</year><object-id pub-id-type="publisher-id">102049</object-id><pub-id pub-id-type="doi">10.1016/j.labinv.2024.102049</pub-id></element-citation></ref><ref id="bib171"><label>171</label><element-citation publication-type="journal" id="sbref138"><person-group person-group-type="author"><name><surname>Jansen</surname><given-names>I.</given-names></name><name><surname>Lucas</surname><given-names>M.</given-names></name><name><surname>Savci-Heijink</surname><given-names>C.D.</given-names></name><name><surname>Meijer</surname><given-names>S.L.</given-names></name><name><surname>Liem</surname><given-names>E.I.M.L.</given-names></name><name><surname>de Boer</surname><given-names>O.J.</given-names></name><etal/></person-group><article-title>Three-dimensional histopathological reconstruction of bladder tumours</article-title><source>Diagn Pathol</source><volume>14</volume><year>2019</year><fpage>25</fpage><pub-id pub-id-type="doi">10.1186/s13000-019-0803-7</pub-id><pub-id pub-id-type="pmid">30922406</pub-id>
</element-citation></ref><ref id="bib172"><label>172</label><element-citation publication-type="journal" id="sbref139"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>N.</given-names></name><name><surname>Magee</surname><given-names>D.</given-names></name><name><surname>Song</surname><given-names>Y.</given-names></name><name><surname>Brabazon</surname><given-names>K.</given-names></name><name><surname>Shires</surname><given-names>M.</given-names></name><name><surname>Crellin</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Toward routine use of 3d histopathology as a research tool</article-title><source>Am J Pathol</source><volume>180</volume><year>2012</year><fpage>1835</fpage><lpage>1842</lpage><pub-id pub-id-type="doi">10.1016/j.ajpath.2012.01.033</pub-id><pub-id pub-id-type="pmid">22490922</pub-id>
</element-citation></ref><ref id="bib173"><label>173</label><element-citation publication-type="journal" id="sbref140"><person-group person-group-type="author"><name><surname>Sakamoto</surname><given-names>H.</given-names></name><name><surname>Nishimura</surname><given-names>M.</given-names></name><name><surname>Teplov</surname><given-names>A.</given-names></name><name><surname>Leung</surname><given-names>G.</given-names></name><name><surname>Ntiamoah</surname><given-names>P.</given-names></name><name><surname>Cesmecioglu</surname><given-names>E.</given-names></name><etal/></person-group><article-title>A pilot study of micro-CT-based whole tissue imaging (WTI) on endoscopic submucosal dissection (ESD) specimens</article-title><source>Sci Rep</source><volume>12</volume><year>2022</year><fpage>9889</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-13907-6</pub-id><pub-id pub-id-type="pmid">35701447</pub-id>
</element-citation></ref><ref id="bib174"><label>174</label><element-citation publication-type="journal" id="sbref141"><person-group person-group-type="author"><name><surname>Mai</surname><given-names>H.</given-names></name><name><surname>Lu</surname><given-names>D.</given-names></name></person-group><article-title>Tissue clearing and its applications in human tissues: a review</article-title><source>VIEW</source><volume>5</volume><year>2024</year><object-id pub-id-type="publisher-id">20230046</object-id><pub-id pub-id-type="doi">10.1002/VIW.20230046</pub-id></element-citation></ref><ref id="bib175"><label>175</label><element-citation publication-type="journal" id="sbref142"><person-group person-group-type="author"><name><surname>Glaser</surname><given-names>A.K.</given-names></name><name><surname>Bishop</surname><given-names>K.W.</given-names></name><name><surname>Barner</surname><given-names>L.A.</given-names></name><name><surname>Susaki</surname><given-names>E.A.</given-names></name><name><surname>Kubota</surname><given-names>S.I.</given-names></name><name><surname>Gao</surname><given-names>G.</given-names></name><etal/></person-group><article-title>A hybrid open-top light-sheet microscope for versatile multi-scale imaging of cleared tissues</article-title><source>Nat Methods</source><volume>19</volume><year>2022</year><fpage>613</fpage><lpage>619</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01468-5</pub-id><pub-id pub-id-type="pmid">35545715</pub-id>
</element-citation></ref><ref id="bib176"><label>176</label><mixed-citation publication-type="other" id="othref0170">Radford A., Kim J.W., Hallacy C., Ramesh A., Goh G., Agarwal S., et al. Learning Transferable Visual Models From Natural Language Supervision 2021. <pub-id pub-id-type="doi">10.48550/arXiv.2103.00020</pub-id>.</mixed-citation></ref><ref id="bib177"><label>177</label><element-citation publication-type="journal" id="sbref143"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Vasudevan</surname><given-names>V.</given-names></name><name><surname>Yeung</surname><given-names>L.</given-names></name><name><surname>Seyedhosseini</surname><given-names>M.</given-names></name><name><surname>Wu</surname><given-names>Y.</given-names></name></person-group><article-title>Coca: Contrastive captioners are image-text foundation models</article-title><source>arXiv preprint arXiv</source><year>2022</year><object-id pub-id-type="publisher-id">2205.01917</object-id><comment>Harvard</comment></element-citation></ref><ref id="bib178"><label>178</label><mixed-citation publication-type="other" id="othref0175">Neidlinger P., Nahhas O.S.M.E., Muti H.S., Lenz T., Hoffmeister M., Brenner H., et al. Benchmarking foundation models as feature extractors for weakly-supervised computational pathology 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2408.15823</pub-id>.</mixed-citation></ref><ref id="bib179"><label>179</label><element-citation publication-type="journal" id="sbref144"><person-group person-group-type="author"><name><surname>Jaume</surname><given-names>G.</given-names></name><name><surname>Oldenburg</surname><given-names>L.</given-names></name><name><surname>Vaidya</surname><given-names>A.</given-names></name><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Peeters</surname><given-names>T.</given-names></name><etal/></person-group><article-title>Transcriptomics-guided</article-title><source>Slide Represent Learn Comput Pathol</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2405.11618</pub-id></element-citation></ref><ref id="bib180"><label>180</label><mixed-citation publication-type="other" id="othref0180">Kumar S., Chatterjee S. HistoSPACE: Histology-Inspired Spatial Transcriptome Prediction And Characterization Engine 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2408.03592</pub-id>.</mixed-citation></ref><ref id="bib181"><label>181</label><mixed-citation publication-type="other" id="othref0185">Zhong Y., Zhang J., Ren X. Spatial Transcriptomics Prediction from Histology Images at Single-cell Resolution using RedeHist 2024:2024.06.17.599464. <pub-id pub-id-type="doi">10.1101/2024.06.17.599464</pub-id>.</mixed-citation></ref><ref id="bib182"><label>182</label><element-citation publication-type="journal" id="sbref145"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>L.</given-names></name><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name></person-group><article-title>THItoGene: a deep learning method for predicting spatial transcriptomics from histological images</article-title><source>Brief Bioinform</source><volume>25</volume><year>2024</year><object-id pub-id-type="publisher-id">bbad464</object-id><pub-id pub-id-type="doi">10.1093/bib/bbad464</pub-id></element-citation></ref><ref id="bib183"><label>183</label><element-citation publication-type="journal" id="sbref146"><person-group person-group-type="author"><name><surname>Rosner</surname><given-names>A.</given-names></name><name><surname>Miyoshi</surname><given-names>K.</given-names></name><name><surname>Landesman-Bollag</surname><given-names>E.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name><name><surname>Seldin</surname><given-names>D.C.</given-names></name><name><surname>Moser</surname><given-names>A.R.</given-names></name><etal/></person-group><article-title>Pathway pathology</article-title><source>Am J Pathol</source><volume>161</volume><year>2002</year><fpage>1087</fpage><lpage>1097</lpage><pub-id pub-id-type="pmid">12213737</pub-id>
</element-citation></ref><ref id="bib184"><label>184</label><mixed-citation publication-type="other" id="othref0190">Spatial Gene Expression. 10x Genomics n.d. <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/spatial-gene-expression" id="ir0125">&#x02329;https://www.10xgenomics.com/products/spatial-gene-expression&#x0232a;</ext-link> (accessed December 5, 2024).</mixed-citation></ref><ref id="bib185"><label>185</label><mixed-citation publication-type="other" id="othref0195">HD Products: Visium HD Spatial Gene Expression. 10x Genomics n.d. <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/products/visium-hd-spatial-gene-expression" id="ir0130">&#x02329;https://www.10xgenomics.com/products/visium-hd-spatial-gene-expression&#x0232a;</ext-link> (accessed December 5, 2024).</mixed-citation></ref><ref id="bib186"><label>186</label><mixed-citation publication-type="other" id="othref0200">CosMx SMI - Single-Cell Imaging. NanoString 2024. <ext-link ext-link-type="uri" xlink:href="https://nanostring.com/products/cosmx-spatial-molecular-imager/single-cell-imaging-overview/" id="ir0135">&#x02329;https://nanostring.com/products/cosmx-spatial-molecular-imager/single-cell-imaging-overview/&#x0232a;</ext-link> (accessed December 5, 2024).</mixed-citation></ref><ref id="bib187"><label>187</label><mixed-citation publication-type="other" id="othref0205">MERFISH Spatial Transcriptomics. Vizgen n.d. <ext-link ext-link-type="uri" xlink:href="https://vizgen.com/technology/" id="ir0140">&#x02329;https://vizgen.com/technology/&#x0232a;</ext-link> (accessed December 5, 2024).</mixed-citation></ref><ref id="bib188"><label>188</label><mixed-citation publication-type="other" id="othref0210">Xenium In Situ Platform. 10x Genomics 2024. <ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/platforms/xenium" id="ir0145">&#x02329;https://www.10xgenomics.com/platforms/xenium&#x0232a;</ext-link> (accessed December 5, 2024).</mixed-citation></ref><ref id="bib189"><label>189</label><element-citation publication-type="journal" id="sbref147"><person-group person-group-type="author"><name><surname>Schmauch</surname><given-names>B.</given-names></name><name><surname>Romagnoni</surname><given-names>A.</given-names></name><name><surname>Pronier</surname><given-names>E.</given-names></name><name><surname>Saillard</surname><given-names>C.</given-names></name><name><surname>Maill&#x000e9;</surname><given-names>P.</given-names></name><name><surname>Calderaro</surname><given-names>J.</given-names></name><etal/></person-group><article-title>A deep learning model to predict RNA-Seq expression of tumours from whole slide images</article-title><source>Nat Commun</source><volume>11</volume><year>2020</year><fpage>3877</fpage><pub-id pub-id-type="doi">10.1038/s41467-020-17678-4</pub-id><pub-id pub-id-type="pmid">32747659</pub-id>
</element-citation></ref><ref id="bib190"><label>190</label><element-citation publication-type="journal" id="sbref148"><person-group person-group-type="author"><name><surname>Hoang</surname><given-names>D.-T.</given-names></name><name><surname>Dinstag</surname><given-names>G.</given-names></name><name><surname>Shulman</surname><given-names>E.D.</given-names></name><name><surname>Hermida</surname><given-names>L.C.</given-names></name><name><surname>Ben-Zvi</surname><given-names>D.S.</given-names></name><name><surname>Elis</surname><given-names>E.</given-names></name><etal/></person-group><article-title>A deep-learning framework to predict cancer treatment response from histopathology images through imputed transcriptomics</article-title><source>Nat Cancer</source><volume>5</volume><year>2024</year><fpage>1305</fpage><lpage>1317</lpage><pub-id pub-id-type="doi">10.1038/s43018-024-00793-2</pub-id><pub-id pub-id-type="pmid">38961276</pub-id>
</element-citation></ref><ref id="bib191"><label>191</label><element-citation publication-type="journal" id="sbref149"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>Y.</given-names></name><name><surname>Wei</surname><given-names>Z.</given-names></name><name><surname>Yu</surname><given-names>W.</given-names></name><name><surname>Yin</surname><given-names>R.</given-names></name><name><surname>Yuan</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name><etal/></person-group><article-title>Spatial transcriptomics prediction from histology jointly through Transformer and graph neural networks</article-title><source>Brief Bioinform</source><volume>23</volume><year>2022</year><object-id pub-id-type="publisher-id">bbac297</object-id><pub-id pub-id-type="doi">10.1093/bib/bbac297</pub-id></element-citation></ref><ref id="bib192"><label>192</label><element-citation publication-type="journal" id="sbref150"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>D.</given-names></name><name><surname>Schroeder</surname><given-names>A.</given-names></name><name><surname>Yan</surname><given-names>H.</given-names></name><name><surname>Yang</surname><given-names>H.</given-names></name><name><surname>Hu</surname><given-names>J.</given-names></name><name><surname>Lee</surname><given-names>M.Y.Y.</given-names></name><etal/></person-group><article-title>Inferring super-resolution tissue architecture by integrating spatial transcriptomics with histology</article-title><source>Nat Biotechnol</source><volume>42</volume><year>2024</year><fpage>1372</fpage><lpage>1377</lpage><pub-id pub-id-type="doi">10.1038/s41587-023-02019-9</pub-id><pub-id pub-id-type="pmid">38168986</pub-id>
</element-citation></ref><ref id="bib193"><label>193</label><mixed-citation publication-type="other" id="othref0215">Xie R., Pang K., Chung S.W., Perciani C.T., MacParland S.A., Wang B., et al. Spatially Resolved Gene Expression Prediction from H&#x00026;E Histology Images via Bi-modal Contrastive Learning 2023. <pub-id pub-id-type="doi">10.48550/arXiv.2306.01859</pub-id>.</mixed-citation></ref><ref id="bib194"><label>194</label><element-citation publication-type="journal" id="sbref151"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Y.</given-names></name><name><surname>Liang</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>D.</given-names></name><name><surname>Chang</surname><given-names>Y.</given-names></name><name><surname>Ma</surname><given-names>Q.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>A contrastive learning approach to integrate spatial transcriptomics and histological images</article-title><source>Comput Struct Biotechnol J</source><volume>23</volume><year>2024</year><fpage>1786</fpage><lpage>1795</lpage><pub-id pub-id-type="doi">10.1016/j.csbj.2024.04.039</pub-id><pub-id pub-id-type="pmid">38707535</pub-id>
</element-citation></ref><ref id="bib195"><label>195</label><element-citation publication-type="journal" id="sbref152"><person-group person-group-type="author"><name><surname>Min</surname><given-names>W.</given-names></name><name><surname>Shi</surname><given-names>Z.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Wan</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name></person-group><article-title>Multimodal contrastive learning for spatial gene expression prediction using histology images</article-title><source>Brief Bioinform</source><volume>25</volume><year>2024</year><object-id pub-id-type="publisher-id">bbae551</object-id><pub-id pub-id-type="doi">10.1093/bib/bbae551</pub-id></element-citation></ref><ref id="bib196"><label>196</label><mixed-citation publication-type="other" id="othref0220">Xie R., Pang K., Chung S.W., Perciani C.T., MacParland S.A., Wang B., et al. Spatially Resolved Gene Expression Prediction from H&#x00026;E Histology Images via Bi-modal Contrastive Learning 2023. <pub-id pub-id-type="doi">10.48550/arXiv.2306.01859</pub-id>.</mixed-citation></ref><ref id="bib197"><label>197</label><element-citation publication-type="journal" id="sbref153"><person-group person-group-type="author"><name><surname>Pati</surname><given-names>P.</given-names></name><name><surname>Sofia Karkampouna</surname></name><name><surname>Francesco Bonollo</surname></name><name><surname>Eva Comp&#x000e9;rat</surname></name><name><surname>Martina Radi&#x00107;</surname></name><name><surname>Martin Spahn</surname></name><name><surname>Adriano Martinelli</surname></name><name><surname>Martin Wartenberg</surname></name><name><surname>Marianna Kruithof-de Julio</surname></name><name><surname>Marianna</surname><given-names>R.</given-names></name></person-group><article-title>Accelerating histopathology workflows with generative AI-based virtually multiplexed tumour profiling</article-title><source>Nature Machine Intelligence</source><volume>6</volume><issue>9</issue><year>2024</year><fpage>1077</fpage><lpage>1093</lpage></element-citation></ref><ref id="bib198"><label>198</label><element-citation publication-type="book" id="sbref154"><person-group person-group-type="author"><name><surname>Jaume</surname><given-names>G.</given-names></name><name><surname>Vaidya</surname><given-names>A.</given-names></name><name><surname>Zhang</surname><given-names>A.</given-names></name><name><surname>Song</surname><given-names>A.H.</given-names></name><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Sahai</surname><given-names>S.</given-names></name><name><surname>Mo</surname><given-names>D.</given-names></name><name><surname>Madrigal</surname><given-names>E.</given-names></name><name><surname>Le</surname><given-names>L.P.</given-names></name><name><surname>Mahmood</surname><given-names>F.</given-names></name></person-group><part-title>Multistain Pretraining for Slide Representation Learning in Pathology</part-title><series>European Conference on Computer Vision</series><year>2025</year><publisher-name>Springer</publisher-name><publisher-loc>Cham</publisher-loc><fpage>19</fpage><lpage>37</lpage></element-citation></ref><ref id="bib199"><label>199</label><element-citation publication-type="journal" id="sbref155"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>H.</given-names></name><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>Lin</surname><given-names>Y.</given-names></name><name><surname>Chan</surname><given-names>R.C.K.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>H.</given-names></name></person-group><article-title>Deep learning for computational cytology: a survey</article-title><source>Med Image Anal</source><volume>84</volume><year>2023</year><object-id pub-id-type="publisher-id">102691</object-id><pub-id pub-id-type="doi">10.1016/j.media.2022.102691</pub-id></element-citation></ref><ref id="bib200"><label>200</label><element-citation publication-type="journal" id="sbref156"><person-group person-group-type="author"><name><surname>Hoang</surname><given-names>D.-T.</given-names></name><name><surname>Shulman</surname><given-names>E.D.</given-names></name><name><surname>Turakulov</surname><given-names>R.</given-names></name><name><surname>Abdullaev</surname><given-names>Z.</given-names></name><name><surname>Singh</surname><given-names>O.</given-names></name><name><surname>Campagnolo</surname><given-names>E.M.</given-names></name><name><surname>Lalchungnunga</surname><given-names>H.</given-names></name><etal/></person-group><article-title>Prediction of DNA methylation-based tumor types from histopathology in central nervous system tumors with deep learning</article-title><source>Nature Medicine</source><year>2024</year><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="bib201"><label>201</label><element-citation publication-type="journal" id="sbref157"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>R.J.</given-names></name><name><surname>Lu</surname><given-names>M.Y.</given-names></name><name><surname>Williamson</surname><given-names>D.F.K.</given-names></name><name><surname>Chen</surname><given-names>T.Y.</given-names></name><name><surname>Lipkova</surname><given-names>J.</given-names></name><name><surname>Noor</surname><given-names>Z.</given-names></name><etal/></person-group><article-title>Pan-cancer integrative histology-genomic analysis via multimodal deep learning</article-title><source>Cancer Cell</source><volume>40</volume><year>2022</year><fpage>865</fpage><lpage>878.e6</lpage><pub-id pub-id-type="doi">10.1016/j.ccell.2022.07.004</pub-id><pub-id pub-id-type="pmid">35944502</pub-id>
</element-citation></ref><ref id="bib202"><label>202</label><mixed-citation publication-type="other" id="othref0225">Jaume G., Vaidya A., Chen R., Williamson D., Liang P., Mahmood F. Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2304.06819</pub-id>.</mixed-citation></ref><ref id="bib203"><label>203</label><element-citation publication-type="journal" id="sbref158"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>E.</given-names></name><name><surname>Poli</surname><given-names>M.</given-names></name><name><surname>Durrant</surname><given-names>M.G.</given-names></name><name><surname>Kang</surname><given-names>B.</given-names></name><name><surname>Katrekar</surname><given-names>D.</given-names></name><name><surname>Li</surname><given-names>D.B.</given-names></name><etal/></person-group><article-title>Sequence modeling and design from molecular to genome scale with Evo</article-title><source>Science</source><volume>386</volume><year>2024</year><object-id pub-id-type="publisher-id">eado9336</object-id><pub-id pub-id-type="doi">10.1126/science.ado9336</pub-id></element-citation></ref><ref id="bib204"><label>204</label><mixed-citation publication-type="other" id="othref0230">Zhou Z., Ji Y., Li W., Dutta P., Davuluri R., Liu H. DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2306.15006</pub-id>.</mixed-citation></ref><ref id="bib205"><label>205</label><element-citation publication-type="journal" id="sbref159"><person-group person-group-type="author"><name><surname>Abramson</surname><given-names>J.</given-names></name><name><surname>Jonas Adler</surname></name><name><surname>Jack</surname><given-names>D.</given-names></name><name><surname>Evans</surname><given-names>R.</given-names></name><name><surname>Green</surname><given-names>T.</given-names></name><name><surname>Alexander Pritzel</surname></name><name><surname>Olaf Ronneberger</surname></name><etal/></person-group><article-title>Accurate structure prediction of biomolecular interactions with AlphaFold 3</article-title><source>Nature</source><year>2024</year><fpage>1</fpage><lpage>3</lpage></element-citation></ref><ref id="bib206"><label>206</label><element-citation publication-type="journal" id="sbref160"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>J.</given-names></name><name><surname>Novati</surname><given-names>G.</given-names></name><name><surname>Pan</surname><given-names>J.</given-names></name><name><surname>Bycroft</surname><given-names>C.</given-names></name><name><surname>&#x0017d;emgulyt&#x00117;</surname><given-names>A.</given-names></name><name><surname>Applebaum</surname><given-names>T.</given-names></name><name><surname>Pritzel</surname><given-names>A.</given-names></name><etal/></person-group><article-title>Accurate proteome-wide missense variant effect prediction with AlphaMissense</article-title><source>Science</source><volume>381</volume><issue>6664</issue><year>2023</year><object-id pub-id-type="publisher-id">eadg7492</object-id></element-citation></ref><ref id="bib207"><label>207</label><mixed-citation publication-type="other" id="othref0235">Blankemeier L., Cohen J.P., Kumar A., Veen D.V., Gardezi S.J.S., Paschali M., et al. Merlin: A Vision Language Foundation Model for 3D Computed Tomography 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2406.06512</pub-id>.</mixed-citation></ref><ref id="bib208"><label>208</label><element-citation publication-type="journal" id="sbref161"><person-group person-group-type="author"><name><surname>Pai</surname><given-names>S.</given-names></name><name><surname>Bontempi</surname><given-names>D.</given-names></name><name><surname>Hadzic</surname><given-names>I.</given-names></name><name><surname>Prudente</surname><given-names>V.</given-names></name><name><surname>Soka&#x0010d;</surname><given-names>M.</given-names></name><name><surname>Chaunzwa</surname><given-names>T.L.</given-names></name><etal/></person-group><article-title>Foundation model for cancer imaging biomarkers</article-title><source>Nat Mach Intell</source><volume>6</volume><year>2024</year><fpage>354</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1038/s42256-024-00807-9</pub-id><pub-id pub-id-type="pmid">38523679</pub-id>
</element-citation></ref><ref id="bib209"><label>209</label><mixed-citation publication-type="other" id="othref0240">Barbano C.A., Brunello M., Dufumier B., Grangetto M. Anatomical Foundation Models for Brain MRIs 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2408.07079</pub-id>.</mixed-citation></ref><ref id="bib210"><label>210</label><element-citation publication-type="journal" id="sbref162"><person-group person-group-type="author"><name><surname>Wick</surname><given-names>M.R.</given-names></name></person-group><article-title>The hematoxylin and eosin stain in anatomic pathology-An often-neglected focus of quality assurance in the laboratory</article-title><source>Semin Diagn Pathol</source><volume>36</volume><year>2019</year><fpage>303</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1053/j.semdp.2019.06.003</pub-id><pub-id pub-id-type="pmid">31230963</pub-id>
</element-citation></ref><ref id="bib211"><label>211</label><element-citation publication-type="journal" id="sbref163"><person-group person-group-type="author"><name><surname>Atallah</surname><given-names>N.M.</given-names></name><name><surname>Toss</surname><given-names>M.S.</given-names></name><name><surname>Verrill</surname><given-names>C.</given-names></name><name><surname>Salto-Tellez</surname><given-names>M.</given-names></name><name><surname>Snead</surname><given-names>D.</given-names></name><name><surname>Rakha</surname><given-names>E.A.</given-names></name></person-group><article-title>Potential quality pitfalls of digitalized whole slide image of breast pathology in routine practice</article-title><source>Mod Pathol</source><volume>35</volume><year>2022</year><fpage>903</fpage><lpage>910</lpage><pub-id pub-id-type="doi">10.1038/s41379-021-01000-8</pub-id><pub-id pub-id-type="pmid">34961765</pub-id>
</element-citation></ref><ref id="bib212"><label>212</label><element-citation publication-type="journal" id="sbref164"><person-group person-group-type="author"><name><surname>Yagi</surname><given-names>Y.</given-names></name></person-group><article-title>Color standardization and optimization in Whole Slide Imaging</article-title><source>Diagn Pathol</source><volume>6</volume><year>2011</year><object-id pub-id-type="publisher-id">S15</object-id><pub-id pub-id-type="doi">10.1186/1746-1596-6-S1-S15</pub-id></element-citation></ref><ref id="bib213"><label>213</label><element-citation publication-type="journal" id="sbref165"><person-group person-group-type="author"><name><surname>Vahadane</surname><given-names>A.</given-names></name><name><surname>Peng</surname><given-names>T.</given-names></name><name><surname>Sethi</surname><given-names>A.</given-names></name><name><surname>Albarqouni</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Baust</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Structure-preserving color normalization and sparse stain separation for histological images</article-title><source>IEEE Trans Med Imaging</source><volume>35</volume><year>2016</year><fpage>1962</fpage><lpage>1971</lpage><pub-id pub-id-type="doi">10.1109/TMI.2016.2529665</pub-id><pub-id pub-id-type="pmid">27164577</pub-id>
</element-citation></ref><ref id="bib214"><label>214</label><mixed-citation publication-type="other" id="othref0245">Macenko M., Niethammer M., Marron J.S., Borland D., Woosley J.T., Guan X., et al. A method for normalizing histology slides for quantitative analysis. 2009 IEEE Int. Symp. Biomed. Imaging Nano Macro, 2009, p. 1107&#x02013;1110. <pub-id pub-id-type="doi">10.1109/ISBI.2009.5193250</pub-id>.</mixed-citation></ref><ref id="bib215"><label>215</label><mixed-citation publication-type="other" id="othref0250">CielAl. CielAl/torch-staintools 2024.</mixed-citation></ref><ref id="bib216"><label>216</label><mixed-citation publication-type="other" id="othref0255">Anand D., Ramakrishnan G., Sethi A. Fast GPU-Enabled Color Normalization for Digital Pathology. 2019 Int. Conf. Syst. Signals Image Process. IWSSIP, 2019, p. 219&#x02013;224. <pub-id pub-id-type="doi">10.1109/IWSSIP.2019.8787328</pub-id>.</mixed-citation></ref><ref id="bib217"><label>217</label><element-citation publication-type="journal" id="sbref166"><person-group person-group-type="author"><name><surname>Tellez</surname><given-names>D.</given-names></name><name><surname>Litjens</surname><given-names>G.</given-names></name><name><surname>B&#x000e1;ndi</surname><given-names>P.</given-names></name><name><surname>Bulten</surname><given-names>W.</given-names></name><name><surname>Bokhorst</surname><given-names>J.-M.</given-names></name><name><surname>Ciompi</surname><given-names>F.</given-names></name><etal/></person-group><article-title>Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology</article-title><source>Med Image Anal</source><volume>58</volume><year>2019</year><object-id pub-id-type="publisher-id">101544</object-id><pub-id pub-id-type="doi">10.1016/j.media.2019.101544</pub-id></element-citation></ref><ref id="bib218"><label>218</label><element-citation publication-type="journal" id="sbref167"><person-group person-group-type="author"><name><surname>Faryna</surname><given-names>K.</given-names></name><name><surname>van der Laak</surname><given-names>J.</given-names></name><name><surname>Litjens</surname><given-names>G.</given-names></name></person-group><article-title>Automatic data augmentation to improve generalization of deep learning in H&#x00026;E stained histopathology</article-title><source>Comput Biol Med</source><volume>170</volume><year>2024</year><object-id pub-id-type="publisher-id">108018</object-id><pub-id pub-id-type="doi">10.1016/j.compbiomed.2024.108018</pub-id></element-citation></ref><ref id="bib219"><label>219</label><element-citation publication-type="journal" id="sbref168"><person-group person-group-type="author"><name><surname>Marini</surname><given-names>N.</given-names></name><name><surname>Otalora</surname><given-names>S.</given-names></name><name><surname>Wodzinski</surname><given-names>M.</given-names></name><name><surname>Tomassini</surname><given-names>S.</given-names></name><name><surname>Dragoni</surname><given-names>A.F.</given-names></name><name><surname>Marchand-Maillet</surname><given-names>S.</given-names></name><etal/></person-group><article-title>Data-driven color augmentation for H&#x00026;E stained images in computational pathology</article-title><source>J Pathol Inf</source><volume>14</volume><year>2023</year><object-id pub-id-type="publisher-id">100183</object-id><pub-id pub-id-type="doi">10.1016/j.jpi.2022.100183</pub-id></element-citation></ref><ref id="bib220"><label>220</label><mixed-citation publication-type="other" id="othref0260">Zaffar I., Jaume G., Rajpoot N., Mahmood F. Embedding Space Augmentation for Weakly Supervised Learning in Whole-Slide Images. 2023 IEEE 20th Int. Symp. Biomed. Imaging ISBI, 2023, p. 1&#x02013;4. <pub-id pub-id-type="doi">10.1109/ISBI53787.2023.10230723</pub-id>.</mixed-citation></ref><ref id="bib221"><label>221</label><element-citation publication-type="journal" id="sbref169"><person-group person-group-type="author"><name><surname>de Bel</surname><given-names>T.</given-names></name><name><surname>Bokhorst</surname><given-names>J.-M.</given-names></name><name><surname>van der Laak</surname><given-names>J.</given-names></name><name><surname>Litjens</surname><given-names>G.</given-names></name></person-group><article-title>Residual cyclegan for robust domain transformation of histopathological tissue slides</article-title><source>Med Image Anal</source><volume>70</volume><year>2021</year><object-id pub-id-type="publisher-id">102004</object-id><pub-id pub-id-type="doi">10.1016/j.media.2021.102004</pub-id></element-citation></ref><ref id="bib222"><label>222</label><element-citation publication-type="journal" id="sbref170"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>G.</given-names></name><name><surname>Lou</surname><given-names>W.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name><name><surname>Wan</surname><given-names>X.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><etal/></person-group><article-title>Diffusion-based</article-title><source>Data Augment Nucl Image Segm</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2310.14197</pub-id></element-citation></ref><ref id="bib223"><label>223</label><element-citation publication-type="journal" id="sbref171"><person-group person-group-type="author"><name><surname>Ktena</surname><given-names>I.</given-names></name><name><surname>Wiles</surname><given-names>O.</given-names></name><name><surname>Albuquerque</surname><given-names>I.</given-names></name><name><surname>Rebuffi</surname><given-names>S.-A.</given-names></name><name><surname>Tanno</surname><given-names>R.</given-names></name><name><surname>Roy</surname><given-names>A.G.</given-names></name><etal/></person-group><article-title>Generative models improve fairness of medical classifiers under distribution shifts</article-title><source>Nat Med</source><volume>30</volume><year>2024</year><fpage>1166</fpage><lpage>1173</lpage><pub-id pub-id-type="doi">10.1038/s41591-024-02838-6</pub-id><pub-id pub-id-type="pmid">38600282</pub-id>
</element-citation></ref><ref id="bib224"><label>224</label><element-citation publication-type="journal" id="sbref172"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Z.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Yuan</surname><given-names>C.-A.</given-names></name><name><surname>Qin</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>F.</given-names></name></person-group><article-title>A diffusion model multi-scale feature fusion network for imbalanced medical image classification research</article-title><source>Comput Methods Prog Biomed</source><volume>256</volume><year>2024</year><object-id pub-id-type="publisher-id">108384</object-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2024.108384</pub-id></element-citation></ref><ref id="bib225"><label>225</label><mixed-citation publication-type="other" id="othref0265">Quality assessment of whole-slide images through artifact detection - Grand Challenge. Gd-Challengeorg n.d. <ext-link ext-link-type="uri" xlink:href="https://grand-challenge.org/algorithms/quality-assessment-of-whole-slide-images-through-a/" id="ir0195">&#x02329;https://grand-challenge.org/algorithms/quality-assessment-of-whole-slide-images-through-a/&#x0232a;</ext-link> (accessed September 20, 2024).</mixed-citation></ref><ref id="bib226"><label>226</label><element-citation publication-type="journal" id="sbref173"><person-group person-group-type="author"><name><surname>Jurgas</surname><given-names>A.</given-names></name><name><surname>Wodzinski</surname><given-names>M.</given-names></name><name><surname>D&#x02019;Amato</surname><given-names>M.</given-names></name><name><surname>van der Laak</surname><given-names>J.</given-names></name><name><surname>Atzori</surname><given-names>M.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>H.</given-names></name></person-group><article-title>Improving quality control of whole slide images by explicit artifact augmentation</article-title><source>Sci Rep</source><volume>14</volume><year>2024</year><object-id pub-id-type="publisher-id">17847</object-id><pub-id pub-id-type="doi">10.1038/s41598-024-68667-2</pub-id></element-citation></ref><ref id="bib227"><label>227</label><mixed-citation publication-type="other" id="othref0270">He Z., He J., Ye J., Shen Y. Artifact Restoration in Histology Images with Diffusion Probabilistic Models 2023. <pub-id pub-id-type="doi">10.48550/arXiv.2307.14262</pub-id>.</mixed-citation></ref><ref id="bib228"><label>228</label><element-citation publication-type="journal" id="sbref174"><person-group person-group-type="author"><name><surname>Selvaraju</surname><given-names>R.R.</given-names></name><name><surname>Cogswell</surname><given-names>M.</given-names></name><name><surname>Das</surname><given-names>A.</given-names></name><name><surname>Vedantam</surname><given-names>R.</given-names></name><name><surname>Parikh</surname><given-names>D.</given-names></name><name><surname>Batra</surname><given-names>D.</given-names></name></person-group><article-title>Grad-CAM: visual explanations from deep networks via gradient-based localization</article-title><source>Int J Comput Vis</source><volume>128</volume><year>2020</year><fpage>336</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1007/s11263-019-01228-7</pub-id></element-citation></ref><ref id="bib229"><label>229</label><mixed-citation publication-type="other" id="othref0275">Chattopadhyay A., Sarkar A., Howlader P., Balasubramanian V.N. Grad-CAM+&#x02009;+: Improved Visual Explanations for Deep Convolutional Networks. 2018 IEEE Winter Conf Appl Comput Vis WACV 2018:839&#x02013;847. <pub-id pub-id-type="doi">10.1109/WACV.2018.00097</pub-id>.</mixed-citation></ref><ref id="bib230"><label>230</label><element-citation publication-type="journal" id="sbref175"><person-group person-group-type="author"><name><surname>Dolezal</surname><given-names>J.M.</given-names></name><name><surname>Wolk</surname><given-names>R.</given-names></name><name><surname>Hieromnimon</surname><given-names>H.M.</given-names></name><name><surname>Howard</surname><given-names>F.M.</given-names></name><name><surname>Srisuwananukorn</surname><given-names>A.</given-names></name><name><surname>Karpeyev</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Deep learning generates synthetic cancer histology for explainability and education</article-title><source>NPJ Precis Oncol</source><volume>7</volume><year>2023</year><fpage>49</fpage><pub-id pub-id-type="doi">10.1038/s41698-023-00399-4</pub-id><pub-id pub-id-type="pmid">37248379</pub-id>
</element-citation></ref><ref id="bib231"><label>231</label><element-citation publication-type="journal" id="sbref176"><person-group person-group-type="author"><name><surname>Xiang</surname><given-names>H.</given-names></name><name><surname>Shen</surname><given-names>J.</given-names></name><name><surname>Yan</surname><given-names>Q.</given-names></name><name><surname>Xu</surname><given-names>M.</given-names></name><name><surname>Shi</surname><given-names>X.</given-names></name><name><surname>Zhu</surname><given-names>X.</given-names></name></person-group><article-title>Multi-scale representation attention based deep multiple instance learning for gigapixel whole slide image analysis</article-title><source>Med Image Anal</source><volume>89</volume><year>2023</year><object-id pub-id-type="publisher-id">102890</object-id><pub-id pub-id-type="doi">10.1016/j.media.2023.102890</pub-id></element-citation></ref><ref id="bib232"><label>232</label><element-citation publication-type="journal" id="sbref177"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Gong</surname><given-names>T.</given-names></name><name><surname>Fu</surname></name></person-group><article-title>E2-MIL: An explainable and evidential multiple instance learning framework for whole slide image classification</article-title><source>Medical Image Analysis</source><volume>97</volume><year>2024</year><fpage>103294</fpage><pub-id pub-id-type="pmid">39128377</pub-id>
</element-citation></ref><ref id="bib233"><label>233</label><element-citation publication-type="journal" id="sbref178"><person-group person-group-type="author"><name><surname>Diao</surname><given-names>J.A.</given-names></name><name><surname>Wang</surname><given-names>J.K.</given-names></name><name><surname>Chui</surname><given-names>W.F.</given-names></name><name><surname>Mountain</surname><given-names>V.</given-names></name><name><surname>Gullapally</surname><given-names>S.C.</given-names></name><name><surname>Srinivasan</surname><given-names>R.</given-names></name><etal/></person-group><article-title>Human-interpretable image features derived from densely mapped cancer pathology slides predict diverse molecular phenotypes</article-title><source>Nat Commun</source><volume>12</volume><year>2021</year><fpage>1613</fpage><pub-id pub-id-type="doi">10.1038/s41467-021-21896-9</pub-id><pub-id pub-id-type="pmid">33712588</pub-id>
</element-citation></ref><ref id="bib234"><label>234</label><element-citation publication-type="journal" id="sbref179"><person-group person-group-type="author"><name><surname>Amgad</surname><given-names>M.</given-names></name><name><surname>Hodge</surname><given-names>J.M.</given-names></name><name><surname>Elsebaie</surname><given-names>M.A.T.</given-names></name><name><surname>Bodelon</surname><given-names>C.</given-names></name><name><surname>Puvanesarajah</surname><given-names>S.</given-names></name><name><surname>Gutman</surname><given-names>D.A.</given-names></name><etal/></person-group><article-title>A population-level digital histologic biomarker for enhanced prognosis of invasive breast cancer</article-title><source>Nat Med</source><volume>30</volume><year>2024</year><fpage>85</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1038/s41591-023-02643-7</pub-id><pub-id pub-id-type="pmid">38012314</pub-id>
</element-citation></ref><ref id="bib235"><label>235</label><element-citation publication-type="journal" id="sbref180"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>S.</given-names></name><name><surname>Vu</surname><given-names>Q.D.</given-names></name><name><surname>Raza</surname><given-names>S.E.A.</given-names></name><name><surname>Azam</surname><given-names>A.</given-names></name><name><surname>Tsang</surname><given-names>Y.W.</given-names></name><name><surname>Kwak</surname><given-names>J.T.</given-names></name><etal/></person-group><article-title>Hover-Net: simultaneous segmentation and classification of nuclei in multi-tissue histology images</article-title><source>Med Image Anal</source><volume>58</volume><year>2019</year><object-id pub-id-type="publisher-id">101563</object-id><pub-id pub-id-type="doi">10.1016/j.media.2019.101563</pub-id></element-citation></ref><ref id="bib236"><label>236</label><mixed-citation publication-type="other" id="othref0280">H&#x000f6;rst F., Rempe M., Heine L., Seibold C., Keyl J., Baldini G., et al. CellViT: Vision Transformers for Precise Cell Segmentation and Classification 2023. <pub-id pub-id-type="doi">10.48550/arXiv.2306.15350</pub-id>.</mixed-citation></ref><ref id="bib237"><label>237</label><element-citation publication-type="journal" id="sbref181"><person-group person-group-type="author"><name><surname>Hinata</surname><given-names>M.</given-names></name><name><surname>Ushiku</surname><given-names>T.</given-names></name></person-group><article-title>Detecting immunotherapy-sensitive subtype in gastric cancer using histologic image-based deep learning</article-title><source>Sci Rep</source><volume>11</volume><year>2021</year><object-id pub-id-type="publisher-id">22636</object-id><pub-id pub-id-type="doi">10.1038/s41598-021-02168-4</pub-id></element-citation></ref><ref id="bib238"><label>238</label><element-citation publication-type="journal" id="sbref182"><person-group person-group-type="author"><name><surname>Adachi</surname><given-names>M.</given-names></name><name><surname>Taki</surname><given-names>T.</given-names></name><name><surname>Kojima</surname><given-names>M.</given-names></name><name><surname>Sakamoto</surname><given-names>N.</given-names></name><name><surname>Matsuura</surname><given-names>K.</given-names></name><name><surname>Hayashi</surname><given-names>R.</given-names></name><etal/></person-group><article-title>Predicting lymph node recurrence in cT1-2N0 tongue squamous cell carcinoma: collaboration between artificial intelligence and pathologists</article-title><source>J Pathol Clin Res</source><volume>10</volume><year>2024</year><object-id pub-id-type="publisher-id">e12392</object-id><pub-id pub-id-type="doi">10.1002/2056-4538.12392</pub-id></element-citation></ref><ref id="bib239"><label>239</label><element-citation publication-type="journal" id="sbref183"><person-group person-group-type="author"><name><surname>DeGrave</surname><given-names>A.J.</given-names></name><name><surname>Cai</surname><given-names>Z.R.</given-names></name><name><surname>Janizek</surname><given-names>J.D.</given-names></name><name><surname>Daneshjou</surname><given-names>R.</given-names></name><name><surname>Lee</surname><given-names>S.-I.</given-names></name></person-group><article-title>Auditing the inference processes of medical-image classifiers by leveraging generative AI and the expertise of physicians</article-title><source>Nat Biomed Eng</source><year>2023</year><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41551-023-01160-9</pub-id><pub-id pub-id-type="pmid">36683043</pub-id>
</element-citation></ref><ref id="bib240"><label>240</label><mixed-citation publication-type="other" id="othref0285">&#x0017d;igutyt&#x00117; L., Lenz T., Han T., Hewitt K.J., Reitsam N.G., Foersch S., et al. Counterfactual Diffusion Models for Mechanistic Explainability of Artificial Intelligence Models in Pathology 2024:2024.10.29.620913. <pub-id pub-id-type="doi">10.1101/2024.10.29.620913</pub-id>.</mixed-citation></ref><ref id="bib241"><label>241</label><mixed-citation publication-type="other" id="othref0290">Kaczmarzyk J.R., Saltz J.H., Koo P.K. Explainable AI for computational pathology identifies model limitations and tissue biomarkers 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2409.03080</pub-id>.</mixed-citation></ref><ref id="bib242"><label>242</label><mixed-citation publication-type="other" id="othref0295">Sun Y., Zhu C., Zheng S., Zhang K., Sun L., Shui Z., et al. PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2305.15072</pub-id>.</mixed-citation></ref><ref id="bib243"><label>243</label><mixed-citation publication-type="other" id="othref0300">Saab K., Tu T., Weng W.-H., Tanno R., Stutz D., Wulczyn E., et al. Capabilities of Gemini Models in Medicine 2024. <pub-id pub-id-type="doi">10.48550/arXiv.2404.18416</pub-id>.</mixed-citation></ref><ref id="bib244"><label>244</label><mixed-citation publication-type="other" id="othref0305">Modella A.I. | Judith n.d. <ext-link ext-link-type="uri" xlink:href="https://www.modella.ai/judith.html" id="ir0235">&#x02329;https://www.modella.ai/judith.html&#x0232a;</ext-link> (accessed December 5, 2024).</mixed-citation></ref></ref-list><ack id="ack0005"><title>Acknowledgment</title><p id="p0430">We thank Enago (<ext-link ext-link-type="uri" xlink:href="http://www.enago.jp" id="ir0025">www.enago.jp</ext-link>) for the English language review.</p></ack></back></article>