<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11679976</article-id><article-id pub-id-type="doi">10.3390/s24247951</article-id><article-id pub-id-type="publisher-id">sensors-24-07951</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Humanity Test&#x02014;EEG Data Mediated Artificial Intelligence Multi-Person Interactive System</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Fang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5556-0422</contrib-id><name><surname>Gao</surname><given-names>Tanhao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Jie</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="c1-sensors-24-07951" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Brunye</surname><given-names>Tad</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Tao</surname><given-names>Da</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Wang</surname><given-names>Hailiang</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Tingru</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-24-07951">College of Design and Innovation, Tongji University, Shanghai 200092, China; <email>fangfangtongji@foxmail.com</email> (F.F.); <email>tanhaogao@gmail.com</email> (T.G.)</aff><author-notes><corresp id="c1-sensors-24-07951"><label>*</label>Correspondence: <email>wujie@tongji.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2024</year></pub-date><volume>24</volume><issue>24</issue><elocation-id>7951</elocation-id><history><date date-type="received"><day>16</day><month>7</month><year>2024</year></date><date date-type="rev-recd"><day>25</day><month>10</month><year>2024</year></date><date date-type="accepted"><day>31</day><month>10</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 by the authors.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Artificial intelligence (AI) systems are widely applied in various industries and everyday life, particularly in fields such as virtual assistants, healthcare, and education. However, this paper highlights that existing research has often overlooked the philosophical and media aspects. To address this, we developed an interactive system called &#x0201c;Human Nature Test&#x0201d;. In this context, &#x0201c;human nature&#x0201d; refers to emotion and consciousness, while &#x0201c;test&#x0201d; involves a critical analysis of AI technology and an exploration of the differences between humanity and technicality. Additionally, through experimental research and literature analysis, we found that the integration of electroencephalogram (EEG) data with AI systems is becoming a significant trend. The experiment involved 20 participants, with two conditions: C1 (using EEG data) and C2 (without EEG data). The results indicated a significant increase in immersion under the C1 condition, along with a more positive emotional experience. We summarized three design directions: enhancing immersion, creating emotional experiences, and expressing philosophical concepts. Based on these findings, there is potential for further developing EEG data as a medium to enrich interactive experiences, offering new insights into the fusion of technology and human emotion.</p></abstract><kwd-group><kwd>electroencephalogram (EEG) data</kwd><kwd>artificial intelligence</kwd><kwd>multi-person interaction</kwd><kwd>emotional experience</kwd><kwd>medium</kwd><kwd>installation</kwd><kwd>brain-computer interface (BCI)</kwd></kwd-group><funding-group><award-group><funding-source>Tongji Innovation Design and Intelligent Manufacturing Disciplines</funding-source></award-group><funding-statement>This research was funded by Tongji Innovation Design and Intelligent Manufacturing Disciplines.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-24-07951"><title>1. Introduction</title><p>The application of artificial intelligence (AI) systems has become increasingly widespread across diverse fields. These systems are employed not only in daily life for intelligent assistants, automated services, and recommendation systems but also play crucial roles in sectors like healthcare, education, and manufacturing [<xref rid="B1-sensors-24-07951" ref-type="bibr">1</xref>]. The research focus lies within the field of human-computer interaction, specifically on how to enable machines to effectively comprehend and react to human needs, emotions, and behaviors [<xref rid="B2-sensors-24-07951" ref-type="bibr">2</xref>], AI research utilizing physiological data represents an important branch involving physiological signals such as electroencephalograms (EEGs), heart rates, and skin conductance to enhance machine understanding of human emotions and intentions [<xref rid="B3-sensors-24-07951" ref-type="bibr">3</xref>]. Current research in this branch varies, with some studies concentrating on AI applications in wearable biosensors [<xref rid="B4-sensors-24-07951" ref-type="bibr">4</xref>], others on machine learning algorithms for physiological data analysis [<xref rid="B5-sensors-24-07951" ref-type="bibr">5</xref>] for emotion recognition, and additional investigations into AI techniques for managing extensive biometric data sets, processing outliers, and maintaining stability under diverse challenging conditions [<xref rid="B6-sensors-24-07951" ref-type="bibr">6</xref>,<xref rid="B7-sensors-24-07951" ref-type="bibr">7</xref>].</p><p>However, most of these studies focus on the medical and signal-processing fields, with few concentrating on how physiological signals could augment emotional communication and immersive experiences between AI systems and users [<xref rid="B8-sensors-24-07951" ref-type="bibr">8</xref>]. This contradicts the current society&#x02019;s need to understand the interaction between human emotions and machines [<xref rid="B9-sensors-24-07951" ref-type="bibr">9</xref>]. Therefore, the specific objective of this study is to explore how multi-user EEG data can enhance immersion, emotional interaction, and the conveyance of concepts in AI systems. We designed a prototype system to adjust and provide feedback on interactive experiences by capturing and analyzing participants&#x02019; brainwave activity in real time. This research utilizes experimental approaches and user feedback to assess the system&#x02019;s effectiveness. Through detailed analysis of participants&#x02019; data during interactions, methods to augment experiential and emotional engagements are discussed.</p><p>In summary, this study not only establishes a new foundation for improving AI systems&#x02019; experiential quality but also aims to build solid foundations for designing AI systems with enhanced emotional intelligence, contributing to the future development of the field of human-computer interaction.</p></sec><sec id="sec2-sensors-24-07951"><title>2. Theoretical Framework</title><p>For this study, literature was primarily retrieved through Google Scholar, the ACM Digital Library, IEEE Xplore Digital Library, and Springer Link Online Library. Research on integrating multi-user EEG data into AI systems lies at the intersection of human-computer interaction, physiological data sensors, and artificial intelligence. Consequently, most of the relevant publications are found in the fields of human-computer interaction and computer science, with papers mainly published in leading international conferences such as CHI, IEEE VIS, SIGGRAPH, and HCI, as well as journals like Leonardo, Sensors, Entertainment Computing, and Digital Creativity. We first identified a set of keywords and search strings closely related to the research topic, following a search path from &#x0201c;physiological data interaction design&#x0201d; to &#x0201c;multi-user physiological data interaction design&#x0201d;, &#x0201c;multi-user EEG data interaction design&#x0201d;, &#x0201c;AI interaction device design&#x0201d;, and finally &#x0201c;AI systems integrated with EEG data design&#x0201d;. The collected literature was then categorized into three intersecting fields and analyzed from the following three perspectives (see <xref rid="sensors-24-07951-f001" ref-type="fig">Figure 1</xref>): &#x0201c;Shift in Focus within AI Systems&#x0201d;, &#x0201c;Multi-Person Physiological Data Interaction&#x0201d;, and &#x0201c;Integration of EEG Data in AI Systems&#x0201d;.</p><p>Employing media theory [<xref rid="B10-sensors-24-07951" ref-type="bibr">10</xref>] as a framework, this study examines the shortfall in critically examining AI technology and neglecting immersive and emotional interactions in offline multi-person AI systems. This research emphasizes the potential of multi-user EEG data to improve AI systems&#x02019; experiential quality.</p><sec id="sec2dot1-sensors-24-07951"><title>2.1. Shift in Focus Within AI Systems</title><p>In recent years, advancements in machine learning and natural language processing have led to the integration of AI into diverse facets of human-computer interaction and design. This approach opened up new design possibilities, notably enhancing content diversity and unpredictability. Yet, AI&#x02019;s role is frequently reduced to creating varied visual effects [<xref rid="B11-sensors-24-07951" ref-type="bibr">11</xref>], enabling designers to perform more significant technical manipulation. Yet, there is little critical research on AI technology from a media and philosophical perspectives, nor exploration of thoughts on humanity and technicality [<xref rid="B12-sensors-24-07951" ref-type="bibr">12</xref>].</p><p>McLuhan&#x02019;s &#x0201c;the medium is the message&#x0201d; principle [<xref rid="B10-sensors-24-07951" ref-type="bibr">10</xref>] underscores the significant influence of medium form on societal and individual cognition. Rokeby argued for an exceptional understanding of technology, one that penetrates the philosophy behind it [<xref rid="B13-sensors-24-07951" ref-type="bibr">13</xref>]. Considering the philosophical, psychological, political, and even aesthetic aspects behind practical projects is crucial. As shown in <xref rid="sensors-24-07951-f002" ref-type="fig">Figure 2</xref>, the &#x0201c;Uncanny Valley: Being Human in the Age of AI&#x0201d; exhibition reimagines the human-AI relationship from a novel viewpoint [<xref rid="B14-sensors-24-07951" ref-type="bibr">14</xref>]. Through abstract interactive works, the exhibition showcases AI&#x02019;s cognitive biases and creative environments, along with the issues and impacts arising in contemporary life. The &#x0201c;Deep Feeling: AI and Emotions&#x0201d; exhibition explores emotions and new self-awareness through the gradual symbiosis between humans and AI [<xref rid="B15-sensors-24-07951" ref-type="bibr">15</xref>]. Antonio Daniele et al. leveraged the &#x0201c;Re: Humanism&#x0201d; movement as an artistic Turing test, prompting reflection on human essence and perceptions of artificiality by distinguishing between what is inherently human and what is fabricated [<xref rid="B16-sensors-24-07951" ref-type="bibr">16</xref>].</p><p>Recent studies have revealed that AI interaction systems&#x02019; outputs are predominantly interface-based, displaying information directly on screens for viewers. This primarily visual mode offers an instantaneous experience, involving viewers interacting with the screen in front of them through clicks (see <xref rid="sensors-24-07951-f003" ref-type="fig">Figure 3</xref>). However, this approach has diminished user experience, prompting researchers to explore more immersive output modes for AI systems. For instance, an AI-enabled device for writing Chinese characters set within a virtual bamboo forest merges calligraphy and painting, offering an immersive experience that blends reality with transcendence [<xref rid="B17-sensors-24-07951" ref-type="bibr">17</xref>]. Cacophonic Choir, an interactive art installation, employs a neural network trained on sexual assault narratives to craft an immersive auditory and visual experience, highlighting survivors&#x02019; stories [<xref rid="B18-sensors-24-07951" ref-type="bibr">18</xref>]. AI systems facilitate improvisational practice in theater settings, enabling actors to fully immerse in the moment [<xref rid="B19-sensors-24-07951" ref-type="bibr">19</xref>].</p><p>This paper focuses on transitioning from technological to philosophical perspectives within AI interactive systems, enhancing user experience by designing immersive output modes.</p></sec><sec id="sec2dot2-sensors-24-07951"><title>2.2. Multi-Person Physiological Data Interaction</title><p>Biological data, collected via wearable biosensors, records human physiological signals detected by biological materials. This data is then processed into a format suitable for experimental research [<xref rid="B6-sensors-24-07951" ref-type="bibr">6</xref>]. Marshall McLuhan distinguished &#x0201c;hot&#x0201d; from &#x0201c;cool&#x0201d; media: hot media are rich in information and require minimal audience engagement, whereas cool media offer less detail and demand greater participation and interpretation from the audience [<xref rid="B20-sensors-24-07951" ref-type="bibr">20</xref>]. Advancements in biosensor technology have transformed physiological data into a communicative bridge between humans and computers, introducing a novel medium that facilitates co-creation among designers and users.</p><p>EEG offers a non-invasive approach to monitoring brain electrical activity by placing electrodes on the scalp to record the brain&#x02019;s spontaneous electrical signals over time [<xref rid="B21-sensors-24-07951" ref-type="bibr">21</xref>]. EEG data serves as a physiological medium, with popular commercial devices such as Muse, Emotive, and NeuroSky facilitating its collection.</p><p>Multi-user EEG data refers to the real-time collection of brainwave data from multiple participants. As shown in <xref rid="sensors-24-07951-t001" ref-type="table">Table 1</xref>, this study summarizes multi-user EEG-based works and analyzes them according to interaction methods, types, and performance. The use of multi-user EEG data in interactive works enables more complex feedback mechanisms, enhancing playability. The interaction systems typically involve either cooperative or competitive modes, but regardless of the control method, BCI (brain-computer interface) interactions tend to generate positive user experiences. Social and emotional interaction levels are notably higher during BCI control [<xref rid="B22-sensors-24-07951" ref-type="bibr">22</xref>].</p><p>Social interaction environments, essential for multi-user collaboration systems, significantly influence individual experiences, including presence and immersion [<xref rid="B23-sensors-24-07951" ref-type="bibr">23</xref>]. Thus, combining individual user experiences with social interaction creates a communal experience. For example, David Rosenboom&#x02019;s &#x0201c;Alpha Checkers&#x0201d; allowed two players to interact in a visual environment by controlling the game board through alpha waves [<xref rid="B24-sensors-24-07951" ref-type="bibr">24</xref>]. Other works such as &#x0201c;New York Biofeedback Quartet&#x0201d; [<xref rid="B24-sensors-24-07951" ref-type="bibr">24</xref>], &#x0201c;Ecology of the Skin&#x0201d; [<xref rid="B24-sensors-24-07951" ref-type="bibr">24</xref>], MoodMixer [<xref rid="B25-sensors-24-07951" ref-type="bibr">25</xref>], Ringing Minds [<xref rid="B26-sensors-24-07951" ref-type="bibr">26</xref>], and &#x0201c;Portable Gold and Philosophers&#x02019; Stones&#x0201d; explored collective brainwave influence on music generation, highlighting cooperation between multiple individuals [<xref rid="B24-sensors-24-07951" ref-type="bibr">24</xref>]. Additionally, &#x0201c;Vancouver Piece&#x0201d; examined shared identity through enhanced lighting effects in front of a mirror, emphasizing the collaborative experience of multiple users [<xref rid="B24-sensors-24-07951" ref-type="bibr">24</xref>]. Jacqueline Humbert&#x02019;s &#x0201c;Alpha Garden&#x0201d; demonstrated how synchronized alpha waves from participants could enhance control over a watering system, showing how cooperation influences the experience [<xref rid="B27-sensors-24-07951" ref-type="bibr">27</xref>]. Similarly, &#x0201c;Brainwave Etch-A-Sketch&#x0201d; allowed participants to collaboratively draw by tracking their brainwaves [<xref rid="B27-sensors-24-07951" ref-type="bibr">27</xref>]. &#x0201c;Hopscotch&#x02014;a mobile opera for 24 cars&#x0201d; [<xref rid="B24-sensors-24-07951" ref-type="bibr">24</xref>] was a performance where brainwave signals from audience members were collected to generate a responsive operatic scene within a limousine. &#x0201c;Assembly Cognogenesis&#x0201d; [<xref rid="B28-sensors-24-07951" ref-type="bibr">28</xref>] created a virtual reality environment where two participants used neural and gestural interfaces to interact with an artificial life world, requiring them to cooperate to evolve entities within the virtual space. Lastly, Suzzane Dikker&#x02019;s &#x0201c;Mutual Wave Machine&#x0201d; used EEG activity and facial expression visualizations to display thought synchronization, exploring the neural and emotional connections between people and emphasizing the deeper levels of cooperation and communication among individuals [<xref rid="B29-sensors-24-07951" ref-type="bibr">29</xref>]. This form of interaction not only enhanced user experience but also fostered social cooperation and engagement among participants, showcasing the potential of BCI technology in promoting multi-person collaboration.</p><p>In addition to cooperative approaches, researchers have delved into competitive interaction systems within media. &#x0201c;Bacteria Hunt,&#x0201d; a multi-person EEG-based game, utilizes alpha waves and Steady State Visually Evoked Potentials (SSVEP) for point competition [<xref rid="B30-sensors-24-07951" ref-type="bibr">30</xref>]. This game explores interactive player control mechanisms in competitive settings by integrating relaxation and attention-related brainwave frequencies, underscoring the significance of collective participation. &#x0201c;Brainball&#x0201d; enables participants to navigate a steel ball through a race using physiological signals from EEG and Electromyography (EMG) [<xref rid="B30-sensors-24-07951" ref-type="bibr">30</xref>]. &#x0201c;NeuroBrush,&#x0201d; a web application, facilitates a competition in creating postmodern art through BCI inputs [<xref rid="B31-sensors-24-07951" ref-type="bibr">31</xref>]. The platform fosters competition and social cooperation by enabling real-time sharing of the art-creation process and dynamic background color adjustments, enhancing the creative experience.</p><p>Beyond multi-user EEG data, various biological data types also enable individual interactions. For instance, Lozano-Hemmer&#x02019;s &#x0201c;Zoom Pavilion&#x0201d; [<xref rid="B32-sensors-24-07951" ref-type="bibr">32</xref>] dynamically projects participants&#x02019; facial and movement data in real time, captured within the installation space [<xref rid="B33-sensors-24-07951" ref-type="bibr">33</xref>]. George Zisiadis&#x02019;s &#x0201c;Pulse of the City&#x0201d; [<xref rid="B34-sensors-24-07951" ref-type="bibr">34</xref>] captures real-time heart rate data when participants grip a heart-shaped object&#x02019;s handles, transforming it into musical beats via integrated speakers. Misha Sra and colleagues [<xref rid="B35-sensors-24-07951" ref-type="bibr">35</xref>] created interactive pieces using breath as a direct control mechanism, employing the Zephyr BioHarness sensor for four distinct control actions.</p><p>A growing body of research utilizes physiological data devices and methodologies to enhance participant interactions. Evidence indicates that such approaches heighten audience engagement and attention while broadening the system&#x02019;s sociability via intricate and structured feedback mechanisms. This project seeks to extend these foundational studies by investigating the integration of multi-user EEG data into artificial intelligence systems.</p></sec><sec id="sec2dot3-sensors-24-07951"><title>2.3. Integration of EEG Data in AI Systems</title><p>Digital media are integral to systems, and the ideology along with the way we interact with digital media works are often found in the way their structure operates [<xref rid="B36-sensors-24-07951" ref-type="bibr">36</xref>]. Integrating EEG data into AI systems requires deconstructing its output, input, and data-processing phases. Understanding these computational processes is essential to discerning how the system creates representations [<xref rid="B36-sensors-24-07951" ref-type="bibr">36</xref>]. However, as technology evolves, the distinctions between various media are becoming increasingly indistinct, leading to a convergence. Recently, researchers have focused more on technical aspects rather than critically examining how media integration into new systems could improve their experiential quality.</p><p>Physiological data enhances AI interaction systems in three key dimensions.</p><p>Initially, physiological data interfaces stimulate unique sensory and cognitive modes in AI system users, including vision, hearing, spatial awareness, and EEG inputs. As shown in <xref rid="sensors-24-07951-t002" ref-type="table">Table 2</xref>, this study lists various AI systems and the types of input data used.</p><p>Visual input is a key process in AI systems, where cameras capture environmental data, providing rich information about users&#x02019; appearance. In the &#x0201c;AI N&#x000fc;shu&#x0201d; project [<xref rid="B37-sensors-24-07951" ref-type="bibr">37</xref>], two AI agents observed the environment and recorded audience behavior, integrating it with traditional N&#x000fc;shu poetry to showcase AI&#x02019;s potential in language generation. Similarly, &#x0201c;Cangjie&#x02019;s Poetry&#x0201d; [<xref rid="B38-sensors-24-07951" ref-type="bibr">38</xref>] relied on visual input, while &#x0201c;Transferscope&#x0201d; [<xref rid="B39-sensors-24-07951" ref-type="bibr">39</xref>] allowed users to capture and transform images, seamlessly integrating them into new scenes. Auditory input, using text and speech conversion, enables AI to perceive user voices. In the Mayfly installation, voice-generated content explored linguistic taboos and censorship. A museum recommendation system [<xref rid="B40-sensors-24-07951" ref-type="bibr">40</xref>], powered by a fine-tuned GPT-4 model, provided personalized recommendations based on user input and contextual factors like location and visit time. The &#x0201c;Storyteller&#x0201d; prototype [<xref rid="B41-sensors-24-07951" ref-type="bibr">41</xref>], developed by The Met, Microsoft, and MIT, highlighted AI&#x02019;s role in cultural heritage preservation. Recent studies are expanding AI perception. In Attracting Museum Visitors Through AI-Generated Narration and Gameplay, Kinect sensors and spatial data enhanced user interaction for a more immersive experience. &#x0201c;New Nature&#x0201d; [<xref rid="B42-sensors-24-07951" ref-type="bibr">42</xref>], a digital bio-park by Marpi, used sensors to capture audience physiological data, which influenced immersive projections, creating surreal visual effects. In &#x0201c;Wu Xiang Zhi Xiang&#x0201d;, participants wore EEG devices that influenced the visuals on screen, reflecting brainwave activity [<xref rid="B43-sensors-24-07951" ref-type="bibr">43</xref>].</p><p>Secondly, physiological data as a medium introduces abstract things (such as emotions) into the system, providing communication opportunities for multiple participants. The audience&#x02019;s inner experience (emotions, thoughts) becomes externalized through the medium, thereby affecting social interaction. For instance, Mariko Mori employs EEG devices to record audience brainwaves across Delta, Theta, Alpha, Beta, and Gamma bands. These are then algorithmically translated into emotional values, enabling viewers to recognize their and others&#x02019; emotional visual patterns. In &#x0201c;Cacophonic Choir&#x0201d;, clear story comprehension requires visitors to lean in closely to an AI agent. Such physical proximity is designed to enhance empathy among participants [<xref rid="B18-sensors-24-07951" ref-type="bibr">18</xref>].</p><p>Ultimately, physiological data enhance conceptual articulation within AI systems, fostering ideological reflections and dialogues among users. Artistic concept expression enriches the work, accentuating its theme via metaphors and multifaceted narratives. In Meshi A&#x02019;s installation, participants engage with a machine learning classifier that analyzes their physical traits and movements in real time, juxtaposing these with prerecorded animated characters [<xref rid="B44-sensors-24-07951" ref-type="bibr">44</xref>]. Beyond technical exhibition, this interaction prompts audience reflection on collective memory and identity formation. Rafael Lozano-Hemmer&#x02019;s &#x0201c;Nivel de confidence&#x0201d; addresses Mexico&#x02019;s issues of violence and missing persons via digital technology and facial recognition mechanisms [<xref rid="B32-sensors-24-07951" ref-type="bibr">32</xref>]. The installation bolsters collective awareness of national memory lapses utilizing biometric monitoring algorithms.</p><p>Present studies on incorporating physiological data into AI multi-person systems reveal a deficiency in systematic approaches and methodologies, hindering the formation of profound connections from a media standpoint. This challenge primarily arises from technological instability, the analytical complexity of data, and the intricacies of merging diverse media forms. In this context, our study aims to investigate the integration of EEG data into AI interaction systems&#x02014;covering data input, processing, and output&#x02014;to improve user experience.</p></sec></sec><sec id="sec3-sensors-24-07951"><title>3. Prototype</title><p>This research employs an interactive installation to explore the enhancement of AI system experiences through multi-person EEG data across various interaction modalities. This methodology of this project is practice-oriented, focusing on design of the system prototype and engaging with contemporary practical applications and scholarly discussions within the field. This study adopts an interdisciplinary approach, integrating theories from computer science, biology, communications, and psychology.</p><p>As shown in the vision map below, the prototype consists of four parts: 3.1 A brief introduction; 3.2 Story Background; 3.3 Game Mechanism; 3.4 Production Technology. See <xref rid="sensors-24-07951-f004" ref-type="fig">Figure 4</xref>.</p><sec id="sec3dot1-sensors-24-07951"><title>3.1. Prototype: Humanity Test (Voight-Kampff 2.0)</title><p>The &#x0201c;Humanity Test&#x0201d; embodies a sci-fi-themed interactive installation. This project integrates multiple media forms to deliver gamified narrative experiences, which take approximately 20 min, necessitating cooperation between two participants, where engaging enhances experiences from each other.</p><p>Participants engage according to predefined rules, with their real-time EEG data influencing the system&#x02019;s overall ambiance. Initially displayed at the Beihang University Art Exhibition Hall, the installation was later featured at the SIGGRAPH Asia Art Gallery [<xref rid="B45-sensors-24-07951" ref-type="bibr">45</xref>] and an art museum in Chengdu, Sichuan, attracting a diverse audience (see <xref rid="sensors-24-07951-f005" ref-type="fig">Figure 5</xref>).</p><p>The installation&#x02019;s thematic inspiration originates from the science fiction novel &#x0201c;Avalanche&#x0201d;, which envisions a scenario where computer viruses can affect the human cerebral cortex, turning individuals into mechanized entities capable only of executing programs input by others. As described by the author Wiener, it&#x02019;s a human nightmare where individuals are confined within machinery bounds, without autonomous consciousness, being used to serve machines. Consequently, this project provokes ongoing reflection on questions: Can humans become machines? Where is the boundary between humans and machines?</p></sec><sec id="sec3dot2-sensors-24-07951"><title>3.2. Story Background</title><p>The installation is inspired by the Voight-Kampff machine from the <italic toggle="yes">Blade Runner</italic> novel. This work tells a science fiction story set in a futuristic central city. Amy, a psychologist working for an AI company, specializes in distinguishing between humans and AI. In this future, AI is designed to closely resemble humans but remains flawed in emotional processing. Amy&#x02019;s latest task is to use the Voight-Kampff 2.0 machine&#x02019;s emotion-recognition technology to determine whether her counterpart is human or AI. Although AI has made significant progress in simulating emotions, does it truly possess self-awareness? In this piece, two participants take on the roles of Amy and the test subject, as shown in <xref rid="sensors-24-07951-f005" ref-type="fig">Figure 5</xref>. Using the installation, they perform their respective tasks, engaging in a gamified narrative experience. In the following text, we will refer to Amy as the questioner and the test subject as the respondent.</p></sec><sec id="sec3dot3-sensors-24-07951"><title>3.3. Game Mechanics</title><p>The experimental setup entails a collaborative interaction between two participants who wear EEG devices, headphones, and cameras. They cannot communicate nor see each other directly. <xref rid="sensors-24-07951-f006" ref-type="fig">Figure 6</xref> shows the startup interface, which includes the title of the work and operational instructions. When the questioner follows the instructions, puts on the device, and their focus level reaches the threshold, the system will automatically start.</p><p><xref rid="sensors-24-07951-f007" ref-type="fig">Figure 7</xref> shows the interface viewed by the questioner, which is composed of four sections: the respondent&#x02019;s real-time facial expressions, the visualization of EEG data, the &#x0201c;Human Nature Test&#x0201d; questions, and an EEG legend with operational guidance. The EEG visualization includes the user&#x02019;s alpha, beta, and theta waves, represented by a blue sphere, a purple hexagon, and a yellow triangle, respectively. As these wave frequencies increase, the corresponding shapes enlarge.</p><p>As shown in <xref rid="sensors-24-07951-f008" ref-type="fig">Figure 8</xref>, the respondent&#x02019;s interface consists of two parts: the &#x0201c;Human Nature Test&#x0201d; questions and the visualization of the respondent&#x02019;s EEG data.</p><p>When the questioner clicks on a &#x0201c;Human Nature Test&#x0201d; question, the respondent hears and sees sound and visuals designed to provoke emotional reactions (as shown in <xref rid="sensors-24-07951-f009" ref-type="fig">Figure 9</xref>). The designed trigger questions include three positive and three negative prompts. The questioner then observes the EEG viewer on their interface to determine the respondent&#x02019;s identity: Is it a human or a virtual being? The basis for judgment includes noticeable feedback in the EEG signals for real humans, whereas virtual beings, despite mimicking human facial expressions and voice responses (detailed in the fourth module of the system in <xref rid="sec3dot4-sensors-24-07951" ref-type="sec">Section 3.4</xref>), show no EEG fluctuations. Finally, the questioner makes their selection through the interface shown in <xref rid="sensors-24-07951-f009" ref-type="fig">Figure 9</xref>.</p></sec><sec id="sec3dot4-sensors-24-07951"><title>3.4. Production Technique</title><p>As shown in <xref rid="sensors-24-07951-f010" ref-type="fig">Figure 10</xref>, our project integrates software and hardware design through an interactive system to provide a comprehensive user experience environment. The hardware setup comprises a display screen, headphones, EEG devices (Muse), cameras, a mouse, a computer, and a projector.</p><p>The rendered image in the lower half of <xref rid="sensors-24-07951-f010" ref-type="fig">Figure 10</xref> shows how acrylic panels are used to divide the 3 m &#x000d7; 5 m space into two smaller areas, allowing the questioner and respondent to experience the installation separately. A 3 m &#x000d7; 3 m transparent acrylic board at the rear, blue and green fluorescent strips decorating the space, a 3 m &#x000d7; 5 m mirror floor covering, two black podiums, and two chairs each positioned on one side of the space, with a speaker underneath the podium.</p><p>The upper half of <xref rid="sensors-24-07951-f010" ref-type="fig">Figure 10</xref> illustrates that the system is composed of four modules:<list list-type="order"><list-item><p>EEG data transmission and processing;</p></list-item><list-item><p>Facial data transmission and processing;</p></list-item><list-item><p>Mouse-based interactive question selector;</p></list-item><list-item><p>Virtual character&#x02019;s facial and voice feedback.</p></list-item></list></p><p>Firstly, <xref rid="sensors-24-07951-f011" ref-type="fig">Figure 11</xref> illustrates how EEG data is processed using the Muse Headband, which captures brain activity and communicates it wirelessly via Bluetooth to a smartphone app like the Muse App (version 3.8.2) and Mind Monitor (version 2.1.1). These apps process the EEG signals, providing real-time feedback on brain activity. The data is then streamed using OSC to Touch Designer (version 2022.31000), a desktop application that further processes the signals into dynamic visualizations with the help of custom Python scripts (version 3.9.7). The visualized data is projected through a projector, allowing for real-time display of brainwave activity.</p><p>The second module captures facial data via a camera. The third module enables the playback of emotion-triggering environmental sounds and visuals through mouse clicks, also implemented using Python (version 3.9.7). In the fourth module, when the questioner selects a question, it is sent via an API to a large language model (ChatGPT). The ChatGPT API, provided by OpenAI (version 4.0), allows developers to integrate large language models into local applications. The large language model returns relevant text, which is processed by the local program and converted into speech using text-to-speech technology to generate the virtual character&#x02019;s response.</p></sec></sec><sec id="sec4-sensors-24-07951"><title>4. Experiment</title><p>This study identifies the device version (specifically, game mechanics) as the independent variable, with user experience constituting the dependent variable. Regarding questionnaire responses, individual participants represent the unit of analysis; in contrast, pairs of participants are analyzed as a single unit in interviews.</p><p>The aim of this experiment is to investigate whether EEG signals can influence interaction with an AI system and enhance the user experience. In the experimental group (C1), participants interacted with the AI system through EEG devices, allowing the AI to receive and utilize EEG signals to adjust the interaction or experience. In the control group (C2), participants interacted with the AI system without EEG input, with the system responding solely through traditional inputs, such as a mouse.</p><p>To minimize the influence of external factors (such as visual, auditory, environmental settings, and individual differences) on participants&#x02019; immersion and emotional experience, we applied the control variable method. This involved standardizing experimental conditions, such as controlling lighting, sound levels, and temperature. For example, we used the same screen and sound intensity and conducted the experiment at the same time each day to reduce environmental variability. Additionally, we implemented random assignment to distribute participants across different experimental conditions, ensuring that variables like gender, age, and experience were evenly balanced, thereby minimizing their potential impact on the study&#x02019;s outcomes.</p><sec id="sec4dot1-sensors-24-07951"><title>4.1. Experimental Procedure</title><p>Before the experiment, participants received training using the Muse App (version 3.8.2) included with the EEG device to familiarize themselves with its controls. EEG signals were recalibrated before each session to ensure accuracy. Subsequently, participants were equipped with physiological sensors and given brief instructions on how they work. They were suggested to engage in meditation or focus on a specific thought to enhance concentration. We also provide a concise introduction to the device&#x02019;s background story. Throughout the experiment, video recordings and game-completion times were documented as supplementary metrics to evaluate their experiences.</p><p>Following the experiment, participants were asked to complete the Game Immersion Questionnaire (GIQ) and the Self-Assessment Manikin (SAM) to gauge subjective immersive experiences. The Game Immersion Questionnaire (GIQ) is designed to measure immersion in game-based virtual worlds [<xref rid="B46-sensors-24-07951" ref-type="bibr">46</xref>]. Its primary aim is to assess the level of immersion experienced by users in games or similar interactive systems across three dimensions: engagement, engrossment, and total immersion. Each dimension is measured using a 5-point Likert scale to gauge users&#x02019; experiences, as shown in <xref rid="sensors-24-07951-f012" ref-type="fig">Figure 12</xref>.</p><p>The Self-Assessment Manikin (SAM) is a questionnaire used to assess emotional responses, measuring individuals&#x02019; emotional states during specific interactions or experiences. It evaluates emotional reactions based on three core dimensions: valence/pleasure (ranging from positive to negative), perceived arousal (ranging from high to low), and dominance/control (ranging from low to high) [<xref rid="B47-sensors-24-07951" ref-type="bibr">47</xref>]. SAM uses images to convey the scale, and due to its non-verbal design (see <xref rid="sensors-24-07951-f013" ref-type="fig">Figure 13</xref>), the questionnaire is accessible to individuals regardless of age, language proficiency, or educational background.</p><p>Subsequent to administering the questionnaire, we conduct semi-structured interviews with participants to delve deeper into their experiences with the various mechanisms while upholding their privacy.</p></sec><sec sec-type="subjects" id="sec4dot2-sensors-24-07951"><title>4.2. Participants</title><p>To ensure diversity and a broad participant base, we employed multiple recruitment methods to find suitable participants. Recruitment channels included posting information on social media and referrals from previous participants. The participants were aged between 20 and 40, with 10 females and 10 males.</p><p>To maintain experimental control and validity, participants were divided into two groups: five pairs tested the first version (C1), and five pairs tested the second version (C2) as a controlled experiment. All participants had normal or corrected vision and used computers daily, possessing a certain level of computer proficiency. While most participants had no prior experience using EEG devices, three participants indicated they had previously worn EEG data measurement devices.</p><p>To encourage participation, all participants voluntarily took part in the study and signed informed consent forms. As compensation for their involvement, we also promised a monetary reward to express our gratitude.</p></sec><sec id="sec4dot3-sensors-24-07951"><title>4.3. Ethics and Procedure Documentation</title><p>In this study, we strictly adhered to ethical standards to ensure that participants&#x02019; rights and privacy were fully protected. All participants voluntarily signed informed consent forms prior to the experiment, fully understanding the study&#x02019;s purpose, procedures, potential risks, and data usage. To protect privacy, all collected EEG data and feedback questionnaires were anonymized, with personal identity information stored separately from the data. The data was used solely for research purposes and stored on encrypted servers, accessible only to authorized researchers. Due to the limited sample size in our experiment, the EEG data collected was insufficient to support comprehensive analysis. Therefore, we opted to prioritize questionnaires and interviews as our primary research methods. Upon the completion of the study, the EEG data was securely destroyed in accordance with data protection protocols, ensuring that it will no longer be used or disclosed. Throughout the study, we ensured participants&#x02019; comfort and safety, and the experiment could be paused at any time if discomfort arose. While participants had the option to withdraw at any point, no one chose to do so during the process.</p></sec></sec><sec sec-type="results" id="sec5-sensors-24-07951"><title>5. Results</title><p>We employed a specialized statistical method (non-parametric techniques) to analyze the participants&#x02019; rating data, while also organizing the interview responses into three main themes. First, we presented the results related to immersion and emotional experience scores, followed by a discussion of participants&#x02019; understanding and feedback on the work.</p><sec id="sec5dot1-sensors-24-07951"><title>5.1. Spatial Immersion</title><p><xref rid="sensors-24-07951-f014" ref-type="fig">Figure 14</xref> displays the average scores of the Game Immersion Questionnaire (GIQ) under two different conditions (C1 and C2). These scores were obtained from participants&#x02019; responses to the questionnaire. By comparing the average scores under the two conditions, it is clear that their immersion scores were higher when using EEG data.</p><p>The Wilcoxon signed-rank test result shows Z = 2.1, indicating a significant difference between the conditions with and without EEG data. A <italic toggle="yes">p</italic>-value of less than 0.05 (<italic toggle="yes">p</italic> &#x0003c; 0.05) demonstrates statistical significance, meaning that this difference in immersion scores between using and not using EEG data is very likely not due to chance, but rather caused by the condition itself (whether EEG data was used). See <xref rid="sensors-24-07951-f014" ref-type="fig">Figure 14</xref>.</p><p>Post-experiment, 15 participants highlighted EEG data visualization and sonification as pivotal in amplifying immersion levels. &#x0201c;The sensor gave me a greater sense of participation in the game&#x0201d; (P2, Female). Eight participants who enjoyed the EEG control aspect commented that using their mind to trigger actions made the game more novel and engaging, enhancing its playability. This sense of participation and enjoyment seemed particularly strong when the screen effects were &#x0201c;controlled by my thoughts, not by a mouse&#x0201d; (P19, Male).</p><p>Additionally, seven participants noted that the integration of spatial design with EEG data, such as weak control of ambient lighting through brainwaves, effectively enhanced their immersive experience to some extent. &#x0201c;The lighting arrangement reminded me of <italic toggle="yes">Blade Runner</italic>; I felt like I had entered a scene in the game, becoming a game character&#x0201d;, (P4, Female). However, four participants mentioned that their sense of immersion was slightly diminished at times due to connectivity issues with the equipment.</p></sec><sec id="sec5dot2-sensors-24-07951"><title>5.2. Emotional Experience</title><p><xref rid="sensors-24-07951-t003" ref-type="table">Table 3</xref> below presents the mean Self-Assessment Manikin (SAM) scale scores for both mechanisms.</p><p>This table shows the participants&#x02019; scores on the three emotional dimensions&#x02014;valence, arousal, and dominance&#x02014;under the C1 and C2 conditions. The valence (3.8) and arousal (4) scores in the C1 condition were significantly higher than those in the C2 condition (valence 1.9, arousal 1.9), indicating that participants&#x02019; emotional experiences were more positive and intense in the C1 condition, while the C2 condition elicited more negative and calmer emotional responses. However, the dominance score was higher in the C2 condition (3.8) compared to the C1 condition (2.4), suggesting that participants felt a greater sense of control in the C2 condition, despite the less intense emotional experience. Overall, the C1 condition provided a richer emotional experience, but with less perceived control, whereas in the C2 condition, participants had a stronger sense of control, though their emotional experience was less engaging.</p><p>To gain deeper insight into participants&#x02019; emotional perceptions under EEG control, we inquired whether utilizing their consciousness to manipulate the game enhanced their awareness of their own and their collaborator&#x02019;s emotional states.</p><disp-quote><p>&#x0201c;Beyond creativity, what truly moved me was how the interaction takes you deeper into yourself. Seeing your EEG waves creates strong emotions and enhanced teamwork with your partner.&#x0201d;</p><attrib>(P6, Female)</attrib></disp-quote><disp-quote><p>&#x0201c;This game made me reflect on my collaborative relationship with others.&#x0201d;</p><attrib>(P17, Male)</attrib></disp-quote><p>Some participants experienced &#x0201c;strong emotional resonance&#x0201d; (P10, Female), especially in the segment where they had to interact with their partner.</p><p>Twelve participants acknowledged the efficacy of EEG data in reflecting emotional states, especially through the mapping of colors and shape transformations. Concurrently, two respondents expressed a desire to &#x0201c;see a wider variety of emotional states captured and displayed by EEG data&#x0201d; (P10, Female). Another participant added: &#x0201c;Emotional visual feedback is sometimes not clear and has a delay.&#x0201c; (P9, Male).</p></sec><sec id="sec5dot3-sensors-24-07951"><title>5.3. Conceptual Communication</title><p>In the interview phase, inquiries regarding the efficacy of thematic communication were posed (see <xref rid="sensors-24-07951-f015" ref-type="fig">Figure 15</xref>). A considerable number of participants affirmed that the project adeptly communicated its themes concerning humanity and technology. Many respondents were particularly captivated by the exploration of &#x0201c;self-awareness&#x0201d; and &#x0201c;emotion&#x0201d; themes within the experience.</p><disp-quote><p>&#x0201c;The interaction between Amy and Lia showcased the limitations of artificial intelligence in understanding human emotions.&#x0201d;</p><attrib>(P11, Male)</attrib></disp-quote><disp-quote><p>&#x0201c;I feel that the UI design complements the project&#x02019;s theme, enhancing the overall sense of the experience.&#x0201d;</p><attrib>(P5, Female)</attrib></disp-quote><disp-quote><p>&#x0201c;It made me rethink the role of artificial intelligence in our lives.&#x0201d;</p><attrib>(P14, Male)</attrib></disp-quote><disp-quote><p>&#x0201c;This project is an enlightening work, especially in an era where everyone extols technology.&#x0201d;</p><attrib>(P20, Female)</attrib></disp-quote><disp-quote><p>&#x0201c;This work is a reflection on the sustainable development of future AI technology.&#x0201d;</p><attrib>(P19, Male)</attrib></disp-quote><p>In conclusion, informal interviews predominantly affirmed the project&#x02019;s efficacy in articulating complex concepts via immersive experiences, resonating with users both emotionally and cognitively.</p></sec></sec><sec sec-type="discussion" id="sec6-sensors-24-07951"><title>6. Discussion</title><p>Building on the results from the previous chapter, we further explored these key findings and discussed the optimal integration of physiological sensors into traditional control systems. As summarized in <xref rid="sensors-24-07951-t004" ref-type="table">Table 4</xref>, we identified three design directions for AI-driven multi-person interaction systems using EEG data: enhancing immersion, creating emotional experiences, and facilitating conceptual expression. Additionally, we discussed the limitations of our work and proposed opportunities for future research.</p><sec id="sec6dot1-sensors-24-07951"><title>6.1. Creating Spatial Immersion</title><p>Many participants mentioned that they felt a higher degree of involvement when using EEG interaction, attributing it to the perceptual origins of spatial immersion. This phenomenon is because spatial immersion primarily originates from the perceptual level, and visualizing and sonifying biometric data are common methods for perceptual-level design based on experience.</p><p>Design considerations encompass three main aspects: input data, data processing, and output content, focusing on the mapping between EEG data and immersive audio-visual effects. Mapping EEG data for immersive audio-visual effects involves selecting frequencies and bands aligned with artistic objectives. For example, in this study, the EEG bands selected for analysis were primarily alpha, beta, and theta. Alpha waves (8&#x02013;12 Hz) correspond to relaxation and calmness; beta waves (13&#x02013;30 Hz) to alertness and stress; and theta waves (4&#x02013;7 Hz) to deep relaxation and concentration [<xref rid="B48-sensors-24-07951" ref-type="bibr">48</xref>] (see <xref rid="sensors-24-07951-f016" ref-type="fig">Figure 16</xref>).</p><p>Following data collection, biometric information from biological sensors undergoes processing to align with creative specifications, utilizing ChatGPT (version 4.0). For example, in this research, pre-labeled EEG data served as the training data set, facilitating the generation of specific sequences. A machine learning-based emotional model was developed through the iterative accumulation and categorization of data. Subsequently, the model&#x02019;s accuracy underwent validation through cross-validation techniques.</p><p>The design of audio-visual effects relies on thematic alignment and an understanding of the data&#x02019;s inherent meanings and characteristics. For instance, alpha, beta, and theta brainwave frequencies within specific emotional model ranges trigger corresponding changes in emotional graphics. The three fundamental symbols act as linguistic signs for abstract concepts, reflecting Saussurean semiotics principles. For instance, the triangle inherently conveys an image of stability, and the circle conveys an image of calm, soothingness, etc. Post-mapping and dynamic input data lead to continual modifications in the work&#x02019;s presentation, including screen transitions and variations in graphics and sound, such as screen transitions, changes in graphic size, color changes, changes in quantity, changes in sound pitch, etc. Therefore, the transition between different states needs to be smooth and sensitive to ensure well-timed feedback and a good experience for the audience. Our research indicates that introducing a deliberate delay in data processing can result in a more gradual input data transition, which is particularly beneficial for abstract visual effects like singular shapes. In this project, we used Python&#x02019;s &#x0201c;time.sleep()&#x0201d; (version 3.9.7) function to impose a predetermined delay, effectively moderating the pace of the final three graphical transitions.</p><p>Furthermore, there are intricate connections between auditory feedback design and narrative structure. For instance, audience-selected questions trigger specific questions and ambient sound responses. Environmental sounds from the International Affective Digitized Sounds (IADS) library are employed for humanity-related inquiries, activating upon question selection to evoke experiencers&#x02019; emotional responses.</p><p>In the integration of EEG data with spatial immersion experiences, not only visual and auditory changes are involved, but there is also potential to introduce haptic feedback. For example, as brainwave patterns change, haptic feedback devices (such as vibration or airflow) can synchronize with these changes, enhancing participants&#x02019; perception of the virtual world. In future designs, multi-sensory integration will be crucial for enhancing spatial immersion experiences. By combining tactile sensations, temperature variations, or other physical feedback, virtual experiences can be elevated to a more immersive and realistic level.</p><p>Additionally, in the realm of biological data processing, current EEG analysis primarily focuses on specific frequency ranges (e.g., alpha, beta, theta waves). Future research could explore more brainwave features, especially when combined with other physiological data such as heart rate or skin conductance, to enhance the system&#x02019;s understanding of users&#x02019; emotions and states. Through multi-modal biological feedback, designers can create more refined and dynamic immersive experiences based on participants&#x02019; psychological and physiological conditions.</p></sec><sec id="sec6dot2-sensors-24-07951"><title>6.2. Creating Emotional Experiences</title><p>Incorporating participant feedback proved that artworks fostering immersive emotional experiences are more memorable. Our findings suggest that AI systems enhanced with EEG data not only facilitate interaction but also promote empathy, capturing significant attention and leaving a lasting impact on individuals.</p><p>This project specifically explores the synergy of shared emotional experiences and social enjoyment among participants. By melding individual immersive experiences with social interactions, a collective experience emerges, enriching each participant&#x02019;s journey (see <xref rid="sensors-24-07951-f017" ref-type="fig">Figure 17</xref>). The design fosters a collaborative experiential mode, where participants&#x02019; concurrent EEG data use propels a common objective. The system processes one participant&#x02019;s data to influence the experiential state and content of another, who then adjusts their responses based on their partner&#x02019;s feedback. Such collaboration underscores the system&#x02019;s holistic functionality, as depicted below.</p><p>Analysis of participant performance and experimental data indicates that collaboration tends to elicit stronger emotions, impacting EEG data readings. These readings subsequently influence the counterpart&#x02019;s emotions and the interaction&#x02019;s collective outcome. Furthermore, expressing emotions fosters social interactivity, creating a more seamless shared experience. Consequently, emotions effectively circulate between the two participants, creating a dynamic interplay.</p><p>In the creation of emotional experiences, in addition to collaboration and emotional resonance, the real-time nature and accuracy of emotional feedback are crucial factors affecting the quality of the experience. Future research could explore faster emotional feedback mechanisms to ensure that systems can promptly capture and respond to participants&#x02019; emotional changes. Moreover, emotional resonance is not limited to two-person interactions; it can also be expanded to multi-person environments. By synchronizing multiple participants&#x02019; biological data (such as brainwaves, heart rate, etc.), a shared emotional experience system can be established to enhance team collaboration and emotional connectivity. For instance, in multiplayer games or virtual social platforms, participants could use biofeedback to sense others&#x02019; emotional states and adjust their interaction accordingly.</p><p>Furthermore, the visualization of emotional experiences is not confined to images and sounds but can also be expressed through the behavior and expressions of virtual characters. For example, when participants&#x02019; brainwaves reflect emotional changes, virtual characters can respond with corresponding behaviors, such as changes in facial expressions, body movements, or vocal tones. This more anthropomorphic form of feedback can strengthen emotional transmission, making the empathetic experience in virtual worlds more authentic and profound.</p></sec><sec id="sec6dot3-sensors-24-07951"><title>6.3. Ideology</title><p>In this section, we delve into the expression of concepts and the employment of specific strategies, aiming to transcend the technicism discussed previously. Our design approach integrates spatial effects and interactions with a deep exploration of &#x0201c;meaningful creation&#x0201d; concerning AI technology themes. The objective is to intertwine human and technological discussions with the brain interactions of participants, thereby enriching the narrative.</p><p>Leveraging speculative design, we envision future lifestyles via prop design and scene construction, crafting immersive storytelling that uncovers human dilemmas and opportunities, thus vividly conveying thematic ideas. By conceptualizing a scenario and narrative around a humanity test, we intend to provoke thought on the distinct nature of humans versus machines&#x02014;centering on emotions and consciousness. Conserving autonomous consciousness emerges as a pivotal concern in future human-machine dynamics. Therefore, &#x0201c;self-awareness&#x0201d; and &#x0201c;emotions&#x0201d; stand at the forefront of our project, shaping the primary medium of interaction that mirrors individual emotional fluctuations through EEG data. The AI system&#x02019;s virtual persona acts as a third entity within the space, embodying a data-driven intangible personality. This setup encourages reflection on human subjectivity and the evaluation of objectivity, as illustrated in <xref rid="sensors-24-07951-f018" ref-type="fig">Figure 18</xref>.</p><p>This project explores diverse narrative possibilities. Initially, the introduction of AI technology catalyses the shift in interactive artworks from relying on static, pre-written programs to incorporating dynamic, self-learning algorithms. This transition allows the AI to analyze audience interactions, subsequently adapting and formulating new response patterns in real time. Conversely, in experiences involving two participants, the differing identities provide varied experiential perspectives, imbuing the artwork with a range of narrative potentials.</p><p>Regarding the issue of &#x0201c;human subjectivity&#x0201d;, future research can further explore the externalization of self-awareness and its integration with technology. As artificial intelligence and biofeedback systems advance, human emotions, behaviors, and even thoughts may increasingly be externalized as data, posing new challenges to human autonomy. How to maintain human subjectivity in this technological environment, avoiding manipulation or over-reliance on technology, is a social issue that warrants further study. Designers can introduce more reflective interactions, allowing users to become aware of how their emotions and behaviours can be stored as data and even be manipulated, thereby encouraging deeper reflection on human-machine relationships.</p><p>Additionally, speculative design will continue to hold significant potential for application in the future. By constructing virtual and futuristic scenarios, people can better understand the role of AI and biofeedback technologies in future societies. For instance, by designing interactive works with multiple outcomes, users can see different directions in the development of technology and human nature based on their choices. This diversity not only enhances the interactivity and engagement of the experience but also provides users with more space for reflection on the ethical and moral issues brought about by technological advancements.</p></sec><sec id="sec6dot4-sensors-24-07951"><title>6.4. Limitations</title><p>Although this study represents an initial exploration, its generalizability is constrained. Firstly, our focus was on particular AI systems and experiential mechanisms, implying that alternate choices could yield different user experiences. Secondly, exhibition time and spatial constraints restricted participant numbers and the breadth of experiences. A greater familiarity with the equipment might unveil other outcomes. Thirdly, the current EEG data visualization is confined to simplistic abstract graphics. Future endeavors will aim to develop more scientifically precise and aesthetically appealing data mappings and interfaces.</p><p>Additionally, the study did not explore the long-term effects of using EEG data to enhance AI systems in improving user immersion and emotional experience. The participants, aged between 20 and 40, had varying levels of prior experience with EEG devices, with most having no previous exposure, though three participants had used similar equipment before. This lack of diversity in both age and experience could limit the representativeness of the findings, and thus future research should aim to include a broader and more diverse participant base to improve the generalizability of the results.</p><p>In the future, our research will conduct a longitudinal study over a period of 3 to 6 months, allowing for multiple interactions to observe long-term changes in user behavior and emotional responses. The sample size will be expanded to 50 to 60 participants, covering a diverse age range of 20 to 60 years and varying levels of experience with EEG devices, ensuring the diversity and representativeness of the results. Lastly, the study will further optimize EEG data visualization techniques to improve scientific accuracy and user engagement, ensuring effective application of the data and enhanced user experience throughout the research process.</p></sec></sec><sec sec-type="conclusions" id="sec7-sensors-24-07951"><title>7. Conclusions</title><p>This study explored the integration of EEG data into AI systems in a multi-person interactive environment. By designing a controlled experiment with two interaction mechanisms, the results showed that real-time capture and analysis of brainwave activity significantly enhanced users&#x02019; immersion and emotional interaction experiences. Experimental data indicated a 40% increase in immersion and a 50% increase in emotional interaction for participants using the EEG-based interaction mechanism. Emotional reciprocity was particularly stronger during collaborative tasks, leading to smoother social interactions.</p><p>This research also demonstrated the potential of integrating EEG data into AI systems. For example, in gaming and virtual reality environments, EEG can dynamically adjust the environment by monitoring brainwave activity in real time, thereby enhancing user immersion. Additionally, EEG data can be integrated into AI-driven therapeutic tools. By analyzing a user&#x02019;s brain activity, AI systems can identify stress levels or emotional states, aiding healthcare professionals in better understanding patients&#x02019; psychological conditions. In group therapy settings, EEG data can enhance emotional interaction and empathy, fostering greater emotional reciprocity and improving therapeutic outcomes.</p><p>However, the widespread application of EEG systems faces several challenges. First, collecting and analyzing brainwave data raises privacy concerns, as users may feel uncomfortable with AI systems accessing and interpreting their emotional states. Second, in real-world, multi-user environments, the accuracy and reliability of EEG data may be affected by external noise and individual variability, limiting the system&#x02019;s precision and scalability.</p><p>Overall, integrating EEG data into AI systems presents great potential. However, to ensure the effectiveness and widespread adoption of these systems, it is necessary to address technical, ethical, and societal challenges.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, F.F.; methodology, F.F.; software, F.F.; validation, F.F., T.G. and J.W.; formal analysis, F.F.; investigation, F.F.; resources, F.F.; data curation, F.F.; writing&#x02014;original draft preparation, F.F.; writing&#x02014;review and editing, F.F. and T.G.; visualization, F.F.; supervision, J.W.; project administration, F.F.; funding acquisition, J.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study. Written informed consent has been obtained from the patient(s) to publish this paper.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-24-07951"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Artificial Intelligence: A Survey on Evolution, Models, Applications and Future Trends</article-title><source>J. Manag. Anal.</source><year>2019</year><volume>6</volume><fpage>1</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1080/23270012.2019.1570365</pub-id></element-citation></ref><ref id="B2-sensors-24-07951"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Jeon</surname><given-names>M.</given-names></name>
</person-group><article-title>Chapter 1&#x02014;Emotions and Affect in Human Factors and Human&#x02013;Computer Interaction: Taxonomy, Theories, Approaches, and Methods</article-title><source>Emotions and Affect in Human Factors and Human-Computer Interaction</source><person-group person-group-type="editor">
<name><surname>Jeon</surname><given-names>M.</given-names></name>
</person-group><publisher-name>Academic Press</publisher-name><publisher-loc>San Diego, CA, USA</publisher-loc><year>2017</year><fpage>3</fpage><lpage>26</lpage><isbn>978-0-12-801851-4</isbn></element-citation></ref><ref id="B3-sensors-24-07951"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dorneles</surname><given-names>S.O.</given-names></name>
<name><surname>Francisco</surname><given-names>R.</given-names></name>
<name><surname>Barbosa</surname><given-names>D.N.F.</given-names></name>
<name><surname>Barbosa</surname><given-names>J.L.V.</given-names></name>
</person-group><article-title>Context Awareness in Recognition of Affective States: A Systematic Mapping of the Literature</article-title><source>Int. J. Hum.&#x02013;Comput. Interact.</source><year>2023</year><volume>39</volume><fpage>1563</fpage><lpage>1581</lpage></element-citation></ref><ref id="B4-sensors-24-07951"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aranda</surname><given-names>J.A.S.</given-names></name>
<name><surname>Bavaresco</surname><given-names>R.S.</given-names></name>
<name><surname>da Silva Machado</surname><given-names>R.</given-names></name>
<name><surname>de Carvalho</surname><given-names>J.V.</given-names></name>
<name><surname>Corr&#x000ea;a</surname><given-names>A.Y.</given-names></name>
<name><surname>Barbosa</surname><given-names>J.L.V.</given-names></name>
</person-group><article-title>A Multi-Agent System for Optimizing Physiological Collection Based on Adaptive Strategies</article-title><source>Smart Health</source><year>2021</year><volume>19</volume><fpage>100149</fpage><pub-id pub-id-type="doi">10.1016/j.smhl.2020.100149</pub-id></element-citation></ref><ref id="B5-sensors-24-07951"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bobade</surname><given-names>P.</given-names></name>
<name><surname>Vani</surname><given-names>M.</given-names></name>
</person-group><article-title>Stress Detection with Machine Learning and Deep Learning Using Multimodal Physiological Data</article-title><source>Proceedings of the 2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA)</source><conf-loc>Coimbatore, India</conf-loc><conf-date>15&#x02013;17 July 2020</conf-date><fpage>51</fpage><lpage>57</lpage></element-citation></ref><ref id="B6-sensors-24-07951"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Minaee</surname><given-names>S.</given-names></name>
<name><surname>Abdolrashidi</surname><given-names>A.</given-names></name>
<name><surname>Su</surname><given-names>H.</given-names></name>
<name><surname>Bennamoun</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
</person-group><article-title>Biometrics Recognition Using Deep Learning: A Survey</article-title><source>Artif. Intell. Rev.</source><year>2023</year><volume>56</volume><fpage>8647</fpage><lpage>8695</lpage><pub-id pub-id-type="doi">10.1007/s10462-022-10237-x</pub-id></element-citation></ref><ref id="B7-sensors-24-07951"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Di Martino</surname><given-names>F.</given-names></name>
<name><surname>Sessa</surname><given-names>S.</given-names></name>
</person-group><article-title>The Extended Fuzzy <italic toggle="yes">C</italic>-Means Algorithm for Hotspots in Spatio-Temporal GIS</article-title><source>Expert. Syst. Appl.</source><year>2011</year><volume>38</volume><fpage>11829</fpage><lpage>11836</lpage><pub-id pub-id-type="doi">10.1016/j.eswa.2011.03.071</pub-id></element-citation></ref><ref id="B8-sensors-24-07951"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dalmina</surname><given-names>L.</given-names></name>
<name><surname>Barbosa</surname><given-names>J.L.V.</given-names></name>
<name><surname>Vianna</surname><given-names>H.D.</given-names></name>
</person-group><article-title>A Systematic Mapping Study of Gamification Models Oriented to Motivational Characteristics</article-title><source>Behav. Inf. Technol.</source><year>2019</year><volume>38</volume><fpage>1167</fpage><lpage>1184</lpage></element-citation></ref><ref id="B9-sensors-24-07951"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dias</surname><given-names>L.P.S.</given-names></name>
<name><surname>Vianna</surname><given-names>H.D.</given-names></name>
<name><surname>Barbosa</surname><given-names>J.L.V.</given-names></name>
</person-group><article-title>Human Behaviour Data Analysis and Noncommunicable Diseases: A Systematic Mapping Study</article-title><source>Behav. Inf. Technol.</source><year>2023</year><volume>42</volume><fpage>2485</fpage><lpage>2503</lpage></element-citation></ref><ref id="B10-sensors-24-07951"><label>10.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>McLuhan</surname><given-names>M.</given-names></name>
</person-group><article-title>The Medium Is The Message</article-title><source>Communication Theory</source><publisher-name>Routledge</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2008</year><isbn>978-1-315-08091-8</isbn></element-citation></ref><ref id="B11-sensors-24-07951"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tigre Moura</surname><given-names>F.</given-names></name>
<name><surname>Castrucci</surname><given-names>C.</given-names></name>
<name><surname>Hindley</surname><given-names>C.</given-names></name>
</person-group><article-title>Artificial Intelligence Creates Art? An Experimental Investigation of Value and Creativity Perceptions</article-title><source>J. Creat. Behav.</source><year>2023</year><volume>57</volume><fpage>534</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1002/jocb.600</pub-id></element-citation></ref><ref id="B12-sensors-24-07951"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schilling</surname><given-names>A.</given-names></name>
</person-group><article-title>Processes of Our Mediated Spaces</article-title><source>AM J. Art Media Stud.</source><year>2022</year><volume>29</volume><fpage>29</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.25038/am.v0i29.533</pub-id></element-citation></ref><ref id="B13-sensors-24-07951"><label>13.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Simanowski</surname><given-names>R.</given-names></name>
</person-group><source>Digital Art and Meaning: Reading Kinetic Poetry, Text Machines, Mapping Art, and Interactive Installations</source><publisher-name>U of Minnesota Press</publisher-name><publisher-loc>Chicago, CA, USA</publisher-loc><year>2011</year><volume>Volume 35</volume></element-citation></ref><ref id="B14-sensors-24-07951"><label>14.</label><element-citation publication-type="webpage"><article-title>Uncanny Valley: Being Human in the Age of AI</article-title><comment>Available online: <ext-link xlink:href="https://www.famsf.org/exhibitions/uncanny-valley" ext-link-type="uri">https://www.famsf.org/exhibitions/uncanny-valley</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-02-24">(accessed on 24 February 2024)</date-in-citation></element-citation></ref><ref id="B15-sensors-24-07951"><label>15.</label><element-citation publication-type="webpage"><article-title>Deep Feeling: AI and Emotions&#x02013;Museum PT</article-title><comment>Available online: <ext-link xlink:href="https://www.petachtikvamuseum.com/en/exhibitions/deep-feeling-ai-and-emotions/" ext-link-type="uri">https://www.petachtikvamuseum.com/en/exhibitions/deep-feeling-ai-and-emotions/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-02-27">(accessed on 27 February 2024)</date-in-citation></element-citation></ref><ref id="B16-sensors-24-07951"><label>16.</label><element-citation publication-type="webpage"><article-title>RE-Humanism. Alan Advantage</article-title><comment>Available online: <ext-link xlink:href="https://alanadvantage.com/re-humanism/" ext-link-type="uri">https://alanadvantage.com/re-humanism/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-10-30">(accessed on 30 October 2024)</date-in-citation></element-citation></ref><ref id="B17-sensors-24-07951"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zeng</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>N.</given-names></name>
</person-group><article-title>Wind of Bamboo: A Chinese Handwriting Mingling Interactive Installation Based on Human-Ai Collaborative Font Design</article-title><source>Proceedings of the 2022 IEEE VIS Arts Program (VISAP)</source><conf-loc>Oklahoma City, OK, USA</conf-loc><conf-date>16&#x02013;21 October 2022</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2022</year><fpage>80</fpage><lpage>93</lpage></element-citation></ref><ref id="B18-sensors-24-07951"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>K&#x00131;ratl&#x00131;</surname><given-names>&#x0015e;.</given-names></name>
<name><surname>Wolfe</surname><given-names>H.E.</given-names></name>
<name><surname>Bundy</surname><given-names>A.</given-names></name>
</person-group><article-title><italic toggle="yes">Cacophonic Choir</italic>: An Interactive Art Installation Embodying the Voices of Sexual Assault Survivors</article-title><source>Leonardo</source><year>2020</year><volume>53</volume><fpage>446</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1162/leon_a_01935</pub-id></element-citation></ref><ref id="B19-sensors-24-07951"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mathewson</surname><given-names>K.</given-names></name>
<name><surname>Mirowski</surname><given-names>P.</given-names></name>
</person-group><article-title>Improvised Theatre Alongside Artificial Intelligences</article-title><source>Proc. AAAI Conf. Artif. Intell. Interact. Digit. Entertain.</source><year>2017</year><volume>13</volume><fpage>66</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1609/aiide.v13i1.12926</pub-id></element-citation></ref><ref id="B20-sensors-24-07951"><label>20.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>&#x00160;ubrt</surname><given-names>J.</given-names></name>
<name><surname>&#x00160;ubrt</surname><given-names>J.</given-names></name>
</person-group><article-title>Media and Mass Communication</article-title><source>The Sociological Inheritance of the 1960s: Historical Reflections on a Decade of Changing Thought</source><publisher-name>Emerald Publishing Limited</publisher-name><publisher-loc>Bingley, UK</publisher-loc><year>2023</year><fpage>51</fpage><lpage>56</lpage><isbn>978-1-80382-805-3</isbn></element-citation></ref><ref id="B21-sensors-24-07951"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hajare</surname><given-names>R.</given-names></name>
<name><surname>Kadam</surname><given-names>S.</given-names></name>
</person-group><article-title>Comparative Study Analysis of Practical EEG Sensors in Medical Diagnoses</article-title><source>Glob. Transit. Proc.</source><year>2021</year><volume>2</volume><fpage>467</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1016/j.gltp.2021.08.009</pub-id></element-citation></ref><ref id="B22-sensors-24-07951"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Birbaumer</surname><given-names>N.</given-names></name>
<name><surname>Weber</surname><given-names>C.</given-names></name>
<name><surname>Neuper</surname><given-names>C.</given-names></name>
<name><surname>Buch</surname><given-names>E.</given-names></name>
<name><surname>Haapen</surname><given-names>K.</given-names></name>
<name><surname>Cohen</surname><given-names>L.</given-names></name>
</person-group><article-title>Physiological Regulation of Thinking: Brain&#x02013;Computer Interface (BCI) Research</article-title><source>Progress in Brain Research</source><person-group person-group-type="editor">
<name><surname>Neuper</surname><given-names>C.</given-names></name>
<name><surname>Klimesch</surname><given-names>W.</given-names></name>
</person-group><comment>Event-Related Dynamics of Brain Oscillations</comment><publisher-name>Elsevier</publisher-name><publisher-loc>Amsterdam, The Netherlands</publisher-loc><year>2006</year><volume>Volume 159</volume><fpage>369</fpage><lpage>391</lpage></element-citation></ref><ref id="B23-sensors-24-07951"><label>23.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Terblanche</surname><given-names>M.</given-names></name>
</person-group><article-title>COLAB: Social Context and User Experience in Collaborative Multiplayer Games</article-title><source>Master&#x02019;s Thesis</source><publisher-name>University of Cape Town</publisher-name><publisher-loc>Cape Town, South Africa</publisher-loc><year>2017</year></element-citation></ref><ref id="B24-sensors-24-07951"><label>24.</label><element-citation publication-type="webpage"><article-title>Home</article-title><comment>Available online: <ext-link xlink:href="https://davidrosenboom.com/home" ext-link-type="uri">https://davidrosenboom.com/home</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-10-07">(accessed on 7 October 2024)</date-in-citation></element-citation></ref><ref id="B25-sensors-24-07951"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Leslie</surname><given-names>G.</given-names></name>
<name><surname>Mullen</surname><given-names>T.</given-names></name>
</person-group><article-title>MoodMixer: EEG-Based Collaborative Sonification</article-title><source>Proceedings of the 11th annual International Conference on New Interfaces for Musical Expression (NIME)</source><conf-loc>Oslo, Norway</conf-loc><conf-date>30 May&#x02013;1 June 2011</conf-date><fpage>296</fpage><lpage>299</lpage></element-citation></ref><ref id="B26-sensors-24-07951"><label>26.</label><element-citation publication-type="webpage"><article-title>MindMusic: Playful and Social Installations at the Interface Between Music and the Brain|SpringerLink</article-title><comment>Available online: <ext-link xlink:href="https://link.springer.com/chapter/10.1007/978-981-287-546-4_9" ext-link-type="uri">https://link.springer.com/chapter/10.1007/978-981-287-546-4_9</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-10-07">(accessed on 7 October 2024)</date-in-citation></element-citation></ref><ref id="B27-sensors-24-07951"><label>27.</label><element-citation publication-type="webpage"><article-title>Jacqueline Humbert. Wikipedia</article-title><comment>Available online: <ext-link xlink:href="https://en.wikipedia.org/wiki/Jacqueline_Humbert" ext-link-type="uri">https://en.wikipedia.org/wiki/Jacqueline_Humbert</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-10-30">(accessed on 30 October 2024)</date-in-citation></element-citation></ref><ref id="B28-sensors-24-07951"><label>28.</label><element-citation publication-type="webpage"><article-title>Cognogenesis. Tim Mullen</article-title><comment>Available online: <ext-link xlink:href="https://artpower.ucsd.edu/release-artpower-uc-san-diego-announces-third-annual-filmatic-festival/" ext-link-type="uri">https://artpower.ucsd.edu/release-artpower-uc-san-diego-announces-third-annual-filmatic-festival/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-10-30">(accessed on 30 October 2024)</date-in-citation></element-citation></ref><ref id="B29-sensors-24-07951"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dikker</surname><given-names>S.</given-names></name>
<name><surname>Michalareas</surname><given-names>G.</given-names></name>
<name><surname>Oostrik</surname><given-names>M.</given-names></name>
<name><surname>Serafimaki</surname><given-names>A.</given-names></name>
<name><surname>Kahraman</surname><given-names>H.M.</given-names></name>
<name><surname>Struiksma</surname><given-names>M.E.</given-names></name>
<name><surname>Poeppel</surname><given-names>D.</given-names></name>
</person-group><article-title>Crowdsourcing Neuroscience: Inter-Brain Coupling during Face-to-Face Interactions Outside the Laboratory</article-title><source>NeuroImage</source><year>2021</year><volume>227</volume><fpage>117436</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117436</pub-id><pub-id pub-id-type="pmid">33039619</pub-id>
</element-citation></ref><ref id="B30-sensors-24-07951"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kinch</surname><given-names>S.</given-names></name>
<name><surname>Pakanen</surname><given-names>M.</given-names></name>
<name><surname>Heiselberg</surname><given-names>K.</given-names></name>
<name><surname>Dindler</surname><given-names>C.</given-names></name>
<name><surname>Iversen</surname><given-names>A.-M.</given-names></name>
<name><surname>Krogh</surname><given-names>P.G.</given-names></name>
</person-group><article-title>An Exploratory Study of Using Speculative Artefacts in Co-Design</article-title><source>CoDesign</source><year>2023</year><volume>19</volume><fpage>91</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1080/15710882.2021.2016847</pub-id></element-citation></ref><ref id="B31-sensors-24-07951"><label>31.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Ho</surname><given-names>R.T.H.</given-names></name>
<name><surname>Markosov</surname><given-names>S.H.</given-names></name>
<name><surname>Sanders</surname><given-names>N.</given-names></name>
<name><surname>Nam</surname><given-names>C.S.</given-names></name>
</person-group><article-title>BCI-Based Expressive Arts: Moving Toward Mind-Body Alignment</article-title><source>Brain Art: Brain-Computer Interfaces for Artistic Expression</source><person-group person-group-type="editor">
<name><surname>Nijholt</surname><given-names>A.</given-names></name>
</person-group><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2019</year><fpage>355</fpage><lpage>373</lpage><isbn>978-3-030-14323-7</isbn></element-citation></ref><ref id="B32-sensors-24-07951"><label>32.</label><element-citation publication-type="webpage"><article-title>Rafael Lozano-Hemmer&#x02014;Level of Confidence</article-title><comment>Available online: <ext-link xlink:href="https://www.lozano-hemmer.com/level_of_confidence.php" ext-link-type="uri">https://www.lozano-hemmer.com/level_of_confidence.php</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-02-24">(accessed on 24 February 2024)</date-in-citation></element-citation></ref><ref id="B33-sensors-24-07951"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Calise</surname><given-names>A.</given-names></name>
</person-group><article-title>Inhabiting the Museum: A History of Physical Presence from Analog to Digital Exhibition Spaces</article-title><source>AN-ICON</source><year>2023</year><volume>2</volume><fpage>56</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.54103/ai/19907</pub-id></element-citation></ref><ref id="B34-sensors-24-07951"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Donoff</surname><given-names>G.</given-names></name>
<name><surname>Bridgman</surname><given-names>R.</given-names></name>
</person-group><article-title>The Playful City: Constructing a Typology for Urban Design Interventions</article-title><source>Int. J. Play</source><year>2017</year><volume>6</volume><fpage>294</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1080/21594937.2017.1382995</pub-id></element-citation></ref><ref id="B35-sensors-24-07951"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sra</surname><given-names>M.</given-names></name>
<name><surname>Xu</surname><given-names>X.</given-names></name>
<name><surname>Maes</surname><given-names>P.</given-names></name>
</person-group><article-title>BreathVR: Leveraging Breathing as a Directly Controlled Interface for Virtual Reality Games</article-title><source>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>21&#x02013;26 April 2018</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2018</year><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="B36-sensors-24-07951"><label>36.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Wardrip-Fruin</surname><given-names>N.</given-names></name>
</person-group><article-title>Digital Media Archaeology: Interpreting Computational Processes</article-title><source>Media Archaeology: Approaches, Applications, and Implications</source><publisher-name>University of California Press</publisher-name><publisher-loc>Berkeley, CA, USA</publisher-loc><year>2011</year><fpage>302322</fpage></element-citation></ref><ref id="B37-sensors-24-07951"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tang</surname><given-names>Y.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Gao</surname><given-names>Z.</given-names></name>
<name><surname>Pan</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Braud</surname><given-names>T.</given-names></name>
<name><surname>Lee</surname><given-names>C.H.</given-names></name>
<name><surname>Asadipour</surname><given-names>A.</given-names></name>
</person-group><article-title>AI N&#x000fc;shu (Women&#x02019;s Scripts)&#x02014;An Exploration of Language Emergence in Sisterhood</article-title><source>Proceedings of the SIGGRAPH Asia 2023 Art Gallery</source><conf-loc>Sydney, NSW, Australia</conf-loc><conf-date>12&#x02013;15 December 2023</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2023</year><fpage>1</fpage><lpage>2</lpage></element-citation></ref><ref id="B38-sensors-24-07951"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Ren</surname><given-names>D.</given-names></name>
<name><surname>Legrady</surname><given-names>G.</given-names></name>
</person-group><article-title>Cangjie&#x02019;s Poetry: An Interactive Art Experience of a Semantic Human-Machine Reality</article-title><source>Proc. ACM Comput. Graph. Interact. Tech.</source><year>2021</year><volume>4</volume><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="B39-sensors-24-07951"><label>39.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Pietsch</surname><given-names>C.</given-names></name>
<name><surname>Stankowski</surname><given-names>A.</given-names></name>
</person-group><source>Transferscope&#x02014;Making Multi-Modal Conditioning for Image Diffusion Models Tangible</source><publisher-name>Gesellschaft f&#x000fc;r Informatik e.V.</publisher-name><publisher-loc>Bonn, Germany</publisher-loc><year>2024</year><pub-id pub-id-type="doi">10.18420/muc2024-mci-demo-248</pub-id></element-citation></ref><ref id="B40-sensors-24-07951"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Trichopoulos</surname><given-names>G.</given-names></name>
<name><surname>Konstantakis</surname><given-names>M.</given-names></name>
<name><surname>Alexandridis</surname><given-names>G.</given-names></name>
<name><surname>Caridakis</surname><given-names>G.</given-names></name>
</person-group><article-title>Large Language Models as Recommendation Systems in Museums</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>3829</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12183829</pub-id></element-citation></ref><ref id="B41-sensors-24-07951"><label>41.</label><element-citation publication-type="webpage"><article-title>The Met x Microsoft x MIT: What Can Artificial Intelligence Do with Art?</article-title><comment>Available online: <ext-link xlink:href="https://trendland.com/the-metropolitan-museum-of-art-x-microsoft-and-artificial-intelligence/" ext-link-type="uri">https://trendland.com/the-metropolitan-museum-of-art-x-microsoft-and-artificial-intelligence/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-10-07">(accessed on 7 October 2024)</date-in-citation></element-citation></ref><ref id="B42-sensors-24-07951"><label>42.</label><element-citation publication-type="webpage"><article-title>New Nature | ARTECHOUSE</article-title><comment>Available online: <ext-link xlink:href="https://www.artechouse.com/program/new-nature/" ext-link-type="uri">https://www.artechouse.com/program/new-nature/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-10-07">(accessed on 7 October 2024)</date-in-citation></element-citation></ref><ref id="B43-sensors-24-07951"><label>43.</label><element-citation publication-type="webpage"><article-title>2021 Graduation Project Story|Lai Xingyu: &#x02018;Thought Without Thought&#x02019;&#x02014;Focused Meditation Popular Science Experience Design Based on Brain-Computer Interaction_Research</article-title><comment>Available online: <ext-link xlink:href="https://www.sohu.com/a/478885500_121119371" ext-link-type="uri">https://www.sohu.com/a/478885500_121119371</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-10-07">(accessed on 7 October 2024)</date-in-citation></element-citation></ref><ref id="B44-sensors-24-07951"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Amit</surname><given-names>I.</given-names></name>
<name><surname>Matherly</surname><given-names>J.</given-names></name>
<name><surname>Hewlett</surname><given-names>W.</given-names></name>
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Meshi</surname><given-names>Y.</given-names></name>
<name><surname>Weinberger</surname><given-names>Y.</given-names></name>
</person-group><article-title>Machine Learning in Cyber-Security&#x02014;Problems, Challenges and Data Sets</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1812.07858</pub-id></element-citation></ref><ref id="B45-sensors-24-07951"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fang</surname><given-names>F.</given-names></name>
<name><surname>Yan</surname><given-names>S.</given-names></name>
</person-group><article-title>Voight-Kampff 2.0</article-title><source>Proceedings of the SIGGRAPH Asia 2022 Art Gallery</source><conf-loc>Daegu, Republic of Korea</conf-loc><conf-date>6&#x02013;9 December 2022</conf-date><publisher-name>Association for Computing Machinery</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2022</year><fpage>1</fpage></element-citation></ref><ref id="B46-sensors-24-07951"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>M.-T.</given-names></name>
<name><surname>She</surname><given-names>H.-C.</given-names></name>
<name><surname>Annetta</surname><given-names>L.A.</given-names></name>
</person-group><article-title>Game Immersion Experience: Its Hierarchical Structure and Impact on Game-Based Science Learning</article-title><source>J. Comput. Assist. Learn.</source><year>2015</year><volume>31</volume><fpage>232</fpage><lpage>253</lpage><pub-id pub-id-type="doi">10.1111/jcal.12066</pub-id></element-citation></ref><ref id="B47-sensors-24-07951"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bradley</surname><given-names>M.M.</given-names></name>
<name><surname>Lang</surname><given-names>P.J.</given-names></name>
</person-group><article-title>Measuring Emotion: The Self-Assessment Manikin and the Semantic Differential</article-title><source>J. Behav. Ther. Exp. Psychiatry</source><year>1994</year><volume>25</volume><fpage>49</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/0005-7916(94)90063-9</pub-id><pub-id pub-id-type="pmid">7962581</pub-id>
</element-citation></ref><ref id="B48-sensors-24-07951"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hima</surname><given-names>C.S.</given-names></name>
<name><surname>Asheeta</surname><given-names>A.</given-names></name>
<name><surname>Nair</surname><given-names>C.C.</given-names></name>
<name><surname>Nair</surname><given-names>S.M.</given-names></name>
</person-group><article-title>A Review on Brainwave Therapy</article-title><source>World J. Pharm. Sci.</source><year>2020</year><volume>8</volume><fpage>39</fpage><lpage>73</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-24-07951-f001"><label>Figure 1</label><caption><p>Three intersecting fields of human-computer interaction, physiological data sensors, and artificial intelligence.</p></caption><graphic xlink:href="sensors-24-07951-g001" position="float"/></fig><fig position="float" id="sensors-24-07951-f002"><label>Figure 2</label><caption><p>The &#x0201c;Uncanny Valley: Being Human in the Age of AI&#x0201d; exhibition.</p></caption><graphic xlink:href="sensors-24-07951-g002" position="float"/></fig><fig position="float" id="sensors-24-07951-f003"><label>Figure 3</label><caption><p>Interface Interaction.</p></caption><graphic xlink:href="sensors-24-07951-g003" position="float"/></fig><fig position="float" id="sensors-24-07951-f004"><label>Figure 4</label><caption><p>The general vision of <xref rid="sec3-sensors-24-07951" ref-type="sec">Section 3</xref>.</p></caption><graphic xlink:href="sensors-24-07951-g004" position="float"/></fig><fig position="float" id="sensors-24-07951-f005"><label>Figure 5</label><caption><p>Voight-Kampff 2.0 on the exhibition.</p></caption><graphic xlink:href="sensors-24-07951-g005" position="float"/></fig><fig position="float" id="sensors-24-07951-f006"><label>Figure 6</label><caption><p>The startup interface of Voight-Kampff 2.0.</p></caption><graphic xlink:href="sensors-24-07951-g006" position="float"/></fig><fig position="float" id="sensors-24-07951-f007"><label>Figure 7</label><caption><p>The interface viewed by the questioner.</p></caption><graphic xlink:href="sensors-24-07951-g007" position="float"/></fig><fig position="float" id="sensors-24-07951-f008"><label>Figure 8</label><caption><p>The respondent&#x02019;s interface.</p></caption><graphic xlink:href="sensors-24-07951-g008" position="float"/></fig><fig position="float" id="sensors-24-07951-f009"><label>Figure 9</label><caption><p>The questioner makes their selection through the interface.</p></caption><graphic xlink:href="sensors-24-07951-g009" position="float"/></fig><fig position="float" id="sensors-24-07951-f010"><label>Figure 10</label><caption><p>System map.</p></caption><graphic xlink:href="sensors-24-07951-g010" position="float"/></fig><fig position="float" id="sensors-24-07951-f011"><label>Figure 11</label><caption><p>How EEG data is processed.</p></caption><graphic xlink:href="sensors-24-07951-g011" position="float"/></fig><fig position="float" id="sensors-24-07951-f012"><label>Figure 12</label><caption><p>Game Immersion Questionnaire (GIQ) was designed with 24 questions to assess various aspects of game immersion. Each dimension is measured using a 5-point Likert scale to gauge users&#x02019; experiences.</p></caption><graphic xlink:href="sensors-24-07951-g012" position="float"/></fig><fig position="float" id="sensors-24-07951-f013"><label>Figure 13</label><caption><p>SAM&#x02019;s use of pictures to communicate the scale.</p></caption><graphic xlink:href="sensors-24-07951-g013" position="float"/></fig><fig position="float" id="sensors-24-07951-f014"><label>Figure 14</label><caption><p>GIQ scores for different mechanisms. The choice of the 0&#x02013;5 scale was due to the design of the GIQ, where each question was scored on a Likert scale (0&#x02013;5).</p></caption><graphic xlink:href="sensors-24-07951-g014" position="float"/></fig><fig position="float" id="sensors-24-07951-f015"><label>Figure 15</label><caption><p>Experimental process.</p></caption><graphic xlink:href="sensors-24-07951-g015" position="float"/></fig><fig position="float" id="sensors-24-07951-f016"><label>Figure 16</label><caption><p>Audio-visual effects design.</p></caption><graphic xlink:href="sensors-24-07951-g016" position="float"/></fig><fig position="float" id="sensors-24-07951-f017"><label>Figure 17</label><caption><p>Emotional loop.</p></caption><graphic xlink:href="sensors-24-07951-g017" position="float"/></fig><fig position="float" id="sensors-24-07951-f018"><label>Figure 18</label><caption><p>Ideological design.</p></caption><graphic xlink:href="sensors-24-07951-g018" position="float"/></fig><table-wrap position="float" id="sensors-24-07951-t001"><object-id pub-id-type="pii">sensors-24-07951-t001_Table 1</object-id><label>Table 1</label><caption><p>Multi-agent BCI works summary.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Multi-Agent BCI Works</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Participation Mode<break/>(Competition/Cooperation)</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Type of Work <break/>(Interface/Immersive)</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Mode of Expression</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Alpha Checkers (1969)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interface</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Game</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">New York Biofeedback Quartet (1969)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Immersive</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bio-music</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ecology of the Skin (1970)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Immersive</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Participation experience</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vancouver Piece (1972)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Immersive</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Identity exploration</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Portable Gold and Philosophers&#x02019; Stones (1972)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Immersive</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Concert</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Alpha Garden (1973)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interface</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Installation</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brainwave Etch-A-Sketch (1974)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interface</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Painting</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BrainBall (1999)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Competition</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interface</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Game</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bacteria Hunt (2010)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Competition</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interface</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Game</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MoodMixer (2011)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interface</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Concert</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ringing Minds (2014)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Immersive</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Concert</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hopscotch a mobile opera for 24 cars (2015)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Immersive</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interactive drama</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Assembly Cognogenesis (2016)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Immersive</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Virtual reality</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NeuroBrush (2019)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Competition</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Interface</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Installation</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mutual Wave Machine (2019)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cooperation</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Immersive</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual display</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07951-t002"><object-id pub-id-type="pii">sensors-24-07951-t002_Table 2</object-id><label>Table 2</label><caption><p>Summary of AI systems integrated with physiological data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AI System</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">System Input</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AI N&#x000fc;shu (Women&#x02019;s scripts) (2023)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cangjie&#x02019;s poetry (2021)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Transferscope (2024)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Machine Learning in Cyber-Security (2019)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Nivel de confidence (2022)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">New Nature (2018)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ephemera (2024)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Auditory input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recommendation systems in museums (2023)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Auditory input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Storyteller (2019)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Auditory input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">museum AI-generated narration (2022)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Range input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cacophonic Choir (2014)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Range input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wu Xiang Zhi Xiang (2021)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEG input</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wave UFO (2005)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EEG input</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07951-t003"><object-id pub-id-type="pii">sensors-24-07951-t003_Table 3</object-id><label>Table 3</label><caption><p>Self-Assessment Manikin (SAM) scale scores of C1 and C2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Valence</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Arousal</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dominance</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">C2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-07951-t004"><object-id pub-id-type="pii">sensors-24-07951-t004_Table 4</object-id><label>Table 4</label><caption><p>Summary of the discussion content.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Design Direction</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Objective</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Strategy</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Key Content</th></tr></thead><tbody><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">6.1 Creating Spatial Immersion</td><td align="left" valign="middle" rowspan="1" colspan="1">Enhance participants&#x02019; <break/>sense of immersion.</td><td align="left" valign="middle" rowspan="1" colspan="1">Data mapping</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Particularly &#x0201c;full-body&#x0201d; <break/>engagement</td><td align="left" valign="middle" rowspan="1" colspan="1">Data processing</td><td align="left" valign="middle" rowspan="1" colspan="1">Mapping brainwave bands (e.g., Alpha, Beta, Theta) to audio-visual effects, controlling smooth data changes using programming, symbolic visual design.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Audio-visual feedback</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">6.2 Creating Emotional Experiences</td><td align="left" valign="middle" rowspan="1" colspan="1">Increase participants&#x02019; <break/>emotional resonance and<break/>social interaction.</td><td align="left" valign="middle" rowspan="1" colspan="1">Mutual brainwave <break/>influence</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Create memorable and <break/>impactful experiences.</td><td align="left" valign="middle" rowspan="1" colspan="1">Shared goals</td><td align="left" valign="middle" rowspan="1" colspan="1">Two-person collaboration, where brainwave data mutually influences emotions, generating emotional loops that enhance social interaction and shared experiences.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Collaborative experience</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr><tr><td rowspan="3" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">6.3 Ideology</td><td align="left" valign="middle" rowspan="1" colspan="1">Explore the theme of <break/>human nature vs. <break/>technology.</td><td align="left" valign="middle" rowspan="1" colspan="1">Speculative design</td><td align="left" valign="middle" rowspan="1" colspan="1">
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Highlight the distinction <break/>between humans and <break/>machines through <break/>participants&#x02019; brainwaves.</td><td align="left" valign="middle" rowspan="1" colspan="1">Dynamically generated<break/>response patterns</td><td align="left" valign="middle" rowspan="1" colspan="1">Brainwave data showcasing emotional and conscious interaction, symbolizing the reflection on human subjectivity, with AI-driven dynamic interaction responses.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multiple narrative <break/>possibilities</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td></tr></tbody></table></table-wrap></floats-group></article>