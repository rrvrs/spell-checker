<!--

File produced by pipelineRunner package (for JATS 2 SCJATS with pipeline SCJATS)
At: 2025-05-22T15:12:29.717Z

Version        : 1.16.1
Last update    : 2024-08-27
Modified by    : dunnm

-->
<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Database (Oxford)</journal-id><journal-id journal-id-type="iso-abbrev">Database (Oxford)</journal-id><journal-id journal-id-type="publisher-id">databa</journal-id><journal-title-group><journal-title>Database: The Journal of Biological Databases and Curation</journal-title></journal-title-group><issn pub-type="epub">1758-0463</issn><publisher><publisher-name>Oxford University Press</publisher-name><publisher-loc>UK</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40392751</article-id><article-id pub-id-type="pmc">PMC12090995</article-id>
<article-id pub-id-type="doi">10.1093/database/baaf008</article-id><article-id pub-id-type="publisher-id">baaf008</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Article</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI00960</subject></subj-group></article-categories><title-group><article-title>An exploratory study combining Virtual Reality and Semantic Web for life science research using Graph2VR</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6108-5552</contrib-id><name><surname>Kellmann</surname><given-names>Alexander J</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0004-3743-650X</contrib-id><name><surname>van den Hoek</surname><given-names>Sander</given-names></name><aff>
<institution content-type="department">Department of Genetics, Genomics Coordination Center, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff></contrib><contrib contrib-type="author"><name><surname>Postema</surname><given-names>Max</given-names></name><aff>
<institution content-type="department">Department of Genetics, Genomics Coordination Center, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7086-0637</contrib-id><name><surname>Maassen</surname><given-names>W T Kars</given-names></name><aff>
<institution content-type="department">Department of Genetics, Genomics Coordination Center, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9937-2928</contrib-id><name><surname>Hijmans</surname><given-names>Brenda S</given-names></name><aff>
<institution content-type="department">Department of Genetics, Genomics Coordination Center, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1919-6547</contrib-id><name><surname>van der Geest</surname><given-names>Marije A</given-names></name><aff>
<institution content-type="department">Department of Genetics, Genomics Coordination Center, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0934-8375</contrib-id><name><surname>van der Velde</surname><given-names>K Joeri</given-names></name><aff>
<institution content-type="department">Department of Genetics, Genomics Coordination Center, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff><xref rid="FT0001" ref-type="author-notes"/></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2440-3993</contrib-id><name><surname>van Enckevort</surname><given-names>Esther J</given-names></name><aff>
<institution content-type="department">Department of Genetics, Genomics Coordination Center, University Medical Center Groningen</institution>, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, <country country="NL">The Netherlands</country></aff><xref rid="FT0001" ref-type="author-notes"/></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0979-3401</contrib-id><name><surname>Swertz</surname><given-names>Morris A</given-names></name><!--m.a.swertz@rug.nl--><xref rid="COR0001" ref-type="corresp"/></contrib></contrib-group><author-notes><corresp id="COR0001">*Corresponding authors. Morris A. Swertz, Department of Genetics, Genomics Coordination Center, University Medical Center Groningen, Antonius Deusinglaan 1, Groningen, Groningen 9713 AV, The Netherlands. E-mail: <email xlink:href="m.a.swertz@rug.nl">m.a.swertz@rug.nl</email></corresp><fn id="FT0001"><label>&#x02021;</label><p>Equal contribution.</p></fn></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub" iso-8601-date="2025-05-20"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>5</month><year>2025</year></pub-date><volume>2025</volume><elocation-id>baaf008</elocation-id><history><date date-type="received"><day>26</day><month>4</month><year>2024</year></date><date date-type="rev-recd"><day>21</day><month>6</month><year>2024</year></date><date date-type="accepted"><day>27</day><month>1</month><year>2025</year></date><date date-type="editorial-decision"><day>19</day><month>11</month><year>2024</year></date><date date-type="corrected-typeset"><day>20</day><month>5</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025. Published by Oxford University Press.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="baaf008.pdf"/><abstract><title>Abstract</title><p>We previously described Graph2VR, a prototype that enables researchers to use virtual reality (VR) to explore and navigate through Linked Data graphs using SPARQL queries (see <ext-link xlink:href="https://doi.org/10.1093/database/baae008" ext-link-type="uri">https://doi.org/10.1093/database/baae008</ext-link>). Here we evaluate the use of Graph2VR in three realistic life science use cases. The first use case visualizes metadata from large-scale multi-center cohort studies across Europe and Canada via the EUCAN Connect catalogue. The second use case involves a set of genomic data from synthetic rare disease patients, which was processed through the Variant Interpretation Pipeline and then converted into Resource Description Format for visualization. The third use case involves enriching a graph with additional information, in this case, the Dutch Anatomical Therapeutic Chemical code Ontology with the DrugID from Drugbank. These examples collectively showcase Graph2VR&#x02019;s potential for data exploration and enrichment, as well as some of its limitations. We conclude that the endless three-dimensional space provided by VR indeed shows much potential for the navigation of very large knowledge graphs, and we provide recommendations for data preparation and VR tooling moving forward.</p><p>
<bold>Database URL</bold>: <ext-link xlink:href="https://doi.org/10.1093/database/baaf008" ext-link-type="uri">https://doi.org/10.1093/database/baaf008</ext-link></p></abstract><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>H2020 Health</institution><institution-id institution-id-type="DOI">10.13039/100010677</institution-id></institution-wrap>
</funding-source><award-id>EC H2020 Grant Agreement No 824989</award-id></award-group></funding-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>H2020 Health</institution><institution-id institution-id-type="DOI">10.13039/100010677</institution-id></institution-wrap>
</funding-source><award-id>EC H2020 Grant Agreement No 824989</award-id></award-group></funding-group><counts><page-count count="19"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Data in life sciences and health is increasing in volume and complexity. It is stored in countless places and offered in equally many shapes and forms Often, Semantic Web technology is now chosen to represent these fragmented and heterogeneous data as Semantic Web statements, e.g. using Resource Description Format (RDF) triples, in a &#x0201c;knowledge graph&#x0201d; that can be cross-linked across the web for joined querying, analysis, and visualization. However, querying datasets in VR remains an unexplored challenge. For instance, while a PubMed search for &#x0201c;linked data&#x0201d; or &#x0201c;semantic web&#x0201d; or &#x0201c;RDF&#x0201d; yields over 5000 hits, and a search for &#x0201c;virtual reality&#x0201d; yields over 20&#x02009;000 hits, a combined search of those terms only yields six hits. This shows that virtual reality (VR) techniques are being discussed in research and health care in the life sciences domain, but that the combination of Linked Data and VR is nearly nonexistent. Furthermore, of the six hits that were found, none of them actually describe the integration of these two techniques.</p><p>In a previous work, we developed the Graph2VR prototype to visualize Linked Data triples in VR, offering an immersive experience for users to intuitively explore and interact with Semantic Web data [<xref rid="R1" ref-type="bibr">1</xref>]. In this manuscript, we evaluate how Graph2VR performs when applied to realistic life science use cases.</p><p>Graph2VR is able to visualize SPARQL &#x0201c;CONSTRUCT&#x0201d; query results in VR, offering an immersive experience for users to explore and interact with data in a virtual environment. SPARQL (SPARQL Protocol and RDF Query Language) is a query language used to retrieve and manipulate data stored in RDF format in the form of triples. SPARQL
offers different forms of queries like &#x0201c;SELECT,&#x0201d; &#x0201c;DESCRIBE,&#x0201d; &#x0201c;ASK&#x0201d; and &#x0201c;CONSTRUCT&#x0201d; queries [<xref rid="R2" ref-type="bibr">2</xref>]. While SELECT queries return table-like result sets, CONSTRUCT queries return results in a graph structure. In Graph2VR, the user can see these graphs as nodes connected by edges that represent the predicates connecting the nodes. The user can then grab and move the nodes around in a three-dimensional space. The nodes and edges can also be set as parameters in a query to create a template for the SPARQL query, providing a visual and easy-to-understand representation of the query. Graph2VR also allows navigation through complex knowledge graphs by expanding or collapsing edges that connect the different nodes, allowing users to drill down into the data and explore the relationships between different nodes. This is not restricted to one database; it is possible to switch between databases and graphs on the fly and to expand the data with triples from different sources.</p><p>Our aim in this study is to demonstrate the use of Graph2VR with realistic scientific data to show its practical application and versatility. To do so, we focus on three distinct use cases selected to evaluate the use of Graph2VR on different types of data in the life science field. The first use case involves the MOLGENIS catalogue, part of the EU-CAN Connect project [<xref rid="R3" ref-type="bibr">3</xref>], which visualizes metadata for large-scale cohort studies and real-world evidence data [<xref rid="R4" ref-type="bibr">4</xref>]. This catalogue is used by researchers to make their datasets insightful and facilitates cooperation between different studies. The second use case examines the Variant Interpretation Pipeline (VIP), a tool developed by the University Medical Center Groningen Genomics Coordination Center for prioritizing and interpreting potentially disease-causing genetic variations from next-generation sequencing results in diagnostics [<xref rid="R5" ref-type="bibr">5</xref>]. The third use case explores data from Drugbank [<xref rid="R6" ref-type="bibr">6</xref>] and PubChem [<xref rid="R7" ref-type="bibr">7</xref>], focusing on the intersection of Dutch drug names with Anatomical Therapeutic Chemical (ATC) codes. We believe that Use case 1 represents databases involving complex conceptual knowledge with many different elements, whereas Use case 2 represents a highly detailed molecular data use case and Use case 3 represents studies that involve huge data graphs spanning multiple databases.</p><p>Below we first describe the methods and materials used for all use cases. Subsequently, we describe the experimentation with each use case. For each, we first developed different scenarios to evaluate the added value of Graph2VR. We then selected and converted data relevant to the use case into RDF format. Finally, we uploaded the RDF into a Virtuoso server, visualized the different datasets in Graph2VR, and evaluated how this served the use cases. For each use case, we provide specific observations, followed by a cross-use case discussion summarizing our findings and recommendations.</p></sec><sec id="s2"><title>Materials and Methods</title><p>To run Graph2VR, we used a Quest 2 headset (the Quest 3 also works), a Windows PC with a modern graphics card (GTX 1060 or higher), and Docker to run a Virtuoso server. The full description of the requirements can be found on GitHub [<xref rid="R8" ref-type="bibr">8</xref>]. We also expect users to have some knowledge of the Semantic Web and SPARQL. For each of the three use cases, we prepared the data, uploaded it to SPARQL endpoints, configured Graph2VR, and then explored the results. We used the dockerized versions of the Open Link Virtuoso server [<xref rid="R9" ref-type="bibr">9</xref>] to upload the data for each use case into separate SPARQL endpoints. To use the three datasets within Graph2VR, the existing data had to be translated into a graph database format like RDF or Web Ontology Language. This process varied across the different projects. After preparing each dataset, we uploaded the dataset to a Virtuoso server, enabling access to the data with Graph2VR via the SPARQL endpoint. The server and starting query that Graph2VR uses could then be configured. After selecting a starting point in the graph, the user could use the menu to expand the graph and the controls to interact with the graph and move through space. Since the setup of the Virtuoso server and configuring Graph2VR are consistent across all use cases, below we first explain these processes in general terms, followed by a detailed description of each use case.</p><sec id="s2-s1"><title>Setting up a local Virtuoso Server using Docker</title><p>The choice of Virtuoso as the SPARQL Endpoint for Graph2VR was driven by its ability to: (I) efficiently host large amounts of triples and (ii) serve as a backend for Graph2VR. A Virtuoso server can be used to load and expose Linked Data via a SPARQL Endpoint (we did not explicitly implement support for other servers, although DotNetRDF would offer the functions to connect to them). More information can be found in the GitHub Readme in the Graph2VR</p><p>Manual [<xref rid="R8" ref-type="bibr">8</xref>, <xref rid="R10" ref-type="bibr">10</xref>] A Virtuoso server as a backend can be set up via Docker. Virtuoso&#x02019;s web interface can also be used to upload small .owl or .rdf files. Larger files must be loaded via the Virtuoso bulk loader.</p></sec><sec id="s2-s2"><title>Setting up Graph2VR</title><p>Graph2VR can be downloaded from GitHub via <ext-link xlink:href="https://github.com/molgenis/Graph2VR" ext-link-type="uri">https://github.com/molgenis/Graph2VR</ext-link>. There are currently two versions: a stand-alone version in the form of an .apk file that can be loaded on the Quest 2 headset and a Windows version in the Graph2VR.zip file that can be unzipped and run on Windows [<xref rid="R1" ref-type="bibr">1</xref>, <xref rid="R8" ref-type="bibr">8</xref>]. While the stand-alone version does not require that the headset is connected to a computer, the Windows version requires a continuous connection between the computer and headset via cable or WiFi. The advantage of this is that the computer&#x02019;s graphics card can be used, allowing for better performance and the ability to handle more nodes simultaneously. Use of the stand-alone version requires uploading the .apk file or Graph2VR save files to the VR headset (see Figure A.11). Sidequest, a third-party program, allows access to the Quest 2 headset to upload the .apk file [<xref rid="R11" ref-type="bibr">11</xref>].</p><p>The settings.txt file can be used to configure Graph2VR. It is used to set the SPARQL Endpoint, to optionally limit the queries to a graph, and to define a query for the initial graph. It can also be used to specify
additional databases to switch to. Switching between graphs can be done via the settings menu. For more details, see the Graph2VR manual [<xref rid="R10" ref-type="bibr">10</xref>]. For the stand-alone version, the settings file needs to be stored in the folder (&#x0201d;sdcard/Android/data/com.Graph2VR.Graph2VR/files&#x0201d;) on the headset. Graph2VR creates this folder when a save state is created. If the uploaded settings file is not a valid settings file (e.g. not a valid JSON file), Graph2VR will apply internal default settings. The consequence of this is that the initial SPARQL query needs to be JSON-encoded. Only CONSTRUCT queries can be used as initial queries. While certain functionalities (such as BIND, MINUS, or subqueries) are not yet implemented as options within Graph2VR&#x02019;s GUI, these can still be used within the initial query. An explanation of most settings of the settings.txt can be found in section E.4.</p></sec><sec id="s2-s3"><title>Preparing the use cases</title><p>We prepared the three use cases described in the &#x0201c;Introduction&#x0201d; section following the same procedure. First, we held a workshop with experts associated with the use case in which we defined what data might be useful and what potential objectives of the Graph2VR application would be relevant. This resulted in a list of queries in the form of user scenarios similar to &#x0201c;as a x I would like to y in order to z&#x0201d; in which &#x0201c;x&#x0201d; represents a specific user persona, &#x0201c;y&#x0201d; describes an activity, and &#x0201c;z&#x0201d; defines the envisioned impact. Subsequently, we prepared the RDF dataset and uploaded that into Virtuoso. The datasets were structured and semantically annotated to suit each user scenario. The details for each use case are described below. We then entered the main part of the Graph2VR evaluation by having a second workshop in VR. One of the participants operated the headset, while the others could see VR headset&#x02019;s view on a computer monitor. For the first use case, we observed errors in the rdf export of the dataset that was addressed by fixing the data export in Molgenis. Each use case experiment was completed by discussion of the findings. In the &#x0201c;Results&#x0201d; section, we summarize the findings for each use case.</p></sec><sec id="s2-s4"><title>Experimental Setup</title><p>The lead authors reached out to experts within associated projects and invited them to participate in the project and become co-authors. The background of these users was a combination of bioinformatics, i.e. some technical background, and domain knowledge, e.g. being an expert in genome data analysis. These experts had seen demos of Graph2VR or even participated as test users in the Graph2VR usability study [<xref rid="R1" ref-type="bibr">1</xref>]. During the experiment, in most cases the &#x0201c;user&#x0201d; was actually a combination of a developer, who operated the Graph2VR controls, and a domain expert, working together to achieve the use case result. In the manuscript, we refer to either or both of these roles as the &#x0201c;user&#x0201d; for readability. We conducted brainstorming sessions to identify use cases, described the necessary steps, built the required data, and converted it into RDF format. We then performed a test run (half a day) with a group of 3 to 4 people to execute the test scenario. This process typically revealed issues like missing data or relationships, which led to three or four iterations until a complete use case could be demonstrated. We included screenshots in the manuscript to illustrate the user experience. We evaluated the performance of VR compared to 2D visualization subjectively by asking the test users. Note that we did not prioritize the readability of the text because in VR, everything looks larger and cropping the image would make it impossible to see the bigger picture and the spacious aspect of VR.</p></sec></sec><sec id="s3"><title>Results</title><p>Below we report the results from the use case studies. For each use case, we provide the background of the case and a description of the usage scenarios. We then describe the data preparation and conversion into RDF. Subsequently, we describe the main part of the use case studies: the application of Graph2VR to these data. Finally, we summarize observations made during the use case.</p><sec id="s3-s1"><title>Use case 1: Finding interoperable cohort data</title><sec id="s3-s1-s1"><title>Use case 1: Background</title><p>Use case 1 is driven by the desire to integrate data from multiple populations for joint analysis. In the past decades, large cohort studies have collected health-related data from large numbers of individuals to study complex effects on health and disease. To increase the sample size and diversity of the data, and therefore the statistical power, researchers want to share, combine, and harmonize data from different studies and geographic locations. Therefore, many projects are now cataloguing these cohorts [<xref rid="R4" ref-type="bibr">4</xref>] and describing the harmonization of their heterogeneous variables onto agreed-upon standard variables as the basis for integrated analysis of data from multiple cohorts [<xref rid="R12" ref-type="bibr">12</xref>]. These catalogues contain only metadata (not the privacy-sensitive data itself), but they can guide researchers on how to find and request data access to the sources they require for pooled data analysis.</p></sec><sec id="s3-s1-s2"><title>Use case 1: Scenarios</title><p>Based on discussions with the domain experts, we selected two main scenarios for Graph2VR. The first scenario is focused on finding and grouping resources (cohorts, databanks, etc.) to match a research topic, based on the idea of a principal investigator (PI) who wants to create a research project/consortium. In this scenario, the user needs to first navigate the topics available and select a topic (or topics) of interest, e.g. &#x0201c;smoking&#x0201d; or &#x0201c;blood pressure&#x0201d; or &#x0201c;tobacco.&#x0201d; From this topic, the user then needs to navigate to the resources linking to these topics (using the keywords). In this scenario, the resource would be a cohort that the PI might want to contact in the end. Based on this result, the user will want to make a query to find all the other resources that have the same matches. Subsequently, the user could annotate the resources into a new graph.</p><p>The second scenario goes one step further and is focused on identifying data variables available in each of the cohorts as a basis for the study execution. Here, the user again selects a topic of interest, but instead of browsing for cohorts, they navigate to the variables defined on this topic. This would allow the PI to assess to what extent data are readily available, either because it is collected directly in the relevant form or because data harmonization has already been done.</p></sec><sec id="s3-s1-s3"><title>Use case 1: methods and materials</title><sec id="s3-s1-s3-s1"><title>Data</title><p>As a dataset, we used a sample from the catalogue that the MOLGENIS group works on in EU projects such as EUCAN Connect, LifeCycle, European Human Exposome Network, and IMI Conception [<xref rid="R13" ref-type="bibr">13&#x02013;16</xref>]. The catalogue is stored on a server that runs on the EMX2 version of MOLGENIS <ext-link xlink:href="https://molgeniscatalogue" ext-link-type="uri">https://molgeniscatalogue</ext-link>. org. EMX2 allows automatic export of data in the form of RDF, and several of the metadata elements were already encoded using the underlying URIs. Therefore, the export of the catalogue data was done just using the export functionality of Molgenis. To ease user understanding, we used only a small random subsample of the data, containing only a few cohorts.</p></sec><sec id="s3-s1-s3-s2"><title>RDF representation</title><p>The first automatic RDF export of the catalogue data proved to be rudimentary, with the structure of the RDF in the catalogue insufficient or malformed. For example, the same URI was used simultaneously as a class, individual, and predicate, something that should not happen unintentionally. As this led to several issues, we worked with the EMX2 team to improve the RDF export and finally imported the RDF file into a Virtuoso server. We then defined a starting query in the settings of Graph2VR. While the starting query had to be set via the Unity editor in the settings singleton in earlier versions of Graph2VR, it is now possible to edit the settings via the file settings.txt (It is necessary to encode the SPARQL query as JSON. We used an online tool to encode the SPARQL query [<xref rid="R17" ref-type="bibr">17</xref>]) without using Unity.</p></sec></sec><sec id="s3-s1-s4"><title>Use case 1: Results</title><sec id="s3-s1-s4-s1"><title>Scenario 1: Finding cohorts containing &#x0201c;smoking&#x0201d; data</title><p>In the Molgenis Catalogue use case, we decided to start with a class hierarchy with a depth of 1, similar to the class hierarchy in Prote&#x000b4;ge&#x000b4; [<xref rid="R18" ref-type="bibr">18</xref>]. To do so, our starting query for Graph2VR was written as follows:</p><p>
<disp-quote><preformat position="float" xml:space="preserve">PREFIX rdf: &#x0003c;<ext-link xlink:href="http://www.w3.org/1999/02/22-rdf-syntax-ns#" ext-link-type="uri">http://www.w3.org/1999/02/22-rdf-syntax-ns#</ext-link>&#x0003e; PREFIX owl: &#x0003c;<ext-link xlink:href="http://www.w3.org/2002/07/owl#" ext-link-type="uri">http://www.w3.org/2002/07/owl#</ext-link>&#x0003e;
PREFIX rdfs: &#x0003c;<ext-link xlink:href="http://www.w3.org/2000/01/rdf-schema" ext-link-type="uri">http://www.w3.org/2000/01/rdf-schema</ext-link>#&#x0003e;
Construct{
&#x000a0;&#x000a0;&#x000a0;?superclass rdfs:subClassOf owl:Thing.
&#x000a0;&#x000a0;&#x000a0;?superclass rdfs:label ?label.}
WHERE {
&#x000a0;&#x000a0;&#x000a0;?superclass rdfs:subClassOf owl:Thing.
&#x000a0;&#x000a0;&#x000a0;Optional {
&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;?superclass rdfs:label ?label.
&#x000a0;&#x000a0;}
}
</preformat></disp-quote>
</p><p>The result was a hierarchy showing all the data classes available in the catalogue (see <xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref>).</p><fig position="float" id="F1" fig-type="figure"><label>Figure&#x000a0;1.</label><caption><p>Screenshot of the Molgenis catalogue&#x02019;s class hierarchy in Graph2VR. The instances of the &#x0201c;subcohorts&#x0201d; are expanded.</p></caption><graphic xlink:href="baaf008f1" position="float"/></fig><p>The user was then able to locate &#x0201c;the&#x0201d; Areas of information cohorts&#x02019; class and therein navigate to the topic &#x0201c;Tobacco.&#x0201d; (Within the catalogue, all information related to smoking is tagged with tobacco.) From this topic, the user could navigate to one of the cohorts containing data on this topic, in this particular example, the DBNC study. Based on this example, the user could then create a query pattern that limits the topic to &#x0201c;Tobacco&#x0201d; and then find all cohorts with the same pattern (see <xref rid="F2" ref-type="fig">Fig.&#x000a0;2</xref>) to display them instead of the raw URIs.</p><fig position="float" id="F2" fig-type="figure"><label>Figure&#x000a0;2.</label><caption><p>An example figure showing the selection of the topic &#x0201c;tobacco,&#x0201d; followed by navigation to the &#x0201c;DNBC&#x0201d; cohort. The users then specified a query pattern and saw that the PSYCONN cohort was also linked to the same topic in this example set (the original catalogue holds many more records). (The labels were requested manually in Graph2VR).</p></caption><graphic xlink:href="baaf008f2" position="float"/></fig></sec><sec id="s3-s1-s4-s2"><title>Scenario 2: Finding datasets with variables harmonized with &#x0201c;smoking&#x0201d;</title><p>In the second scenario, the user started with the &#x0201c;Variable mappings&#x0201d; and aimed to find data harmonizations that had the harmonized variable of &#x0201c;Blood pressure&#x0201d; as a target. Here, the user navigated from &#x0201c;Variable mappings&#x0201d; to the &#x0201c;target variable&#x0201d; to those target variables that have &#x0201c;Blood pressure&#x0201d; as their main keyword. Subsequently, the user navigated to the &#x0201c;source variables&#x0201d; of that harmonization and then on to the datasets that hold those variables (see <xref rid="F3" ref-type="fig">Fig.&#x000a0;3</xref>). Based on this example graph, the user can define a query pattern in order to retrieve all other source variables with similar mappings, which is a way to identify all the &#x0201c;smoking&#x0201d;-relevant datasets in the catalogue. <xref rid="F3" ref-type="fig">Figure&#x000a0;3</xref> shows how such a query pattern is defined.</p><fig position="float" id="F3" fig-type="figure"><label>Figure&#x000a0;3.</label><caption><p>How the user made a query pattern for the variable &#x0201c;blood pressure&#x0201d; and then found all variable-mappings that fulfil this condition. There are three cohorts (G21, ENVIRONAGE, and GENR) in this sample dataset that match, as seen in the middle diamond. This example data was taken from the LifeCycle project. For a complete study overview, see [<xref rid="R14" ref-type="bibr">14</xref>].</p></caption><graphic xlink:href="baaf008f3" position="float"/></fig></sec></sec><sec id="s3-s1-s5"><title>Use case 1: Evaluation</title><p>In the catalogue RDF representation, the user was sometimes misled by the domain or range predicate information. In SPARQL, the predicate rdfs:domain should define the class of an individual used as a subject when using a specific predicate. Similarly, rdfs:range is used to limit the permissible input values for the object for forming valid triples with the respective predicate. While Graph2VR is agnostic about the meaning of any predicates for expanding the graph, users might come across incoming predicates like rdfs:domain or rdfs:range. These lead to nodes describing predicates instead of classes, individuals, or literals.</p><p>In addition, we observed that metadata collected at the level of variables is relatively sparse. While cohorts generally have huge data dictionaries, these are not annotated against a common ontology. It is only in cases of data harmonization that a lot of effort is spent on mapping the harmonized variables to ontologies. Therefore, the second scenario in this use case is rather artificial. A more realistic use case would be for new researchers to search for previously harmonized variables.</p></sec></sec><sec id="s3-s2"><title>Use case 2: DNA variant interpretation pipeline</title><sec id="s3-s2-s1"><title>Use case 2: Background</title><p>Genome diagnostics is the process of analyzing the DNA of patients with a suspected heritable disease. The patient&#x02019;s DNA is sequenced, and current methods can now measure all genes (i.e. protein-coding DNA, also called whole-exome sequencing) or even all DNA (whole-genome sequencing). The DNA measurement is then compared against the known human reference genome to identify differences, which typically yields 1000 to 100&#x02009;000 DNA differences called variants. Finding the correct diagnosis is a process of matching diseases, genes, phenotypes, and variants in genes, which involves connecting different data sources. We speculated that visualizing relationships between these different layers of information in VR could speed up the process of prioritizing disease-causing variants. As a basis for this use case, we used the MOLGENIS VIP. VIP uses a decision tree that applies different methods based on best practices from genome diagnostics and research. VIP starts by checking for confirmed variants from databases, then filters for only &#x0201c;rare&#x0201d; variants (those not commonly observed in the general population), and finally continues with statistical methods. If that does not work, VIP uses machine learning methods like CAPICE to predict how a genetic variant should be classified [<xref rid="R19" ref-type="bibr">19</xref>]. By applying the decision tree, VIP is able to: present a shortlist of variants that might be relevant to rare diseases; annotate each of these variants with a suggested classification of either Pathogenic (P), Likely Pathogenic (LP), Variant of Unknown Significance (VUS), Likely Benign (LB) or Benign (B); and apply filters to keep only P/LP/VUS variants for expert evaluation [<xref rid="R5" ref-type="bibr">5</xref>]. The suggested list of potential disease-causing pathogenic variants is then interpreted by lab specialists who make a final genetic diagnosis. A clinical geneticist then communicates the diagnosis to the patient. Knowing the genetic diagnosis may help to provide a prognosis and even to select an appropriate treatment.</p></sec><sec id="s3-s2-s2"><title>Use case 2: Scenarios</title><p>Together with the use case experts, we chose three scenarios to use Graph2VR for joint analysis of a cohort of multiple patients with an unclear genetic diagnosis. We speculated that analysis of such unsolved &#x0201c;cohorts&#x0201d; would result in the most interesting questions where visualization as graph in VR could provide unexpected insights [<xref rid="R20" ref-type="bibr">20</xref>]</p><list list-type="bullet"><list-item><p>The first scenario is to find and explore affected genes (i.e. genes with causal variants) that are shared between patients. Here, the Graph2VR user aimed to shortlist relevant DNA variations that are potentially associated with a rare disease, find the associated genes, and view them in groups.</p></list-item><list-item><p>The second scenario explored patients by clustering them based on their phenotypes. In this scenario, given a group of patients, the Graph2VR user had to find out what phenotypes have been observed for each patient and overlap them. The overall goal was to identify which phenotypes are shared among many patients (disease-specific), which occur rarely, and which occur specifically in disease-affected individuals.</p></list-item><list-item><p>The third scenario aimed to understand if a patient cohort could be clustered based on the differences in phenotypes in relationships with their causal DNA variants. Here, given the group of patients, the Graph2VR user had to find out what phenotypes are associated to the observed genes and cluster these. The overall goal of this scenario was to identify new genes that are not yet well documented to be linked to disease, which could later be used to short-list DNA variants for individual patients.</p></list-item></list></sec><sec id="s3-s2-s3"><title>Use case 2: methods and materials</title><sec id="s3-s2-s3-s1"><title>Dataset</title><p>Because patient data is sensitive, we used a public dataset for this evaluation. We extracted a subset of patients from the Trujillano <italic toggle="yes">et&#x000a0;al</italic>. whole-exome sequencing of 1000 cases (see &#x0201c;<xref rid="s8" ref-type="sec">Supplementary Table S3</xref>&#x0201d; of [<xref rid="R21" ref-type="bibr">21</xref>]), specifically patients with the phenotypes &#x0201c;Visual impairment&#x0201d; (HP:0000505) or &#x0201c;Cerebral visual impairment&#x0201d; (HP:0100704). We then synthesized realistic data for the use case by combining these patient data with variants from the general population cohort GoNL [<xref rid="R22" ref-type="bibr">22</xref>]. For this, each patient was run through the Ensembl Variant Effect Predictor, after which they were enriched with variants from GoNL, before being processed by VIP [<xref rid="R23" ref-type="bibr">23</xref>]. For reproducibility, we have uploaded our scripts on Zenodo with a README file detailing the exact process [<xref rid="R24" ref-type="bibr">24</xref>].</p></sec><sec id="s3-s2-s3-s2"><title>RDF representation</title><p>Part of the output from VIP was converted to RDF Turtle files using a custom application [<xref rid="R25" ref-type="bibr">25</xref>], including features relevant to the use case as described below. This custom application takes a VIP output .vcf file and a VIP sample-sheet file containing patient metadata, together with two additional files to enrich this data, and then converts them into a Turtle RDF file. These two enrichment files are an Ensembl glossary .owl file and a Human Phenotype Ontology (HPO) .owl file. These are used to add relevant information and/or convert non-RDF data to RDF (IRIs). The output Turtle files were then added to the Virtuoso server [<xref rid="R24" ref-type="bibr">24</xref>, <xref rid="R26" ref-type="bibr">26&#x02013;28</xref>]. These are used to generate relevant Internationalized Resource Identifiers (IRI) for Semantic Web usage and to retrieve additional information. Where possible, pre-existing standards and IRI were used to define the information. An overview of the namespaces used and what they were used for can be found in <xref rid="s8" ref-type="sec">Supplementary Appendix Table C.1</xref>. The design of the final RDF output from a VIP .vcf file is shown in <xref rid="F4" ref-type="fig">Fig.&#x000a0;4</xref>. The VIP vcf files do contain additional fields (such as more data about individual transcripts). These were not included, as they were not relevant for this use case. The generated RDF data was then loaded into both GraphDB and Virtuoso (for Graph2VR) [<xref rid="R29" ref-type="bibr">29</xref>]. Within GraphDB, several SPARQL queries were written so that a visual comparison could be made to investigate how a 3D virtual environment could provide added value compared to 2D visualizations on a screen.</p><fig position="float" id="F4" fig-type="figure"><label>Figure&#x000a0;4.</label><caption><p>This figure provides a schematic overview of the structure and interconnections within the RDF data (turtle files) generated from VIP VCF files including potential references to external sources. Each box represents a node. Each line represents a predicate. The arrow points towards the object of a triple (subject&#x02014;predicate&#x02014;object). The text within the brackets of a box shows the rdf:type of that node. For the lines, the text within brackets represents the IRI used as predicate to link nodes. Note that multiple objects are allowed for certain subjects, even though this is not explicitly mentioned in the figure (e.g. a patient can have multiple variants).</p></caption><graphic xlink:href="baaf008f4" position="float"/></fig></sec></sec><sec id="s3-s2-s4"><title>Use case 2: Results</title><sec id="s3-s2-s4-s1"><title>Scenario 1: Cluster patients per gene</title><p>For the VIP data, we developed queries that can identify individuals who share variations in the same gene. We visualized the results in both Graph2VR and GraphDB (see <xref rid="F5" ref-type="fig">Fig.&#x000a0;5</xref>). Individuals sharing variations in the same gene are likely to experience similar health issues (or benefits) due to these variations, particularly if they have the same variation. This is because such variations can directly alter the function or structure of the protein(s) the gene encodes, which can have various implications. <xref rid="F5" ref-type="fig">Figure&#x000a0;5</xref> illustrates that the same relations can be displayed in Graph2VR and GraphDB. In Graph2VR, the automatic layouts were used to position the nodes, while in GraphDB, they were placed manually. The advantage of the GraphDB representation in this visualization is that nodes are colored differently based on their rdf:type, so it is immediately visible which node belongs to which group [<xref rid="R30" ref-type="bibr">30</xref>]. This has certain limitations, however, for example if a node has multiple rdf:type&#x02019;s, or with federated querying, where it might not be possible to automatically process the necessary information. A distinct random coloring that is consistently used for each rdf:type of a node is something also used by other tools, such as Gruff [<xref rid="R31" ref-type="bibr">31</xref>]. Currently, the coloring of nodes and edges in Graph2VR is mainly based on the VOWL schema. As a result, however, both individual people and genes are treated as instances in this visualization, and both are represented as blue nodes with labels. The labels of the nodes are written within the nodes in GraphDB and below the nodes in Graph2VR.</p><fig position="float" id="F5" fig-type="figure"><label>Figure&#x000a0;5.</label><caption><p>(a) Graph2VR using automatic layouts (first Class hierarchy layout, then 3D Force directed). Even though the graphs in the screenshot look very small, they are only spread out, and the user can easily navigate closer in VR to interact with them. (b) GraphDB manual layout where the red nodes represent patients and the blue nodes genes. Individuals in the dataset, clustered per gene (the order of the graphs in Graph2VR and GraphDB varies).</p></caption><graphic xlink:href="baaf008f5" position="float"/></fig></sec><sec id="s3-s2-s4-s2"><title>Scenario 2: cluster patients per phenotype (with more than two patients per phenotype)</title><p>Patients can manifest multiple disease phenotypes, which they can share with other individuals. This can be visualized in a network of patients and phenotypes, where shared phenotypes lead to a denser network (see <xref rid="F6" ref-type="fig">Fig.&#x000a0;6</xref>). Using this visualization, it is evident that patients do not necessarily share the same cause of their phenotypes. A cluster of individuals may share many of the same phenotypes, hinting that they all have the same or related diseases, while other patients only share one phenotype and most likely have a very different disease. The subset was filtered on &#x0201c;Visual impairment&#x0201d; (HP:0000505) and &#x0201c;Cerebral visual impairment&#x0201d; (HP:0100704), explaining the high number of patients associated with &#x0201c;Visual impairment.&#x0201d;</p><fig position="float" id="F6" fig-type="figure"><label>Figure&#x000a0;6.</label><caption><p>(a) Graph2VR layout. (b): GraphDB default layout. Red nodes represent patients. Blue nodes represent phenotypes. Clustering of individuals with the same phenotype (minimum of two per HPO) in Graph2VR 3D force-directed layout (a) and in GraphDB automatic layout (b).</p></caption><graphic xlink:href="baaf008f6" position="float"/></fig></sec><sec id="s3-s2-s4-s3"><title>Scenario 3: closing the loop with DisGeNET</title><p>DisGeNET is a resource of gene&#x02013;disease associations (GDAs) [<xref rid="R32" ref-type="bibr">32</xref>]. It offers a SPARQL Endpoint that allows users to query for those associations. Known variations in the gene sequence of the actual patient data can be compared with known GDAs from the database to see which of the patient&#x02019;s phenotypes match phenotypes known to be associated with the gene carrying the variant. In this way, we can create &#x0201c;closed loops.&#x0201d; These loops start with the patient, their phenotypes, and the genes associated with their variant&#x02019;s transcripts. Within DisGeNET, &#x0201c;any&#x0201d; Disease, Disorder, or Finding that is associated with the patient&#x02019;s phenotypes is retrieved, together with any GDAs linked to it. From here, the loop is closed by connecting the GDAs from DisGeNET to the genes associated with a patient. Any GDAs that cannot form a closed loop are not included in the graph. Additionally, any evidence (source) is included where available. Building such a loop can help users interpret these data and pinpoint the causal variant in the patient in order to provide them a genetic diagnosis. <xref rid="F7" ref-type="fig">Figure&#x000a0;7</xref> shows different visualizations of this data.</p><fig position="float" id="F7" fig-type="figure"><label>Figure&#x000a0;7.</label><caption><p>(a) GraphDB layout after manually ordering the nodes. An overlay was added to distinguish which nodes belong to which dataset. The different colours indicate that the nodes have different rdf:types (e.g. light blue being a gene and dark blue a &#x0201c;Disease, Disorder or Finding&#x0201d;). However, not all data was colour-labelled by GraphDB, probably due to the use of a federated query where some data might not have been automatically processable for these functionalities. (b) Graph2VR visualisation using the hierarchical view as layout (not the class hierarchy). Comparison of data visualisations of GraphDB and Graph2VR.</p></caption><graphic xlink:href="baaf008f7" position="float"/></fig></sec></sec></sec><sec id="s3-s3"><title>Use case 2: Evaluation</title><p>For scenario 1, we ultimately found that the user did not experience added benefit from using VR over a conventional 2D visualization because of the small number of nodes that could also be easily displayed and navigated in 2D. In retrospect, we should have filtered the original dataset less or not at all so that it would have been larger and necessitate the use of 3D in VR. Alternatively, we could also have expanded the graph with additional data sources to increase its size and complexity. In scenario 2, VR became much more useful because the networks became complex and interconnected. Here, the 3D VR representation allows the user to find nodes more easily because they are spaced out much farther than is possible in 2D, while relationships are also easily explored by grabbing and moving nodes along with their edges. We hypothesize that visualizing relationships between these different layers of genotypic and phenotypic information can accelerate the prioritization of disease-causing DNA variants. Lastly, in scenario 3, a manual 2D layout for one patient is insightful, but when scaling up to multiple patients and working with automated layouts, VR offers much more freedom and interactivity to explore these rich and complicated networks, especially as it allows users to expand specific predicates with more precision. This ability to expand the graph using multiple different endpoints is a feature of Graph2VR that not many comparable tools (like Relfinder) offer [<xref rid="R33" ref-type="bibr">33</xref>].</p></sec><sec id="s3-s4"><title>Use case 3: PubChem</title><sec id="s3-s4-s1"><title>Use case 3: background</title><p>PubChem and Drugbank are two large databases about chemical substances and drugs. The PubChem website describes itself as &#x0201c;the world&#x02019;s largest collection of freely accessible chemical information,&#x0201d; whereas Drugbank is a &#x0201c;knowledge base of about 500&#x02009;000+ drugs and drug products&#x0201d; [<xref rid="R6" ref-type="bibr">6</xref>, <xref rid="R34" ref-type="bibr">34</xref>, <xref rid="R35" ref-type="bibr">35</xref>]. Thus, while Drugbank contains data about drugs and their attributes, PubChem contains chemical information. Both datasets, but especially PubChem, also provide good examples of larger datasets. There is also overlap between them, and in many cases there are references from one dataset to the other. If this is not (yet) the case, identifiers like Chemical Abstracts Service (CAS) numbers might be considered for matching. We chose this example with the idea that large graph databases may impact the real-time user experience in Graph2VR. Besides testing the handling of such large datasets, the goal of this scenario was to find out whether it is possible to use Graph2VR to enrich data about ATC codes with additional chemical information from PubChem, using Drugbank as an intermediate database.</p></sec><sec id="s3-s4-s2"><title>Use case 3: Scenarios</title><p>The context of the scenario is to combine Drugbank, PubChem, and Dutch ATC codes to enable users to understand how drug names might match underlying chemical compounds and their properties.</p><p>ATC codes classify drugs not only by their chemical components, like aspirin, but also by how they are applied (whether it is a cream, an injection, or a pill) and what the drug is used for. Most drugs have only one ATC code, but this can vary. Cortisone, for example, has more than 10 different ATC codes. During the COVID-19 pandemic in 2020, the Lifelines cohort study collected data from participants across the northern Netherlands. On 30 March 2020, a Lifelines cohort study started to identify protective and risk factors for SARS-CoV-2 susceptibility [<xref rid="R36" ref-type="bibr">36</xref>, <xref rid="R37" ref-type="bibr">37</xref>]. They sent out online questionnaires to capture several factors, including which drugs participants used. For this, our group developed a method to semiautomatically map participants&#x02019; free-text answers about the drugs to ATC codes. We created an ontology that referred from ATC codes to Dutch drug names to support experts in mapping those drugs to standardized codes faster. This would help to standardize and then effectively filter datasets based on the groups within the ATC hierarchy. This ATC standardization enables statistical evaluation to identify protective or risk factors based on certain groups of medications. The ATC code Ontology contains Dutch drugs with different names, but those are related to ATC codes within an English ATC code ontology [<xref rid="R36" ref-type="bibr">36</xref>]. In this use case, we wanted to see whether this ontology could be enriched with information from Drugbank or PubChem. The URIs used in the ontology were created generically based on a hash function. Those URIs will probably not match any common URL standard, but the ATC codes themselves can be used to connect the ontology to other drugs.</p><p>The focus here was to start with an existing entry, which could be the ATC code of a Dutch drug, and see what medical and chemical information can be added. For example, there are dozens of drugs that actually contain aspirin, and these could be treated as one drug for analysis (depending on the question), but this requires that users are able to resolve this information.</p></sec><sec id="s3-s4-s3"><title>Use case 3: methods and materials datasets used</title><p>We used the following datasets:</p><list list-type="bullet"><list-item><p>&#x0201c;Dutch ATC code Ontology&#x0201d; is a collection of Dutch drug names put together for semiautomatic mapping and connected to their respective ATC codes. The dataset is structured by the ATC code ontology [<xref rid="R36" ref-type="bibr">36</xref>].</p></list-item><list-item><p>&#x0201c;Drugbank&#x0201d; contains information about drugs, including names, different IDs, and targets. It is available for academic research [<xref rid="R6" ref-type="bibr">6</xref>].</p></list-item><list-item><p>&#x0201c;PubChem&#x0201d; holds general information about almost all registered small chemical compounds. It is available as RDF [<xref rid="R7" ref-type="bibr">7</xref>, <xref rid="R38" ref-type="bibr">38</xref>].</p></list-item></list><sec id="s3-s4-s3-s1"><title>RDF preparation</title><p>To execute this query, it is necessary to allow insert statements, which can be done with the isql command before running the actual insert query.
grant execute on DB.DBA.SPARQL_INSERT_DICT_CONTENT to &#x0201c;SPARQL&#x0201d;;</p></sec><sec id="s3-s4-s3-s2"><title>Graph2VR application</title><p>We prepared Graph2VR by entering the address of our local Virtuoso instance with Drugbank and PubChem in separate instances of Virtuoso. As recommended, we uploaded each of PubChem&#x02019;s graphs (substances, compounds, synonyms, etc.) into separate graphs in Virtuoso. It is not necessary to load the whole dataset, just the relevant graph(s).</p></sec><sec id="s3-s4-s3-s3"><title>Specific example queries/challenges</title><p>In this example, data conversion is not the issue. PubChem already offers its data as Linked Data, and a script for the conversion into RDF is readily available for Drugbank. We used version5.1.10 of Drugbank (April 2023) and converted it to n-triples. To do so, we reused a script from the Bio2RDF repository [<xref rid="R39" ref-type="bibr">39&#x02013;41</xref>]. The script translated the .xml file from Drugbank into n-triples, which were then uploaded to a Virtuoso server. One particular issue with these two datasets compared to those in the previous use cases is their size. At 7 GB, Drugbank is too big to be uploaded into Virtuoso via the web interface, so we had to use the bulk loader to load the data into the Virtuoso server [<xref rid="R9" ref-type="bibr">9</xref>]. While the Drugbank import was relatively swift, the PubChem dataset occupied nearly 1TB (938 GB) of storage once imported into Virtuoso. In addition, some known issues with Virtuoso 7.x caused memory leaks. This issue can be avoided by using an older version of Virtuoso (e.g. version 6.1) [<xref rid="R42" ref-type="bibr">42</xref>]. It took almost 3&#x02009;days to import all of PubChem&#x02019;s graphs into a Virtuoso server using the bulk loader. As mentioned, PubChem is divided into several graphs, and selecting only the relevant ones will save time and storage. We imported into Virtuoso following the guidelines provided on the PubChem website [<xref rid="R7" ref-type="bibr">7</xref>]. Based on the structure of PubChem (see Table&#x000a0;1 of [<xref rid="R38" ref-type="bibr">38</xref>]), we know that the relevant information to connect substances in PubChem with their respective counterparts in Drugbank is connected via the predicate sio:has-attribute to a depositor, which then has a value (sio:hasValue) and a type. The prefix &#x0201c;sio&#x0201d; stands for the Semanticscience integrated ontology (SIO) and is a shortcut for &#x0201c;<ext-link xlink:href="http://semanticscience.org/resource/" ext-link-type="uri">http://semanticscience.org/resource/</ext-link>&#x0201d; [<xref rid="R43" ref-type="bibr">43</xref>]. The type is encoded with CHEMINF 000406 for the DrugID, which refers directly to the ID within Drugbank. Alternatively, CHEMINF 000446 is the type for the CAS number, which is a unique classifier for each chemical substance in the CAS Registry. The prefix &#x0201c;CHEMINF&#x0201d; stands for Chemical Information Ontology and is an abbreviation for &#x0201c;<ext-link xlink:href="http://semanticscience.org/resource/" ext-link-type="uri">http://semanticscience.org/resource/</ext-link>&#x0201d; [<xref rid="R44" ref-type="bibr">44</xref>].</p><p>
<disp-quote><preformat position="float" xml:space="preserve">For PubChem, such a SPARQL CONSTRUCT query could look like this:
PREFIX sio: &#x0003c;<ext-link xlink:href="http://semanticscience.org/resource/" ext-link-type="uri">http://semanticscience.org/resource/</ext-link>&#x0003e;
PREFIX rdf: &#x0003c;<ext-link xlink:href="http://www.w3.org/1999/02/22-rdf-syntax-ns#" ext-link-type="uri">http://www.w3.org/1999/02/22-rdf-syntax-ns#</ext-link>&#x0003e; PREFIX cheminf: &#x0003c;<ext-link xlink:href="http://semanticscience.org/resource/" ext-link-type="uri">http://semanticscience.org/resource/</ext-link>&#x0003e;
PREFIX compound: &#x0003c;<ext-link xlink:href="http://rdf.ncbi.nlm.nih.gov/pubchem/compound/" ext-link-type="uri">http://rdf.ncbi.nlm.nih.gov/pubchem/compound/</ext-link>&#x0003e;
Construct{
&#x000a0;&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier sio:SIO_000011 ?compound.
&#x000a0;&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier sio:SIO_000300 ?DrugID.
&#x000a0;&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier rdf:type cheminf:CHEMINF_000406.
&#x000a0;&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier2 sio:SIO_000011 ?compound.
&#x000a0;&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier2 sio:SIO_000300 ?CASno.
&#x000a0;&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier2 rdf:type cheminf:CHEMINF_000446.}
WHERE {
&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier sio:SIO_000011 ?compound.
&#x000a0;&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier sio:SIO_000300 ?DrugID.
&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier rdf:type cheminf:CHEMINF_000406.
OPTIONAL{
&#x000a0;&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier2 sio:SIO_000011 ?compound.
&#x000a0;&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier2 sio:SIO_000300 ?CASno.
&#x000a0;&#x000a0;&#x000a0;&#x000a0;?depositor_identyfier2 rdf:type cheminf:CHEMINF_000446.
}
} ORDER BY (?DrugID) Limit 10
</preformat></disp-quote>
</p><p>PubChem uses the URIs for the compounds or substances to refer to the actual substances, but some depositors do describe the relations to identifiers. In this case, those are the SIO codes sio:SIO_000011 (&#x0201d;is attribute of&#x0201d;) that connect those references with the actual compound, the predicate sio:SIO_000300 (&#x0201d;has value of&#x0201d;) that actually holds the identifier, and an identifier that the node is linking to, in this case cheminf:CHEMINF_000406 (&#x0201d;Drugbank identifier&#x0201d;) and cheminf:CHEMINF_00446 (&#x0201d;CASNO&#x0201d;) to describe the CAS number of a compound. Since PubChem has information about 30&#x02009;000 Drugs, we limited the number of results to 10. Otherwise, we would probably run into a timeout.</p><p>In the rdf version of Drugbank that we created with the Bio2RDF scripts, the ATC codes are related to each drug using the following schema:</p><p>&#x0003c;<ext-link xlink:href="http://bio2rdf.org/drugbank:DB00945" ext-link-type="uri">http://bio2rdf.org/drugbank:DB00945</ext-link>&#x0003e;</p><p>&#x0003c;<ext-link xlink:href="http://bio2rdf.org/drugbank_vocabulary:x-atc" ext-link-type="uri">http://bio2rdf.org/drugbank_vocabulary:x-atc</ext-link>&#x0003e;</p><p>&#x0003c;<ext-link xlink:href="http://bio2rdf.org/atc:B01AC06" ext-link-type="uri">http://bio2rdf.org/atc:B01AC06</ext-link>&#x0003e;.</p><p>This triple encodes aspirin with the Drugbank ID &#x0201c;DB00945&#x0201d; has the ATC code &#x0201c;B01AC06.&#x0201d;</p><p>The third data source, the Dutch ATC code ontology represents ATC codes as URIs, as well as the Drugbank representation. Since there is no reference between the two and their URIs differ (although they should be the same), we generated the relation, which made navigation between the graphs in Graph2VR easier later on. The only difference between the two is the prefix of each database, which is followed by the actual ATC code. Since both define ATC codes, we can use the owl:sameAs relation to connect them.
<disp-quote><preformat position="float" xml:space="preserve">PREFIX uatc: &#x0003c;<ext-link xlink:href="http://purl.bioontology.org/ontology/UATC/" ext-link-type="uri">http://purl.bioontology.org/ontology/UATC/</ext-link>&#x0003e;
PREFIX owl: &#x0003c;<ext-link xlink:href="http://www.w3.org/2002/07/owl" ext-link-type="uri">http://www.w3.org/2002/07/owl</ext-link>#&#x0003e;
PREFIX atc: &#x0003c;<ext-link xlink:href="http://bio2rdf.org/atc" ext-link-type="uri">http://bio2rdf.org/atc</ext-link>:&#x0003e;
INSERT {
GRAPH &#x0003c;<ext-link xlink:href="http://localhost:8896/ATC" ext-link-type="uri">http://localhost:8896/ATC</ext-link>&#x0003e; {
?uatcUri owl:sameAs ?atcUri.
}
}
WHERE {
?uatcUri rdf:type owl:Class.
FILTER(STRSTARTS(STR(?uatcUri), &#x0201c;<ext-link xlink:href="http://purl.bioontology.org/ontology/UATC/" ext-link-type="uri">http://purl.bioontology.org/ontology/UATC/</ext-link>&#x0201d;)) BIND(URI(CONCAT(&#x0201c;<ext-link xlink:href="http://bio2rdf.org/atc" ext-link-type="uri">http://bio2rdf.org/atc</ext-link>:,&#x0201d; REPLACE(STR(?uatcUri), &#x0201c;<ext-link xlink:href="http://purl.bioontology.org/ontology/UATC/" ext-link-type="uri">http://purl.bioontology.org/ontology/UATC/</ext-link>&#x0201d;, &#x0201c;&#x0201c;))) AS ?atcUri)
}
</preformat></disp-quote></p><p>As a starting query, we wanted to combine all three databases. Therefore, this federated SPARQL query was added to the settings file to serve as starting query. In this case, the databases are hosted on different local instances of Virtuoso. However, for the navigation with Graph2VR, it would have been easier to put all of them in one server but in different graphs. In the following example PubChem, the local Virtuoso server with the PubChem database is accessible on port 8890, Drugbank on 8891, and the ATC code ontology on port 8896. (We limit the number of results to 10 again, since the number of results would be overwhelming in Graph2VR given that it could even handle so many nodes, and the query would most likely run into a timeout.)
<disp-quote><preformat position="float" xml:space="preserve">PREFIX sio: &#x0003c;<ext-link xlink:href="http://semanticscience.org/resource/" ext-link-type="uri">http://semanticscience.org/resource/</ext-link>&#x0003e;
PREFIX rdf: &#x0003c;<ext-link xlink:href="http://www.w3.org/1999/02/22-rdf-syntax-ns" ext-link-type="uri">http://www.w3.org/1999/02/22-rdf-syntax-ns</ext-link>#&#x0003e;
PREFIX cheminf: &#x0003c;<ext-link xlink:href="http://semanticscience.org/resource/" ext-link-type="uri">http://semanticscience.org/resource/</ext-link>&#x0003e;
PREFIX drugbank: &#x0003c;<ext-link xlink:href="http://bio2rdf.org/drugbank_vocabulary" ext-link-type="uri">http://bio2rdf.org/drugbank_vocabulary</ext-link>:&#x0003e;
PREFIX bio2rdf: &#x0003c;<ext-link xlink:href="http://bio2rdf.org/bio2rdf_vocabulary" ext-link-type="uri">http://bio2rdf.org/bio2rdf_vocabulary</ext-link>:&#x0003e;
PREFIX xsd: &#x0003c;<ext-link xlink:href="http://www.w3.org/2001/XMLSchema" ext-link-type="uri">http://www.w3.org/2001/XMLSchema</ext-link>#&#x0003e;
PREFIX uatc: &#x0003c;<ext-link xlink:href="http://purl.bioontology.org/ontology/UATC/" ext-link-type="uri">http://purl.bioontology.org/ontology/UATC/</ext-link>&#x0003e;
PREFIX owl: &#x0003c;<ext-link xlink:href="http://www.w3.org/2002/07/owl" ext-link-type="uri">http://www.w3.org/2002/07/owl</ext-link>#&#x0003e;
PREFIX rdfs: &#x0003c;<ext-link xlink:href="http://www.w3.org/2000/01/rdf-schema" ext-link-type="uri">http://www.w3.org/2000/01/rdf-schema</ext-link>#&#x0003e;
CONSTRUCT {
?drugbankURI bio2rdf:identifier ?DrugbankID.
?drugbankURI drugbank:x-atc ?atcUri.
?drugbankURI rdfs:label ?label.
?uatcUri owl:sameAs ?atcUri.
?compound sio:SIO_000011 ?depositor_identifier.
?depositor_identifier sio:SIO_000300 ?DrugbankID.
?depositor_identifier2 sio:SIO_000011 ?compound.
?depositor_identifier2 sio:SIO_000300 ?CASString.
}
WHERE {
SERVICE &#x0003c;<ext-link xlink:href="http://localhost:8891/sparql" ext-link-type="uri">http://localhost:8891/sparql</ext-link>&#x0003e; {
?drugbankURI bio2rdf:identifier ?x.
?drugbankURI drugbank:x-atc ?atcUri.
?drugbankURI rdfs:label ?label. BIND(str(?x) AS ?DrugbankID)
FILTER (str(?DrugbankID) = &#x0201c;DB00945&#x0201d;).
}
SERVICE &#x0003c;<ext-link xlink:href="http://localhost:8896/sparql" ext-link-type="uri">http://localhost:8896/sparql</ext-link>&#x0003e; {
?uatcUri owl:sameAs ?atcUri.
}
SERVICE &#x0003c;<ext-link xlink:href="http://localhost:8890/sparql" ext-link-type="uri">http://localhost:8890/sparql</ext-link>&#x0003e; {
?depositor_identifier sio:SIO_000011 ?compound.
?depositor_identifier sio:SIO_000300 ?DrugbankID.
?depositor_identifier rdf:type cheminf:CHEMINF_000406. OPTIONAL {
?depositor_identifier2 sio:SIO_000011 ?compound.
?depositor_identifier2 sio:SIO_000300 ?CASString.
?depositor_identifier2 rdf:type cheminf:CHEMINF_000446.
}
}
}
LIMIT 10
</preformat></disp-quote></p></sec></sec><sec id="s3-s4-s4"><title>Use case 3: Results</title><p>The result of this query is a graph that contains information about aspirin but extracts this information from all three databases (see <xref rid="F8" ref-type="fig">Fig.&#x000a0;8</xref>). In Graph2VR, we can now start to navigate and expand the resulting graph. Depending on which database is required to deliver additional information, it is possible to switch between the databases and to open new connections or start querying. Having the databases on one server would allow us to navigate the graph without having to select the database, even though we could do that by selecting the respective graph. However, as PubChem is so large, queries often time-out or take a long time, so having the databases in separate Virtuoso instances or selecting the respective graph can speed the process up. Once we have that graph in Graph2VR, further exploration can start. The default timeout for Virtuoso is 20&#x02009;s; this can be increased. However, PubChem is so large that queries can take hours, which is not feasible in a VR application, where results should be displayed almost instantly.</p><fig position="float" id="F8" fig-type="figure"><label>Figure&#x000a0;8.</label><caption><p>The graph resulting from the query is based on all three graphs. It connects the entries about the ATC codes of aspirin from Drugbank with the ATC code ontology, connects the Drugbank entry for aspirin to PubChem, and displays its CAS number(s).</p></caption><graphic xlink:href="baaf008f8" position="float"/></fig><p>In <xref rid="F9" ref-type="fig">Fig.&#x000a0;9</xref>, the PubChem parts of the graph have been pinned down on the left side, the ATC code Ontology is pinned to the right, and the Drugbank part in the middle has been expanded with additional knowledge.</p><fig position="float" id="F9" fig-type="figure"><label>Figure&#x000a0;9.</label><caption><p>The graph from the previous query about Aspirin. It has been expanded using data from Drugbank and stretched manually. At left are the CAS numbers from PubChem, in the middle the expanded Drugbank entry, and on the right the ATC code from the third database, demonstrating that this can be used as a starting point in Graph2VR.</p></caption><graphic xlink:href="baaf008f9" position="float"/></fig></sec><sec id="s3-s4-s5"><title>Use case 3: Evaluation</title><p>Use case 3 was difficult to execute for multiple reasons. First, the available query endpoints for PubChem and Drugbank are only offered by third parties, and their data versions are about 10&#x02009;years old. Even if there was an up-to-date endpoint for PubChem, it would probably be too slow for querying due to the high volumes of data. Second, while the RDF resources are up-to-date, they are not trivial to use because they must be imported into a local triple store before querying. Even then, queries must be cleverly constructed to ensure execution in a reasonable time-frame (seconds to minutes rather than hours to days). Third, even if we manage to get relevant query results into Graph2VR, the user might still be confused about some node and edge values, such as &#x0201c;SIO_000300&#x0201d; and &#x0201c;CHEMINF_00446.&#x0201d; Those CUIs are not very intuitive because most people will not immediately know the meaning of the machine-readable CUIs employed by the different databases. To replace these with more human-friendly labels like &#x0201c;has value&#x0201d; and &#x0201c;has CAS Registry Number,&#x0201d; we must import additional ontologies, such as SIO and CHEMINF [<xref rid="R43" ref-type="bibr">43</xref>, <xref rid="R44" ref-type="bibr">44</xref>]. After we dealt with these issues, Graph2VR was able to effectively navigate, expand, and visualize these data. In particular, Graph2VR&#x02019;s ability to expand the graph using multiple different endpoints was instrumental in implementing this use case.</p></sec></sec></sec><sec id="s4"><title>Conclusion</title><p>From the three use cases, we learned several lessons:</p><list list-type="bullet"><list-item><p>Complex data that features many layers and dimensions benefits greatly from the use of VR, as compared to traditional 2D visualizations, because VR offers endless space to order the data and fast navigation tools such as teleportation.</p></list-item><list-item><p>Expanding and exploring a graph in Graph2VR can be much faster than writing SPARQL queries. The relevant predicates can often be identified simply by looking at the predicate&#x02019;s label in the menu in Graph2VR.</p></list-item><list-item><p>Often, multiple different endpoints are used when exploring data, which is something Graph2VR can do, in contrast to most comparable tools.</p></list-item><list-item><p>Nodes and edges often contain only machine-readable labels that must first be replaced with additional ontologies before a human can make intuitive sense of the graphs.</p></list-item><list-item><p>Very large data sources can be problematic, regardless of remote or local query execution. To make use of them, optimized imports and smart queries must be designed first.</p></list-item></list><p>Based on these initial findings and the mainstreaming of VR headsets, we believe that it holds much potential for research. However, there are many opportunities and challenges ahead, which we discuss below.</p></sec><sec id="s5"><title>Discussion</title><p>In this section, we first delineate the pressing challenges we faced during this pilot study and then discuss opportunities for future development.</p><sec id="s5-s1"><title>Scalability issues limit direct use of big Linked Data</title><p>In the last use case, we faced major challenges related to the size of the data and the limitations of Semantic Web technology and VR rendering. What we learned from the Drugbank use case is that SPARQL Endpoints need to respond relatively fast for a seamless Graph2VR experience. Timeouts cannot be indicated in Graph2VR, so the user does not immediately know whether a delay indicates a timeout, a bug, or a connection issue. In addition, very large graphs reduce the framerate of the headset. We found that we reached the limit when rendering 5000 nodes on a GTX 1060 or 17&#x02009;000 nodes on the RTX 4090 with 14 fps during layouting. An example of such a large graph including images is shown in <xref rid="F10" ref-type="fig">Fig.&#x000a0;10</xref>.</p><fig position="float" id="F10" fig-type="figure"><label>Figure&#x000a0;10.</label><caption><p>Example of a large graph demonstrating the rendering capabilities of Graph2VR on an RTX 4090. It shows about 17&#x02009;000 nodes handled by Graph2VR. A framerate drop to 14 fps was observed during the layout phase (the fps increase significantly afterwards). Dataset: DBpedia.</p></caption><graphic xlink:href="baaf008f10" position="float"/></fig></sec><sec id="s5-s2"><title>Disconnect between user mental concept and graph structure</title><p>In addition to scaling issues, we noticed that it often took users quite some time to internalise the Semantic Web structure. In these cases, the concepts and relationships that the user expected to see did not match the knowledge graph provided. These could be small issues, such as labels being different than expected, or bigger issues where relationships that users expected to be direct actually required intermediate steps in the knowledge graph. For example, in Use case 1, the user(s) expected variables to be directly linked to cohort studies. In actual fact, however, they are grouped either by datasets, collection events, or subpopulations. Therefore, in Use case 2, we had to invest time in creating a small application ontology to make sure the data were projected in a knowledge graph that suited the scenarios. An alternative solution might be tooling to help users understand the knowledge graph, as was done in Relfinder (no longer accessible since the end-of-life of Adobe Flashplayer), which enabled users to easily find a path given a starting and ending node.</p></sec><sec id="s5-s3"><title>Representativeness of the use cases</title><p>A huge topic of debate between the study participants was whether our initial findings could be generalized to other life science use cases. We selected our use cases to provide diversity in complexity and scale and in the level of refined semantic annotation and to range from molecular detail to high-level general concepts. Additionally, we have done a short survey of the available literature on the use of Linked Data and/or VR technology in life sciences. For example, when analysing Bioportal categories, the ontologies cover health, organisms, different levels of molecular data, and imaging [<xref rid="R45" ref-type="bibr">45</xref>]. In addition, we explored emerging uses of VR via a PubMed search for &#x0201c;virtual reality&#x0201d; (over 20&#x02009;000 hits). When analyzing this for research applications, we observed an over-representation of studies involving &#x0201c;surgical,&#x0201d; &#x0201c;molecular,&#x0201d; &#x0201c;anatomical,&#x0201d; and &#x0201c;imaging&#x0201d; analysis applications. We speculate that our use cases are representative of many of the ontologies listed. However, we are curious how Graph2VR performs when applied to very specific use cases that we did not cover, such as microscopy and imaging. Therefore, we hope to achieve future funding to run a new experiment to assess the tool in a more systematic way, choosing the users that do the evaluation with some specific background and identifying the required knowledge that the users must have to run those experiments and creating a proper evaluation of the performance of the users in those experiments when doing specific tasks.</p></sec><sec id="s5-s4"><title>Envisioning future visual enhancements</title><p>During the use case experiments, there were many instances where we discussed the potential added value of visual cues to enhance the experience. In the current Graph2VR, we use a standardized panel of visualizations for the nodes and edges. While we think this generality (i.e. not hard-coding shapes and colours based on assumptions on the RDF representation) is the power of Graph2VR, there are ideas about how to enable user annotation to influence the rendering. In our previous Graph2VR paper, we discussed additional options for the use of colors, shapes, and layouts [<xref rid="R1" ref-type="bibr">1</xref>]. We believe a first layer of enhancements would be to allow the user to add a knowledge graph that details how different elements should be visualized. For example, based on class or property differences, the user could specify what colors could be applied or even what shapes could be used, e.g. to find patterns in the data quickly.</p><p>For the long-term future, we dream of mixing the knowledge graph with 3D representations of real-world artifacts. For example, users could be enabled to create a view in Graph2VR that shows a map of the brain with Linked Data about the phenotypes, diseases and genes/variants known to be associated with that part of the brain. Similar projects (without interactive SPARQL capabilities) already exist [<xref rid="R46" ref-type="bibr">46</xref>].</p></sec></sec><sec id="s6"><title>Future work</title><p>While we believe Graph2VR has been a good tool to explore the potential of combining VR and the Semantic Web for scientific use cases, we encountered many areas in which we believe it can be improved. A public list of feature requests is available at <ext-link xlink:href="https://github.com/molgenis/Graph2VR/issues" ext-link-type="uri">https://github.com/molgenis/Graph2VR/issues</ext-link>. Below, we summarize the most important suggestions for the future, hoping for community improvement of this open-source tool.</p><sec id="s6-s1"><title>Improving the search function</title><p>The search function in Graph2VR is currently rather limited. Because we could not make assumptions about the availability of optimized search indexes available in the source, we based the search on either a regex search or, if available, a bif:contains full-text search [<xref rid="R47" ref-type="bibr">47</xref>]. The snippets below show the regex query and bif:contains query, respectively. (Please note that this is an expert from Graph2VR&#x02019;s C# code for generating a SPARQL query, not a valid SPARQL query):
<disp-quote><preformat position="float" xml:space="preserve">select distinct {variableNode.GetQueryLabel()} AS ?uri (SAMPLE(?name) AS ?name) where {{
{variableNode.graph.GetTriplesString()}
{variableNode.GetQueryLabel()} rdfs:label ?name.
?uri(&#x002c6;(&#x0003c;&#x0003e;| !&#x0003c;&#x0003e;) | rdfs:label | skos:altLabel) ?entity. BIND(STR(?entity) AS ?name).
FILTER REGEX(?name, &#x02018;{searchTerm}&#x02019;, &#x02018;i&#x02019;).
}}
LIMIT {searchResultsLimit}&#x0201d;;
select distinct {variableNode.GetQueryLabel()} AS ?uri (SAMPLE(?name) AS ?name) where {{
{variableNode.graph.GetTriplesString()}
{variableNode.GetQueryLabel()} rdfs:label ?name.
?name bif:contains &#x0201c;&#x0201c;&#x02018;{AddStar(searchTerm)}&#x02019;&#x0201c;&#x0201c;.
}}
LIMIT {searchResultsLimit}&#x0201d;;
</preformat></disp-quote></p><p>For the search function, we adapted SPARQL queries from a paper on &#x0201c;Efficient and Effective SPARQL Autocompletion on Very Large Knowledge Graphs&#x0201d; [<xref rid="R48" ref-type="bibr">48</xref>]. However, the results of this search function often do not match the intended search, e.g. when searching on DBpedia for &#x0201c;Donald,&#x0201d; one would expect results like &#x0201c;Donald Duck,&#x0201d; but the first result we got was &#x0201c;Amy McDonald.&#x0201d; This result is factually not wrong, but there are often many such irrelevant results. We are confident that there are more effective ways to search, and a better search function would make navigating the database much more straightforward. We did not invest much time in implementing a better search functionality, even though the application would clearly benefit. An instance of advanced search functionality for Linked Data is demonstrated by the commercial tool Metaphactory [<xref rid="R49" ref-type="bibr">49</xref>]. The fact that Graph2VR also operates on external databases makes it more challenging to implement a good search because we cannot simply upload the whole database to a local SOLR or Elastic search server, but rather have to limit ourselves to the options that SPARQL queries offer [<xref rid="R50" ref-type="bibr">50</xref>, <xref rid="R51" ref-type="bibr">51</xref>].</p></sec><sec id="s6-s2"><title>Improving query options</title><p>In the current release of Graph2VR, the user has several options for building queries and interacting with the data. Users can expand, collapse, and build queries; add nodes and edges; turn nodes into variables; and define optional triples, limits, and ordering. However, not all SPARQL commands have been implemented, even though the DotNetRDF engine Graph2VR uses would support them and the graphs could be displayed. We would like the GUI to expose more of these functions, especially nested queries, filters, offsets, and recursive requests, e.g.
to iterate opening the rdfs:subClassOf predicates upwards to owl:thing. For example, for Use case 3, we had to create a starter query that used a nested query to combine data from the three data sources using the bind option to align mismatched ATC code URIs (PubChem, ATC, Drugbank). Ideally, we would like to enable the user to perform this interactively through the user interface.</p></sec><sec id="s6-s3"><title>Multi-player VR</title><p>During the use case evaluation, only one participant at a time could experience VR. This forced us to evaluate the added value of VR versus the 2D representation. The collaborators on the use cases took turns being the VR controller. While this worked surprisingly well, it greatly limited the potential for interaction between participants. We all experienced that being submerged in VR makes it easier and more natural to work with and navigate large/multiple graphs as the screen feels 360 degrees instead of only a small square window. An obvious future enhancement would thus be enabling multiple users to work in the same VR. We believe that adding other VR headsets as observers would be relatively easy; however, having multiple actors with controls and interaction at the same time might require a fundamental rewrite of the software.</p></sec><sec id="s6-s4"><title>Augmented Reality</title><p>In the current version of Graph2VR, the user is in an endless world with a small platform that is a visual anchor in the virtual environment. It would be interesting to project the graph in the real world instead, especially in combination with a multiplayer feature where multiple people can interact with the graph(s) together. There are already similar projects with the Microsoft Hololens, but as far as we have seen, these are more for visualization of network graphs and 3D models and less for interaction with Linked Data [<xref rid="R46" ref-type="bibr">46</xref>]. That tool actually provides gesture and voice controls.</p></sec><sec id="s6-s5"><title>Safety and Security</title><p>While both critical in software systems, safety and security address different aspects of system integrity. Safety ensures the system&#x02019;s reliability and correct function, whereas security protects against external threats and unauthorized access.</p><p>Safety: We know that Graph2VR can and probably will crash when the GPU is overwhelmed with too many nodes. When the application crashes, we might like to continue from a restored session from shortly before it crashed. Therefore, an auto-save function, e.g. saved just before sending a query, could be used to prevent data loss. Data loss in this case means only loss of temporary graph(s) in Graph2VR, as Graph2VR does not write anything back to the database. Security: In terms of security, Graph2VR needs some improvement. It can send SPARQL queries to SPARQL Endpoints, but it does not yet support using a login (username and password). Therefore it requires the database to be open. For linked open data like DBpedia or any public SPARQL endpoints, this is not an issue. But when a Virtuoso Server is set up locally, its SPARQL Endpoint is, by default, accessible to anyone on the local network. When working with sensitive data, a tool that does not support login is problematic. We dealt with this by using a distinct, separate network to which only the relevant devices had access, rather than a shared network.</p></sec><sec id="s6-s6"><title>Enable combining endpoints</title><p>Currently, Graph2VR users cannot create a query graph that involves multiple RDF sources at once (e.g. combining PubChem, ATC, and Drugbank), while the users intuitively expect to be able to do so. Therefore, to be able to use different endpoints, it would be a useful feature to see the overlap between the nodes that are present in the graph and those that are also part of the second database. The necessity to preidentify nodes present in two separate data sources, or the process of trial and error, significantly limits usability, especially in a tool meant to be used for exploration. It might be helpful if it would display nodes that are not present in the current Endpoint as transparent, so that users can immediately see where connection points are to be expected. This might be quite extensive and lead to the necessity to request the presence of each node in the other database. For sensitive datasets like that in the VIP use case, this would be unwanted.</p></sec><sec id="s6-s7"><title>SPARQL query logging</title><p>While the Graph2VR GUI does much of the heavy lifting in querying, there are instances where it would be useful for the user to be able to see the SPARQL query that has actually been executed. Such a feature would aid not only in debugging but also in verifying queries and ensuring documentation and reproducibility. It could also be useful to be able to display the current query within the application to see what the selected patterns and options translated to. Exporting and storing SPARQL queries created in Graph2VR should be easy to implement, but this has not yet been done.</p></sec><sec id="s6-s8"><title>Shivering graph</title><p>Graph2VR tends to be intuitive for the exploration of graph databases. However, one issue that we faced regularly was that the graph often began to increasingly &#x0201c;shiver&#x0201d; or jitter over time while it was grabbed. The reason for this seems to be the angle between the controllers, the center of the graph, and a reference point. The center is recalculated regularly, as is the reference point. The further these three points are from each other, the more the graph begins to shiver. The reference point is also affected by the layout algorithms. This might be a floating point/rounding error and is something that should be improved upon.</p></sec><sec id="s6-s9"><title>Preserving save state</title><p>During our improvements of Graph2VR, we often created new versions of the tool. To install an update on the headset, the old version of Graph2VR has to be removed. Uninstalling the application also removes its save states due to the way the Quest handles uninstalling applications. We therefore recommend that users manually back up the save states before removing Graph2VR from the headset. This backup can be performed using Sidequest.</p></sec><sec id="s6-s10"><title>Compatibility of Graph2VR with different VR headsets</title><p>Graph2VR was designed to be compatible with the Vive and Quest2/Quest3 headsets. The OpenXR toolkit that we used to bind the controls checks for the headset to set the bindings and adjusts the angle of the laser [<xref rid="R52" ref-type="bibr">52</xref>]. Currently, there is no specific support for additional headsets. If we had used SteamVR instead of OpenXR, many headsets would have been supported out of the box. We switched to OpenXR mainly because it allowed us to build a stand-alone version of the application on the headset.</p></sec><sec id="s6-s11"><title>Compatibility of Graph2VR with different graph databases</title><p>Graph2VR was tested with Virtuoso servers like DBpedia and local Virtuoso servers. Initially, we thought, DotNetRDF would also support other databases like GraphDB, Allegro Graph, and more [<xref rid="R53" ref-type="bibr">53</xref>]. In principle, it does, but it seems to use different functions to talk to the different Endpoints. Currently, we have implemented only one of them. However, it should not be too difficult to add support for additional types of databases.</p></sec><sec id="s6-s12"><title>Improved colors</title><p>In terms of color, Graph2VR largely follows the VOWL schema, although it has not been fully implemented and adjustments were necessary for the 3D representation [<xref rid="R54" ref-type="bibr">54</xref>]. Whether this color scheme is the best choice remains a point of discussion. Other tools such as Gruff or GraphDB, use different colors to code for different types of nodes [<xref rid="R31" ref-type="bibr">31</xref>]. This approach allows easy visual distinction of different types of nodes, which can be useful, as in <xref rid="F5 F6" ref-type="fig">Figs&#x000a0;5, 6</xref>, and <xref rid="F7" ref-type="fig">7</xref>. It is already possible to adjust the colours in the settings file, but this could be made even more flexible by specifying the predicate or type and the color the entities should have.</p></sec><sec id="s6-s13"><title>Layouts</title><p>One major advantage of VR over 2D layouts is that it is much harder to run out of space. Graph2VR already offers four different layout algorithms, but there could be more. Those could use other criteria, predicates, and algorithms to restructure the data. During the user evaluation, some users specifically asked for a layout that would situate the data around the user, e.g. projecting the data on the inside of a large sphere or the ability to enter a subgraph like in the semantic bubble chart [<xref rid="R55" ref-type="bibr">55</xref>].</p></sec><sec id="s6-s14"><title>Combining Graphs</title><p>In some situations, it would be helpful to merge different graphs in Graph2VR. We have already added this as a feature for queries, i.e. to either have a stack of result graphs or to combine them into a single graph. However, it would be useful to be able to do this with two independent graphs already present in Graph2VR. Users would expect that, once they add an edge, they could visually merge the two graphs and handle them as one, and that layout algorithms could be applied to that merged graph. This involves a design choice how to handle nodes with the same URI present in the two graphs while merging: (I) Merging all nodes with the same URI to one node, or (II) keeping them separate and allowing duplicates in a graph, which could interfere with other parts of the codes like expanding the graph.</p></sec><sec id="s6-s15"><title>Bridging points between different endpoints</title><p>When working with multiple SPARQL Endpoints in the same Graph2VR session, it might be useful to be able to see which of the visible nodes are also present in the other endpoint. Upon switching databases, there is no immediate way to determine which of the present nodes exist in the new database,
i.e. which ones can potentially serve as connectors for further expansion and exploration. The idea of automatically verifying which nodes are present in another SPARQL Endpoint is appealing. Nodes that are not present in the server might be displayed as semitransparent. We decided not to do this automatically because the user might not want to share that data, for various reasons, like having a slow database or not wanting to send too many queries or sensitive information. It should be up to the user to determine whether to share the current graph&#x02019;s content for finding the overlapping nodes.</p></sec></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>baaf008_Supp</label><media xlink:href="baaf008_supp.zip"/></supplementary-material></sec></body><back><ack id="ack1"><title>Acknowledgements</title><p>We acknowledge the contributions from all cohorts and research communities, which spend vast amounts of time submitting metadata and providing feedback on the catalogue. Finally, we thank Kate McIntyre for her valuable editorial input during the writing of the manuscript and for editing this manuscript.</p></ack><sec id="s7"><title>Author contributions</title><p>A.J.K., M.A.S., K.J.V.D.V., and S.V.D.H. drafted the manuscript with contributions from the co-authors and performed the exploration of the data with Graph2VR. During this study, we asked the developers of the Graph2VR software M.P. and A.J.K. to make small adaptations. For use case 1 &#x0201c;catalogue data,&#x0201d; E.J.V.E. translated data for this use case into Linked Data; M.A.S., E.J.V.E., K.J.V.D.V., and the Molgenis team created an export function for EMX2 in RDF format; and A.J.K., M.A.S., B.S.H., M.A.V.D.G., and E.J.V.E. worked on the exploration and the use case for Graph2VR for the Molgenis Catalogue data. For use case 2 &#x0201c;Variant Interpretation Pipeline&#x0201d;: S.V.D.H. translated data for this use case into RDF and created the GraphDB visualizations; A.J.K., M.A.S., S.V.D.H., K.J.V.D.V, and W.T.K.M. worked on the exploration and the use case for Graph2VR for the VIP pipeline data; and W.T.K.M. explained the basics of the VIP Pipeline. For use case 3, &#x0201c;Drugbank &#x00026; PubChem,&#x0201d; A.J.K. translated data for this use case into Linked Data and worked on the exploration and the use case for Graph2VR for the VIP pipeline data.</p></sec><sec id="s8"><title>Supplementary data</title><p>
<xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> is available at <italic toggle="yes">Database</italic> online.</p></sec><sec sec-type="COI-statement" id="s9"><title>Conflict of interest:</title><p>The authors declare no conflict of interest.</p></sec><sec id="s10"><title>Funding</title><p>This project has received funding from EUCAN Connect (EC H2020 Grant Agreement No 824989).</p></sec><sec sec-type="data-availability" id="s11"><title>Data and software availability</title><p>This article is based on Graph2VR software version 1.2.6, see GitHub for version history <ext-link xlink:href="https://github.com/molgenis/Graph2VR" ext-link-type="uri">https://github. com/molgenis/Graph2VR</ext-link> [<xref rid="R1" ref-type="bibr">1</xref>] The usage of Graph2VR is explained in the user manual [<xref rid="R10" ref-type="bibr">10</xref>]. The software/source code of Graph2VR is open source under the permissive open-source license LGPLv3 and includes thirdparty software that is available under its own license) We welcome contributions and suggestions for improvement at Github: <ext-link xlink:href="https://github.com/molgenis/Graph2VR/issues" ext-link-type="uri">https://github.com/molgenis/Graph2VR/issues</ext-link> A Graph2VR tutorial playlist is available on YouTube [<xref rid="R56" ref-type="bibr">56</xref>]: <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link></p><p>The data used in this article are available online.</p><list list-type="bullet"><list-item><p>For use case 1, we added a snapshot of the Molgenis catalogue data as it was when we used it, see <xref rid="s8" ref-type="sec">supplementary file</xref> catalogue-demo.rdf.</p></list-item><list-item><p>For use case 2, all data and scripts are available at <ext-link xlink:href="https://zenodo.org/doi/10.5281/zenodo" ext-link-type="uri">https://zenodo.org/doi/10.5281/zenodo</ext-link>. 10&#x02009;848&#x02009;352.</p></list-item><list-item><p>For use case 3, the ATC code ontology is available via <ext-link xlink:href="https://github.com/AJKellmann/Scripts" ext-link-type="uri">https://github.com/AJKellmann/Scripts</ext-link>for-the-translation-of-medicine-usage-data-to-ATC-codes/tree/main. The PubChem and Drugbank data can be obtained from Drugbank <ext-link xlink:href="https://go.drugbank.com" ext-link-type="uri">https://go.drugbank.com</ext-link> and PubChem <ext-link xlink:href="https://ftp.ncbi.nlm.nih.gov/pubchem/RDF/" ext-link-type="uri">https://ftp.ncbi.nlm.nih.gov/pubchem/RDF/</ext-link> directly.</p></list-item></list></sec><ref-list id="ref1"><title>References</title><ref id="R1"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kellmann</surname> &#x000a0;<given-names>AJ</given-names></string-name>, <string-name><surname>Postema</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>de Keijser</surname> &#x000a0;<given-names>J</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Visualisation and exploration of linked data using virtual reality</article-title>. <source><italic toggle="yes">Database</italic></source> 2024;<volume>2024</volume>: <page-range>baae008</page-range>. doi: <pub-id pub-id-type="doi">10.1093/database/baae008</pub-id></mixed-citation></ref><ref id="R2"><label>2.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>W3C</collab>
</person-group>. <article-title>SPARQL Query Language for RDF</article-title>. <year>2013</year>. <ext-link xlink:href="https://www.w3.org/TR/rdf-sparql-query/" ext-link-type="uri">https://www.w3.org/TR/rdf-sparql-query/</ext-link> &#x000a0;<comment>(25 April 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R3"><label>3.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>EUCANConnect Catalogue</collab>
</person-group>. <year>2023</year>. <ext-link xlink:href="https://catalogue.eucanconnect.eu/#/" ext-link-type="uri">https://catalogue.eucanconnect.eu/#/</ext-link> (<comment>17 August 2023, date last accessed</comment>).</mixed-citation></ref><ref id="R4"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Swertz</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>van Enckevort</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Oliveira</surname> &#x000a0;<given-names>JL</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Towards an interoperable ecosystem of research cohort and real&#x02010;world data catalogues enabling multi&#x02010;center studies</article-title>. <source><italic toggle="yes">Yearbook Med Inform</italic></source> &#x000a0;<year>2022</year>;<volume>31</volume>:<fpage>262</fpage>&#x02013;<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.1055/s-0042-1742522</pub-id></mixed-citation></ref><ref id="R5"><label>5.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Maassen</surname> &#x000a0;<given-names>WTK</given-names></string-name>, <string-name><surname>Johansson</surname> &#x000a0;<given-names>LF</given-names></string-name>, <string-name><surname>Charbon</surname> &#x000a0;<given-names>B</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>MOLGENIS VIP: an opensource and modular pipeline for highthroughput and integrated DNA variant analysis</article-title>. <year>2024</year>.</mixed-citation></ref><ref id="R6"><label>6.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>DrugBank Online</collab>
</person-group>. <article-title>Database for Drug and Drug Target Info</article-title>. <year>2024</year>. <ext-link xlink:href="https://go.drugbank.com/" ext-link-type="uri">https://go.drugbank.com/</ext-link> &#x000a0;<comment>(19 February 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R7"><label>7.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>PubChemRDF</collab>
</person-group>. <year>2023</year>. <ext-link xlink:href="https://pubchem.ncbi.nlm.nih.gov/docs/rdf" ext-link-type="uri">https://pubchem.ncbi.nlm.nih.gov/docs/rdf</ext-link> &#x000a0;<comment>(19 October 2023, date last accessed</comment>).</mixed-citation></ref><ref id="R8"><label>8.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Kellmann</surname> &#x000a0;<given-names>AJ</given-names></string-name>, <string-name><surname>Postema</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>de Keijser</surname> &#x000a0;<given-names>J</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Graph2VR Github Repository: Visualising/Exploring Linked Data (SPARQL Construct Queries) in Virtual Reality</article-title>. <year>2023</year>. <ext-link xlink:href="https://github.com/molgenis/Graph2VR" ext-link-type="uri">https://github.com/molgenis/Graph2VR</ext-link> &#x000a0;<comment>(6 February 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R9"><label>9.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>tenforce/virtuoso</collab>
</person-group>. <article-title>Docker Image</article-title>. <ext-link xlink:href="https://hub.docker.com/r/tenforce/virtuoso" ext-link-type="uri">https://hub.docker.com/r/tenforce/virtuoso</ext-link> &#x000a0;<comment>(19 October 2023, date last accessed</comment>).</mixed-citation></ref><ref id="R10"><label>10.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Kellmann</surname> &#x000a0;<given-names>AJ</given-names></string-name>, <string-name><surname>Postema</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>de Keijser</surname> &#x000a0;<given-names>J</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Graph2VR Manual</article-title>. <source><italic toggle="yes">Zenodo</italic></source> &#x000a0;<year>2023</year>.</mixed-citation></ref><ref id="R11"><label>11.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Sidequest</collab>
</person-group>. <year>2023</year>. <ext-link xlink:href="https://sidequestvr.com/" ext-link-type="uri">https://sidequestvr.com/</ext-link> (<comment>29 January 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R12"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fortier</surname> &#x000a0;<given-names>I</given-names></string-name>, <string-name><surname>Raina</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>van den Heuvel</surname> &#x000a0;<given-names>ER</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Maelstrom research guidelines for rigorous retrospective data harmonization</article-title>. <source><italic toggle="yes">Int J Epidemiol</italic></source> &#x000a0;<year>2017</year>;<volume>46</volume>:<fpage>103</fpage>&#x02013;<lpage>05</lpage>. doi: <pub-id pub-id-type="doi">10.1093/ije/dyw075</pub-id><pub-id pub-id-type="pmid">27272186</pub-id>
</mixed-citation></ref><ref id="R13"><label>13.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>EUCAN Connect</collab>
</person-group>. <year>2023</year>. <ext-link xlink:href="https://eucanconnect.com/" ext-link-type="uri">https://eucanconnect.com/</ext-link> &#x000a0;<comment>(11 August 2023, date last accessed</comment>).</mixed-citation></ref><ref id="R14"><label>14.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pinot de Moira</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Haakma</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Strandberg-Larsen</surname> &#x000a0;<given-names>K</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The EU Child Cohort Network&#x02019;s core data: establishing a set of findable, accessible, interoperable and reusable (FAIR) variables</article-title>. <source><italic toggle="yes">Eur J Epidemiol</italic></source> &#x000a0;<year>2021</year>;<volume>36</volume>:<fpage>565</fpage>&#x02013;<lpage>80</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s10654-021-00733-9</pub-id><pub-id pub-id-type="pmid">33884544</pub-id>
</mixed-citation></ref><ref id="R15"><label>15.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>The European Human Exposome Network (EHEN)</collab>
</person-group>. <article-title>The World&#x02019;s Largest Project Network Studying the Impact of Environmental Exposure on Health</article-title>. <year>2023</year>. <ext-link xlink:href="https://www.humanexposome.eu/" ext-link-type="uri">https://www.humanexposome.eu/</ext-link> &#x000a0;<comment>(22 May 2023, date last accessed</comment>).</mixed-citation></ref><ref id="R16"><label>16.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Thurin</surname> &#x000a0;<given-names>NH</given-names></string-name>, <string-name><surname>Pajouheshnia</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Roberto</surname> &#x000a0;<given-names>G</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>From inception to conception: Genesis of a network to support better monitoring and communication of medication safety during pregnancy and breastfeeding</article-title>. <source><italic toggle="yes">Clin Pharmacol Ther</italic></source> &#x000a0;<year>2022</year>;<volume>111</volume>:<fpage>321</fpage>&#x02013;<lpage>31</lpage>. doi: <pub-id pub-id-type="doi">10.1002/cpt.2476</pub-id><pub-id pub-id-type="pmid">34826340</pub-id>
</mixed-citation></ref><ref id="R17"><label>17.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>NDDAPP</collab>
</person-group>. <article-title>Online JSON Encoder</article-title>. <year>2024</year>. <ext-link xlink:href="https://nddapp.com/json-encoder.html" ext-link-type="uri">https://nddapp.com/json-encoder.html</ext-link> &#x000a0;<comment>(29 January 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R18"><label>18.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Musen</surname> &#x000a0;<given-names>MA</given-names></string-name>
</person-group>. <article-title>The Prot&#x000e9;g&#x000e9; Project: a look back and a look forward</article-title>. <source><italic toggle="yes">AI Matters</italic></source> &#x000a0;<year>2015</year>;<volume>1</volume>:<fpage>4</fpage>&#x02013;<lpage>12</lpage>. doi: <pub-id pub-id-type="doi">10.1145/2757001.2757003</pub-id><pub-id pub-id-type="pmid">27239556</pub-id>
</mixed-citation></ref><ref id="R19"><label>19.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Li</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>van der Velde</surname> &#x000a0;<given-names>KJ</given-names></string-name>, <string-name><surname>de Ridder</surname> &#x000a0;<given-names>D</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>CAPICE: a computational method for consequenceagnostic pathogenicity interpretation of clinical exome variations</article-title>. <source><italic toggle="yes">Genome Med</italic></source> &#x000a0;<year>2020</year>;<volume>12</volume>:<page-range>75</page-range>. doi: <pub-id pub-id-type="doi">10.1186/s13073-020-00775-w</pub-id></mixed-citation></ref><ref id="R20"><label>20.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zurek</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Ellwanger</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Vissers</surname> &#x000a0;<given-names>LELM</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>SolveRD: systematic panEuropean data sharing and collaborative analysis to solve rare diseases</article-title>. <source><italic toggle="yes">Eur J Hum Genet</italic></source> &#x000a0;<year>2021</year>;<volume>29</volume>:<fpage>1325</fpage>&#x02013;<lpage>31</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41431-021-00859-0</pub-id><pub-id pub-id-type="pmid">34075208</pub-id>
</mixed-citation></ref><ref id="R21"><label>21.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Trujillano</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>BertoliAvella</surname> &#x000a0;<given-names>AM</given-names></string-name>, <string-name><surname>Kumar</surname> &#x000a0;<given-names>K</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Clinical exome sequencing: results from 2819 samples reflecting 1000 families</article-title>. <source><italic toggle="yes">Eur J Hum Genet</italic></source> &#x000a0;<year>2016</year>;<volume>25</volume>:<fpage>176</fpage>&#x02013;<lpage>82</lpage>. doi: <pub-id pub-id-type="doi">10.1038/ejhg.2016.146</pub-id><pub-id pub-id-type="pmid">27848944</pub-id>
</mixed-citation></ref><ref id="R22"><label>22.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Genome of the Netherlands</collab>
</person-group>. <year>2024</year>. <ext-link xlink:href="https://www.nlgenome.nl/" ext-link-type="uri">https://www.nlgenome.nl/</ext-link> &#x000a0;<comment>(25 January 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R23"><label>23.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Ensembl Variant Effect Predictor</collab>
</person-group>. <year>2024</year>. <ext-link xlink:href="https://www.ensembl.org/Multi/Tools/VEP" ext-link-type="uri">https://www.ensembl.org/Multi/Tools/VEP</ext-link> (<comment>18 March 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R24"><label>24.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>van den Hoek</surname> &#x000a0;<given-names>S</given-names></string-name>
</person-group>. <article-title>Graph2VR &#x02013; Usecase 2: Data &#x00026; Scripts</article-title>. <year>2024</year>. https://zenodo.org/doi/10.5281/zenodo.10848352 <comment>(21 March 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R25"><label>25.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>vip2rdf</collab>
</person-group>. <year>2024</year>. <ext-link xlink:href="https://github.com/molgenis/vip2rdf/" ext-link-type="uri">https://github.com/molgenis/vip2rdf/</ext-link> (<comment>19 March 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R26"><label>26.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>GitHub</collab>
</person-group>. <article-title>Ensembl/ensemblglossary</article-title>. <year>2023</year>. <ext-link xlink:href="https://github.com/Ensembl/ensemblglossary/" ext-link-type="uri">https://github.com/Ensembl/ensemblglossary/</ext-link> &#x000a0;<comment>(19 October 2023, date last accessed</comment>)</mixed-citation></ref><ref id="R27"><label>27.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Calculated Consequences</collab>
</person-group>. <year>2023</year>. <ext-link xlink:href="http://www.ensembl.org/info/genome/variation/prediction/predicted_data.html" ext-link-type="uri">http://www.ensembl.org/info/genome/variation/prediction/predicted_data.html</ext-link> &#x000a0;<comment>(19 October 2023, date last accessed</comment>).</mixed-citation></ref><ref id="R28"><label>28.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Human Phenotype Ontology</collab>
</person-group>. <year>2023</year>. <ext-link xlink:href="https://github.com/obophenotype/human-phenotype-ontology/releases/latest/download/hp.obo" ext-link-type="uri">https://github.com/obophenotype/human-phenotype-ontology/releases/latest/download/hp.obo</ext-link> &#x000a0;<comment>(09 October 2023, date last accessed</comment>).</mixed-citation></ref><ref id="R29"><label>29.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>GraphDB Downloads and Resources</collab>
</person-group>. <year>2020</year>. <ext-link xlink:href="https://graphdb.ontotext.com/" ext-link-type="uri">https://graphdb.ontotext.com/</ext-link> &#x000a0;<comment>(05 March 2021, date last accessed</comment>).</mixed-citation></ref><ref id="R30"><label>30.</label><mixed-citation publication-type="other">Ontotext. <article-title>Visualize and Explore &#x02014; GraphDB 10.6 Documentation</article-title>. <year>2024</year>. <ext-link xlink:href="https://graphdb.ontotext.com/documentation/10.6/visualize-and-explore.html?highlight=visualize#explore-resources" ext-link-type="uri">https://graphdb.ontotext.com/documentation/10.6/visualize-and-explore.html?highlight=visualize#explore-resources</ext-link> &#x000a0;<comment>(18 March 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R31"><label>31.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Franz Inc. Gruff</collab>
</person-group>. <year>2021</year>. <ext-link xlink:href="https://allegrograph.com/products/gruff/" ext-link-type="uri">https://allegrograph.com/products/gruff/</ext-link> &#x000a0;<comment>(25 February 2021, date last accessed</comment>).</mixed-citation></ref><ref id="R32"><label>32.</label><mixed-citation publication-type="other">DisGeNet Team. <article-title>DisGeNET &#x02013; a Database of Gene-Disease Associations</article-title>. <year>2023</year>. <ext-link xlink:href="https://www.disgenet.org/" ext-link-type="uri">https://www.disgenet.org/</ext-link> &#x000a0;<comment>(25 February 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R33"><label>33.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Heim</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>RelFinder</surname> &#x000a0;<given-names>LS</given-names></string-name></person-group>. <article-title>Visual Data Web</article-title>. <year>2021</year>. <ext-link xlink:href="http://www.visualdataweb.org/relfinder.php" ext-link-type="uri">http://www.visualdataweb.org/relfinder.php</ext-link> &#x000a0;<comment>(25 February 2021, date last accessed</comment>).</mixed-citation></ref><ref id="R34"><label>34.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>PubChem</collab>
</person-group>. <year>2024</year>. <ext-link xlink:href="https://pubchem.ncbi.nlm.nih.gov/" ext-link-type="uri">https://pubchem.ncbi.nlm.nih.gov/</ext-link> (<comment>19 February 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R35"><label>35.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wishart</surname> &#x000a0;<given-names>DS</given-names></string-name>, <string-name><surname>Feunang</surname> &#x000a0;<given-names>YD</given-names></string-name>, <string-name><surname>Guo</surname> &#x000a0;<given-names>AC</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>DrugBank 5.0: a major update to the DrugBank database for 2018</article-title>. <source><italic toggle="yes">Nucleic Acids Res</italic></source> &#x000a0;<year>2018</year>;<volume>46</volume>:<fpage>D1074</fpage>&#x02013;<lpage>D1082</lpage>. doi: <pub-id pub-id-type="doi">10.1093/nar/gkx1037</pub-id><pub-id pub-id-type="pmid">29126136</pub-id>
</mixed-citation></ref><ref id="R36"><label>36.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kellmann</surname> &#x000a0;<given-names>AJ</given-names></string-name>, <string-name><surname>Lanting</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Franke</surname> &#x000a0;<given-names>L</given-names></string-name></person-group> &#x000a0;<etal>et al</etal> &#x000a0;<article-title>Semiautomatic translation of medicine usage data (in Dutch, freetext) from lifelines COVID19 questionnaires to ATC codes</article-title>. <source><italic toggle="yes">Database</italic></source> 2023;<volume>2023</volume>:<page-range>baad019</page-range>. doi: <pub-id pub-id-type="doi">10.1093/database/baad019</pub-id></mixed-citation></ref><ref id="R37"><label>37.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mc Intyre</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Lanting</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Deelen</surname> &#x000a0;<given-names>P</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Lifelines COVID19 cohort: investigating COVID19 infection and its health and societal impacts in a Dutch populationbased cohort</article-title>. <source><italic toggle="yes">BMJ Open</italic></source> &#x000a0;<year>2021</year>;<volume>11</volume>:<page-range>e044474</page-range>. doi: <pub-id pub-id-type="doi">10.1136/bmjopen-2020-044474</pub-id></mixed-citation></ref><ref id="R38"><label>38.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fu</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Batchelor</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Dumontier</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>PubChemRDF: Towards the semantic annotation of PubChem compound and substance databases</article-title>. <source><italic toggle="yes">J Cheminform</italic></source> &#x000a0;<year>2015</year>;<volume>7</volume>:<page-range>34</page-range>. doi: <pub-id pub-id-type="doi">10.1186/s13321-015-0084-4</pub-id></mixed-citation></ref><ref id="R39"><label>39.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Callahan</surname> &#x000a0;<given-names>A</given-names></string-name>
<string-name>
<surname>CruzToledo</surname> &#x000a0;<given-names>J</given-names></string-name>
<string-name>
<surname>Ansell</surname> &#x000a0;<given-names>P</given-names></string-name>
</person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<part-title>Bio2RDF Release 2: improved coverage, interoperability and provenance of life science linked data</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Cimiano</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Corcho</surname> &#x000a0;<given-names>O</given-names></string-name>, <string-name><surname>Presutti</surname> &#x000a0;<given-names>V</given-names></string-name></person-group> &#x000a0;<italic toggle="yes">et&#x000a0;al</italic>. (eds), <source><italic toggle="yes">The Semantic Web: Semantics and Big Data</italic></source>. <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2013</year>, <fpage>200</fpage>&#x02013;<lpage>12</lpage>.</mixed-citation></ref><ref id="R40"><label>40.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>GitHub</collab>
</person-group>. <article-title>bio2rdfscripts/drugbank at master &#x000b7; bio2rdf/bio2rdfscripts</article-title>. <year>2023</year>. <ext-link xlink:href="https://github.com/bio2rdf/bio2rdfscripts/tree/master/drugbank" ext-link-type="uri">https://github.com/bio2rdf/bio2rdfscripts/tree/master/drugbank</ext-link> &#x000a0;<comment>(19 October 2023, date last accessed</comment>).</mixed-citation></ref><ref id="R41"><label>41.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>GitHub</collab>
</person-group>. <article-title>Run the Bio2RDF parsers</article-title>. <year>2023</year>. <ext-link xlink:href="https://github.com/bio2rdf/bio2rdfscripts/wiki/RuntheBio2RDFparsers" ext-link-type="uri">https://github.com/bio2rdf/bio2rdfscripts/wiki/RuntheBio2RDFparsers</ext-link> &#x000a0;<comment>(19 October 2023, date last accessed</comment>).</mixed-citation></ref><ref id="R42"><label>42.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>GitHub</collab>
</person-group>. <article-title>RAM consumption much higher than expected</article-title>. <source>Issue #128, openlink/virtuosoopensource</source>. <year>2024</year>. <ext-link xlink:href="https://github.com/openlink/virtuosoopensource/issues/128" ext-link-type="uri">https://github.com/openlink/virtuosoopensource/issues/128</ext-link> &#x000a0;<comment>(01 February 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R43"><label>43.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Semanticscience Integrated Ontology (SIO)</collab>
</person-group>. <year>2023</year>. <ext-link xlink:href="https://www.ebi.ac.uk/ols4/ontologies/sio" ext-link-type="uri">https://www.ebi.ac.uk/ols4/ontologies/sio</ext-link> &#x000a0;<comment>(16 October 2023, date last accessed</comment>).</mixed-citation></ref><ref id="R44"><label>44.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hastings</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Chepelev</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Willighagen</surname> &#x000a0;<given-names>E</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The chemical information ontology: provenance and disambiguation for chemical data on the biological semantic web</article-title>. <source><italic toggle="yes">PLoS One</italic></source> &#x000a0;<year>2011</year>;<volume>6</volume>:<page-range>e25513</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0025513</pub-id></mixed-citation></ref><ref id="R45"><label>45.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kamdar</surname> &#x000a0;<given-names>MR</given-names></string-name>, <string-name><surname>Walk</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Tudorache</surname> &#x000a0;<given-names>T</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Analyzing user interactions with biomedical ontologies: a visual perspective</article-title>. <source><italic toggle="yes">Web Semant</italic></source> &#x000a0;<year>2018</year>;<volume>49</volume>:<fpage>16</fpage>&#x02013;<lpage>30</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.websem.2017.12.002</pub-id><pub-id pub-id-type="pmid">29657560</pub-id>
</mixed-citation></ref><ref id="R46"><label>46.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Panahi</surname> &#x000a0;<given-names>A</given-names></string-name>
</person-group>. <article-title>Big data visualization platform for mixed reality</article-title>. <source>Ph.D. Thesis,</source> &#x000a0;<publisher-name>VCU Libraries</publisher-name>, <year>2017</year>.</mixed-citation></ref><ref id="R47"><label>47.</label><mixed-citation publication-type="other">
<article-title>16.3.1. Using full text search in SPARQL</article-title>. <year>2022</year>. <ext-link xlink:href="https://docs.openlinksw.com/virtuoso/rdfsparqlrulefulltext/" ext-link-type="uri">https://docs.openlinksw.com/virtuoso/rdfsparqlrulefulltext/</ext-link> &#x000a0;<comment>(02 December 2022, date last accessed</comment>).</mixed-citation></ref><ref id="R48"><label>48.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bast</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>Kalmbach</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Klumpp</surname> &#x000a0;<given-names>T</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Efficient and effective SPARQL autocompletion on very large knowledge graphs</article-title>. In: (eds), <italic toggle="yes">Proceedings of the 31st ACM International Conference on Information &#x00026; Knowledge Management</italic>, Atlanta GA USA, <volume>2022</volume>. <conf-date>October 17 - 21, 2022</conf-date>, <fpage>2893</fpage>&#x02013;<lpage>902</lpage>, 2022. doi: <pub-id pub-id-type="doi">10.1145/3511808.3557093</pub-id>.</mixed-citation></ref><ref id="R49"><label>49.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Metaphacts</collab>
</person-group>. <article-title>Keyword Search Component [metaphactory]</article-title>. <year>2022</year>. <ext-link xlink:href="https://help.metaphacts.com/resource/Help:KeywordSearch" ext-link-type="uri">https://help.metaphacts.com/resource/Help:KeywordSearch</ext-link> &#x000a0;<comment>(13 December 2022, date last accessed</comment>).</mixed-citation></ref><ref id="R50"><label>50.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Shahi</surname> &#x000a0;<given-names>D</given-names></string-name>
</person-group>. <source><italic toggle="yes">Apache Solr</italic></source>. <publisher-loc>Berkeley, CA</publisher-loc>: <publisher-name>Apress</publisher-name>, <year>2015</year>.</mixed-citation></ref><ref id="R51"><label>51.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>Elastic</collab>
</person-group>. <article-title>Elasticsearch Platform</article-title>. <year>2024</year>. <ext-link xlink:href="https://www.elastic.co/de/" ext-link-type="uri">https://www.elastic.co/de/</ext-link> (<comment>20 March 2024, date last accessed</comment>).</mixed-citation></ref><ref id="R52"><label>52.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>The Khronos Group</collab>
</person-group>. <article-title>OpenXR &#x02013; Highperformance Access to AR and VR &#x02014; Collectively Known as XR &#x02014; Platforms and Devices</article-title>. <year>2016</year>. <ext-link xlink:href="https://www.khronos.org/openxr/" ext-link-type="uri">https://www.khronos.org/openxr/</ext-link> &#x000a0;<comment>(29 September 2022, date last accessed</comment>).</mixed-citation></ref><ref id="R53"><label>53.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>dotNetRDF Project</collab>
</person-group>. <article-title>dotnetrdf: An Open Source.NET Library for RDF</article-title>. <year>2020</year>. <ext-link xlink:href="https://www.dotnetrdf.org/" ext-link-type="uri">https://www.dotnetrdf.org/</ext-link> &#x000a0;<comment>(15 September 2020, date last accessed</comment>).</mixed-citation></ref><ref id="R54"><label>54.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lohmann</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Negru</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Haag</surname> &#x000a0;<given-names>F</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Visualizing ontologies with VOWL</article-title>. <source><italic toggle="yes">Semant Web</italic></source> &#x000a0;<year>2016</year>;<volume>7</volume>:<fpage>399</fpage>&#x02013;<lpage>419</lpage>. doi: <pub-id pub-id-type="doi">10.3233/SW-150200</pub-id></mixed-citation></ref><ref id="R55"><label>55.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Onorati</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>D&#x000ed;az</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Zarraonandia</surname> &#x000a0;<given-names>T</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The immersive bubble chart</article-title>. In: (eds), <italic toggle="yes">Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology Adjunct</italic>, Berlin Germany, <volume>2018</volume>. <conf-date>14 October 2018</conf-date> , <fpage>176</fpage>&#x02013;<lpage>78</lpage>, 2018. doi: <pub-id pub-id-type="doi">10.1145/3266037.3271642</pub-id></mixed-citation></ref><ref id="R56"><label>56.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Kellmann</surname> &#x000a0;<given-names>AJ</given-names></string-name>, <string-name><surname>Postema</surname> &#x000a0;<given-names>M</given-names></string-name></person-group>. <article-title>Graph2VR Tutorial Part 1&#x02013;5</article-title>. <year>2023</year>. <ext-link xlink:href="https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH" ext-link-type="uri">https://www.youtube.com/playlist?list=PLRQCsKSUyhNIdUzBNRTmE-_JmuiOEZbdH</ext-link> (<comment>04 December 2023, date last accessed</comment>).</mixed-citation></ref></ref-list></back></article>