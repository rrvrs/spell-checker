<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Open Mind (Camb)</journal-id><journal-id journal-id-type="iso-abbrev">Open Mind (Camb)</journal-id><journal-id journal-id-type="publisher-id">opmi</journal-id><journal-title-group><journal-title>Open Mind : Discoveries in Cognitive Science</journal-title></journal-title-group><issn pub-type="epub">2470-2986</issn><publisher><publisher-name>MIT Press</publisher-name><publisher-loc>255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA journals-info@mit.edu</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40013087</article-id><article-id pub-id-type="pmc">PMC11864798</article-id><article-id pub-id-type="publisher-id">opmi_a_00189</article-id><article-id pub-id-type="doi">10.1162/opmi_a_00189</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Approximating Human-Level 3D Visual Inferences With Deep Neural Networks</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>O&#x02019;Connell</surname><given-names>Thomas P.</given-names></name><xref rid="cor1" ref-type="corresp">*</xref><xref rid="aff1" ref-type="aff"/></contrib><contrib contrib-type="author"><name><surname>Bonnen</surname><given-names>Tyler</given-names></name><xref rid="aff2" ref-type="aff"/></contrib><contrib contrib-type="author"><name><surname>Friedman</surname><given-names>Yoni</given-names></name><xref rid="aff1" ref-type="aff"/></contrib><contrib contrib-type="author"><name><surname>Tewari</surname><given-names>Ayush</given-names></name><xref rid="aff3" ref-type="aff"/></contrib><contrib contrib-type="author"><name><surname>Sitzmann</surname><given-names>Vincent</given-names></name><xref rid="aff3" ref-type="aff"/></contrib><contrib contrib-type="author"><name><surname>Tenenbaum</surname><given-names>Joshua B.</given-names></name><xref rid="aff4" ref-type="aff"/></contrib><contrib contrib-type="author"><name><surname>Kanwisher</surname><given-names>Nancy</given-names></name><xref rid="aff1" ref-type="aff"/></contrib></contrib-group><aff id="aff1">Brain &#x00026; Cognitive Sciences, MIT, Cambridge, MA, USA</aff><aff id="aff2">EECS, University of California, Berkeley, Berkeley, CA, USA</aff><aff id="aff3">CSAIL, MIT, Cambridge, MA, USA</aff><aff id="aff4">Brain &#x00026; Cognitive Sciences, CSAIL, MIT, Cambridge, MA, USA</aff><author-notes><fn fn-type="COI-statement"><p>Competing Interests: The authors declare no conflict of interests.</p></fn><corresp id="cor1">* Corresponding Author: <email xlink:href="mailto:tpo@mit.edu">tpo@mit.edu</email></corresp></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub"><day>16</day><month>2</month><year>2025</year></pub-date><volume>9</volume><fpage>305</fpage><lpage>324</lpage><history>
<date date-type="received"><day>09</day><month>5</month><year>2024</year></date>
<date date-type="accepted"><day>14</day><month>1</month><year>2025</year></date>
</history><permissions><copyright-statement>&#x000a9; 2025 Thomas P. O&#x02019;Connell, Tyler Bonnen, Yoni Friedman, Ayush Tewari, Vincent Sitzmann, Joshua B. Tenenbaum, and Nancy Kanwisher</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Thomas P. O&#x02019;Connell, Tyler Bonnen, Yoni Friedman, Ayush Tewari, Vincent Sitzmann, Joshua B. Tenenbaum, and Nancy Kanwisher</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. For a full description of the license, please visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><self-uri xlink:href="opmi-09-305.pdf"/><abstract><title>Abstract</title><p>Humans make rich inferences about the geometry of the visual world. While deep neural networks (DNNs) achieve human-level performance on some psychophysical tasks (e.g., rapid classification of object or scene categories), they often fail in tasks requiring inferences about the underlying shape of objects or scenes. Here, we ask whether and how this gap in 3D shape representation between DNNs and humans can be closed. First, we define the problem space: after generating a stimulus set to evaluate 3D shape inferences using a match-to-sample task, we confirm that standard DNNs are unable to reach human performance. Next, we construct a set of candidate 3D-aware DNNs including 3D neural field (Light Field Network), autoencoder, and convolutional architectures. We investigate the role of the learning objective and dataset by training single-view (the model only sees one viewpoint of an object per training trial) and multi-view (the model is trained to associate multiple viewpoints of each object per training trial) versions of each architecture. When the same object categories appear in the model training and match-to-sample test sets, multi-view DNNs approach human-level performance for 3D shape matching, highlighting the importance of a learning objective that enforces a common representation across viewpoints of the same object. Furthermore, the 3D Light Field Network was the model most similar to humans across all tests, suggesting that building in 3D inductive biases increases human-model alignment. Finally, we explore the generalization performance of multi-view DNNs to out-of-distribution object categories not seen during training. Overall, our work shows that multi-view learning objectives for DNNs are necessary but not sufficient to make similar 3D shape inferences as humans and reveals limitations in capturing human-like shape inferences that may be inherent to DNN modeling approaches. We provide a methodology for understanding human 3D shape perception within a deep learning framework and highlight out-of-domain generalization as the next challenge for learning human-like 3D representations with DNNs.</p></abstract><kwd-group><kwd>3D shape perception</kwd><kwd>deep neural networks</kwd><kwd>neural fields</kwd><kwd>psychophysics</kwd></kwd-group><funding-group specific-use="FundRef"><award-group award-type="grant"><funding-source>
<institution-wrap><institution>National Institutes of Health</institution></institution-wrap>
</funding-source><award-id>DP1HD091947</award-id><principal-award-recipient id="recipient1">
<name><surname>Kanwisher</surname><given-names>Nancy</given-names></name>
</principal-award-recipient></award-group></funding-group><funding-group specific-use="FundRef"><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Office of Naval Research MURI</institution></institution-wrap>
</funding-source><award-id>PO #BB01540322</award-id><principal-award-recipient id="recipient2">
<name><surname>Tenenbaum</surname><given-names>Joshua B.</given-names></name>
</principal-award-recipient></award-group></funding-group><funding-group specific-use="FundRef"><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Center for Brains, Minds, and Machines (CBMM) funded by NSF STC</institution></institution-wrap>
</funding-source><award-id>CCF-1231216</award-id></award-group></funding-group><counts><page-count count="20"/></counts><custom-meta-group><custom-meta><meta-name>citation</meta-name><meta-value>O&#x02019;Connell, T. P., Bonnen, T., Friedman, Y., Tewari, A., Sitzmann, V., Tenenbaum, J. B., &#x00026; Kanwisher, N. (2025). Approximating Human-Level 3D Visual Inferences With Deep Neural Networks. <italic toggle="yes">Open Mind: Discoveries in Cognitive Science</italic>, <italic toggle="yes">9</italic>, 305&#x02013;324. <ext-link xlink:href="https://doi.org/10.1162/opmi_a_00189" ext-link-type="uri">https://doi.org/10.1162/opmi_a_00189</ext-link></meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="sec1"><title>INTRODUCTION</title><p>When we look at an object, we do more than recognize its category or position in space. We perceive the orientation and curvature of its visible surfaces, and from this we can infer its global 3D shape (Ullman, <xref rid="bib52" ref-type="bibr">1987</xref>). This ability enables us to imagine what any given object might look like from a different perspective, for example, or to understand an object&#x02019;s affordances, such that we can grasp or interact with objects, even when they have arbitrary, unfamiliar shapes (Gibson, <xref rid="bib23" ref-type="bibr">2014</xref>). Remarkably, humans can infer 3D shape from a single image (Hassanin et al., <xref rid="bib25" ref-type="bibr">2021</xref>). Inferring these underlying properties from a single image is an ill-posed problem (Pizlo, <xref rid="bib38" ref-type="bibr">2001</xref>)&#x02014;that is, any given image is consistent with multiple interpretations of it&#x02019;s underlying geometry. As such, inferring object geometry depends on our ability to resolve formidable computational challenges. A rich body of empirical evidence exists to understand this ability, and many theories have been proposed to account for these data (B&#x000fc;lthoff &#x00026; Mallot, <xref rid="bib11" ref-type="bibr">1988</xref>; Janssen et al., <xref rid="bib29" ref-type="bibr">2000</xref>; Kersten et al., <xref rid="bib32" ref-type="bibr">2004</xref>; Marr, <xref rid="bib35" ref-type="bibr">2010</xref>; Shepard &#x00026; Metzler, <xref rid="bib45" ref-type="bibr">1971</xref>; Ullman, <xref rid="bib51" ref-type="bibr">1979</xref>; Wertheimer, <xref rid="bib54" ref-type="bibr">1938</xref>). In recent years, there has been a renewed interest in developing formal, computational accounts of 3D shape perception, including analysis-by-synthesis approaches (Yildirim et al., <xref rid="bib58" ref-type="bibr">2020</xref>; Yuille &#x00026; Kersten, <xref rid="bib60" ref-type="bibr">2006</xref>). Despite some promising progress, the factors that lead to human-level 3D understanding in computational models remain unclear.</p><p>Over the past 10 years, deep neural networks (DNNs) have emerged as a promising methodological framework for studying primate vision (Doerig et al., <xref rid="bib18" ref-type="bibr">2023</xref>; Kriegeskorte, <xref rid="bib34" ref-type="bibr">2015</xref>; Yamins &#x00026; DiCarlo, <xref rid="bib56" ref-type="bibr">2016</xref>). DNNs are appealing as models of visual processing, in part, because their architecture was loosely inspired by the organization of the primate visual system. Critically, DNNs are &#x02018;stimulus-computable&#x02019; models, such that they make predictions of behavioral and neural responses directly from images (Khaligh-Razavi &#x00026; Kriegeskorte, <xref rid="bib33" ref-type="bibr">2014</xref>; Rajalingham et al., <xref rid="bib39" ref-type="bibr">2018</xref>; Schrimpf et al., <xref rid="bib43" ref-type="bibr">2020</xref>; Yamins et al., <xref rid="bib57" ref-type="bibr">2014</xref>) and currently provide the most quantitatively accurate accounts of neural responses and behaviors that depend on the ventral visual stream (Doerig et al., <xref rid="bib18" ref-type="bibr">2023</xref>; Kriegeskorte, <xref rid="bib34" ref-type="bibr">2015</xref>; Yamins &#x00026; DiCarlo, <xref rid="bib56" ref-type="bibr">2016</xref>). Nonetheless, significant gaps remain between standard DNNs and human performance on many visual tasks, most notably inferring the shape of objects. ImageNet-trained CNNs are biased to classify images based on texture, for example, whereas humans show a strong shape classification bias (Baker et al., <xref rid="bib4" ref-type="bibr">2018</xref>; Geirhos et al., <xref rid="bib22" ref-type="bibr">2018</xref>; Hermann et al., <xref rid="bib27" ref-type="bibr">2020</xref>). In 3D shape tasks that require matching images that depicting the same object from different viewpoints, humans outperform standard CNNs by a wide margin (Bonnen et al., <xref rid="bib10" ref-type="bibr">2021</xref>; Rajalingham et al., <xref rid="bib39" ref-type="bibr">2018</xref>). It is unclear what components of this modeling approach (e.g., model architecture, training objective, or dataset) leads to the misalignment with human 3D shape inference abilities.</p><p>The limitations of standard DNNs trained on large-scale datasets (e.g., ImageNet) in capturing the 3D shape of objects and scenes is well recognized in the computer vision community (Abbas &#x00026; Deny, <xref rid="bib1" ref-type="bibr">2023</xref>; Alcorn et al., <xref rid="bib2" ref-type="bibr">2019</xref>; Cooper et al., <xref rid="bib15" ref-type="bibr">2021</xref>; Geirhos et al., <xref rid="bib22" ref-type="bibr">2018</xref>; Reizenstein et al., <xref rid="bib41" ref-type="bibr">2021</xref>). Several computational methods have recently been developed to better infer 3D geometry in DNNs (Xie et al., <xref rid="bib55" ref-type="bibr">2022</xref>). Unlike standard DNNs, these models&#x02019; architectures and training procedures were designed to directly account for the 3D properties of objects and scenes. One set of recent 3D-aware architectures, 3D neural fields, learn a continuous function that maps xyz or ray coordinates from a 3D volume to shape and/or color, given the object&#x02019;s pose. 3D neural fields are typically trained with a multi-view rendering objective, in which the model computes the geometry from some set of input images and outputs renders of the object or scene from novel viewpoints not included in the inputs. The most common class of 3D neural fields, NEural Radiance Fields (NERFs), are optimized directly on many views of an individual object or scene, and can be used to create near photo-realistic 3D models (Mildenhall et al., <xref rid="bib36" ref-type="bibr">2021</xref>). Other methods, such as conditional 3D neural fields, learn a generalizable shape space which can recover the global 3D shape of objects from a single image (Sitzmann et al., <xref rid="bib46" ref-type="bibr">2021</xref>; Yu et al., <xref rid="bib59" ref-type="bibr">2021</xref>). The improved performance of these models reconstructing 3D shape from images raises the question of whether they might be more aligned with human 3D inference abilities.</p><p>Here, we evaluate what properties of DNNs are necessary for them to make similar 3D shape inferences as humans. First, we construct a 3D match-to-sample task in which human participants match images depicting the same object from two different viewpoints. As expected, we observe a large gap between standard ImageNet-trained DNNs and humans. Next, we train a set of DNNs to investigate the role of architecture and learning objective on model alignment to humans. This set includes 3D Light Field Networks (LFNs), convolutional neural networks (CNNs, e.g., resnet50), and autoencoder models, each trained with both single-view and multi-view learning objectives. We find that DNNs trained with a multi-view objective make similar 3D shape inferences to humans. We create a series of &#x02018;adversarial&#x02019; conditions for models, and find that the performance of humans and multi-view DNNs is robust even for these difficult trials. Notably, 3D-LFNs show the highest alignment to humans. Finally, we characterize the generalization of multi-view DNNs to out-of-distribution object categories not included in training. Overall, we demonstrate that DNNs are capable of making similar 3D shape judgements as humans when trained with a multi-view learning objective and tested on objects that overlap with the training distribution. Our approach provides a framework for studying human 3D shape inferences alongside deep learning models, identifying out-of-domain generalization to novel shape categories as a primary challenge for future research.</p></sec><sec id="sec2"><title>RESULTS</title><sec id="sec3"><title>3D Shape Judgements in Humans and ImageNet DNNs</title><p>To probe 3D shape representations in humans and DNNs, we use a multi-view match-to-sample task. For the human experiments, three images are shown concurrently in each trial (<xref rid="F1" ref-type="fig">Figure 1a</xref>) and remain on the screen until the participant makes their response. The top image (sample) depicts an object from one viewpoint. For the two images below, one depicts the same object as the sample from a different viewpoint (target) and the other depicts a different object from the same category (lure). Viewpoints for all objects were sampled randomly from a sphere around the object. The task for humans is to simply choose which of the target and lure images is the same object depicted in the sample image. Similar tasks have been used previously to probe for 3D shape representations in both models and humans (Bonnen et al., <xref rid="bib10" ref-type="bibr">2021</xref>; Rajalingham et al., <xref rid="bib39" ref-type="bibr">2018</xref>). Rajalingham et al. (<xref rid="bib39" ref-type="bibr">2018</xref>) showed that ImageNet CNNs are aligned to human shape matching judgements when the target and lure objects are drawn from different categories, but show a large gap to humans when the target and lure objects are both from the same category. This provides the motivation for our design where target and lure pairs are always drawn from the same category, so solving the task requires more fine-grained shape judgements rather than being solvable via categorization. For all experiments presented in this paper, we collect human behavioral responses (<italic toggle="yes">N</italic> = 200) online using Prolific (<ext-link xlink:href="http://www.prolific.co" ext-link-type="uri">www.prolific.co</ext-link>).</p><fig position="float" id="F1"><label><bold>Figure 1.</bold>&#x02003;</label><caption><p>a.) Example trials from the match-to-sample task used to probe 3D shape representations in humans and models. Participants saw all three images concurrently, and their task was choose which of the bottom two objects (target and lure) depict the top object (sample) from a new viewpoint. For visualization purposes in this figure, the correct responses are indicated with a red dot. b.) Model procedure for match-to-sample tasks with baseline ImageNet models. For a given trial, the sample, target, and lure images were separately run through the network, and unit activity from the final layer before the classification layer were extracted as features. The cosine similarity between the sample/target features and sample/lure features was computed, and if the sample/target similarity was higher the trial is counted as correct. The green schematic shows the process for CNNs and the blue for ViTs. c.) Match-to-sample results for within-category 3D shape judgements. Each purple dot is a single human participant (<italic toggle="yes">N</italic> = 200). For human data, the y-axis encodes the average self-similarity of one held out participant to the mean of all other human participants (noise-ceiling). The errorbars along the y-axis show one standard deviation in accuracies across participants, and the errorbars along the y-axis show one standard deviation in the leave-one-subject-out noise-ceiling across participants. The shaded grey distribution is a random observer baseline that shows the expected range of trial-wise similarity to humans across accuracies if the correct and incorrect responses are randomly distributed across trials (see <xref rid="sec12" ref-type="sec">Methods</xref> - <xref rid="sec20" ref-type="sec">Random Observer Baseline</xref>).</p></caption><graphic xlink:href="opmi-09-305-g001" position="float"/></fig><p>Stimuli were rendered from objects in the ShapeNet dataset (Chang et al., <xref rid="bib12" ref-type="bibr">2015</xref>). We use a subset of ShapeNet that has 30565 training objects and 8736 test objects drawn from 13 manmade categories. Match-to-sample trials were generated by randomly pairing two test objects from the same category, then creating two trials with those objects&#x02019; renders (one where object A is the sample/target and object B is lure, and one where object B is the sample/target and object A is the lure). Each participant only saw one trial for a given pair so all objects were novel throughout the experiment. Examples can be seen in <xref rid="F1" ref-type="fig">Figure 1a</xref>.</p><p>To complete the analogous task on DNNs (<xref rid="F1" ref-type="fig">Figure 1b</xref>), each of the three images for a given trial are fed into a given network and unit activity was extracted from the penultimate layer. The match-to-sample task is completed via a similarity-based measure; cosine similarity is computed between the sample/target and sample/lure unit activity, and the trial is counted as correct if the sample-target similarity is higher than the sample-lure similarity. For the baseline DNN model zoo, we use 25 CNNs from PyTorch (Reizenstein et al., <xref rid="bib41" ref-type="bibr">2021</xref>) and 25 ViTs from the timm package (<ext-link xlink:href="http://timm.fast.ai" ext-link-type="uri">timm.fast.ai</ext-link>), all pretrained for object classification on ImageNet. We also test 3 resnet50 CNNs trained on Stylized-ImageNet (Stylized-IN), a version of ImageNet which texture cues related to object category and increase human-alignment on an adversarial texture vs shape categorization task (Geirhos et al., <xref rid="bib22" ref-type="bibr">2018</xref>), and four DinoV2 models, which are self-supervised ViT models also trained on ImageNet (Oquab et al., <xref rid="bib37" ref-type="bibr">2023</xref>).</p><p>We report how far a given model accuracy or trial-wise similarity to humans score is from the equivalent human accuracy or noise ceiling in units of standard deviations of the human accuracies (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub>) or noise ceilings (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub>).</p><p>As expected, we observe a large gap in both accuracy and trial-level similarity between standard DNNs and human behavior (<xref rid="F1" ref-type="fig">Figure 1c</xref>). Humans performed well on the task (<italic toggle="yes">M</italic> = 0.89, <italic toggle="yes">STD</italic> = 0.051, <xref rid="F1" ref-type="fig">Figure 1c</xref>, x-axis). A noise ceiling was computed as the cosine similarity between a given participant&#x02019;s trial-wise performance and the mean trial-wise accuracies across all other participants, computed in a leave-one-participant-out fashion and averaged across participants (<italic toggle="yes">M</italic> = 0.96, <italic toggle="yes">STD</italic> = 0.020, <xref rid="F1" ref-type="fig">Figure 1c</xref>, y-axis). As with the human noise-ceiling, similarity to human behavior for each DNN model was computed as the cosine similarity between the vector of model responses across trials and the average human accuracy for each trial. We see much lower performance for ImageNet CNNs, with gaps in both accuracy (<italic toggle="yes">M</italic> = 0.70, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = &#x02212;3.75) and trial-level similarity to humans (<italic toggle="yes">M</italic> = 0.85, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> = &#x02212;5.47). ImageNet ViTs showed a similar gap compared to humans as CNNs in both accuracy (<italic toggle="yes">M</italic> = 0.71, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = &#x02212;3.58) and trial-level similarity to humans (<italic toggle="yes">M</italic> = 0.86, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> = &#x02212;4.99). The three CNNs trained on Stylized-ImageNet, which is designed to induce a shape bias, performed slightly worse than standard ImageNet CNNs on accuracy (<italic toggle="yes">M</italic> = 0.68, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = &#x02212;4.21) and trial-level similarity to humans (<italic toggle="yes">M</italic> = 0.84, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> = &#x02212;6.11). Finally, while the four DinoV2 models had the best average performance of any baseline model type in both accuracy (<italic toggle="yes">M</italic> = 0.73, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = &#x02212;3.17) and trial-wise similarity to humans (<italic toggle="yes">M</italic> = 0.88, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> = &#x02212;4.15), they still fell well short of human performance and alignment. These findings highlight the 3D shape processing gap that has been reported in the literature between standard DNNs trained on large corpuses of natural images and humans that we aim to address with the following work.</p></sec><sec id="sec4"><title>Model Zoo</title><p>In order to identify what types of architectures and learning objectives increase alignment with human 3D shape inferences, we construct a model zoo of 3D Light Field Networks, resnet50 CNNs, and autoencoders trained with either single-view or multi-view learning objectives.</p><sec id="sec5"><title>3D Light Field Networks.</title><p>Light fields are an idealized representation that encodes the color of rays of light passing through every possible position in a scene at every possible orientation. A 3D Light Field Network (3D-LFN) (Sitzmann et al., <xref rid="bib46" ref-type="bibr">2021</xref>) instantiates this idea in a neural network by mapping from coordinates defining a ray through a volume to the RGB value of that ray. The mapping is implemented as a multi-layer perceptron (MLP), and once trained the weights of the MLP define the 3D light field, and thus the 3D shape, for a given object.</p><p>The flow of information through 3D-LFNs (<xref rid="F1" ref-type="fig">Figure 1a</xref>), during both training and inference, follows three steps (<xref rid="F2" ref-type="fig">Figure 2a</xref>): 1.) Infer a set of shape latents (256D) from an RGB image, 2.) Map from shape latents to the neural field weights, 3.) Given a camera position, query rays from the neural field to render out an image. To infer shape latents from images (1), an RGB image is encoded using a CNN (resnet50) and 256D shape latents are linearly read out from the last convolutional layer. To map from latents to the neural field weights (2), a hypernetwork (a neural network trained to output the weights of another neural network) implemented using a series of two-layer MLPs with ReLU activation functions. The neural field itself is implemented as an eight-layer MLP with ReLU activation functions. The inputs to the field are coordinates defining a ray that passes through the space encoded by the field, and the output is the RGB value of the queried ray. To render the output image (3), a camera position and image plane are defined, and rays that pass from the camera through the image plane are sampled. The coordinates defining each ray are passed into the neural field, and the returned RGB value becomes the RGB value in the rendered image at the point where that ray intersects the image plane.</p><fig position="float" id="F2"><label><bold>Figure 2.</bold>&#x02003;</label><caption><p>a.) Schematic for the 3D Light Field Networks. For all model variants, two versions were trained: a single-view model which operates over individual images and a multi-view version with a learning objective that encourages similar representations for two different views of the same object. b.) Schematic for autoencoder models. c.) Schematic for CNN models. d.) Multi-view DNN models are more aligned to human 3D shape judgements than the baseline DNNs. The 3D Light Field Networks achieve the highest accuracy and alignment to humans. Single-view versions of the same models trained on ShapeNet perform worse than the standard models, highlighting the important role of the multi-view learning objective in reaching human alignment. The shaded grey distribution is a random observer baseline that shows the expected range of trial-wise similarity to humans across accuracies if the correct and incorrect responses are randomly distributed across trials (see <xref rid="sec12" ref-type="sec">Methods</xref> - <xref rid="sec20" ref-type="sec">Random Observer Baseline</xref>).</p></caption><graphic xlink:href="opmi-09-305-g002" position="float"/></fig><p>During training, a standard 3D-LFN is optimized end-to-end using a 3D multi-view learning objective. The input image depicts an object from one viewpoint, and the model&#x02019;s objective is to render the same object from a different viewpoint defined by a camera matrix. The rendered image is compared to the ground-truth image of the object from the new viewpoint, and mean-squared-error between the rendered and ground-truth images provides the loss used to update network&#x02019;s weights. The loss is back = propagated through the weights of the hypernetwork and CNN encoder. The CNN encoder is not pretrained, and its weights are entirely learned in tandem with the 3D-NF. During inference for test images, the latents and weights defining the 3D-LFN MLP are computed via a single feedforward pass through the model. Camera matrices can then be provided to render out the new object from the specified viewpoints, but the 3D-LFN latents and weights feature-spaces are computed without needing camera matrices.</p><p>This standard 3D-LFN training procedure is referred to as the multi-view learning objective using our nomenclature for these experiments. To create a single-view version of the 3D-LFN, where the architecture is identical but the model is not exposed to multiple viewpoints of the same object during training, the rendering objective for the model is made to be the same viewpoint as the input image. We use the shape latents and neural field weights as feature spaces for the subsequent 3D shape tasks.</p></sec><sec id="sec6"><title>Autoencoders.</title><p>Autoencoders are autoregressive DNN encoder-decoder architectures that use a series of encoder layers to map the input to a low-dimensional latent space, then use a series of decoder layers to reconstruct the input from the latent space (<xref rid="F1" ref-type="fig">Figure 1b</xref>). We use a convolutional autoencoder architecture that encodes images to a 256D latent space using a a resnet50 encoder, then reconstructs the input image from the latents using a series of deconvolutional layers. This standard autoregressive autoencoder is the single-view learning objective variant.</p><p>To make a multi-view version of the autoencoder, we use the same architecture with two modifications. First, we adjust the model&#x02019;s rendered output objective to be the input object from a different viewpoint instead of the input viewpoint. Second, we append an embedding of the output viewpoint camera matrix to the 256D latent space before the latents are passed into the decoder. This camera matrix embedding tells the model what the output viewpoint should be (similar to how 3D-LFNs use a provided camera matrix to define the output viewpoint).</p><p>For both variants, we use the 256D latent space as the feature space for the 3D shape tasks.</p></sec><sec id="sec7"><title>CNNs.</title><p>For our CNN models, we use a resnet50 architecture (<xref rid="F2" ref-type="fig">Figure 2c</xref>). In the single-view case, we train the CNN to do 13-way ShapeNet category classification. We use the final convolutional layer (penultimate layer before classification layer) as the feature space for the 3D shape tasks.</p><p>For the multi-view case, we train self-supervised CNN using a resnet50 architecture and a modified MOCO contrastive loss (He et al., <xref rid="bib26" ref-type="bibr">2020</xref>) to learn similar embeddings for images depicting different viewpoints of the same objects (similar to Aubret et al., <xref rid="bib3" ref-type="bibr">2022</xref>). In standard MOCO CNNs, different image manipulations (e.g., translation, left-right flip, negative colors) are applied, and the model&#x02019;s loss makes the embeddings computed from two modified versions of the same image as similar as possible relative to the embeddings computed for other images. We use this same scheme, but rather than applying image manipulations we provide two different viewpoints of the same ShapeNet object as input, compute the two embeddings in a siamese fashion with the same resnet50 encoder, then compute the loss over the embeddings for the two images. The model has a multi-view learning objective in that it must associate different viewpoints of the same objects as similar in its learned embedding space, but differs from the multi-view 3D-LFN and autoencoder in that the multi-view objective is not actually rendering out the input from novel viewpoints. We use the output embeddings as the feature space for the 3D shape tasks.</p></sec></sec><sec id="sec8"><title>Multi-View DNNs and Humans Make Similar 3D Shape Judgements</title><p>We trained all models outlined in the previous section on the ShapeNet dataset used for the match-to-sample task administered to humans, CNNs, and ViTs from the previous section. This training data includes 30565 objects from 13 object categories with 50 viewpoints per object, which are distinct from the test set of 8736 objects from which the stimuli for the match-to-sample experiments were rendered. The categories in the training and test sets are the same, but the specific objects are distinct. The shape latents and weights of the 3D-LFNs, the latent space of the autoencoders, and final pre-readout layer of the CNNs are extracted as separate sets of features, and the task is performed on all models using the same similarity-based measure applied to CNNs and ViTs.</p><p>For the multi-view 3D-LFN latents (<xref rid="F2" ref-type="fig">Figure 2b</xref>), we see a marked improvement in both accuracy (<italic toggle="yes">M</italic> = 0.89, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = &#x02212;0.040) and trial-wise similarity to humans (<italic toggle="yes">M</italic> = 0.95, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> = &#x02212;.49). The multi-view 3D-LFN weights do even better (<xref rid="F2" ref-type="fig">Figure 2b</xref>), reaching the mean human accuracy (<italic toggle="yes">M</italic> = 0.93, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = 0.78) and within one standard deviation of the human noise ceiling (<italic toggle="yes">M</italic> = 0.96, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> = 0.24). The multi-view CNN performance was within one standard deviation of human accuracy (<italic toggle="yes">M</italic> = 0.86, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = &#x02212;0.67) and much closer to reaching the trial-wise similarity noise ceiling than any baseline model (<italic toggle="yes">M</italic> = 0.93, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> = &#x02212;1.60). The multi-view autoencoder performed worse than all other multi-view models for both accuracy (<italic toggle="yes">M</italic> = 0.78, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = &#x02212;2.25) and trial-wise similarity (<italic toggle="yes">M</italic> = 0.88, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> = &#x02212;3.94), with performance and alignment to humans similar to the top baseline models.</p><p>The single-view 3D-LFN, autoencoder, and CNN all were the worst performing models, with much lower accuracies (<italic toggle="yes">M</italic> &#x0003c; 0.60, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> &#x0003c; &#x02212;5.73) and trial-wise similarity to humans (<italic toggle="yes">M</italic> &#x0003c; 0.77, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> &#x0003c; &#x02212;9.28) than any of the baseline models.</p><p>Overall, these results show that the multi-view 3D-LFN supports 3D shape inferences that are aligned to humans for within-category comparisons, with both the latents and weights of the 3D-LFN falling within one standard deviation of both human accuracy and the human noise ceiling. Other multi-view models performed well, with the multi-view CNN approaching human-alignment and the multi-view autoencoder outperforming almost all of the baseline models. The extremely poor performance of the single-view variants of the same model highlights that a multi-view learning objective that encourages a viewpoint-invariant representation is a necessary condition for models to make similar 3D shape judgements as humans.</p></sec><sec id="sec9"><title>3D-LFNs and Humans Make Similar 3D Shape Judgements for &#x0201c;Adversarial&#x0201d; CNN-Defined Matching Judgements</title><p>Next, we used an &#x0201c;adversarial&#x0201d; stimulus-selection procedure to accentuate the failure modes of the baseline DNNs (i.e., where their performance deviates most sharply from humans). Using the 25 ImageNet CNNs, we filtered hundreds of thousands of potential within-category object pairs from ShapeNet to construct match-to-sample trials. We bin the object pairs according to the average CNN accuracy and sample from these bins to create a series of 5 adversarially-defined difficulty conditions. In the most difficult condition, the CNN average accuracy is 0.14 (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = &#x02212;6.95) and the trial-wise similarity to humans is 0.38 (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> = &#x02212;12.68). In the easiest condition, the CNN average accuracy is 0.68 (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = &#x02212;3.95) and the trial-wise similarity to humans is 0.83 (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> = &#x02212;5.81).</p><p>Humans (<italic toggle="yes">N</italic> = 200) were largely unaffected by the CNN-defined difficulty conditions, showing high performance across all conditions (<italic toggle="yes">M</italic> &#x0003e; 0.83, <xref rid="F3" ref-type="fig">Figure 3a</xref>). Out of the baseline models, ImageNet DinoV2s (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> &#x0003e; &#x02212;6.90) performed best, followed by ImageNet ViTs (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> &#x0003e; &#x02212;9.37), StylizedImageNet CNNs (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> &#x0003e; &#x02212;12.10), and ImageNetCNNs (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> &#x0003e; &#x02212;12.68), but all baseline models were far from human alignment (<xref rid="F3" ref-type="fig">Figure 3</xref>).</p><fig position="float" id="F3"><label><bold>Figure 3.</bold>&#x02003;</label><caption><p>a.) In order to focus on the gap between humans and standard CNNs, many potential match-to-sample trials were filtered with the 25 ImageNet CNNs to select trials that fall in five difficulty conditions that adversarially accentuate the different between ImageNet-trained models and humans. The multi-view DNN features fall within the distribution of human response for all adversarially-defined conditions, including those where the baseline models perform much worse than humans. The shaded grey distribution is a random observer baseline that shows the expected range of trial-wise similarity to humans across accuracies if the correct and incorrect responses are randomly distributed across trials (see <xref rid="sec12" ref-type="sec">Methods</xref> - <xref rid="sec20" ref-type="sec">Random Observer Baseline</xref>). b.) Human and model performance and trial-wise similarity averaged across conditions.</p></caption><graphic xlink:href="opmi-09-305-g003" position="float"/></fig><p>Critically, 3D-LFNs were well-aligned with human behavior across difficulty levels for both latents (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> &#x0003e; &#x02212;1.58) and weights (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> &#x0003e; &#x02212;0.45). Across all difficulty conditions, the 3D-LFN weights were within 1 STD of the mean accuracy and noise ceiling compared to humans. The multi-view CNN (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> &#x0003e; &#x02212;2.022) and autoencoder (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> &#x0003e; &#x02212;2.09) models were also markedly more aligned with humans across difficulties than any of the baseline models. These results show that even when selecting trials where standard ImageNet models struggle, 3D-LFNs and other multi-view DNNs support 3D shape inferences similar to humans.</p></sec><sec id="sec10"><title>3D-LFNs and Multi-View DNNs Are Not Human-Aligned for Novel Shape Categories Outside Their Training Set</title><p>Finally, we test the extent to which the 3D-LFN and multi-view DNN models generalize to novel shape categories not included in training. Human 3D vision is able to represent novel shapes we have never encountered in our visual experience. If we come upon an abstract sculpture in a park, we are able to perceive its shape even if it occupies a new part of our shape space that has never been activated. In these experiments, one possibility is that the 3D-LFN and multi-view DNNs learn a broad shape space that can generalize to novel categories of objects they have not seen before. The alternative is that the 3D-LFN and multi-view DNNs learn a space that can generalize to novel exemplars of categories included during training (as we have shown in all previous experiments) but fail for new categories of shapes that fall outside their visual experience.</p><p>To test the generality of the shape spaces learned by the 3D-LFN, multi-view autoencoder, and multi-view CNN, we first re-trained new versions of each multi-view model on ShapeNet holding out all cars, planes, and chairs in turn. We then constructed 3D match-to-sample experiments from only objects in each held-out category using the same adversarial stimulus selection procedure as earlier. With the CNN model zoo, we filter many possible object pairs for each held-out category and select the 150 pairs that best exemplify the performance gap between humans and standard DNN models. To evaluate generalization performance, we compare accuracies for the hold-one-category-out multi-view models described above to the multi-view models trained on all ShapeNet categories. For all new tasks, we find that all multi-view DNNs perform markedly worse when generalizing out-of-distribution to a new object category not included in the training set (<xref rid="F4" ref-type="fig">Figure 4a</xref>&#x02013;<xref rid="F4" ref-type="fig">c</xref>).</p><fig position="float" id="F4"><label><bold>Figure 4.</bold>&#x02003;</label><caption><p>Out-of-distribution generalization for multi-view DNNs to novel object categories. For each ShapeNet category (a. cars, b. planes, c. chairs), all objects from that category were held out during training for new multi-view DNN models. Match-to-sample trials were adversarially selected for the held-out category using the CNN model zoo to accentuate the performance gap between baseline models and humans. We observe that the multi-view DNN models fail to generalize out-of-distribution to novel ShapeNet categories not included in their training distribution relative to the in-distribution multi-view models trained on all ShapeNet categories. d.) Example procedurally-generated ShapeGen stimuli. e.) Results testing the baseline model zoo and multi-view DNNs trained on all ShapeNet categories on the out-of-distribution ShapeGen stimuli. The shaded grey distribution is a random observer baseline that shows the expected range of trial-wise similarity to humans across accuracies if the correct and incorrect responses are randomly distributed across trials (see <xref rid="sec12" ref-type="sec">Methods</xref> - <xref rid="sec20" ref-type="sec">Random Observer Baseline</xref>). We observe that the ShapeNet-trained multi-view models reach human-level accuracy for ShapeGen stimuli, but the trial-wise similarity between humans and models is no higher than expected from randomly distributed correct trials.</p></caption><graphic xlink:href="opmi-09-305-g004" position="float"/></fig><p>The above results do not allow an equivalent comparison between models and humans given humans&#x02019; extensive prior experience with chairs, cars, and planes. To directly compare human and multi-view model generalization performance, we procedurally-generated a set of abstract 3D objects that should be similarly novel to both models and humans. We generated these objects using the ShapeGenerator (ShapeGen) extension for Blender. ShapeGen starts with a base shape, randomly extrudes mesh faces according to a few parameters, then applies a Catmull-Clark modifier to produce a smooth final object (<xref rid="F4" ref-type="fig">Figure 4d</xref>). We used the same rendering procedure applied to ShapeNet to render images from these objects with viewpoints sampled from a sphere around the object, created 300 match-to-sample trials using the same CNN-based selection scheme applied to the held-out ShapeNet categories, and collected behavioral responses online (<italic toggle="yes">N</italic> = 140).</p><p>Human participants performed worse (<italic toggle="yes">M</italic> = 0.65, <italic toggle="yes">STD</italic> = 0.98) on this task than the ShapeNet tasks, but with a spread of performance from chance up to 0.88 (<xref rid="F4" ref-type="fig">Figure 4d</xref>). This drop in human performance was expected as these shapes are complex, have protrusions that can be self-occluded from certain viewpoints, and humans have limited priors over the shape space to infer the unseen surfaces from a given view. Next, we evaluated whether the multi-view DNNs from previous sections trained on the full set of 13 ShapeNet categories generalize to make human-like 3D shape judgements on these abstract shapes. While the best performing multi-view model (3D-LFN weights) was very close to average human accuracy (<italic toggle="yes">M</italic> = 0.64, <italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanAccuracy</italic></sub> = &#x02212;0.066), all models fell below the human noise ceiling for trial-wise similarity (<italic toggle="yes">STDs</italic><sub><italic toggle="yes">toHumanNoiseCeiling</italic></sub> &#x0003c; &#x02212;1.221) indicating that they did not make human-like judgements at a trial-wise level. This point is reinforced by all multi-view models falling within the random observer baseline (<xref rid="F4" ref-type="fig">Figure 4d</xref>, grey distribution, see <xref rid="sec12" ref-type="sec">Methods</xref> - <xref rid="sec20" ref-type="sec">Random Observer Baseline</xref>) indicating that their trial-wise similarity to human behavior is no higher than expected if the responses were randomly distributed across trials.</p><p>Overall, these results show limitations in the out-of-distribution generalization performance of the evaluated 3D-LFNs and multi-view DNNs. While the multi-view models did achieve human-level accuracy for matching abstract ShapeGen objects, their trial-wise performance was not correlated with human trial-wise performance beyond what is expected by randomly distributed correct trials and the multi-view models show a marked drop in accuracy for ShapeNet categories held-out of the training set. It is important to note that the 3D object shape datasets we used to train our multi-view models are much smaller than what we expect human observers to experience during development, both in the number of unique object categories and the number of unique objects per category. With larger scale 3D shape training datasets, the difference between human observers and multi-view DNNs for abstract ShapeGen shapes may shrink or vanish.</p></sec></sec><sec id="sec11"><title>DISCUSSION</title><p>Our work has characterized the modeling choices that enable DNNs to approximate human-level 3D visual inferences, as well as the conditions under which this correspondence fails. We first replicate previous findings that standard (i.e., trained on ImageNet) DNN models fail to reach human performance on a within category match-to-sample task. We then construct a series of models consisting of 3D-LFN, autoencoder, and resnet50 models trained with both single-view and multi-view learning objectives. We find that only models trained with a multi-view learning objective come close to human performance, when evaluating on left-out objects from the categories these models were trained on. We then construct a series of &#x02018;adversarial&#x02019; trials, designed to reveal the failures of standard DNNs. We find that multi-view models consistently fall within the human distribution for both accuracy and trial-wise similarity, vastly outperforming all other models. However, all multi-view models&#x02019; performance falls when generalizing to novel ShapeNet categories not included in the training distribution, and despite achieving human-level accuracy for abstract shapes the multi-view models trial-wise performance was not meaningfully correlated to human performance. These findings outline a tractable framework for studying human 3D vision within a deep learning framework, while highlighting limitations of current modeling approaches.</p><p>We emphasize that while a multi-view learning objective seems essential for models to achieve human-level 3D inferences under any conditions, our work points toward open questions about the additional contributions of model architecture and training objective. While all architectures trained with a multi-view objective approximated human performance, different models used different multi-view objectives. 3D-LFNs were trained to reconstruct objects from novel viewpoints (i.e., given input image from viewpoint A, render that same object from viewpoint B), while convolutional architectures were trained with a contrastive loss (i.e., represent an object from different viewpoints more similar than it is to different objects). As such, our observation that the 3D-LFNs best matched human accuracies and trial-wise patterns of behavior is ambiguous, because these 3D-LFNs differed from other multi-view models in both their architecture and their training objective. Future work is needed to adjudicate the role of architectures and learning objective in achieving human-like 3D shape judgements.</p><p>The importance of seeing objects from multiple viewpoints to support 3D visual learning, observed in our modeling results, is in line with research characterizing the visual inputs to developing human children (Smith et al., <xref rid="bib47" ref-type="bibr">2018</xref>). Frontward views of faces are the most common visual input in infant&#x02019;s first 3 months, a pattern that continues but decreases over the following 9 months as seeing body parts of other humans and hands manipulating objects, especially during mealtimes, become increasingly common (Clerkin et al., <xref rid="bib13" ref-type="bibr">2017</xref>; Fausey et al., <xref rid="bib20" ref-type="bibr">2016</xref>; Jayaraman et al., <xref rid="bib30" ref-type="bibr">2015</xref>, <xref rid="bib31" ref-type="bibr">2017</xref>). Crucially, the viewpoints at which human toddlers see objects via their own self manipulation does not appear to be random, but curated to support visual statistical learning. Even for 4&#x02013;7 month old infants, the ability to recognize the 3D shape of objects is most strongly linked to visual-motor manipulation abilities, which allow infants to expose themselves to many views of a novel object (Soska et al., <xref rid="bib48" ref-type="bibr">2010</xref>). Considering this empirical evidence, it is perhaps not surprising that computational models trained on standard datasets (e.g., Imagenet) do not spontaneously learn to represent objects in 3D, given that there is little in the training procedures to encourage a common representation across multiple viewpoints of the same object. Similarly, when we train models using such multi-view exposure to objects, they achieve human-level performance on within-category 3D shape judgements.</p><p>Recent work on learning online invariances in CNNs also supports the importance of multi-view exposure during training to learn 3D representations. Biscione and Bowers (<xref rid="bib6" ref-type="bibr">2022</xref>) showed that CNNs can develop online invariances to common transformations, but only when the model&#x02019;s training set included variation along the relevant dimension (Biscione &#x00026; Bowers, <xref rid="bib6" ref-type="bibr">2022</xref>). They trained CNNs for object classification or same-different judgements on images rendered from ShapeNet with variation in the viewpoint, rotation, scale, translation, and brightness of the objects. Evaluating on a similar multi-view match-to-sample task as the one used here, they find that CNNs trained on images with a particular transformation learn invariance to that transformation. Most similar to the current work, they find that CNNs can learn to be viewpoint-invariant, but only when exposed to multiple viewpoints and rotations of the same object during training. While they do not evaluate the alignment between their models and humans, they suggest that special architectures may not be necessary to learn human-like invariances for viewpoint, position, and lighting provided the right training diet and task. These predictions are consistent with our findings that a variety of DNN architectures are aligned to human 3D shape judgements, but only when the learning objective enforces learning a common representation across two viewpoints of the same object. While a special architecture directly representing 3D space was most aligned to human behavior (3D-LFNs), a standard CNN architecture with a contrastive loss enforcing common representation across viewpoints showed a marked jump in alignment to human 3D shape judgements. Future work will further adjudicate the role of architectures, special or otherwise, in achieving human-like 3D shape judgements, but exposure to multiple-viewpoints of objects during training is evidently essential.</p><p>This work shows how much of the performance gap can be closed between DNN and human abilities at matching objects across view points. However, this does not mean that these models necessarily implement the same processes as humans. As discussed next, a gap still exists in generalization beyond the training set, and the models remain biologically implausible.</p><p>Even the best performing models struggled to generalize to novel shapes not included in their training distribution. DNNs&#x02019; failure to generalize beyond their training set has been well characterized in the computer science literature (Geirhos et al., <xref rid="bib21" ref-type="bibr">2020</xref>; Recht et al., <xref rid="bib40" ref-type="bibr">2019</xref>; Zhang et al., <xref rid="bib61" ref-type="bibr">2021</xref>) and these generalization failures stem from DNNs&#x02019; tendency to overfit on their training set and interpolate within the training distribution at test time. This limitation is the driving force in the ongoing quest for larger and larger datasets to train DNNs so that more information is within-training-distribution. Through this lens, the mixed generalization performance of the multi-view DNNs is broadly consistent with expected DNN behavior. It is important to note that even the full ShapeNet dataset we use for training here is much smaller, both in number of object categories and number of unique objects per category, than what humans are exposed to during development. For this reason, while we observed limitations in models&#x02019; out-of-distribution generalization for this set of experiments, it&#x02019;s possible the model generalization will improve with much larger 3D shape training datasets. It will be important for future work to test generalization to novel shape categories with DNNs trained on much larger datasets, and it remains possible that modeling frameworks other than DNNs may be necessary in the long run to achieve human-level generalization.</p><p>Many aspects of the models we have evaluated here are biologically implausible. For example, 3D-LFNs are models that regress 3D shape latents from a CNN, map the latents to the 3D field space, then use a series of graphics techniques to render an image of the represented object from a viewpoint specified by a provided ground-truth camera matrix. There is no reason to expect that these graphical techniques&#x02014;such as casting rays through an image-plane to render an pixel-level RGB values of the depicted object&#x02014;are implemented by biological neurons. Additionally, these methods require ground-truth information about camera position&#x02014;a form of direct 3D supervision which is never provided to human observers. We are not proposing that the brain is using computer graphics techniques such as rendering and casting rays (or backpropogation for that matter) to learn 3D representations; this is simply one avenue to enforce 3D representations in models to produce human-aligned 3D shape judgements. An important direction for future research is to identify novel learning objectives that support 3D learning, perhaps via analysis-by-synthesis techniques, that can be implemented by neurons without standard graphics tools.</p><p>The 3D shape representations in the multi-view DNNs described in this work are implicit 3D representations encoded in the patterns of artificial neurons in DNNs. Given the biological implausibility of several components in our models, we do not make a claim here that humans are using a 3D shape representation of a similar format to the ones in our models to complete the 3D matching tasks. In addition to implicit representations encoded in neurons, other possibilities for the format of the human 3D shape representation include metric 3D representations such as depth or surface normal maps that encode local aspects of geometry across a scene or object, Biederman&#x02019;s recognition-by-components model (Biederman, <xref rid="bib5" ref-type="bibr">1987</xref>) which uses local image features to perform 3D shape tasks, optic flow maps that track how objects move over time (Stewart et al., <xref rid="bib50" ref-type="bibr">2022</xref>, <xref rid="bib49" ref-type="bibr">2024</xref>), correspondence-based measures (e.g., 3D point matching algorithms in computer vision; El Banani et al., <xref rid="bib19" ref-type="bibr">2024</xref>), not to mention explicit 3D shape representations such as triangular meshes and point clouds. An important direction for future work is determining the format of representation used by humans to complete 3D matching tasks and encapsulating such findings in computational models.</p><p>What kind of constraints should be used to design more biologically plausible models of 3D perception? The field has long considered late stages of processing within the ventral visual stream (VVS) as representing object shape (DiCarlo et al., <xref rid="bib17" ref-type="bibr">2012</xref>). More recent work suggests that the VVS may, in fact, simply provide a basis space of texture-like representations which can be used to infer shape-level object properties (Jagadeesh &#x00026; Gardner, <xref rid="bib28" ref-type="bibr">2022</xref>). This claim inverts a common critique of standard DNNs (e.g., Geirhos et al., <xref rid="bib22" ref-type="bibr">2018</xref>), suggesting that the VVS, like CNNs, is &#x02018;texture-biased.&#x02019; As such, it&#x02019;s possible that while some 3D shape properties might be represented within the VVS, there are neuroanatomical structures beyond the VVS which are critical for this ability. The dorsal stream, for example, has been implicated in 3D shape processing (Sereno et al., <xref rid="bib44" ref-type="bibr">2002</xref>; Van Dromme et al., <xref rid="bib53" ref-type="bibr">2016</xref>), suggesting that the sorts of structured 3D representations captured by DNNs may share capture more variance in dorsal neural responses than standard DNNs. In recent work, Bonnen et al. (<xref rid="bib10" ref-type="bibr">2021</xref>) has shown that medial temporal cortex (MTC), downstream from the VVS, plays a causal role in 3D object perception (Bonnen et al., <xref rid="bib10" ref-type="bibr">2021</xref>). Moreover, these inferences depend on reliable visuospatial sampling policies (Bonnen et al., <xref rid="bib9" ref-type="bibr">2023</xref>), suggesting that there are behavioral signatures of shape inferences. These data provide clear neuroanatomical and algorithmic constraints on models which aim for more human-like 3D object perception.</p><p>In summary, we have shown that DNNs with multi-view learning objectives can approximate human 3D shape inferences for object types included in the DNN training distribution, but show mixed results when generalizing to out-of-distribution shapes. The limitations of standard DNN models are well recognized, yet it has been unclear what model components (e.g., architecture, training objective, or dataset) leads to their misalignment with human 3D shape inferences. The field has historically focused on the role of model architecture (Richards et al., <xref rid="bib42" ref-type="bibr">2019</xref>; Yamins &#x00026; DiCarlo, <xref rid="bib56" ref-type="bibr">2016</xref>), yet in recent years the role of training data has become increasingly prominent (Conwell et al., <xref rid="bib14" ref-type="bibr">2022</xref>). Our work extends this line of reasoning into the domain of 3D shape perception by independently manipulating the effects of architecture and dataset. Of course, there are obvious differences between the current 3D models and biological brains, but resolving these engineering challenges is a critical step in demonstrating that DNNs are capable of human-aligned 3D inference. We leave it to future work to use neural and behavioral constraints, DNN approaches, and other modeling frameworks, such as probabilistic programming, to design and evaluate more biologically plausible computational models of human 3D object perception.</p></sec><sec id="sec12"><title>METHODS</title><sec id="sec13"><title>3D Shape Datasets</title><p>ShapeNet is a large collection of 3D object meshes drawn from a variety of manmade categories. We use the 13 largest ShapeNet categories, with a total of 39301 objects across all categories. The categories are: airplanes, tables, cars, lamps, chairs, stereos, benches, phones, guns, couches, cabinets, boats, monitors. 30565 objects were assigned to a training set for learning 3D models, and 8736 were assigned to the test set. All objects used in the match-to-sample tasks for humans and models were drawn from the test set for all experiments. Using Blender (Blender Online Community, <xref rid="bib7" ref-type="bibr">2018</xref>), we rendered 50 images of each object from viewpoints randomly sampled from a sphere around the object. The objects were rendered matte grey without color or textures to emphasize shape processing in the models and experiments.</p><p>ShapeGenerator (ShapeGen) is a tool for procedurally generating abstract objects in Blender (<ext-link xlink:href="https://blendermarket.com/products/shape-generator" ext-link-type="uri">https://blendermarket.com/products/shape-generator</ext-link>). It provides a simple interface through which a user can generate abstract objects by fusing together several simpler shapes of different sizes, each of which can be rotated, beveled, smoothed, or extruded according to predefined user specifications. To generate the shapes in our dataset, we predefined ranges for several parameters and sampled values uniformly within those ranges, which provided smooth and interesting variation across shapes. More concretely, our shapes were generated by taking a single cube as a base object (whose initial length and width is set by a random seed), extruding a random face on that object between 5&#x02013;10 times (each time at a random angle, and random length), and rounding the edges by applying a catmull-clark modifier to the resulting mesh, to create a smooth shape. 10000 objects were generated. As with ShapeNet, all objects used in the match-to-sample tasks were drawn from the test set, Blender was used to render 25 viewpoints of each object, and objects were rendered matte grey. The viewpoints for ShapeGen objects were sampled randomly from a sphere around each object.</p></sec><sec id="sec14"><title>Human Behavioral Experiments</title><p>Human behavioral experiments were conducted online using Prolific. Participants were each paid $15/hr for participating. Experiments were approved by the MIT Committee on the Use of Humans as Experimental Subjects. Participants were notified of their rights before the experiment, and were free to terminate participation at any time by closing the browser window. Experiments consisted of an initial set of instructions, 6 practice trials with feedback, and 150 main trials with no feedback. Participants were not able to progress to the main experiment until they successfully completed all 6 practice trials. Each experiment took an average of 15 minutes for participants to complete. Experiments were presented to participants using a standard JsPsych toolkit (de Leeuw et al., <xref rid="bib16" ref-type="bibr">2023</xref>). Participants were screened for participation in prior studies in this paper, so each participant only appears once and in just one experiment.</p><p>For all experiments, trials were constructed by assigning two objects to a pair. For each pair of objects, two trials were constructed, one with object A as the sample and target and B as the lure, and one vice versa. In the behavioral experiments, participants were split into two batches and each batch only saw one trial for a given object pair ensuring that all objects were novel for each trial.</p></sec><sec id="sec15"><title>Model Zoo of Standard DNNs</title><p>The model zoo of standard DNNs consisted of 25 pretrained convolutional neural networks (CNNs) and 25 pretrained visual transformers (ViTs).</p><p>The 25 CNNs were pretrained for object classification on ImageNet and downloaded from PyTorch (<ext-link xlink:href="http://pytorch.org" ext-link-type="uri">pytorch.org</ext-link>). The models used were: alexnet, densenet121, densenet161, densenet169, densenet201, resnet101, resnet101wide, resnet152, resnet18, resnet34, resnet50, resnet50wide, resnext101, resnext50, shufflenetv2, squeezenet1.0, squeezenet1.1, vgg11, vgg11bn, vgg13, vgg13bn, vgg16, vgg16bn, vgg19, vgg19bn. For all analyses, the unit activity for the penultimate layer before the classification layer were extracted.</p><p>The 25 ViTs were pretrained on ImageNet and downloaded from the timm (<ext-link xlink:href="http://timm.fast.ai" ext-link-type="uri">timm.fast.ai</ext-link>) package. As with the CNNs, all models were pretrained for object classification on ImageNet. The models used were: convit_base, convit_small, convit_tiny, mvitv2_base, mvitv2_base_cls, mvitv2_huge_cls, mvitv2_large, mvitv2_large_cls, mvitv2_small, vit_base_patch16_224, vit_base_patch16_clip_224, vit_base_patch32_clip_224, vit_base_patch8_224, vit_base_r50_s16_224, vit_large_patch14_clip_224, vit_large_patch32_224, vit_relpos_base_patch16_224, vit_relpos_base_patch16_clsgap_224, vit_relpos_medium_patch16_224, vit_relpos_medium_patch16_rpn_224, vit_small_patch16_224, vit_small_r26_s32_224, vit_srelpos_medium_patch16_224, vit_srelpos_small_patch16_224, vit_tiny_r_s16_p8_224. For all analyses, the unit activity for the penultimate layer before the classification layer were extracted.</p></sec><sec id="sec16"><title>3D Light Field Networks</title><p>The essence of a neural field generally is a neural network model that takes some set of coordinates as input and outputs a property at those coordinates. Common examples of fields are magnetic or gravitational fields, both of which encode the continuous function of magnetic or gravitational force across space. In computer graphics, 3D shape fields are learned by mapping from coordinates in space (e.g., xyz, ray coordinates) to color or volume properties at that location. Once the neural network is trained, an image depicting the encoded space from a given viewpoint can be rendered by querying the neural field to recover shape/color. The most commonly used neural fields, neural radiance fields (NeRF) (Mildenhall et al., <xref rid="bib36" ref-type="bibr">2021</xref>), take xyz coordinates as input and output RGB and volume density at the queried coordinate. The field is then queried using a volumetric renderer to produce images of the scene from a given viewpoint. NeRFS have been used in computer graphics to create photorealistic encodings of objects and scenes by overfitting the field with many (&#x0003e;100) views from an individual scene. More relevant for modeling perception, a class of neural fields called conditional neural fields compute the continuous 3D function for objects from images (conditional because the field is conditioned on the input image). The models used here fall into this latter camp.</p><p>3D Light Field Networks (3D-LFNs) (Sitzmann et al., <xref rid="bib46" ref-type="bibr">2021</xref>) are models that compute a continuous function defining the light field of a volume in space using neural networks. 3D-LFNs follow three stages, each with their own architecture: 1. Inferring 3D shape latents from images using a CNN encoder, 2. Predicting the weights of the neural light field from the shape latents using hypernetworks, and 3. Using the neural light field to render an image from a given camera viewpoint. These models are part of a broader class of models in computer vision called conditional neural fields (Xie et al., <xref rid="bib55" ref-type="bibr">2022</xref>), which estimate neural fields from images rather than fit them directly to a given scene with many viewpoints, as in common in computer graphics (Xie et al., <xref rid="bib55" ref-type="bibr">2022</xref>).</p><p>The inference stage (1) of the 3D-LFNs used here was implemented as a resnet50 CNN encoder. The final classification layers were removed, and a linear layer was used to map from the final convolutional layer to a 256D fully-connected layer that represents the 3D latent space for the model. To map from shape latents to the weights of the 3D light field (2), we used a hypernetwork. A hypernetwork is a class of neural networks used in metalearning (Ha et al., <xref rid="bib24" ref-type="bibr">2017</xref>) that is trained to predict the weights of another neural network. The hypernetwork is implemented as a series of two layer multi-layer perceptron (MLP) with 256 units per layer that takes the 256D shape latents as inputs and outputs the weights for one layer of the neural field. There is one hypernetwork for each adjacent pair of layers in the neural field. The neural light field itself is implemented as an 8 layer MLP. The first layer input is a six dimensions for the pl&#x000fc;cker coordinate defining a ray through the scene, the output layer is three dimensions for the RGB value of the ray, and the hidden layers have 256 hidden units. For the rendering procedure during training, a ground-truth camera matrix is provided, an image-plane defined at a fixed distance between the camera and object, and rays are queried such that they pass through the camera plane from the camera position. One ray is queried per pixel in the output render, and the RGB value returned by the neural field for a given ray becomes the RGB value for the pixel where that ray intersects the image plane. The loss for training the model is the mean-squared-error between the output render and the ground-truth image of the object from the same camera position. The loss is backpropogated to update the weights in the hypernetworks and CNN encoder.</p><p>For all 3D-LFN analyses, we extracted the shape latents and weights defining the 3D-LFN MLP as features. The 3D-LFN latents are a 256D fuly-connected layer linearlly read out from the CNN encoder. The 3D-LFN weights were dimensionality reduced to a 150D feature-space using PCA before subsequent analysis.</p></sec><sec id="sec17"><title>Autoencoders</title><p>We implemented 2 different types of autoencoders. The first is a standard autoencoder that takes an image as an input, runs it through a resnet50 encoder and maps to an 256D latent space, then uses a series of deconvolutional layers to map from the latent space back to reconstruct the input image. In our experiments, we use this as a single-view variant of the autoencoder. We also train a multi-view autoencoder that identical to the single-view version, but modified to support a multi-view learning objective. In this version, the input image and output rendering target are two different viewpoints of the same object. The camera matrix embedding appended to the latent space defines the target output viewpoint the deconvolutional block should reconstruct. These models were implemented with custom PyTorch code.</p><p>For all autoencoder models, the match-to-sample tasks were completed using the same similarity-based measure as all other models applied to the ND latent space.</p></sec><sec id="sec18"><title>Multi-View CNN</title><p>For the multi-view CNN, we used a standard resnet50 CNN architecture with a modified Momentum Contrast (MOCO) style learning objective (He et al., <xref rid="bib26" ref-type="bibr">2020</xref>). In standard MOCO models, a target image is augmented in two ways (possible manipulations: random cropping, random cropping + resizing, color jittering, random greyscale conversion, random horizontal flip, random gaussian blur). Each augmented image is passed through the same resnet50 and mapped to an 2046D output latent space. The model&#x02019;s learning objective is to make the latents for the two ablated versions of the input image as similar as possible (positive samples) relative to augmented versions of other images in the same minibatch (negative samples). To modify this for a multi-view learning objective, we instead provide the model with two images of the same object from different viewpoints (without any augmentations). The model&#x02019;s objective then becomes to make the two output latents for different viewpoints of the same object as similar as possible relative to negative samples from un-augmented images of other objects in the minibatch. We used a modified version of the implementation in the PyContrast repository (<ext-link xlink:href="http://github.com/HobbitLong/PyContrast" ext-link-type="uri">github.com/HobbitLong/PyContrast</ext-link>). To train a single-view version of the CNN, we train a ShapeNet classification resnet50 (13 classes) using a cross-entropy classification loss.</p></sec><sec id="sec19"><title>Model Alignment to Human Behavior</title><p>Human and model behaivor was compared in a trial-level fashion using cosine similarity. Average trial-level human accuracies were computed across participants for each trial, producing a vector of accuracies. For models, the binary correct/incorrect performance for each trial was also arranged into a vector over trials. Similarity to Human Behavior was computed as one minus the cosine distance between the human and model accuracy vectors.</p><p>The human noise-ceiling was computed using a similar procedure within the human data. In a leave one out fashion, the similarity between the binary correct/incorrect vector for one participant and the average accuracies for the remaining participants was computed. This is done for each participant, and the average across participants is the noise-ceiling for the model comparisons.</p></sec><sec id="sec20"><title>Random Observer Baseline</title><p>As human accuracy increases on our 3D matching tasks, there are less trials for discrepancies between the models and humans for the trial-wise similarity measure to detect. At a minimum, we want to ensure that each model&#x02019;s trial-wise similarity to humans is higher than would be expected if the correct/incorrect responses were randomly distributed over trials. To provide such a reference baseline when plotting these results, we compute a random observer baseline distribution that shows the expected trial-wise similarity to humans across accuracies if the correct and incorrect responses were randomly distributed over trials. For each behavioral experiment, we simulated 1000 participant responses with their corresponding correct/incorrect responses randomly distributed over trials for each possible accuracy (0.01 to 1 at intervals of .01). We computed trial-wise cosine similarity between each of the 1000 simulated subjects/models to the average human behavioral accuracies. The shaded baseline distribution plotted in each results figure is the 95% confidence interval over the 1000 simulations for each accuracy.</p></sec></sec><sec id="sec21"><title>ACKNOWLEDGMENTS</title><p>We thank Kevin Smith and Ratan Murty for helpful feedback on this work.</p></sec><sec id="sec22"><title>FUNDING INFORMATION</title><p>This work was supported by National Institutes of Health grant DP1HD091947 awarded to Nancy Kanwisher, Office of Naval Research MURI grant PO #BB01540322 awarded in part to Joshua B. Tenenbaum, and the Center for Brains, Minds, and Machines (CBMM) funded by NSF STC award CCF-1231216.</p></sec><sec id="sec23"><title>AUTHOR CONTRIBUTIONS</title><p>TPO, JBT, and NK conceived of the research. TPO, NK, and TB designed the experiments with input from JBT. YF and TPO collected the human behavioral data. TPO constructed the models with assistance from AT and VS. TPO conducted the modeling and behavioral analyses with input from TB. TPO, NK, TB, and JBT interpreted the results. TPO and TB wrote the paper with input from NGK, JBT, YF, and VS.</p></sec><sec sec-type="data-availability" id="sec24"><title>DATA AVAILABILITY STATEMENT</title><p>The software packages and datasets used to create all stimuli are available online:<list list-type="bullet"><list-item><p>ShapeNet.v2 Dataset (<ext-link xlink:href="http://shapenet.org" ext-link-type="uri">shapenet.org</ext-link>)</p></list-item><list-item><p>Blender (<ext-link xlink:href="http://blender.org" ext-link-type="uri">blender.org</ext-link>)</p></list-item><list-item><p>ShapeGenerator (<ext-link xlink:href="https://blendermarket.com/products/shape-generator" ext-link-type="uri">https://blendermarket.com/products/shape-generator</ext-link>)</p></list-item></list>The following Python packages were used for the modeling:<list list-type="bullet"><list-item><p>All analyses were completed using Python (<ext-link xlink:href="http://python.org" ext-link-type="uri">python.org</ext-link>)</p></list-item><list-item><p>Code to train Light Field Networks is available on GitHub (<ext-link xlink:href="http://github.com/vsitzmann/light-field-networks" ext-link-type="uri">github.com/vsitzmann/light-field-networks</ext-link>)</p></list-item><list-item><p>The ImageNet CNNs are available in Pytorch (<ext-link xlink:href="http://pytorch.org" ext-link-type="uri">pytorch.org</ext-link>)</p></list-item><list-item><p>The ImageNet ViTs are available in timm (<ext-link xlink:href="http://timm.fast.ai" ext-link-type="uri">timm.fast.ai</ext-link>)</p></list-item><list-item><p>The Multi-View CNN was trained with a modified version of PyContrast (<ext-link xlink:href="http://github.com/HobbitLong/PyContrast" ext-link-type="uri">github.com/HobbitLong/PyContrast</ext-link>)</p></list-item><list-item><p>The DinoV2s are available on GitHub (<ext-link xlink:href="https://github.com/facebookresearch/dinov2" ext-link-type="uri">https://github.com/facebookresearch/dinov2</ext-link>)</p></list-item><list-item><p>The Stylized ImageNet models are available on GitHub (<ext-link xlink:href="https://github.com/rgeirhos/texture-vs-shape" ext-link-type="uri">https://github.com/rgeirhos/texture-vs-shape</ext-link>)</p></list-item></list>Behavioral data from <xref rid="sec9" ref-type="sec">3D-LFNs and Humans Make Similar 3D Shape Judgements for &#x0201c;Adversarial&#x0201d; CNN-Defined Matching Judgements</xref> section and similar data for evaluating multi-view consistency between models and humans is available at <ext-link xlink:href="https://huggingface.co/datasets/tzler/MOCHI" ext-link-type="uri">https://huggingface.co/datasets/tzler/MOCHI</ext-link> (Bonnen et al., <xref rid="bib8" ref-type="bibr">2024</xref>).</p></sec></body><back><ref-list><title>REFERENCES</title><ref id="bib1"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Abbas</surname>, <given-names>A.</given-names></string-name>, &#x00026; <string-name><surname>Deny</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Progress and limitations of deep networks to recognize objects in unusual poses</article-title>. In <source>Proceedings of the 37th AAAI Conference on Artificial Intelligence</source> (pp. <fpage>160</fpage>&#x02013;<lpage>168</lpage>). <publisher-name>AAAI Press</publisher-name>. <pub-id pub-id-type="doi">10.1609/aaai.v37i1.25087</pub-id></mixed-citation></ref><ref id="bib2"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Alcorn</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Gong</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Mai</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ku</surname>, <given-names>W.-S.</given-names></string-name>, &#x00026; <string-name><surname>Nguyen</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects</article-title>. In <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source> (pp. <fpage>4840</fpage>&#x02013;<lpage>4849</lpage>). <publisher-name>IEEE</publisher-name>. <pub-id pub-id-type="doi">10.1109/CVPR.2019.00498</pub-id></mixed-citation></ref><ref id="bib3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aubret</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ernst</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Teuli&#x000e8;re</surname>, <given-names>C.</given-names></string-name>, &#x00026; <string-name><surname>Triesch</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Time to augment self-supervised visual representation learning</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.2207.13492</pub-id></mixed-citation></ref><ref id="bib4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baker</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Erlikhman</surname>, <given-names>G.</given-names></string-name>, &#x00026; <string-name><surname>Kellman</surname>, <given-names>P. J.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Deep convolutional networks do not classify based on global object shape</article-title>. <source>PLoS Computational Biology</source>, <volume>14</volume>(<issue>12</issue>), <fpage>e1006613</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006613</pub-id>, <pub-id pub-id-type="pmid">30532273</pub-id>
</mixed-citation></ref><ref id="bib5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biederman</surname>, <given-names>I.</given-names></string-name></person-group> (<year>1987</year>). <article-title>Recognition-by-components: A theory of human image understanding</article-title>. <source>Psychological Review</source>, <volume>94</volume>(<issue>2</issue>), <fpage>115</fpage>&#x02013;<lpage>147</lpage>. <pub-id pub-id-type="doi">10.1037/0033-295X.94.2.115</pub-id>, <pub-id pub-id-type="pmid">3575582</pub-id>
</mixed-citation></ref><ref id="bib6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biscione</surname>, <given-names>V.</given-names></string-name>, &#x00026; <string-name><surname>Bowers</surname>, <given-names>J. S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Learning online visual invariances for novel objects via supervised and self-supervised training</article-title>. <source>Neural Networks</source>, <volume>150</volume>, <fpage>222</fpage>&#x02013;<lpage>236</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2022.02.017</pub-id>, <pub-id pub-id-type="pmid">35334437</pub-id>
</mixed-citation></ref><ref id="bib7"><mixed-citation publication-type="book"><collab collab-type="author">Blender Online Community</collab>. (<year>2018</year>). <source>Blender - A 3D modelling and rendering package</source>. <publisher-name>Stichting Blender Foundation</publisher-name>. <ext-link xlink:href="https://www.blender.org" ext-link-type="uri">https://www.blender.org</ext-link></mixed-citation></ref><ref id="bib8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonnen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Fu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bai</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>O&#x02019;Connell</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Friedman</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name>, &#x00026; <string-name><surname>Efros</surname>, <given-names>A. A.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Evaluating multiview object consistency in humans and image models</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.2409.05862</pub-id></mixed-citation></ref><ref id="bib9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonnen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wagner</surname>, <given-names>A. D.</given-names></string-name>, &#x00026; <string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Medial temporal cortex supports compositional visual inferences</article-title>. <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2023.09.07.556737</pub-id></mixed-citation></ref><ref id="bib10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonnen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name>, &#x00026; <string-name><surname>Wagner</surname>, <given-names>A. D.</given-names></string-name></person-group> (<year>2021</year>). <article-title>When the ventral visual stream is not enough: A deep learning account of medial temporal lobe involvement in perception</article-title>. <source>Neuron</source>, <volume>109</volume>(<issue>17</issue>), <fpage>2755</fpage>&#x02013;<lpage>2766</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2021.06.018</pub-id>, <pub-id pub-id-type="pmid">34265252</pub-id>
</mixed-citation></ref><ref id="bib11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>B&#x000fc;lthoff</surname>, <given-names>H. H.</given-names></string-name>, &#x00026; <string-name><surname>Mallot</surname>, <given-names>H. A.</given-names></string-name></person-group> (<year>1988</year>). <article-title>Integration of depth modules: Stereo and shading</article-title>. <source>Journal of the Optical Society of America A</source>, <volume>5</volume>(<issue>10</issue>), <fpage>1749</fpage>&#x02013;<lpage>1758</lpage>. <pub-id pub-id-type="doi">10.1364/JOSAA.5.001749</pub-id>, <pub-id pub-id-type="pmid">3204438</pub-id>
</mixed-citation></ref><ref id="bib12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chang</surname>, <given-names>A. X.</given-names></string-name>, <string-name><surname>Funkhouser</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Guibas</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Hanrahan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Savarese</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Savva</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Su</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Xiao</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Yi</surname>, <given-names>L.</given-names></string-name>, &#x00026; <string-name><surname>Yu</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2015</year>). <source>ShapeNet: An information-rich 3D model repository</source>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1512.03012</pub-id></mixed-citation></ref><ref id="bib13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clerkin</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Hart</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Rehg</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name>, &#x00026; <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Real-world visual statistics and infants&#x02019; first-learned object names</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>372</volume>(<issue>1711</issue>), <fpage>20160055</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2016.0055</pub-id>, <pub-id pub-id-type="pmid">27872373</pub-id>
</mixed-citation></ref><ref id="bib14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Conwell</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Prince</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Alvarez</surname>, <given-names>G. A.</given-names></string-name>, &#x00026; <string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2022</year>). <article-title>What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?</article-title>
<source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2022.03.28.485868</pub-id></mixed-citation></ref><ref id="bib15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cooper</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Boix</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Harari</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Madan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pfister</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Sasaki</surname>, <given-names>T.</given-names></string-name>, &#x00026; <string-name><surname>Sinha</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Emergent neural network mechanisms for generalization to objects in novel orientations</article-title>
<source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.2109.13445</pub-id></mixed-citation></ref><ref id="bib16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de Leeuw</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Gilbert</surname>, <given-names>R. A.</given-names></string-name>, &#x00026; <string-name><surname>Luchterhandt</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2023</year>). <article-title>jsPsych: Enabling an open-source collaborative ecosystem of behavioral experiments</article-title>. <source>Journal of Open Source Software</source>, <volume>8</volume>(<issue>85</issue>), <fpage>5351</fpage>. <pub-id pub-id-type="doi">10.21105/joss.05351</pub-id></mixed-citation></ref><ref id="bib17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Zoccolan</surname>, <given-names>D.</given-names></string-name>, &#x00026; <string-name><surname>Rust</surname>, <given-names>N. C.</given-names></string-name></person-group> (<year>2012</year>). <article-title>How does the brain solve visual object recognition?</article-title>
<source>Neuron</source>, <volume>73</volume>(<issue>3</issue>), <fpage>415</fpage>&#x02013;<lpage>434</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id>, <pub-id pub-id-type="pmid">22325196</pub-id>
</mixed-citation></ref><ref id="bib18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Doerig</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sommers</surname>, <given-names>R. P.</given-names></string-name>, <string-name><surname>Seeliger</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Richards</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ismael</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lindsay</surname>, <given-names>G. W.</given-names></string-name>, <string-name><surname>Kording</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Konkle</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>van Gerven</surname>, <given-names>M. A. J.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &#x00026; <string-name><surname>Kietzmann</surname>, <given-names>T. C.</given-names></string-name></person-group> (<year>2023</year>). <article-title>The neuroconnectionist research programme</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>24</volume>(<issue>7</issue>), <fpage>431</fpage>&#x02013;<lpage>450</lpage>. <pub-id pub-id-type="doi">10.1038/s41583-023-00705-w</pub-id>, <pub-id pub-id-type="pmid">37253949</pub-id>
</mixed-citation></ref><ref id="bib19"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>El Banani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Raj</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Maninis</surname>, <given-names>K.-K.</given-names></string-name>, <string-name><surname>Kar</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Rubinstein</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Guibas</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>J.</given-names></string-name>, &#x00026; <string-name><surname>Jampani</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2024</year>). <article-title>Probing the 3D awareness of visual foundation models</article-title>. In <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source> (pp. <fpage>21795</fpage>&#x02013;<lpage>21806</lpage>). <publisher-name>IEEE</publisher-name>. <pub-id pub-id-type="doi">10.1109/CVPR52733.2024.02059</pub-id></mixed-citation></ref><ref id="bib20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fausey</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Jayaraman</surname>, <given-names>S.</given-names></string-name>, &#x00026; <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>2016</year>). <article-title>From faces to hands: Changing visual input in the first two years</article-title>. <source>Cognition</source>, <volume>152</volume>, <fpage>101</fpage>&#x02013;<lpage>107</lpage>. <pub-id pub-id-type="doi">10.1016/j.cognition.2016.03.005</pub-id>, <pub-id pub-id-type="pmid">27043744</pub-id>
</mixed-citation></ref><ref id="bib21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Geirhos</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Jacobsen</surname>, <given-names>J.-H.</given-names></string-name>, <string-name><surname>Michaelis</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zemel</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Brendel</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &#x00026; <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Shortcut learning in deep neural networks</article-title>. <source>Nature Machine Intelligence</source>, <volume>2</volume>(<issue>11</issue>), <fpage>665</fpage>&#x02013;<lpage>673</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-020-00257-z</pub-id></mixed-citation></ref><ref id="bib22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Geirhos</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Rubisch</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Michaelis</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name>, &#x00026; <string-name><surname>Brendel</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2018</year>). <article-title>ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1811.12231</pub-id></mixed-citation></ref><ref id="bib23"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gibson</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2014</year>). <source>The ecological approach to visual perception: Classic edition</source>. <publisher-name>Psychology Press</publisher-name>. <pub-id pub-id-type="doi">10.4324/9781315740218</pub-id></mixed-citation></ref><ref id="bib24"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ha</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>A. M.</given-names></string-name>, &#x00026; <string-name><surname>Le</surname>, <given-names>Q. V.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Hypernetworks</article-title>. In <source>International Conference on Learning Representations (ICLR)</source>.</mixed-citation></ref><ref id="bib25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hassanin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Khan</surname>, <given-names>S.</given-names></string-name>, &#x00026; <string-name><surname>Tahtali</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Visual affordance and function understanding: A survey</article-title>. <source>ACM Computing Surveys (CSUR)</source>, <volume>54</volume>(<issue>3</issue>), <fpage>1</fpage>&#x02013;<lpage>35</lpage>. <pub-id pub-id-type="doi">10.1145/3446370</pub-id></mixed-citation></ref><ref id="bib26"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Fan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Xie</surname>, <given-names>S.</given-names></string-name>, &#x00026; <string-name><surname>Girshick</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Momentum contrast for unsupervised visual representation learning</article-title>. In <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source> (pp. <fpage>9726</fpage>&#x02013;<lpage>9735</lpage>). <publisher-name>IEEE</publisher-name>. <pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00975</pub-id></mixed-citation></ref><ref id="bib27"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hermann</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>T.</given-names></string-name>, &#x00026; <string-name><surname>Kornblith</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2020</year>). <article-title>The origins and prevalence of texture bias in convolutional neural networks</article-title>. In <source>Proceedings of the 34th International Conference on Neural Information Processing Systems</source> (pp. <fpage>19000</fpage>&#x02013;<lpage>19015</lpage>). <publisher-name>Curran Associates Inc.</publisher-name></mixed-citation></ref><ref id="bib28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jagadeesh</surname>, <given-names>A. V.</given-names></string-name>, &#x00026; <string-name><surname>Gardner</surname>, <given-names>J. L.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Texture-like representation of objects in human visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>119</volume>(<issue>17</issue>), <fpage>e2115302119</fpage>. <pub-id pub-id-type="doi">10.1073/pnas.2115302119</pub-id>, <pub-id pub-id-type="pmid">35439063</pub-id>
</mixed-citation></ref><ref id="bib29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Janssen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Vogels</surname>, <given-names>R.</given-names></string-name>, &#x00026; <string-name><surname>Orban</surname>, <given-names>G. A.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Selectivity for 3D shape that reveals distinct areas within macaque inferior temporal cortex</article-title>. <source>Science</source>, <volume>288</volume>(<issue>5473</issue>), <fpage>2054</fpage>&#x02013;<lpage>2056</lpage>. <pub-id pub-id-type="doi">10.1126/science.288.5473.2054</pub-id>, <pub-id pub-id-type="pmid">10856221</pub-id>
</mixed-citation></ref><ref id="bib30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jayaraman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fausey</surname>, <given-names>C. M.</given-names></string-name>, &#x00026; <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>2015</year>). <article-title>The faces in infant-perspective scenes change over the first year of life</article-title>. <source>PLOS ONE</source>, <volume>10</volume>(<issue>5</issue>), <fpage>e0123780</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0123780</pub-id>, <pub-id pub-id-type="pmid">26016988</pub-id>
</mixed-citation></ref><ref id="bib31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jayaraman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fausey</surname>, <given-names>C. M.</given-names></string-name>, &#x00026; <string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Why are faces denser in the visual experiences of younger than older infants?</article-title>
<source>Developmental Psychology</source>, <volume>53</volume>(<issue>1</issue>), <fpage>38</fpage>&#x02013;<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1037/dev0000230</pub-id>, <pub-id pub-id-type="pmid">28026190</pub-id>
</mixed-citation></ref><ref id="bib32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kersten</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mamassian</surname>, <given-names>P.</given-names></string-name>, &#x00026; <string-name><surname>Yuille</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2004</year>). <article-title>Object perception as Bayesian inference</article-title>. <source>Annual Review of Psychology</source>, <volume>55</volume>, <fpage>271</fpage>&#x02013;<lpage>304</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.psych.55.090902.142005</pub-id>, <pub-id pub-id-type="pmid">14744217</pub-id>
</mixed-citation></ref><ref id="bib33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khaligh-Razavi</surname>, <given-names>S.-M.</given-names></string-name>, &#x00026; <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title>. <source>PLoS Computational Biology</source>, <volume>10</volume>(<issue>11</issue>), <fpage>e1003915</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id>, <pub-id pub-id-type="pmid">25375136</pub-id>
</mixed-citation></ref><ref id="bib34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Deep neural networks: A new framework for modeling biological vision and brain information processing</article-title>. <source>Annual Review of Vision Science</source>, <volume>1</volume>, <fpage>417</fpage>&#x02013;<lpage>446</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035447</pub-id>, <pub-id pub-id-type="pmid">28532370</pub-id>
</mixed-citation></ref><ref id="bib35"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Marr</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2010</year>). <source>Vision: A computational investigation into the human representation and processing of visual information</source>. <publisher-name>MIT Press</publisher-name>. <pub-id pub-id-type="doi">10.7551/mitpress/9780262514620.001.0001</pub-id></mixed-citation></ref><ref id="bib36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mildenhall</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Srinivasan</surname>, <given-names>P. P.</given-names></string-name>, <string-name><surname>Tancik</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Barron</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Ramamoorthi</surname>, <given-names>R.</given-names></string-name>, &#x00026; <string-name><surname>Ng</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2021</year>). <article-title>NeRF: Representing scenes as neural radiance fields for view synthesis</article-title>. <source>Communications of the ACM</source>, <volume>65</volume>(<issue>1</issue>), <fpage>99</fpage>&#x02013;<lpage>106</lpage>. <pub-id pub-id-type="doi">10.1145/3503250</pub-id></mixed-citation></ref><ref id="bib37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Oquab</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Darcet</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Moutakanni</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Vo</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Szafraniec</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Khalidov</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Fernandez</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Haziza</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Massa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>El-Nouby</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Assran</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ballas</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Galuba</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Howes</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>P.-Y.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>S.-W.</given-names></string-name>, <string-name><surname>Misra</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Rabbat</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sharma</surname>, <given-names>V.</given-names></string-name>, &#x02026; <string-name><surname>Bojanowski</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2023</year>). <article-title>DINOv2: Learning robust visual features without supervision</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.2304.07193</pub-id></mixed-citation></ref><ref id="bib38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pizlo</surname>, <given-names>Z.</given-names></string-name></person-group> (<year>2001</year>). <article-title>Perception viewed as an inverse problem</article-title>. <source>Vision Research</source>, <volume>41</volume>(<issue>24</issue>), <fpage>3145</fpage>&#x02013;<lpage>3161</lpage>. <pub-id pub-id-type="doi">10.1016/S0042-6989(01)00173-0</pub-id>, <pub-id pub-id-type="pmid">11711140</pub-id>
</mixed-citation></ref><ref id="bib39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rajalingham</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Issa</surname>, <given-names>E. B.</given-names></string-name>, <string-name><surname>Bashivan</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kar</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>K.</given-names></string-name>, &#x00026; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks</article-title>. <source>Journal of Neuroscience</source>, <volume>38</volume>(<issue>33</issue>), <fpage>7255</fpage>&#x02013;<lpage>7269</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0388-18.2018</pub-id>, <pub-id pub-id-type="pmid">30006365</pub-id>
</mixed-citation></ref><ref id="bib40"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Recht</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Roelofs</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>L.</given-names></string-name>, &#x00026; <string-name><surname>Shankar</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Do ImageNet classifiers generalize to ImageNet?</article-title> In <source>Proceedings of the 36th International Conference on Machine Learning</source> (<volume>Vol. 97</volume>, pp. <fpage>5389</fpage>&#x02013;<lpage>5400</lpage>). <publisher-name>PMLR</publisher-name>.</mixed-citation></ref><ref id="bib41"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Reizenstein</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shapovalov</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Henzler</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Sbordone</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Labatut</surname>, <given-names>P.</given-names></string-name>, &#x00026; <string-name><surname>Novotny</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Common objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction</article-title>. In <source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source> (pp. <fpage>10881</fpage>&#x02013;<lpage>10891</lpage>). <publisher-name>IEEE</publisher-name>. <pub-id pub-id-type="doi">10.1109/ICCV48922.2021.01072</pub-id></mixed-citation></ref><ref id="bib42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richards</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Lillicrap</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Beaudoin</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bogacz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Christensen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Clopath</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ponte Costa</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>de Berker</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ganguli</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gillon</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Hafner</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Kepecs</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Latham</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lindsay</surname>, <given-names>G. W.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>K. D.</given-names></string-name>, <string-name><surname>Naud</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Pack</surname>, <given-names>C. C.</given-names></string-name>, &#x02026; <string-name><surname>Kording</surname>, <given-names>K. P.</given-names></string-name></person-group> (<year>2019</year>). <article-title>A deep learning framework for neuroscience</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>(<issue>11</issue>), <fpage>1761</fpage>&#x02013;<lpage>1770</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id>, <pub-id pub-id-type="pmid">31659335</pub-id>
</mixed-citation></ref><ref id="bib43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schrimpf</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kubilius</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Murty</surname>, <given-names>N. A. R.</given-names></string-name>, <string-name><surname>Ajemian</surname>, <given-names>R.</given-names></string-name>, &#x00026; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Integrative benchmarking to advance neurally mechanistic models of human intelligence</article-title>. <source>Neuron</source>, <volume>108</volume>(<issue>3</issue>), <fpage>413</fpage>&#x02013;<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.040</pub-id>, <pub-id pub-id-type="pmid">32918861</pub-id>
</mixed-citation></ref><ref id="bib44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sereno</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Trinath</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Augath</surname>, <given-names>M.</given-names></string-name>, &#x00026; <string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Three-dimensional shape representation in monkey cortex</article-title>. <source>Neuron</source>, <volume>33</volume>(<issue>4</issue>), <fpage>635</fpage>&#x02013;<lpage>652</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(02)00598-6</pub-id>, <pub-id pub-id-type="pmid">11856536</pub-id>
</mixed-citation></ref><ref id="bib45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shepard</surname>, <given-names>R. N.</given-names></string-name>, &#x00026; <string-name><surname>Metzler</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1971</year>). <article-title>Mental rotation of three-dimensional objects</article-title>. <source>Science</source>, <volume>171</volume>(<issue>3972</issue>), <fpage>701</fpage>&#x02013;<lpage>703</lpage>. <pub-id pub-id-type="doi">10.1126/science.171.3972.701</pub-id>, <pub-id pub-id-type="pmid">5540314</pub-id>
</mixed-citation></ref><ref id="bib46"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sitzmann</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Rezchikov</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Freeman</surname>, <given-names>W. T.</given-names></string-name>, <string-name><surname>Tenenbaum</surname>, <given-names>J.</given-names></string-name>, &#x00026; <string-name><surname>Durand</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Light field networks: Neural scene representations with single-evaluation rendering</article-title>. In <source>Proceedings of the 35th International Conference on Neural Information Processing Systems</source> (pp. <fpage>19313</fpage>&#x02013;<lpage>19325</lpage>). <publisher-name>Curran Associates Inc.</publisher-name></mixed-citation></ref><ref id="bib47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smith</surname>, <given-names>L. B.</given-names></string-name>, <string-name><surname>Jayaraman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Clerkin</surname>, <given-names>E.</given-names></string-name>, &#x00026; <string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2018</year>). <article-title>The developing infant creates a curriculum for statistical learning</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>22</volume>(<issue>4</issue>), <fpage>325</fpage>&#x02013;<lpage>336</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2018.02.004</pub-id>, <pub-id pub-id-type="pmid">29519675</pub-id>
</mixed-citation></ref><ref id="bib48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Soska</surname>, <given-names>K. C.</given-names></string-name>, <string-name><surname>Adolph</surname>, <given-names>K. E.</given-names></string-name>, &#x00026; <string-name><surname>Johnson</surname>, <given-names>S. P.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Systems in development: Motor skill acquisition facilitates three-dimensional object completion</article-title>. <source>Developmental Psychology</source>, <volume>46</volume>(<issue>1</issue>), <fpage>129</fpage>&#x02013;<lpage>138</lpage>. <pub-id pub-id-type="doi">10.1037/a0014618</pub-id>, <pub-id pub-id-type="pmid">20053012</pub-id>
</mixed-citation></ref><ref id="bib49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stewart</surname>, <given-names>E. E. M.</given-names></string-name>, <string-name><surname>Fleming</surname>, <given-names>R. W.</given-names></string-name>, &#x00026; <string-name><surname>Sch&#x000fc;tz</surname>, <given-names>A. C.</given-names></string-name></person-group> (<year>2024</year>). <article-title>A simple optical flow model explains why certain object viewpoints are special</article-title>. <source>Proceedings of the Royal Society B</source>, <volume>291</volume>(<issue>2026</issue>), <fpage>20240577</fpage>. <pub-id pub-id-type="doi">10.1098/rspb.2024.0577</pub-id>, <pub-id pub-id-type="pmid">38981528</pub-id>
</mixed-citation></ref><ref id="bib50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stewart</surname>, <given-names>E. E. M.</given-names></string-name>, <string-name><surname>Hartmann</surname>, <given-names>F. T.</given-names></string-name>, <string-name><surname>Morgenstern</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Storrs</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Maiello</surname>, <given-names>G.</given-names></string-name>, &#x00026; <string-name><surname>Fleming</surname>, <given-names>R. W.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Mental object rotation based on two-dimensional visual representations</article-title>. <source>Current Biology</source>, <volume>32</volume>(<issue>21</issue>), <fpage>R1224</fpage>&#x02013;<lpage>R1225</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2022.09.036</pub-id>, <pub-id pub-id-type="pmid">36347228</pub-id>
</mixed-citation></ref><ref id="bib51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ullman</surname>, <given-names>S.</given-names></string-name></person-group> (<year>1979</year>). <article-title>The interpretation of structure from motion</article-title>. <source>Proceedings of the Royal Society of London, Series B: Biological Sciences</source>, <volume>203</volume>(<issue>1153</issue>), <fpage>405</fpage>&#x02013;<lpage>426</lpage>. <pub-id pub-id-type="doi">10.1098/rspb.1979.0006</pub-id>, <pub-id pub-id-type="pmid">34162</pub-id>
</mixed-citation></ref><ref id="bib52"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ullman</surname>, <given-names>S.</given-names></string-name></person-group> (<year>1987</year>). <article-title>Visual routines</article-title>. In <person-group person-group-type="editor"><string-name><given-names>M. A.</given-names>
<surname>Fischler</surname></string-name> &#x00026; <string-name><given-names>O.</given-names>
<surname>Firschein</surname></string-name></person-group> (Eds.), <source>Readings in computer vision: Issues, problem, principles, and paradigms</source> (pp. <fpage>298</fpage>&#x02013;<lpage>328</lpage>). <publisher-name>Elsevier</publisher-name>. <pub-id pub-id-type="doi">10.1016/B978-0-08-051581-6.50035-0</pub-id></mixed-citation></ref><ref id="bib53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Dromme</surname>, <given-names>I. C.</given-names></string-name>, <string-name><surname>Premereur</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Verhoef</surname>, <given-names>B.-E.</given-names></string-name>, <string-name><surname>Vanduffel</surname>, <given-names>W.</given-names></string-name>, &#x00026; <string-name><surname>Janssen</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Posterior parietal cortex drives inferotemporal activations during three-dimensional object vision</article-title>. <source>PLoS Biology</source>, <volume>14</volume>(<issue>4</issue>), <fpage>e1002445</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.1002445</pub-id>, <pub-id pub-id-type="pmid">27082854</pub-id>
</mixed-citation></ref><ref id="bib54"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wertheimer</surname>, <given-names>M.</given-names></string-name></person-group> (<year>1938</year>). <article-title>Laws of organization in perceptual forms</article-title>. In <person-group person-group-type="editor"><string-name><given-names>W. D.</given-names>
<surname>Ellis</surname></string-name></person-group> (Ed.), <source>A source book of Gestalt psychology</source> (pp. <fpage>71</fpage>&#x02013;<lpage>88</lpage>). <publisher-name>Kegan Paul, Trench, Trubner &#x00026; Company</publisher-name>. <pub-id pub-id-type="doi">10.1037/11496-005</pub-id></mixed-citation></ref><ref id="bib55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xie</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Takikawa</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Saito</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Litany</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Yan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Khan</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Tombari</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Tompkin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sitzmann</surname>, <given-names>V.</given-names></string-name>, &#x00026; <string-name><surname>Sridhar</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Neural fields in visual computing and beyond</article-title>. <source>Computer Graphics Forum</source>, <volume>41</volume>(<issue>2</issue>), <fpage>641</fpage>&#x02013;<lpage>676</lpage>. <pub-id pub-id-type="doi">10.1111/cgf.14505</pub-id></mixed-citation></ref><ref id="bib56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name>, &#x00026; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>(<issue>3</issue>), <fpage>356</fpage>&#x02013;<lpage>365</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4244</pub-id>, <pub-id pub-id-type="pmid">26906502</pub-id>
</mixed-citation></ref><ref id="bib57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yamins</surname>, <given-names>D. L. K.</given-names></string-name>, <string-name><surname>Hong</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Cadieu</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Solomon</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Seibert</surname>, <given-names>D.</given-names></string-name>, &#x00026; <string-name><surname>DiCarlo</surname>, <given-names>J. J.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>111</volume>(<issue>23</issue>), <fpage>8619</fpage>&#x02013;<lpage>8624</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id>, <pub-id pub-id-type="pmid">24812127</pub-id>
</mixed-citation></ref><ref id="bib58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yildirim</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Belledonne</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Freiwald</surname>, <given-names>W.</given-names></string-name>, &#x00026; <string-name><surname>Tenenbaum</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Efficient inverse graphics in biological face processing</article-title>. <source>Science Advances</source>, <volume>6</volume>(<issue>10</issue>), <fpage>eaax5979</fpage>. <pub-id pub-id-type="doi">10.1126/sciadv.aax5979</pub-id>, <pub-id pub-id-type="pmid">32181338</pub-id>
</mixed-citation></ref><ref id="bib59"><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ye</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Tancik</surname>, <given-names>M.</given-names></string-name>, &#x00026; <string-name><surname>Kanazawa</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2021</year>). <article-title>pixelNeRF: Neural radiance fields from one or few images</article-title>. In <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source> (pp. <fpage>4576</fpage>&#x02013;<lpage>4585</lpage>). <publisher-name>IEEE</publisher-name>. <pub-id pub-id-type="doi">10.1109/CVPR46437.2021.00455</pub-id></mixed-citation></ref><ref id="bib60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuille</surname>, <given-names>A.</given-names></string-name>, &#x00026; <string-name><surname>Kersten</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Vision as Bayesian inference: Analysis by synthesis?</article-title>
<source>Trends in Cognitive Sciences</source>, <volume>10</volume>(<issue>7</issue>), <fpage>301</fpage>&#x02013;<lpage>308</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2006.05.002</pub-id>, <pub-id pub-id-type="pmid">16784882</pub-id>
</mixed-citation></ref><ref id="bib61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hardt</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Recht</surname>, <given-names>B.</given-names></string-name>, &#x00026; <string-name><surname>Vinyals</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Understanding deep learning (still) requires rethinking generalization</article-title>. <source>Communications of the ACM</source>, <volume>64</volume>(<issue>3</issue>), <fpage>107</fpage>&#x02013;<lpage>115</lpage>. <pub-id pub-id-type="doi">10.1145/3446776</pub-id></mixed-citation></ref></ref-list></back></article>