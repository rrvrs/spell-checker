<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Chem Sci</journal-id><journal-id journal-id-type="iso-abbrev">Chem Sci</journal-id><journal-id journal-id-type="publisher-id">SC</journal-id><journal-id journal-id-type="coden">CSHCBM</journal-id><journal-title-group><journal-title>Chemical Science</journal-title></journal-title-group><issn pub-type="ppub">2041-6520</issn><issn pub-type="epub">2041-6539</issn><publisher><publisher-name>The Royal Society of Chemistry</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40271036</article-id><article-id pub-id-type="pmc">PMC12012632</article-id>
<article-id pub-id-type="publisher-id">d5sc00108k</article-id><article-id pub-id-type="doi">10.1039/d5sc00108k</article-id><article-categories><subj-group subj-group-type="heading"><subject>Chemistry</subject></subj-group></article-categories><title-group><article-title>Accurate prediction of the kinetic sequence of physicochemical states using generative artificial intelligence<xref rid="fn1" ref-type="fn">&#x02020;</xref></article-title><fn-group><fn id="fn1"><label>&#x02020;</label><p>Electronic supplementary information (ESI) available. See DOI: <uri xlink:href="https://doi.org/10.1039/d5sc00108k">https://doi.org/10.1039/d5sc00108k</uri></p></fn></fn-group></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Bera</surname><given-names>Palash</given-names></name><xref rid="affa" ref-type="aff">a</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-1090-5199</contrib-id><name><surname>Mondal</surname><given-names>Jagannath</given-names></name><xref rid="affa" ref-type="aff">a</xref></contrib><aff id="affa">
<label>a</label>
<institution>Tata Institute of Fundamental Research Hyderabad</institution>
<state>Telangana 500046</state>
<country>India</country>
<email>palashb@tifrh.res.in</email>
<email>jmondal@tifrh.res.in</email>
</aff></contrib-group><pub-date publication-format="electronic" date-type="pub" iso-8601-date="2025-04-10"><day>10</day><month>4</month><year>2025</year></pub-date><pub-date publication-format="electronic" date-type="collection" iso-8601-date="2025-05-21"><day>21</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>10</day><month>4</month><year>2025</year></pub-date><!--PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>.--><volume>16</volume><issue>20</issue><fpage>8735</fpage><lpage>8751</lpage><history><date date-type="received" iso-8601-date="2025-01-07"><day>7</day><month>1</month><year>2025</year></date><date date-type="accepted" iso-8601-date="2025-04-10"><day>10</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>This journal is &#x000a9; The Royal Society of Chemistry</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>The Royal Society of Chemistry</copyright-holder><ali:free_to_read xmlns:ali="http://www.niso.org/schemas/ali/1.0/"/><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense" start_date="2025-04-10">https://creativecommons.org/licenses/by/3.0/</ali:license_ref><license-p>This article is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution 3.0 Unported Licence</ext-link>. You can use material from this article in other publications without requesting further permissions from the RSC, provided that the correct acknowledgement is given.</license-p></license></permissions><abstract><p>Capturing the time evolution and predicting kinetic sequences of states of physicochemical systems present significant challenges due to the precision and computational effort required. In this study, we demonstrate that &#x02018;Generative Pre-trained Transformer (GPT)&#x02019;, an artificial intelligence model renowned for machine translation and natural language processing, can be effectively adapted to predict the dynamical state-to-state transition kinetics of biologically relevant physicochemical systems. Specifically, by using sequences of time-discretized states from Molecular Dynamics (MD) simulation trajectories akin to the vocabulary corpus of a language, we show that a GPT-based model can learn the complex syntactic and semantic relationships within the trajectory. This enables GPT to predict kinetically accurate sequences of states for a diverse set of biomolecules of varying complexity, at a much quicker pace than traditional MD simulations and with a better efficiency than other baseline time-series prediction approaches. More significantly, the approach is found to be equally adept at forecasting the time evolution of out-of-equilibrium active systems that do not maintain detailed balance. An analysis of the mechanism inherent in GPT reveals the crucial role of the &#x02018;self-attention mechanism&#x02019; in capturing the long-range correlations necessary for accurate state-to-state transition predictions. Together, our results highlight generative artificial intelligence's ability to generate kinetic sequences of states of physicochemical systems with statistical precision.</p></abstract><abstract abstract-type="toc"><p>GPT-based generative modeling of MD trajectories enables efficient prediction of state transitions by capturing long-range correlations, offering accurate kinetic and thermodynamic forecasts for diverse physicochemical systems.<graphic xlink:href="d5sc00108k-ga.jpg" id="ga" position="float"/></p></abstract><funding-group><award-group><funding-source>
<institution-wrap><institution>Department of Atomic Energy, Government of India</institution><institution-id institution-id-type="doi">10.13039/501100001502</institution-id></institution-wrap>
</funding-source><award-id>RTI 4007</award-id></award-group></funding-group><counts><page-count count="17"/></counts><custom-meta-group><custom-meta><meta-name>pubstatus</meta-name><meta-value>Paginated Article</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec><title>Introduction</title><p>The time evolution of any physical system undergoes various state-to-state transitions. Understanding the dynamics of these systems particularly at the molecular level poses a significant challenge due to the complexity of their transitions between various states. Traditional approaches, such as molecular dynamics simulations (MDs), offer valuable insights into these transitions. However, MDs are computationally very expensive, limiting their applicability to large-scale systems or long-term predictions. Moreover, describing the actual phase space of a physical system typically involves handling data of very high dimensions. The lower dimensional representation of this data along some order parameters can provide information about various states and transitions between them. Nonetheless, achieving a comprehensive understanding of these transitions over a very long time requires performing highly resource-intensive MD simulations.</p><p>To understand the long-timescale behavior from experimental and simulated trajectories, various kinetic models such as the Markov state model (MSM)<sup><xref rid="cit1" ref-type="bibr">1&#x02013;3</xref></sup> and Hidden Markov model (HMM)<sup><xref rid="cit4" ref-type="bibr">4&#x02013;6</xref></sup> were employed to predict state transitions and identify metastable states, thereby providing insights into the underlying mechanisms of molecular processes. The initial step in constructing these models involves discretizing trajectories into a specified number of states along some collective variables (CVs). To identify effective CVs for state decomposition, a range of techniques were employed, including linear methods like Principal Component Analysis (PCA)<sup><xref rid="cit7" ref-type="bibr">7&#x02013;9</xref></sup> and time-lagged independent component analysis (tICA),<sup><xref rid="cit10" ref-type="bibr">10&#x02013;12</xref></sup> as well as non-linear approaches, particularly machine learning (ML) techniques such as Autoencoders and time-lagged Autoencoders.<sup><xref rid="cit13" ref-type="bibr">13&#x02013;18</xref></sup> Recently, an ML approach known as VAMPnets<sup><xref rid="cit19" ref-type="bibr">19</xref></sup> has been proposed, which combines the principles of Autoencoders and tICA to learn molecular kinetics. Notably, VAMPnets offer the potential to streamline the entire, lengthy process of constructing the MSM by substituting it with a single deep neural network. Another approach, known as dynamic graphical models (DGMs),<sup><xref rid="cit20" ref-type="bibr">20</xref></sup> offers an efficient alternative for predicting molecular kinetics and unobserved configurations, utilizing fewer parameters than the traditional MSM. As the field advances, various deep generative state-less molecular simulation surrogates, including Boltzmann Generators,<sup><xref rid="cit21" ref-type="bibr">21</xref></sup> Normalizing Flows,<sup><xref rid="cit22" ref-type="bibr">22&#x02013;24</xref></sup> Implicit Transfer Operators (ITO),<sup><xref rid="cit25" ref-type="bibr">25</xref></sup> and Timewarp,<sup><xref rid="cit26" ref-type="bibr">26</xref></sup> have emerged as powerful tools for sampling and predicting molecular dynamics. These approaches aim to provide efficient and effective alternatives to conventional MD simulations by leveraging advanced computational techniques.</p><p>In recent years, state-of-the-art recurrent neural networks (RNNs) and large language models (LLMs) have become promising tools in addressing various challenges.<sup><xref rid="cit27" ref-type="bibr">27&#x02013;33</xref></sup> However, recurrent neural networks (RNNs) and their advanced variant, long short-term memory (LSTM) networks,<sup><xref rid="cit34" ref-type="bibr">34</xref></sup> excel at capturing sequential patterns in time-series data and sequence-to-sequence tasks, overcoming the vanishing gradient problem. However, these precedent methods face limitations in modeling complex syntactic and semantic relationships in large language models (LLMs). To overcome these issues, the pioneering work by Vaswani <italic toggle="yes">et al.</italic><sup><xref rid="cit33" ref-type="bibr">33</xref></sup> introduced the attention-based model known as Transformer. The concept of self-attention mechanisms can encode contextual information from the input text and generate coherent and contextually relevant responses. Although the original work of the Transformer was mainly designed for machine translation, the various parts of the Transformer can be used for different purposes. For instance, the encoder component can be applied to classification problems, while the decoder component can be used for sequence-to-sequence generation.</p><p>In our study, we have utilized the decoder component of the Transformer architecture to predict the kinetic sequence of states of diverse physicochemical as well as biologically relevant systems. Our investigations primarily focus on a set of systems of hierarchical complexity, ranging from hand-crafted three-state and four-state model potentials to a globular folded protein namely Trp-cage and an intrinsically disordered protein (IDP) &#x003b1;-synuclein. We demonstrate that our protocol can effectively learn the time evolution of different states in MD or other kinetic simulations along certain low-dimensional order parameters. As a result, the model can generate a sequence of states that are kinetically and thermodynamically accurate. Interestingly, the model is remarkably powerful, as it can accurately generate the sequence of states even for an active system that is out of equilibrium, as would be demonstrated for an active worm-like polymer chain and its passive counterpart. Moreover, for more complex systems, we have found that the attention mechanism plays a crucial role in maintaining the relationships between the states, enabling the model to generate the sequence of states correctly. Our results show that the GPT model outperforms traditional MSM and LSTM networks in predicting the kinetic sequence of states.</p></sec><sec><title>Results</title><sec><title>Learning molecular dynamics trajectory using the transformer model</title><p>In any language model, the input is a series of characters or words. During the training process, the weights and biases of the various layers are optimized, enabling the model to understand the context and relationships between different parts of the input sequence. Once the model is trained, it can generate the subsequent sequence for a given input sequence in an auto-regressive manner. As illustrated in <xref rid="fig1" ref-type="fig">Fig. 1</xref>, here we implement a comprehensive scheme to learn the kinetic trajectory <italic toggle="yes">via</italic> the transformer and then to generate a sequence of states that could be segmented into three stages:</p><fig position="float" id="fig1"><label>Fig. 1</label><caption><title>A schematic representation of training data preparation and the architecture of the decoder-only transformer. The left-hand side figure represents the schematic representation of the discretization of molecular dynamics simulation (MDs) trajectory achieved through the identification of collective variables (CVs) and K-means clustering. A total of <italic toggle="yes">n</italic><sub>s</sub> = 10 segments are randomly selected from the discretized trajectory to train an equal number of independent generative pre-trained transformer (GPT) models. Each trained model generates subsequent sequences starting from where the respective segment ended, using a few sequences as prompts. The right-hand side figure depicts the various layers of a decoder-only transformer. The model architecture consists of input embedding, positional embeddings, and multiple blocks of masked multi-head attention, normalization, and feed-forward layers. The model is optimized using cross-entropy as the loss function.</title></caption><graphic xlink:href="d5sc00108k-f1" position="float"/></fig><p>&#x02022; (A) Segmentation of MD trajectories into discrete states.</p><p>&#x02022; (B) Training a decoder-only transformer using MD-derived states as input.</p><p>&#x02022; (C) Generating a kinetic sequence of states from the pre-trained transformer.</p><p>Below we describe the different stages of our scheme.</p><sec><label>A.</label><title>Discretization of molecular dynamics trajectory into meaningful state space</title><p>The time evolution of a physical system involves various conformational or state changes and transitions between them. Our study aims to predict the kinetic sequence of states of physicochemical systems using a large language model, specifically a decoder-only transformer. Interestingly, the state prediction problem can be mapped to the sequence generation of any language model where each state corresponds to a distinct vocabulary item within a corpus. For all of the systems, we have trained the model with molecular dynamics (MD) simulation trajectories to learn the occurrence of kinetic states. However, MD trajectories are continuous, requiring discretization of the trajectory into grid points or states before inputting it into a language model. For simple systems, the particle position can be used for discretization. However, complex systems with high dimensionality or degrees of freedom require defining some order parameters or collective variables (CVs) to discretize the trajectory into a certain number of states. To identify the various states of a physical system, we performed K-means clustering along CVs. Consequently, the trajectory is segmented into distinct sequences of states. These sequences can then be used as inputs for a decoder-only transformer model. Hereafter, these sequences of states will be referred to as sequences of tokens. For training the decoder-only transformer model, we randomly chose <italic toggle="yes">n</italic><sub>s</sub> segments from the discretized trajectory and trained <italic toggle="yes">n</italic><sub>s</sub> independent models with the same architecture. From each independently trained model, we generated the next sequences from where the corresponding segment ended by providing a few sequences as a prompt. All the results presented here are averaged over these independent runs. The left side of <xref rid="fig1" ref-type="fig">Fig. 1</xref> illustrates a schematic overview of this process. The specific details regarding the amounts of data used for training, validation, and testing across the various systems are provided in Table S1.<xref rid="fn1" ref-type="fn">&#x02020;</xref> Our approach and model are different from the methodology employed by Tsai <italic toggle="yes">et al.</italic><sup><xref rid="cit35" ref-type="bibr">35,36</xref></sup> In their study, they utilized an LSTM-based architecture to learn the MD simulation trajectory, and the states were generated from the training data itself.</p></sec><sec><label>B.</label><title>Training a decoder-only transformer with MD-derived states as input</title><p>Now we will delve into the architecture of the decoder-only transformer model. The neural network-based decoder-only transformer<sup><xref rid="cit33" ref-type="bibr">33</xref></sup> consists of several layers as depicted in the right side of <xref rid="fig1" ref-type="fig">Fig. 1</xref>. The first layer is an input embedding or token embedding layer that transforms each token of the input sequence into a dense vector. The dimension of the vector is a hyperparameter, which is called the embedding dimension. For a sequence of length <italic toggle="yes">l</italic> and embedding dimension <italic toggle="yes">d</italic>, this layer transforms each token into a d-dimensional vector. Consequently, the embedding layer generates a (<italic toggle="yes">l</italic> &#x000d7; <italic toggle="yes">d</italic>)-dimensional matrix, often referred to as the embedding or weight matrix. Throughout the training process, the elements of this weight matrix undergo optimization and the model will learn the semantic relationship between different tokens of the time sequence data.</p><p>To enable the GPT model to learn the sequence order of time series data, the positional information for each token is required, which can be achieved through positional embeddings. For an input sequence of length <italic toggle="yes">l</italic>, the position of the <italic toggle="yes">k</italic><sup>th</sup> token can be represented as follows:<sup><xref rid="cit33" ref-type="bibr">33</xref></sup><disp-formula id="eqn1"><label>1</label>PE(<italic toggle="yes">k</italic>, 2<italic toggle="yes">i</italic>) = sin(<italic toggle="yes">k</italic>/10&#x02009;000<sup>2<italic toggle="yes">i</italic>/<italic toggle="yes">d</italic></sup>)</disp-formula><disp-formula id="eqn2"><label>2</label>PE(<italic toggle="yes">k</italic>, 2<italic toggle="yes">i</italic> + 1) = cos(<italic toggle="yes">k</italic>/10&#x02009;000<sup>2<italic toggle="yes">i</italic>/<italic toggle="yes">d</italic></sup>)</disp-formula>where <italic toggle="yes">d</italic> is the dimension of the output embedding and, for each <italic toggle="yes">k</italic>(0 &#x02264; <italic toggle="yes">k</italic> &#x0003c; <italic toggle="yes">l</italic>), <italic toggle="yes">i</italic> can take a value from 0 to <italic toggle="yes">d</italic>/2. Hence, one can achieve the final embedding layer by adding these two embeddings.</p><p>The final embedding layer is followed by multiple blocks of layers (<italic toggle="yes">N</italic><sub>b</sub>). Each block typically comprises various components, including masked multi-head attention with a specified number of heads, <italic toggle="yes">N</italic><sub>h</sub>, normalization layers, and feed-forward layers. Among these, the masked multi-head attention layer is particularly significant, serving as a communication mechanism between tokens in the sequence. To calculate the key (<italic toggle="yes">K</italic>), query (<italic toggle="yes">Q</italic>), and value (<italic toggle="yes">V</italic>) tensors, the final embedded vector, denoted as <italic toggle="yes">X</italic><sub>f</sub>, is multiplied with three trainable weight matrices (<italic toggle="yes">W</italic><sub>k</sub>, <italic toggle="yes">W</italic><sub>q</sub>, and <italic toggle="yes">W</italic><sub>v</sub>) as <italic toggle="yes">K</italic> = <italic toggle="yes">X</italic><sub>f</sub>&#x000b7;<italic toggle="yes">W</italic><sub>k</sub>, <italic toggle="yes">Q</italic> = <italic toggle="yes">X</italic><sub>f</sub>&#x000b7;<italic toggle="yes">W</italic><sub>q</sub>, and <italic toggle="yes">V</italic> = <italic toggle="yes">X</italic><sub>f</sub>&#x000b7;<italic toggle="yes">W</italic><sub>v</sub>. The attention score <italic toggle="yes">A</italic><sub>s</sub> is then calculated as <inline-graphic xlink:href="d5sc00108k-t1.jpg" id="ugt1"/>, and the output of the attention layer is given by<sup><xref rid="cit33" ref-type="bibr">33</xref></sup><disp-formula id="eqn3"><label>3</label><graphic xlink:href="d5sc00108k-t2.jpg" id="ugt2" position="float"/></disp-formula>where <inline-graphic xlink:href="d5sc00108k-t3.jpg" id="ugt3"/>. This mechanism enables the model to discern the relative importance of tokens to each other, providing a clear sense of context and relationships between them.</p><p>Finally, the normalized outputs of the <italic toggle="yes">N</italic><sub>b</sub> layer are used as inputs of a fully connected dense linear layer. Given that the transformer model functions as a probabilistic model, the output of this dense layer is passed through a softmax function to yield categorical probabilities. We have used cross-entropy as our loss function to minimize the loss between the final output of the model <italic toggle="yes">&#x000d4;</italic><sup>(<italic toggle="yes">t</italic>)</sup> and the actual target output <italic toggle="yes">O</italic><sup>(<italic toggle="yes">t</italic>)</sup>, which is defined as<disp-formula id="eqn4"><label>4</label><graphic xlink:href="d5sc00108k-t4.jpg" id="ugt4" position="float"/></disp-formula>where <italic toggle="yes">T</italic> represents the total time of the trajectory, equivalent to the sequence length (<italic toggle="yes">l</italic>). The model has been trained over 10&#x02009;000 epochs, with all hyperparameters across all of the systems provided in Table S2.<xref rid="fn1" ref-type="fn">&#x02020;</xref> Fig. S1(a&#x02013;f)<xref rid="fn1" ref-type="fn">&#x02020;</xref> show the training and validation loss as a function of epochs for six distinct systems that would be elaborated in the upcoming sections of the present article. The plots indicate that the loss curves saturate or fluctuate slightly around the mean after a certain number of epochs, suggesting robust training of the transformer model without overfitting.</p></sec><sec><label>C.</label><title>Generating kinetic sequence of states from the pre-trained transformer</title><p>After training the transformer model, it can generate any desired number of time series states by inputting an initial sequence of tokens as a prompt. For a given sequence, the model will generate a probability distribution over the entire vocabulary/states. From this probability distribution, the next element of the sequence can be sampled using a multinomial distribution (see supplemental results SR1 for details). As we have generated the kinetic sequence of states across all systems using our trained model, hereafter we will refer to it as the Generative Pre-Trained Transformer (GPT) model. The GPT model was built using PyTorch<sup><xref rid="cit37" ref-type="bibr">37</xref></sup> and our implementation is available on GitHub at the following URL: <uri xlink:href="https://github.com/palash892/gpt_state_generation">https://github.com/palash892/gpt_state_generation</uri>.</p></sec></sec><sec><title>GPT precisely captures inter-state transition kinetics in model multi-state systems</title><p>To begin, we will delve into two hand-crafted model systems: 2D Brownian dynamics (BD) simulations of a single particle in 3-state and 4-state potentials. The mathematical representations of the potentials and simulation details are provided in the &#x0201c;Methods&#x0201d; section. We employ the BD trajectories to compute the 2D free energy of the two model systems within their <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> coordinate space, defined as <inline-graphic xlink:href="d5sc00108k-t20.jpg" id="ugt20"/>, where <italic toggle="yes">P</italic> is the probability, calculated by using a 2D histogram of the coordinates. <xref rid="fig2" ref-type="fig">Fig. 2(a)</xref> represents the free energy surface (FES) plot for the 3-state toy models. The plot exhibits three minima in the FES and the particle can hop from one minimum to another. The states are marked in the plots using magenta color, identified through K-means clustering in coordinate space (<xref rid="fig2" ref-type="fig">Fig. 2(b)</xref>). After clustering the data, the entire trajectory is discretized into a specific number of states. <xref rid="fig2" ref-type="fig">Fig. 2(c)</xref> shows the trajectory after spatial discretization, where each cluster index corresponds to a metastable state. The trajectory demonstrates that the particle can stay in a particular state for some time and also exhibits transitions between various states. Now, from both the actual (<italic toggle="yes">i.e.</italic> BD-simulated) and GPT-generated time series data, we can compute the probability of each state by counting the occurrences of that particular state and dividing by the total count. For instance, if the count of state 0 is <italic toggle="yes">C</italic><sub>0</sub> and the total count of all states is <italic toggle="yes">C</italic><sub>tot</sub>, then the probability of state 0 is <inline-graphic xlink:href="d5sc00108k-t5.jpg" id="ugt5"/>. <xref rid="fig2" ref-type="fig">Fig. 2(d)</xref> depicts a comparison between the actual and GPT-generated state probabilities for the 3-state model. The plot suggests that there is a close match between the actual and GPT-generated state probabilities.</p><fig position="float" id="fig2"><label>Fig. 2</label><caption><title>Kinetics and thermodynamics for the toy model system. (a) Free energy surface (FES) plot for the 3-state toy model in its <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> coordinate space. The particle can transition from one minimum to the other. (b) Scatter plots of the <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> coordinates, with distinct clusters representing metastable states identified through K-means clustering. (c) The trajectory of the particle in the 3-state potential after state decomposition. (d) The comparison of state probabilities between the actual and GPT-generated time series data for the 3-state toy model. The plot highlights the accuracy of the GPT model in predicting the state probabilities. (e-g) Transition counts as a function of commit time for a 3-state toy model. These plots indicate the ability of the GPT model to learn contextual relationships among the states and generate a sequence of states that are kinetically and thermodynamically significant. Here the error bar represents the standard error and the commit time is in units of <italic toggle="yes">&#x003c4;</italic><sub>BD</sub> (see the &#x0201c;Methods&#x0201d;).</title></caption><graphic xlink:href="d5sc00108k-f2" position="float"/></fig><p>To effectively compare the kinetics between actual and GPT-generated time series data, one must analyze the transitions between different states over time. To facilitate this analysis, we utilized the concept of &#x0201c;commitment time or commit time&#x0201d;. This metric represents the duration a particle remains in a given state before transitioning to another.<sup><xref rid="cit35" ref-type="bibr">35,36</xref></sup> We calculated the total transition count for different commit times, considering all possible state pairs (<sup><italic toggle="yes">n</italic></sup><italic toggle="yes">C</italic><sub>2</sub>, <italic toggle="yes">n</italic> is the total number of states) and both forward and backward directions. <xref rid="fig2" ref-type="fig">Fig. 2(e&#x02013;g)</xref> represent the transition count as a function of commit time for a 3-state toy model. These plots reveal that the decay of transition counts as a function of commit time is very similar between actual and GPT-generated data in both directions. Furthermore, the 2D FES plot (<xref rid="fig2" ref-type="fig">Fig. 2(a)</xref>) indicates that states 0 and 2 are spatially distant. The actual data corroborate this by showing no direct transitions between these states. On the other hand, the GPT model predicts a finite number of transitions between them, although the overall frequency of such transitions remains very small (approximately three), indicating the rare nature of such transitions. This might lead one to believe that the GPT model has perhaps missed to fully capture the spatial disconnection between these states. Alternatively, it could also reflect the model's attempt to account for the statistical possibility&#x02014;however small and rare&#x02014;of transitions between distant states within the learned context of the trajectory data.</p><p>We assessed the accuracy of this approach for a 4-state toy model system, as depicted in Fig. S2(a&#x02013;j).<xref rid="fn1" ref-type="fn">&#x02020;</xref> Barring slight deviations in probabilities and a few transitions (Fig. S2(f) and (i)<xref rid="fn1" ref-type="fn">&#x02020;</xref>), the results are very similar for both actual and GPT-generated states. The free energy surface (FES) plot in Fig. S2(a)<xref rid="fn1" ref-type="fn">&#x02020;</xref> indicates that states 1 and 0, as well as states 2 and 3, are spatially distant, with no direct transitions between them in the actual data. Remarkably, the GPT-generated states capture these trends very nicely. However, upon closer examination, the FES (Fig. S2(a)<xref rid="fn1" ref-type="fn">&#x02020;</xref>) suggests that although states 0 and 2, as well as states 1 and 3, are spatially proximate, the trajectory is not sampled properly. This discrepancy may contribute to the slight deviations in transition counting between actual and GPT-generated data for these pairs of states (0&#x02013;2, 2&#x02013;0, 3&#x02013;1, and 1&#x02013;3). Nevertheless, the results collectively indicate that the GPT model effectively learns the context and relationships between states, enabling it to generate a sequence of states that are both kinetically and thermodynamically significant.</p></sec><sec><title>Predicting the ensemble probabilities and state-to-state transition kinetics in Trp-cage mini protein</title><p>Encouraged by the promising results in model potential, we extended our approach to a globular 20-residue mini-protein Trp-cage. This biomolecule is known for complex and multi-state conformational ensembles, despite its small size and remains an ideal candidate for experimental and computational studies of protein folding. Towards this end, we utilized a long (100 &#x003bc;s) unbiased simulation trajectory of Trp-cage provided by D. E. Shaw Research.<sup><xref rid="cit38" ref-type="bibr">38,39</xref></sup> For this system, due to higher degrees of freedom, it is necessary to define suitable order parameters or collective variables (CVs) to discretize the time series data into a certain number of states. Nonetheless, the identification of precise CVs is a challenging problem. To address these challenges, we employ an encoder-decoder-based unsupervised deep neural network called Autoencoder.<sup><xref rid="cit16" ref-type="bibr">16,40,41</xref></sup> An Autoencoder is a powerful non-linear dimension reduction technique that is used to transform the high-dimensional input data in a lower-dimensional space known as the latent space. The encoder component of the Autoencoder maps the input data to the latent space, while the decoder reverses this process, reconstructing the original input from the latent space. During this process, the model optimizes its weights and biases to preserve the most important information from the input data in the lower-dimensional representation. <xref rid="fig3" ref-type="fig">Fig. 3(a)</xref> represents a schematic of Autoencoder architecture, where the distances between <italic toggle="yes">C</italic><sub>&#x003b1;</sub> atoms serve as input features. During the training of the Autoencoder, we monitor the fraction of variation explained (FVE) score to determine the optimal dimension of the latent space. A detailed description of the Autoencoder architecture and the various hyperparameters is provided in the &#x0201c;Methods&#x0201d; section and Table S3.</p><fig position="float" id="fig3"><label>Fig. 3</label><caption><title>Kinetics and thermodynamics for Trp-cage mini protein. (a) A schematic representation of the Autoencoder. In this setup, <italic toggle="yes">d</italic><sub>1</sub>, <italic toggle="yes">d</italic><sub>2</sub>,&#x02026;, <italic toggle="yes">d</italic><sub><italic toggle="yes">n</italic></sub> denote the input and output nodes, delineating the dimensions of the input and output data, while <italic toggle="yes">h</italic><sub>1</sub>, <italic toggle="yes">h</italic><sub>2</sub>,&#x02026;, <italic toggle="yes">h</italic><sub><italic toggle="yes">n</italic></sub> represent the hidden nodes. Furthermore, <italic toggle="yes">&#x003c7;</italic><sub>1</sub>,&#x02026;, <italic toggle="yes">&#x003c7;</italic><sub>3</sub> represent the latent nodes. (b) 2D FES plot along latent space <italic toggle="yes">&#x003c7;</italic><sub>1</sub> and <italic toggle="yes">&#x003c7;</italic><sub>2</sub> for Trp-cage with three distinct minima and extracted conformations. (c) The state decomposition of the MD trajectory is achieved through K-means clustering on the latent space, which divides the entire trajectory into distinct sequences of states. (d) The trajectory of Trp-cage after state decomposition. (e) The comparison of state probabilities between actual and GPT-generated time series data. These plots suggest that the GPT model effectively captures the probabilities with minor deviations. (f&#x02013;h) Comparison of transition counts between actual and GPT-generated states, showcasing the GPT model's ability to accurately capture state transitions. Here, the error bar represents the standard error.</title></caption><graphic xlink:href="d5sc00108k-f3" position="float"/></fig><p>
<xref rid="fig3" ref-type="fig">Fig. 3(b)</xref> represents the 2D FES plot along the latent space <italic toggle="yes">&#x003c7;</italic><sub>1</sub> and <italic toggle="yes">&#x003c7;</italic><sub>2</sub>, obtained from the Autoencoder. The figure clearly shows three distinct minima in the FES plot, indicating distinct conformations. To visualize different conformations, we extracted a few conformations near each minimum in the FES and overlaid them. The superimposed conformations reveal mainly three metastable states: folded &#x003b1;-helix (state-0), partially folded &#x003b1;-helix (state-1), and unfolded random coils (state-2). After clustering in the latent space, the entire trajectory comprises metastable states and their transitions, as depicted in <xref rid="fig3" ref-type="fig">Fig. 3(c)</xref>. <xref rid="fig3" ref-type="fig">Fig. 3(d)</xref> illustrates the discretized trajectory, with the majority of transitions occurring between states 1 and 2. <xref rid="fig3" ref-type="fig">Fig. 3(e)</xref> represents a comparison of the state probabilities between the actual and GPT-generated time series data for the Trp-cage protein. These figures demonstrate that the GPT model has effectively captured the probabilities, with minor deviations observed for a few states. Importantly, these deviations are within the error bars.</p><p>Next, to probe the kinetics between various states of the Trp-cage protein, we calculated the transition counts as a function of commit time, akin to methodologies employed for 3-state and 4-state toy models. <xref rid="fig3" ref-type="fig">Fig. 3(f&#x02013;h)</xref> compare the transition counts between actual and GPT-generated states for the Trp-cage protein. These figures encompass all possible pairs and transitions in both forward and reverse directions. The results indicate that the GPT model accurately captures the transitions between states. Quite interestingly, the GPT model can also predict the highest number of transitions between states 1 and 2 accurately, which aligns with our observations from the trajectory itself (<xref rid="fig3" ref-type="fig">Fig. 3(e)</xref>). In summary, together these results suggest that the GPT model effectively captures the probability and the transition counts between various states of the Trp-cage protein, providing valuable insights into the kinetics and thermodynamics of these systems.</p></sec><sec><title>GPT flourishes in kinetic prediction of state sequences of complex intrinsically disordered protein</title><p>Next, we turn our attention to an intricately complex system: &#x003b1;-synuclein. It is a prototypical intrinsically disordered protein (IDP) that predominantly resides in the human brain, especially nerve cells.<sup><xref rid="cit42" ref-type="bibr">42</xref></sup> It plays a crucial role in the communication between these cells by releasing neurotransmitters, which are messenger chemicals.<sup><xref rid="cit43" ref-type="bibr">43</xref></sup> However, the excessive accumulation of these proteins can lead to several neurodegenerative disorders such as Parkinson's disease and other synucleinopathies.<sup><xref rid="cit44" ref-type="bibr">44&#x02013;46</xref></sup> For &#x003b1;-synuclein, we utilize the 73 &#x003bc;s trajectory provided by D. E. Shaw Research (DESRES).<sup><xref rid="cit38" ref-type="bibr">38,39</xref></sup></p><p>We used the radius of gyration (<italic toggle="yes">R</italic><sub>g</sub>) as CVs to decompose the entire trajectory into a specific number of states. <xref rid="fig4" ref-type="fig">Fig. 4(a)</xref> represents the (<italic toggle="yes">R</italic><sub>g</sub>) (red color) of &#x003b1;-synuclein as a function of time, showcasing the protein's diverse conformational possibilities. Through K-means clustering in the <italic toggle="yes">R</italic><sub>g</sub> space,<sup><xref rid="cit47" ref-type="bibr">47</xref></sup> we discretize the total trajectories into distinct states (<xref rid="fig4" ref-type="fig">Fig. 4(a)</xref> magenta color plot). Specifically, three states have been identified: intermediated compact (state-0), collapsed (state-1), and extended (state-2), with a superimposition of snapshots revealing significant conformational heterogeneity within each state (<xref rid="fig4" ref-type="fig">Fig. 4(b)</xref>).</p><fig position="float" id="fig4"><label>Fig. 4</label><caption><title>The radius of gyration (<italic toggle="yes">R</italic><sub>g</sub>) and transition count comparison for &#x003b1;-synuclein. (a) The (<italic toggle="yes">R</italic><sub>g</sub>) as a function of time for &#x003b1;-synuclein (red plot). The magenta color plot represents the trajectory after state decomposition <italic toggle="yes">via</italic> K-means clustering. (b) The diverse conformational states are identified as intermediated compact (state-0), collapsed (state-1), and extended (state-2). (c&#x02013;e) Transition count comparison between the actual and GPT-generated time series data as a function of commit time for &#x003b1;-synuclein. The transition dynamics fairly match with actual and GPT-generated data, except at a smaller commit time. Here the error bar represents the standard error.</title></caption><graphic xlink:href="d5sc00108k-f4" position="float"/></fig><p>Fig. S3<xref rid="fn1" ref-type="fn">&#x02020;</xref> compares the state probabilities between actual and GPT-generated time series data for &#x003b1;-synuclein. The figure clearly shows the deviation in the state probability values. Next, to analyze the transition dynamics, we have calculated the transition count of each state. <xref rid="fig4" ref-type="fig">Fig. 4(d&#x02013;f)</xref> depict the comparison of transition counts as a function of commit time between actual and GPT-generated time series data for &#x003b1;-synuclein. While there are some deviations in probability values, the transition dynamics align fairly well with actual and GPT-generated data, except for a smaller commit time. Specifically, the GPT model generates a lower count for the transition (2 &#x02192; 0) compared to actual data. The deviations in state probability and transition dynamics indicate the intricacies involved in accurately predicting the dynamical behavior of such a complex system.</p></sec><sec><title>Assessing the transformer's predictive ability in a far-from-equilibrium system</title><p>In the preceding sections, we have mainly focused on capturing the kinetics and thermodynamics of various models and real systems that are in thermodynamic equilibrium. Finally, in this section, we shift our attention to an active system. Most living organisms are active and their movement is powered by energy consumption from internal or external sources. The continuous consumption and dissipation of energy drive these systems far from equilibrium. Notably, the activity or self-propulsion force plays a crucial role in the formation of many self-organized collective phenomena such as pattern formation,<sup><xref rid="cit48" ref-type="bibr">48&#x02013;51</xref></sup> motility-induced phase separation,<sup><xref rid="cit52" ref-type="bibr">52&#x02013;55</xref></sup> swarming motion,<sup><xref rid="cit56" ref-type="bibr">56&#x02013;59</xref></sup><italic toggle="yes">etc.</italic> In this study, we employ a model system, an active worm-like polymer chain,<sup><xref rid="cit60" ref-type="bibr">60</xref></sup> where the activity or self-propulsion force acts tangentially along all bonds. We have utilized our in-house BD simulation trajectory to study the active polymer chain (see the Methods for details).</p><p>After training the Autoencoder by using inter-bead distances as features, we have chosen two-dimensional latent space as CVs. For active systems, the quantity <inline-graphic xlink:href="d5sc00108k-t6.jpg" id="ugt6"/> may not correspond to the free energy in the same way as in a passive description. To avoid confusion, we refer to it as the effective free energy. <xref rid="fig5" ref-type="fig">Fig. 5(a)</xref> shows the effective FES plot across the latent space <italic toggle="yes">&#x003c7;</italic><sub>1</sub> and <italic toggle="yes">&#x003c7;</italic><sub>2</sub> for the active polymer chain and the corresponding metastable states are highlighted in magenta color. The overlaid plots suggest that there are mainly two metastable states: a bending state (state-1) and a spiral state (state-0). Notably, despite the apparent simplicity of the two states, the visualization of the trajectory (Fig. S4(a)<xref rid="fn1" ref-type="fn">&#x02020;</xref>) suggests that the system spends a very long time within each state along with spontaneous spiral formation and breakup occurrences. Subsequently, we employ K-means clustering on the latent space derived from the Autoencoder to discretize the trajectory (<xref rid="fig5" ref-type="fig">Fig. 5(b)</xref>).</p><fig position="float" id="fig5"><label>Fig. 5</label><caption><title>The effective free energy surface and transition count comparison for the active worm-like polymer chain. (a) The effective FES plot of the active polymer chain across the latent space <italic toggle="yes">&#x003c7;</italic><sub>1</sub> and <italic toggle="yes">&#x003c7;</italic><sub>2</sub>. This plot highlights two metastable states: a bending state (state-1) and a spiral state (state-0). (b) The latent space has been clustered <italic toggle="yes">via</italic> K-means clustering to discretize the trajectory. (c) The comparison of transition counts between actual and GPT-generated data for the active polymer chain, reflecting the system's long stay at particular states before transition and the violation of detailed balance. The GPT model accurately generates a kinetic sequence of states, maintaining saturation and detailed balance violation, albeit with some small deviation from actual data. Here the error bar represents the standard error and the commit time is in units of <italic toggle="yes">&#x003c4;</italic><sub>BD</sub> (see the &#x0201c;Methods&#x0201d;).</title></caption><graphic xlink:href="d5sc00108k-f5" position="float"/></fig><p>Fig. S4(b)<xref rid="fn1" ref-type="fn">&#x02020;</xref> compares the state probabilities between actual and GPT-generated time series data for the active polymer chain. The comparison reveals the deviations in the state probability. <xref rid="fig5" ref-type="fig">Fig. 5(c)</xref> depicts the comparison of transition counts as a function of commit time between actual and GPT-generated time series data. However, interestingly, the GPT model accurately generates the transition for the active system. As mentioned earlier, for the active polymer chain, the system stays at a particular state for a long time before transitioning to the other state. This behaviour is reflected in the plots, where both curves saturate after a certain commit time. Moreover, the forward and backward transition curves suggest a violation of detailed balance, which is an inherent feature of active systems that are far from thermodynamic equilibrium. Remarkably, the GPT model successfully generates a sequence of states that maintain the saturation nature as well as the violation of detailed balance, albeit with some deviation from actual data within the error bar. These findings indicate that the GPT model is very powerful for future state predictions, even in complex active systems. In a similar spirit, for comparison purposes, we have also computed similar metrics for a &#x02018;passive&#x02019; polymer chain for enhanced clarity (Fig. S5<xref rid="fn1" ref-type="fn">&#x02020;</xref>). Here too, we observed a strong alignment between the kinetics and thermodynamics captured by the GPT model and the actual BD-generated data.</p></sec><sec><title>Deciphering the inner workings of GPT's prediction accuracy of a kinetic sequence of states</title><p>In the previous sections, our focus has been on exploring the thermodynamics and kinetics of diverse model and real systems, whether in thermodynamic equilibrium or out of equilibrium. We have observed that despite variations in state probabilities, the GPT model generates a sequence of states that accurately maintain transition dynamics in a statistical sense. Now, in this section, we delve into identifying the pivotal factors that maintain these precise transition dynamics.</p><p>In the field of natural language processing (NLP), the seminal work by Vaswani <italic toggle="yes">et al.</italic> titled &#x0201c;Attention Is All You Need&#x0201d;<sup><xref rid="cit33" ref-type="bibr">33</xref></sup> introduces the paradigm-shifting concept of self-attention. The state-of-the-art transformer model, based on attention mechanisms, has demonstrated superior performance compared to traditional recurrent neural networks (RNNs). To understand the role of attention, we computed the attention score from the multi-head attention layer, which is defined as <inline-graphic xlink:href="d5sc00108k-t7.jpg" id="ugt7"/> (see <xref rid="eqn3" ref-type="disp-formula">eqn (3)</xref> for details), where <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic>, and <italic toggle="yes">d</italic> are the query, the key, and the dimension of the embedding layer, respectively. In our study, we randomly selected 20 chunks of sequence length 128 from GPT-generated time series data and fed them as inputs for the trained GPT model. Subsequently, we computed the attention scores from each head, averaging them across all heads and the 20 random chunks. Physically, these attention scores unveil correlations among different tokens within a sequence of time series data. <xref rid="fig6" ref-type="fig">Fig. 6</xref> show the heat map of the masked attention score for all the systems. These plots highlight the presence of significant finite, non-zero attention among various tokens within sequences. Notably, some tokens exhibit clear evidence of long-range attention, underscoring the model's ability to understand the relationships between events that are far apart.</p><fig position="float" id="fig6"><label>Fig. 6</label><caption><title>Attention score for all the systems analyzed. The heat map of the attention score computed from the multi-head attention layer of a trained GPT model, for six distinct systems under consideration. These scores reveal that there is finite attention among various tokens and highlight the evidence of long-range attention.</title></caption><graphic xlink:href="d5sc00108k-f6" position="float"/></fig><p>Based on our examination of attention scores, we now pose the following question: how does attention impact the transition dynamics between different states? To address this, we trained the GPT model on all six systems by removing the multi-head attention layer, while maintaining other hyperparameters consistent with our previous model architecture. This approach ensures that the model can no longer capture any short- or long-range correlations between tokens. Our findings indicate that for simple systems, such as a 3-state toy model, the transition dynamics captured by the GPT model remain similar regardless of the presence or absence of the attention layer (see Fig. S6<xref rid="fn1" ref-type="fn">&#x02020;</xref>). This suggests that attention may not play a significant role in these simple systems. However, for other systems, there are substantial deviations in GPT-generated transition dynamics compared to actual data. <xref rid="fig7" ref-type="fig">Fig. 7(a&#x02013;j)</xref> show the comparison of transition counts as a function of commit time between the actual and GPT-generated time series data for all of the systems under consideration as highlighted in the text of each plot. These plots suggest that while in many cases the GPT model can recover the transition counts in one direction, it completely predicts the wrong transitions in other directions in the absence of attention. Here, we have only shown the plots where the deviations are prominent. The comprehensive plots for all the systems with all possible transitions are given in the ESI (Fig. S7&#x02013;S10<xref rid="fn1" ref-type="fn">&#x02020;</xref>). Together these analyses identify the power of the attention layer. Even in physicochemical systems, attention is crucial for learning the context and relationships or correlations among various tokens of time series data, enabling the model to generate a physically meaningful kinetic sequence of states.</p><fig position="float" id="fig7"><label>Fig. 7</label><caption><title>Impact of attention on transition dynamics in the GPT model. (a&#x02013;j) The comparison of transition counts over commit time between the actual and GPT-generated time series data for all of the systems in the absence of attention mechanisms. There are significant deviations in GPT-generated transition dynamics compared to the actual data. While the GPT model can sometimes accurately predict transitions in one direction, it frequently mispredicts transitions in another direction.</title></caption><graphic xlink:href="d5sc00108k-f7" position="float"/></fig></sec><sec><title>Performance comparison: MD simulation <italic toggle="yes">vs.</italic> GPT state generation</title><p>After training the GPT model, the generation of the sequence of states is extremely fast compared to conventional MD simulations of equivalent systems. For example, the model can generate 20&#x02009;000 subsequent sequences for &#x003b1;-synuclein within &#x0223c;60 minutes, corresponding to a 20 &#x003bc;s simulation of this system with a data saving frequency of 1 ns. Similarly, the model can generate 100&#x02009;000 sequences for the Trp-cage mini protein within &#x0223c;32 minutes, equivalent to 20 &#x003bc;s simulation with a data dumping frequency of 200 ps. As mentioned earlier, we have analyzed six systems; among them the Trp-cage mini protein and &#x003b1;-synuclein are particularly relevant in biophysical contexts, and their simulations were conducted in real-time units. Thus, our primary focus here is to compare the performance of these two systems. To compare this generation's efficiency against actual MD simulation times, we conducted 10 ns simulations for these systems, maintaining all parameters such as box size, salt concentration, temperature, and time step as per the study by Robustelli <italic toggle="yes">et al.</italic><sup><xref rid="cit38" ref-type="bibr">38</xref></sup> As the data saving frequency for these two systems is different, we define a quantity <inline-graphic xlink:href="d5sc00108k-t8.jpg" id="ugt8"/>, where <italic toggle="yes">P</italic><sub>MD/GPT</sub> is the performance of the MD or GPT model and <italic toggle="yes">f</italic><sub>dump</sub> is the saving frequency of the data. This metric can normalize the performance of each system by its respective data saving frequency. All the MD simulations and training of the GPT models were performed on an Intel(R) Xeon(R) Platinum 8352Y CPU at 2.20 GHz, along with an NVIDIA RTX A6000 GPU. <xref rid="tab1" ref-type="table">Tables 1</xref> and S4<xref rid="fn1" ref-type="fn">&#x02020;</xref> show all of the details of the performance and memory usage of the MD simulations as well as GPT state generation. <xref rid="tab1" ref-type="table">Table 1</xref> suggests that the performance and normalized performance of the GPT model surpass those of traditional MD simulations, demonstrating its efficiency in generating the kinetic sequence of states of the systems.</p><table-wrap position="float" id="tab1"><label>Table 1</label><caption><title>The comparison of the performance between MD simulation and GPT state generation</title></caption><table frame="hsides" rules="groups"><colgroup span="1"><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1">System</th><th rowspan="1" colspan="1">
<italic toggle="yes">f</italic>
<sub>dump</sub>
</th><th rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
<sub>MD</sub> (ns per day)</th><th rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
<sup arrange="stack">N</sup>
<sub arrange="stack">MD</sub> (per day)</th><th rowspan="1" colspan="1">Training time (minutes)</th><th rowspan="1" colspan="1">Gen. sequence</th><th rowspan="1" colspan="1">Gen. time (minutes)</th><th rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
<sub>GPT</sub> (&#x003bc;s per day)</th><th rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
<sup arrange="stack">N</sup>
<sub arrange="stack">GPT</sub> (10<sup>3</sup> per day)</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">&#x003b1;-Synuclein</td><td rowspan="1" colspan="1">1.0 ns</td><td rowspan="1" colspan="1">&#x0223c;35.0</td><td rowspan="1" colspan="1">&#x0223c;35.0</td><td rowspan="1" colspan="1">&#x0223c;127.0</td><td rowspan="1" colspan="1">20&#x02009;000</td><td rowspan="1" colspan="1">&#x0223c;6.0</td><td rowspan="1" colspan="1">&#x0223c;4800.0</td><td rowspan="1" colspan="1">&#x0223c;4800.0</td></tr><tr><td rowspan="1" colspan="1">Trp-cage</td><td rowspan="1" colspan="1">200 ps</td><td rowspan="1" colspan="1">&#x0223c;832.0</td><td rowspan="1" colspan="1">&#x0223c;4160.0</td><td rowspan="1" colspan="1">&#x0223c;35.0</td><td rowspan="1" colspan="1">100&#x02009;000</td><td rowspan="1" colspan="1">&#x0223c;32.0</td><td rowspan="1" colspan="1">&#x0223c;900.0</td><td rowspan="1" colspan="1">&#x0223c;4500.0</td></tr></tbody></table></table-wrap><p>What should be the typical size of training data? To evaluate the impact of training data size on our model's ability to predict a kinetic sequence of states in a biophysical system, we trained our GPT model using varying amounts of data. We selected the Trp-cage mini protein, which provides 500&#x02009;000 frames, in contrast to the 73&#x02009;124 frames available for &#x003b1;-synuclein (see Table S1<xref rid="fn1" ref-type="fn">&#x02020;</xref>). We generated the same number of states as depicted in <xref rid="fig3" ref-type="fig">Fig. 3</xref> but varied the training data size. Initially, the model was trained with 60% of the total data. We have now conducted additional training with 10%, 40%, and 50% of the data. Fig. S11(a&#x02013;i) and (j&#x02013;l)<xref rid="fn1" ref-type="fn">&#x02020;</xref> show the transition counts over time and state probabilities for these different training data percentages. These figures indicate a significant alignment between the actual and GPT-generated data after utilizing 40% of the training data. This suggests that a sufficient amount of training data is crucial for the model to predict a kinetic sequence of states accurately. However, the transition count plots demonstrate that even with just 10% of the training data, the model can still capture complex relationships between various states, generating a kinetic sequence of states that are not entirely erroneous.</p><p>In many machine learning tasks, performance improves with training data size up to a saturation point. A similar approach can be applied to a new system by incrementally increasing the dataset size and monitoring the convergence of performance metrics. More complex systems, particularly those with numerous metastable states or intricate free energy landscapes, may require larger datasets to capture transition dynamics effectively. One practical approach is to test whether kinetic properties, such as state transition probabilities, are stable across different dataset sizes.</p></sec><sec><title>GPT outperforms precedent baseline approaches in kinetic sequence generation</title><p>In the previous sections, we explored the capabilities of the GPT model in capturing the state-to-state transition dynamics of various systems. In this section, we focus on a detailed comparison between the GPT model and two established approaches: Long Short-Term Memory (LSTM)<sup><xref rid="cit34" ref-type="bibr">34</xref></sup> networks and MSM.<sup><xref rid="cit1" ref-type="bibr">1&#x02013;3</xref></sup> Both models were trained using the same clustered trajectories as the GPT model. The MSM was originally developed for equilibrium systems, relying on the assumption of the Markovian properties of the system. This means that the transition probabilities in the MSM are calculated under the conditions that the system satisfies detailed balance, which is a key requirement for equilibrium systems. However, for active systems, this assumption is not valid, as such systems inherently violate detailed balance due to their non-equilibrium nature. As a result, using the MSM to analyze an active system will lead to incorrect kinetic information. In our study, we used the MSM to compare its results with those of the GPT model. We also observed that the MSM provides incorrect results for the active system, as expected.</p><p>We first examine the LSTM networks, which are known for their ability to model sequential data by capturing long-range dependencies. To ensure consistency, we used the same embedding dimension for the input data as with the GPT model, while all other hyperparameters are detailed in Table S5. We observed that in most of the cases, the LSTM captures the state-to-state transition accurately for various systems under consideration. However, in a few cases, there are significant deviations in the state-to-state transition from the actual data. <xref rid="fig8" ref-type="fig">Fig. 8(a&#x02013;d)</xref> show the transition count as a function of commit time for various systems as highlighted in the title of each plot. Here, we have highlighted only the plots where the deviations are most significant. For a comprehensive overview, all other plots for each system, including all possible transitions, can be found in the ESI (Fig. S12&#x02013;S14<xref rid="fn1" ref-type="fn">&#x02020;</xref>). Since the GPT model uses the self-attention mechanism, it likely enhances its ability to capture long-range dependencies more effectively, enabling it to generate a more accurate kinetic sequence of states compared to the LSTM model.</p><fig position="float" id="fig8"><label>Fig. 8</label><caption><title>Comparison of state-to-state transition dynamics in LSTM and MSM models. (a&#x02013;d) State-to-state transition counts as a function of commit time between actual data and LSTM-generated time series data for various systems. (e&#x02013;h) Transition counts over commit time between actual data and MSM-generated time series data for various systems. The MSM shows consistent deviations across all systems, failing to capture the correct sequence of state transitions. (i&#x02013;l) Comparison of state probabilities between actual and MSM-generated data for various systems. Although the MSM struggles with dynamic predictions, it aligns well with actual data in terms of state probabilities.</title></caption><graphic xlink:href="d5sc00108k-f8" position="float"/></fig><p>Next, we constructed Markov State Models (MSMs), a powerful framework for analyzing the kinetics of molecular systems by discretizing the state space into metastable states. The first step in building an MSM is selecting an appropriate lag time for calculating the transition probability matrix, ensuring that the model behaves in a Markovian manner. For all systems, the lag time was chosen based on the implied time scales (ITSs) or relaxation time scale plots as a function of lag time (Fig. S15 (a&#x02013;d)<xref rid="fn1" ref-type="fn">&#x02020;</xref>). We selected the lag time where the ITS plots approximately level off, denoted by a vertical magenta line. Subsequently, a Chapman&#x02013;Kolmogorov test was performed to further verify the Markovianity of the model (Fig. S16&#x02013;S19<xref rid="fn1" ref-type="fn">&#x02020;</xref>). After constructing the MSM at the chosen lag time, we generated a kinetic sequence of states using the transition probabilities of the previous states. <xref rid="fig8" ref-type="fig">Fig. 8(e&#x02013;h)</xref> represent the state-to-state transition counts as a function of commit time across various systems. These plots suggest significant deviations between the actual data and the MSM-generated kinetic sequence of states for all systems. While we have highlighted a few transitions here, all other possible transitions are presented in the ESI (Fig. S20&#x02013;S22<xref rid="fn1" ref-type="fn">&#x02020;</xref>), where similar deviations are observed. However, quite interestingly, the MSM-generated state probabilities align well with the actual data (<xref rid="fig8" ref-type="fig">Fig. 8(i&#x02013;l)</xref>). Together, these observations suggest that while the MSM accurately predicts the thermodynamics of the systems, it fails to correctly capture the temporal sequence of states (dynamics). To gain deeper insights, we built the MSM with a lag time of one step (the data dumping frequency), regardless of whether the ITS curve had plateaued for all systems. Fig. S23&#x02013;S26<xref rid="fn1" ref-type="fn">&#x02020;</xref> depict the transition counts as a function of commit time for the systems under consideration. While the MSM accurately generates a kinetic sequence of states for simpler systems, such as a 3-state model, it fails to match the state-to-state transition counts with the actual data for more complex systems, particularly the Trp-cage mini protein and active systems.</p><p>Based on these observations, we conclude that our GPT model outperforms traditional methods like the MSM in generating future states in the correct sequential order. The MSM requires the assumption of detailed balance for transition matrix calculation and the selection of an appropriate lag time to ensure Markovian behavior. In contrast, the GPT model does not rely on the Markov properties of the system and can generate a kinetic sequence of states with the same temporal precision as its training data, regardless of any intrinsic time scale (such as the lag time required by the MSM) it learns from the system.</p><p>In our previous analyses, we discretized the trajectory into a few states based on the minima in the free energy surface (FES). Now, we have discretized the trajectory into a larger set of states, effectively fine-graining the FES. Our focus is on two systems: the Trp-cage mini protein and &#x003b1;-synuclein. We clustered the data into 20 clusters along their collective variables (CVs). Using the same protocol as before, we trained both the GPT model and MSM on this clustered data. After training, we generated a kinetic sequence of states using both models and compared the results. The lag time selection for the MSM was determined based on the approximate plateau observed in the ITS plots (Fig. S27<xref rid="fn1" ref-type="fn">&#x02020;</xref>).</p><p>
<xref rid="fig9" ref-type="fig">Fig. 9(a)&#x02013;(d)</xref> depict the state-to-state transition count as a function of commit time for the Trp-cage mini protein, derived from the GPT model and MSM, respectively. Similarly, <xref rid="fig9" ref-type="fig">Fig. 9(e)&#x02013;(h)</xref> represent the state-to-state transition count as a function of commit time for &#x003b1;-synuclein, obtained from the GPT model and MSM, respectively. These plots suggest that, even with a larger number of clusters, the GPT model does a better job than the traditional MSM in accurately predicting the kinetic sequence of states for both systems. It is important to note that with 20 states, there are now 190 (<sup>20</sup><italic toggle="yes">C</italic><sub>2</sub> = 190) possible transitions. In this representation, we have only shown the two transitions with the highest transition counts. The complete set of all possible transitions is provided in the ESI (Fig. S28&#x02013;S47<xref rid="fn1" ref-type="fn">&#x02020;</xref>).</p><fig position="float" id="fig9"><label>Fig. 9</label><caption><title>Comparison of state-to-state transition dynamics for Trp-cage mini protein and &#x003b1;-synuclein using GPT and MSM models. (a)&#x02013;(d) State-to-state transition counts as a function of commit time for the Trp-cage mini protein, derived from the GPT model and MSM, respectively. (e)&#x02013;(h) State-to-state transition counts as a function of commit time for &#x003b1;-synuclein, obtained from the GPT model and MSM, respectively. These results demonstrate that the GPT model more accurately predicts the kinetic sequence of states compared to the traditional MSM for both systems.</title></caption><graphic xlink:href="d5sc00108k-f9" position="float"/></fig><p>We now aim to reconstruct the one-dimensional free energy plots using the GPT model. For &#x003b1;-synuclein, we used the radius of gyration (<italic toggle="yes">R</italic><sub>g</sub>) as the one-dimensional CV to discretize the trajectory into specific states. For this free energy calculation, we focused solely on &#x003b1;-synuclein, discretizing the trajectory by binning along the CV as proposed by Tsai <italic toggle="yes">et al.</italic><sup><xref rid="cit35" ref-type="bibr">35</xref></sup> Fig. S48<xref rid="fn1" ref-type="fn">&#x02020;</xref> compares the GPT-generated one-dimensional free energy plot with the actual data along <italic toggle="yes">R</italic><sub>g</sub>. The comparison reveals a strong agreement between the actual and GPT-generated free energy plots.</p></sec></sec><sec><title>Discussions</title><p>The time evolution of biophysical systems undergoes various conformation changes, depending on environmental conditions. Understanding these dynamics typically requires experiments or Molecular Dynamics (MD) simulations. However, comprehending the long-term behaviour of these systems requires running computationally expensive simulations. In this study, we present a comprehensive approach, employing state-of-the-art machine learning models, particularly decoder-only transformers, to predict the kinetic sequence of states of physicochemical systems. Through an extensive analysis of the MD trajectory of various models and real systems, we have demonstrated the efficacy of the GPT model in capturing both the kinetics and thermodynamics of these systems.</p><p>Our study began with simplified model systems, namely 3-state and 4-state toy models. We employed K-means clustering on coordinate space for this simple system to discretize the trajectory. Subsequently, we delved into more complex systems such as the Trp-cage mini protein, 32-bead passive polymer chain, and intrinsically disordered protein &#x003b1;-synuclein. However, for these systems, we utilized another ML-based technique, Autoencoder, to identify the relevant collective variables for discretization. Our results highlight the ability of the GPT model to accurately predict the probabilities of different states and capture the transition dynamics between them. Furthermore, we extended our analysis to include an active system, a 32-bead active worm-like polymer chain, where the system is far from thermodynamic equilibrium. Remarkably, the GPT model successfully predicted the kinetics and thermodynamics of the active system.</p><p>A key aspect of our study is the ability of the GPT model to capture and reconstruct complex transition dynamics in molecular systems, offering valuable chemical and physical insights beyond traditional kinetic modeling approaches. One of the most striking findings is the role of the attention mechanism in preserving long-range dependencies within kinetic sequences. Through attention score analysis, we observed significant correlations between distant states, highlighting the model's ability to recognize intricate transition patterns that may be obscured in traditional MSMs. Furthermore, by removing the attention layer from the model, we observed substantial deviations in transition dynamics for complex systems. Therefore, these findings strongly suggest that the attention mechanism plays a pivotal role in maintaining accurate predictions.</p><p>While our model does not predict entirely new states beyond the training data, its ability to generate statistically precise kinetic sequences provides valuable insight into transition dynamics, helping to reconstruct long-timescale behavior from limited MD trajectories. By leveraging learned transition patterns through the attention mechanism, the model can rapidly generate statistically robust kinetic pathways, enabling accurate estimations of state-to-state transition probabilities, especially for complex systems and active systems, where the traditional MSM-based model failed to accurately predict the kinetic sequence of future states. The attention maps highlight how the model internally focuses on long-range temporal relationships in the trajectory. This &#x0201c;mechanistic introspection&#x0201d; offers a data-driven window into how conformational history affects future evolution&#x02014;a level of mechanistic interpretability not directly accessible from traditional MD or even MSMs.</p><p>Although the transformer-based large language models (LLMs) were specially developed for tasks like machine translation and natural language processing, our study demonstrates their effectiveness in predicting the kinetics and thermodynamics of a diverse array of biophysical systems. One notable limitation of our GPT model is that it never generates new states beyond its training data. In terms of language, the transformer-based model is always unable to generate new vocabulary. Nonetheless, the model can learn the complex syntactic and semantic relationships present in the sequence of tokens, which help them to generate a kinetic sequence of states of the system very correctly. If some system shows a completely new state at a very long time that is not present in the training sequence, the model cannot generate that particular state. This suggests that one needs very good MD sampling data for a physical system to predict the kinetic sequence of states of the system. Another concern is the large amount of MD simulation data required to effectively train the GPT model. While we have demonstrated fairly accurate state probabilities using only 10% of the simulation data for Trp-Cage, this training data included multiple back-and-forth transitions between metastable states. However, recent advances in ML have introduced innovative techniques such as variational autoencoders (VAEs),<sup><xref rid="cit61" ref-type="bibr">61&#x02013;63</xref></sup> generative adversarial networks (GANs),<sup><xref rid="cit64" ref-type="bibr">64,65</xref></sup> diffusion models,<sup><xref rid="cit66" ref-type="bibr">66&#x02013;68</xref></sup><italic toggle="yes">etc.</italic>, which can be utilized for generating new conformations and improving sampling in biophysical systems.<sup><xref rid="cit69" ref-type="bibr">69&#x02013;71</xref></sup> While these techniques enhance sampling quality, they may lack the temporal information crucial for understanding the dynamics of the system. In conclusion, our findings highlight the potential of LLMs as powerful tools for advancing our understanding of biophysical systems, offering new avenues for further exploration and improvement in this field. In all our analyses, we have relied on unbiased MD data to predict the kinetic sequence of states of various physicochemical systems. In the future, it would be logical and interesting to extend our model to incorporate biased enhanced sampling simulations for learning the long-timescale behavior in molecular dynamics.<sup><xref rid="cit72" ref-type="bibr">72</xref></sup></p></sec><sec><title>Methods</title><sec><title>3-state and 4-state toy models</title><p>To simulate a particle in a 2D 3-state and 4-state potential well, we adopted the same functional form for the potential as Tsai <italic toggle="yes">et al.</italic><sup><xref rid="cit35" ref-type="bibr">35</xref></sup> The potential for the 3-state model is given by<disp-formula id="eqn5"><label>5</label><italic toggle="yes">V</italic><sub>3</sub>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) = <italic toggle="yes">W</italic><sub>3</sub>(<italic toggle="yes">x</italic><sup>6</sup> + <italic toggle="yes">y</italic><sup>6</sup>) &#x02212; <italic toggle="yes">G</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">x</italic><sub>1</sub>)<italic toggle="yes">G</italic>(<italic toggle="yes">y</italic>, <italic toggle="yes">y</italic><sub>1</sub>) &#x02212; <italic toggle="yes">G</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">x</italic><sub>2</sub>)<italic toggle="yes">G</italic>(<italic toggle="yes">y</italic>, <italic toggle="yes">y</italic><sub>2</sub>) &#x02212; <italic toggle="yes">G</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">x</italic><sub>3</sub>)<italic toggle="yes">G</italic>(<italic toggle="yes">y</italic>, <italic toggle="yes">y</italic><sub>3</sub>)</disp-formula>Similarly, the potential for the 4-state model is given by<disp-formula id="eqn6"><label>6</label><italic toggle="yes">V</italic><sub>4</sub>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) = <italic toggle="yes">W</italic><sub>4</sub>(<italic toggle="yes">x</italic><sup>4</sup> + <italic toggle="yes">y</italic><sup>4</sup>) &#x02212; <italic toggle="yes">G</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">x</italic><sub>1</sub>)<italic toggle="yes">G</italic>(<italic toggle="yes">y</italic>, <italic toggle="yes">y</italic><sub>1</sub>) &#x02212; <italic toggle="yes">G</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">x</italic><sub>2</sub>)<italic toggle="yes">G</italic>(<italic toggle="yes">y</italic>, <italic toggle="yes">y</italic><sub>2</sub>) &#x02212; <italic toggle="yes">G</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">x</italic><sub>3</sub>)<italic toggle="yes">G</italic>(<italic toggle="yes">y</italic>, <italic toggle="yes">y</italic><sub>3</sub>) &#x02212; <italic toggle="yes">G</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">x</italic><sub>4</sub>)<italic toggle="yes">G</italic>(<italic toggle="yes">y</italic>, <italic toggle="yes">y</italic><sub>4</sub>) &#x02212; <italic toggle="yes">G</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">x</italic><sub>5</sub>)<italic toggle="yes">G</italic>(<italic toggle="yes">y</italic>, <italic toggle="yes">y</italic><sub>5</sub>)</disp-formula>where <italic toggle="yes">G</italic> is a Gaussian function as <inline-graphic xlink:href="d5sc00108k-t9.jpg" id="ugt9"/>. Here, <italic toggle="yes">x</italic><sub>0</sub> and <italic toggle="yes">&#x003c3;</italic> are the mean and standard deviation of the distribution. In our simulation, we kept <italic toggle="yes">W</italic><sub>3</sub> = <italic toggle="yes">W</italic><sub>4</sub> = 0.0001 and <italic toggle="yes">&#x003c3;</italic> = 0.8. The mean values of Gaussian distribution for the 3-state model are given by <italic toggle="yes">x</italic><sub>1</sub> = 0.0, <italic toggle="yes">y</italic><sub>1</sub> = 0.0, <italic toggle="yes">x</italic><sub>2</sub> = &#x02212;1.5, <italic toggle="yes">y</italic><sub>2</sub> = &#x02212;1.5, and <italic toggle="yes">x</italic><sub>3</sub> = 1.5, <italic toggle="yes">y</italic><sub>3</sub> = 1.5. Similarly, for the 4-state model, these values are <italic toggle="yes">x</italic><sub>1</sub> = 0.0, <italic toggle="yes">y</italic><sub>1</sub> = 0.0, <italic toggle="yes">x</italic><sub>2</sub> = 2.0, <italic toggle="yes">y</italic><sub>2</sub> = &#x02212;1.0, <italic toggle="yes">x</italic><sub>3</sub> = 0.5, <italic toggle="yes">y</italic><sub>3</sub> = 2.0, <italic toggle="yes">x</italic><sub>4</sub> = &#x02212;0.5, <italic toggle="yes">y</italic><sub>4</sub> = &#x02212;2.0, and <italic toggle="yes">x</italic><sub>5</sub> = &#x02212;2.0, <italic toggle="yes">y</italic><sub>5</sub> = 1.0. We performed Brownian dynamics simulations for these two systems by integrating the equation of motion:<disp-formula id="eqn7"><label>7</label><graphic xlink:href="d5sc00108k-t10.jpg" id="ugt10" position="float"/></disp-formula>where <italic toggle="yes">&#x003b3;</italic> is the friction coefficient, <italic toggle="yes">V</italic> is the potential energy, and <italic toggle="yes"><private-char name="ITALIC SMALL ETA, GREEK, VECTOR" description="italic small eta, Greek, vector"><!-- PBM data was replaced with SVG by xgml2pxml:
<glyph-data id="z.eegvec.i" format="PBM" resolution="300" x-size="9" y-size="14" xml:space="preserve">
000000100
001111110
000000100
000000000
001100110
010101010
001010100
001100100
001000100
010001000
010001000
000001000
000001000
000001000
</glyph-data>
-->
<glyph-data id="z.eegvec.i" format="SVG" resolution="300" x-size="9" y-size="14" xml:space="preserve">
<![CDATA[
<svg xmlns="http://www.w3.org/2000/svg" version="1.0" width="11.000000pt" height="16.000000pt" viewBox="0 0 11.000000 16.000000" preserveAspectRatio="xMidYMid meet"><metadata>
Created by potrace 1.16, written by Peter Selinger 2001-2019
</metadata><g transform="translate(1.000000,15.000000) scale(0.012500,-0.012500)" fill="currentColor" stroke="none"><path d="M480 1080 l0 -40 -160 0 -160 0 0 -40 0 -40 160 0 160 0 0 -40 0 -40 40 0 40 0 0 40 0 40 40 0 40 0 0 40 0 40 -40 0 -40 0 0 40 0 40 -40 0 -40 0 0 -40z M160 760 l0 -40 -40 0 -40 0 0 -40 0 -40 40 0 40 0 0 -120 0 -120 -40 0 -40 0 0 -80 0 -80 40 0 40 0 0 80 0 80 40 0 40 0 0 40 0 40 40 0 40 0 0 40 0 40 40 0 40 0 0 40 0 40 40 0 40 0 0 -120 0 -120 -40 0 -40 0 0 -200 0 -200 40 0 40 0 0 200 0 200 40 0 40 0 0 120 0 120 40 0 40 0 0 80 0 80 -80 0 -80 0 0 -40 0 -40 -40 0 -40 0 0 -40 0 -40 -40 0 -40 0 0 80 0 80 -80 0 -80 0 0 -40z m80 -80 l0 -40 40 0 40 0 0 -40 0 -40 -40 0 -40 0 0 40 0 40 -40 0 -40 0 0 40 0 40 40 0 40 0 0 -40z m320 0 l0 -40 -40 0 -40 0 0 40 0 40 40 0 40 0 0 -40z"/></g></svg>
]]>
</glyph-data></private-char></italic>(<italic toggle="yes">t</italic>) is a random noise, satisfying the fluctuation&#x02013;dissipation theorem, <italic toggle="yes">i.e.</italic> &#x03008;<italic toggle="yes"><private-char name="ITALIC SMALL ETA, GREEK, VECTOR" description="italic small eta, Greek, vector"><glyph-ref glyph-data="z.eegvec.i"/></private-char></italic>(<italic toggle="yes">t</italic>)&#x000b7;<italic toggle="yes"><private-char name="ITALIC SMALL ETA, GREEK, VECTOR" description="italic small eta, Greek, vector"><glyph-ref glyph-data="z.eegvec.i"/></private-char></italic>(<italic toggle="yes">t&#x02032;</italic>)&#x03009; = 4<italic toggle="yes">kT&#x003b3;</italic><sup>&#x02212;1</sup><italic toggle="yes">&#x003b4;</italic>(<italic toggle="yes">t</italic> &#x02212; <italic toggle="yes">t&#x02032;</italic>). Here, all simulation times are in units of <inline-graphic xlink:href="d5sc00108k-t11.jpg" id="ugt11"/>, where <italic toggle="yes">&#x003c3;</italic> = 1 is the diameter of the particle. Integration of <xref rid="eqn7" ref-type="disp-formula">eqn (7)</xref> was performed using the Euler method with a time step &#x003b4;<italic toggle="yes">t</italic> = 0.01<italic toggle="yes">&#x003c4;</italic><sub>BD</sub> by setting <inline-graphic xlink:href="d5sc00108k-t12.jpg" id="ugt12"/></p></sec><sec><title>Passive polymer chain</title><p>We performed a very long molecular dynamics simulation (5.74 &#x003bc;s) for a passive polymer chain, with the model and simulation parameters of the polymer system detailed in a previous study by our group.<sup><xref rid="cit73" ref-type="bibr">73</xref></sup></p></sec><sec><title>Trp-cage mini protein and &#x003b1;-synuclein</title><p>For Trp-cage and &#x003b1;-synuclein, we utilized very long molecular dynamics simulation trajectories from D. E. Shaw Research.<sup><xref rid="cit38" ref-type="bibr">38</xref></sup> The Trp-cage trajectory spanned 100 &#x003bc;s, while the &#x003b1;-synuclein trajectory was 73 &#x003bc;s long. These simulations were performed using the a99SB-disp force field on Anton specialized hardware.<sup><xref rid="cit39" ref-type="bibr">39</xref></sup> The detailed simulation protocols can be found in the original paper by Robustelli <italic toggle="yes">et al.</italic><sup><xref rid="cit38" ref-type="bibr">38</xref></sup> However, in the original paper, the trajectory of &#x003b1;-synuclein was 30 &#x003bc;s long. Due to some periodic image issues in the original trajectory, the authors provided the extended 73 &#x003bc;s trajectory for &#x003b1;-synuclein, maintaining the same simulation setup as before but increasing the box size.</p></sec><sec><title>Active worm-like polymer chain</title><p>The two-dimensional worm-like polymer chain consists of <italic toggle="yes">N</italic> = 32 beads connected by a stiff spring with spring constant <italic toggle="yes">k</italic><sub>0</sub>. Each bead has a diameter of <italic toggle="yes">&#x003c3;</italic> and an equilibrium bond length of <italic toggle="yes">d</italic><sub>0</sub>. The dynamics of the polymer chain are governed by overdamped motion, described by the equation<disp-formula id="eqn8"><label>8</label><graphic xlink:href="d5sc00108k-t13.jpg" id="ugt13" position="float"/></disp-formula>where <italic toggle="yes">&#x003b3;</italic> is the friction coefficient and <italic toggle="yes">V</italic><sub>tot</sub> is the total potential energy of the system, which includes bonding potential <italic toggle="yes">V</italic><sub>bond</sub>, bending potential <italic toggle="yes">V</italic><sub>bend</sub>, and non-bonded potential <italic toggle="yes">V</italic><sub>nb</sub>. The bonding potential <italic toggle="yes">V</italic><sub>bond</sub> is given by<disp-formula id="eqn9"><label>9</label><graphic xlink:href="d5sc00108k-t14.jpg" id="ugt14" position="float"/></disp-formula>The bending potential <italic toggle="yes">V</italic><sub>bend</sub> is given by<disp-formula id="eqn10"><label>10</label><graphic xlink:href="d5sc00108k-t15.jpg" id="ugt15" position="float"/></disp-formula>where <italic toggle="yes">k</italic><sub>ang</sub> is the bending rigidity and <italic toggle="yes">&#x003b8;</italic><sub>0</sub> is the equilibrium bond angle. The non-bonded potential <italic toggle="yes">V</italic><sub>nb</sub> is taken as the Hertzian potential:<disp-formula id="eqn11"><label>11</label><graphic xlink:href="d5sc00108k-t16.jpg" id="ugt16" position="float"/></disp-formula>These potentials are very generic for the simulation of any passive polymer chain. However, the most important force for an active polymer chain is the motility or self-propulsion force. Here, <italic toggle="yes">&#x00066;&#x020d1;</italic><sub>m</sub> represents the self-propulsion force, which acts tangentially along all bonds. The random noise <italic toggle="yes"><private-char name="ITALIC SMALL ETA, GREEK, VECTOR" description="italic small eta, Greek, vector"><glyph-ref glyph-data="z.eegvec.i"/></private-char></italic><sub><italic toggle="yes">i</italic></sub>(<italic toggle="yes">t&#x02032;</italic>) satisfies the fluctuation&#x02013;dissipation theorem, <italic toggle="yes">i.e.</italic> &#x03008;<italic toggle="yes"><private-char name="ITALIC SMALL ETA, GREEK, VECTOR" description="italic small eta, Greek, vector"><glyph-ref glyph-data="z.eegvec.i"/></private-char></italic><sub><italic toggle="yes">i</italic></sub>(<italic toggle="yes">t</italic>)&#x000b7;<italic toggle="yes"><private-char name="ITALIC SMALL ETA, GREEK, VECTOR" description="italic small eta, Greek, vector"><glyph-ref glyph-data="z.eegvec.i"/></private-char></italic><sub><italic toggle="yes">j</italic></sub>(<italic toggle="yes">t&#x02032;</italic>)&#x03009; = 4<italic toggle="yes">kT&#x003b3;</italic><sup>&#x02212;1</sup><italic toggle="yes">&#x003b4;</italic><sub><italic toggle="yes">i</italic>,<italic toggle="yes">j</italic></sub><italic toggle="yes">&#x003b4;</italic>(<italic toggle="yes">t</italic> &#x02212; <italic toggle="yes">t&#x02032;</italic>). All lengths and energies are in units of <italic toggle="yes">&#x003c3;</italic> and <italic toggle="yes">kT</italic>, respectively, and the simulation time is in units of <inline-graphic xlink:href="d5sc00108k-t17.jpg" id="ugt17"/>. The Brownian dynamics simulation for an active polymer chain was performed using a time step of d<italic toggle="yes">t</italic> = 0.001<italic toggle="yes">&#x003c4;</italic><sub>BD</sub>. The other simulation parameters are <italic toggle="yes">kT</italic> = 1.0, <italic toggle="yes">&#x003c3;</italic> = 1.0, <italic toggle="yes">d</italic><sub>0</sub> = 0.5<italic toggle="yes">&#x003c3;</italic>, <inline-graphic xlink:href="d5sc00108k-t18.jpg" id="ugt18"/>, <italic toggle="yes">k</italic><sub>ang</sub> = 45.0 <italic toggle="yes">kT</italic>, <italic toggle="yes">&#x003b3;</italic> = 200 <italic toggle="yes">kT&#x003c4;</italic><sub>BD</sub>/<italic toggle="yes">&#x003c3;</italic><sup>2</sup>, <italic toggle="yes">f</italic><sub>m</sub> = 5.0 <italic toggle="yes">kT</italic>/<italic toggle="yes">&#x003c3;</italic>, <italic toggle="yes">&#x003b8;</italic><sub>0</sub> = <italic toggle="yes">&#x003c0;</italic>, and <italic toggle="yes">E</italic> = 10&#x02009;000.0 <italic toggle="yes">kT</italic>/<italic toggle="yes">&#x003c3;</italic><sup>3</sup>.</p></sec><sec><title>Training details of the Autoencoder</title><p>The Autoencoder architecture and training hyperparameters are presented in Table S3.<xref rid="fn1" ref-type="fn">&#x02020;</xref> For the Trp-cage mini protein, we utilized the distance between all <italic toggle="yes">C</italic><sub>&#x003b1;</sub> atoms as input features, resulting in 190 (<sup>20</sup><italic toggle="yes">C</italic><sub>2</sub> = 190) features for its 20 residues. Conversely, for active and passive polymer chains, we employed inter-bead distances, selecting 8 effective beads in an arithmetic progression with a step of 4, providing 28 (<sup>8</sup><italic toggle="yes">C</italic><sub>2</sub> = 28) input features.<sup><xref rid="cit13" ref-type="bibr">13</xref></sup> Throughout the Autoencoder training process, we monitored two metrics: training loss and the fraction of variation explained (FVE) score. The FVE is defined by the equation<disp-formula id="eqn12"><label>12</label><graphic xlink:href="d5sc00108k-t19.jpg" id="ugt19" position="float"/></disp-formula>where <bold>X</bold>(<italic toggle="yes">i</italic>), <bold>Y</bold>(<italic toggle="yes">i</italic>), and <bold>&#x00058;&#x00304;</bold> represent the input, output, and mean input, respectively, and <italic toggle="yes">N</italic> corresponds to the total number of features. The FVE score indicates the proportion of input data variance explained by the Autoencoder's reconstruction. Fig. S49(a&#x02013;f)<xref rid="fn1" ref-type="fn">&#x02020;</xref> depict the plots of these two metrics for all systems as described within the figure text. For active and passive polymer chains, a 2D latent dimension was chosen, whereas for the Trp-cage mini protein, a 4D latent dimension was utilized. Across all these latent dimensions, the FVE scores exceed 0.80, indicating that the Autoencoder's reconstruction explains at least 80% of the original data variance. Furthermore, the gradual decrease followed by saturation of the training loss curve suggests no overfitting during the Autoencoder's training process. However, in the case of the IDP &#x003b1;-synuclein, even with a high latent dimension of <italic toggle="yes">L</italic><sub>d</sub> = 7, we have observed a relatively low FVE score (&#x0223c;0.60) (Fig. S50(a)<xref rid="fn1" ref-type="fn">&#x02020;</xref>). Additionally, we have plotted the FES using the first two components of the latent space (Fig. S50(b)<xref rid="fn1" ref-type="fn">&#x02020;</xref>). This plot indicates a lack of distinct minima in the free energy surface. Consequently, clustering the data for a proper state decomposition within this latent space is not feasible. Therefore, for &#x003b1;-synuclein, we have opted to use the radius of gyration (<italic toggle="yes">R</italic><sub>g</sub>) as the reaction coordinate to discretize the trajectory into a specific number of states.</p><p>We conducted simulations for 3-state, 4-state, and active worm-like polymer chains using our in-house scripts written in C++. To simulate passive polymer chains, we utilized the open-source Software GROMACS-2022.<sup><xref rid="cit74" ref-type="bibr">74,75</xref></sup> Our Autoencoder model was trained using Python implementation of Tensorflow<sup><xref rid="cit76" ref-type="bibr">76</xref></sup> and Keras<sup><xref rid="cit77" ref-type="bibr">77</xref></sup> and the GPT model was built using PyTorch.<sup><xref rid="cit37" ref-type="bibr">37</xref></sup> The Markov State Model (MSM) analyses were performed using PyEMMA<sup><xref rid="cit78" ref-type="bibr">78</xref></sup> (v2.5.12), a Python library designed for efficient estimation and analysis of MSMs from molecular dynamics simulations.</p></sec></sec><sec sec-type="data-availability"><title>Data and code availability</title><p>The manuscript contains all the data. The code and detailed documentation for training the Autoencoder and GPT model are available on GitHub at the following URL: <uri xlink:href="https://github.com/palash892/gpt_state_generation">https://github.com/palash892/gpt_state_generation</uri>.</p></sec><sec><title>Author contributions</title><p>Palash Bera: conceptualization, methodology, software, validation, formal analysis, investigation, data curation, writing &#x02013; original draft, and writing &#x02013; review &#x00026; editing; Jagannath Mondal: conceptualization, methodology, validation, resources, writing &#x02013; original draft, writing &#x02013; review &#x00026; editing, supervision, project administration, and funding acquisition.</p></sec><sec sec-type="COI-statement"><title>Conflicts of interest</title><p>There are no conflicts to declare.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="SC-016-D5SC00108K-s001" position="float" content-type="local-data"><label>SC-016-D5SC00108K-s001</label><media xlink:href="SC-016-D5SC00108K-s001.pdf" id="d67e1894" position="anchor"/></supplementary-material></sec></body><back><ack><p>All the authors acknowledge the Tata Institute of Fundamental Research Hyderabad, India, for providing support with computing resources. We acknowledge support from the Department of Atomic Energy, Government of India, under Project Identification No. RTI 4007. JM acknowledges Core Research grants provided by the Department of Science and Technology (DST) of India (CRG/2023/001426). We would like to thank D. E. Shaw Research for providing us with access to a long simulation trajectory of Trp-cage and monomeric &#x003b1;-synuclein.</p></ack><ref-list><title>References</title><ref id="cit1"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Husic</surname><given-names>B. E.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Pande</surname><given-names>V. S.</given-names></name>
</person-group><article-title>Markov state models: From an art to a science</article-title><source>J. Am. Chem. Soc.</source><year>2018</year><volume>140</volume><fpage>2386</fpage><lpage>2396</lpage><pub-id pub-id-type="doi">10.1021/jacs.7b12191</pub-id><pub-id pub-id-type="pmid">29323881</pub-id>
</element-citation></ref><ref id="cit2"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Prinz</surname><given-names>J.-H.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>H.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Sarich</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Keller</surname><given-names>B.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Senne</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Held</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Chodera</surname><given-names>J. D.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Sch&#x000fc;tte</surname><given-names>C.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>No&#x000e9;</surname><given-names>F.</given-names></name>
</person-group><article-title>Markov models of molecular kinetics: Generation and validation</article-title><source>J. Chem. Phys.</source><year>2011</year><volume>134</volume><fpage>174105</fpage><pub-id pub-id-type="doi">10.1063/1.3565032</pub-id><pub-id pub-id-type="pmid">21548671</pub-id>
</element-citation></ref><ref id="cit3"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Olsson</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>H.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Paul</surname><given-names>F.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Clementi</surname><given-names>C.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>No&#x000e9;</surname><given-names>F.</given-names></name>
</person-group><article-title>Combining experimental and simulation data of molecular processes <italic toggle="yes">via</italic> augmented Markov models</article-title><source>Proc. Natl. Acad. Sci. U. S. A.</source><year>2017</year><volume>114</volume><fpage>8265</fpage><lpage>8270</lpage><pub-id pub-id-type="doi">10.1073/pnas.1704803114</pub-id><pub-id pub-id-type="pmid">28716931</pub-id>
</element-citation></ref><ref id="cit4"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Eddy</surname><given-names>S. R.</given-names></name>
</person-group><article-title>What is a hidden Markov model?</article-title><source>Nat. Biotechnol.</source><year>2004</year><volume>22</volume><fpage>1315</fpage><lpage>1316</lpage><pub-id pub-id-type="doi">10.1038/nbt1004-1315</pub-id><pub-id pub-id-type="pmid">15470472</pub-id>
</element-citation></ref><ref id="cit5"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rabiner</surname><given-names>L. R.</given-names></name>
</person-group><article-title>A tutorial on hidden Markov models and selected applications in speech recognition</article-title><source>Proc. IEEE</source><year>1989</year><volume>77</volume><fpage>257</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1109/5.18626</pub-id></element-citation></ref><ref id="cit6"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>No&#x000e9;</surname><given-names>F.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>H.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Prinz</surname><given-names>J.-H.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Plattner</surname><given-names>N.</given-names></name>
</person-group><article-title>Projected and hidden Markov models for calculating kinetics and metastable states of complex molecules</article-title><source>J. Chem. Phys.</source><year>2013</year><volume>139</volume><fpage>184114</fpage><pub-id pub-id-type="doi">10.1063/1.4828816</pub-id><pub-id pub-id-type="pmid">24320261</pub-id>
</element-citation></ref><ref id="cit7"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jolliffe</surname><given-names>I. T.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Cadima</surname><given-names>J.</given-names></name>
</person-group><article-title>Principal component analysis: a review and recent developments</article-title><source>Philos. Trans. R. Soc., A</source><year>2016</year><volume>374</volume><fpage>20150202</fpage><pub-id pub-id-type="doi">10.1098/rsta.2015.0202</pub-id><pub-id pub-id-type="pmid">26953178</pub-id>
</element-citation></ref><ref id="cit8"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Teodoro</surname><given-names>M. L.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Phillips Jr</surname><given-names>G. N.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Kavraki</surname><given-names>L. E.</given-names></name>
</person-group><article-title>Understanding protein flexibility through dimensionality reduction</article-title><source>J. Comput. Biol.</source><year>2003</year><volume>10</volume><fpage>617</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1089/10665270360688228</pub-id><pub-id pub-id-type="pmid">12935348</pub-id>
</element-citation></ref><ref id="cit9"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>David</surname><given-names>C. C.</given-names></name>
</person-group> and <person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>D. J.</given-names></name></person-group>, <article-title>Principal component analysis: a method for determining the essential dynamics of proteins</article-title>, <source>Protein dynamics: Methods and protocols</source>, <year>2014</year>, pp. 193&#x02013;226</mixed-citation></ref><ref id="cit10"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Molgedey</surname><given-names>L.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Schuster</surname><given-names>H. G.</given-names></name>
</person-group><article-title>Separation of a mixture of independent signals using time delayed correlations</article-title><source>Phys. Rev. Lett.</source><year>1994</year><volume>72</volume><fpage>3634</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.72.3634</pub-id><pub-id pub-id-type="pmid">10056251</pub-id>
</element-citation></ref><ref id="cit11"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>P&#x000e9;rez-Hern&#x000e1;ndez</surname><given-names>G.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Paul</surname><given-names>F.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Giorgino</surname><given-names>T.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>De Fabritiis</surname><given-names>G.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>No&#x000e9;</surname><given-names>F.</given-names></name>
</person-group><article-title>Identification of slow molecular order parameters for Markov model construction</article-title><source>J. Chem. Phys.</source><year>2013</year><volume>139</volume><fpage>015102</fpage><pub-id pub-id-type="doi">10.1063/1.4811489</pub-id><pub-id pub-id-type="pmid">23822324</pub-id>
</element-citation></ref><ref id="cit12"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Naritomi</surname><given-names>Y.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Fuchigami</surname><given-names>S.</given-names></name>
</person-group><article-title>Slow dynamics in protein fluctuations revealed by time-structure based independent component analysis: The case of domain motions</article-title><source>J. Chem. Phys.</source><year>2011</year><volume>134</volume><fpage>065101</fpage><pub-id pub-id-type="doi">10.1063/1.3554380</pub-id><pub-id pub-id-type="pmid">21322734</pub-id>
</element-citation></ref><ref id="cit13"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bandyopadhyay</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Mondal</surname><given-names>J.</given-names></name>
</person-group><article-title>A deep autoencoder framework for discovery of metastable ensembles in biomacromolecules</article-title><source>J. Chem. Phys.</source><year>2021</year><volume>155</volume><fpage>114106</fpage><pub-id pub-id-type="doi">10.1063/5.0059965</pub-id><pub-id pub-id-type="pmid">34551528</pub-id>
</element-citation></ref><ref id="cit14"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bandyopadhyay</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Mondal</surname><given-names>J.</given-names></name>
</person-group><article-title>A deep encoder&#x02013;decoder framework for identifying distinct ligand binding pathways</article-title><source>J. Chem. Phys.</source><year>2023</year><volume>158</volume><fpage>194103</fpage><pub-id pub-id-type="doi">10.1063/5.0145197</pub-id><pub-id pub-id-type="pmid">37184003</pub-id>
</element-citation></ref><ref id="cit15"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bera</surname><given-names>P.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Mondal</surname><given-names>J.</given-names></name>
</person-group><article-title>Machine learning unravels inherent structural patterns in Escherichia coli Hi-C matrices and predicts chromosome dynamics</article-title><source>Nucleic Acids Res.</source><year>2024</year><fpage>gkae749</fpage></element-citation></ref><ref id="cit16"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hinton</surname><given-names>G. E.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Salakhutdinov</surname><given-names>R. R.</given-names></name>
</person-group><article-title>Reducing the dimensionality of data with neural networks</article-title><source>Science</source><year>2006</year><volume>313</volume><fpage>504</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1126/science.1127647</pub-id><pub-id pub-id-type="pmid">16873662</pub-id>
</element-citation></ref><ref id="cit17"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ward</surname><given-names>M. D.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Zimmerman</surname><given-names>M. I.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Meller</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Chung</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Swamidass</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Bowman</surname><given-names>G. R.</given-names></name>
</person-group><article-title>Deep learning the structural determinants of protein biochemical properties by comparing structural ensembles with DiffNets</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><fpage>3023</fpage><pub-id pub-id-type="doi">10.1038/s41467-021-23246-1</pub-id><pub-id pub-id-type="pmid">34021153</pub-id>
</element-citation></ref><ref id="cit18"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wehmeyer</surname><given-names>C.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>No&#x000e9;</surname><given-names>F.</given-names></name>
</person-group><article-title>Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics</article-title><source>J. Chem. Phys.</source><year>2018</year><volume>148</volume><fpage>241703</fpage><pub-id pub-id-type="doi">10.1063/1.5011399</pub-id><pub-id pub-id-type="pmid">29960344</pub-id>
</element-citation></ref><ref id="cit19"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mardt</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Pasquali</surname><given-names>L.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>H.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>No&#x000e9;</surname><given-names>F.</given-names></name>
</person-group><article-title>VAMPnets for deep learning of molecular kinetics</article-title><source>Nat. Commun.</source><year>2018</year><volume>9</volume><fpage>5</fpage><pub-id pub-id-type="doi">10.1038/s41467-017-02388-1</pub-id><pub-id pub-id-type="pmid">29295994</pub-id>
</element-citation></ref><ref id="cit20"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Olsson</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>No&#x000e9;</surname><given-names>F.</given-names></name>
</person-group><article-title>Dynamic graphical models of molecular kinetics</article-title><source>Proc. Natl. Acad. Sci. U. S. A.</source><year>2019</year><volume>116</volume><fpage>15001</fpage><lpage>15006</lpage><pub-id pub-id-type="doi">10.1073/pnas.1901692116</pub-id><pub-id pub-id-type="pmid">31285323</pub-id>
</element-citation></ref><ref id="cit21"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>No&#x000e9;</surname><given-names>F.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Olsson</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>K&#x000f6;hler</surname><given-names>J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>H.</given-names></name>
</person-group><article-title>Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning</article-title><source>Science</source><year>2019</year><volume>365</volume><fpage>eaaw1147</fpage><pub-id pub-id-type="doi">10.1126/science.aaw1147</pub-id><pub-id pub-id-type="pmid">31488660</pub-id>
</element-citation></ref><ref id="cit22"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>Rezende</surname><given-names>D.</given-names></name>
</person-group> and <person-group person-group-type="author"><name><surname>Mohamed</surname><given-names>S.</given-names></name></person-group>, <article-title>Variational inference with normalizing flows</article-title>. <source>International
conference on machine learning</source>, <year>2015</year>, pp. 1530&#x02013;1538</mixed-citation></ref><ref id="cit23"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>K&#x000f6;hler</surname><given-names>J.</given-names></name>
</person-group>, <person-group person-group-type="author"><name><surname>Invernizzi</surname><given-names>M.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>De Haan</surname><given-names>P.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>No&#x000e9;</surname><given-names>F.</given-names></name></person-group>, <article-title>Rigid body flows for sampling molecular crystal structures</article-title>, <source>International Conference on Machine Learning</source>, <year>2023</year>, pp. 17301&#x02013;17326</mixed-citation></ref><ref id="cit24"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>K&#x000f6;hler</surname><given-names>J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Kr&#x000e4;mer</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Noe</surname><given-names>F.</given-names></name>
</person-group><article-title>Smooth Normalizing Flows</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><fpage>2796</fpage><lpage>2809</lpage></element-citation></ref><ref id="cit25"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schreiner</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Winther</surname><given-names>O.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Olsson</surname><given-names>S.</given-names></name>
</person-group><article-title>Implicit Transfer Operator Learning: Multiple Time-Resolution Models for Molecular Dynamics</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2023</year><fpage>36449</fpage><lpage>36462</lpage></element-citation></ref><ref id="cit26"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Klein</surname><given-names>L.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Foong</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Fjelde</surname><given-names>T.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Mlodozeniec</surname><given-names>B.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Brockschmidt</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Nowozin</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Noe</surname><given-names>F.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Tomioka</surname><given-names>R.</given-names></name>
</person-group><article-title>Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2023</year><fpage>52863</fpage><lpage>52883</lpage></element-citation></ref><ref id="cit27"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Cho</surname><given-names>K.</given-names></name>
</person-group>, <person-group person-group-type="author"><name><surname>Van Merri&#x000eb;nboer</surname><given-names>B.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Gulcehre</surname><given-names>C.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Bahdanau</surname><given-names>D.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Bougares</surname><given-names>F.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Schwenk</surname><given-names>H.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group>, <article-title>Learning phrase representations using RNN encoder-decoder for statistical machine translation</article-title>, <source>arXiv</source>, <year>2014</year>, preprint, arXiv:1406.1078, <pub-id pub-id-type="doi">10.48550/arXiv.1406.1078</pub-id></mixed-citation></ref><ref id="cit28"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>Sundermeyer</surname><given-names>M.</given-names></name>
</person-group>, <person-group person-group-type="author"><name><surname>Schl&#x000fc;ter</surname><given-names>R.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Ney</surname><given-names>H.</given-names></name></person-group>, <source>Lstm neural networks for language modeling</source>, <publisher-name>Interspeech</publisher-name>, <year>2012</year>, pp. 194&#x02013;197</mixed-citation></ref><ref id="cit29"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Luko&#x00161;evi&#x0010d;ius</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Jaeger</surname><given-names>H.</given-names></name>
</person-group><article-title>Reservoir computing approaches to recurrent neural network training</article-title><source>Comput. Sci. Rev.</source><year>2009</year><volume>3</volume><fpage>127</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.cosrev.2009.03.005</pub-id></element-citation></ref><ref id="cit30"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schuster</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Paliwal</surname><given-names>K. K.</given-names></name>
</person-group><article-title>Bidirectional recurrent neural networks</article-title><source>IEEE Trans. Signal Process.</source><year>1997</year><volume>45</volume><fpage>2673</fpage><lpage>2681</lpage><pub-id pub-id-type="doi">10.1109/78.650093</pub-id></element-citation></ref><ref id="cit31"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>Sutskever</surname><given-names>I.</given-names></name>
</person-group>, <person-group person-group-type="author"><name><surname>Martens</surname><given-names>J.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group>, <article-title>Generating text with recurrent neural networks</article-title>, <source>Proceedings of the 28th international conference on machine learning (ICML-11)</source>, <year>2011</year>, pp. 1017&#x02013;1024</mixed-citation></ref><ref id="cit32"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Lipton</surname><given-names>Z. C.</given-names></name>
</person-group>, <person-group person-group-type="author"><name><surname>Berkowitz</surname><given-names>J.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Elkan</surname><given-names>C.</given-names></name></person-group>, <article-title>A critical review of recurrent neural networks for sequence learning</article-title>, <source>arXiv</source>, <year>2015</year>, preprint, arXiv:1506.00019, <pub-id pub-id-type="doi">10.48550/arXiv.1506.00019</pub-id></mixed-citation></ref><ref id="cit33"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vaswani</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Shazeer</surname><given-names>N.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Parmar</surname><given-names>N.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Uszkoreit</surname><given-names>J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Jones</surname><given-names>L.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Gomez</surname><given-names>A. N.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Kaiser</surname><given-names>&#x00141;.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Polosukhin</surname><given-names>I.</given-names></name>
</person-group><article-title>Attention is all you need</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>30</volume><fpage>5998</fpage><lpage>6008</lpage></element-citation></ref><ref id="cit34"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hochreiter</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Schmidhuber</surname><given-names>J.</given-names></name>
</person-group><article-title>Long short-term memory</article-title><source>Neural Comput.</source><year>1997</year><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id>
</element-citation></ref><ref id="cit35"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tsai</surname><given-names>S.-T.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Kuo</surname><given-names>E.-J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Tiwary</surname><given-names>P.</given-names></name>
</person-group><article-title>Learning molecular dynamics with simple language model built upon long short-term memory neural network</article-title><source>Nat. Commun.</source><year>2020</year><volume>11</volume><fpage>5115</fpage><pub-id pub-id-type="doi">10.1038/s41467-020-18959-8</pub-id><pub-id pub-id-type="pmid">33037228</pub-id>
</element-citation></ref><ref id="cit36"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tsai</surname><given-names>S.-T.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Fields</surname><given-names>E.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>Y.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Kuo</surname><given-names>E.-J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Tiwary</surname><given-names>P.</given-names></name>
</person-group><article-title>Path sampling of recurrent neural networks by incorporating known physics</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><fpage>7231</fpage><pub-id pub-id-type="doi">10.1038/s41467-022-34780-x</pub-id><pub-id pub-id-type="pmid">36433982</pub-id>
</element-citation></ref><ref id="cit37"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Paszke</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Gross</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Massa</surname><given-names>F.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Lerer</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Bradbury</surname><given-names>J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Chanan</surname><given-names>G.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Killeen</surname><given-names>T.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>Z.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Gimelshein</surname><given-names>N.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Antiga</surname><given-names>L.</given-names></name>
</person-group><article-title>
<italic toggle="yes">et al.</italic>, Pytorch: an imperative style, high-performance deep learning library</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2019</year><volume>32</volume><fpage>8026</fpage><lpage>8037</lpage></element-citation></ref><ref id="cit38"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Robustelli</surname><given-names>P.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Piana</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Shaw</surname><given-names>D. E.</given-names></name>
</person-group><article-title>Developing a molecular dynamics force field for both folded and disordered protein states</article-title><source>Proc. Natl. Acad. Sci. U. S. A.</source><year>2018</year><volume>115</volume><fpage>E4758</fpage><lpage>E4766</lpage><pub-id pub-id-type="doi">10.1073/pnas.1800690115</pub-id><pub-id pub-id-type="pmid">29735687</pub-id>
</element-citation></ref><ref id="cit39"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>Shaw</surname><given-names>D. E.</given-names></name>
</person-group>, <person-group person-group-type="author"><name><surname>Dror</surname><given-names>R. O.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Salmon</surname><given-names>J. K.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Grossman</surname><given-names>J.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Mackenzie</surname><given-names>K. M.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Bank</surname><given-names>J. A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Young</surname><given-names>C.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Deneroff</surname><given-names>M. M.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Batson</surname><given-names>B.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Bowers</surname><given-names>K. J.</given-names></name></person-group>, <article-title><italic toggle="yes">et al.</italic>, Millisecond-scale molecular dynamics simulations on Anton</article-title>, <source>Proceedings of the conference on high performance computing networking, storage and analysis</source>, <year>2009</year>, pp. 1&#x02013;11</mixed-citation></ref><ref id="cit40"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>Zhai</surname><given-names>J.</given-names></name>
</person-group>, <person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Chen</surname><given-names>J.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>He</surname><given-names>Q.</given-names></name></person-group>, <article-title>Autoencoder and its various variants</article-title>, <source>2018 IEEE international conference on systems, man, and cybernetics (SMC)</source>, <year>2018</year>, pp. 415&#x02013;419</mixed-citation></ref><ref id="cit41"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liou</surname><given-names>C.-Y.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>W.-C.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Liou</surname><given-names>J.-W.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Liou</surname><given-names>D.-R.</given-names></name>
</person-group><article-title>Autoencoder for words</article-title><source>Neurocomputing</source><year>2014</year><volume>139</volume><fpage>84</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2013.09.055</pub-id></element-citation></ref><ref id="cit42"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Maroteaux</surname><given-names>L.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Campanelli</surname><given-names>J. T.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Scheller</surname><given-names>R. H.</given-names></name>
</person-group><article-title>Synuclein: a neuron-specific protein localized to the nucleus and presynaptic nerve terminal</article-title><source>J. Neurosci.</source><year>1988</year><volume>8</volume><fpage>2804</fpage><lpage>2815</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.08-08-02804.1988</pub-id><pub-id pub-id-type="pmid">3411354</pub-id>
</element-citation></ref><ref id="cit43"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vekrellis</surname><given-names>K.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Rideout</surname><given-names>H. J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Stefanis</surname><given-names>L.</given-names></name>
</person-group><article-title>Neurobiology of &#x003b1;-synuclein</article-title><source>Mol. Neurobiol.</source><year>2004</year><volume>30</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1385/MN:30:1:001</pub-id><pub-id pub-id-type="pmid">15247485</pub-id>
</element-citation></ref><ref id="cit44"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Spillantini</surname><given-names>M. G.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>M. L.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>V. M.-Y.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Trojanowski</surname><given-names>J. Q.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Jakes</surname><given-names>R.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Goedert</surname><given-names>M.</given-names></name>
</person-group><article-title>&#x003b1;-Synuclein in Lewy bodies</article-title><source>Nature</source><year>1997</year><volume>388</volume><fpage>839</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1038/42166</pub-id><pub-id pub-id-type="pmid">9278044</pub-id>
</element-citation></ref><ref id="cit45"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Breydo</surname><given-names>L.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J. W.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Uversky</surname><given-names>V. N.</given-names></name>
</person-group><article-title>&#x003b1;-Synuclein misfolding and Parkinson's disease</article-title><source>Biochim. Biophys. Acta, Mol. Basis Dis.</source><year>2012</year><volume>1822</volume><fpage>261</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.bbadis.2011.10.002</pub-id><pub-id pub-id-type="pmid">22024360</pub-id>
</element-citation></ref><ref id="cit46"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kahle</surname><given-names>P. J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Haass</surname><given-names>C.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Kretzschmar</surname><given-names>H. A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Neumann</surname><given-names>M.</given-names></name>
</person-group><article-title>Structure/function of &#x003b1;-synuclein in health and disease: rational development of animal models for Parkinson&#x02019;s and related diseases</article-title><source>J. Neurochem.</source><year>2002</year><volume>82</volume><issue>3</issue><fpage>449</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1046/j.1471-4159.2002.01020.x</pub-id><pub-id pub-id-type="pmid">12153470</pub-id>
</element-citation></ref><ref id="cit47"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Menon</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Mondal</surname><given-names>J.</given-names></name>
</person-group><article-title>Conformational plasticity in &#x003b1;-synuclein and how crowded environment modulates it</article-title><source>J. Phys. Chem. B</source><year>2023</year><volume>127</volume><fpage>4032</fpage><lpage>4049</lpage><pub-id pub-id-type="doi">10.1021/acs.jpcb.3c00982</pub-id><pub-id pub-id-type="pmid">37114769</pub-id>
</element-citation></ref><ref id="cit48"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Farrell</surname><given-names>F.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Marchetti</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Marenduzzo</surname><given-names>D.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Tailleur</surname><given-names>J.</given-names></name>
</person-group><article-title>Pattern formation in self-propelled particles with density-dependent motility</article-title><source>Phys. Rev. Lett.</source><year>2012</year><volume>108</volume><fpage>248101</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.108.248101</pub-id><pub-id pub-id-type="pmid">23004336</pub-id>
</element-citation></ref><ref id="cit49"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tyson</surname><given-names>R.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Lubkin</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Murray</surname><given-names>J. D.</given-names></name>
</person-group><article-title>A minimal mechanism for bacterial pattern formation</article-title><source>Proc. R. Soc. London, Ser. B</source><year>1999</year><volume>266</volume><fpage>299</fpage><lpage>304</lpage></element-citation></ref><ref id="cit50"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ben-Jacob</surname><given-names>E.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>I.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Levine</surname><given-names>H.</given-names></name>
</person-group><article-title>Cooperative self-organization of microorganisms</article-title><source>Adv. Phys.</source><year>2000</year><volume>49</volume><fpage>395</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1080/000187300405228</pub-id></element-citation></ref><ref id="cit51"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bera</surname><given-names>P.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Wasim</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Ghosh</surname><given-names>P.</given-names></name>
</person-group><article-title>A mechanistic understanding of microcolony morphogenesis: coexistence of mobile and sessile aggregates</article-title><source>Soft Matter</source><year>2023</year><volume>19</volume><fpage>1034</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1039/D2SM01365G</pub-id><pub-id pub-id-type="pmid">36648295</pub-id>
</element-citation></ref><ref id="cit52"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cates</surname><given-names>M. E.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Tailleur</surname><given-names>J.</given-names></name>
</person-group><article-title>Motility-induced phase separation</article-title><source>Annu. Rev. Condens. Matter Phys.</source><year>2015</year><volume>6</volume><fpage>219</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1146/annurev-conmatphys-031214-014710</pub-id></element-citation></ref><ref id="cit53"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fily</surname><given-names>Y.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Marchetti</surname><given-names>M. C.</given-names></name>
</person-group><article-title>Athermal phase separation of self-propelled particles with no alignment</article-title><source>Phys. Rev. Lett.</source><year>2012</year><volume>108</volume><fpage>235702</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.108.235702</pub-id><pub-id pub-id-type="pmid">23003972</pub-id>
</element-citation></ref><ref id="cit54"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Buttinoni</surname><given-names>I.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Bialk&#x000e9;</surname><given-names>J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>K&#x000fc;mmel</surname><given-names>F.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>L&#x000f6;wen</surname><given-names>H.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Bechinger</surname><given-names>C.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Speck</surname><given-names>T.</given-names></name>
</person-group><article-title>Dynamical clustering and phase separation in suspensions of self-propelled colloidal particles</article-title><source>Phys. Rev. Lett.</source><year>2013</year><volume>110</volume><fpage>238301</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.110.238301</pub-id><pub-id pub-id-type="pmid">25167534</pub-id>
</element-citation></ref><ref id="cit55"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bera</surname><given-names>P.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Wasim</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Ghosh</surname><given-names>P.</given-names></name>
</person-group><article-title>Interplay of cell motility and self-secreted extracellular polymeric substance induced depletion effects on spatial patterning in a growing microbial colony</article-title><source>Soft Matter</source><year>2023</year><volume>19</volume><fpage>8136</fpage><lpage>8149</lpage><pub-id pub-id-type="doi">10.1039/D3SM01144E</pub-id><pub-id pub-id-type="pmid">37847026</pub-id>
</element-citation></ref><ref id="cit56"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ariel</surname><given-names>G.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Rabani</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Benisty</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Partridge</surname><given-names>J. D.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Harshey</surname><given-names>R. M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Be'er</surname><given-names>A.</given-names></name>
</person-group><article-title>Swarming bacteria migrate by L&#x000e9;vy Walk</article-title><source>Nat. Commun.</source><year>2015</year><volume>6</volume><fpage>8396</fpage><pub-id pub-id-type="doi">10.1038/ncomms9396</pub-id><pub-id pub-id-type="pmid">26403719</pub-id>
</element-citation></ref><ref id="cit57"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ilkanaiv</surname><given-names>B.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Kearns</surname><given-names>D. B.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Ariel</surname><given-names>G.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Be'er</surname><given-names>A.</given-names></name>
</person-group><article-title>Effect of cell aspect ratio on swarming bacteria</article-title><source>Phys. Rev. Lett.</source><year>2017</year><volume>118</volume><fpage>158002</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.118.158002</pub-id><pub-id pub-id-type="pmid">28452529</pub-id>
</element-citation></ref><ref id="cit58"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Be&#x02019;er</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Ilkanaiv</surname><given-names>B.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Gross</surname><given-names>R.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Kearns</surname><given-names>D. B.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Heidenreich</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>B&#x000e4;r</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Ariel</surname><given-names>G.</given-names></name>
</person-group><article-title>A phase diagram for bacterial swarming</article-title><source>Commun. Phys.</source><year>2020</year><volume>3</volume><fpage>66</fpage><pub-id pub-id-type="doi">10.1038/s42005-020-0327-1</pub-id></element-citation></ref><ref id="cit59"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bera</surname><given-names>P.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Wasim</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Mondal</surname><given-names>J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Ghosh</surname><given-names>P.</given-names></name>
</person-group><article-title>Mechanistic underpinning of cell aspect ratio-dependent emergent collective motions in swarming bacteria</article-title><source>Soft Matter</source><year>2021</year><volume>17</volume><fpage>7322</fpage><lpage>7331</lpage><pub-id pub-id-type="doi">10.1039/D1SM00311A</pub-id><pub-id pub-id-type="pmid">34286783</pub-id>
</element-citation></ref><ref id="cit60"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Isele-Holder</surname><given-names>R. E.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Elgeti</surname><given-names>J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Gompper</surname><given-names>G.</given-names></name>
</person-group><article-title>Self-propelled worm-like filaments: spontaneous spiral formation, structure, and dynamics</article-title><source>Soft Matter</source><year>2015</year><volume>11</volume><fpage>7181</fpage><lpage>7190</lpage><pub-id pub-id-type="doi">10.1039/C5SM01683E</pub-id><pub-id pub-id-type="pmid">26256415</pub-id>
</element-citation></ref><ref id="cit61"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Kingma</surname><given-names>D. P.</given-names></name>
</person-group> and <person-group person-group-type="author"><name><surname>Welling</surname><given-names>M.</given-names></name></person-group>, <article-title>Auto-encoding variational bayes</article-title>, <source>arXiv</source>, <year>2013</year>, preprint, arXiv:1312.6114, 10.48550, <pub-id pub-id-type="doi">10.48550/arXiv.1312.6114</pub-id></mixed-citation></ref><ref id="cit62"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kingma</surname><given-names>D. P.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Welling</surname><given-names>M.</given-names></name>
</person-group><article-title>
<italic toggle="yes">et al.</italic>, An introduction to variational autoencoders</article-title><source>Found. Trends Mach. Learn.</source><year>2019</year><volume>12</volume><fpage>307</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1561/2200000056</pub-id></element-citation></ref><ref id="cit63"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Doersch</surname><given-names>C.</given-names></name>
</person-group>, <article-title>Tutorial on variational autoencoders</article-title>, <source>arXiv</source>, <year>2016</year>, preprint, arXiv:1606.05908, <pub-id pub-id-type="doi">10.48550/arXiv.1606.05908</pub-id></mixed-citation></ref><ref id="cit64"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Goodfellow</surname><given-names>I.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Pouget-Abadie</surname><given-names>J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Mirza</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>B.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Warde-Farley</surname><given-names>D.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Ozair</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Courville</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Bengio</surname><given-names>Y.</given-names></name>
</person-group><article-title>Generative adversarial nets</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2014</year><volume>27</volume></element-citation></ref><ref id="cit65"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Brock</surname><given-names>A.</given-names></name>
</person-group>, <person-group person-group-type="author"><name><surname>Donahue</surname><given-names>J.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name></person-group>, <article-title>Large scale GAN training for high fidelity natural image synthesis</article-title>, <source>arXiv</source>, <year>2018</year>, preprint, arXiv:1809.11096, <pub-id pub-id-type="doi">10.48550/arXiv.1809.11096</pub-id></mixed-citation></ref><ref id="cit66"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>Sohl-Dickstein</surname><given-names>J.</given-names></name>
</person-group>, <person-group person-group-type="author"><name><surname>Weiss</surname><given-names>E.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Maheswaranathan</surname><given-names>N.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Ganguli</surname><given-names>S.</given-names></name></person-group>, <article-title>Deep unsupervised learning using nonequilibrium thermodynamics</article-title>, <source>International conference on machine learning</source>, <year>2015</year>, pp. 2256&#x02013;2265</mixed-citation></ref><ref id="cit67"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ho</surname><given-names>J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Jain</surname><given-names>A.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Abbeel</surname><given-names>P.</given-names></name>
</person-group><article-title>Denoising diffusion probabilistic models</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>6840</fpage><lpage>6851</lpage></element-citation></ref><ref id="cit68"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>Bera</surname><given-names>P.</given-names></name>
</person-group> and <person-group person-group-type="author"><name><surname>Mondal</surname><given-names>J.</given-names></name></person-group>, <article-title>Assessing Generative Diffusion Models for Enhanced Sampling of Folded and Disordered Protein States Across Scales and in All-atom Resolution</article-title>, <source>bioRxiv</source>, <year>2025</year>, preprint, <pub-id pub-id-type="doi">10.1101/2025.01.16.633470</pub-id></mixed-citation></ref><ref id="cit69"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sidky</surname><given-names>H.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>W.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Ferguson</surname><given-names>A. L.</given-names></name>
</person-group><article-title>Molecular latent space simulators</article-title><source>Chem. Sci.</source><year>2020</year><volume>11</volume><fpage>9459</fpage><lpage>9467</lpage><pub-id pub-id-type="doi">10.1039/D0SC03635H</pub-id><pub-id pub-id-type="pmid">34094212</pub-id>
</element-citation></ref><ref id="cit70"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Herron</surname><given-names>L.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Tiwary</surname><given-names>P.</given-names></name>
</person-group><article-title>From data to noise to data for mixing physics across temperatures with generative artificial intelligence</article-title><source>Proc. Natl. Acad. Sci. U. S. A.</source><year>2022</year><volume>119</volume><fpage>e2203656119</fpage><pub-id pub-id-type="doi">10.1073/pnas.2203656119</pub-id><pub-id pub-id-type="pmid">35925885</pub-id>
</element-citation></ref><ref id="cit71"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mehdi</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Smith</surname><given-names>Z.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Herron</surname><given-names>L.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Zou</surname><given-names>Z.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Tiwary</surname><given-names>P.</given-names></name>
</person-group><article-title>Enhanced Sampling with Machine Learning</article-title><source>Annu. Rev. Phys. Chem.</source><year>2024</year><volume>75</volume><pub-id pub-id-type="doi">10.1146/annurev-physchem-083122-125941</pub-id><pub-id pub-id-type="pmid">38382572</pub-id>
</element-citation></ref><ref id="cit72"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>Yao</surname><given-names>Y.</given-names></name>
</person-group> and <person-group person-group-type="author"><name><surname>Zeng</surname><given-names>W.</given-names></name></person-group>, <article-title>Learning Long Timescale in Molecular Dynamics by Nano-GPT</article-title>, <source>ICML 2024 AI for Science Workshop</source>, <year>2024</year></mixed-citation></ref><ref id="cit73"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mukherjee</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Mondal</surname><given-names>J.</given-names></name>
</person-group><article-title>Osmolyte-Induced collapse of a charged macromolecule</article-title><source>J. Phys. Chem. B</source><year>2019</year><volume>123</volume><fpage>4636</fpage><lpage>4644</lpage><pub-id pub-id-type="doi">10.1021/acs.jpcb.9b01383</pub-id><pub-id pub-id-type="pmid">31091409</pub-id>
</element-citation></ref><ref id="cit74"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Abraham</surname><given-names>M. J.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Murtola</surname><given-names>T.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Schulz</surname><given-names>R.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>P&#x000e1;ll</surname><given-names>S.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Smith</surname><given-names>J. C.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Hess</surname><given-names>B.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Lindahl</surname><given-names>E.</given-names></name>
</person-group><article-title>GROMACS: High performance molecular simulations through multi-level parallelism from laptops to supercomputers</article-title><source>SoftwareX</source><year>2015</year><volume>1</volume><fpage>19</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/j.softx.2015.06.001</pub-id></element-citation></ref><ref id="cit75"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Van Der Spoel</surname><given-names>D.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Lindahl</surname><given-names>E.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Hess</surname><given-names>B.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Groenhof</surname><given-names>G.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Mark</surname><given-names>A. E.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Berendsen</surname><given-names>H. J.</given-names></name>
</person-group><article-title>GROMACS: fast, flexible, and free</article-title><source>J. Comput. Chem.</source><year>2005</year><volume>26</volume><fpage>1701</fpage><lpage>1718</lpage><pub-id pub-id-type="doi">10.1002/jcc.20291</pub-id><pub-id pub-id-type="pmid">16211538</pub-id>
</element-citation></ref><ref id="cit76"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Abadi</surname><given-names>M.</given-names></name>
</person-group>, <person-group person-group-type="author"><name><surname>Agarwal</surname><given-names>A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Barham</surname><given-names>P.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Brevdo</surname><given-names>E.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Citro</surname><given-names>C.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Corrado</surname><given-names>G. S.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Davis</surname><given-names>A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Dean</surname><given-names>J.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Devin</surname><given-names>M.</given-names></name></person-group>, <article-title><italic toggle="yes">et al.</italic>, Tensorflow: large-scale machine learning on heterogeneous distributed systems</article-title>, <source>arXiv</source>, <year>2016</year>, preprint, arXiv:1603.04467, <pub-id pub-id-type="doi">10.48550/arXiv.1603.04467</pub-id></mixed-citation></ref><ref id="cit77"><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>Bisong</surname><given-names>E.</given-names></name>
</person-group> and <person-group person-group-type="author"><name><surname>Bisong</surname><given-names>E.</given-names></name></person-group>, <article-title>Tensorflow 2.0 and keras</article-title>, <source>Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners</source>, <year>2019</year>, pp. 347&#x02013;399</mixed-citation></ref><ref id="cit78"><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Scherer</surname><given-names>M. K.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Trendelkamp-Schroer</surname><given-names>B.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Paul</surname><given-names>F.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>P&#x000e9;rez-Hern&#x000e1;ndez</surname><given-names>G.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Hoffmann</surname><given-names>M.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Plattner</surname><given-names>N.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Wehmeyer</surname><given-names>C.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>Prinz</surname><given-names>J.-H.</given-names></name>
</person-group><person-group person-group-type="author">
<name><surname>No&#x000e9;</surname><given-names>F.</given-names></name>
</person-group><article-title>PyEMMA 2: A software package for estimation, validation, and analysis of Markov models</article-title><source>J. Chem. Theory Comput.</source><year>2015</year><volume>11</volume><fpage>5525</fpage><lpage>5542</lpage><pub-id pub-id-type="doi">10.1021/acs.jctc.5b00743</pub-id><pub-id pub-id-type="pmid">26574340</pub-id>
</element-citation></ref></ref-list></back></article>