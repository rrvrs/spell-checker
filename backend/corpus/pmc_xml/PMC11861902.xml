<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006446</article-id><article-id pub-id-type="pmc">PMC11861902</article-id><article-id pub-id-type="doi">10.3390/s25041217</article-id><article-id pub-id-type="publisher-id">sensors-25-01217</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Semi-Autonomous Telemanipulation Order-Picking Control Based on Estimating Operator Intent for Box-Stacking Storage Environments</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Min</surname><given-names>Donggyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><name><surname>Yoon</surname><given-names>Hojin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1814-2683</contrib-id><name><surname>Lee</surname><given-names>Donghun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-01217" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Hahn</surname><given-names>Michael E.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01217">Mechanical Engineering Department, Soongsil University, Seoul 06978, Republic of Korea; <email>dg.min0246@gmail.com</email> (D.M.); <email>yhj14531@gmail.com</email> (H.Y.)</aff><author-notes><corresp id="c1-sensors-25-01217"><label>*</label>Correspondence: <email>dhlee04@gmail.com</email></corresp></author-notes><pub-date pub-type="epub"><day>17</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1217</elocation-id><history><date date-type="received"><day>08</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>10</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>15</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Teleoperation-based order picking in logistics warehouse environments has been advancing steadily. However, the accuracy of such operations varies depending on the type of human&#x02013;robot interface (HRI) employed. Immersive HRI, which uses a head-mounted display (HMD) and controllers, can significantly reduce task accuracy due to the limited field of view in virtual environments. To address this limitation, this study proposes a semi-autonomous telemanipulation order-picking control method based on operator intent estimation using intersection points between the end-effector and the target logistics plane in box-stacking storage environments. The proposed method consists of two stages. The first stage involves operator intent estimation, which approximates the target logistics plane using objects identified through camera vision and calculates the intersection points by intersecting the end-effector heading vector with the plane. These points are accumulated and modeled as a Gaussian distribution, with the probability density function (PDF) of each target object treated as its likelihood. Bayesian probability filtering is then applied to estimate target probabilities, and predefined conditions are used to switch control between autonomous and manual controllers. Results show that the proposed operator intent estimation method identified the correct target in 74.6% of the task&#x02019;s duration. The proposed semi-autonomous control method successfully transferred control to the autonomous controller within 32.2% of the total task duration using a combination of three parameters. This approach inferred operator intent based solely on manipulator motion and reduced the fatigue of the operator. This method demonstrates potential for broad application in teleoperation systems, offering high operational efficiency regardless of operator expertise or training level.</p></abstract><kwd-group><kwd>logistics warehouse environment</kwd><kwd>suction-based telemanipulation order picking</kwd><kwd>semi-autonomous control method</kwd><kwd>operator intent estimation</kwd></kwd-group><funding-group><award-group><funding-source>Ministry of Education</funding-source><award-id>RS-2022-NR073933</award-id></award-group><award-group><funding-source>Korean Government (MOTIE)</funding-source><award-id>P0017123</award-id></award-group><funding-statement>This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (RS-2022-NR073933) and a Korea Institute for Advancement of Technology (KIAT) grant funded by the Korean Government (MOTIE) (P0017123, HRD Program for Industrial Innovation).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01217"><title>1. Introduction</title><p>The advancement of robotics and teleoperation technologies has significantly enhanced the efficiency and ease of operations of logistics warehouses. Among the diverse tasks in logistics, such as transportation and packaging, Pick-and-Place operations stand out as a primary application of manipulators. Telemanipulation technologies enable operators to handle these complex and demanding tasks more effectively [<xref rid="B1-sensors-25-01217" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01217" ref-type="bibr">2</xref>].</p><p>A variety of human&#x02013;robot interfaces (HRI) are available for telemanipulation in logistics applications. Joystick-based telemanipulation provides relatively simple control, while haptic feedback systems enable fine-grained adjustments of grip forces [<xref rid="B3-sensors-25-01217" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01217" ref-type="bibr">4</xref>]. Virtual reality (VR)- and augmented reality (AR)-based telemanipulation offer an intuitive, 3D understanding of the robot&#x02019;s movements and its surrounding environment [<xref rid="B5-sensors-25-01217" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01217" ref-type="bibr">6</xref>].</p><p>Although these intuitive HRI systems improve the usability of telemanipulation for operators, they often fall short in tasks requiring high precision. Specifically, as shown in <xref rid="sensors-25-01217-f001" ref-type="fig">Figure 1</xref>, the success of picking operations using suction grippers is highly dependent on the relative pose between the suction cup and the surface of the object, as well as the unique characteristics of the object [<xref rid="B7-sensors-25-01217" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01217" ref-type="bibr">8</xref>]. Consequently, careful planning and precise execution of contact locations are critical for effective suction-based grasping. Additionally, in side-picking tasks, a precise relative pose between the suction cup and the surface of the target object becomes even more critical. Unlike down-facing picking tasks, excessive vertical forces applied to the side plane of the target object in these tasks can disrupt the arrangement of other objects. This may exacerbate discrepancies between the virtual and real environments or cause interference with other objects.</p><p>In teleoperation environments, operators are generally required to independently plan and execute these contact locations. However, HRI limitations, such as restricted visibility of the workspace, can make it challenging to achieve precise Pick-and-Place operations using telemanipulation systems alone [<xref rid="B9-sensors-25-01217" ref-type="bibr">9</xref>]. For instance, camera-based interfaces often fail to provide sufficient depth information for unstructured objects, while VR/AR-based interfaces may face precision challenges in environments with limited visibility [<xref rid="B10-sensors-25-01217" ref-type="bibr">10</xref>].</p><p>To overcome these challenges, this study proposes a semi-autonomous control method for telemanipulation tasks. The proposed approach predicts the operator&#x02019;s intent during picking operations, identifies the target object, and employs sensor-based control of the end-effector to perform suction-based picking. This method switches between manual operation by the operator and automated control by the robot, leveraging enhanced precision to improve both the efficiency and accuracy of picking operations in logistics settings [<xref rid="B11-sensors-25-01217" ref-type="bibr">11</xref>].</p><sec><title>Related Work</title><p>Semi-autonomous control methods for telemanipulation have been extensively studied to improve operational accuracy and reduce the cognitive load on human operators.</p><list list-type="bullet"><list-item><p>Rubagotti et al. proposed a teleoperation framework integrating Model Predictive Control to enhance task performance in cluttered environments [<xref rid="B12-sensors-25-01217" ref-type="bibr">12</xref>]. The study employs a motion capture system to predict the hand posture of a human operator. Based on the data collected from the motion capture system, the hand posture is defined relative to the shoulder joint and used to configure the position and orientation of the robot&#x02019;s end-effector. The predicted hand posture is then fed into a Model Predictive Control algorithm, enabling real-time replanning of the robot&#x02019;s motion to avoid obstacles while following movements of the operator.</p></list-item><list-item><p>Castro et al. proposed a semi-autonomous prosthetic control system that uses a depth sensor mounted on the hand to improve continuous control [<xref rid="B13-sensors-25-01217" ref-type="bibr">13</xref>]. This system automatically adjusts the shape, size, and wrist orientation of a prosthetic hand based on environmental conditions. The depth sensor continuously estimates the size and orientation of target objects, allowing for dynamic adaptation until the user is satisfied. This approach enables intuitive interaction by allowing users to understand actions of the system through their movements and adjust their intent accordingly.</p></list-item><list-item><p>Hauser et al. developed a framework for teleoperation that predicts and recognizes user intent to support autonomous task execution [<xref rid="B14-sensors-25-01217" ref-type="bibr">14</xref>]. This framework employs a Dynamic Bayesian Network to analyze user inputs and infer task types using the Freeform Task Inference Engine. The inferred task parameters are passed to a Cooperative Motion Planner, which pre-plans the robot&#x02019;s movements to follow the predicted trajectory efficiently.</p></list-item><list-item><p>Menon et al. proposed a shared control system for assistive robots based on user intent prediction and hyperdimensional recall of reactive behavior [<xref rid="B15-sensors-25-01217" ref-type="bibr">15</xref>]. Their system leverages Hyperdimensional Computing to analyze electromyography signals and accelerometer data, identifying recent user actions and predicting subsequent actions using probabilistic reasoning. By combining intent recognition with sensor feedback, the system determines the most appropriate operations, reducing the control burden while maintaining autonomy.</p></list-item><list-item><p>Wang et al. introduced an intent inference method for shared-control teleoperation systems that consider user behavior [<xref rid="B16-sensors-25-01217" ref-type="bibr">16</xref>]. Their approach uses Short-Term Path Distance and Short-Term Directionality Deviation to model user behavior, enabling goal prediction and path planning. This method enhances the ability of the robot to accurately understand user intentions and perform tasks aligned with their expectations.</p></list-item><list-item><p>Jain and Argall proposed a Recursive Bayesian Intent Recognition model for assistive teleoperation systems [<xref rid="B17-sensors-25-01217" ref-type="bibr">17</xref>]. This model evaluates joystick input direction and magnitude to calculate the likelihood of achieving the intended target of the user. By estimating transition probabilities in relation to potential goals, the system supports efficient intent recognition and collaborative control in human&#x02013;robot interactions.</p></list-item></list></sec></sec><sec id="sec2-sensors-25-01217"><title>2. Materials and Methods</title><p>In this section, we introduce the components of the proposed system, including the hardware, and then we present the core methods. As shown in <xref rid="sensors-25-01217-f002" ref-type="fig">Figure 2</xref>, the proposed approach consists of two main methods. The first is an operator intent estimation method, which operates at the perception level, and the second is a semi-autonomous control method that transitions based on specific triggers at the control level. A detailed explanation of these two methods is provided in <xref rid="sec2dot2-sensors-25-01217" ref-type="sec">Section 2.2</xref> and <xref rid="sec2dot3-sensors-25-01217" ref-type="sec">Section 2.3</xref>.</p><p>In summary, the operator intent estimation method utilizes an intersection-based approach, incorporating manipulator motion data and target object position information. This method estimates the likelihood that a given target object is the one the operator intends to pick by analyzing how far the object&#x02019;s center is from the Gaussian probability distribution derived from the manipulator&#x02019;s motion, using both the distribution&#x02019;s center and a covariance matrix.</p><p>Furthermore, in the semi-autonomous control method, the probability calculated in the previous step is used to continuously evaluate whether predefined conditions&#x02014;determined through a parameter study&#x02014;are met. If the conditions are satisfied, control authority is transferred to the autonomous controller, enabling a transition to fully autonomous control. All controllers in this process ensure reliability through the use of the RTDE Python package provided by Universal Robots.</p><sec id="sec2dot1-sensors-25-01217"><title>2.1. System Components</title><p>This study developed a telemanipulation system where a human operator performs tasks in an immersive environment using an HMD and a controller. An RGB-D camera was installed near the end-effector to enable the robot to identify appropriate contact locations for the suction cup.</p><p>The manipulator used in this study is the UR5e, an industrial robot known for its precision and reliability. Excluding the suction cup, it features an 850 mm working radius, a 5 kg payload, and &#x000b1;0.03 mm repeatability. Its built-in force and torque sensors provide appropriate feedback during the picking process, enabling robust and efficient Pick-and-Place operations.</p><p>For the RGB-D camera, Realsense D405 was selected due to its ability to provide high-precision depth data at close ranges. The camera is mounted on the upper side of the end-effector, allowing it to recognize objects in the logistics domain and provide the system with information on contact locations and obstacles. The Realsense D405 has a maximum depth range of 0.5 m, which aligns well with the 850 mm working radius of the manipulator. Additionally, it offers &#x000b1;0.1 mm depth accuracy, a resolution of 1280 &#x000d7; 800, and a maximum frame rate of 90 FPS, ensuring high performance.</p><p>The hardware described above is controlled by a human operator using Meta Quest2 v50 through teleoperation software developed in Unity. This software primarily serves two functions: providing the operator with precise visual feedback in a fully immersive environment and collecting sensor data from the controller.</p><p>The process of providing an immersive environment consists of two key steps. The first step involves generating a point cloud map that closely resembles the experimental environment using RTAB-Map [<xref rid="B18-sensors-25-01217" ref-type="bibr">18</xref>]. In this study, we used the open-source RTAB-Map package without modifications. The second step is transmitting the generated point cloud map from ROS to the Unity client. During this process, the transmitted point cloud is rendered in the Unity client without any data loss. Through this approach, the operator experiences a fully realized teleoperation environment while seamlessly interacting with the system.</p><p>The second function of this software is collecting sensor data from the controller. It gathers gyroscope sensor data from the controller and transmits them to the robot, which then converts the data into appropriate linear velocity commands using manipulator inverse kinematics. Furthermore, when the proposed semi-autonomous control method is activated, the system provides the human operator with haptic feedback signals, intuitively indicating that manual operation is no longer required.</p><p>Meta Quest2 was chosen for this study as it provides an immersive experience while simultaneously collecting real-time position and velocity data from the controller. As shown in <xref rid="sensors-25-01217-t001" ref-type="table">Table 1</xref>, it features a resolution of 1832 &#x000d7; 1920 per eye, supports a refresh rate of 72&#x02013;90 Hz, and includes four built-in cameras for 6-degree-of-freedom (6 DoF) tracking. This enables precise motion tracking without the need for external sensors. Additionally, the controller is equipped with an accelerometer and a gyroscope, allowing for accurate position and velocity data acquisition.</p></sec><sec id="sec2dot2-sensors-25-01217"><title>2.2. Estimating Operator Intent</title><p>As shown in <xref rid="sensors-25-01217-f003" ref-type="fig">Figure 3</xref>, the proposed intent estimation method of the human operator is designed based on the intersection point between the heading vector of the end-effector and the target logistics plane. Here, the target logistics plane is defined as the approximated plane based on the set of center points of the target objects. This method assumes that as the end-effector moves along the optimal path to pick a specific target, the intersection points generated at a given time <italic toggle="yes">t</italic> exhibit consistent tendencies.</p><p>First, the intersection points between the heading vector of the end-effector and the target logistics plane are calculated at each time step <italic toggle="yes">t</italic>. The heading vector of the end-effector is defined as a vector originating from position of the end-effector at time <italic toggle="yes">t</italic> and directed along its linear velocity. The target logistics plane is determined by calculating eigenvectors and eigenvalues from the set of central points on the front-facing surfaces of the target objects. The eigenvector corresponding to the smallest eigenvalue is selected as the normal vector of the target logistics plane. This normal vector is incorporated into the plane equation, and the plane is defined to have a <italic toggle="yes">d</italic> value that best approximates the solution. These intersection points are continuously accumulated from the start to the end of the picking operation, and, at each loop, the set of intersection points is modeled as a Gaussian distribution. Subsequently, the probability density function (PDF) values for all detected target objects are computed based on this Gaussian distribution.</p><p>Next, the obtained PDF values are normalized such that its value at the center of the distribution is 1.0. The resulting normalized Gaussian density (NGD) represents the relative distance from the center of the Gaussian distribution, with values increasing as the distance from the distribution center decreases, making it suitable for probability calculations.</p><p>Intent estimation is conducted using a Bayesian probability-based framework to assign each detected object an independent probability of being the final target object, determined by its NGD value. As shown in <xref rid="sensors-25-01217-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-01217-f005" ref-type="fig">Figure 5</xref>, the application of a Bayesian probability filter ensures temporal consistency and mitigates noise in the NGD model, enhancing the stability and reliability of the results. High probabilities observed momentarily are balanced by incorporating prior probabilities, achieving robustness in inference.</p><p>Each object is initialized with equal prior probabilities, and the NGD values at each time step are utilized as likelihood inputs within the Bayesian probability filter. These likelihoods are bifurcated into probabilities representing whether an object is the target object or not, enabling the calculation of posterior probabilities [<xref rid="B19-sensors-25-01217" ref-type="bibr">19</xref>]. The process is performed recursively in every loop from the start of the picking task, facilitating the cumulative update of inference probabilities for all objects.</p><p>By leveraging Bayesian inference, the system seamlessly integrates prior probabilities with new observations to dynamically adjust to uncertainty in operator intent estimation [<xref rid="B17-sensors-25-01217" ref-type="bibr">17</xref>]. This recursive approach ensures robust and real-time intent estimation, even in dynamically changing environments [<xref rid="B20-sensors-25-01217" ref-type="bibr">20</xref>].</p><p>Specifically, each object is initialized with equal prior probabilities, and the NGD values at each time step are used as likelihood inputs in the Bayesian probability filter. These values are bifurcated into the probabilities of being or not being the target object, allowing posterior probabilities to be calculated. This process is repeated in every loop from the start of the picking task, enabling cumulative calculation of inference probabilities for all target objects.</p></sec><sec id="sec2dot3-sensors-25-01217"><title>2.3. Semi-Autonomous Control Method</title><p>The proposed semi-autonomous control method operates based on the geometric information and probability of the most probable target object, as provided by the proposed intersection-based operator intent estimation.</p><p>First, as shown in <xref rid="sensors-25-01217-f006" ref-type="fig">Figure 6</xref>, the manual control input from the human operator and the autonomous control input from the autonomous controller are calculated independently. The manual control input is derived from the gyroscope and joystick inputs of the controller in the VR-based HRI, which are processed and converted into linear velocity commands for the end-effector. In contrast, the autonomous control input is determined by the path planner of the end-effector.</p><p>Specifically, the optimal path for the target object with the highest probability is generated. This optimal path treats all objects except the target as obstacles that may cause collisions. The final goal pose is determined by selecting the center point of the surface on the target object detected by the RGB-D camera, where the angle between the normal vector of the target logistics plane and the normal vector of the target object surface is minimized. The selected normal vector is defined as the heading vector and used to generate the optimal path.</p><p>Path planning algorithms vary depending on the objectives, making the selection of an appropriate method crucial [<xref rid="B21-sensors-25-01217" ref-type="bibr">21</xref>]. In this study, the Open Motion Planning Library (OMPL) was employed to generate the optimal path. OMPL is a path planning library that incorporates various sampling-based algorithms, offering advantages like scalability, reusability, rapid computation, and effective obstacle avoidance [<xref rid="B22-sensors-25-01217" ref-type="bibr">22</xref>]. This study leveraged OMPL for its fast computation capabilities, enabling real-time updates of the optimal path in every loop.</p><p>As shown in <xref rid="sensors-25-01217-f007" ref-type="fig">Figure 7</xref>, the path generated by OMPL provides the position, velocity, and effort values required for each joint of the manipulator. During path execution, these joint values are converted into the linear velocity of the end-effector through forward kinematics, ensuring compatibility with the control commands from the VR-based HRI.</p><p>The independently calculated linear velocity commands from the VR-based HRI and the autonomous controller are determined based on the Bayesian probability provided by the intersection-based operator intent estimation. This approach infers the intent of the operator from manual inputs and transitions to automated control when sufficient trust is established, aiming to overcome the operational limitations of human operators in the logistics warehouse environment.</p><p>Specifically, the Bayesian probability obtained in the preceding stage represents the independent probability of each object being the target object, and three conditions are continuously checked for fulfillment. The first condition is whether the highest probability exceeds the predefined threshold, the second is whether it is significantly higher than the probabilities of other objects, and the last is whether sufficient intersections are accumulated to produce stable data.</p><p>The first condition ensures that the highest probability surpasses the threshold, which indicates that the center of the Gaussian distribution continuously aligns with center coordinates of the target object. Consistently high probabilities are required to produce a robust posterior probability through the Bayesian probability filter, making this condition the most critical factor.</p><p>The second condition verifies whether the probability difference between the object with the highest likelihood and others is sufficiently large. Because each probability is independently calculated, multiple objects can simultaneously have a high probability of being the target. Thus, even if one object has a high likelihood, the system checks the relative difference to ensure additional stability.</p><p>Additionally, in autonomous control mode, unexpected obstacles or interruptions during operation allow the human operator to issue a forced control transfer command to regain control. In the absence of such exceptions, once the task transitions to automation, it remains in automated mode until the task is completed. During this process, the VR-based HRI provides haptic feedback in the form of vibrations to notify the human operator that control has been fully transferred to the automated system. Furthermore, during automated operation, inputs from the preceding intent estimation stage are ignored to address potential exceptions [<xref rid="B23-sensors-25-01217" ref-type="bibr">23</xref>].</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-01217"><title>3. Results and Discussion</title><sec id="sec3dot1-sensors-25-01217"><title>3.1. Experimental Procedure</title><p>The experimental design of this study was developed to achieve two primary objectives. First, it evaluates the accuracy of the proposed intent estimation method for the human operator. Second, it examines how quickly and reliably the control authority can be transferred using the proposed method.</p><p>Specifically, the manipulator is placed in its home pose, after which the experimenter instructs the human operator, equipped with the VR-based HRI, on which target object to pick. The human operator performs the assigned task, and the experimenter assigns a different target object for each trial. To minimize disturbances caused by the skill level of the operator, the same target object is assigned for at least five repetitions. This process is repeated multiple times with different participants. In addition, during the experiment, the following data are recorded during the experiments.</p><p>First, to evaluate the suitability of the proposed intersection-based operator intent estimation, data, such as the center position and the covariance matrix of the Gaussian distribution, the geometric information of all target objects, PDF values, and posterior probabilities derived from Bayesian probability filtering, are collected.</p><p>Next, to assess the proposed semi-autonomous control method, raw manual control input, autonomous control input, and combined control input values generated through the intersection-based operator intent estimation are recorded. Additionally, to determine how accurately these control commands are executed, data on the position, velocity, and effort values of each manipulator joint, as well as the linear velocity and angular velocity of the end-effector calculated through forward kinematics, are collected.</p></sec><sec id="sec3dot2-sensors-25-01217"><title>3.2. Experimental Environment</title><p>The experimental environment in this study simulated a logistics warehouse environment, specifically focusing on box-stacking storage. The scenario replicated a mobile manipulator reaching the front of a rack where boxes were stacked. Within the reachable workspace of the manipulator, nine box-shaped objects were placed as task candidates. Each box had a square April Tag measuring 40 mm attached to the center of its front surface. April Tag is a visual marker system designed to ensure stable recognition under various lighting conditions and angles. It allows for precise estimation of the 3D position, orientation, and ID of the tag through cameras, making it well-suited to this experiment [<xref rid="B24-sensors-25-01217" ref-type="bibr">24</xref>]. The IDs are unique to each object and randomly assigned without duplication. Moreover, the position of the April Tag, once detected, is updated only when it is deemed valid, ensuring that the final position of the tag and the target is determined based on reliable data. This approach guarantees both the accuracy and continuity of the positional data.</p><p>Moreover, the boxes were divided into two size categories: six boxes measuring 220 mm &#x000d7; 90 mm and three boxes measuring 270 mm &#x000d7; 150 mm. This arrangement allowed for the evaluation of performance differences based on box size. The constructed experimental environment is illustrated in <xref rid="sensors-25-01217-f008" ref-type="fig">Figure 8</xref>, and the specific arrangement of each target relative to the base_link frame of the manipulator can be referenced therein.</p><p>Additionally, considering the working range of the manipulator, the distance between the mobile manipulator and the target objects falls within a relatively short range, making external interference unlikely during the experiment. Therefore, no additional noise factors were introduced in the experimental setup.</p></sec><sec id="sec3dot3-sensors-25-01217"><title>3.3. Data Analysis</title><p>All experimental data were recorded using the rosbag feature in ROS. Relevant data topics were selected and stored in *.bag format, which were then exported to CSV format. This method ensured that the data timestamps were based on the host system&#x02019;s time, allowing all data to be saved in a raw state with minimal loss. Because the resulting CSV files were asynchronous logs, the data were merged into 100 ms intervals for ease of processing.</p><p>To evaluate the performance of the proposed method, the following topics were recorded and analyzed. These data points were used for performance evaluation and analysis.</p><list list-type="bullet"><list-item><p>/intention/log/gaussian (geometry_msgs/PoseWithCovarianceStamped): Header, Gaussian distribution center coordinates, and covariance matrix.</p></list-item><list-item><p>/box_objects/pdf (custom_msgs/BoxObjectMultiArrayWithPDF): Header, geometric information of all vision-recognized target objects, and their PDF values at each time step.</p></list-item><list-item><p>/control_mode (std_msgs/UInt16): Information about the currently applied mode.</p></list-item><list-item><p>/desired_box (std_msgs/UInt16): Unique ID of the target object intended by the operator.</p></list-item></list><p>Additionally, all data collected in this experiment were recorded in a network-based teleoperation environment, making it essential to verify that network issues did not distort the data. To ensure this, the number of retransmitted packets, latency (RTT, Round Trip Time), and the data transfer rate were measured, with data collection performed using Wireshark software 4.2.10.</p></sec><sec id="sec3dot4-sensors-25-01217"><title>3.4. Experimental Results</title><sec id="sec3dot4dot1-sensors-25-01217"><title>3.4.1. Network Performance Analysis</title><p>Before analyzing the experimental data, network packet data were collected during the teleoperation experiments to measure the number of retransmitted packets, the server&#x02013;client Round Trip Time (RTT), and the data transfer rate per second. The results are shown in <xref rid="sensors-25-01217-t002" ref-type="table">Table 2</xref>.</p><p>The measurements showed an average of one retransmitted packet every 10 s, an average RTT of 5 ms, and an average data transfer rate of 21 kB/s. In general, in real-time communication environments, a network delay of less than 10 ms and a packet loss rate below 1% are known to have minimal impact on system performance. Additionally, considering that high-speed internet offers up to 100 Mbps and gigabit internet supports up to 1 Gbps, the data transmission rate observed in this experiment is well within acceptable limits and does not pose any performance issues.</p><p>Comparing these results with established benchmarks, the measured network performance in this experiment is considered highly stable. Therefore, the network environment used in this study is unlikely to introduce data distortion or affect experimental results.</p></sec><sec id="sec3dot4dot2-sensors-25-01217"><title>3.4.2. Phase I: Estimating Operator Intent</title><p>First, the accuracy of the proposed operator intent estimation method was evaluated by defining a loop where one iteration of the Bayesian probability filter was considered one step and prediction accuracy was measured. Prediction accuracy was calculated as follows:<disp-formula id="FD1-sensors-25-01217"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1.0</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000a0;</mml:mo><mml:mi>A</mml:mi><mml:mi>N</mml:mi><mml:mi>D</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">*</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>&#x0003e;</mml:mo><mml:mn>0.0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.0</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-01217"><label>(2)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This configuration was adopted because at the beginning of the task, all Gaussian distributions exhibited excessive uncertainty, resulting in uniformly low and similar prediction probabilities for all objects. Therefore, cases where the highest probability was equal to the second-highest probability were treated as exceptions.</p><p>As shown in <xref rid="sensors-25-01217-f009" ref-type="fig">Figure 9</xref>, the results showed that the average prediction success rate during the task was 74.7%, with a standard deviation of 0.152. Notably, with 50% of the data achieving a success rate of 81.3%, the majority of the dataset demonstrates high success rates. The lower average appears to be influenced by a small number of outliers with low success probabilities. This indicates that the proposed intersection-based operator intent estimation method maintains robust and reliable success rates overall.</p><p>Next, during the process of performing suction-based picking with varying targets, the Euclidean distance between the calculated center of the final Gaussian distribution and the actual center of the target object&#x02019;s front-facing plane was measured. This Euclidean distance will henceforth be referred to as the bias. Considering the dimensions of the box-shaped target objects used in the experiment (width: 220&#x02013;270 mm; height: 90&#x02013;150 mm), the average bias was deemed acceptable. However, for IDs 121 and 122 in <xref rid="sensors-25-01217-t003" ref-type="table">Table 3</xref>, the bias exceeded 100 mm, surpassing the minimum height of some objects, which could pose potential issues. This issue appears to be a fundamental limitation of using the heading vector to generate intersection points. While this approach performs adequately when the center of the target object is relatively close to the origin of the target logistics plane, it was observed to introduce significant offsets when the target object is positioned further away along specific axes.</p><p>Additionally, although there were variations in deviation depending on the target box position, the average bias was measured at 77.826 mm. This suggests that the proposed method can effectively determine boxes with a height of up to 80 mm. Furthermore, considering that the suction gripper used in this experiment has four suction cups, which require a minimum contact surface of 60 mm &#x000d7; 60 mm to perform a stable Pick-and-Place operation, this level of bias is deemed sufficiently acceptable.</p></sec><sec id="sec3dot4dot3-sensors-25-01217"><title>3.4.3. Phase II: Semi-Autonomous Control</title><p>Next, the accuracy of the proposed semi-autonomous control method was evaluated by examining how quickly the predefined thresholds could be exceeded. For this purpose, the semi-autonomous control method was not applied, and the human operator performed the picking task while the results from the Bayesian probability filter were recorded. The following three conditions were applied to these data:<list list-type="bullet"><list-item><p>The highest Bayesian probability exceeds the predefined threshold;</p></list-item><list-item><p>The difference between the highest and the second-highest Bayesian probabilities exceeds the predefined threshold;</p></list-item><list-item><p>The total number of intersection points exceeds the predefined threshold.</p></list-item></list></p><p>The first time these three conditions were all satisfied was divided by the total task duration to determine how much of the task had progressed when the conditions were met. Additionally, whether the object with the highest Bayesian probability at that time matched the object the operator intended to pick was recorded.</p><p>Multiple thresholds were tested for each of the three conditions. The thresholds ranged from 0.1 to 0.7, 0.1 to 0.5, and 1 to 20, respectively. The results were used to determine the optimal parameters by examining the time at which the conditions were met and whether the identified object matched the intended target.</p><p>Specifically, the ranges for three parameters were defined and then sliced into fixed intervals. For each combination of parameters within these ranges, we examined whether the predicted target object matched the predefined target and determined at which stage of the overall task the decision was made.</p><p>As shown in <xref rid="sensors-25-01217-t004" ref-type="table">Table 4</xref>, the resulting dataset was then reorganized by grouping identical parameter values, allowing us to calculate the recognition time ratio and the matching accuracy for each parameter. Although this approach requires significant computational time, it enables a comprehensive analysis of all parameter variations, making it the most accurate method for evaluation.</p><p>To evaluate the optimal parameters, the average recognition time and the matching accuracy were converted into scores. This transformation was necessary because the optimal condition in this study requires a low recognition time ratio (RTR) and high matching accuracy (MA), but their scales differ. Therefore, using Equations (3)&#x02013;(5), we normalized RTR and MA and then applied the weight distribution defined in this study to compute the final score. Its trend is shown in <xref rid="sensors-25-01217-f010" ref-type="fig">Figure 10</xref>.<disp-formula id="FD3-sensors-25-01217"><label>(3)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000a0;</mml:mo></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-01217"><label>(4)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>M</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD5-sensors-25-01217"><label>(5)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn><mml:mo>&#x02a2f;</mml:mo><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.7</mml:mn><mml:mo>&#x02a2f;</mml:mo><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Specifically, MA was normalized using a conventional scaling method. However, because RTR should reward faster recognition with a higher score, we applied an inverse normalization by subtracting the normalized value from 1.0, ensuring that it aligns with the same scale as MA. Finally, the two normalized scores were weighted at 0.3 and 0.7, respectively, reconstructing them into a final score ranging between 0.0 and 1.0.</p><p>Using these optimal parameters, it was observed that, on average, the target object could be identified at 34% of the total task duration. The average task duration across all tasks was 8.066 s, meaning the target object was identified approximately 2.74 s into the task.</p><p>In extreme cases, as shown in <xref rid="sensors-25-01217-f011" ref-type="fig">Figure 11</xref>, the target object was identified as early as 8.2% of the task or as late as 77.5%. In the former case, the target box ID recorded a high likelihood from the start, allowing for rapid identification. In the latter case, despite no external interference, the center of the Gaussian distribution was significantly offset from the center of the target object, causing the Bayesian probability filter to exceed the threshold much later.</p></sec></sec></sec><sec sec-type="conclusions" id="sec4-sensors-25-01217"><title>4. Conclusions</title><p>This study addresses the limitations of HMD-based immersive HRI in box-stacking storage logistics warehouse environments, including, specifically, the degradation of relative pose accuracy between the end-effector and the target object caused by restricted visibility during the final stages of a task. Existing solutions, such as extending the field of view with additional cameras, compromise intuitiveness, while pure digital-twin-based telemanipulation alone fails to improve the relative pose. To overcome these challenges, we proposed a semi-autonomous control method that estimates operator intent and transitions control to an autonomous controller once sufficient intent inference has been achieved, enabling the determination of the final relative pose.</p><p>The proposed method models the set of intersection points between the heading vector of the end-effector and the target logistics plane as a Gaussian distribution. The degree of deviation of each target object from this Gaussian model is calculated and used as the likelihood in a Bayesian probability filter. This approach achieved an average success rate of 74.7% across all tasks, where the proposed inference model correctly identified the object the user intended to pick. Additionally, the average bias between the center of the Gaussian distribution and the April Tag of the intended target object was approximately 77 mm, which is considered sufficiently accurate given the smallest object size used in the experiment (220 mm &#x000d7; 90 mm).</p><p>Control transfer was designed to occur when two conditions were simultaneously met: (1) the highest independent probability exceeded a predefined threshold and (2) the highest probability was significantly higher than those of other objects and (3) the number of intersection points was high enough to make stable decisions. This design accounts for the independent nature of the probability model. Experimental results showed that these conditions were satisfied at an average of 34.5% of the task duration. Given an average task duration of 8.066 s, user intent was determined within approximately 2.78 s.</p><p>However, some limitations were identified. First, a tendency for the center of the Gaussian distribution to exhibit specific offsets was observed. These offsets were not consistent along a single axis but were distinct and could potentially affect tasks with more densely stacked objects, even though they had minimal impact in the 3 &#x000d7; 3 box-stacking environment used in this study. This is presumably due to the lack of an effective outlier removal process. Therefore, incorporating a method to eliminate such outliers is expected to not only enhance the overall performance of the proposed approach but also reduce variance significantly.</p><p>Second, the stability of the control transfer trigger was insufficient. While the recognition time ratio for control transfer was, on average, 34.5%, the standard deviation was 0.256, indicating significant variability. The large difference between the minimum and maximum values further suggests that the trigger was not reliably stable across all tasks. Further studies are required to develop robust and consistent control transfer conditions.</p><p>These results stem from the fact that this experiment relies entirely on human operator control. Specifically, cases with Min ARTP occur when the operator&#x02019;s manipulation follows a nearly linear trajectory and the Gaussian distribution center, derived from the generated intersection points, closely aligns with the center of a specific object. Conversely, cases with abnormally high Max ARTP arise under opposite conditions.</p><p>Therefore, future research will focus on refining the Gaussian distribution model to better align its center with the target object&#x02019;s center or accurately measuring offsets for model correction. Specifically, cases where Max ARTP is abnormally high will be considered outliers, and appropriate filtering methods will be explored to address these anomalies.</p><p>Lastly, there are inherent limitations in this study due to its underlying assumptions. In a real warehouse environment, objects of various shapes are often randomly stacked, whereas this study focuses solely on box-shaped objects, with the additional constraint of requiring April Tag for identification. This suggests that in more complex real-world settings, the proposed method may be susceptible to noise factors, potentially affecting its performance.</p><p>Furthermore, external disturbances were minimally considered in this study. In actual environments, other robots, human workers, and unexpected incidents introduce significant disturbances. While the working range of the mobile manipulator structurally limits external interference, the absence of explicit consideration of these factors may still result in notable differences between the experimental setup and real-world applications.</p><p>Despite these limitations, this study successfully inferred operator intent based solely on manipulator motion, enabling the semi-automation of telemanipulation tasks and reducing operator fatigue.</p><p>Additionally, all of the above work was conducted in the GitHub repository at <uri xlink:href="https://github.com/7cmdehdrb/project_semi_remote">https://github.com/7cmdehdrb/project_semi_remote</uri> (accessed on 8 January 2025). The source code and the data, except for the libraries provided by the manufacturers of the RGB-D camera, the suction gripper, the manipulator, and Moveit, were written directly by the authors. These materials are open-source and freely available for anyone to access and use.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, D.M. and D.L.; methodology, D.M. and D.L.; software, D.M.; validation, D.M., H.Y. and D.L.; formal analysis, D.M., H.Y. and D.L.; investigation, D.M. and H.Y.; resources, D.L.; data curation, D.M.; writing&#x02014;original draft preparation, D.M. and D.L.; writing&#x02014;review and editing, D.M. and D.L.; visualization, D.M., H.Y. and D.L.; supervision, D.L.; project administration, D.L.; funding acquisition, D.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original data presented in this study are openly available in Zenodo at <uri xlink:href="https://doi.org/10.5281/zenodo.14625181">https://doi.org/10.5281/zenodo.14625181</uri>.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">HRI</td><td align="left" valign="middle" rowspan="1" colspan="1">Human&#x02013;robot interface</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HMD</td><td align="left" valign="middle" rowspan="1" colspan="1">Head-mounted display</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PDF</td><td align="left" valign="middle" rowspan="1" colspan="1">Probability density function</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NGD</td><td align="left" valign="middle" rowspan="1" colspan="1">Normalized Gaussian density </td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VR</td><td align="left" valign="middle" rowspan="1" colspan="1">Virtual reality</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AR</td><td align="left" valign="middle" rowspan="1" colspan="1">Augmented reality</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">OMPL</td><td align="left" valign="middle" rowspan="1" colspan="1">Open Motion Planning Library</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RTR</td><td align="left" valign="middle" rowspan="1" colspan="1">Recognition time ratio</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MA</td><td align="left" valign="middle" rowspan="1" colspan="1">Matching accuracy</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-01217"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Halawa</surname><given-names>F.</given-names></name>
<name><surname>Dauod</surname><given-names>H.</given-names></name>
<name><surname>Lee</surname><given-names>I.G.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Yoon</surname><given-names>S.W.</given-names></name>
<name><surname>Chung</surname><given-names>S.H.</given-names></name>
</person-group><article-title>Introduction of a real-time location system to enhance the warehouse safety and operational efficiency</article-title><source>Int. J. Prod. Econ.</source><year>2020</year><volume>224</volume><fpage>107541</fpage><pub-id pub-id-type="doi">10.1016/j.ijpe.2019.107541</pub-id></element-citation></ref><ref id="B2-sensors-25-01217"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Luo</surname><given-names>J.</given-names></name>
<name><surname>Lin</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>C.</given-names></name>
</person-group><article-title>A teleoperation framework for mobile robots based on shared control</article-title><source>IEEE Robot. Autom. Lett.</source><year>2020</year><volume>5</volume><fpage>377</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1109/LRA.2019.2959442</pub-id></element-citation></ref><ref id="B3-sensors-25-01217"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>J.</given-names></name>
<name><surname>Yoon</surname><given-names>H.</given-names></name>
<name><surname>Lee</surname><given-names>D.</given-names></name>
</person-group><article-title>A stable tele-operation of a mobile robot with the haptic feedback</article-title><source>IFAC-PapersOnLine</source><year>2018</year><volume>51</volume><fpage>13</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.ifacol.2018.11.511</pub-id></element-citation></ref><ref id="B4-sensors-25-01217"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Giudici</surname><given-names>G.</given-names></name>
<name><surname>Bonzini</surname><given-names>A.A.</given-names></name>
<name><surname>Coppola</surname><given-names>C.</given-names></name>
<name><surname>Althoefer</surname><given-names>K.</given-names></name>
<name><surname>Farkhatdinov</surname><given-names>I.</given-names></name>
<name><surname>Jamone</surname><given-names>L.</given-names></name>
</person-group><article-title>Leveraging tactile sensing to render both haptic feedback and virtual reality 3D object reconstruction in robotic telemanipulation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2412.02644</pub-id></element-citation></ref><ref id="B5-sensors-25-01217"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Su</surname><given-names>Y.-P.</given-names></name>
<name><surname>Chen</surname><given-names>X.-Q.</given-names></name>
<name><surname>Zhou</surname><given-names>C.</given-names></name>
<name><surname>Pearson</surname><given-names>L.H.</given-names></name>
<name><surname>Pretty</surname><given-names>C.G.</given-names></name>
<name><surname>Chase</surname><given-names>J.G.</given-names></name>
</person-group><article-title>Integrating virtual, mixed, and augmented reality into remote robotic applications: A brief review of extended reality-enhanced robotic systems for intuitive telemanipulation and telemanufacturing tasks in hazardous conditions</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>12129</elocation-id><pub-id pub-id-type="doi">10.3390/app132212129</pub-id></element-citation></ref><ref id="B6-sensors-25-01217"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>T.-C.</given-names></name>
<name><surname>Krishnan</surname><given-names>A.U.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
</person-group><article-title>Comparison of haptic and augmented reality visual cues for assisting telemanipulation</article-title><source>Proceedings of the 2022 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>23&#x02013;27 May 2022</conf-date><fpage>9309</fpage><lpage>9316</lpage></element-citation></ref><ref id="B7-sensors-25-01217"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cao</surname><given-names>H.</given-names></name>
<name><surname>Fang</surname><given-names>H.S.</given-names></name>
<name><surname>Liu</surname><given-names>W.</given-names></name>
<name><surname>Lu</surname><given-names>C.</given-names></name>
</person-group><article-title>Suctionnet-1billion: A large-scale benchmark for suction grasping</article-title><source>IEEE Robot. Autom. Lett.</source><year>2021</year><volume>6</volume><fpage>8718</fpage><lpage>8725</lpage><pub-id pub-id-type="doi">10.1109/LRA.2021.3115406</pub-id></element-citation></ref><ref id="B8-sensors-25-01217"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yoo</surname><given-names>Y.</given-names></name>
<name><surname>Eom</surname><given-names>J.</given-names></name>
<name><surname>Park</surname><given-names>M.</given-names></name>
<name><surname>Cho</surname><given-names>K.-J.</given-names></name>
</person-group><article-title>Compliant suction gripper with seamless deployment and retraction for robust picking against depth and tilt errors</article-title><source>IEEE Robot. Autom. Lett.</source><year>2023</year><volume>8</volume><fpage>1311</fpage><lpage>1318</lpage><pub-id pub-id-type="doi">10.1109/LRA.2023.3238903</pub-id></element-citation></ref><ref id="B9-sensors-25-01217"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Suzuki</surname><given-names>R.</given-names></name>
<name><surname>Karim</surname><given-names>A.</given-names></name>
<name><surname>Xia</surname><given-names>T.</given-names></name>
<name><surname>Hedayati</surname><given-names>H.</given-names></name>
<name><surname>Marquardt</surname><given-names>N.</given-names></name>
</person-group><article-title>Augmented reality and robotics: A survey and taxonomy for AR-enhanced human-robot interaction and robotic interfaces</article-title><source>Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>29 April&#x02013;5 May 2022</conf-date><publisher-name>ACM</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2022</year><comment>Article No. 553</comment><fpage>1</fpage><lpage>33</lpage></element-citation></ref><ref id="B10-sensors-25-01217"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Naceri</surname><given-names>A.</given-names></name>
<name><surname>Mazzanti</surname><given-names>D.</given-names></name>
<name><surname>Bimbo</surname><given-names>J.</given-names></name>
<name><surname>Prattichizzo</surname><given-names>D.</given-names></name>
<name><surname>Caldwell</surname><given-names>D.G.</given-names></name>
<name><surname>Mattos</surname><given-names>L.S.</given-names></name>
<name><surname>Deshpande</surname><given-names>N.</given-names></name>
</person-group><article-title>Towards a virtual reality interface for remote robotic teleoperation</article-title><source>Proceedings of the 2019 19th International Conference on Advanced Robotics (ICAR)</source><conf-loc>Belo Horizonte, Brazil</conf-loc><conf-date>2&#x02013;6 December 2019</conf-date><fpage>284</fpage><lpage>289</lpage></element-citation></ref><ref id="B11-sensors-25-01217"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>G.</given-names></name>
<name><surname>Li</surname><given-names>Q.</given-names></name>
<name><surname>Yang</surname><given-names>C.</given-names></name>
<name><surname>Su</surname><given-names>Y.</given-names></name>
<name><surname>Yuan</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>X.</given-names></name>
</person-group><article-title>The classification and new trends of shared control strategies in telerobotic systems: A survey</article-title><source>IEEE Trans. Haptics</source><year>2023</year><volume>16</volume><fpage>118</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1109/TOH.2023.3253856</pub-id><pub-id pub-id-type="pmid">37028362</pub-id>
</element-citation></ref><ref id="B12-sensors-25-01217"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rubagotti</surname><given-names>M.</given-names></name>
<name><surname>Taunyazov</surname><given-names>T.</given-names></name>
<name><surname>Omarali</surname><given-names>B.</given-names></name>
<name><surname>Shintemirov</surname><given-names>A.</given-names></name>
</person-group><article-title>Semi-autonomous robot teleoperation with obstacle avoidance via model predictive control</article-title><source>IEEE Robot. Autom. Lett.</source><year>2019</year><volume>4</volume><fpage>2746</fpage><lpage>2753</lpage><pub-id pub-id-type="doi">10.1109/LRA.2019.2917707</pub-id></element-citation></ref><ref id="B13-sensors-25-01217"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Castro</surname><given-names>M.N.</given-names></name>
<name><surname>Dosen</surname><given-names>S.</given-names></name>
</person-group><article-title>Continuous semi-autonomous prosthesis control using a depth sensor on the hand</article-title><source>Front. Neurorobot.</source><year>2022</year><volume>16</volume><elocation-id>814973</elocation-id><pub-id pub-id-type="doi">10.3389/fnbot.2022.814973</pub-id><pub-id pub-id-type="pmid">35401136</pub-id>
</element-citation></ref><ref id="B14-sensors-25-01217"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hauser</surname><given-names>K.</given-names></name>
</person-group><article-title>Recognition, prediction, and planning for assisted teleoperation of freeform tasks</article-title><source>Auton. Robots</source><year>2013</year><volume>35</volume><fpage>241</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1007/s10514-013-9350-3</pub-id></element-citation></ref><ref id="B15-sensors-25-01217"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Menon</surname><given-names>A.</given-names></name>
<name><surname>Olascoaga</surname><given-names>L.I.G.</given-names></name>
<name><surname>Balanaga</surname><given-names>V.</given-names></name>
<name><surname>Natarajan</surname><given-names>A.</given-names></name>
<name><surname>Ruffing</surname><given-names>J.</given-names></name>
<name><surname>Ardalan</surname><given-names>R.</given-names></name>
<name><surname>Rabaey</surname><given-names>J.M.</given-names></name>
</person-group><article-title>Shared control of assistive robots through user-intent prediction and hyperdimensional recall of reactive behavior</article-title><source>Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>London, UK</conf-loc><conf-date>29 May&#x02013;2 June 2023</conf-date><fpage>12638</fpage><lpage>12644</lpage></element-citation></ref><ref id="B16-sensors-25-01217"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>Q.</given-names></name>
<name><surname>Lam</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
</person-group><article-title>Intent inference in shared-control teleoperation system in consideration of user behavior</article-title><source>Complex Intell. Syst.</source><year>2022</year><volume>8</volume><fpage>2971</fpage><lpage>2981</lpage><pub-id pub-id-type="doi">10.1007/s40747-021-00533-4</pub-id></element-citation></ref><ref id="B17-sensors-25-01217"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jain</surname><given-names>S.</given-names></name>
<name><surname>Argall</surname><given-names>B.</given-names></name>
</person-group><article-title>Recursive Bayesian human intent recognition in shared-control robotics</article-title><source>Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Madrid, Spain</conf-loc><conf-date>1&#x02013;5 October 2018</conf-date><fpage>3905</fpage><lpage>3912</lpage></element-citation></ref><ref id="B18-sensors-25-01217"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Labb&#x000e9;</surname><given-names>M.</given-names></name>
<name><surname>Michaud</surname><given-names>F.</given-names></name>
</person-group><article-title>RTAB-Map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation</article-title><source>J. Field Robot.</source><year>2019</year><volume>36</volume><fpage>416</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1002/rob.21831</pub-id></element-citation></ref><ref id="B19-sensors-25-01217"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ravichandar</surname><given-names>H.C.</given-names></name>
<name><surname>Kumar</surname><given-names>A.</given-names></name>
<name><surname>Dani</surname><given-names>A.</given-names></name>
</person-group><article-title>Bayesian human intention inference through multiple model filtering with gaze-based priors</article-title><source>Proceedings of the 2016 19th International Conference on Information Fusion (FUSION)</source><conf-loc>Heidelberg, Germany</conf-loc><conf-date>5&#x02013;8 July 2016</conf-date><fpage>2296</fpage><lpage>2302</lpage></element-citation></ref><ref id="B20-sensors-25-01217"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Panagopoulos</surname><given-names>D.</given-names></name>
<name><surname>Petousakis</surname><given-names>G.</given-names></name>
<name><surname>Stolkin</surname><given-names>R.</given-names></name>
<name><surname>Nikolaou</surname><given-names>G.</given-names></name>
<name><surname>Chiou</surname><given-names>M.</given-names></name>
</person-group><article-title>A Bayesian-based approach to human operator intent recognition in remote mobile robot navigation</article-title><source>Proceedings of the 2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</source><conf-loc>Melbourne, Australia</conf-loc><conf-date>17&#x02013;20 October 2021</conf-date><fpage>125</fpage><lpage>131</lpage></element-citation></ref><ref id="B21-sensors-25-01217"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bai</surname><given-names>T.</given-names></name>
<name><surname>Fan</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Zheng</surname><given-names>R.</given-names></name>
</person-group><article-title>Multiple waypoints path planning for a home mobile robot</article-title><source>Proceedings of the 2018 Ninth International Conference on Intelligent Control and Information Processing (ICICIP)</source><conf-loc>Wanzhou, China</conf-loc><conf-date>9&#x02013;11 November 2018</conf-date><fpage>53</fpage><lpage>58</lpage></element-citation></ref><ref id="B22-sensors-25-01217"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sucan</surname><given-names>I.A.</given-names></name>
<name><surname>Moll</surname><given-names>M.</given-names></name>
<name><surname>Kavraki</surname><given-names>L.E.</given-names></name>
</person-group><article-title>The Open Motion Planning Library</article-title><source>IEEE Robot. Autom. Mag.</source><year>2012</year><volume>19</volume><fpage>72</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1109/MRA.2012.2205651</pub-id></element-citation></ref><ref id="B23-sensors-25-01217"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>M.</given-names></name>
<name><surname>Yu</surname><given-names>L.</given-names></name>
<name><surname>Dong</surname><given-names>D.</given-names></name>
</person-group><article-title>The design of multi-mode control strategy and switching method for unmanned electric locomotive</article-title><source>Proceedings of the 2018 IEEE 3rd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)</source><conf-loc>Chongqing, China</conf-loc><conf-date>12&#x02013;14 October 2018</conf-date><fpage>882</fpage><lpage>886</lpage></element-citation></ref><ref id="B24-sensors-25-01217"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Olson</surname><given-names>E.</given-names></name>
</person-group><article-title>AprilTag: A robust and flexible visual fiducial system</article-title><source>Proceedings of the 2011 IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Shanghai, China</conf-loc><conf-date>9&#x02013;13 May 2011</conf-date><fpage>3400</fpage><lpage>3407</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01217-f001"><label>Figure 1</label><caption><p>Relative pose between the suction cup and the surface of the target object. (<bold>a</bold>) Successful task execution enabled by a precise relative pose. (<bold>b</bold>) Task failure caused by a misaligned relative pose.</p></caption><graphic xlink:href="sensors-25-01217-g001" position="float"/></fig><fig position="float" id="sensors-25-01217-f002"><label>Figure 2</label><caption><p>The framework of the proposed semi-autonomous control method. (<bold>Left</bold>) Target object estimation via intersection points and Bayesian probability filter. (<bold>Right</bold>) Determination of control input based on Bayesian posterior probabilities.</p></caption><graphic xlink:href="sensors-25-01217-g002" position="float"/></fig><fig position="float" id="sensors-25-01217-f003"><label>Figure 3</label><caption><p>The proposed intersection-based operator intent estimation method for box-stacking storage environments. The green curve represents the trajectory of the end-effector throughout the task; the red arrows indicate the heading vector of the end-effector at specific time steps; the purple dots represent the accumulated intersection points from the start of the task to a given time step; the yellow ellipse is modeled as a Gaussian distribution based on the purple points.</p></caption><graphic xlink:href="sensors-25-01217-g003" position="float"/></fig><fig position="float" id="sensors-25-01217-f004"><label>Figure 4</label><caption><p>Real-time probability estimation for each target object using the Bayesian probability filter. (<bold>a</bold>) Without the filter, instantaneous probability values are directly reflected. (<bold>b</bold>) With the filter, instantaneous probability values are incorporated into posterior probabilities, resulting in stable and robust probability estimates.</p></caption><graphic xlink:href="sensors-25-01217-g004" position="float"/></fig><fig position="float" id="sensors-25-01217-f005"><label>Figure 5</label><caption><p>The overall flow of the proposed intersection-based operator intent estimation to obtain the probability of each target object: [LEVEL1] Gaussian distribution modeling; [LEVEL2] normalize the Gaussian distribution; [LEVEL3] application of Bayesian probability filter to independent probabilities.</p></caption><graphic xlink:href="sensors-25-01217-g005" position="float"/></fig><fig position="float" id="sensors-25-01217-f006"><label>Figure 6</label><caption><p>The flow of manual control input: converting gyro sensor values obtained from the HMD controller into appropriate manual control commands and communicating with the ROS server via TCP/IP.</p></caption><graphic xlink:href="sensors-25-01217-g006" position="float"/></fig><fig position="float" id="sensors-25-01217-f007"><label>Figure 7</label><caption><p>The flow of autonomous control input: generating automated control commands in a closed-loop controller using trajectory data planned by the OMPL planner and state information acquired from the manipulator.</p></caption><graphic xlink:href="sensors-25-01217-g007" position="float"/></fig><fig position="float" id="sensors-25-01217-f008"><label>Figure 8</label><caption><p>Experimental environment simulating a box-stacking storage logistics warehouse. Right: The actual experimental environment with April-Tag-attached boxes arranged in a 3 &#x000d7; 3 configuration. Target poses are generated based on the tags, and control is transferred according to the defined trigger conditions. Left: Target boxes projected onto the target logistics plane along with the Gaussian distribution. (Note: the <italic toggle="yes">X</italic>-axis is inverted due to the ROS coordinate system). Final posterior probabilities are calculated through Bayesian probability filtering based on this information.</p></caption><graphic xlink:href="sensors-25-01217-g008" position="float"/></fig><fig position="float" id="sensors-25-01217-f009"><label>Figure 9</label><caption><p>Success rate of proposed intersection-based operator intent estimation.</p></caption><graphic xlink:href="sensors-25-01217-g009" position="float"/></fig><fig position="float" id="sensors-25-01217-f010"><label>Figure 10</label><caption><p>Trend of score values based on predefined parameters. (<bold>a</bold>) Trend of score values with respect to the number of intersection points, showing valid scores recorded only from n &#x02265; 9. (<bold>b</bold>) Trend of score values based on the highest Bayesian probability threshold (blue line) and the difference between the highest and second-highest Bayesian probabilities (orange line), with lower values for both resulting in better scores.</p></caption><graphic xlink:href="sensors-25-01217-g010" position="float"/></fig><fig position="float" id="sensors-25-01217-f011"><label>Figure 11</label><caption><p>Bayesian probability values (solid lines) for all boxes over time and the decision time (blue solid line) satisfying all conditions when parameters are set to 0.1, 0.1, and 9. (<bold>a</bold>) Case of identifying the target object in the shortest time (8.2% of the total task). (<bold>b</bold>) Case of identifying the target object in the longest time (77.5% of the total task).</p></caption><graphic xlink:href="sensors-25-01217-g011" position="float"/></fig><table-wrap position="float" id="sensors-25-01217-t001"><object-id pub-id-type="pii">sensors-25-01217-t001_Table 1</object-id><label>Table 1</label><caption><p>Hardware specifications used in the study and the experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Device</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Hardware Information</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" colspan="1">UR5e</td><td align="center" valign="middle" rowspan="1" colspan="1">Working Radius</td><td align="center" valign="middle" rowspan="1" colspan="1">850 mm</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Payload</td><td align="center" valign="middle" rowspan="1" colspan="1">5 kg</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Repeatability</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.03 mm</td></tr><tr><td rowspan="4" align="center" valign="middle" colspan="1">D405</td><td align="center" valign="middle" rowspan="1" colspan="1">Maximum Depth Range</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5 m</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Depth Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.1 mm</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Resolution (RGB)</td><td align="center" valign="middle" rowspan="1" colspan="1">1280 &#x000d7; 800</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Frame Rate</td><td align="center" valign="middle" rowspan="1" colspan="1">90 FPS</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Meta Quest2</td><td align="center" valign="middle" rowspan="1" colspan="1">Resolution</td><td align="center" valign="middle" rowspan="1" colspan="1">1832 &#x000d7; 1920 (each eye)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Refresh Rate</td><td align="center" valign="middle" rowspan="1" colspan="1">72&#x02013;90 Hz</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tracking DOF <sup>1</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6 DOF</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> Degree of freedom.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-01217-t002"><object-id pub-id-type="pii">sensors-25-01217-t002_Table 2</object-id><label>Table 2</label><caption><p>Network performance metrics during the experiment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Re-Transmission Packet</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Round Trip Time</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Data Transfer Speed</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1 packets/sec</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5 ms</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">21.6 kbytes/s</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01217-t003"><object-id pub-id-type="pii">sensors-25-01217-t003_Table 3</object-id><label>Table 3</label><caption><p>Average bias and standard deviation between the centroid of the Gaussian distribution and the target object. The target box ID is defined as the unique ID of the April Tag attached to the target box. In addition, the target box IDs are arranged based on the actual placement order of the objects, sorted from left to right and from top to bottom.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Target Box ID</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Average Bias (mm)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Standard Deviation</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">121</td><td align="center" valign="middle" rowspan="1" colspan="1">100.112</td><td align="center" valign="middle" rowspan="1" colspan="1">37.987</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">125</td><td align="center" valign="middle" rowspan="1" colspan="1">68.55</td><td align="center" valign="middle" rowspan="1" colspan="1">11.072</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">122</td><td align="center" valign="middle" rowspan="1" colspan="1">147.631</td><td align="center" valign="middle" rowspan="1" colspan="1">29.434</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">16</td><td align="center" valign="middle" rowspan="1" colspan="1">44.327</td><td align="center" valign="middle" rowspan="1" colspan="1">11.611</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">126</td><td align="center" valign="middle" rowspan="1" colspan="1">21.236</td><td align="center" valign="middle" rowspan="1" colspan="1">4.622</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">124</td><td align="center" valign="middle" rowspan="1" colspan="1">69.774</td><td align="center" valign="middle" rowspan="1" colspan="1">25.047</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">56.893</td><td align="center" valign="middle" rowspan="1" colspan="1">19.343</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">17</td><td align="center" valign="middle" rowspan="1" colspan="1">16.777</td><td align="center" valign="middle" rowspan="1" colspan="1">11.491</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">85.458</td><td align="center" valign="middle" rowspan="1" colspan="1">23.309</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">Total</italic>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.826</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.093</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01217-t004"><object-id pub-id-type="pii">sensors-25-01217-t004_Table 4</object-id><label>Table 4</label><caption><p>The mean, standard deviation, minimum, and maximum of the recognition time ratio for each target box. The target box ID is defined as the unique ID of the April Tag attached to the target box, and the recognition time ratio is defined as the time of successful recognition, normalized to 1.0 using the total task duration. The arrangement of target box IDs is the same as in <xref rid="sensors-25-01217-t003" ref-type="table">Table 3</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Target Box ID</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ARTR <sup>1</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">STD <sup>2</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Min ARTR <sup>3</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Max ARTR <sup>4</sup></th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">121</td><td align="center" valign="middle" rowspan="1" colspan="1">0.364</td><td align="center" valign="middle" rowspan="1" colspan="1">0.262</td><td align="center" valign="middle" rowspan="1" colspan="1">0.2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.667</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">125</td><td align="center" valign="middle" rowspan="1" colspan="1">0.612</td><td align="center" valign="middle" rowspan="1" colspan="1">0.093</td><td align="center" valign="middle" rowspan="1" colspan="1">0.512</td><td align="center" valign="middle" rowspan="1" colspan="1">0.697</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">122</td><td align="center" valign="middle" rowspan="1" colspan="1">0.494</td><td align="center" valign="middle" rowspan="1" colspan="1">0.351</td><td align="center" valign="middle" rowspan="1" colspan="1">0.089</td><td align="center" valign="middle" rowspan="1" colspan="1">0.705</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.372</td><td align="center" valign="middle" rowspan="1" colspan="1">0.217</td><td align="center" valign="middle" rowspan="1" colspan="1">0.148</td><td align="center" valign="middle" rowspan="1" colspan="1">0.582</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">126</td><td align="center" valign="middle" rowspan="1" colspan="1">0.113</td><td align="center" valign="middle" rowspan="1" colspan="1">0.006</td><td align="center" valign="middle" rowspan="1" colspan="1">0.107</td><td align="center" valign="middle" rowspan="1" colspan="1">0.119</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">124</td><td align="center" valign="middle" rowspan="1" colspan="1">0.132s</td><td align="center" valign="middle" rowspan="1" colspan="1">0.049</td><td align="center" valign="middle" rowspan="1" colspan="1">0.094</td><td align="center" valign="middle" rowspan="1" colspan="1">0.188</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.482</td><td align="center" valign="middle" rowspan="1" colspan="1">0.309</td><td align="center" valign="middle" rowspan="1" colspan="1">0.127</td><td align="center" valign="middle" rowspan="1" colspan="1">0.692</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">17</td><td align="center" valign="middle" rowspan="1" colspan="1">0.212</td><td align="center" valign="middle" rowspan="1" colspan="1">0.074</td><td align="center" valign="middle" rowspan="1" colspan="1">0.134</td><td align="center" valign="middle" rowspan="1" colspan="1">0.282</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">0.322</td><td align="center" valign="middle" rowspan="1" colspan="1">0.392</td><td align="center" valign="middle" rowspan="1" colspan="1">0.093</td><td align="center" valign="middle" rowspan="1" colspan="1">0.775</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Total</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.345</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.256</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.089</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.775</td></tr></tbody></table><table-wrap-foot><fn><p><sup>1</sup> Average recognition time ratio. <sup>2</sup> Standard deviation. <sup>3</sup> Minimum average recognition time ratio. <sup>4</sup> Maximum average recognition time ratio.</p></fn></table-wrap-foot></table-wrap></floats-group></article>