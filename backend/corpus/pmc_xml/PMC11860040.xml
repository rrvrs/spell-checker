<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006409</article-id><article-id pub-id-type="pmc">PMC11860040</article-id><article-id pub-id-type="doi">10.3390/s25041181</article-id><article-id pub-id-type="publisher-id">sensors-25-01181</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multi-Mode Hand Gesture-Based VR Locomotion Technique for Intuitive Telemanipulation Viewpoint Control in Tightly Arranged Logistic Environments</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jeong</surname><given-names>Jaehoon</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><name><surname>Choi</surname><given-names>Haegyeom</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1814-2683</contrib-id><name><surname>Lee</surname><given-names>Donghun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-01181" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Allison</surname><given-names>Robert S.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01181">Mechanical Engineering Department, Soongsil University, Seoul 06978, Republic of Korea; <email>wogns1218@soongsil.ac.kr</email> (J.J.); <email>choihg@soongsil.ac.kr</email> (H.C.)</aff><author-notes><corresp id="c1-sensors-25-01181"><label>*</label>Correspondence: <email>dhlee04@gmail.com</email></corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1181</elocation-id><history><date date-type="received"><day>03</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>06</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>13</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Telemanipulation-based object-side picking with a suction gripper often faces challenges such as occlusion of the target object or the gripper and the need for precise alignment between the suction cup and the object&#x02019;s surface. These issues can significantly affect task success rates in logistics environments. To address these problems, this study proposes a multi-mode hand gesture-based virtual reality (VR) locomotion method to enable intuitive and precise viewpoint control. The system utilizes a head-mounted display (HMD) camera to capture hand skeleton data, which a multi-layer perceptron (MLP) model processes. The model classifies gestures into three modes: translation, rotation, and fixed, corresponding to fist, pointing, and unknown gestures, respectively. Translation mode moves the viewpoint based on the wrist&#x02019;s displacement, rotation mode adjusts the viewpoint&#x02019;s angle based on the wrist&#x02019;s angular displacement, and fixed mode stabilizes the viewpoint when gestures are ambiguous. A dataset of 4312 frames was used for training and validation, with 666 frames for testing. The MLP model achieved a classification accuracy of 98.4%, with precision, recall, and F1-score exceeding 0.98. These results demonstrate the system&#x02019;s ability to address the challenges of telemanipulation tasks by enabling accurate gesture recognition and seamless mode transitions.</p></abstract><kwd-group><kwd>telemanipulation</kwd><kwd>VR locomotion</kwd><kwd>logistics robots</kwd><kwd>hand gesture recognition</kwd><kwd>multi-layer perceptron</kwd></kwd-group><funding-group><award-group><funding-source>National Research Foundation of Korea (NRF) funded by the Ministry of Education</funding-source><award-id>RS-2022-NR073933</award-id></award-group><award-group><funding-source>Institute of Information &#x00026; Communications Technology Planning &#x00026; Evaluation (IITP) grant funded by the Korean government (MSIT)</funding-source><award-id>2022-0-00218</award-id></award-group><funding-statement>This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (RS-2022-NR073933); the Institute of Information &#x00026; Communications Technology Planning &#x00026; Evaluation (IITP) grant funded by the Korean government (MSIT) (2022-0-00218, Development of XR twin-based training content technology for rehabilitation).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01181"><title>1. Introduction</title><p>With the advancements in deep learning and robotics technologies, the seamless integration of production and logistics processes has become a critical requirement in intelligent factories and manufacturing automation. In particular, achieving a smooth connection between production lines and warehouses is essential for implementing Just-in-Time (JIT) production systems and highly efficient material supply systems. However, it is challenging to perform automated pick-and-place tasks in complex working environments with densely arranged objects or rapidly changing object placements. Numerous studies have addressed these issues [<xref rid="B1-sensors-25-01181" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01181" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01181" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01181" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01181" ref-type="bibr">5</xref>]. Among them, systems combining remote control and VR technologies, such as the method proposed by Galarza et al. (2023) [<xref rid="B3-sensors-25-01181" ref-type="bibr">3</xref>], have garnered significant attention. Nevertheless, these systems face challenges when the target object or gripper is occluded, making task execution difficult. This issue becomes even more pronounced in telemanipulation-based object-side picking with a suction gripper, as illustrated in <xref rid="sensors-25-01181-f001" ref-type="fig">Figure 1</xref>. As shown in <xref rid="sensors-25-01181-f001" ref-type="fig">Figure 1</xref>c, the success of pick-and-place tasks involving a mobile manipulator and a suction gripper critically depends on the precise positional relationship between the suction cup and the contact surface of the object. To enable precise control of the mobile manipulator&#x02019;s end effector, operators must have a clear and stable viewpoint that allows for accurate observation of the target object. Unlike conventional VR locomotion techniques designed for navigating large virtual environments, the primary challenge in this study lies in managing viewpoint transitions within a constrained workspace to facilitate precise telemanipulation tasks. Therefore, an effective viewpoint control mechanism is necessary to ensure seamless and intuitive adjustments without relying on additional physical movement or external tracking devices.</p><p>Boletsis (2017) [<xref rid="B6-sensors-25-01181" ref-type="bibr">6</xref>] classified VR locomotion techniques into four categories&#x02014;roomscale-based, controller-based, teleportation-based, and motion-based&#x02014;based on interaction type (physical or artificial), VR motion type (continuous or non-continuous), and VR interaction type (open or limited). The three methods, excluding the Motion-based approach, exhibit limitations in telemanipulating mobile manipulators within logistics environments.</p><list list-type="bullet"><list-item><p>Roomscale-based method: The roomscale-based method, which reflects the user&#x02019;s physical movement in the virtual environment, offers high immersion and intuitive interaction. Bozgeyikli et al. (2019) [<xref rid="B7-sensors-25-01181" ref-type="bibr">7</xref>] implemented this approach using tracking technology. Unlike other locomotion techniques, this method minimizes cybersickness because the user&#x02019;s real-world movement directly corresponds to the visual motion in the virtual environment, preventing sensory conflict between the visual and vestibular systems. As a result, users experience a more natural sense of presence with reduced discomfort. However, this method is constrained by the physical size of the real-world environment, limiting user movement to within the boundaries of the actual space. For instance, in confined spaces or environments with many obstacles, user movement is restricted, making it challenging to secure visibility for the telemanipulation of a mobile manipulator. Consequently, this method is unsuitable for logistics environments requiring expansive workspaces.</p></list-item><list-item><p>Teleportation-based method: The teleportation-based method [<xref rid="B8-sensors-25-01181" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01181" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01181" ref-type="bibr">10</xref>] allows users to move to a selected location instantly without being limited by the size of the physical space. Bozgeyikli et al. (2016) [<xref rid="B8-sensors-25-01181" ref-type="bibr">8</xref>] implemented this approach using Point and Teleport technology, enabling fast and intuitive position changes. However, this method lacks continuity in movement, and the sudden shift in perspective can cause users to lose spatial awareness. Furthermore, logistics environments often require tasks at varying heights and complex operations. Still, most Teleportation-based methods only support movement at fixed heights, making it challenging to maintain the necessary visibility. These limitations make it difficult to perform complex tasks effectively in logistics settings.</p></list-item><list-item><p>Controller-based method: The controller-based method uses a joystick or sensors to enable users to control their viewpoint continuously in VR without physical movement. Englmeier et al. (2020) [<xref rid="B11-sensors-25-01181" ref-type="bibr">11</xref>] proposed a technique utilizing a sphere-shaped device that allows users to control their viewpoint through rotation and tilting, demonstrating that precise and intuitive control is achievable in virtual environments. However, this approach requires additional devices and sensors, such as a joystick or Inertial Measurement Unit (IMU), beyond an HMD, leading to increased initial costs and additional expenses for maintenance.</p></list-item></list><p>The motion-based method [<xref rid="B12-sensors-25-01181" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01181" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01181" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01181" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-01181" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01181" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01181" ref-type="bibr">18</xref>] supports continuous movement within virtual environments and, in some cases, can be implemented using only an HMD. However, not all motion-based methods are free from physical space constraints. While techniques such as walk-in-place and gesture-based movement operate in confined spaces, others&#x02014;like Redirected Walking&#x02014;still require a minimum physical area to function properly. Additionally, motion-based methods vary in their hardware requirements. Some, such as gesture-based approaches, can be implemented using only an HMD&#x02019;s built-in tracking system. However, others, like arm swinging, typically require external controllers or sensors to ensure accurate tracking. Even with inside-out tracking, maintaining hand visibility within the HMD&#x02019;s field of view can be challenging, especially for full-arm motion techniques. While certain motion-based methods reduce hardware dependencies, their practical implementation depends on tracking capabilities, physical constraints, and the requirements of the task.</p><list list-type="bullet"><list-item><p>Walk-in-place method: The walk-in-place method detects the user&#x02019;s stepping motion in place to generate a velocity vector and translates it into movement within the virtual environment. Lee et al. (2018) [<xref rid="B12-sensors-25-01181" ref-type="bibr">12</xref>] proposed a method that detects stepping motions based on HMD posture data and connects them to virtual navigation. While this approach provides users with a natural movement experience, it requires continuous stepping, which can be physically exhausting. Additionally, when combined with the telemanipulation of a mobile manipulator, unintended hand movements may occur, reducing the precision of the telemanipulation.</p></list-item><list-item><p>Arm swinging method: The arm swinging method uses the user&#x02019;s arm movements as input to implement navigation. Pai and Kunze (2017) [<xref rid="B14-sensors-25-01181" ref-type="bibr">14</xref>] proposed a technique that detects arm motions to control movement within a virtual environment. However, this method requires continuous use of both arms, making it unsuitable for telemanipulation of a mobile manipulator, where precise hand control is needed. Additionally, the repeated arm swinging motion can be physically tiring, making it difficult for users to maintain over long periods.</p></list-item><list-item><p>Gesture-based method: The gesture-based method uses the user&#x02019;s body movements to enable navigation and interaction within virtual environments. This includes hand gestures and head, arm, and full-body motions. Users can easily adjust their perspective or navigate the virtual environment with simple actions, reducing physical strain compared to other methods.</p></list-item></list><p>The gesture-based method utilizes various body movements, with hand gestures being the most widely used primary input technique. As a result, hand gesture recognition (HGR) has emerged as a core technology for gesture-based methods. HGR refers to accurately identifying user hand movements and linking them to tasks such as navigation or viewpoint control in virtual environments. It can be divided into sensor-based and computer vision-based approaches. The sensor-based approach collects data using devices such as IMUs [<xref rid="B19-sensors-25-01181" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01181" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01181" ref-type="bibr">21</xref>], electromyography (EMG) sensors [<xref rid="B22-sensors-25-01181" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01181" ref-type="bibr">23</xref>], and flex sensors [<xref rid="B24-sensors-25-01181" ref-type="bibr">24</xref>]. While this method offers high precision, it requires additional hardware, such as microcontrollers, for data processing and transmission. On the other hand, the computer vision-based approach does not require sensors to be attached to the user&#x02019;s body, offering greater convenience. In VR, this approach leverages cameras mounted on the HMD, allowing for implementation without additional hardware. Oudah et al. (2020) [<xref rid="B25-sensors-25-01181" ref-type="bibr">25</xref>] categorized computer vision-based HGR technologies into seven types based on recognition methods. Among these, skeleton recognition [<xref rid="B26-sensors-25-01181" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01181" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-01181" ref-type="bibr">28</xref>] analyzes hand movements using skeletal structure, enhancing the detection of complex features. A notable example is MediaPipe, proposed by Lugaresi et al. (2019) [<xref rid="B28-sensors-25-01181" ref-type="bibr">28</xref>], enabling real-time hand skeleton data extraction using a single camera image.</p><p>To address these limitations, this study proposes a method that eliminates workspace constraints while using only an HMD for tracking. While previous studies, such as Kirihata and Ishikawa (2024) [<xref rid="B18-sensors-25-01181" ref-type="bibr">18</xref>], explored single-hand gesture-based locomotion, their approach was based on teleportation, which allows discrete movement transitions rather than continuous viewpoint control. In contrast, the proposed method enables continuous viewpoint manipulation using a single hand, allowing operators to adjust their position and orientation smoothly without predefined teleportation points. By extracting hand skeleton data through the HMD&#x02019;s built-in cameras and processing it with an MLP model, the system classifies gestures in real time and applies transformations in translation, rotation, and fixed modes. This approach allows for more precise viewpoint adjustments in telemanipulation tasks, where operators need fluid motion rather than stepwise relocation. The ability to control the viewpoint with one hand also ensures that the other hand remains available for object manipulation, making this method more suitable for logistics and industrial applications.</p></sec><sec id="sec2-sensors-25-01181"><title>2. Materials and Methods</title><p>This section explains the structure and operation of the VR locomotion technique designed to enable operators to intuitively control their viewpoint in a virtual environment for logistics tasks. <xref rid="sensors-25-01181-f002" ref-type="fig">Figure 2</xref> illustrates the overall structure of the proposed method, encompassing the processes of real-time hand skeleton data extraction and hand gesture classification and linking these to viewpoint translation and rotation within the virtual environment.</p><p>The operator visually navigates the virtual environment using an HMD and controls the viewpoint based on hand movements. The HMD&#x02019;s camera captures real-time images of the operator&#x02019;s hand and extracts 3D position and orientation data for 24 joints (skeleton data). These data are input into an MLP model, classifying the gestures into three categories: fist, pointing, and unknown. These gestures control viewpoint translation and rotation within the virtual environment. Each gesture is mapped to one of three actions: translation, rotation, or fixed, enabling the operator to manipulate the viewpoint without being constrained by physical space.</p><sec id="sec2dot1-sensors-25-01181"><title>2.1. VR User Interface</title><p>The user interface for the proposed VR locomotion technique is designed to connect the operator&#x02019;s physical movements with the virtual environment while providing environmental information about the robot. As shown in <xref rid="sensors-25-01181-f003" ref-type="fig">Figure 3</xref>, the interface consists of a virtual hand, which virtualizes the operator&#x02019;s real hand, and a 3D point cloud map generated using RTAB-Map [<xref rid="B29-sensors-25-01181" ref-type="bibr">29</xref>].</p><p>The virtual hand is generated based on hand skeleton data tracked by the HMD camera, reflecting the skeleton data in real time. The operator&#x02019;s hand movements are accurately represented in the virtual environment through the virtual hand, allowing the operator to understand the relationship between gestures and viewpoint control clearly.</p><p>The 3D point cloud map is a pre-generated dataset created using RTAB-Map. RTAB-Map utilizes sensors like RGB-D cameras to represent the workspace&#x02019;s structure and objects&#x02019; positions as three-dimensional data. This map provides detailed information about the workspace within the virtual environment, allowing the operator to understand their current position and surroundings accurately. Compared to 2D images, 3D data offer a clear representation of depth and spatial structure, making it easier to identify objects&#x02019; relative positions and sizes. Additionally, 3D data include comprehensive spatial information, enabling the operator to explore the workspace from different angles and reducing information loss during viewpoint transitions.</p></sec><sec id="sec2dot2-sensors-25-01181"><title>2.2. Hand Skeleton Tracking</title><p>To reflect the operator&#x02019;s hand movement data in the virtual environment in real time, this study utilizes the Oculus Integration SDK [<xref rid="B30-sensors-25-01181" ref-type="bibr">30</xref>] to extract hand skeleton data. It implements a virtual hand based on these data. The extracted skeleton data consist of 24 joints, including the wrist, with each joint represented by a 3D position (x, y, z) and rotation (quaternion: qx, qy, qz, qw).</p><p>The camera embedded in the HMD captures real-time images of the operator&#x02019;s hand, and the extracted data consist of the following joints:<list list-type="bullet"><list-item><p>Forearm: the forearm joint, which connects to the wrist.</p></list-item><list-item><p>Wrist: the wrist joint, connecting to the finger joints.</p></list-item><list-item><p>Thumb: it consists of five joints&#x02014;trapezium, metacarpal, proximal, distal, and tip.</p></list-item><list-item><p>Index, Middle, Ring: each finger comprises four joints&#x02014;proximal, intermediate, distal, and tip.</p></list-item><list-item><p>Pinky: it includes five joints&#x02014;metacarpal, proximal, intermediate, distal, and tip.</p></list-item></list></p><p>These data are used to map the operator&#x02019;s hand movements to the virtual hand within the virtual environment and serves as the core input for the hand gesture-based locomotion technique proposed in this study.</p></sec><sec id="sec2dot3-sensors-25-01181"><title>2.3. MLP-Based Hand Gesture Recognition</title><p>This study designed the MLP model based on the previously described hand skeleton data to classify the operator&#x02019;s hand gestures into three categories: fist, pointing, and unknown. This process establishes a foundation for intuitively controlling viewpoint translation and rotation within the virtual environment using hand gestures. Notably, the skeleton data extracted from the Oculus Quest 2 (Meta Platforms, Menlo Park, CA, USA), which include the joint data, are converted into a relative coordinate system centered on the wrist joint. This ensures data consistency, allowing gestures to be recognized uniformly regardless of the angle at which the operator raises their arm.</p><p>The dataset consists of 4312 frames, which were extracted from a continuous video where a person repeatedly performed random hand gestures. These frames were divided into a training set (80%) and a validation set (20%). The training set was used to optimize the model&#x02019;s weights, while the validation set was used to monitor overfitting and determine the optimal stopping point during training. Since the hand gesture recognition model classifies gestures on a frame-by-frame basis, each frame was manually labeled according to the gesture being performed at that moment. The segmentation process involved reviewing the continuous video and assigning a gesture label to every individual frame, ensuring that each frame accurately represented a single hand posture. To maintain consistency, no overlapping frames were included, and ambiguous transitions between gestures were carefully excluded. The dataset composition used in this study is summarized in <xref rid="sensors-25-01181-t001" ref-type="table">Table 1</xref>.</p><p>The MLP model consists of an input layer, two hidden layers, and an output layer. The input layer comprises 168 nodes, combining the skeleton data. The subsequent two hidden layers contain 128 and 64 nodes, respectively. The ReLU (Rectified Linear Unit) activation function is applied in the hidden layers, which helps introduce non-linearity by ensuring that only positive input values are passed forward. In contrast, negative values are replaced with zero. This enables the model to learn complex patterns and relationships within the data. Finally, the output layer uses a SoftMax function to calculate probability values for the three gesture categories: fist, pointing, and unknown.</p><p>The loss function used for training is categorical cross-entropy, widely utilized for multi-class classification problems. The Adam optimizer was chosen as the optimization algorithm. The batch size was set to 32 to enhance training efficiency, and the model was trained over 100 epochs. During training, the loss and accuracy of the training and validation sets were monitored at each epoch. To prevent overfitting, the early stopping technique was applied, terminating the training process early if no further improvement in performance was observed on the validation set.</p><p>After training, the model achieved approximately 99.5% accuracy on the training data and about 97.7% accuracy on the validation set. This demonstrates that the MLP model, utilizing skeleton data transformed into a relative coordinate system, classifies hand gestures with high accuracy. Subsequently, the three gestures are mapped to translation mode, rotation mode, and fixed mode within the virtual environment, serving as the input for the VR locomotion technique proposed in this study.</p></sec><sec id="sec2dot4-sensors-25-01181"><title>2.4. Multi-Mode Hand Gesture-Based VR Locomotion Method</title><p>Building on the MLP-based hand gesture recognition results, this section introduces a VR locomotion technique that integrates three modes&#x02014;translation, rotation, and fixed&#x02014;within the virtual environment. In this method, a hand gesture classified as fist activates the translation mode, enabling the operator to move the viewpoint by displacing the wrist. A gesture recognized as pointing enables the rotation mode, allowing the operator to rotate the viewpoint by adjusting the wrist&#x02019;s angular displacement. The pointing gesture was chosen for this mode because the direction of the extended index finger provides an intuitive reference for rotational movement, making it easier for operators to control the rotation axis based on natural wrist motion. Lastly, for gestures classified as unknown, the fixed mode is triggered, preventing unintended viewpoint movements and ensuring seamless transitions between modes based on the operator&#x02019;s hand movements.</p><p><xref rid="sensors-25-01181-f004" ref-type="fig">Figure 4</xref> illustrates the overall process of the translation mode. When a hand gesture is recognized as a fist, the system checks whether the wrist joint&#x02019;s initial state has been set. If the initial state is not yet defined, the current wrist position is recorded as the initial state. Subsequently, if the hand gesture continues to be recognized as a fist, the displacement <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mo mathvariant="normal">&#x00394;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> from the initial position <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mmultiscripts><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow></mml:mrow></mml:math></inline-formula> to the current position <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mmultiscripts><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated as shown in Equation (1). The current viewpoint position <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mmultiscripts><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow></mml:mrow></mml:math></inline-formula> is then updated to the position obtained by moving the initial viewpoint position <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mmultiscripts><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow></mml:math></inline-formula> by <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mo mathvariant="normal">&#x00394;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. This process is expressed in Equation (2).<disp-formula id="FD1-sensors-25-01181"><label>(1)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo mathvariant="normal">&#x00394;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mmultiscripts><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts><mml:mo>&#x02212;</mml:mo><mml:mmultiscripts><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-01181"><label>(2)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mmultiscripts><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts><mml:mo>=</mml:mo><mml:mrow><mml:mmultiscripts><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow><mml:mo>+</mml:mo><mml:mo mathvariant="normal">&#x00394;</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><xref rid="sensors-25-01181-f005" ref-type="fig">Figure 5</xref> illustrates the overall process of the rotation mode. When a hand gesture is recognized as pointing, the system first checks whether the initial orientation of the wrist joint has been set. If the initial orientation is not yet defined, the current wrist orientation (quaternion) is recorded as the initial orientation. Subsequently, if the hand gesture continues to be recognized as pointing, the angular displacement <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mo mathvariant="normal">&#x00394;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> from the initial orientation <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mmultiscripts><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow></mml:math></inline-formula> to the current orientation <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mmultiscripts><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow></mml:math></inline-formula> is calculated as shown in Equation (3). The initial viewpoint orientation <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mmultiscripts><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow></mml:math></inline-formula> is then rotated by <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mo mathvariant="normal">&#x00394;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> to update the current viewpoint orientation <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mmultiscripts><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow></mml:mrow></mml:math></inline-formula>. The formula for updating the current orientation is provided in Equation (4).<disp-formula id="FD3-sensors-25-01181"><label>(3)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo mathvariant="normal">&#x00394;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mmultiscripts><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mmultiscripts><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-01181"><label>(4)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mmultiscripts><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts><mml:mo>=</mml:mo><mml:mo mathvariant="normal">&#x00394;</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mmultiscripts><mml:mi>q</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:none/><mml:mprescripts/><mml:none/><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:mmultiscripts></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, when a hand gesture is classified as an unknown motion, the fixed mode is applied, keeping the current viewpoint stationary. This prevents unintended viewpoint movements caused by accidental hand gestures during operations and helps maintain a stable visual composition. The three modes can transition seamlessly between each other, allowing the operator to navigate, rotate, or fix their viewpoint within the virtual environment using only fist and pointing gestures.</p></sec></sec><sec sec-type="results" id="sec3-sensors-25-01181"><title>3. Results</title><sec id="sec3dot1-sensors-25-01181"><title>3.1. Performance of MLP-Based Hand Gesture Recognition</title><p>In this study, a test dataset consisting of 666 sequentially collected data points was used to evaluate the performance of hand gesture recognition. The dataset was obtained from a single participant who randomly repeated hand gestures in a controlled environment. The participant was not given any specific task but was instructed to perform gestures naturally to ensure variability in execution. As shown in <xref rid="sensors-25-01181-f006" ref-type="fig">Figure 6</xref>, a confusion matrix was used to compare the model&#x02019;s predictions with the actual values, visually representing correctly classified and misclassified cases for each class.</p><p>Subsequently, four performance metrics were measured: accuracy, precision, recall, and F1-score. Accuracy represents the proportion of test data the model correctly predicted, calculated as 0.984 in this study, meaning that approximately 98.4% of the 666 data points were correctly classified. Precision indicates the proportion of true positive cases among those predicted as positive, and a precision score of 0.985 signifies that 98.5% of the predicted hand gestures were actual gestures. Recall, which measures the proportion of true positive cases identified by the model out of all actual positive cases, was recorded as 0.984, confirming that the model successfully recognized most hand gestures without significant omission. Finally, the F1-score, which reflects the balance between precision and recall, was measured at 0.985, demonstrating consistently high performance across both metrics.</p><p>The confusion matrix analysis results and the four performance metrics demonstrate that this study&#x02019;s MLP-based model achieves high accuracy and reliable predictive capability in the hand gesture recognition task.</p></sec><sec id="sec3dot2-sensors-25-01181"><title>3.2. Target Object Observation with Multi-Mode Hand Gesture-Based VR Locomotion Method</title><p>In this section, a task was conducted to observe a target object using the hand gesture-based VR locomotion method with the three modes, as shown in <xref rid="sensors-25-01181-f007" ref-type="fig">Figure 7</xref>. Each gesture triggered a specific viewpoint movement mode, and the proposed method was evaluated for its proper functionality across the three modes. The experimental results showed that each gesture accurately facilitated viewpoint adjustments, enabling the operator to observe the target object effectively.</p><p>The fist gesture was set to activate the translation mode, which is designed to move the viewpoint based on the displacement of the wrist joint. After the fist gesture was input during the experiment, the viewpoint shifted according to the wrist&#x02019;s displacement. This allowed the operator to observe the object from closer or farther distances, with the viewpoint movement accurately responding to the displacement specified by the operator. This transformation process was implemented using Equations (1) and (2) from <xref rid="sec2dot4-sensors-25-01181" ref-type="sec">Section 2.4</xref>. Specifically, the system captured the initial position of the wrist joint upon detecting the fist gesture and then computed the displacement relative to the initial position. The resulting transformation matrix was applied to the viewpoint position, ensuring that the movement was precisely aligned with the operator&#x02019;s hand motion.</p><p>The pointing gesture corresponds to the rotation mode, which is designed to rotate the viewpoint based on changes in the angle of the wrist joint. After inputting the pointing gesture, the viewpoint rotated according to the angular displacement of the wrist joint. In the experiment, the operator could observe the target object from their desired angle, with the viewpoint rotation precisely matching the wrist&#x02019;s movement. This process was governed by Equations (3) and (4), where the initial wrist orientation was recorded upon gesture detection, and the angular displacement was calculated to adjust the viewpoint rotation. The system applied this transformation in real time, ensuring smooth and natural rotation control.</p><p>The unknown gesture was assigned to the fixed mode to maintain the current viewpoint without any changes. During the experiment, the system kept the viewpoint stationary when an unknown gesture was input. The fixed mode functioned as a standby state, holding the viewpoint steady while awaiting movement input from the other two modes.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-01181"><title>4. Discussion</title><p>This study proposed a multi-mode hand gesture-based VR locomotion method that enables viewpoint control without requiring external sensors or controllers beyond the HMD. The proposed system consists of translation, rotation, and fixed modes, where real-time gesture recognition is used to adjust the viewpoint accordingly. The ability to operate with a single hand allows the operator to manipulate objects with the other hand, which is particularly beneficial in telemanipulation environments. Experimental results demonstrated that the MLP-based gesture classification model achieved a high accuracy of 98.4%, ensuring stable transitions between movement modes.</p><p>The proposed approach differs from existing gesture-based locomotion techniques in two key aspects. First, unlike the teleportation method used in Kirihata and Ishikawa (2024) [<xref rid="B16-sensors-25-01181" ref-type="bibr">16</xref>], this study enables continuous viewpoint control. While teleportation offers rapid movement, it may cause spatial disorientation due to the abrupt relocation of the operator&#x02019;s virtual position. In contrast, this study allows for gradual viewpoint adjustments based on hand movement, providing a more intuitive control experience. Second, whereas Zhang et al. (2017) [<xref rid="B15-sensors-25-01181" ref-type="bibr">15</xref>] implemented a two-hand gesture-based viewpoint control system, the proposed method enables single-handed operation, allowing operators to adjust their viewpoint while using the other hand for object manipulation. This makes the system more suitable for teleoperation tasks where precise control of both viewpoint and end effector is required.</p><p>Despite these advantages, the system has certain limitations that require further investigation. Experimental observations revealed that cybersickness was minimal during short usage sessions or at low movement speeds but increased with prolonged usage or rapid viewpoint adjustments. This suggests that sensory conflict between visual feedback and the vestibular system can intensify if movement speed exceeds a certain threshold. In particular, repeated rapid rotational movements were found to increase the likelihood of cybersickness, which aligns with previous studies on VR locomotion. To address this issue, future research should explore methods to optimize movement speed thresholds or dynamically adjust movement sensitivity based on user motion patterns.</p><p>Additionally, this study was conducted in a controlled environment, and its performance in real-world telemanipulation or complex work environments has yet to be validated. In logistics operations, for example, various obstacles may be present, and further evaluation is needed to determine whether the proposed locomotion method effectively enhances operator visibility in such settings.</p><p>Future research will focus on enabling simultaneous control of translation and rotation with a single hand, as well as implementing motion speed constraints to mitigate cybersickness. Furthermore, additional experiments will be conducted to evaluate the impact of the proposed method on work efficiency and operator fatigue in real-world telemanipulation scenarios. These efforts will contribute to further validating the system&#x02019;s practical applicability and expanding its potential use in logistics and remote operation environments.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-01181"><title>5. Conclusions</title><p>This study introduced a single-hand gesture-based VR locomotion technique designed to simplify viewpoint control without requiring external devices beyond the HMD. The proposed system demonstrated its effectiveness through its ability to enable precise and continuous viewpoint adjustments using three distinct modes&#x02014;translation, rotation, and fixed. By leveraging an MLP-based gesture recognition model, the system achieved robust classification performance, ensuring reliable operation across varied telemanipulation tasks.</p><p>The primary contribution of this research lies in its ability to integrate gesture-based interaction into a unified locomotion system, offering single-hand control that frees the other hand for object manipulation. This distinguishes the method from existing techniques, which often rely on dual-hand input or discrete teleportation. Additionally, the system&#x02019;s hardware simplicity and seamless operation make it particularly suitable for logistics and other task-oriented virtual environments.</p><p>While the system demonstrated high accuracy and usability, findings indicated that extended use or rapid movement may increase the likelihood of cybersickness. These observations highlight the need for further refinement of motion sensitivity and system optimization to enhance long-term comfort.</p><p>Future research will focus on validating the system in complex real-world environments and exploring novel gesture designs to enhance control flexibility. Through these developments, the proposed approach has the potential to become a practical tool for improving operator efficiency and precision in virtual environments.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><app-group><app id="app1-sensors-25-01181"><title>Supplementary Materials</title><p>The following supporting information can be downloaded at <uri xlink:href="https://www.mdpi.com/article/10.3390/s25041181/s1">https://www.mdpi.com/article/10.3390/s25041181/s1</uri>, Video S1: Viewpoint adjustment using the proposed method for enhanced visibility of the target object.</p><supplementary-material id="sensors-25-01181-s001" position="float" content-type="local-data"><media xlink:href="sensors-25-01181-s001.zip"/></supplementary-material></app></app-group><notes><title>Author Contributions</title><p>Conceptualization, J.J.; methodology, J.J.; software, J.J. and H.C.; validation, J.J., H.C. and D.L.; formal analysis, J.J., H.C. and D.L.; investigation, D.L.; resources, D.L.; data curation, J.J. and H.C.; writing&#x02014;original draft preparation, J.J. and D.L.; writing&#x02014;review and editing, J.J. and D.L.; visualization, J.J., H.C. and D.L.; supervision, D.L.; project administration, D.L.; funding acquisition, D.L. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data used to support the findings of this study are available from the corresponding author upon request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">JIT</td><td align="left" valign="middle" rowspan="1" colspan="1">Just-in-Time</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">VR</td><td align="left" valign="middle" rowspan="1" colspan="1">Virtual reality</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IMU</td><td align="left" valign="middle" rowspan="1" colspan="1">Inertial Measurement Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HMD</td><td align="left" valign="middle" rowspan="1" colspan="1">Head-mounted display</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EMG</td><td align="left" valign="middle" rowspan="1" colspan="1">Electromyography</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MLP</td><td align="left" valign="middle" rowspan="1" colspan="1">Multi-layer perceptron</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ReLU</td><td align="left" valign="middle" rowspan="1" colspan="1">Rectified Linear Unit</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-01181"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Iriondo</surname><given-names>A.</given-names></name>
<name><surname>Lazkano</surname><given-names>E.</given-names></name>
<name><surname>Susperregi</surname><given-names>L.</given-names></name>
<name><surname>Urain</surname><given-names>J.</given-names></name>
<name><surname>Fernandez</surname><given-names>A.</given-names></name>
<name><surname>Molina</surname><given-names>J.</given-names></name>
</person-group><article-title>Pick and Place Operations in Logistics Using a Mobile Manipulator Controlled with Deep Reinforcement Learning</article-title><source>Appl. Sci.</source><year>2019</year><volume>9</volume><elocation-id>348</elocation-id><pub-id pub-id-type="doi">10.3390/app9020348</pub-id></element-citation></ref><ref id="B2-sensors-25-01181"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zeng</surname><given-names>A.</given-names></name>
<name><surname>Song</surname><given-names>S.</given-names></name>
<name><surname>Yu</surname><given-names>K.T.</given-names></name>
<name><surname>Donlon</surname><given-names>E.</given-names></name>
<name><surname>Hogan</surname><given-names>F.R.</given-names></name>
<name><surname>Bauza</surname><given-names>M.</given-names></name>
<name><surname>Ma</surname><given-names>D.</given-names></name>
<name><surname>Taylor</surname><given-names>O.</given-names></name>
<name><surname>Liu</surname><given-names>M.</given-names></name>
<name><surname>Romo</surname><given-names>E.</given-names></name>
<etal/>
</person-group><article-title>Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching</article-title><source>Int. J. Robot. Res.</source><year>2022</year><volume>41</volume><fpage>690</fpage><lpage>705</lpage><pub-id pub-id-type="doi">10.1177/0278364919868017</pub-id></element-citation></ref><ref id="B3-sensors-25-01181"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Galarza</surname><given-names>B.R.</given-names></name>
<name><surname>Ayala</surname><given-names>P.</given-names></name>
<name><surname>Manzano</surname><given-names>S.</given-names></name>
<name><surname>Garcia</surname><given-names>M.V.</given-names></name>
</person-group><article-title>Virtual reality teleoperation system for mobile robot manipulation</article-title><source>Robotics</source><year>2023</year><volume>12</volume><elocation-id>163</elocation-id><pub-id pub-id-type="doi">10.3390/robotics12060163</pub-id></element-citation></ref><ref id="B4-sensors-25-01181"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Krupke</surname><given-names>D.</given-names></name>
<name><surname>Einig</surname><given-names>L.</given-names></name>
<name><surname>Langbehn</surname><given-names>E.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Steinicke</surname><given-names>F.</given-names></name>
</person-group><article-title>Immersive remote grasping: Realtime gripper control by a heterogenous robot control system</article-title><source>Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology</source><conf-loc>Munich, Germany</conf-loc><conf-date>2&#x02013;4 November 2016</conf-date></element-citation></ref><ref id="B5-sensors-25-01181"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Krupke</surname><given-names>D.</given-names></name>
<name><surname>Steinicke</surname><given-names>F.</given-names></name>
<name><surname>Lubos</surname><given-names>P.</given-names></name>
<name><surname>Jonetzko</surname><given-names>Y.</given-names></name>
<name><surname>Gorner</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
</person-group><article-title>Comparison of multimodal heading and pointing gestures for co-located mixed reality human-robot interaction</article-title><source>Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Madrid, Spain</conf-loc><conf-date>1&#x02013;5 October 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2018</year></element-citation></ref><ref id="B6-sensors-25-01181"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Boletsis</surname><given-names>C.</given-names></name>
</person-group><article-title>The new era of virtual reality locomotion: A systematic literature review of techniques and a proposed typology</article-title><source>Multimodal Technol. Interact.</source><year>2017</year><volume>1</volume><elocation-id>24</elocation-id><pub-id pub-id-type="doi">10.3390/mti1040024</pub-id></element-citation></ref><ref id="B7-sensors-25-01181"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bozgeyikli</surname><given-names>E.</given-names></name>
<name><surname>Raij</surname><given-names>A.</given-names></name>
<name><surname>Katkoori</surname><given-names>S.</given-names></name>
<name><surname>Dubey</surname><given-names>R.</given-names></name>
</person-group><article-title>Locomotion in virtual reality for room scale tracked areas</article-title><source>Int. J. Hum. Comput. Stud.</source><year>2019</year><volume>122</volume><fpage>38</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.ijhcs.2018.08.002</pub-id></element-citation></ref><ref id="B8-sensors-25-01181"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bozgeyikli</surname><given-names>E.</given-names></name>
<name><surname>Raij</surname><given-names>A.</given-names></name>
<name><surname>Katkoori</surname><given-names>S.</given-names></name>
<name><surname>Dubey</surname><given-names>R.</given-names></name>
</person-group><article-title>Point &#x00026; teleport locomotion technique for virtual reality</article-title><source>Proceedings of the 2016 Annual Symposium on Computer-Human Interaction in Play</source><conf-loc>Austin, TX, USA</conf-loc><conf-date>16&#x02013;19 October 2016</conf-date><fpage>205</fpage><lpage>216</lpage></element-citation></ref><ref id="B9-sensors-25-01181"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Von Willich</surname><given-names>J.</given-names></name>
<name><surname>Schmitz</surname><given-names>M.</given-names></name>
<name><surname>M&#x000fc;ller</surname><given-names>F.</given-names></name>
<name><surname>Schmitt</surname><given-names>D.</given-names></name>
<name><surname>M&#x000fc;hlh&#x000e4;user</surname><given-names>M.</given-names></name>
</person-group><article-title>Podoportation: Foot-based locomotion in virtual reality</article-title><source>Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>25&#x02013;30 April 2020</conf-date><fpage>1</fpage><lpage>14</lpage></element-citation></ref><ref id="B10-sensors-25-01181"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sch&#x000e4;fer</surname><given-names>A.</given-names></name>
<name><surname>Reis</surname><given-names>G.</given-names></name>
<name><surname>Stricker</surname><given-names>D.</given-names></name>
</person-group><article-title>Controlling teleportation-based locomotion in virtual reality with hand gestures: A comparative evaluation of two-handed and one-handed techniques</article-title><source>Electronics</source><year>2021</year><volume>10</volume><elocation-id>715</elocation-id><pub-id pub-id-type="doi">10.3390/electronics10060715</pub-id></element-citation></ref><ref id="B11-sensors-25-01181"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Englmeier</surname><given-names>D.</given-names></name>
<name><surname>Fan</surname><given-names>F.</given-names></name>
<name><surname>Butz</surname><given-names>A.</given-names></name>
</person-group><article-title>Rock or roll&#x02013;locomotion techniques with a handheld spherical device in virtual reality</article-title><source>Proceedings of the 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</source><conf-loc>Virtual</conf-loc><conf-date>9&#x02013;13 November 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2020</year><fpage>618</fpage><lpage>626</lpage></element-citation></ref><ref id="B12-sensors-25-01181"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>J.</given-names></name>
<name><surname>Ahn</surname><given-names>S.C.</given-names></name>
<name><surname>Hwang</surname><given-names>J.-I.</given-names></name>
</person-group><article-title>A walking-in-place method for virtual reality using position and orientation tracking</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>2832</elocation-id><pub-id pub-id-type="doi">10.3390/s18092832</pub-id><pub-id pub-id-type="pmid">30150586</pub-id>
</element-citation></ref><ref id="B13-sensors-25-01181"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chan</surname><given-names>L.</given-names></name>
<name><surname>Mi</surname><given-names>T.-W.</given-names></name>
<name><surname>Hsueh</surname><given-names>Z.H.</given-names></name>
<name><surname>Huang</surname><given-names>Y.-C.</given-names></name>
<name><surname>Hsu</surname><given-names>M.Y.</given-names></name>
</person-group><article-title>Seated-WIP: Enabling walking-in-place locomotion for stationary chairs in confined spaces</article-title><source>Proceedings of the CHI Conference on Human Factors in Computing Systems</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>11&#x02013;16 May 2024</conf-date><fpage>1</fpage><lpage>13</lpage></element-citation></ref><ref id="B14-sensors-25-01181"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wilson</surname><given-names>P.T.</given-names></name>
<name><surname>Kalescky</surname><given-names>W.</given-names></name>
<name><surname>MacLaughlin</surname><given-names>A.</given-names></name>
<name><surname>Williams</surname><given-names>B.</given-names></name>
</person-group><article-title>VR locomotion: Walking&#x0003e; walking in place&#x0003e; arm swinging</article-title><source>Proceedings of the 15th ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in Industry-Volume 1</source><conf-loc>Zhuhai, China</conf-loc><conf-date>3&#x02013;4 December 2016</conf-date><fpage>243</fpage><lpage>249</lpage></element-citation></ref><ref id="B15-sensors-25-01181"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>R.</given-names></name>
<name><surname>Harris-Adamson</surname><given-names>C.</given-names></name>
<name><surname>Odell</surname><given-names>D.</given-names></name>
<name><surname>Rempel</surname><given-names>D.</given-names></name>
</person-group><article-title>Design of finger gestures for locomotion in virtual reality</article-title><source>Virtual Real. Intell. Hardw.</source><year>2019</year><volume>1</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.3724/SP.J.2096-5796.2018.0007</pub-id></element-citation></ref><ref id="B16-sensors-25-01181"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kronemberger</surname><given-names>V.</given-names></name>
<name><surname>Cherullo</surname><given-names>R.</given-names></name>
<name><surname>Azevedo</surname><given-names>T.</given-names></name>
<name><surname>Porcino</surname><given-names>T.</given-names></name>
<name><surname>Raposo</surname><given-names>A.</given-names></name>
</person-group><article-title>Hand gestures for continuous locomotion and snap turn for vr experiences</article-title><source>Proceedings of the 25th Symposium on Virtual and Augmented Reality</source><conf-loc>Rio Grande, Brazil</conf-loc><conf-date>6&#x02013;9 November 2023</conf-date><fpage>243</fpage><lpage>247</lpage></element-citation></ref><ref id="B17-sensors-25-01181"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>F.</given-names></name>
<name><surname>Chu</surname><given-names>S.</given-names></name>
<name><surname>Pan</surname><given-names>R.</given-names></name>
<name><surname>Ji</surname><given-names>N.</given-names></name>
<name><surname>Xi</surname><given-names>N.</given-names></name>
</person-group><article-title>Double hand-gesture interaction for walk-through in VR environment</article-title><source>Proceedings of the 2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)</source><conf-loc>Wuhan, China</conf-loc><conf-date>24&#x02013;26 May 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2017</year><fpage>539</fpage><lpage>544</lpage></element-citation></ref><ref id="B18-sensors-25-01181"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kirihata</surname><given-names>H.</given-names></name>
<name><surname>Ishikawa</surname><given-names>T.</given-names></name>
</person-group><article-title>Handwindowteleportation: Locomotion with hand gestures for virtual reality games</article-title><source>Proceedings of the VISIGRAPP (1): GRAPP, HUCAPP, IVAPP</source><conf-loc>Rome, Italy</conf-loc><conf-date>27&#x02013;29 February 2024</conf-date><fpage>170</fpage><lpage>176</lpage></element-citation></ref><ref id="B19-sensors-25-01181"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Makaussov</surname><given-names>O.</given-names></name>
<name><surname>Krassavin</surname><given-names>M.</given-names></name>
<name><surname>Zhabinets</surname><given-names>M.</given-names></name>
<name><surname>Fazli</surname><given-names>S.</given-names></name>
</person-group><article-title>A low-cost, imu-based real-time on device gesture recognition glove</article-title><source>Proceedings of the 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</source><conf-loc>Toronto, ON, Canada</conf-loc><conf-date>11&#x02013;14 October 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>3346</fpage><lpage>3351</lpage></element-citation></ref><ref id="B20-sensors-25-01181"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>M.</given-names></name>
<name><surname>Cho</surname><given-names>J.</given-names></name>
<name><surname>Lee</surname><given-names>S.</given-names></name>
<name><surname>Jung</surname><given-names>Y.</given-names></name>
</person-group><article-title>Imu sensor-based hand gesture recognition for human-machine interfaces</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>3827</elocation-id><pub-id pub-id-type="doi">10.3390/s19183827</pub-id><pub-id pub-id-type="pmid">31487894</pub-id>
</element-citation></ref><ref id="B21-sensors-25-01181"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mummadi</surname><given-names>C.K.</given-names></name>
<name><surname>Leo</surname><given-names>F.P.P.</given-names></name>
<name><surname>Verma</surname><given-names>K.D.</given-names></name>
<name><surname>Kasireddy</surname><given-names>S.</given-names></name>
<name><surname>Scholl</surname><given-names>P.M.</given-names></name>
<name><surname>Kempfle</surname><given-names>J.</given-names></name>
<name><surname>Laerhoven</surname><given-names>K.V.</given-names></name>
</person-group><article-title>Real-Time and Embedded Detection of Hand Gestures with an IMU-Based Glove</article-title><source>Informatics</source><year>2018</year><volume>5</volume><elocation-id>28</elocation-id><pub-id pub-id-type="doi">10.3390/informatics5020028</pub-id></element-citation></ref><ref id="B22-sensors-25-01181"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jaramillo</surname><given-names>A.G.</given-names></name>
<name><surname>Benalcazar</surname><given-names>M.E.</given-names></name>
</person-group><article-title>Real-time hand gesture recognition with EMG using machine learning</article-title><source>Proceedings of the 2017 IEEE second Ecuador technical chapters meeting (ETCM)</source><conf-loc>Salinas, Ecuador</conf-loc><conf-date>16&#x02013;20 October 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B23-sensors-25-01181"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qi</surname><given-names>J.</given-names></name>
<name><surname>Jiang</surname><given-names>G.</given-names></name>
<name><surname>Li</surname><given-names>G.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Tao</surname><given-names>B.</given-names></name>
</person-group><article-title>Surface EMG hand gesture recognition system based on PCA and GRNN</article-title><source>Neural Comput. Appl.</source><year>2020</year><volume>32</volume><fpage>6343</fpage><lpage>6351</lpage><pub-id pub-id-type="doi">10.1007/s00521-019-04142-8</pub-id></element-citation></ref><ref id="B24-sensors-25-01181"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Panda</surname><given-names>A.K.</given-names></name>
<name><surname>Chakravarty</surname><given-names>R.</given-names></name>
<name><surname>Moulik</surname><given-names>S.</given-names></name>
</person-group><article-title>Hand gesture recognition using flex sensor and machine learning algorithms</article-title><source>Proceedings of the 2020 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES)</source><conf-loc>Langkawi, Malaysia</conf-loc><conf-date>1&#x02013;3 March 2021</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>449</fpage><lpage>453</lpage></element-citation></ref><ref id="B25-sensors-25-01181"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Oudah</surname><given-names>M.</given-names></name>
<name><surname>Al-Naji</surname><given-names>A.</given-names></name>
<name><surname>Chahl</surname><given-names>J.</given-names></name>
</person-group><article-title>Hand gesture recognition based on computer vision: A review of techniques</article-title><source>J. Imaging</source><year>2020</year><volume>6</volume><elocation-id>73</elocation-id><pub-id pub-id-type="doi">10.3390/jimaging6080073</pub-id><pub-id pub-id-type="pmid">34460688</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01181"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Nguyen</surname><given-names>X.S.</given-names></name>
<name><surname>Brun</surname><given-names>L.</given-names></name>
<name><surname>L&#x000e9;zoray</surname><given-names>O.</given-names></name>
<name><surname>Bougleux</surname><given-names>S.</given-names></name>
</person-group><article-title>A neural network based on SPD manifold learning for skeleton-based hand gesture recognition</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#x02013;20 June 2019</conf-date><fpage>12036</fpage><lpage>12045</lpage></element-citation></ref><ref id="B27-sensors-25-01181"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Devineau</surname><given-names>G.</given-names></name>
<name><surname>Moutarde</surname><given-names>F.</given-names></name>
<name><surname>Xi</surname><given-names>W.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep learning for hand gesture recognition on skeletal data</article-title><source>Presented at 2018 13th IEEE International Conference on Automatic Face &#x00026; Gesture Recognition (FG 2018)</source><conf-loc>Xi&#x02019;an, China</conf-loc><conf-date>15&#x02013;19 May 2018</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><fpage>106</fpage><lpage>113</lpage></element-citation></ref><ref id="B28-sensors-25-01181"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lugaresi</surname><given-names>C.</given-names></name>
<name><surname>Tang</surname><given-names>J.</given-names></name>
<name><surname>Nash</surname><given-names>H.</given-names></name>
<name><surname>McClanahan</surname><given-names>C.</given-names></name>
<name><surname>Uboweja</surname><given-names>E.</given-names></name>
<name><surname>Hays</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>F.</given-names></name>
<name><surname>Chang</surname><given-names>C.L.</given-names></name>
<name><surname>Yong</surname><given-names>M.G.</given-names></name>
<name><surname>Lee</surname><given-names>J.</given-names></name>
</person-group><article-title>Mediapipe: A framework for building perception pipelines</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1906.08172</pub-id></element-citation></ref><ref id="B29-sensors-25-01181"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Labb&#x000e9;</surname><given-names>M.</given-names></name>
<name><surname>Michaud</surname><given-names>F.</given-names></name>
</person-group><article-title>Rtab-map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation</article-title><source>J. Field Robot.</source><year>2019</year><volume>36</volume><fpage>416</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1002/rob.21831</pub-id></element-citation></ref><ref id="B30-sensors-25-01181"><label>30.</label><element-citation publication-type="webpage"><article-title>Oculus Integration SDK</article-title><comment>Available online: <ext-link xlink:href="https://developers.meta.com/horizon/downloads/package/unity-integration/" ext-link-type="uri">https://developers.meta.com/horizon/downloads/package/unity-integration/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-31">(accessed on 31 December 2024)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01181-f001"><label>Figure 1</label><caption><p>Challenges in telemanipulation-based object handling in logistics environments. (<bold>a</bold>) Illustrates telemanipulation-based object-side picking with a suction gripper in a logistics environment. (<bold>b</bold>) Illustrates the occlusion of the target object or gripper, which hinders task execution. (<bold>c</bold>) Between the suction cup and the object&#x02019;s contact surface, larger &#x003b8; values reduce the chances of successful picking.</p></caption><graphic xlink:href="sensors-25-01181-g001" position="float"/></fig><fig position="float" id="sensors-25-01181-f002"><label>Figure 2</label><caption><p>Framework of the proposed hand gesture-based VR locomotion technique. (<bold>a</bold>) VR user interface: real-time visualization of the current viewpoint; (<bold>b</bold>) hand skeleton tracking: real-time extraction of 24 joint positions and orientations using HMD cameras; (<bold>c</bold>) MLP-based hand gesture recognition method: classification of hand gestures (fist, pointing, and unknown) for VR locomotion; (<bold>d</bold>) multi-mode hand gesture-based VR locomotion method: integration of translation, rotation, and fixed modes based on hand gestures; and (<bold>e</bold>) viewpoint update: gesture-controlled updates of the viewpoint according to the user&#x02019;s actions in the virtual environment.</p></caption><graphic xlink:href="sensors-25-01181-g002" position="float"/></fig><fig position="float" id="sensors-25-01181-f003"><label>Figure 3</label><caption><p>Illustrations of the VR user interface featuring a virtual hand and 3D point cloud map.</p></caption><graphic xlink:href="sensors-25-01181-g003" position="float"/></fig><fig position="float" id="sensors-25-01181-f004"><label>Figure 4</label><caption><p>Illustration of the overall process of translation mode in the proposed VR locomotion method. (<bold>a</bold>) The flowchart depicts the process of translation mode. (<bold>b</bold>) Schematic representation of the translation mode operation.</p></caption><graphic xlink:href="sensors-25-01181-g004" position="float"/></fig><fig position="float" id="sensors-25-01181-f005"><label>Figure 5</label><caption><p>Illustration of the overall process of rotation mode in the proposed VR locomotion method. (<bold>a</bold>) The flowchart depicts the process of rotation mode. (<bold>b</bold>) Schematic representation of the rotation mode operation.</p></caption><graphic xlink:href="sensors-25-01181-g005" position="float"/></fig><fig position="float" id="sensors-25-01181-f006"><label>Figure 6</label><caption><p>Confusion matrix of the prediction results for the test dataset.</p></caption><graphic xlink:href="sensors-25-01181-g006" position="float"/></fig><fig position="float" id="sensors-25-01181-f007"><label>Figure 7</label><caption><p>The proposed multi-mode hand gesture-based VR locomotion method represents the target object observation process. (<bold>top</bold>) Translation mode is activated by the fist gesture, moving the viewpoint closer to the target object. (<bold>middle</bold>) Rotation mode is triggered by the pointing gesture, adjusting the viewpoint angle to align with the target object. (<bold>bottom</bold>) The unknown gesture maintains the fixed mode, stabilizing the viewpoint near the target object. For a more detailed demonstration, please refer to the <xref rid="app1-sensors-25-01181" ref-type="app">Supplementary Materials Video S1</xref>.</p></caption><graphic xlink:href="sensors-25-01181-g007" position="float"/></fig><table-wrap position="float" id="sensors-25-01181-t001"><object-id pub-id-type="pii">sensors-25-01181-t001_Table 1</object-id><label>Table 1</label><caption><p>Distribution of gesture samples extracted from a continuous video and split into training and validation sets (8:2 ratio).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Fist</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pointing</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Unknown</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sum</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Train set</td><td align="center" valign="middle" rowspan="1" colspan="1">1076</td><td align="center" valign="middle" rowspan="1" colspan="1">1436</td><td align="center" valign="middle" rowspan="1" colspan="1">939</td><td align="center" valign="middle" rowspan="1" colspan="1">3451</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Validation set</td><td align="center" valign="middle" rowspan="1" colspan="1">268</td><td align="center" valign="middle" rowspan="1" colspan="1">359</td><td align="center" valign="middle" rowspan="1" colspan="1">234</td><td align="center" valign="middle" rowspan="1" colspan="1">861</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sum</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1344</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1795</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1173</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4312</td></tr></tbody></table></table-wrap></floats-group></article>