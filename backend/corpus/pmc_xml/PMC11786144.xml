<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Cureus</journal-id><journal-id journal-id-type="iso-abbrev">Cureus</journal-id><journal-id journal-id-type="issn">2168-8184</journal-id><journal-title-group><journal-title>Cureus</journal-title></journal-title-group><issn pub-type="epub">2168-8184</issn><publisher><publisher-name>Cureus</publisher-name><publisher-loc>Palo Alto (CA)</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11786144</article-id><article-id pub-id-type="doi">10.7759/cureus.76775</article-id><article-categories><subj-group subj-group-type="heading"><subject>Other</subject></subj-group><subj-group><subject>Medical Education</subject></subj-group><subj-group><subject>Healthcare Technology</subject></subj-group></article-categories><title-group><article-title>ChatGPT (GPT-4V) Performance on the Healthcare Information Technologist Examination in Japan</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Muacevic</surname><given-names>Alexander</given-names></name></contrib><contrib contrib-type="editor"><name><surname>Adler</surname><given-names>John R</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Ishida</surname><given-names>Kai</given-names></name><xref rid="aff-1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Hanada</surname><given-names>Eisuke</given-names></name><xref rid="aff-2" ref-type="aff">2</xref></contrib></contrib-group><aff id="aff-1">
<label>1</label>
Faculty of Engineering, Shonan Institute of Technology, Fujisawa, JPN </aff><aff id="aff-2">
<label>2</label>
Faculty of Science and Engineering, Saga University, Saga, JPN </aff><author-notes><corresp id="cor1">
Kai Ishida <email>gishikai310@gmail.com</email>
</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>1</day><month>1</month><year>2025</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>1</month><year>2025</year></pub-date><volume>17</volume><issue>1</issue><elocation-id>e76775</elocation-id><history><date date-type="accepted"><day>1</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025, Ishida et al.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Ishida et al.</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the Creative Commons Attribution License CC-BY 4.0., which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri xlink:href="https://www.cureus.com/articles/331265-chatgpt-gpt-4v-performance-on-the-healthcare-information-technologist-examination-in-japan">This article is available from https://www.cureus.com/articles/331265-chatgpt-gpt-4v-performance-on-the-healthcare-information-technologist-examination-in-japan</self-uri><abstract><p>Introduction</p><p>The Chat Generative Pretrained Transformer (ChatGPT) has developed rapidly and is used in many fields, including healthcare informatics. This study evaluated ChatGPT (GPT-4V)'s performance on the Healthcare Information Technologist (HCIT) certification exam in Japan, which assesses certified professionals who work with electronic health records to improve patient care.</p><p>Methodology</p><p>Four hundred seventy-six questions from the HCIT exam were targeted over three years.&#x000a0;ChatGPT (GPT-4V) was tested on its ability to answer questions from an HCIT exam to determine if it could perform as well as or better than aspirants taking the exam. Moreover, its performance was evaluated for each academic category, format, presence or absence of images, and calculations.</p><p>Results</p><p>The mean correct answer rate for all questions was 84%.&#x000a0;ChatGPT achieved the exam passing criteria. The correct answer rate for simple-choice (A-type) questions was higher than that for multiple-choice (X2-type) questions (<italic>P</italic> &#x0003c; 0.05). The success rate for questions with images was lower than for text-only questions (<italic>P</italic> &#x0003c; 0.01), and the success rate for questions requiring calculations was lower than for those without calculations (<italic>P</italic> &#x0003c; 0.05).</p><p>Conclusions</p><p>ChatGPT (GPT-4V) met the passing criteria for the 19th to 21st HCIT exams, suggesting that its performance is effective in passing the HCIT exam. ChatGPT may possess the minimum required knowledge, understanding, and application skills for the HCIT certification.</p></abstract><kwd-group kwd-group-type="author"><kwd>artificial intelligence</kwd><kwd>chatgpt</kwd><kwd>healthcare information technologist</kwd><kwd>medical education</kwd><kwd>multimodal large language models</kwd></kwd-group></article-meta></front><body><sec sec-type="intro"><title>Introduction</title><p>Artificial intelligence (AI) has quickly gained popularity in various fields. Huge tasks previously performed only by humans are now easily performed by AI-assisted software and robots. In particular, multimodal large language models (MLLMs) can provide answers to questions and also generate new sentences, images, music, and videos. The Chat Generative Pretrained Transformer (ChatGPT), released by Open AI in November 2022, is an MLLM that has attracted attention because of its ability to generate detailed answers to questions in various fields [<xref rid="REF1" ref-type="bibr">1</xref>].</p><p>ChatGPT answers medical questions with a certain degree of accuracy [<xref rid="REF2" ref-type="bibr">2</xref>,<xref rid="REF3" ref-type="bibr">3</xref>] and can be applied to various medical areas. For example, it is used to support the diagnosis of common complaints, cancer screening, automatic generation of diagnostic reports, and medical education applications [<xref rid="REF4" ref-type="bibr">4</xref>-<xref rid="REF7" ref-type="bibr">7</xref>]. Therefore, ChatGPT can assist medical and healthcare students and professionals. Its ability to pass various medical and healthcare licensing exams has been reported. Many researchers have found that ChatGPT can pass national exams for physicians, pharmacists, nurses, and other healthcare professionals [<xref rid="REF8" ref-type="bibr">8</xref>-<xref rid="REF20" ref-type="bibr">20</xref>]. Thus, ChatGPT has some clinical, basic, and related medical knowledge, including pharmacy, diagnostics, rehabilitation, and nutrition. However, we have reported the weaknesses of ChatGPT; for example, ChatGPT could not recognize images and lacked knowledge of standards and medical laws [<xref rid="REF21" ref-type="bibr">21</xref>].</p><p>Healthcare information systems such as electronic medical records and ordering systems have been introduced and used in clinical settings to promote efficiency, reliability, and safety. However, persons with appropriate knowledge are needed to use these systems safely and effectively. A healthcare information technologist (HCIT) is an expert in these systems. Although studies have evaluated the accuracy of purely medical qualification examinations, such as those for physicians, pharmacists, and nurses, studies that examine the performance of MLLMs on questions involving healthcare information technology expertise or knowledge are lacking. This study aimed to evaluate the accuracy of the responses of the current ChatGPT to the HCIT exams in Japan to address this research gap.</p></sec><sec sec-type="materials|methods"><title>Materials and methods</title><p>Overview of HCITs in Japan</p><p>An HCIT certified by the Japan Association for Medical Informatics (JAMI) is a professional in the health, medical, and welfare field who can safely and appropriately manage, use, and provide healthcare information based on the characteristics of medical care and using optimal information processing technology to improve the quality and safety of health, medical care, and welfare. The abilities required for an HCIT are categorized into three knowledge and skill areas: medicine and medical care, information processing technology, and healthcare information systems. Additionally, three key abilities are emphasized: communication with other professionals, collaboration with various professions, and coordination to make adjustments between departments and professions.</p><p>The HCIT exam assesses whether a candidate has the necessary knowledge and skills to use information processing technology to improve business operations safely, appropriately manage healthcare information, and use medical data. This exam has 160 questions in three sections: healthcare (50 items), information technology (50 items), and health information systems (60 items). Each section also includes 8-10 subsections. Table <xref rid="TAB1" ref-type="table">1</xref> presents examples of the main topics of each section. Healthcare section contains basic and clinical medicine, diagnosis and treatment, medical records, and clinical databases. Information technology contains hard and software, network, database, and security. Health information systems contain the introduction and operation of systems, standardization, and related law and guidelines. The exam has A-type questions, where an examinee selects one correct answer from five choices and X2-type questions, where two correct answers are selected from five choices. The X2-type will be asked in the healthcare and health&#x000a0;information systems sections.&#x000a0;There are no specific qualifications to take the exam to become a HCIT.&#x000a0;Many of the test takers are healthcare professionals such as nurses, pharmacists, and clinical radiologists, engineers and sales staff from manufacturers that deal with health&#x000a0;information systems, and students studying healthcare information. The HCIT exam is held every August with a passing score of approximately 60% or higher for all areas. Approximately 3,000 people take it with a pass rate of about 30%.</p><table-wrap position="float" id="TAB1"><label>Table 1</label><caption><title>Overview of the main topics of the HCIT exam in each academic field.</title><p>HCIT,&#x000a0;healthcare information technologist; DICOM,&#x000a0;Digital Imaging and Communications in Medicine; HL7,&#x000a0;Health Level 7; IHE,&#x000a0;Integrating the Healthcare Enterprise</p></caption><table frame="hsides" rules="groups"><tbody><tr style="background-color:#ccc"><td rowspan="1" colspan="1">&#x03000;</td><td rowspan="1" colspan="1">Sub-sections</td><td rowspan="1" colspan="1">Main topics</td></tr><tr><td rowspan="11" colspan="1">Healthcare&#x000a0;</td><td rowspan="1" colspan="1">Medicine/Medical general theory</td><td rowspan="1" colspan="1">Medical policy and Medical ethics</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Healthcare system</td><td rowspan="1" colspan="1">Medical-related laws, Medical professionals, and Regional medical cooperation</td></tr><tr><td rowspan="1" colspan="1">Medical management</td><td rowspan="1" colspan="1">Hospital management and Medical safety&#x000a0;</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Medical process</td><td rowspan="1" colspan="1">Clinical process and Clinical guideline</td></tr><tr><td rowspan="1" colspan="1">Medicine / Pharmacy / Nursing</td><td rowspan="1" colspan="1">Basic and clinical medicine, Pharmaceuticals, Clinical nursing</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Diagnosis</td><td rowspan="1" colspan="1">Clinical examination and Image diagnosis</td></tr><tr><td rowspan="1" colspan="1">Treatment</td><td rowspan="1" colspan="1">Treatment method and Rehabilitation</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Medical records</td><td rowspan="1" colspan="1">Various medical records such as initial and progress records, nursing records, and treatment records&#x000a0;</td></tr><tr><td rowspan="1" colspan="1">Medical research</td><td rowspan="1" colspan="1">Research guidelines, ethics, and evidence</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Medical statistics</td><td rowspan="1" colspan="1">Basic statistics and Statistical test</td></tr><tr><td rowspan="1" colspan="1">Clinical database</td><td rowspan="1" colspan="1">National database and Cancer registration</td></tr><tr style="background-color:#ccc"><td rowspan="9" colspan="1">Information technology</td><td rowspan="1" colspan="1">Information representation</td><td rowspan="1" colspan="1">Binary, Hexadecimal, Logical operation, and Data format</td></tr><tr><td rowspan="1" colspan="1">Hardware</td><td rowspan="1" colspan="1">Arithmetic logic unit, Control unit, Computer data storage, Input device, and Output device</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Software</td><td rowspan="1" colspan="1">Operating system, Application software, and Data algorithms</td></tr><tr><td rowspan="1" colspan="1">Database</td><td rowspan="1" colspan="1">Relational database and Data management</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Network</td><td rowspan="1" colspan="1">Communication protocol and Network services</td></tr><tr><td rowspan="1" colspan="1">Security</td><td rowspan="1" colspan="1">Security management and Encryption technology</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">System development</td><td rowspan="1" colspan="1">Development process and Project management</td></tr><tr><td rowspan="1" colspan="1">System operation and management</td><td rowspan="1" colspan="1">System multiplexing and User management</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Latest technology</td><td rowspan="1" colspan="1">Internet of things, Virtual reality, and Agumented reality</td></tr><tr><td rowspan="8" colspan="1">Health information system</td><td rowspan="1" colspan="1">Characteristics of medical information system</td><td rowspan="1" colspan="1">Primary use/secondary use of information, Ethics and responsibilities of information users</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Functions of hospital information system</td><td rowspan="1" colspan="1">Electronic medical record, Ordering system, and Other hospital information system&#x000a0;</td></tr><tr><td rowspan="1" colspan="1">Introductions of hospital information system</td><td rowspan="1" colspan="1">Procurement, Contract, and Requirements</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Operations of hospital information system</td><td rowspan="1" colspan="1">Organizational structure and User management</td></tr><tr><td rowspan="1" colspan="1">Healthcare supporting system</td><td rowspan="1" colspan="1">Regional medical cooperation system and Telemedicine system</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Medical information standards</td><td rowspan="1" colspan="1">Standard master, DICOM, HL7, and IHE</td></tr><tr><td rowspan="1" colspan="1">Law and related guidelines</td><td rowspan="1" colspan="1">Personal information protection law and other related laws and guidelines</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Data analysis of medical information</td><td rowspan="1" colspan="1">Data analysis of quality of medical care, medical management, and clinical research</td></tr></tbody></table></table-wrap><p>Input to the ChatGPT</p><p>We targeted 476 questions from the 19th to the 21st HCIT exams held between 2021 to 2023. Four questions were excluded from the scoring. This includes multiple-answer questions with due to infelicity. Table <xref rid="TAB2" ref-type="table">2</xref> presents the number of questions of the 19th to 21th HCIT exam in each format, excluding 4 infelicity questions. First, we input the sentence &#x0201c;You are aiming to become a Japanese HCIT. Here, you undergo an HCIT exam. Please explain me the correct answer.&#x0201d; into ChatGPT (GPT-4V). We input the Japanese version of the question texts, including images (figures/tables), and options from the HCIT exams for each question. ChatGPT generated answers along with explanations, and we collected and evaluated the content. We compared the answer options provided by ChatGPT with the correct answers from the JAMI website. For the X2-type questions, the correct answer was when both choices were correct. A researcher who has been licensed as HCIT for over 10 years performed the work. The input was performed between March 5&#x000a0;and April 8, 2024.</p><table-wrap position="float" id="TAB2"><label>Table 2</label><caption><title>Number of questions of the 19th to 21st HCIT exams for each question format.</title><p>HCIT,&#x000a0;healthcare information technologist</p></caption><table frame="hsides" rules="groups"><tbody><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Section</td><td rowspan="1" colspan="1">Question format</td><td rowspan="1" colspan="1">Number of questions</td></tr><tr><td rowspan="2" colspan="1">Healthcare</td><td rowspan="1" colspan="1">A-type</td><td rowspan="1" colspan="1">120</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">X2-type</td><td rowspan="1" colspan="1">30</td></tr><tr><td rowspan="4" colspan="1">Information technology</td><td rowspan="1" colspan="1">Questions with images</td><td rowspan="1" colspan="1">20</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Text-only question</td><td rowspan="1" colspan="1">129</td></tr><tr><td rowspan="1" colspan="1">Questions with calculations</td><td rowspan="1" colspan="1">28</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Non-calculation questions</td><td rowspan="1" colspan="1">121</td></tr><tr><td rowspan="2" colspan="1">Health information technology</td><td rowspan="1" colspan="1">A-type</td><td rowspan="1" colspan="1">148</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">X2-type</td><td rowspan="1" colspan="1">29</td></tr></tbody></table></table-wrap><p>Analysis</p><p>This study calculated the percentage of correct answers for each section. Furthermore, we calculated the correct answer rate for A-type and X2-type questions in the healthcare and health information system sections. We also calculated the percentage of correct answers and the presence or absence of calculations in the information technology sections based on the questions with images and text-only. The statistical analysis of A- and X2-type questions was performed via Microsoft Excel 2021 using the chi-squared test. This test was also used for questions with images, text-only questions and with/without calculations. The significance level was a <italic>P</italic>-value of less than 5%.</p></sec><sec sec-type="results"><title>Results</title><p>The overall results are summarized in Figure <xref rid="FIG1" ref-type="fig">1</xref>. The mean correct answer rate for all questions was 84%. The mean percentages of correct responses for healthcare, information technology, and health information systems were 85%, 90%, and 79%, respectively. The subsection rates for each section are shown in Figures <xref rid="FIG2" ref-type="fig">2</xref>-<xref rid="FIG4" ref-type="fig">4</xref>. In the healthcare section, the mean correct answer rate reached 100% for medical statistics. In the medicine/medical general theory sections, medical management and diagnosis were high. The correct answer was over 90%. Conversely, the treatment and clinical databases did not have a high correct answer rate. It was less than 70%. In the information technology section, the mean correct answer rate reached 100% for the latest technology and close to 100% for network and security. However, software and system development did not have a high correct answer rate. It was less than 80%. In health information systems, the mean correct answer rate was close to 100% for operations of the hospital information system. Conversely, the characteristics of medical systems were low (42%).</p><fig position="anchor" fig-type="figure" id="FIG1"><label>Figure 1</label><caption><title>Total ChatGPT (GPT-4V) performance on the 19th-21st HCIT exams.</title><p>HCIT, healthcare information technologist</p></caption><graphic xlink:href="cureus-0017-00000076775-i01" position="float"/></fig><fig position="anchor" fig-type="figure" id="FIG2"><label>Figure 2</label><caption><title>ChatGPT (GPT-4V) performance on the 19th-21st HCIT exam in the healthcare section.</title><p>HCIT, healthcare information technologist</p></caption><graphic xlink:href="cureus-0017-00000076775-i02" position="float"/></fig><fig position="anchor" fig-type="figure" id="FIG3"><label>Figure 3</label><caption><title>ChatGPT (GPT-4V) performance on the 19th-21st HCIT exam in the information technology section.</title><p>HCIT, healthcare information technologist</p></caption><graphic xlink:href="cureus-0017-00000076775-i03" position="float"/></fig><fig position="anchor" fig-type="figure" id="FIG4"><label>Figure 4</label><caption><title>ChatGPT (GPT-4V) performance on the 19th to 21st HCIT exams in health information systems.</title><p>HCIT, healthcare information technologist</p></caption><graphic xlink:href="cureus-0017-00000076775-i04" position="float"/></fig><p>The mean correct answer rates for question type, questions with images, text-only questions, and with/without calculations are shown in Figures <xref rid="FIG5" ref-type="fig">5</xref>-<xref rid="FIG7" ref-type="fig">7</xref>. These rates for the A-type and X2-type questions were 91% and 80%, respectively, with a significant difference (<italic>P</italic> &#x0003c; 0.05). These rates for questions with images and text only are 40% and 94%, respectively, with a significant difference (<italic>P</italic> &#x0003c; 0.01). The calculation and non-calculation question rates were 79% and 93%, respectively, with a significant difference (<italic>P</italic> &#x0003c; 0.05).</p><fig position="anchor" fig-type="figure" id="FIG5"><label>Figure 5</label><caption><title>ChatGPT (GPT-4V) performance on the 19th to 21st HCIT exams in question format.</title><p>HCIT,&#x000a0;healthcare information technologist</p></caption><graphic xlink:href="cureus-0017-00000076775-i05" position="float"/></fig><fig position="anchor" fig-type="figure" id="FIG6"><label>Figure 6</label><caption><title>ChatGPT (GPT-4V) performance on the 19th&#x02013;21st HCIT exams in sections with questions that include images and text-only content.</title><p>HCIT,&#x000a0;healthcare information technologist</p></caption><graphic xlink:href="cureus-0017-00000076775-i06" position="float"/></fig><fig position="anchor" fig-type="figure" id="FIG7"><label>Figure 7</label><caption><title>ChatGPT (GPT-4V) performance on the 19th-21st HCIT exams in sections with questions that include images and text-only content.</title><p>HCIT,&#x000a0;healthcare information technologist</p></caption><graphic xlink:href="cureus-0017-00000076775-i07" position="float"/></fig><p>Here are some incorrect questions. Figure <xref rid="FIG8" ref-type="fig">8</xref> is a question in the information technology section, focusing on logical operations. While the explanation of each operator is correct, the correct calculation is not performed, making the answer incorrect. The correct answer is 5) Logical Non-Conjunction (NAND). Figure <xref rid="FIG9" ref-type="fig">9</xref> is a question about the characteristics of medical information systems, from the health information systems section. The question asks about the primary use of healthcare information. While the explanations of primary use and secondary use are correct, the explanation for option 4) is incorrect. Additionally, the correct answer is listed as 5) secondary use, but the correct answer should be primary use.</p><fig position="anchor" fig-type="figure" id="FIG8"><label>Figure 8</label><caption><title>Sample question for an incorrect answer in the information technology section.</title></caption><graphic xlink:href="cureus-0017-00000076775-i08" position="float"/></fig><fig position="anchor" fig-type="figure" id="FIG9"><label>Figure 9</label><caption><title>Sample question for an incorrect answer in the health information systems section.</title></caption><graphic xlink:href="cureus-0017-00000076775-i09" position="float"/></fig></sec><sec sec-type="discussion"><title>Discussion</title><p>Our results show that the correct answer rate by ChatGPT for all sections reached more than 75%. Thus, ChatGPT (GPT-4V) meets the passing criteria for the HCIT exams. This suggests that ChatGPT exhibits the performance required to pass the HCIT exam. This multiple-choice question exam assesses not only memory but also understanding, application, analysis, and evaluation skills [<xref rid="REF22" ref-type="bibr">22</xref>,<xref rid="REF23" ref-type="bibr">23</xref>]. Therefore, ChatGPT may possess the minimum required knowledge, understanding, and application skills necessary for an HCIT.</p><p>According to our results, the percentage of correct answers was high for basic/clinical medicine, diagnostics, networks, and security, which require textbook-based knowledge. It is possible that the high rate of correct answers was due to the large amount of learning resources available on the World Wide Web. Alternatively, for some categories, not only knowledge from textbooks but also knowledge from practical experience may be advantageous. For instance, practical knowledge is necessary in categories such as the characteristics of medical information and development, functionality, and implementation of medical information systems. Compared to healthcare and information technology, there is less learning data on the web, and it is thought that the accuracy was low due to the lack of learning in ChatGPT. Additionally, laws, guidelines, standards, etc. are revised and change every certain number of years. Therefore, since knowledge from the past (before the revision) and present (after the revision) is mixed, it is conceivable that there is a high possibility of incorrect answers. It has been reported that the accuracy of answers regarding laws and related standards in other qualification exams is worse than in multiple fields [<xref rid="REF21" ref-type="bibr">21</xref>].</p><p>In this study, there was a significant difference between the correct answer rate for A- and X2-type questions. However, there is no significant difference between them in Japanese physical therapist national exams [<xref rid="REF19" ref-type="bibr">19</xref>]. It is thought that the level of difficulty depends on the question format, which varies depending on the field in question and the type of qualification. Correct answer rates were low for questions that required image recognition and calculations. According to past research, the correct answer rate on ChatGPT generally decreases in questions that include images, tables, or diagrams [<xref rid="REF20" ref-type="bibr">20</xref>]. Questions that require calculations would be worse than simple questions that do not require calculations [<xref rid="REF21" ref-type="bibr">21</xref>]. Specifically, the correct answer rate was low for questions that required recognizing images, creating calculation formulas, and substituting numbers. At present, the accuracy of recognizing images is currently an issue. If precision were improved, the results would be better.</p><p>This study has limitations. First, the ChatGPT output changes depending on the content of the input prompt [<xref rid="REF24" ref-type="bibr">24</xref>]. In the study targeting ChatGPT (GPT-4) for the Japanese national medical licensing exam, this tool successfully met the minimum passing threshold by adjusting the prompt using the previous year&#x02019;s questions and performing input after tuning, such as translating from Japanese to English, [<xref rid="REF13" ref-type="bibr">13</xref>]. Several studies on ChatGPT (GPT-4) that targeted the Japanese national medical and healthcare licensing examinations only considered simple prompts [<xref rid="REF12" ref-type="bibr">12</xref>-<xref rid="REF14" ref-type="bibr">14</xref>, <xref rid="REF17" ref-type="bibr">17</xref>-<xref rid="REF19" ref-type="bibr">19</xref>, <xref rid="REF21" ref-type="bibr">21</xref>]. However, almost all studies met the passing threshold even without special prompt adjustments or translation from Japanese to English. The ChatGPT-4 performance considerably improved compared with the previous version (GPT-3.5). Even without special prompt engineering, ChatGPT-4 can pass national medical and healthcare examinations. OpenAI reported a slight difference in performance related to language differences in GPT-4 [<xref rid="REF25" ref-type="bibr">25</xref>]. Therefore, we implemented only one type of prompt. Other special prompt engineering was not considered. Thus, the results may cause changes per question. On the other hand, since the accuracy of answers to questions in the same field was roughly the same, we believe that we have been able to make a certain level of evaluation based on the accuracy of answers provided by ChatGPT. Among the many MLLMs, we only targeted ChatGPT (GPT-4V). For this reason, we targeted ChatGPT as it has received relatively high attention in the field of MLLM, where research progresses extremely rapidly. Previous research on answer analysis for medical and healthcare licensing examinations was based on ChatGPT [<xref rid="REF7" ref-type="bibr">7</xref>-<xref rid="REF21" ref-type="bibr">21</xref>]. In the future, it may be possible to obtain more accurate results by targeting other MLLMs or a newly emerging one. Moreover,&#x000a0;HCIT qualification in Japan is not universal to other countries.&#x000a0;In interpreting the results, it needs to be compared to an equivalent professional exam. However, we found no studies that evaluated the accuracy of ChatGPT for other equivalent healthcare information qualification exams.&#x000a0;Additionally, the HCIT exam in Japan consists solely of multiple-choice questions and does not include other formats, such as descriptive or argumentative questions. Therefore, this exam may not fully assess ChatGPT's overall performance. In addition, the study does not provide a detailed analysis of the questions answered incorrectly. If incorrect answers are due to gaps in ChatGPT's knowledge, the patterns of incorrect responses might differ across various fields.</p><p>Finally, based on the results so far, we will discuss prospects. ChatGPT's answers to the HCIT exam were highly accurate, and many of the explanations for the answers were appropriate, so we believe it can contribute to the learning of those aiming to take this exam. In addition, in this evaluation, the accuracy of the answers to the healthcare information systems section was low, but if the accuracy of the answers increases with future improvements to the MLLM, it may be possible to contribute to supporting system implementation and operation. However, in all use cases, it is desirable to utilize the generated content while carefully examining it.</p></sec><sec sec-type="conclusions"><title>Conclusions</title><p>This study evaluated the performance of ChatGPT (GPT-4V) in the Japanese HCIT exam using 476 questions from 2021 to 2023. The mean correct answer rate for all questions was 84%. Although GPT-4V performed considerably well in some academic fields, we found that its correct answer rate was lower for questions based on practical experience, knowledge of standards and regulations, as well as those involving figures/tables and calculations. This rate was low. Although general medical knowledge and some information technology knowledge appear sufficient, the correct answer rate could improve with updates to existing and new models.</p></sec></body><back><fn-group content-type="conflict"><title>Disclosures</title><fn fn-type="COI-statement"><p><bold>Human subjects:</bold> All authors have confirmed that this study did not involve human participants or tissue.</p><p><bold>Animal subjects:</bold> All authors have confirmed that this study did not involve animal subjects or tissue.</p><p><bold>Conflicts of interest:</bold> In compliance with the ICMJE uniform disclosure form, all authors declare the following:</p><p><bold>Payment/services info:</bold> All authors have declared that no financial support was received from any organization for the submitted work.</p><p><bold>Financial relationships:</bold> All authors have declared that they have no financial relationships at present or within the previous three years with any organizations that might have an interest in the submitted work.</p><p><bold>Other relationships:</bold> All authors have declared that there are no other relationships or activities that could appear to have influenced the submitted work.</p></fn></fn-group><fn-group content-type="other"><title>Author Contributions</title><fn fn-type="other"><p><bold>Concept and design:</bold>&#x000a0; Kai Ishida, Eisuke Hanada</p><p><bold>Acquisition, analysis, or interpretation of data:</bold>&#x000a0; Kai Ishida, Eisuke Hanada</p><p><bold>Drafting of the manuscript:</bold>&#x000a0; Kai Ishida, Eisuke Hanada</p><p><bold>Supervision:</bold>&#x000a0; Kai Ishida, Eisuke Hanada</p></fn></fn-group><ref-list><title>References</title><ref id="REF1"><label>1</label><element-citation publication-type="webpage"><article-title>Introducing ChatGPT</article-title><year>2022</year><uri xlink:href="https://openai.com/blog/chatgpt/">https://openai.com/blog/chatgpt/</uri></element-citation></ref><ref id="REF2"><label>2</label><element-citation publication-type="journal"><article-title>ChatGPT utility in healthcare education, research, and practice: systematic review on the Promising Perspectives and Valid Concerns</article-title><source>Healthcare (Basel)</source><person-group>
<name><surname>Sallam</surname><given-names>M</given-names></name>
</person-group><volume>11</volume><year>2023</year><uri xlink:href="https://doi.org/10.3390/healthcare11060887">https://doi.org/10.3390/healthcare11060887</uri></element-citation></ref><ref id="REF3"><label>3</label><element-citation publication-type="journal"><article-title>Evaluating ChatGPT's ability to solve higher-order wuestions on the competency-based medical education curriculum in medical biochemistry</article-title><source>Cureus</source><person-group>
<name><surname>Ghosh</surname><given-names>A</given-names></name>
<name><surname>Bir</surname><given-names>A</given-names></name>
</person-group><fpage>0</fpage><volume>15</volume><year>2023</year><uri xlink:href="https://doi.org/10.7759/cureus.37023">https://doi.org/10.7759/cureus.37023</uri></element-citation></ref><ref id="REF4"><label>4</label><element-citation publication-type="journal"><article-title>Diagnostic accuracy of differential-diagnosis lists generated by Generative Pretrained Transformer 3 Chatbot for clinical vignettes with common chief complaints: a pilot study</article-title><source>Int J Environ Res Public Health</source><person-group>
<name><surname>Hirosawa</surname><given-names>T</given-names></name>
<name><surname>Harada</surname><given-names>Y</given-names></name>
<name><surname>Yokose</surname><given-names>M</given-names></name>
<name><surname>Sakamoto</surname><given-names>T</given-names></name>
<name><surname>Kawamura</surname><given-names>R</given-names></name>
<name><surname>Shimizu</surname><given-names>T</given-names></name>
</person-group><volume>20</volume><year>2023</year><uri xlink:href="https://doi.org/10.3390/ijerph20043378">https://doi.org/10.3390/ijerph20043378</uri></element-citation></ref><ref id="REF5"><label>5</label><element-citation publication-type="journal"><article-title>Using ChatGPT to evaluate cancer myths and misconceptions: artificial intelligence and cancer information</article-title><source>JNCI Cancer Spectr</source><person-group>
<name><surname>Johnson</surname><given-names>SB</given-names></name>
<name><surname>King</surname><given-names>AJ</given-names></name>
<name><surname>Warner</surname><given-names>EL</given-names></name>
<name><surname>Aneja</surname><given-names>S</given-names></name>
<name><surname>Kann</surname><given-names>BH</given-names></name>
<name><surname>Bylund</surname><given-names>CL</given-names></name>
</person-group><volume>7</volume><year>2023</year><uri xlink:href="https://doi.org/10.1093/jncics/pkad015">https://doi.org/10.1093/jncics/pkad015</uri></element-citation></ref><ref id="REF6"><label>6</label><element-citation publication-type="journal"><article-title>Using ChatGPT to write patient clinic letters</article-title><source>Lancet Digit Health</source><person-group>
<name><surname>Ali</surname><given-names>SR</given-names></name>
<name><surname>Dobbs</surname><given-names>TD</given-names></name>
<name><surname>Hutchings</surname><given-names>HA</given-names></name>
<name><surname>Whitaker</surname><given-names>IS</given-names></name>
</person-group><fpage>179</fpage><lpage>181</lpage><volume>5</volume><year>2023</year><uri xlink:href="http://doi.org/10.1016/S2589-7500(23)00048-1">http://doi.org/10.1016/S2589-7500(23)00048-1</uri></element-citation></ref><ref id="REF7"><label>7</label><element-citation publication-type="journal"><article-title>Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models</article-title><source>PLOS Digit Health</source><person-group>
<name><surname>Kung</surname><given-names>TH</given-names></name>
<name><surname>Cheatham</surname><given-names>M</given-names></name>
<name><surname>Medenilla</surname><given-names>A</given-names></name>
<etal/>
</person-group><fpage>0</fpage><volume>2</volume><year>2023</year><uri xlink:href="https://doi.org/10.1371/journal.pdig.0000198">https://doi.org/10.1371/journal.pdig.0000198</uri></element-citation></ref><ref id="REF8"><label>8</label><element-citation publication-type="journal"><article-title>Evaluating the performance of ChatGPT-4 on the United Kingdom Medical Licensing Assessment</article-title><source>Front Med (Lausanne)</source><person-group>
<name><surname>Lai</surname><given-names>UH</given-names></name>
<name><surname>Wu</surname><given-names>KS</given-names></name>
<name><surname>Hsu</surname><given-names>TY</given-names></name>
<name><surname>Kan</surname><given-names>JK</given-names></name>
</person-group><fpage>1240915</fpage><volume>10</volume><year>2023</year><uri xlink:href="https://doi.org/10.3389/fmed.2023.1240915">https://doi.org/10.3389/fmed.2023.1240915</uri><pub-id pub-id-type="pmid">37795422</pub-id>
</element-citation></ref><ref id="REF9"><label>9</label><element-citation publication-type="journal"><article-title>ChatGPT in Iranian medical licensing examination: evaluating the diagnostic accuracy and decision-making capabilities of an AI-based model</article-title><source>BMJ Health Care Inform</source><person-group>
<name><surname>Ebrahimian</surname><given-names>M</given-names></name>
<name><surname>Behnam</surname><given-names>B</given-names></name>
<name><surname>Ghayebi</surname><given-names>N</given-names></name>
<name><surname>Sobhrakhshankhah</surname><given-names>E</given-names></name>
</person-group><volume>30</volume><year>2023</year><uri xlink:href="https://doi.org/10.1136/bmjhci-2023-100815">https://doi.org/10.1136/bmjhci-2023-100815</uri></element-citation></ref><ref id="REF10"><label>10</label><element-citation publication-type="journal"><article-title>ChatGPT passes German State Examination in Medicine With Picture Questions Omitted</article-title><source>Dtsch Arztebl Int</source><person-group>
<name><surname>Jung</surname><given-names>LB</given-names></name>
<name><surname>Gudera</surname><given-names>JA</given-names></name>
<name><surname>Wiegand</surname><given-names>TL</given-names></name>
<name><surname>Allmendinger</surname><given-names>S</given-names></name>
<name><surname>Dimitriadis</surname><given-names>K</given-names></name>
<name><surname>Koerte</surname><given-names>IK</given-names></name>
</person-group><fpage>373</fpage><lpage>374</lpage><volume>120</volume><year>2023</year><uri xlink:href="https://doi.org/10.3238/arztebl.m2023.0113">https://doi.org/10.3238/arztebl.m2023.0113</uri><pub-id pub-id-type="pmid">37530052</pub-id>
</element-citation></ref><ref id="REF11"><label>11</label><element-citation publication-type="journal"><article-title>Evaluation of the performance of GPT-3.5 and GPT-4 on the Polish Medical Final Examination</article-title><source>Sci Rep</source><person-group>
<name><surname>Roso&#x00142;</surname><given-names>M</given-names></name>
<name><surname>G&#x00105;sior</surname><given-names>JS</given-names></name>
<name><surname>&#x00141;aba</surname><given-names>J</given-names></name>
<name><surname>Korzeniewski</surname><given-names>K</given-names></name>
<name><surname>M&#x00142;y&#x00144;czak</surname><given-names>M</given-names></name>
</person-group><fpage>20512</fpage><volume>13</volume><year>2023</year><uri xlink:href="https://doi.org/10.1038/s41598-023-46995-z">https://doi.org/10.1038/s41598-023-46995-z</uri><pub-id pub-id-type="pmid">37993519</pub-id>
</element-citation></ref><ref id="REF12"><label>12</label><element-citation publication-type="journal"><article-title>Performance of GPT-3.5 and GPT-4 on the Japanese Medical Licensing Examination: comparison study</article-title><source>JMIR Med Educ</source><person-group>
<name><surname>Takagi</surname><given-names>S</given-names></name>
<name><surname>Watari</surname><given-names>T</given-names></name>
<name><surname>Erabi</surname><given-names>A</given-names></name>
<name><surname>Sakaguchi</surname><given-names>K</given-names></name>
</person-group><fpage>0</fpage><volume>9</volume><year>2023</year><uri xlink:href="https://doi.org/10.2196/48002">https://doi.org/10.2196/48002</uri></element-citation></ref><ref id="REF13"><label>13</label><element-citation publication-type="journal"><article-title>Performance of Generative Pretrained Transformer on the National Medical Licensing Examination in Japan</article-title><source>PLOS Digit Health</source><person-group>
<name><surname>Tanaka</surname><given-names>Y</given-names></name>
<name><surname>Nakata</surname><given-names>T</given-names></name>
<name><surname>Aiga</surname><given-names>K</given-names></name>
<etal/>
</person-group><fpage>0</fpage><volume>3</volume><year>2024</year><uri xlink:href="https://doi.org/10.1371/journal.pdig.0000433">https://doi.org/10.1371/journal.pdig.0000433</uri></element-citation></ref><ref id="REF14"><label>14</label><element-citation publication-type="journal"><article-title>The potential of GPT-4 as a support tool for pharmacists: analytical study using the Japanese National Examination for Pharmacists</article-title><source>JMIR Med Educ</source><person-group>
<name><surname>Kunitsu</surname><given-names>Y</given-names></name>
</person-group><fpage>0</fpage><volume>9</volume><year>2023</year><uri xlink:href="https://doi.org/10.2196/48452">https://doi.org/10.2196/48452</uri></element-citation></ref><ref id="REF15"><label>15</label><element-citation publication-type="journal"><article-title>ChatGPT (GPT-4) passed the Japanese National License Examination for Pharmacists in 2022, answering all items including those with diagrams: a descriptive study</article-title><source>J Educ Eval Health Prof</source><person-group>
<name><surname>Sato</surname><given-names>H</given-names></name>
<name><surname>Ogasawara</surname><given-names>K</given-names></name>
</person-group><fpage>4</fpage><volume>21</volume><year>2024</year><uri xlink:href="https://doi.org/10.3352/jeehp.2024.21.4">https://doi.org/10.3352/jeehp.2024.21.4</uri><pub-id pub-id-type="pmid">38413129</pub-id>
</element-citation></ref><ref id="REF16"><label>16</label><element-citation publication-type="journal"><article-title>Assessing the performance of GPT-3.5 and GPT-4 on the 2023 Japanese Nursing Examination</article-title><source>Cureus</source><person-group>
<name><surname>Kaneda</surname><given-names>Y</given-names></name>
<name><surname>Takahashi</surname><given-names>R</given-names></name>
<name><surname>Kaneda</surname><given-names>U</given-names></name>
<etal/>
</person-group><fpage>0</fpage><volume>15</volume><year>2023</year><uri xlink:href="https://doi.org/10.7759/cureus.42924">https://doi.org/10.7759/cureus.42924</uri></element-citation></ref><ref id="REF17"><label>17</label><element-citation publication-type="journal"><article-title>The performance of GPT-3.5, GPT-4, and Bard on the Japanese National Dentist Examination: a comparison study</article-title><source>Cureus</source><person-group>
<name><surname>Ohta</surname><given-names>K</given-names></name>
<name><surname>Ohta</surname><given-names>S</given-names></name>
</person-group><fpage>0</fpage><volume>15</volume><year>2023</year><uri xlink:href="https://doi.org/10.7759/cureus.50369">https://doi.org/10.7759/cureus.50369</uri></element-citation></ref><ref id="REF18"><label>18</label><element-citation publication-type="journal"><article-title>Artificial intelligence's performance on the Japanese National Dental Examination</article-title><source>Cureus</source><person-group>
<name><surname>Akitomo</surname><given-names>T</given-names></name>
<name><surname>Hamada</surname><given-names>M</given-names></name>
<name><surname>Tsuge</surname><given-names>Y</given-names></name>
<etal/>
</person-group><fpage>0</fpage><volume>16</volume><year>2024</year></element-citation></ref><ref id="REF19"><label>19</label><element-citation publication-type="journal"><article-title>Performance of ChatGPT 4.0 on Japan's National Physical Therapist Examination: a comprehensive analysis of text and visual question handling</article-title><source>Cureus</source><person-group>
<name><surname>Sawamura</surname><given-names>S</given-names></name>
<name><surname>Kohiyama</surname><given-names>K</given-names></name>
<name><surname>Takenaka</surname><given-names>T</given-names></name>
<name><surname>Sera</surname><given-names>T</given-names></name>
<name><surname>Inoue</surname><given-names>T</given-names></name>
<name><surname>Nagai</surname><given-names>T</given-names></name>
</person-group><fpage>0</fpage><volume>16</volume><year>2024</year><uri xlink:href="https://doi.org/10.7759/cureus.67347">https://doi.org/10.7759/cureus.67347</uri></element-citation></ref><ref id="REF20"><label>20</label><element-citation publication-type="journal"><article-title>Potential of ChatGPT to pass the Japanese Medical and Healthcare Professional National Licenses: a literature review</article-title><source>Cureus</source><person-group>
<name><surname>Ishida</surname><given-names>K</given-names></name>
<name><surname>Hanada</surname><given-names>E</given-names></name>
</person-group><fpage>0</fpage><volume>16</volume><year>2024</year><uri xlink:href="https://doi.org/10.7759/cureus.66324">https://doi.org/10.7759/cureus.66324</uri></element-citation></ref><ref id="REF21"><label>21</label><element-citation publication-type="journal"><article-title>Analysis of responses of GPT-4&#x000a0;V to the Japanese National Clinical Engineer Licensing Examination</article-title><source>J Med Syst</source><person-group>
<name><surname>Ishida</surname><given-names>K</given-names></name>
<name><surname>Arisaka</surname><given-names>N</given-names></name>
<name><surname>Fujii</surname><given-names>K</given-names></name>
</person-group><fpage>83</fpage><volume>48</volume><year>2024</year><uri xlink:href="https://doi.org/10.1007/s10916-024-02103-w">https://doi.org/10.1007/s10916-024-02103-w</uri><pub-id pub-id-type="pmid">39259341</pub-id>
</element-citation></ref><ref id="REF22"><label>22</label><element-citation publication-type="journal"><article-title>Developing good multiple-choice tests and test questions</article-title><source>J Geosci Educ</source><person-group>
<name><surname>Fuhrman</surname><given-names>M</given-names></name>
</person-group><fpage>379</fpage><lpage>384</lpage><volume>44</volume><year>1996</year></element-citation></ref><ref id="REF23"><label>23</label><element-citation publication-type="journal"><article-title>Assessment of higher order cognitive skills in undergraduate education: modified essay or multiple choice questions? Research paper</article-title><source>BMC Med Educ</source><person-group>
<name><surname>Palmer</surname><given-names>EJ</given-names></name>
<name><surname>Devitt</surname><given-names>PG</given-names></name>
</person-group><fpage>49</fpage><volume>7</volume><year>2007</year><pub-id pub-id-type="pmid">18045500</pub-id>
</element-citation></ref><ref id="REF24"><label>24</label><element-citation publication-type="journal"><article-title>A prompt pattern catalog to enhance prompt engineering with ChatGPT</article-title><source>arXiv</source><person-group>
<name><surname>White</surname><given-names>J</given-names></name>
<name><surname>Fu</surname><given-names>Q</given-names></name>
<name><surname>Hays</surname><given-names>S</given-names></name>
<etal/>
</person-group><fpage>11382</fpage><volume>2302</volume><year>2023</year><uri xlink:href="https://doi.org/10.48550/arXiv.2302.11382">https://doi.org/10.48550/arXiv.2302.11382</uri></element-citation></ref><ref id="REF25"><label>25</label><element-citation publication-type="journal"><source>arXiv</source><person-group>
<name><surname>Open AI: GPT-4 Technical</surname><given-names>Report</given-names></name>
</person-group><fpage>8774</fpage><volume>2303</volume><year>2024</year><uri xlink:href="https://doi.org/10.48550/arXiv.2303.08774">https://doi.org/10.48550/arXiv.2303.08774</uri></element-citation></ref></ref-list></back></article>