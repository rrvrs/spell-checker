<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006221</article-id><article-id pub-id-type="pmc">PMC11859799</article-id><article-id pub-id-type="doi">10.3390/s25040992</article-id><article-id pub-id-type="publisher-id">sensors-25-00992</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Dynamic Obstacle Avoidance with Enhanced Social Force Model and DWA Algorithm Using SparkLink</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yi</surname><given-names>Hang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-00992" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Lin</surname><given-names>Ruliang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-00992" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Hao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-00992" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Yifang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="af1-sensors-25-00992" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Ying</surname><given-names>Cunchao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-00992" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4908-1361</contrib-id><name><surname>Wang</surname><given-names>Dong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-00992" ref-type="aff">2</xref><xref rid="c1-sensors-25-00992" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5790-0478</contrib-id><name><surname>Feng</surname><given-names>Lihang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af3-sensors-25-00992" ref-type="aff">3</xref><xref rid="c1-sensors-25-00992" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Villanueva</surname><given-names>Guillermo</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Kandris</surname><given-names>Dionisis</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-00992"><label>1</label>Beijing Aerospace Wanyuan Science and Technology Company Ltd., Beijing 100083, China; <email>yihangcalt@163.com</email> (H.Y.); <email>linruliang2024@163.com</email> (R.L.); <email>omarwong@163.com</email> (H.W.); <email>wyf_785412@163.com</email> (Y.W.)</aff><aff id="af2-sensors-25-00992"><label>2</label>School of Instrument Science and Engineering, Southeast University, Nanjing 210096, China; <email>cunchaoying1994@163.com</email></aff><aff id="af3-sensors-25-00992"><label>3</label>College of Electrical Engineering and Control Science, Nanjing Tech University, Nanjing 210096, China</aff><author-notes><corresp id="c1-sensors-25-00992"><label>*</label>Correspondence: <email>kingeast16@seu.edu.cn</email> (D.W.); <email>lfeng8@njtech.edu.cn</email> (L.F.)</corresp></author-notes><pub-date pub-type="epub"><day>07</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>992</elocation-id><history><date date-type="received"><day>03</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>06</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>03</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>In the context of Industry 4.0, addressing the challenge of dynamic obstacle avoidance for Automated Guided Vehicles (AGVs) in complex industrial environments, this paper proposes an algorithm that integrates an enhanced social force model (SFM) and an improved dynamic window approach (DWA), leveraging SparkLink communication technology to enhance data transmission speed and reliability. The introduction of SparkLink technology significantly improves the environmental perception capabilities of AGVs, optimizing their dynamic obstacle-avoidance performance. Experimental results demonstrate that this method effectively increases the efficiency of AGVs in dynamic obstacle avoidance, offering significant practical value.</p></abstract><kwd-group><kwd>SparkLink</kwd><kwd>AGV</kwd><kwd>dynamic obstacle avoidance</kwd><kwd>social force model</kwd><kwd>dynamic window method</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62103184</award-id></award-group><award-group><funding-source>Southeast University</funding-source></award-group><funding-statement>This research was funded by National Natural Science Foundation of China grant number 62103184. And The APC was funded by Southeast University.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-00992"><title>1. Introduction</title><p>In the era of Industry 4.0, the development of efficient material transport systems for intelligent production and large-part assembly is crucial as they can significantly enhance logistics management efficiency and the overall manufacturing process [<xref rid="B1-sensors-25-00992" ref-type="bibr">1</xref>]. As shown in <xref rid="sensors-25-00992-f001" ref-type="fig">Figure 1</xref>, Automated Guided Vehicles (AGVs) navigate along pre-set paths within warehouses, automating the logistics and thereby boosting manufacturing efficiency [<xref rid="B2-sensors-25-00992" ref-type="bibr">2</xref>].</p><p>However, one of the primary challenges faced by AGVs during material transport is dynamic obstacle avoidance, which directly impacts both path planning efficiency and the overall fluidity of logistics systems [<xref rid="B3-sensors-25-00992" ref-type="bibr">3</xref>]. Effective obstacle-avoidance algorithms are essential to ensure that AGVs can respond promptly to obstacles, avoid collisions, and optimize energy consumption and operational efficiency. Therefore, the development and optimization of these algorithms are vital for improving the practical utility and competitiveness of AGVs, making them a key technology for realizing smart logistics and manufacturing [<xref rid="B3-sensors-25-00992" ref-type="bibr">3</xref>].</p><p>In the field of dynamic obstacle avoidance, extensive research has been conducted, and various methods have been proposed to address this challenge. For example, Zhang [<xref rid="B4-sensors-25-00992" ref-type="bibr">4</xref>] improved the Artificial Potential Field (APF) method by modifying the repulsive force range from a circular to an elliptical shape, adjusting it based on obstacle velocity. On the other hand, Model Predictive Control (MPC) can predict obstacle trajectories, and Olcay [<xref rid="B5-sensors-25-00992" ref-type="bibr">5</xref>] combined it with Gaussian Process (GP) algorithms to estimate obstacle positions and generate avoidance trajectories. Zhu [<xref rid="B6-sensors-25-00992" ref-type="bibr">6</xref>] integrated the APF method with MPC, where high-level MPC adjusted the APF avoidance path to generate a smooth trajectory, and low-level MPC followed this path for precise tracking.</p><p>In the case of dynamic window approach (DWA)-based avoidance, Hossain [<xref rid="B7-sensors-25-00992" ref-type="bibr">7</xref>] enhanced the Gap-Following Method (GFM) and DWA by selecting more suitable gaps based on the size of the traversable space and the angle between the robot and its target. Additionally, the objective function of the DWA was improved to consider the benefits of passing through feasible gaps. Cheng [<xref rid="B8-sensors-25-00992" ref-type="bibr">8</xref>] combined an improved A* algorithm with the DWA, modifying A* by checking the alignment of the current node, its parent node, and the grandparent node to reduce unnecessary turns and prune unnecessary expanded sub-nodes for global path planning, avoiding static obstacles, and then using the DWA to avoid dynamic obstacles during motion.</p><p>In terms of using deep learning for dynamic obstacle avoidance, He [<xref rid="B9-sensors-25-00992" ref-type="bibr">9</xref>] combined the Deep Deterministic Policy Gradient (DDPG) with the APF method, where the APF was used to pre-plan a general avoidance direction. The DDPG network then adjusted the repulsive force parameters of the APF to autonomously learn and optimize the avoidance strategy. Similarly, Mul&#x000e1;s-Tejeda [<xref rid="B10-sensors-25-00992" ref-type="bibr">10</xref>] employed Long Short-Term Memory (LSTM) networks by placing multiple cameras around the robot to gather location data, along with the robot&#x02019;s linear velocity, angular velocity, and distance to obstacles, which were all used for avoidance training of the LSTM network.</p><p>Other researchers have proposed alternative methods for dynamic obstacle avoidance. For instance, Yu [<xref rid="B11-sensors-25-00992" ref-type="bibr">11</xref>] introduced a combined learning approach based on Sequential Neural Control Barrier Functions (SN-CBFs), where an SN-CBF model was designed to learn the time-series states of each dynamic obstacle, inferring safe control actions. By aggregating SN-CBF values, a unified value landscape was formed, from which optimal control actions for avoiding all obstacles were computed.</p><p>While current avoidance algorithms show excellent performance in their respective experimental settings, they generally overlook human-controlled dynamic obstacles that exhibit autonomous avoidance behaviors. Ignoring these behaviors can lead to suboptimal material transport, reducing overall production efficiency. Moreover, obstacle trajectory prediction is predominantly reliant on deep learning methods, which can produce unpredictable behaviors when faced with previously unseen scenarios during training. Additionally, factors such as the complexity of the working environment and network conditions (e.g., Wi-Fi latency and interference) can significantly affect the accuracy and timeliness of AGV data reception, further complicating real-time obstacle avoidance.</p><p>To address these challenges, this paper proposes an improved dynamic obstacle-avoidance algorithm that combines an enhanced Social Force Model (SFM) with the DWA and utilizes SparkLink communication technology. This algorithm incorporates human-controlled dynamic obstacles into the SFM, considering their avoidance behavior and integrating their predicted trajectories into the DWA objective function for more efficient avoidance. Furthermore, the SparkLink communication technology, known for its superior anti-interference capabilities and faster transmission speeds [<xref rid="B12-sensors-25-00992" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-00992" ref-type="bibr">13</xref>], enhances the algorithm&#x02019;s performance in practical applications.</p></sec><sec id="sec2-sensors-25-00992"><title>2. Environment Perception and Real-Time Data Transmission Based on SparkLink Technology</title><p>In the era of smart factories, decentralized control of AGV fleets offers enhanced flexibility and stability, enabling optimal resource utilization during varying production phases. Building on this control approach, this paper proposes an improvement to traditional communication methods by replacing the Wi-Fi module with a SparkLink module, as shown in <xref rid="sensors-25-00992-f002" ref-type="fig">Figure 2</xref>. AGVs need to continuously upload critical data such as position, speed, and remaining battery to the server to ensure smooth system operation. However, Wi-Fi limitations, including signal interference and latency, can lead to packet loss, increasing system unpredictability. In contrast, SparkLink technology offers obvious advantages in data transmission speed and reliability, enhancing the environmental perception capabilities of mobile robots.</p><p>The following sections describe the wireless communication architecture of SparkLink and its advantages in terms of environmental perception optimization in detail and compare and analyze the performance of the traditional wireless transmission module and the SparkLink module.</p><sec id="sec2dot1-sensors-25-00992"><title>2.1. Environmental Perception Mechanism Based on SparkLink Technology</title><p>The wireless communication architecture [<xref rid="B14-sensors-25-00992" ref-type="bibr">14</xref>] of SparkLink is shown in <xref rid="sensors-25-00992-f003" ref-type="fig">Figure 3</xref>, which provides two interfaces: SparkLink Low Energy (SLE) and SparkLink Basic (SLB). The SLE [<xref rid="B12-sensors-25-00992" ref-type="bibr">12</xref>] is comparable to Bluetooth and is designed for low power consumption, low latency, and high reliability for applications such as wireless headsets and industrial data acquisition. The SLB [<xref rid="B12-sensors-25-00992" ref-type="bibr">12</xref>], which is similar to Wi-Fi, is designed for smart devices and industrial machinery control and features low latency, high reliability, precise synchronization, and high concurrency.</p><p>In the realm of environmental perception systems, the ranging algorithm of SparkLink [<xref rid="B15-sensors-25-00992" ref-type="bibr">15</xref>] has successfully overcome the precision challenges of traditional wireless technologies in complex environments. This advancement has enhanced the positioning accuracy from the meter to the decimeter level, which facilitates the exact determination of a mobile robot&#x02019;s coordinates within a given scenario. When combined with the distance algorithms of a visual sensing system, this improvement allows for an even more precise identification of the current position of moving obstacles. Following this, an enhanced SFM algorithm can be employed to more accurately predict the position and velocity of moving obstacles at the next moment. Ultimately, the implementation of an improved DWA algorithm enables more precise dynamic obstacle avoidance, reducing the number of necessary stops and thereby increasing the efficiency of material transportation.</p><p>In the aspect of device communication, mobile robots can synchronize and transmit current battery level information to the SparkLink transmission module in a timely and accurate manner through the SLE interface. Subsequently, via the SLB interface, real-time data of the mobile robot&#x02019;s speed, position, and remaining battery level are provided to the computing center.</p></sec><sec id="sec2dot2-sensors-25-00992"><title>2.2. Comparative Analysis of Conventional Wireless Transmission and SparkLink Modules</title><p>The relevant properties [<xref rid="B12-sensors-25-00992" ref-type="bibr">12</xref>,<xref rid="B16-sensors-25-00992" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-00992" ref-type="bibr">17</xref>] required in industry for the conventional wireless transmission modules and SparkLink modules are shown in <xref rid="sensors-25-00992-t001" ref-type="table">Table 1</xref>. SparkLink has significant advantages for industrial applications compared to Bluetooth and Wi-Fi 6. Its ultra-low latency of 20 &#x000b5;s (in SLB mode) outperforms both Wi-Fi 6 (1&#x02013;5 ms) and Bluetooth (10 ms), making it ideal for real-time control in automation. SparkLink also offers decimeter-level positioning accuracy, which is more precise than Wi-Fi 6&#x02019;s 5&#x02013;10-m accuracy, benefiting tasks like asset tracking. Additionally, with a transmission speed of 900 Mbps (SLB mode) and power consumption of less than 2 mA, SparkLink provides a balanced solution for high-volume data transmission with low power usage, crucial for Industrial Internet of Things (IIoT) environments.</p></sec></sec><sec id="sec3-sensors-25-00992"><title>3. Adaptive Dynamic Obstacle-Avoidance Strategy Based on the SFM and the DWA</title><p>As depicted in <xref rid="sensors-25-00992-f004" ref-type="fig">Figure 4</xref>, when individuals encounter other pedestrians or moving obstacles (such as motorcycles) during a walk, they first analyze the movement trends of these obstacles and then adjust their walking state based on their trajectory to preemptively avoid the positions that the obstacles are likely to pass through. Inspired by this observation, this paper proposes an obstacle-avoidance strategy that mimics human behavior when encountering dynamic obstacles: by predicting the behavior of moving obstacles and reacting in advance, the obstacle-avoidance path is optimized, thereby improving the efficiency of dynamic obstacle avoidance and maximizing productivity.</p><sec id="sec3dot1-sensors-25-00992"><title>3.1. Pedestrian Trajectory Prediction</title><p>Pedestrian trajectory prediction is one of the key components of dynamic obstacle avoidance. Scholars worldwide have conducted extensive research in this field, proposing various pedestrian trajectory prediction models. Examples include models based on multi-environmental pedestrian behavior at crosswalks [<xref rid="B18-sensors-25-00992" ref-type="bibr">18</xref>]; stochastic prediction models based on Dynamic Bayesian Networks (DBNs) [<xref rid="B19-sensors-25-00992" ref-type="bibr">19</xref>]; prediction models based on Markov chains [<xref rid="B20-sensors-25-00992" ref-type="bibr">20</xref>]; and the SFM, which analyzes the micro-dynamic characteristics of pedestrians [<xref rid="B21-sensors-25-00992" ref-type="bibr">21</xref>].</p><p>Among these, the SFM, as a typical continuous pedestrian simulation model, has been widely applied in various fields, including traffic simulation, emergency evacuation simulation, and pedestrian trajectory prediction. The SFM transforms pedestrian movement behavior into mechanical interactions, using virtual force fields to simulate the interactions between pedestrians and between pedestrians and their environment. Specifically, pedestrians are influenced by the attraction to their goal, repulsion from others, and the repulsion from obstacles during their movement. Under the influence of the resultant force, they gradually adjust their motion state to ultimately reach their goal. The advantage of the SFM lies in its ability to accurately simulate pedestrian reactions and behaviors in dynamic environments.</p><p>To achieve more precise dynamic obstacle avoidance, this paper employs the SFM to predict the trajectories of pedestrians that a material-handling robot may encounter during its movement. The model assumes the following conditions: (1) pedestrians are solely influenced by external factors when moving or operating a vehicle, and there are no sudden avoidance behaviors triggered by internal causes (e.g., answering a phone); (2) the acceleration of pedestrians remains relatively constant within a given timeframe, without any abrupt changes in acceleration; (3) when pedestrians encounter obstacles, they actively take evasive actions. Building on this, the model&#x02019;s obstacle-avoidance capabilities are further enhanced by incorporating the influence of moving obstacles on pedestrian avoidance behavior as a new parameter, thereby improving the efficiency of traditional obstacle-avoidance algorithms.</p><sec id="sec3dot1dot1-sensors-25-00992"><title>3.1.1. The Traditional SFM</title><p>The SFM is based on the assumption that pedestrians are not only attracted to their goal but also experience repulsion from other pedestrians or obstacles. These forces collectively influence the pedestrian&#x02019;s movement trajectory. In the traditional SFM, the motion of each pedestrian is influenced by three main forces:
<list list-type="bullet"><list-item><p>Self-motivation force: this force propels pedestrians toward their goal, representing their movement intentions;</p></list-item><list-item><p>Interpersonal repulsion force: when two pedestrians come close to each other, a repulsive force is generated to prevent collisions;</p></list-item><list-item><p>Boundary force: when pedestrians approach environmental boundaries or obstacles, a repulsive force is generated, forcing pedestrians to maintain a certain distance.</p></list-item></list></p><p>The synthesis of these forces affects the pedestrian&#x02019;s movement trajectory, thereby predicting their future position and direction of travel, as shown in <xref rid="sensors-25-00992-f005" ref-type="fig">Figure 5</xref>.</p><p>The kinematic equation of a pedestrian subjected to these three forces [<xref rid="B21-sensors-25-00992" ref-type="bibr">21</xref>] is shown in Formula (1).<disp-formula id="FD1-sensors-25-00992"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo mathsize="140%">&#x02211;</mml:mo><mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo mathsize="140%">&#x02211;</mml:mo><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">iw</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the self-driven force applied to the <italic toggle="yes">i</italic>th pedestrian, which describes the subjective expectation of the movement of that pedestrian toward the destination; <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the mass of the <italic toggle="yes">i</italic>th pedestrian; <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the vector sum of the forces on pedestrian <italic toggle="yes">i</italic> for all pedestrians except pedestrian <italic toggle="yes">i</italic>; and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">iw</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the vector sum of the forces acting on pedestrian <italic toggle="yes">i</italic> by the boundary or obstacle.</p></sec><sec id="sec3dot1dot2-sensors-25-00992"><title>3.1.2. The Improved SFM</title><p>This study refines the SFM by integrating mobile robots as obstacles and modeling pedestrians&#x02019; active avoidance behaviors as additional parameters. This enhancement facilitates a more detailed representation of pedestrian movement intentions, essential for adapting the model to complex scenarios involving pedestrian&#x02013;environment and pedestrian&#x02013;pedestrian interactions. The updated model (as shown in Formula (2)) offers a more precise characterization of pedestrian behavior in diverse settings.<disp-formula id="FD2-sensors-25-00992"><label>(2)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo mathsize="140%">&#x02211;</mml:mo><mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munder><mml:mo mathsize="140%">&#x02211;</mml:mo><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">iw</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ip</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula>
where the term <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ip</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> represents the psychological driving force of the <italic toggle="yes">i</italic>th pedestrian. This characterizes the proactive avoidance behavior exhibited by pedestrians upon hearing obstacle-avoidance commands. In practical scenarios, the closer a pedestrian is to the robot, the more pronounced their active avoidance behavior becomes.</p><p>A pedestrian is motivated to travel at a certain speed along the shortest path if undisturbed. In addition, if the pedestrian is disturbed by other pedestrians or the surrounding environment, their walking direction will change, and then the pedestrian will gradually return to the original walking direction under the action of the driving force. The expression for the driving force <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B22-sensors-25-00992" ref-type="bibr">22</xref>] is shown in Formula (3):<disp-formula id="FD3-sensors-25-00992"><label>(3)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mrow><mml:mtext>-</mml:mtext><mml:mi>v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the desired velocity scalar of pedestrian <italic toggle="yes">i</italic>; <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the unit direction vector representing the desired movement direction; <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the actual velocity of pedestrian i at moment <italic toggle="yes">t</italic>; and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the reaction time of the pedestrian to the change in acceleration, which is the relaxation time when pedestrian <italic toggle="yes">i</italic> adjusts the current speed to the desired speed. In order to determine the ideal walking direction of the pedestrian, the pedestrian in front of the mobile robot can be observed using the various on-board sensors of the mobile robot, and then the line connecting the starting and ending points of the pedestrian walking during this observation time &#x00394;<italic toggle="yes">T</italic> can be used as this direction, as shown in <xref rid="sensors-25-00992-f006" ref-type="fig">Figure 6</xref>.</p><p><inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> in Formula (2) is the vector sum of the forces on pedestrian <italic toggle="yes">i</italic> for all pedestrians except pedestrian <italic toggle="yes">i</italic>. This force is used to describe the current tendency of a pedestrian to avoid walking with another person when they are approaching. The expression of <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B22-sensors-25-00992" ref-type="bibr">22</xref>] is shown in Formula (4):<disp-formula id="FD4-sensors-25-00992"><label>(4)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mo>(</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mtext>-</mml:mtext><mml:mi>d</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are force strength and action range constants, respectively; <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> the radius of pedestrian <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>, respectively. <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the distance between the centroids of pedestrians <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>. Additionally, <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is a unit vector indicating the direction of the force exerted from pedestrian <italic toggle="yes">j</italic> to pedestrian <italic toggle="yes">i</italic>.</p><p>The <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>iw</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B22-sensors-25-00992" ref-type="bibr">22</xref>] in Formula (2) is the vector sum of the forces acting on pedestrian <italic toggle="yes">i</italic> by the boundary or obstacle. Similar to the pedestrian-to-pedestrian force, this force describes the tendency of pedestrians to exhibit a certain distance from a fixed obstacle, such as a control boundary or a post, when they are in close proximity to it during walking. Since the psychological driving force represents the intention of pedestrians to avoid mobile robots, it is necessary to refine the traditional SFM by eliminating the complex maneuvering associated with pedestrians navigating around obstacles. This modification not only simplifies the model but also accelerates the computational speed of the program. The revised formula is shown in Formula (5):<disp-formula id="FD5-sensors-25-00992"><label>(5)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">iw</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mo>(</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mtext>-</mml:mtext><mml:mi>d</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="italic">iw</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">iw</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the force strength and action range constants, respectively; <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the radius of pedestrian <italic toggle="yes">i</italic>; <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">iw</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is a unit vector indicating the direction of the force pointing from the boundary or obstacle to pedestrian <italic toggle="yes">i</italic>, taken to be perpendicular to the boundary or obstacle direction; and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">iw</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the distance between the pedestrian and the boundary.</p><p>Since the introduction of the SFM by Dirk Helbing and Peter Molnar, many scholars have proposed a series of variants of the social force model based on different application scenarios [<xref rid="B23-sensors-25-00992" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-00992" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-00992" ref-type="bibr">25</xref>]. In order to apply the SFM to the scenario studied in this paper, an additional psychological driver is introduced on top of the above three forces. If a material-transport robot encounters a pedestrian ahead while moving, it typically issues a voice alert to prompt the pedestrian to move out of the way. Consequently, the psychological driving force can be employed in practical scenarios to account for the pedestrian&#x02019;s evasive maneuvers not considered in Formula (5), particularly when pedestrians are required to navigate around corners. This driving force is represented by Formula (6):<disp-formula id="FD6-sensors-25-00992"><label>(6)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ip</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mo>(</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mtext>-</mml:mtext><mml:mi>d</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ip</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ip</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the intensity and range coefficients of the pedestrian mental drive, respectively; <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the distance between the pedestrian and the robot; <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the direction of the pedestrian velocity change; and <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ip</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#x020d1;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the unit vector indicating the direction of the pedestrian velocity change. After calculating the combined force applied to a single pedestrian, the combined force can be directly used as acceleration without considering the mass of the pedestrian, and then its position can be recursively projected according to Formula (7) based on the time step for the purpose of predicting the trajectory of the pedestrian in the future period. In Formula (7), <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the positions of pedestrians at time <italic toggle="yes">t</italic> and <italic toggle="yes">t</italic> + 1; <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the speed of pedestrians at time <italic toggle="yes">t</italic> and <italic toggle="yes">t</italic> + 1; and <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the acceleration at time <italic toggle="yes">t</italic>.<disp-formula id="FD7-sensors-25-00992"><label>(7)</label><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo></mml:mrow><mml:mn>0.5</mml:mn><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec3dot2-sensors-25-00992"><title>3.2. Adaptive Obstacle-Avoidance Strategy</title><p>The proposed adaptive obstacle-avoidance strategy incorporates an enhanced SFM with the DWA algorithm, adjusting dynamically to environmental settings. In narrow spaces, the AGV maintains safe distances using the SFM to preempt collisions. In spacious scenarios, the AGV integrates the improved DWA algorithm with trajectory predictions for optimized obstacle avoidance, ensuring efficient and smooth path navigation.</p><sec id="sec3dot2dot1-sensors-25-00992"><title>3.2.1. Narrow-Space Obstacle-Avoidance Strategy</title><p>The narrow-space obstacle-avoidance strategy of the AGV is shown in <xref rid="sensors-25-00992-f007" ref-type="fig">Figure 7</xref>.</p><p>When the AGV encounters a pedestrian in the narrow corridor of the factory, the traveler&#x02019;s walking position after a period of time can first be predicted by the SFM. After that, a danger radius r is determined, and the farthest position of the AGV walking can be obtained, as shown in <xref rid="sensors-25-00992-f007" ref-type="fig">Figure 7</xref>. If the current velocity of the AGV is <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the distance from the current position to the farthest position that can be walked to is D. Assuming that the mobile robot performs uniform variable speed linear motion, the acceleration of the AGV can be derived from the kinematic equation shown in Formula (8).<disp-formula id="FD8-sensors-25-00992"><label>(8)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mtext>&#x02212;</mml:mtext><mml:mi>r</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>&#x02206;</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>&#x003bd;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>&#x02206;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02206;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the predicted duration. By iterating according to this formula, the process is streamlined through continuous adjustments to the mobile robot&#x02019;s speed, which are based on the calculated acceleration. This ensures that the robot maintains a safe distance from the pedestrian while navigating through the environment. Specifically, the maximum distance <italic toggle="yes">D</italic> that the robot can travel within this predicted duration can be determined using the following Formula (9):<disp-formula id="FD9-sensors-25-00992"><label>(9)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo></mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the current coordinates of the mobile robot; and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> signifies the anticipated position of the pedestrian after the predicted interval. Additionally, the danger radius <italic toggle="yes">r</italic>&#x02014;which is the minimum distance the robot should maintain from the pedestrian to avoid collision&#x02014;is a preset value.</p></sec><sec id="sec3dot2dot2-sensors-25-00992"><title>3.2.2. Spacious-Environment Obstacle-Avoidance Strategy</title><p>In expansive settings, AGVs employ an enhanced DWA algorithm for dynamic obstacle avoidance, which is an improvement upon the algorithm proposed by Fox et al. [<xref rid="B26-sensors-25-00992" ref-type="bibr">26</xref>] based on the Curvature Velocity Method (CVM) [<xref rid="B27-sensors-25-00992" ref-type="bibr">27</xref>]. This approach converts positional constraints to velocity constraints, accounting for mechanical properties and environmental obstacles. Integrated with an improved SFM, AGVs predict obstacle trajectories and refine avoidance paths, ensuring swift responses to dynamic obstacles and efficient navigation.</p><list list-type="bullet"><list-item><p>The traditional DWA algorithm</p></list-item></list><p>In the conventional DWA algorithm, the kinematic model of the robot is first established as shown in <xref rid="sensors-25-00992-f008" ref-type="fig">Figure 8</xref> since the trajectory prediction is performed for all states within the window of the mobile robot.</p><p>In the figure, <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c9;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> are the linear and angular velocities of the mobile robot, respectively. Since the sampling period is extremely short, the curved motion trajectory of the robot can be approximated as a straight line. If the coordinates of the current robot are <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, the position at the next sampling moment can be expressed by Formula (10):<disp-formula id="FD10-sensors-25-00992"><label>(10)</label><mml:math id="mm53" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>y</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#x003c9;</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>According to the kinematic model of Formula (8), the trajectory of the robot after a predicted period of time can be obtained. The speed of the robot at the next moment is limited to a window considering the constraints of the robot itself and its surroundings. The window considers three factors. First, the maximum speed limit of the robot, as shown in Formula (11):<disp-formula id="FD11-sensors-25-00992"><label>(11)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="|" separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mn>0</mml:mn><mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mi>v</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow><mml:mn>0</mml:mn><mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mtext>}</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Second, considering the constraints of the driving force of the mobile robot, there is a maximum acceleration and deceleration limit. If the current linear velocity is <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the angular velocity is <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the velocity limit at the next moment is as in Formula (12):<disp-formula id="FD12-sensors-25-00992"><label>(12)</label><mml:math id="mm57" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="|" separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mtext>-</mml:mtext><mml:mi>a</mml:mi><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>v</mml:mi><mml:mo>&#x02264;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mtext>-</mml:mtext><mml:mi>a</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>&#x00394;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>&#x003c9;</mml:mi><mml:mo>&#x02264;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:msub><mml:mo>&#x00394;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mtext>}</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Third, considering the safety of the robot, in order to be able to stop before hitting an obstacle, the speed limit of the robot with the maximum deceleration is shown in Formula (13):<disp-formula id="FD13-sensors-25-00992"><label>(13)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="|" separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>v</mml:mi><mml:mo>&#x02264;</mml:mo></mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="italic">dist</mml:mi></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msqrt><mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>&#x003c9;</mml:mi><mml:mo>&#x02264;</mml:mo></mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="italic">dist</mml:mi></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:msqrt><mml:mrow><mml:mtext>}</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> in Formula (11) is the distance between the endpoint of the track corresponding to speed <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and the obstacle. To sum up, the final speed window of the mobile robot is the intersection of three sets, as shown in Formula (14):<disp-formula id="FD14-sensors-25-00992"><label>(14)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mtext>&#x02229;</mml:mtext><mml:mi>V</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mtext>&#x02229;</mml:mtext><mml:mi>V</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The final window <italic toggle="yes">V</italic> and the sampling trajectory formed by <italic toggle="yes">V</italic> are shown in <xref rid="sensors-25-00992-f009" ref-type="fig">Figure 9</xref>.</p><p>Among the obtained trajectories, the trajectories corresponding to each set of velocities are feasible. In order to obtain the optimal trajectory, each trajectory needs to be evaluated and the one with the highest score is selected as the optimal solution and executed. The evaluation function is shown in Formula (15):<disp-formula id="FD15-sensors-25-00992"><label>(15)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="italic">heading</mml:mi></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="italic">dist</mml:mi></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="italic">velocity</mml:mi><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where the function term <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> in Formula (13) is the orientation angle, which is used to evaluate the angular difference between the direction of the end of the redirected trajectory and the target point, as shown in <xref rid="sensors-25-00992-f010" ref-type="fig">Figure 10</xref>. This is expressed by the following Formula (16):<disp-formula id="FD16-sensors-25-00992"><label>(16)</label><mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">heading</mml:mi><mml:mo>=</mml:mo></mml:mrow><mml:mn>180</mml:mn><mml:mo>&#x000b0;</mml:mo><mml:mrow><mml:mtext>-</mml:mtext><mml:mo>&#x000a0;</mml:mo></mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The function <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is used to evaluate the distance between the end of the robot&#x02019;s current trajectory and the nearest obstacle. To prevent this term from being over-evaluated, it needs to be set to a constant for scoring paths without obstacles; therefore, the function term <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is used to evaluate the velocity corresponding to this trajectory, and this evaluation allows the mobile robot to approach the target point at a faster speed.</p><p>Since the above three evaluation terms have different meanings and units, they need to be normalized after calculating the three evaluation values with Formula (17):<disp-formula id="FD17-sensors-25-00992"><label>(17)</label><mml:math id="mm67" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">heading</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">normal</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="italic">heading</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mi mathvariant="italic">heading</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">dist</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">normal</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="italic">dist</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mi mathvariant="italic">dist</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">velocity</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">normal</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="italic">velocity</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mi mathvariant="italic">velocity</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The calculated three evaluation values are brought into Formula (15) to obtain the rating of the current state, and then the highest rated set <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is selected from all the states in the window as the state for the next moment, which is the overall process of the traditional dynamic window.</p><list list-type="bullet"><list-item><p>The improved DWA algorithm</p></list-item></list><p>The DWA obstacle-avoidance algorithm, widely used in mobile robotics, is significantly less efficient when dealing with dynamic obstacles. As shown in <xref rid="sensors-25-00992-f011" ref-type="fig">Figure 11</xref>, the conventional DWA predicts states and evaluates the distance between predicted trajectories and the pedestrian&#x02019;s current position, which is optimal for stationary pedestrians. However, with moving pedestrians, this approach can lead to suboptimal solutions due to local optimization, where the robot&#x02019;s movement aligns with the pedestrian&#x02019;s motion, neglecting global optimality.</p><p>To enhance obstacle anticipation and improve safety in tasks such as drug delivery, this paper introduces a novel evaluation term, <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, to the DWA algorithm. This term integrates the robot&#x02019;s trajectory endpoint and the pedestrian&#x02019;s predicted location from the SFM, as demonstrated in <xref rid="sensors-25-00992-f012" ref-type="fig">Figure 12</xref>. By incorporating <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, the algorithm can better predict the movement of dynamic obstacles, optimize avoidance paths, and ensure efficient navigation.</p><p>In <xref rid="sensors-25-00992-f012" ref-type="fig">Figure 12</xref>, <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the current velocity vector of the mobile robot; <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the velocity vector of the robot at the end of the trajectory prediction; and <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is the vector of the pedestrian&#x02019;s current position pointing to the predicted position. Therefore, <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> can be expressed by Formula (18):<disp-formula id="FD18-sensors-25-00992"><label>(18)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">predict</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="{" close="" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x02220;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo mathvariant="normal">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>&#x02220;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo mathvariant="normal">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>&#x0003c;</mml:mo><mml:mn>90</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>180</mml:mn><mml:mo>&#x000b0;</mml:mo><mml:mtext>-</mml:mtext><mml:mo>&#x02220;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo mathvariant="normal">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>&#x02220;</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext></mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo mathvariant="normal">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>&#x02265;</mml:mo><mml:mn>90</mml:mn><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>When the angle between the robot&#x02019;s current velocity direction and the pedestrian&#x02019;s movement direction is less than 90&#x000b0;, it indicates that the robot is moving in roughly the same direction as the pedestrian. In this case, the robot should prioritize trajectories that increase the distance between itself and the pedestrian to avoid trailing too closely, as shown in <xref rid="sensors-25-00992-f013" ref-type="fig">Figure 13</xref>a. Conversely, when the angle between the robot&#x02019;s velocity direction and the pedestrian&#x02019;s direction is greater than 90&#x000b0;, the robot is moving in the opposite direction from the pedestrian. In this scenario, the robot should select trajectories that reduce the angle and prevent a head-on collision by steering away from the pedestrian&#x02019;s path, as shown in <xref rid="sensors-25-00992-f013" ref-type="fig">Figure 13</xref>b.</p><p>Likewise, this scoring term needs to be normalized after it is calculated, as shown in Formula (19):<disp-formula id="FD19-sensors-25-00992"><label>(19)</label><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Finally, the robot&#x02019;s overall evaluation function is updated to include the prediction term, as shown in Formula (20):<disp-formula id="FD20-sensors-25-00992"><label>(20)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="italic">heading</mml:mi></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="italic">dist</mml:mi></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="italic">velocity</mml:mi></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b5;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi mathvariant="italic">predict</mml:mi></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the evaluation Function (20), the robot considers multiple factors: heading toward the goal, distance from obstacles, speed, and the predicted movement of pedestrians. The optimal path is selected by choosing the trajectory that scores the highest based on this comprehensive evaluation, ensuring safe and efficient navigation in dynamic environments.</p></sec></sec></sec><sec id="sec4-sensors-25-00992"><title>4. Simulation Verification and Result Analysis</title><sec id="sec4dot1-sensors-25-00992"><title>4.1. Simulation Setup</title><p>To assess the efficacy of the obstacle-avoidance algorithm presented in this paper, comparative tests were conducted in both narrow aisles and large warehouses under identical pedestrian scenarios. The experiments primarily involved parameters from three areas&#x02014;the mobile robot&#x02019;s specifications, the DWA algorithm settings, and the SFM parameters&#x02014;as detailed in <xref rid="sensors-25-00992-t002" ref-type="table">Table 2</xref>, <xref rid="sensors-25-00992-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-00992-t004" ref-type="table">Table 4</xref>.</p><p>In these simulation experiments, the AGV&#x02019;s kinematic data&#x02014;position, velocity, and angular velocity&#x02014;is gathered through an array of sensors. The AGV&#x02019;s position is ascertained using simulated GPS and LiDAR sensors, which provide real-time coordinate calculations. Velocity is determined via IMUs or wheel encoders: IMUs supply data on linear and angular velocities, while wheel encoders calculate the AGV&#x02019;s speed based on wheel rotation. In addition, angular velocity is predominantly sourced from IMUs. The control program regularly retrieves sensor data through the simulation platform&#x02019;s interfaces to refresh the AGV&#x02019;s motion status.</p><p>To simulate real-world communication conditions, data transmitted from the AGV to the control system pass through a network delay module. The delay configuration of this module is tailored according to the communication technology used. According to <xref rid="sensors-25-00992-t001" ref-type="table">Table 1</xref>, the latency of SparkLink communication technology is set at 20 nanoseconds, while the latency of Wi-Fi communication is set between 1 and 5 milliseconds. This approach allows for a more accurate replication of the AGV&#x02019;s performance under different communication technologies and provides a comparative analysis of data transmission between SparkLink and traditional Wi-Fi, ensuring that the research findings have practical significance.</p></sec><sec id="sec4dot2-sensors-25-00992"><title>4.2. Narrow-Corridor Simulation Results</title><p>As shown in <xref rid="sensors-25-00992-f014" ref-type="fig">Figure 14</xref>a, a comparison of the actual performance between the original DWA algorithm with Wi-Fi and the AGV&#x02019;s narrow-corridor obstacle-avoidance model under the same experimental conditions is presented.</p><p><xref rid="sensors-25-00992-f014" ref-type="fig">Figure 14</xref>b presents the distance&#x02013;time plot showing the relative distance between the robot and pedestrians during the experiment. In this scenario, both the robot and the pedestrian move in the same direction. The robot&#x02019;s initial speed is 0.7 m/s, while the pedestrian moves at 0.8 m/s. The initial distance between the robot and the pedestrian is 1.9 m, with a hazard radius of 1.5 m defined in Formula (8).</p><p>The black curve in <xref rid="sensors-25-00992-f014" ref-type="fig">Figure 14</xref>b represents the performance of the original DWA algorithm with Wi-Fi. Given the algorithm&#x02019;s lack of consideration for the intentions of pedestrians, by the termination of the experimental timeframe, the mobile robot&#x02019;s trajectory resulted in a sustained separation of approximately 3 m from the pedestrian. In contrast, the optimized algorithm predicts the pedestrian&#x02019;s future movement and adjusts the robot&#x02019;s path accordingly, reducing the distance between the robot and the pedestrian to approximately 1.2 m after stabilization. This improvement is attributed to the algorithm&#x02019;s ability to determine that the pedestrian will continue walking forward, allowing the robot to maintain a closer yet safe proximity.</p></sec><sec id="sec4dot3-sensors-25-00992"><title>4.3. Wide-Area Simulation Results</title><sec id="sec4dot3dot1-sensors-25-00992"><title>4.3.1. Comparison of Single-Pedestrian Obstacle-Avoidance Effect</title><p>This simulation examines the effectiveness of the improved algorithm in avoiding a single pedestrian in a wide hall environment. The pedestrian moves from point (250, 430) to (800, 430), as depicted in <xref rid="sensors-25-00992-f015" ref-type="fig">Figure 15</xref>. The dashed line in <xref rid="sensors-25-00992-f015" ref-type="fig">Figure 15</xref>a represents the pedestrian&#x02019;s path, while the black curve below indicates the real-time path planned by the original DWA algorithm with Wi-Fi. In contrast, the green curve above shows the path planned by the wide-area obstacle-avoidance model of the AGV.</p><p>As illustrated in <xref rid="sensors-25-00992-f015" ref-type="fig">Figure 15</xref>b, the mobile robot approaches the pedestrian at point P1 and enters the pedestrian&#x02019;s influence range. As the pedestrian continues to move, the robot, following the original DWA with Wi-Fi evaluation function, selects an optimal trajectory. However, since the original algorithm with Wi-Fi cannot predict the pedestrian&#x02019;s next position, the robot continues to avoid the pedestrian&#x02019;s walking path after point P2, as shown in <xref rid="sensors-25-00992-f015" ref-type="fig">Figure 15</xref>c. Eventually, the robot halts at position P4 (where the window size is 0) and does not resume its movement toward the target point until the pedestrian exits its influence range. During the entire process, the DWA evaluation function was executed 220 times, with a total running time of 11.35 s.</p><p>In contrast, the improved algorithm anticipates the pedestrian&#x02019;s future trajectory, allowing the robot to adjust its path earlier to avoid the predicted direction of pedestrian movement. Once the robot passes point P3 in <xref rid="sensors-25-00992-f015" ref-type="fig">Figure 15</xref>c, it is no longer affected by the pedestrian, which significantly reduces the interaction time and enhances the safety of the robot&#x02019;s obstacle avoidance. The improved algorithm completed the obstacle-avoidance process in 160 iterations, with a total running time of 8.79 s. The comparison of the two algorithms is provided in <xref rid="sensors-25-00992-t005" ref-type="table">Table 5</xref>.</p></sec><sec id="sec4dot3dot2-sensors-25-00992"><title>4.3.2. Multiple-Pedestrian Obstacle-Avoidance Simulation</title><p>This simulation evaluates the obstacle-avoidance performance of the mobile robot when encountering multiple pedestrians in a spacious warehouse setting. As depicted in <xref rid="sensors-25-00992-f016" ref-type="fig">Figure 16</xref>, five pedestrians are involved in the simulation, with their movement directions indicated by black arrows. Pedestrians 1 and 4 decelerate when they encounter the robot, exhibiting avoidance behavior, while the remaining pedestrians continue walking without initiating avoidance.</p><p>As shown in <xref rid="sensors-25-00992-f016" ref-type="fig">Figure 16</xref>b, the original DWA algorithm with Wi-Fi approaches pedestrian 2 at point P1 after successfully passing pedestrians 1 and 4. However, due to the algorithm&#x02019;s inability to predict pedestrian movement, the robot&#x02019;s trajectory is not adjusted in time, leading to a conflict with pedestrian 3 at point P2, as illustrated in <xref rid="sensors-25-00992-f016" ref-type="fig">Figure 16</xref>c. The robot then slows down and comes to a stop before bypassing pedestrian 3. Throughout the obstacle-avoidance process, the original DWA algorithm with Wi-Fi executed 311 evaluations, with a total avoidance time of 15.5 s.</p><p>In contrast, the optimized DWA algorithm with SparkLink, as shown in <xref rid="sensors-25-00992-f016" ref-type="fig">Figure 16</xref>a, predicts pedestrian 2&#x02019;s downward movement at point P1, allowing the robot to proactively avoid pedestrian 2. This early prediction provides sufficient time for the robot to bypass pedestrian 3 at point P3 without stopping, as seen in <xref rid="sensors-25-00992-f016" ref-type="fig">Figure 16</xref>b. The robot later encounters pedestrian 5 at point P5 in <xref rid="sensors-25-00992-f016" ref-type="fig">Figure 16</xref>d and successfully maneuvers upward to reach the target destination. The improved DWA algorithm executed 225 evaluations during the entire process, reducing the total obstacle-avoidance time to 11.2 s. The comparison between the two algorithms is summarized in <xref rid="sensors-25-00992-t006" ref-type="table">Table 6</xref>.</p><p>This comparison highlights the enhanced efficiency and reduced execution time of the optimized DWA algorithm, particularly in multi-pedestrian scenarios. The ability of the improved algorithm to predict pedestrian movement leads to more proactive and safer obstacle-avoidance strategies.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-00992"><title>5. Conclusions</title><p>This article introduces a method for dynamic obstacle avoidance using SparkLink communication, which combines an enhanced SFM and an improved DWA. This approach aims to enhance the capabilities of material-handling robots in industrial mass-production environments, reducing the risk of employee injuries or reduced production efficiency due to suboptimal obstacle avoidance caused by poor network conditions. By leveraging SparkLink positioning technology to obtain accurate information required for the improved SFM, mobile robots are able to score each subsequent state based on the evaluation function in the improved DWA algorithm, with the highest-scoring state dictating the motion trajectory. Experimental results indicate that the enhanced algorithm outperforms the original DWA algorithm equipped with Wi-Fi in terms of both obstacle avoidance and transportation efficiency, as evidenced by its exceptional performance in the simulation environments constructed for this purpose and validated through representative single- and multi-pedestrian scenarios.</p><p>The primary focus of this paper is to improve the original obstacle-avoidance algorithm for mobile robots based on the prediction of pedestrian walking intentions using SparkLink technology. However, in real-world scenarios, factors such as age, gender, and individual differences among pedestrians can affect the predictive accuracy of the SFM [<xref rid="B28-sensors-25-00992" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-00992" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-00992" ref-type="bibr">30</xref>]. Additionally, the simulation of SparkLink in this experiment mainly remains at the theoretical data stage, without specific testing under various network conditions. Although delay parameters were set to simulate the low latency and high stability of SparkLink based on the characteristics of the communication technology, a comprehensive experimental verification of SparkLink&#x02019;s performance under different network conditions was not conducted. Future work will involve more extensive testing of SparkLink communication technology under diverse network environments to assess its performance and reliability in practical applications.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, R.L., C.Y. and D.W.; Methodology, H.Y., R.L. and Y.W.; Software, H.W.; Formal analysis, H.W. and Y.W.; Investigation, H.W.; Resources, Y.W., C.Y., D.W. and L.F.; Writing&#x02014;original draft, H.Y., R.L., H.W., C.Y. and D.W.; Visualization, C.Y.; Supervision, C.Y. and D.W.; Project administration, H.Y. and L.F.; Funding acquisition, L.F. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author due to privacy and ethical restrictions.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Authors Hang Yi, Ruliang Lin, Hao Wang and Yifang Wang were employed by the company Beijing Aerospace Wanyuan Science and Technology Company Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-00992"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Design and Application of an Automatic Bolt Collection and Transfer Assembly Line Based on AGV</article-title><source>Railw. Eng.</source><year>2024</year><volume>64</volume><fpage>163</fpage><lpage>167</lpage></element-citation></ref><ref id="B2-sensors-25-00992"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Du</surname><given-names>Z.</given-names></name>
</person-group><article-title>Design and Implementation of an Intelligent AGV Logistics Sorting System</article-title><source>Master&#x02019;s Thesis</source><publisher-name>Yantai University</publisher-name><publisher-loc>Yantai, China</publisher-loc><year>2024</year></element-citation></ref><ref id="B3-sensors-25-00992"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>M.Z.</given-names></name>
<name><surname>Qian</surname><given-names>T.H.</given-names></name>
<name><surname>Zhang</surname><given-names>S.Z.</given-names></name>
<name><surname>Wang</surname><given-names>J.Q.</given-names></name>
</person-group><article-title>Obstacle Avoidance and Collaborative Path Planning Methods for Warehouse Logistics Robot Clusters</article-title><source>Mod. Electron. Tech.</source><year>2019</year><volume>42</volume><fpage>22</fpage></element-citation></ref><ref id="B4-sensors-25-00992"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Jia</surname><given-names>R.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Shao</surname><given-names>S.</given-names></name>
</person-group><article-title>Dynamic Obstacle Avoidance Path Planning of Unmanned Vehicle Based on Improved APF</article-title><source>Proceedings of the 2023 7th International Symposium on Computer Science and Intelligent Control, ISCSIC</source><conf-loc>Nanjing, China</conf-loc><conf-date>27&#x02013;29 October 2023</conf-date><fpage>135</fpage><lpage>140</lpage></element-citation></ref><ref id="B5-sensors-25-00992"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Olcay</surname><given-names>E.</given-names></name>
<name><surname>Mee&#x000df;</surname><given-names>H.</given-names></name>
<name><surname>Elger</surname><given-names>G.</given-names></name>
</person-group><article-title>Dynamic Obstacle Avoidance for UAVs using MPC and GP-Based Motion Forecast</article-title><source>Proceedings of the 2024 European Control Conference, ECC 2024</source><conf-loc>Stockholm, Sweden</conf-loc><conf-date>25&#x02013;28 June 2024</conf-date><fpage>1024</fpage><lpage>1031</lpage></element-citation></ref><ref id="B6-sensors-25-00992"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>T.</given-names></name>
<name><surname>Mao</surname><given-names>J.</given-names></name>
<name><surname>Han</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
</person-group><article-title>Real-Time Dynamic Obstacle Avoidance for Robot Manipulators Based on Cascaded Nonlinear MPC With Artificial Potential Field</article-title><source>IEEE Trans. Ind. Electron.</source><year>2024</year><volume>71</volume><fpage>7424</fpage><lpage>7434</lpage><pub-id pub-id-type="doi">10.1109/TIE.2023.3306405</pub-id></element-citation></ref><ref id="B7-sensors-25-00992"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hossain</surname><given-names>T.</given-names></name>
<name><surname>Habibullah</surname><given-names>H.</given-names></name>
<name><surname>Islam</surname><given-names>R.</given-names></name>
<name><surname>Padilla</surname><given-names>R.V.</given-names></name>
</person-group><article-title>Local path planning for autonomous mobile robots by integrating modified dynamic-window approach and improved follow the gap method</article-title><source>J. Field Robot.</source><year>2021</year><volume>39</volume><fpage>371</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1002/rob.22055</pub-id></element-citation></ref><ref id="B8-sensors-25-00992"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>C.</given-names></name>
<name><surname>Zhu</surname><given-names>H.</given-names></name>
<name><surname>Xie</surname><given-names>Y.</given-names></name>
</person-group><article-title>Dynamic Obstacle Avoidance Research for Mobile Robots Incorporating Improved A-Star Algorithm and DWA Algorithm</article-title><source>Proceedings of the 2023 3rd International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology, CEI 2023</source><conf-loc>Wuhan, China</conf-loc><conf-date>15&#x02013;17 December 2023</conf-date><fpage>896</fpage><lpage>900</lpage></element-citation></ref><ref id="B9-sensors-25-00992"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>X.</given-names></name>
<name><surname>Ling</surname><given-names>K.V.</given-names></name>
<name><surname>Guo</surname><given-names>Y.</given-names></name>
<name><surname>Su</surname><given-names>R.</given-names></name>
<name><surname>Han</surname><given-names>B.S.</given-names></name>
<name><surname>Wong</surname><given-names>H.Y.A.</given-names></name>
<name><surname>Yao</surname><given-names>J.</given-names></name>
</person-group><article-title>Obstacle Avoidance for Automated Guided Vehicles Based on Deep Reinforcement Learning</article-title><source>Proceedings of the IECON Proceedings (Industrial Electronics Conference)</source><conf-loc>Singapore</conf-loc><conf-date>16&#x02013;19 October 2023</conf-date></element-citation></ref><ref id="B10-sensors-25-00992"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mul&#x000e1;s-Tejeda</surname><given-names>E.</given-names></name>
<name><surname>G&#x000f3;mez-Espinosa</surname><given-names>A.</given-names></name>
<name><surname>Escobedo Cabello</surname><given-names>J.A.</given-names></name>
<name><surname>Cantoral-Ceballos</surname><given-names>J.A.</given-names></name>
<name><surname>Molina-Leal</surname><given-names>A.</given-names></name>
</person-group><article-title>Implementation of a Long Short-Term Memory Neural Network-Based Algorithm for Dynamic Obstacle Avoidance</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3004</elocation-id><pub-id pub-id-type="doi">10.3390/s24103004</pub-id><pub-id pub-id-type="pmid">38793861</pub-id>
</element-citation></ref><ref id="B11-sensors-25-00992"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>H.</given-names></name>
<name><surname>Hirayama</surname><given-names>C.</given-names></name>
<name><surname>Yu</surname><given-names>C.</given-names></name>
<name><surname>Herbert</surname><given-names>S.</given-names></name>
<name><surname>Gao</surname><given-names>S.</given-names></name>
</person-group><article-title>Sequential Neural Barriers for Scalable Dynamic Obstacle Avoidance</article-title><source>Proceedings of the IEEE International Conference on Intelligent Robots and Systems</source><conf-loc>Detroit, MI, USA</conf-loc><conf-date>1&#x02013;5 October 2023</conf-date><fpage>11241</fpage><lpage>11248</lpage></element-citation></ref><ref id="B12-sensors-25-00992"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ma</surname><given-names>Z.</given-names></name>
</person-group><article-title>Advantages, Applications, and Development Considerations of &#x02018;XingShan&#x02019; Technology</article-title><source>Telecommun. Enterp. Manag.</source><year>2023</year><volume>9</volume><fpage>74</fpage><lpage>76</lpage></element-citation></ref><ref id="B13-sensors-25-00992"><label>13.</label><element-citation publication-type="other"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
</person-group><article-title>XingShan Technology: Making the Internet of Everything Faster and More Stable</article-title><source>Science and Technology Daily</source><day>13</day><month>November</month><year>2023</year></element-citation></ref><ref id="B14-sensors-25-00992"><label>14.</label><element-citation publication-type="webpage"><article-title>SparkLink 1.0 Security White Paper&#x02014;Network Security&#x02014;International SparkLink Alliance</article-title><comment>Available online: <ext-link xlink:href="https://SparkLink.org.cn/news_info.php?id=638" ext-link-type="uri">https://SparkLink.org.cn/news_info.php?id=638</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-09-10">(accessed on 10 September 2024)</date-in-citation></element-citation></ref><ref id="B15-sensors-25-00992"><label>15.</label><element-citation publication-type="webpage"><article-title>Starburst Technologies|Hesse.com</article-title><comment>Available online: <ext-link xlink:href="https://www.hisilicon.com/admin/asset/v1/pro/view/3b745aaacc9c4726b96bd1ab53dc60c9.pdf/download" ext-link-type="uri">https://www.hisilicon.com/admin/asset/v1/pro/view/3b745aaacc9c4726b96bd1ab53dc60c9.pdf/download</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-09-10">(accessed on 10 September 2024)</date-in-citation></element-citation></ref><ref id="B16-sensors-25-00992"><label>16.</label><element-citation publication-type="webpage"><article-title>Reduced Latency Benefits of Wi-Fi 6 OFDMA|Wi-Fi Alliance</article-title><comment>Available online: <ext-link xlink:href="https://www.wi-fi.org/beacon/rolf-de-vegt/reduced-latency-benefits-of-wi-fi-6-ofdma" ext-link-type="uri">https://www.wi-fi.org/beacon/rolf-de-vegt/reduced-latency-benefits-of-wi-fi-6-ofdma</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-09-10">(accessed on 10 September 2024)</date-in-citation></element-citation></ref><ref id="B17-sensors-25-00992"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mikhaylov</surname><given-names>K.</given-names></name>
<name><surname>Plevritakis</surname><given-names>N.</given-names></name>
<name><surname>Tervonen</surname><given-names>J.</given-names></name>
</person-group><article-title>Performance Analysis and Comparison of Bluetooth Low Energy with IEEE 802.15.4 and SimpliciTI</article-title><source>J. Sens. Actuator Netw.</source><year>2013</year><volume>2</volume><fpage>589</fpage><lpage>613</lpage><pub-id pub-id-type="doi">10.3390/jsan2030589</pub-id></element-citation></ref><ref id="B18-sensors-25-00992"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bonnin</surname><given-names>S.</given-names></name>
<name><surname>Weisswange</surname><given-names>T.H.</given-names></name>
<name><surname>Kummert</surname><given-names>F.</given-names></name>
<name><surname>Schmuedderich</surname><given-names>J.</given-names></name>
</person-group><article-title>Pedestrian Crossing Prediction Using Multiple Context-based Models</article-title><source>Proceedings of the 17th International IEEE Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Qingdao, China</conf-loc><conf-date>8&#x02013;11 October 2014</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2014</year><fpage>378</fpage><lpage>385</lpage></element-citation></ref><ref id="B19-sensors-25-00992"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hashimoto</surname><given-names>Y.</given-names></name>
<name><surname>Gu</surname><given-names>Y.</given-names></name>
<name><surname>Hsu</surname><given-names>L.T.</given-names></name>
<name><surname>Kamijo</surname><given-names>S.</given-names></name>
</person-group><article-title>Probability Estimation for Pedestrian Crossing Intention at Signalized Crosswalks</article-title><source>Proceedings of the 2015 IEEE Internal Conference on Vehicular Electronics and Safety</source><conf-loc>Yokohama, Japan</conf-loc><conf-date>5&#x02013;7 November 2015</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway Township, NJ, USA</publisher-loc><year>2015</year><fpage>114</fpage><lpage>119</lpage></element-citation></ref><ref id="B20-sensors-25-00992"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Asahara</surname><given-names>A.</given-names></name>
<name><surname>Maruyama</surname><given-names>K.</given-names></name>
<name><surname>Sati</surname><given-names>A.</given-names></name>
<name><surname>Seto</surname><given-names>K.</given-names></name>
</person-group><article-title>Pedestrian-movement prediction based on mixed Markov-chain model</article-title><source>Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</source><conf-loc>Chicago, IL, USA</conf-loc><conf-date>1&#x02013;4 November 2011</conf-date><publisher-name>ACM</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2011</year><fpage>25</fpage><lpage>33</lpage></element-citation></ref><ref id="B21-sensors-25-00992"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Helbing</surname><given-names>D.</given-names></name>
<name><surname>Molnar</surname><given-names>P.</given-names></name>
</person-group><article-title>Social Force Model for Pedestrian Dynamics</article-title><source>Phys. Rev. E Stat. Phys. Plasmas Fluids Relat. Interdiscip. Top.</source><year>1995</year><volume>51</volume><fpage>4282</fpage><lpage>4286</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.51.4282</pub-id><pub-id pub-id-type="pmid">9963139</pub-id>
</element-citation></ref><ref id="B22-sensors-25-00992"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kang</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>K.</given-names></name>
</person-group><article-title>An improved social force model for pedestrian dynamics in shipwrecks</article-title><source>Appl. Math. Comput.</source><year>2018</year><volume>348</volume><fpage>355</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.amc.2018.12.001</pub-id></element-citation></ref><ref id="B23-sensors-25-00992"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gil</surname><given-names>&#x000d3;.</given-names></name>
<name><surname>Garrell</surname><given-names>A.</given-names></name>
<name><surname>Sanfeliu</surname><given-names>A.</given-names></name>
</person-group><article-title>Social Robot Navigation Tasks: Combining Machine Learning Techniques and Social Force Model</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>7087</elocation-id><pub-id pub-id-type="doi">10.3390/s21217087</pub-id><pub-id pub-id-type="pmid">34770395</pub-id>
</element-citation></ref><ref id="B24-sensors-25-00992"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shi</surname><given-names>X.</given-names></name>
<name><surname>Shao</surname><given-names>X.</given-names></name>
<name><surname>Guo</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>G.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
</person-group><article-title>Pedestrian Trajectory Prediction in Extremely Crowded Scenarios</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>1223</elocation-id><pub-id pub-id-type="doi">10.3390/s19051223</pub-id><pub-id pub-id-type="pmid">30862018</pub-id>
</element-citation></ref><ref id="B25-sensors-25-00992"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zeng</surname><given-names>W.L.</given-names></name>
<name><surname>Chen</surname><given-names>P.</given-names></name>
<name><surname>Yu</surname><given-names>G.Z.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Specification and Calibration of a Microscopic Model for Pedestrian Dynamic Simulation at Signalized Intersections: A Hybrid Approach</article-title><source>Transp. Res. Part C Emerg. Technol.</source><year>2017</year><volume>80</volume><fpage>37</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1016/j.trc.2017.04.009</pub-id></element-citation></ref><ref id="B26-sensors-25-00992"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Simmons</surname><given-names>R.</given-names></name>
</person-group><article-title>The curvature-velocity method for local obstacle avoidance</article-title><source>Proceedings of the IEEE International Conference on Robotics and Automation</source><conf-loc>Minneapolis, MN, USA</conf-loc><conf-date>22&#x02013;28 April 1996</conf-date><volume>Volume 4</volume><fpage>3375</fpage><lpage>3382</lpage></element-citation></ref><ref id="B27-sensors-25-00992"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>W.Y.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>H.</given-names></name>
<name><surname>Jin</surname><given-names>W.Q.</given-names></name>
</person-group><article-title>A social force-based prediction model for self-driving vehicle pedestrian trajectory</article-title><source>Highw. Traffic Technol.</source><year>2020</year><volume>37</volume><fpage>127</fpage><lpage>135</lpage></element-citation></ref><ref id="B28-sensors-25-00992"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Siddharth</surname><given-names>S.M.P.</given-names></name>
<name><surname>Vedagiri</surname><given-names>P.</given-names></name>
</person-group><article-title>Modeling the gender effects of pedestrians and calibration of the modified social force model</article-title><source>Transp. Res. Rec.</source><year>2018</year><volume>2672</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1177/0361198118758673</pub-id></element-citation></ref><ref id="B29-sensors-25-00992"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>W.</given-names></name>
<name><surname>Chen</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>B.</given-names></name>
<name><surname>Zheng</surname><given-names>X.</given-names></name>
</person-group><article-title>An Extended Social Force Model via Pedestrian Heterogeneity Affecting the Self-Driven Force</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>7974</fpage><lpage>7986</lpage><pub-id pub-id-type="doi">10.1109/TITS.2021.3074914</pub-id></element-citation></ref><ref id="B30-sensors-25-00992"><label>30.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Tytko</surname><given-names>K.</given-names></name>
<name><surname>Mamica</surname><given-names>M.</given-names></name>
<name><surname>P&#x00119;kala</surname><given-names>A.</given-names></name>
<name><surname>W&#x00105;s</surname><given-names>J.</given-names></name>
</person-group><article-title>Simulating Pedestrians&#x02019; Motion in Different Scenarios with Modified Social Force Model</article-title><source>Parallel Processing and Applied Mathematics</source><series>Lecture Notes in Computer Science (LNCS)</series><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><volume>Volume 12044</volume><fpage>467</fpage><lpage>477</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-00992-f001"><label>Figure 1</label><caption><p>Photo of AGVs in operation.</p></caption><graphic xlink:href="sensors-25-00992-g001" position="float"/></fig><fig position="float" id="sensors-25-00992-f002"><label>Figure 2</label><caption><p>(<bold>a</bold>) The traditional robot communication architecture; (<bold>b</bold>) SparkLink robot communication architecture.</p></caption><graphic xlink:href="sensors-25-00992-g002" position="float"/></fig><fig position="float" id="sensors-25-00992-f003"><label>Figure 3</label><caption><p>Wireless communication system architecture.</p></caption><graphic xlink:href="sensors-25-00992-g003" position="float"/></fig><fig position="float" id="sensors-25-00992-f004"><label>Figure 4</label><caption><p>Walking strategy of a person when encountering obstacles.</p></caption><graphic xlink:href="sensors-25-00992-g004" position="float"/></fig><fig position="float" id="sensors-25-00992-f005"><label>Figure 5</label><caption><p>Schematic diagram of the original SFM.</p></caption><graphic xlink:href="sensors-25-00992-g005" position="float"/></fig><fig position="float" id="sensors-25-00992-f006"><label>Figure 6</label><caption><p>Determination of the ideal direction of the pedestrian.</p></caption><graphic xlink:href="sensors-25-00992-g006" position="float"/></fig><fig position="float" id="sensors-25-00992-f007"><label>Figure 7</label><caption><p>Obstacle avoidance of material-transport robot in narrow corridor.</p></caption><graphic xlink:href="sensors-25-00992-g007" position="float"/></fig><fig position="float" id="sensors-25-00992-f008"><label>Figure 8</label><caption><p>Kinematic model of the mobile robot.</p></caption><graphic xlink:href="sensors-25-00992-g008" position="float"/></fig><fig position="float" id="sensors-25-00992-f009"><label>Figure 9</label><caption><p>Kinematic model of a typical mobile robot. (<bold>a</bold>) The mobile robot determines the final window; (<bold>b</bold>) The multiple trajectories sampled by the robot within the dynamic window.</p></caption><graphic xlink:href="sensors-25-00992-g009" position="float"/></fig><fig position="float" id="sensors-25-00992-f010"><label>Figure 10</label><caption><p>Schematic diagram of heading evaluation items.</p></caption><graphic xlink:href="sensors-25-00992-g010" position="float"/></fig><fig position="float" id="sensors-25-00992-f011"><label>Figure 11</label><caption><p>Schematic diagram of conventional DWA facing dynamic obstacle.</p></caption><graphic xlink:href="sensors-25-00992-g011" position="float"/></fig><fig position="float" id="sensors-25-00992-f012"><label>Figure 12</label><caption><p>Schematic diagram of the evaluation term: <italic toggle="yes">predict</italic>(<italic toggle="yes">v</italic>, <italic toggle="yes">&#x003c9;</italic>).</p></caption><graphic xlink:href="sensors-25-00992-g012" position="float"/></fig><fig position="float" id="sensors-25-00992-f013"><label>Figure 13</label><caption><p>Preferred trajectory of the mobile robot in two cases.(<bold>a</bold>) the angle between the robot&#x02019;s current velocity direction and the pedestrian&#x02019;s movement direction is less than 90&#x000b0;; (<bold>b</bold>) the angle between the robot&#x02019;s current velocity direction and the pedestrian&#x02019;s movement direction is more than 90&#x000b0;.</p></caption><graphic xlink:href="sensors-25-00992-g013" position="float"/></fig><fig position="float" id="sensors-25-00992-f014"><label>Figure 14</label><caption><p>Comparison of the effect of obstacle avoidance.(<bold>a</bold>) Comparison of Original and Improved DWA Algorithms in Narrow Scenarios; (<bold>b</bold>) Simulation Results for Narrow Scenarios.</p></caption><graphic xlink:href="sensors-25-00992-g014" position="float"/></fig><fig position="float" id="sensors-25-00992-f015"><label>Figure 15</label><caption><p>Comparison of the effect of single-pedestrian obstacle avoidance. (<bold>a</bold>) Overall obstacle-avoidance trajectories of the original DWA algorithm and the optimized DWA algorithm in the presence of a single pedestrian; (<bold>b</bold>) Initial phase of the simulation; (<bold>c</bold>) Mid-phase of the simulation; (<bold>d</bold>) The original DWA algorithm encounters a pedestrian, and the experiment stops.</p></caption><graphic xlink:href="sensors-25-00992-g015" position="float"/></fig><fig position="float" id="sensors-25-00992-f016"><label>Figure 16</label><caption><p>Comparison of the effect of multi-pedestrian obstacle avoidance.(<bold>a</bold>) Overall obstacle-avoidance trajectories of the original DWA algorithm and the optimized DWA algorithm in the presence of a single pedestrian; (<bold>b</bold>) Initial phase of the simulation; (<bold>c</bold>) Mid-phase of the simulation; the original DWA algorithm encounters a pedestrian and stops to wait.; (<bold>d</bold>) Late phase of the simulation.</p></caption><graphic xlink:href="sensors-25-00992-g016" position="float"/></fig><table-wrap position="float" id="sensors-25-00992-t001"><object-id pub-id-type="pii">sensors-25-00992-t001_Table 1</object-id><label>Table 1</label><caption><p>Performance comparison of traditional wireless transmission and SparkLink modules.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Transmission Speed</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Latency</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Positioning Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Power Consumption</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bluetooth</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>~</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Centimeter-level</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>~</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wi-Fi 6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>9.6</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>~</mml:mo><mml:mn>5</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>~</mml:mo><mml:mn>10</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SparkLink</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo>:</mml:mo><mml:mn>12</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
<break/>
<inline-formula>
<mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">B</mml:mi><mml:mo>:</mml:mo><mml:mn>900</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo>:</mml:mo><mml:mn>250</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="sans-serif">&#x003bc;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
<break/>
<inline-formula>
<mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">B</mml:mi><mml:mo>:</mml:mo><mml:mn>20</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="sans-serif">&#x003bc;</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Decimeter-level</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">A</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00992-t002"><object-id pub-id-type="pii">sensors-25-00992-t002_Table 2</object-id><label>Table 2</label><caption><p>Parameters of the mobile robot.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Unit</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Minimum Line Speed: V<sub>min</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">m/s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum Linear Speed: V<sub>max</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">m/s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum Linear Acceleration: a<sub>max</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">m/s<sup>2</sup></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum Angular Velocity: &#x003c9;<sub>max</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">deg/s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Minimum Angular Velocity: &#x003c9;<sub>min</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02212;60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">deg/s</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum Angular Acceleration: a<sub>&#x003c9;</sub></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">deg/s<sup>2</sup></td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00992-t003"><object-id pub-id-type="pii">sensors-25-00992-t003_Table 3</object-id><label>Table 3</label><caption><p>Parameters of the DWA algorithm.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Unit</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Related Formulas</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resolution of Time: &#x00394;t</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(10)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resolution of Linear Speed: &#x00394;v</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">m/s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(14)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Resolution of Angular Velocity: &#x00394;&#x003c9;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">deg/s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(14)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Time of Trajectory Prediction: test</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(8)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Maximum Obstacle Distance Threshold: dist</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weights for Azimuth Evaluation: &#x003b1;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(15) and (20)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weights for Obstacle Distance Evaluation: &#x003b2;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(15) and (20)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weights of Robot Speed Evaluation: &#x003b3;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(15) and (20)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weights for Predicting Pedestrian Evaluations</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(20)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00992-t004"><object-id pub-id-type="pii">sensors-25-00992-t004_Table 4</object-id><label>Table 4</label><caption><p>Parameters of SFM.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Name</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Unit</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Related Formulas</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Strength of Pedestrian Interaction Forces: Ai</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(4)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Constant of Pedestrian Interaction Range: Bi</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(4)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Strength of Obstacle Force: Aw</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(5)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Obstacle Force Range Constant: Bw</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(5)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Strength of Psychological Forces on Pedestrians: Ap</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(6)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pedestrian Psychological Force Range Constant: Bp</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02014;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(6)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00992-t005"><object-id pub-id-type="pii">sensors-25-00992-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of obstacle-avoidance results of experiment <xref rid="sec4dot3dot1-sensors-25-00992" ref-type="sec">Section 4.3.1</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Algorithm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Iterations</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Total Time (s)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Total Distance (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Original DWA algorithm with Wi-Fi</td><td align="center" valign="middle" rowspan="1" colspan="1">220</td><td align="center" valign="middle" rowspan="1" colspan="1">11.35</td><td align="center" valign="middle" rowspan="1" colspan="1">12.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improved DWA algorithm with SparkLink</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">160</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.79</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10.73</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00992-t006"><object-id pub-id-type="pii">sensors-25-00992-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of multi-pedestrian obstacle-avoidance results in the lobby.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Algorithm</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Iterations</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Total Time (s)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Total Distance (m)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Original DWA algorithm with Wi-Fi</td><td align="center" valign="middle" rowspan="1" colspan="1">311</td><td align="center" valign="middle" rowspan="1" colspan="1">15.5</td><td align="center" valign="middle" rowspan="1" colspan="1">12.3</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improved DWA algorithm with SparkLink</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">225</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.1</td></tr></tbody></table></table-wrap></floats-group></article>