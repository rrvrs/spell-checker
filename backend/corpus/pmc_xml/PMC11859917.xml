<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006246</article-id><article-id pub-id-type="pmc">PMC11859917</article-id><article-id pub-id-type="doi">10.3390/s25041017</article-id><article-id pub-id-type="publisher-id">sensors-25-01017</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Augmented Reality for Therapeutic Education in Patients with Diabetes: Short- and Mid-Term Learning Benefits</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Calle</surname><given-names>Marcelo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5896-8645</contrib-id><name><surname>Abad</surname><given-names>Francisco</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8764-1470</contrib-id><name><surname>Juan</surname><given-names>M.-Carmen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="c1-sensors-25-01017" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Troussas</surname><given-names>Christos</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Theodosios</surname><given-names>Sapounidis</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Keramopoulos</surname><given-names>Efkleidis</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Volioti</surname><given-names>Christina</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01017">Instituto Universitario de Autom&#x000e1;tica e Inform&#x000e1;tica Industrial, Universitat Polit&#x000e8;cnica de Val&#x000e8;ncia, C/Camino de Vera, s/n, 46022 Valencia, Spain; <email>ancalbus@upv.edu.es</email> (M.C.); <email>fjabad@dsic.upv.es</email> (F.A.)</aff><author-notes><corresp id="c1-sensors-25-01017"><label>*</label>Correspondence: <email>mcarmen@dsic.upv.es</email></corresp></author-notes><pub-date pub-type="epub"><day>08</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1017</elocation-id><history><date date-type="received"><day>16</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>03</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>06</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>This work aims to evaluate the effectiveness of knowledge transfer through an Augmented Reality (AR) application, assessing short- and mid-term retention in children and adults with Type 1 diabetes. One objective is to determine if the AR application helps patients learn about the carbohydrate content of different foods (N = 27 Ecuadorian patients). Another objective is to evaluate the usability and satisfaction perceived by the patients. An additional objective is to compare the data from our study in Ecuador with data from a similar study conducted with Spanish children (N = 42). The results show that the AR application is effective for short-term knowledge transfer (<italic toggle="yes">p</italic> &#x0003c; 0.001) and has a suggestively significant effect on mid-term retention (<italic toggle="yes">p</italic> &#x0003c; 0.05). The AR application had an equalizing effect on knowledge outcomes between the groups (Ecuador and Spain) despite initial differences. The AR application significantly increased patients&#x02019; knowledge (<italic toggle="yes">p</italic> &#x0003c; 0.001) and was effective for both children and adults. Patient satisfaction was high, and learning outcomes were not influenced by age or gender. The AR application is effective for short-term knowledge transfer and mid-term retention, benefiting children and adults regardless of gender. The patients&#x02019; experience was very positive. Therefore, the AR application is a valuable tool for therapeutic education in diabetes since it offers support that is easily accessible on mobile devices, enabling autonomous learning, and it contributes to the creation of innovative, patient-centered healthcare solutions.</p></abstract><kwd-group><kwd>augmented reality</kwd><kwd>therapeutic education</kwd><kwd>diabetes</kwd><kwd>carbohydrate choices</kwd><kwd>learning</kwd><kwd>short-term</kwd><kwd>mid-term</kwd><kwd>assessment</kwd><kwd>user experience</kwd><kwd>user performance</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01017"><title>1. Introduction</title><p>Diabetes mellitus, often called diabetes, is a chronic condition characterized by the body&#x02019;s inability to produce enough insulin or to use the insulin it produces effectively. Insulin, a hormone generated by the pancreas, facilitates the entry of glucose from food into the body&#x02019;s cells for storage or conversion into energy. Insufficient insulin leads to excessively high blood glucose levels (hyperglycemia), which can cause severe long-term macrovascular and microvascular complications. The three main types of diabetes are Type 1, Type 2, and gestational diabetes mellitus. In Type 1 diabetes, the pancreas produces little or no insulin. This is because the body&#x02019;s immune system attacks and destroys the beta cells responsible for insulin production. Type 1 diabetes primarily affects children and young adults, requiring continuous insulin therapy for survival. Type 2 diabetes develops when the body either does not produce enough insulin or becomes resistant to its effects. It primarily develops in adults, though its prevalence among adolescents is increasing. Treatment includes control of diet and exercise and oral antidiabetic medications; in advanced stages, insulin therapy may be required. Gestational diabetes mellitus happens when the body is unable to produce or properly use enough insulin during pregnancy.</p><p>According to the World Health Organization, approximately 422 million people worldwide are affected by diabetes, most of whom live in low- and middle-income countries. Diabetes is directly linked to 1.5 million deaths annually. According to the International Diabetes Federation (<uri xlink:href="https://diabetesatlas.org/data/en/">https://diabetesatlas.org/data/en/</uri> (accessed on 10 December 2024)), the global prevalence of diabetes is projected to increase to 643 million by 2030 and 783 million by 2045.</p><p>Therapeutic education in diabetes (TED) refers to a structured process of teaching people with diabetes about the nature of their condition, how to manage it, and how to integrate effective self-care practices into their daily lives. This education includes understanding diabetes, blood glucose monitoring, insulin administration, nutrition and diet, physical activity, prevention of complications, psychosocial support, and self-management skills. TED encourages patients to take responsibility for their disease management.</p><p>Proper nutritional management is critical in preventing insulin resistance, slowing its progression, and avoiding neurological and vascular complications resulting from poor glycemic control. Therefore, nutrition is a key component of educational programs for all diabetes patients. Nutritional guidelines emphasize regulating daily caloric intake and its distribution across meals, as well as balancing fats, proteins, and carbohydrates. Because carbohydrates directly affect blood glucose levels, understanding the appropriate carbohydrate intake for each meal is essential for effective disease management. This regulation helps achieve a balanced diet and maintain near-normal blood glucose levels.</p><p>The integration of assistive technology systems can enhance TED. Extended reality (encompassing Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR)) offers promising avenues to enhance TED. Notably, AR has been widely employed in educational applications (e.g., [<xref rid="B1-sensors-25-01017" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01017" ref-type="bibr">2</xref>]), where virtual elements are superimposed onto real-world images [<xref rid="B3-sensors-25-01017" ref-type="bibr">3</xref>] to enhance the learning experience. Previous works have demonstrated that AR can improve learning outcomes [<xref rid="B1-sensors-25-01017" ref-type="bibr">1</xref>]. In addition, numerous review publications, both general [<xref rid="B1-sensors-25-01017" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01017" ref-type="bibr">2</xref>] and specific, have highlighted the applications of AR in areas such as language learning [<xref rid="B4-sensors-25-01017" ref-type="bibr">4</xref>], mathematics [<xref rid="B5-sensors-25-01017" ref-type="bibr">5</xref>], chemistry [<xref rid="B6-sensors-25-01017" ref-type="bibr">6</xref>], natural sciences for preschool and primary education [<xref rid="B7-sensors-25-01017" ref-type="bibr">7</xref>], education for students with educational needs [<xref rid="B8-sensors-25-01017" ref-type="bibr">8</xref>], support for individuals who are deaf or hard of hearing [<xref rid="B9-sensors-25-01017" ref-type="bibr">9</xref>], and even in the military training contexts [<xref rid="B10-sensors-25-01017" ref-type="bibr">10</xref>].</p><p>The motivation for this research arises from the growing need for innovative solutions to enhance TED for patients. Traditional educational methods often fail to engage patients effectively. Developing autonomous, easily accessible, and user-friendly tools would enable patients to access these resources more conveniently. The contributions of this work include the adaptation of an AR application tailored for diabetes patients in Ecuador, offering an engaging and accessible approach to therapeutic education in diabetes. The study sample comprises both children and adults. Short- and mid-term learning outcomes are assessed, and the results of learning through the AR application are compared with traditional educational methods. Furthermore, data from Ecuadorian patients are compared with those from Spanish patients. In summary, this work demonstrates the potential of AR-based tools to provide a more engaging complement to conventional methods and highlights their applicability across diverse cultural and demographic contexts. The findings aim to pave the way for broader implementation of similar tools, addressing the global challenge of improving chronic disease management through innovative, patient-centered approaches.</p></sec><sec id="sec2-sensors-25-01017"><title>2. Related Work</title><sec id="sec2dot1-sensors-25-01017"><title>2.1. Virtual Reality</title><p>VR is a technology with significant potential to assist diabetes patients, attracting considerable interest from researchers, as highlighted by recent reviews [<xref rid="B11-sensors-25-01017" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01017" ref-type="bibr">12</xref>]. Hao et al. [<xref rid="B11-sensors-25-01017" ref-type="bibr">11</xref>] conducted a systematic review to identify, critically evaluate, and synthesize the effects of VR on balance in individuals with diabetes. They concluded that VR-based rehabilitation shows promising effects in enhancing balance among individuals with diabetes. Vaughan [<xref rid="B12-sensors-25-01017" ref-type="bibr">12</xref>] offers a comprehensive review of VR and AR applications in diabetes, highlighting key areas of use and providing a foundation for future developments in VR tools for diabetes management. He concluded that VR and AR in diabetes show potential to improve the training of diabetologists while advancing education, prevention, and treatment strategies for both adults and children with Type 1 or Type 2 diabetes. This section explores the use of VR as a tool for managing and treating diabetes, focusing on its applications in education, prevention, and patient care for individuals with Type 1 and Type 2 diabetes.</p><p>For individuals with Type 1 diabetes, different VR tools have been presented. For instance, a platform incorporating a variety of scenarios was created to train individuals with diabetes [<xref rid="B12-sensors-25-01017" ref-type="bibr">12</xref>]. These include training for exercising safely in a VR gym while preventing hypoglycemia for individuals with Type 1 diabetes, advice on carbohydrate counting in a VR kitchen, training on finger-prick techniques for newly diagnosed patients, and a workplace scenario for glucose monitoring. The platform was tested on Android smartphones using Google Cardboard. Similarly, a virtual world game was created to support children with Type 1 Diabetes, suggesting that virtual games should incorporate three essential elements: (1) The player&#x02019;s connection with the game&#x02019;s protagonist; (2) a reward system for adhering to medication and diet; and (3) a focus on the disease&#x02019;s biochemical basics [<xref rid="B13-sensors-25-01017" ref-type="bibr">13</xref>]. Lanning et al. [<xref rid="B14-sensors-25-01017" ref-type="bibr">14</xref>] created a four-part VR intervention to expose adults with Type 1 diabetes to expected Closed Loop system barriers: perceived hassles of using Closed Loop, body image, unwanted social attention, and deskilling fears. The authors concluded that the VR intervention showed great potential for addressing expected barriers in the uptake and use of Closed Loop systems while maintaining enthusiasm and preserving positive expectations toward Closed Loop.</p><p>For individuals with Type 2 diabetes, VR tools have also been developed. For instance, Diabetes Island was developed within a virtual world in Second Life [<xref rid="B15-sensors-25-01017" ref-type="bibr">15</xref>]. This virtual island provides a range of diabetes self-care education activities and resources. The users, identified by their avatar name, could communicate through voice and text chat tools. On Diabetes Island, the participants accessed various educational resources, including scenarios on nutrition labels, educational signage, and a learning center with written materials and videos. Four interactive scenarios focused on healthy eating and physical activity. Health professionals, as avatars, led real-time formal and informal sessions on topics like healthy eating, medication adherence, physical activity, and diabetes care. The participants earned points for engaging in activities, which could be traded for avatar clothing. The study focused on African American women with Type 2 diabetes, characterized by low levels of physical activity and high-fat diets. The impact of the intervention was evaluated using a single-group repeated measures design, with assessments at three time points: (1) baseline; (2) three months (mid-intervention); and (3) six months (immediately post-intervention). The Diabetes Island intervention showed (1) good participant acceptance and regular usage; (2) a reduction in diabetes-related distress, body mass index, and environmental barriers to self-care; and (3) improvements in dietary habits and physical activity. All of this was observed among a group of low-income patients. Nintendo Wii Fit Plus (Nintendo Co., Ltd., Kyoto, Japan) has been used to develop educational games to promote physical activity and improve health outcomes in adults with diabetes. A study involving adults with Type 2 diabetes used a Wii Fit Plus game for 12 weeks [<xref rid="B16-sensors-25-01017" ref-type="bibr">16</xref>]. The conclusions were that the Wii Fit Plus interactive exercise game effectively motivated patients to enhance physical activity, improve glucometabolic control, and boost their quality of life. Reducing diabetes-related distress is another application of VR. With this objective in mind, Ghosal et al. [<xref rid="B17-sensors-25-01017" ref-type="bibr">17</xref>] proposed the development of a VR-based mindfulness application to reduce diabetes distress for patients with Type 2 Diabetes. On the therapeutic side, Lee et al. [<xref rid="B18-sensors-25-01017" ref-type="bibr">18</xref>] investigated the impact of a VR exercise program (VREP) on patients with Type 2 diabetes.</p><p>The program used IoT sensors attached to an indoor bicycle connected to a smartphone, allowing the participants to engage in immersive VR exercises via a headset. The study, conducted three times weekly over two weeks, found significant reductions in mean blood glucose and serum fructosamine levels in the VREP and indoor bicycle exercise (IBE) groups compared to the control group. Although body mass index showed no significant changes, muscle mass notably increased in the VREP and IBE groups. The effects of various feedback modalities on balance within immersive VR have also been explored. For example, Mahmud et al. [<xref rid="B19-sensors-25-01017" ref-type="bibr">19</xref>] investigated the effects of various feedback modalities (auditory, vibrotactile, and visual) on balance within immersive VR. The study included participants with balance impairments due to Type 2 diabetes as well as those without balance impairments. During standing reach-and-grasp tasks, auditory and vibrotactile feedback significantly enhanced balance for all of the participants, while visual feedback showed significant improvement only for those with balance impairments. Cinematic VR has also been explored. For example, Beverly et al. [<xref rid="B20-sensors-25-01017" ref-type="bibr">20</xref>] carried out a pilot study to assess the effectiveness of a cinematic VR training program aimed at care providers. The program centered on an elderly patient with Type 2 diabetes and multiple geriatric syndromes, highlighting the risk of elder abuse and neglect.</p></sec><sec id="sec2dot2-sensors-25-01017"><title>2.2. Augmented Reality</title><p>While AR has shown promise in healthcare, its applications in TED remain relatively underexplored. Previous research works on AR that are not directly related to our work are as follows: The first one is an AR application for learning about antihypertensives for people with Type 2 diabetes [<xref rid="B21-sensors-25-01017" ref-type="bibr">21</xref>]. The AR application scans pre-defined patterns (e.g., a medication package) and shows the augmented information. The second one is Jerry the Bear<sup>&#x000ae;</sup>, a teddy bear toy with image targets embedded in his body [<xref rid="B22-sensors-25-01017" ref-type="bibr">22</xref>] to provide TED to children with Type 1 diabetes. For example, the AR application on a mobile device recognizes the areas of the teddy bear&#x02019;s body where image targets are located. It allows the blood glucose to be measured by simulating a prick in the bear&#x02019;s hand or administering insulin with a pen or pump in Jerry&#x02019;s belly. The process of learning to count carbohydrates is not carried out with AR. The third one was presented by Kurniawan et al. [<xref rid="B23-sensors-25-01017" ref-type="bibr">23</xref>], who developed a mobile AR application to provide diabetes drug information using QR codes to display 3D representations. The study, involving pharmacy students, found that AR technology supported 76.2% of student learning, demonstrating its effectiveness in enhancing educational outcomes. The fourth was presented by Fajriyah et al. [<xref rid="B24-sensors-25-01017" ref-type="bibr">24</xref>], who conducted a study to analyze the effect of AR-based therapeutic patient education on Health Locus of Control in Type 2 diabetes patients. The AR application utilized barcode markers to trigger interactive content, focusing on the five key areas of diabetes management: education, diet, physical activity, blood glucose monitoring, and medication. It also featured quizzes to assess patient knowledge and provided explanations on managing diabetes. The intervention was administered three times a week over a period of three months. The participants were split into two groups: half in the intervention group and half in the control group. Their findings indicated that AR-based Therapeutic Education significantly improved Health Locus of Control scores in Type 2 Diabetes patients. However, no significant difference was found between the intervention and control groups in terms of overall Health Locus of Control improvement.</p><p>Previous research works that are directly related to our work are the following. The first one was presented by Domhardt et al. [<xref rid="B25-sensors-25-01017" ref-type="bibr">25</xref>], who developed an AR application for mobile devices that estimates carbohydrate choices in real food. The application superimposes a virtual mesh over those foods. The users can interact with this virtual mesh to adjust it to the volume of the real food. The application measures the volume of real food and estimates its weight. For the correct functioning of the application, a marker must be placed in front of the food to be measured. Eight patients took part in the study. In 44% of the assessments, the margin of error decreased by a minimum of 6 g of carbohydrates. The second one is from one of our previous works [<xref rid="B26-sensors-25-01017" ref-type="bibr">26</xref>], in which we developed the first version of the AR application for Spanish children. In the study, two post-knowledge questionnaires were used: one containing the same foods as the pre-knowledge questionnaire and a second one containing different foods from those in the pre-knowledge questionnaire. There were no statistically significant differences between the results of these two different post-knowledge questionnaires. In this work, the AR application has been adapted to food for patients from Ecuador; the sample includes children and adults. Short- and mid-term memories were evaluated. The AR outcomes were compared with traditional ones. We have also compared the Spanish data [<xref rid="B26-sensors-25-01017" ref-type="bibr">26</xref>] with our study conducted in Ecuador.</p></sec></sec><sec id="sec3-sensors-25-01017"><title>3. Materials and Methods</title><sec id="sec3dot1-sensors-25-01017"><title>3.1. Augmented Reality Application</title><p>The AR application shows virtual food on a real dish using a mobile device. The goal is to make users believe that they are real foods. When the application recognizes the image in the center of the dish, the virtual food is shown in the center of the physical dish. The users can place the dish wherever they like and view the food in the desired location. Users can freely move their mobile device or dish to view the virtual food from any angle (360&#x000b0;). The users can zoom in or out to view the food closer or farther away. The users can use the application at home to complete the training recommended by their therapists. To do so, they simply need to install the application on their mobile phone and print the image to attach to the dish they wish to use. <xref rid="sensors-25-01017-f001" ref-type="fig">Figure 1</xref> provides a graphical summary of the steps that patients must follow in the AR application, while <xref rid="sensors-25-01017-f002" ref-type="fig">Figure 2</xref> presents a flowchart illustrating its functionality in detail.</p><list list-type="simple"><list-item><label>(1)</label><p>Filling out personal information: Initially, the patients must enter their age and gender. Following this, the application shows the recommended carbohydrate choices for their age for the whole day and the carbohydrate choices for breakfast (1 carbohydrate choice = 15 g of carbohydrates). This equivalency is currently used in Ecuador, the United States, Mexico, and most of Latin America. However, it varies in other countries. For example, in Spain, one carbohydrate choice equals 10 g of carbohydrates, and in Austria, one carbohydrate choice equals 12 g of carbohydrates.</p></list-item><list-item><label>(2)</label><p>The AR application: The application has three levels, each of which focuses on a distinct food category. Specifically, these categories are dairy products, grains, and fruits. The first level (dairy products) includes seven different foods, while the second level (grains) includes eight foods, and the third level (fruits) includes 10 foods. Each level begins with a learning phase where foods are shown in real size along with their weight, carbohydrate choices, glycemic level, and homemade portions of food, such as a tablespoon or cup (this is the additional information that the Foundation shows in its educational materials). After this initial learning phase, the patients undergo an assessment phase to test their knowledge. During this phase, the patients are shown each food in the center of the screen and must select the correct number of carbohydrate choices from the three options provided.</p></list-item><list-item><label>(3)</label><p>The final challenge: After completing the three levels, the patients face a final challenge, which is to identify as many unique breakfast combinations as possible within a set time (one and a half minutes). Each breakfast must adhere to the recommended carbohydrate choices for the users&#x02019; ages. To succeed, the patients must select foods from at least two groups. Each breakfast entry is completed by pressing the button in the upper right corner (represented by a fork and spoon).</p></list-item></list><p>The AR application was developed using Unity (<uri xlink:href="https://unity.com">https://unity.com</uri> (accessed on 10 December 2024)) as the primary development platform, with Vuforia Engine (<uri xlink:href="https://developer.vuforia.com">https://developer.vuforia.com</uri> (accessed on 10 December 2024)) integrated for its AR tracking and registration capabilities. Vuforia Engine provides tracking capabilities for a selection of items and areas, which can be grouped into Images, Objects, and Environments. Specifically, for the development of our AR application, we used an image target, which is glued to a real dish to make it look like it is a design of the dish itself. Another option would have been to select a dish with a design that is suitable for recognition with Vuforia Engine. This image target is recognized and tracked by the Vuforia Engine in real time using the image captured by the device&#x02019;s camera. This is achieved by the Image Target Observer, which begins tracking the image target upon detection. The image is tracked by comparing the natural features extracted from the camera image with a known target stored in the active database. Once the image target is detected, the Vuforia Engine calculates its position and orientation in 3D space. If Extended Tracking is active, the pose information of an image target remains available even when it is no longer visible in the camera&#x02019;s field of view, is occluded, or cannot be tracked for other reasons. Extended Tracking uses the device&#x02019;s pose to improve tracking performance and ensure continuity of tracking, even when the target is out of view. In practice, Extended Tracking means that once the device is moved away from the original target, the augmentations stay in place relative to the real world and remain consistent with the reference frame defined by the target. The Device Pose Observer obtains tracking information about the device&#x02019;s position and orientation in the physical world by analyzing the visual features of the environment captured by the camera and, in certain cases such as Extended Tracking, by using the built-in inertial measurement unit (IMU) sensor to determine the device&#x02019;s six-degree-of-freedom pose. Typical sensors in a mobile device IMU include an accelerometer, gyroscope, and magnetometer. For virtual content mapping, Vuforia Engine anchors the virtual food to the image target&#x02019;s position and dynamically adjusts the rendering to maintain proper alignment as the user or the device moves. The virtual elements (food) were adjusted in Unity on the image target so that when displayed on the screen, they are shown in real size according to their carbohydrate portions. The physical dish, as a movable, rotatable, and inclinable object, serves as a Tangible User Interface by enabling intuitive interaction with the digital representation of food in AR, seamlessly bridging the physical and digital worlds. The virtual guide is displayed using the center of the image target as the origin of the coordinate system to show the other information, providing users with step-by-step instructions or contextual information. Vuforia Engine supports different operating systems, tools, and device versions for developing and running AR applications (<uri xlink:href="https://developer.vuforia.com/library/platform-support/supported-versions">https://developer.vuforia.com/library/platform-support/supported-versions</uri> (accessed on 10 December 2024)). Our AR application can run on mobile devices with an Android operating system and an iOS operating system. In our study, we used a Xiaomi POCO X3 NFC smartphone (Xiaomi Corporation, Beijing, China).</p></sec><sec sec-type="subjects" id="sec3dot2-sensors-25-01017"><title>3.2. Participants</title><p>The study participants are patients who attended one of the activities organized by the Diabetes House, which is based in the cities of Cuenca and Portoviejo (Ecuador). The Diabetes House (<uri xlink:href="https://casadeladiabetes.org.ec">https://casadeladiabetes.org.ec</uri> (accessed on 10 December 2024)) has therapeutic learning activities as well as educational material that can be accessed from its website for patients with Type 1 diabetes. A total of 27 patients from Ecuador with Type 1 diabetes (14 men and 13 women), ranging in age from 5 to 28 years old, participated in the study. The mean age was 15.85 &#x000b1; 5.92 years old. Information about the study was provided to the patients or their parents (in cases where the patients were minors). Informed consent forms were signed by the patients or their parents before participating. The study adhered to the principles of the Declaration of Helsinki and received approval from the Ethics Committee of the University of Cuenca (Ecuador).</p></sec><sec id="sec3dot3-sensors-25-01017"><title>3.3. Measures</title><p>Assessment of the knowledge: To assess the knowledge about carbohydrate choices, there was a knowledge questionnaire in which the patients should indicate the number of carbohydrate choices for nine foods with images selected from the 25 foods shown in the application. All of the foods that appeared on the questionnaire were included in our AR application and the traditional session, and the patients were informed of their carbohydrate choices. The same questionnaire was used to measure the level of knowledge of carbohydrate choices at different stages of the study. The knowledge variable is the sum of correct answers (from 0 to 9) and can be expressed mathematically as the following:</p><p>Knowledge Score = &#x003a3; Correct Answers, where each correct answer is assigned a value of 1, and each incorrect answer a value of 0.</p><p>The following questionnaires were used:<list list-type="simple"><list-item><label>(1)</label><p>Short-UEQ (User Experience Questionnaire) [<xref rid="B27-sensors-25-01017" ref-type="bibr">27</xref>]: The UEQ consists of eight items on a 7-point Likert scale, grouped into three variables: Pragmatic Quality, Hedonic Quality, and Overall. We used the short questionnaire instead of the long one with 26 items so that the session would not be too tedious.</p></list-item><list-item><label>(2)</label><p>Questions: Thirteen questions were included to assess usability and satisfaction with the application. These questions were the same as those used in [<xref rid="B26-sensors-25-01017" ref-type="bibr">26</xref>].</p></list-item><list-item><label>(3)</label><p>Data: This questionnaire gathered data from the participants, including their age and gender.</p></list-item></list></p></sec><sec id="sec3dot4-sensors-25-01017"><title>3.4. Protocol</title><p>The initial study design was a between-subjects design. This implied two separate groups: a control group using traditional learning and an experimental group using the AR application for learning. The study began with the control group. The knowledge outcomes were analyzed, and it was observed that the patients had not significantly increased their knowledge about carbohydrate choices. As a result, it was decided to continue with this group of patients and use a within-subjects design. The objective was to test whether the same patients could learn significantly using the AR application. Therefore, the protocol used in the study presented in this manuscript involved three sessions.</p><p>Session 1: The patients learn by attending a traditional class. The steps followed by the patients were as follows:<list list-type="bullet"><list-item><p>Fill out the knowledge questionnaire;</p></list-item><list-item><p>Attend a traditional session;</p></list-item><list-item><p>Fill out the knowledge questionnaire.</p></list-item></list></p><p>Session 2: The patients learn by using the AR application. The session took place one week after Session 1. The steps followed by the patients were as follows:<list list-type="bullet"><list-item><p>Fill out the knowledge questionnaire;</p></list-item><list-item><p>Learn by using the AR application;</p></list-item><list-item><p>Fill out the knowledge questionnaire;</p></list-item><list-item><p>Fill out the rest of the questionnaires (usability, satisfaction, and data questionnaires).</p></list-item></list></p><p>Session 3: The patients fill out the knowledge questionnaire to test their recall of what has been learned. The session took place two weeks after Session 2. This test focused on the effects of learning on mid-term memory.</p><p>A nutritionist taught the traditional class as she usually does in her regular classes. The session lasted one hour. In the class, images of the food were shown on a real scale and cut out in the shape of the food. The nutritionist talked about the foods included in the AR application and presented all of the data shown in the AR application. All of the foods included in the test were included in the class. This class also explained other concepts, such as the healthy plate. For example, she explained how these foods can be combined to make a healthy meal. A round of questions was also included after the explanation of each food. Some of them were: Can I eat my healthy dish in parts? Is it advisable to eat five meals a day? How can I determine the food choices that are not in the guide? The patients also asked for recommendations on preparing food (homemade recipes and deviating from the guide). The nutritionist answered all of the questions.</p><p>The study was conducted at the Diabetes House facilities in Cuenca and Portoviejo (Ecuador) in one session on a Saturday morning (from 10:00 a.m. to 12:00 a.m.). The patients required approximately 30 min to use the AR application and complete the questionnaires.</p></sec></sec><sec sec-type="results" id="sec4-sensors-25-01017"><title>4. Results</title><p>The Shapiro&#x02013;Wilk test was employed to assess the normal distribution of the variables. Most variables exhibited a non-normal distribution. Non-parametric tests were used for the entire dataset, as they are more appropriate for such distributions. The statistical data are reported as (<italic toggle="yes">Mdn</italic>: median; <italic toggle="yes">IQR</italic>: interquartile range), and all test results are provided as (test <italic toggle="yes">U</italic>: Mann&#x02013;Whitney U/test <italic toggle="yes">W</italic>: Wilcoxon Signed-rank, normal approximation <italic toggle="yes">Z</italic>, <italic toggle="yes">p</italic>-value, <italic toggle="yes">r</italic> effect size). Results with <italic toggle="yes">p</italic> &#x0003c; 0.005 are considered to be statistically significant and are highlighted in bold with the symbol &#x02018;*&#x02019;. Results with 0.005 &#x02264; <italic toggle="yes">p</italic> &#x0003c; 0.05 are considered to be suggestively significant and are highlighted in bold [<xref rid="B28-sensors-25-01017" ref-type="bibr">28</xref>]. Statistical analysis of the data was conducted using the R (4.1.3) statistical toolkit (<uri xlink:href="http://www.r-project.org">http://www.r-project.org</uri> (accessed on 10 December 2024)).</p><sec id="sec4dot1-sensors-25-01017"><title>4.1. Short-Term Learning Outcomes</title><p>Assessing short-term learning outcomes is crucial for understanding the immediate impact of the educational method. To evaluate this, the Wilcoxon Signed-rank test was used to test whether the patients learned using the traditional method (<italic toggle="yes">W</italic> = 43.5, <italic toggle="yes">Z</italic> = &#x02212;1.178, <italic toggle="yes">p</italic> = 0.355, <italic toggle="yes">r</italic> = 0.160). This was performed by comparing the scores obtained before (<italic toggle="yes">Mdn</italic> = 4.0; <italic toggle="yes">IQR</italic> = 2.0) and after the traditional learning session (<italic toggle="yes">Mdn</italic> = 4.0; <italic toggle="yes">IQR</italic> = 2.0).</p><p>As mentioned, since no statistical differences were found between the pre-test and the post-test, the same patients used the AR application one week later. The Wilcoxon Signed-rank test was used to test whether the students learned using the AR application (<italic toggle="yes">W</italic> = 0.0, <italic toggle="yes">Z</italic> = &#x02212;4.526, <bold><italic toggle="yes">p</italic> &#x0003c; 0.001 *</bold>, <italic toggle="yes">r</italic> = 0.616). This was performed by comparing the scores obtained before (<italic toggle="yes">Mdn</italic> = 4.0; <italic toggle="yes">IQR</italic> = 3.0) and after (<italic toggle="yes">Mdn</italic> = 8.0; <italic toggle="yes">IQR</italic> = 1.0) using the AR application.</p><p>The Wilcoxon signed-rank test was used to check whether there were differences in the scores of the same patients when using the two learning methods (<italic toggle="yes">W</italic> = 0.0, <italic toggle="yes">Z</italic> = &#x02212;4.546, <bold><italic toggle="yes">p</italic> &#x0003c; 0.001 *</bold>, <italic toggle="yes">r</italic> = 0.619). This was performed by comparing the score obtained using the traditional method (<italic toggle="yes">Mdn</italic> = 4.0, <italic toggle="yes">IQR</italic> = 2.0) and the score obtained using the AR application (<italic toggle="yes">Mdn</italic> = 8.0, <italic toggle="yes">IQR</italic> = 1.0). <xref rid="sensors-25-01017-f003" ref-type="fig">Figure 3</xref> shows a box plot for the scores obtained by the patients after learning using the AR application and the traditional method.</p><p>From these results, we can conclude that using the AR application for short-term knowledge transfer was effective and offered a statistically significant improvement over the traditional method.</p><p>To determine the learning performance, we compared the knowledge outcomes of our study with those of a similar study of Spanish children [<xref rid="B26-sensors-25-01017" ref-type="bibr">26</xref>]. The same experimental conditions were recreated in both studies. The main difference between the two AR applications was the food used, which in both cases was adapted to the country in which the studies were conducted. The steps in the studies related to the validation of the AR application were similar. The objective of this comparison was to determine whether the application was effective in both countries, taking into account their cultural, educational, social, and healthcare system access disparities. These factors may influence the effectiveness of the AR application in both countries. The Spanish study [<xref rid="B26-sensors-25-01017" ref-type="bibr">26</xref>] compared the use of the same and different questionnaires for the pre-test and the post-test. In our study, the group of 42 children that used the same questionnaire as pre-test and post-test is used for comparison. The children ranged in age from five to fourteen years old. The mean age was 8.74 &#x000b1; 2.56 years old. There were 24 boys (57.1%) and 18 girls (42.9%).</p><p>The initial knowledge assessment establishes a baseline that accounts for potential differences related to the time elapsed between the two studies, as well as inherent differences between the two populations (Spanish and Ecuadorian) and other factors, such as cultural, educational, social, and healthcare system access disparities.</p><p>Mann&#x02013;Whitney U tests were applied to test whether there were differences between the initial knowledge of the Ecuadorian and Spanish children and their knowledge after using the AR applications. When comparing the initial knowledge of the group of children from Ecuador (<italic toggle="yes">Mdn</italic> = 4.0; <italic toggle="yes">IQR</italic> = 3.0) with the group of children from Spain (<italic toggle="yes">Mdn</italic> = 2.0; <italic toggle="yes">IQR</italic> = 3.0), the results were as follows: <italic toggle="yes">U</italic> = 870, <italic toggle="yes">Z</italic> = 3.782, <bold><italic toggle="yes">p</italic> &#x0003c; 0.001 *</bold>, <italic toggle="yes">r</italic> = 0.455. This difference in favor of the Ecuadorian patients can not only be attributed to advances in educational resources and technology but also to differences in the health education strategies applied in each country. When comparing the knowledge after using the AR applications of the group of children from Ecuador (<italic toggle="yes">Mdn</italic> = 8.0; <italic toggle="yes">IQR</italic> = 1.0) with the group of children from Spain (<italic toggle="yes">Mdn</italic> = 7.0; <italic toggle="yes">IQR</italic> = 3.0), the results were as follows: <italic toggle="yes">U</italic> = 577.5, <italic toggle="yes">Z</italic> = 0.133, <italic toggle="yes">p</italic> = 0.899, <italic toggle="yes">r</italic> = 0.016. The small difference in favor of the Ecuadorian children can be explained by the significant difference in initial knowledge. In any case, although the initial knowledge of the two groups was different, the knowledge demonstrated after using the AR applications was not significantly different statistically. These results also suggest that the AR applications significantly increased patients&#x02019; knowledge to high levels, with many scores at or near the maximum.</p><p>Since the ages of the two groups (Spaniards and Ecuadorians) differ, a subgroup of Ecuadorians was selected to include only participants aged 14 or younger. Mann&#x02013;Whitney U tests were applied to test whether there were differences between the initial knowledge of the Ecuadorian subgroup and the Spanish children, as well as differences in their knowledge after using the AR applications. Comparing the initial knowledge of the group of children from Ecuador (<italic toggle="yes">Mdn</italic> = 5.0; <italic toggle="yes">IQR</italic> = 2.25) with the group of children from Spain (<italic toggle="yes">Mdn</italic> = 2.0; <italic toggle="yes">IQR</italic> = 3.0), the results were as follows: <italic toggle="yes">U</italic> = 429.5, <italic toggle="yes">Z</italic> = 3.784, <bold><italic toggle="yes">p</italic> &#x0003c; 0.001 *</bold>, <italic toggle="yes">r</italic> = 0.515. When comparing the knowledge after using the AR applications of the subgroup of children from Ecuador (<italic toggle="yes">Mdn</italic> = 8.0; <italic toggle="yes">IQR</italic> = 1.0) with the group of children from Spain (<italic toggle="yes">Mdn</italic> = 7.0; <italic toggle="yes">IQR</italic> = 3.0), the results were as follows: <italic toggle="yes">U</italic> = 260.5, <italic toggle="yes">Z</italic> = 0.183, <italic toggle="yes">p</italic> = 0.863, <italic toggle="yes">r</italic> = 0.025. Therefore, after removing participants older than 14 years old from the Ecuadorian group, the results remain consistent with the previous ones: statistically significant differences in initial knowledge were observed between the groups, but no differences were found in final knowledge after using the AR applications.</p><p>Mann&#x02013;Whitney U tests were applied to test whether there were differences between the knowledge of the groups of Ecuadorian adults (aged 18 or older) and Ecuadorian children after using the AR applications. Comparing the knowledge demonstrated after using the AR application of the Ecuadorian adult group (<italic toggle="yes">Mdn</italic> = 8.0; <italic toggle="yes">IQR</italic> = 0.75) with the Ecuadorian children group (<italic toggle="yes">Mdn</italic> = 8.0; <italic toggle="yes">IQR</italic> = 1.0), the results were as follows: <italic toggle="yes">U</italic> = 85.0, <italic toggle="yes">Z</italic> = 0.0, <italic toggle="yes">p</italic> = 1.0, <italic toggle="yes">r</italic> = 0.0. Mann&#x02013;Whitney U tests were applied to test whether there were differences between the knowledge of the groups of Ecuadorian adults (aged 18 or older) and Spanish children after using the AR applications. Comparing the knowledge demonstrated after using the AR applications of the Ecuadorian adult group (<italic toggle="yes">Mdn</italic> = 8.0; <italic toggle="yes">IQR</italic> = 0.75) with the Spanish children group (<italic toggle="yes">Mdn</italic> = 7.0; <italic toggle="yes">IQR</italic> = 3.0), the results were as follows: <italic toggle="yes">U</italic> = 215.5, <italic toggle="yes">Z</italic> = 0.132, <italic toggle="yes">p</italic> = 0.905, <italic toggle="yes">r</italic> = 0.018. These results suggest that AR applications are effective for both children and adults.</p></sec><sec id="sec4dot2-sensors-25-01017"><title>4.2. Mid-Term Learning Outcomes</title><p>Assessing mid-term learning outcomes is crucial for understanding the method&#x02019;s ability to promote sustained knowledge retention over time. To evaluate this, only the patients who completed all of the learning questionnaires were included in the analyses of this section. The pre-session test with the AR application was used to verify the patient&#x02019;s retention of what was learned in the traditional learning session. The Wilcoxon Signed-rank test was used to check this retention (<italic toggle="yes">W</italic> = 19.5, <italic toggle="yes">Z</italic> = 1.769, <italic toggle="yes">p</italic> = 0.071, <italic toggle="yes">r</italic> = 0.347) with the post-test of the traditional session (<italic toggle="yes">Mdn</italic> = 4.0; <italic toggle="yes">IQR</italic> = 3.0) when compared with the pre-test of the AR session (<italic toggle="yes">Mdn</italic> = 4.0; <italic toggle="yes">IQR</italic> = 3.0). The Wilcoxon Signed-rank test was also used to check the difference between the initial knowledge (<italic toggle="yes">Mdn</italic> = 4.0; <italic toggle="yes">IQR</italic> = 3.0) before using the traditional method and one week after the learning session (<italic toggle="yes">Mdn</italic> = 4.0; <italic toggle="yes">IQR</italic> = 3.0), with the results: <italic toggle="yes">W</italic> = 8, <italic toggle="yes">Z</italic> = 1.041, <italic toggle="yes">p</italic> = 0.345, <italic toggle="yes">r</italic> = 0.204. The results demonstrate that there were no statistically significant differences between the two analyses. While the patients exhibited slight learning in the traditional session, this learning was not maintained after one week, resulting in a return to a level of knowledge similar to that observed at the beginning.</p><p>The test performed two weeks after the learning session with the AR application was used to verify the patients&#x02019; retention of what was learned in that session. The Wilcoxon signed-rank test was used to test this retention (<italic toggle="yes">W</italic> = 65, <italic toggle="yes">Z</italic> = 2.887, <bold><italic toggle="yes">p</italic> &#x0003c; 0.005 *</bold>, <italic toggle="yes">r</italic> = 0.566), comparing the post-session test using the AR application (<italic toggle="yes">Mdn</italic> = 8.0, <italic toggle="yes">IQR</italic> = 1.0) with the two-week post-session test (<italic toggle="yes">Mdn</italic> = 6.0, <italic toggle="yes">IQR</italic> = 2.0). The Wilcoxon signed-rank test was also used to test the difference between the initial knowledge before the learning session with the AR application (<italic toggle="yes">Mdn</italic> = 4.0; <italic toggle="yes">IQR</italic> = 3.0) and two weeks after the learning session (<italic toggle="yes">Mdn</italic> = 6.0; <italic toggle="yes">IQR</italic> = 2.0), with the results: <italic toggle="yes">W</italic> = 10, <italic toggle="yes">Z</italic> = &#x02212;1.974, <bold><italic toggle="yes">p</italic> = 0.044</bold>, <italic toggle="yes">r</italic> = 0.387. The results demonstrate that there were significant differences in both analyses. The results of the first analysis indicate that the patients had statistically significant less knowledge than they did two weeks earlier. The results of the second analysis indicate that the patients had suggestively significant more knowledge than they had before the learning session with the AR application. Therefore, the patients retained a significant amount (but not all) of what they learned using the AR application in the mid-term. From all of these analyses, it can be concluded that the use of the AR application did indeed have a suggestively significant effect on mid-term knowledge retention.</p></sec><sec id="sec4dot3-sensors-25-01017"><title>4.3. User Experience</title><p>The patient&#x02019;s responses to the short-UEQ questionnaire after using the AR application were analyzed using the UEQ Data Analysis Tool (<uri xlink:href="https://www.ueq-online.org/">https://www.ueq-online.org/</uri> (accessed on 10 December 2024)). By using one of the UEQ downloadable Excel sheets, a graph was created (<xref rid="sensors-25-01017-f004" ref-type="fig">Figure 4</xref>) to evaluate the quality of our AR application in comparison to the products in the benchmark dataset included in the UEQ Data Analysis Tool [<xref rid="B29-sensors-25-01017" ref-type="bibr">29</xref>]. This graph shows that the AR application performs exceptionally well relative to the UEQ benchmark. Based on the interpretation guidelines provided in the UEQ tool and in [<xref rid="B29-sensors-25-01017" ref-type="bibr">29</xref>], our AR application ranks within the top 10% for all three variables or scales.</p><p><xref rid="sensors-25-01017-t001" ref-type="table">Table 1</xref> shows that 7 of the 13 comparisons show significant differences in favor of the Ecuadorians (three of them are statistically significant, and four of them suggestively significant). However, the scores are high enough to indicate that both groups are highly satisfied with the AR applications.</p></sec><sec id="sec4dot4-sensors-25-01017"><title>4.4. Gender and Age</title><p>To determine if gender influences the outcomes obtained by the patients, we applied the Mann&#x02013;Whitney U tests. The learning outcomes of women and men were compared after using the traditional method and after using the AR application. The results are shown in <xref rid="sensors-25-01017-t002" ref-type="table">Table 2</xref>. No significant differences were found for either of the two analyses. <xref rid="sensors-25-01017-f005" ref-type="fig">Figure 5</xref> shows box plots for the scores obtained by women and men after learning using the traditional method and the AR application.</p><p>We used the Mann&#x02013;Whitney U tests to assess whether gender affects the UEQ variables. The UEQ variables for women and men were compared. <xref rid="sensors-25-01017-t003" ref-type="table">Table 3</xref> shows the results. None of the analyses revealed significant differences. We used the Mann&#x02013;Whitney U tests to assess whether gender affects the satisfaction and usability variables. These variables for women and men were compared. None of the analyses revealed significant differences and are not included in this paper.</p><p>The Kruskal&#x02013;Wallis tests were applied to assess whether age has an impact on any of the learning questionnaires filled out by the patients (<xref rid="sensors-25-01017-t004" ref-type="table">Table 4</xref>). Kruskal&#x02013;Wallis tests were also used to determine whether age affected the UEQ variables (<xref rid="sensors-25-01017-t005" ref-type="table">Table 5</xref>) and the usability and satisfaction questionnaire variables. None of the analyses revealed significant differences.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-01017"><title>5. Discussion</title><p>In this work, we present a mobile AR application to support TED and to help patients learn to count carbohydrates. Our AR application recognizes a real dish and shows virtual food on it, giving the patient the feeling that both are real. The food was adapted for patients from Ecuador. In general, managing one&#x02019;s diet is crucial for maintaining good health. Monitoring their food intake is essential [<xref rid="B30-sensors-25-01017" ref-type="bibr">30</xref>] for patients with diabetes. The quantity of carbohydrates consumed during meals directly affects the insulin dosage required. Therefore, knowing the amount of carbohydrates in foods would empower patients to manage their condition independently and enhance their glycemic control. Carbohydrate counting is often challenging for patients with Type 1 diabetes, and inaccurate counting is the primary identified cause of poor postprandial glycemic control [<xref rid="B31-sensors-25-01017" ref-type="bibr">31</xref>]. The nutrition guidelines for patients with Type 1 diabetes recommend teaching them how to count carbohydrates using tools tailored to their preferences [<xref rid="B31-sensors-25-01017" ref-type="bibr">31</xref>]. Our AR application could serve as one of these tools, providing an interactive and personalized approach to support patients in carbohydrate counting. Our study did not examine enhancements in patients&#x02019; glycemic control using our app. Nonetheless, this remains a significant area for future research.</p><p>Our application runs on mobile devices. Considering that the global smartphone penetration rate was estimated at 69% in 2023, a mobile application is an optimal option for reaching the widest audience possible. In our study, we monitor the learning process. Still, the application can be used independently at any time and any place, enhancing flexibility in learning as highlighted in other research [<xref rid="B32-sensors-25-01017" ref-type="bibr">32</xref>]. Additionally, previous works also suggest that flexible and technological approaches could be used to reach young audiences [<xref rid="B33-sensors-25-01017" ref-type="bibr">33</xref>]. Such applications can be particularly beneficial in rural areas, where challenges such as lower health literacy and limited access to diabetologists often hinder effective diabetes management [<xref rid="B20-sensors-25-01017" ref-type="bibr">20</xref>].</p><p>In terms of learning about carbohydrate choices through our AR application in the short-term, the statistically significant differences in patients&#x02019; knowledge before and after using the application demonstrate that such tools are effective for knowledge transfer. This result aligns with other studies on AR, which have shown that using mobile AR for educational purposes can enhance the learning process [<xref rid="B1-sensors-25-01017" ref-type="bibr">1</xref>,<xref rid="B32-sensors-25-01017" ref-type="bibr">32</xref>]. The AR application is also effective for both children and adults, regardless of gender.</p><p>In terms of learning about carbohydrate choices through our AR application in the mid-term, the patients retained a suggestively significant amount (but not all) of what they learned using the AR application in the mid-term. Similarly, Alhamad &#x00026; Agha [<xref rid="B34-sensors-25-01017" ref-type="bibr">34</xref>] conducted a study comparing knowledge acquisition and retention between mobile learning and traditional learning in teaching respiratory therapy to third-year students. The lesson focused on arterial blood gases. Knowledge assessments were conducted before the class, immediately after, and again two weeks later. Their findings on mobile learning align with those of our study on the AR application, indicating that students gained knowledge after training and retained it over time, though retention declined compared to the immediate post-training assessment. Another study that also assessed knowledge retention two weeks after a learning session was conducted by Gargrish et al. [<xref rid="B35-sensors-25-01017" ref-type="bibr">35</xref>]. They assessed memory retention among K-12 students using AR for geometry. The first evaluation took place immediately after the learning session, the second was conducted two weeks later, and the third occurred after two months. Their results showed that the highest scores were obtained immediately after learning, followed by a decline in retention at the two-week mark, with a slight further decrease at the two-month mark. Their findings regarding the initial knowledge gain and the decline in knowledge retention at the two-week mark align with our study&#x02019;s results.</p><p>Using the traditional method, the patients did not significantly improve their initial knowledge. Our explanation for this result is that the length of the session, which was one hour and included more information beyond just carbohydrate choices, potentially overwhelmed the patients, hindering their ability to remember specific data such as the carbohydrate choices of the foods used in the test. However, it is important to acknowledge that traditional teaching methods are effective and have been successfully used for many years. Moreover, other works have compared the learning outcomes using AR applications and traditional methods and found no statistically significant differences [<xref rid="B32-sensors-25-01017" ref-type="bibr">32</xref>,<xref rid="B36-sensors-25-01017" ref-type="bibr">36</xref>]. Therefore, a new study should be conducted to determine the optimal type and duration of traditional sessions needed to achieve knowledge equivalent to an AR session of 30 min.</p><p>Chiang et al. [<xref rid="B37-sensors-25-01017" ref-type="bibr">37</xref>] compared a group using a mobile AR application and a group using a mobile application without AR. Their findings indicated that the average learning outcomes were significantly higher in the AR group compared to the non-AR group. This demonstrates the potential of AR in enhancing educational outcomes, which is in line with the results obtained in our work.</p><p>In terms of usability, several researchers have highlighted its significant impact on educational effectiveness [<xref rid="B38-sensors-25-01017" ref-type="bibr">38</xref>]. According to Sun et al. [<xref rid="B39-sensors-25-01017" ref-type="bibr">39</xref>], systems that are user-friendly enable learners to concentrate more on the content. In our study, our application was highly intuitive (a median of 5 on a scale from 1 to 5 in US#1), and the patients rated their user experience on the Short-UEQ questionnaire as excellent, scoring in the top 10% on all three variables relative to the UEQ benchmark [<xref rid="B29-sensors-25-01017" ref-type="bibr">29</xref>]. Therefore, we can argue that our application helps patients focus on learning about the carbohydrate content of different foods.</p><p>The patients expressed a strong interest in using these applications to expand their knowledge about diabetes, scoring a median of 5 with an interquartile range of 0 (on a scale of 1 to 5). This suggests that the application would be widely welcomed as a valuable educational tool in therapy. We consider our application to be a valuable educational resource due to its tailored approach to the needs of diabetes patients and its strong motivational impact in presenting or reinforcing information about carbohydrate choices.</p><p>Finally, the current state of AR, MR, and robotics, as well as their expected evolution, suggests that the range of possibilities for developing new tools for therapeutic education in diabetes is vast. With headsets such as Apple Vision Pro offering advanced capabilities, the potential for creating immersive, interactive, and effective therapeutic education tools has never been greater.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-01017"><title>6. Conclusions</title><p>This work introduces a mobile AR application that is designed to aid in therapeutic education for diabetes patients. Our findings indicate that patients gained statistically significant new knowledge about carbohydrate choices in the short term using our AR application, and it has a suggestively significant impact on mid-term retention. The AR application benefits both children and adults, regardless of gender.</p><p>AR applications of this type offer great flexibility in the learning process, as the activity can be conducted anywhere and at any time. Patients only require a minimal setup involving a few printed images and a mobile device. The features of our AR application and its minimal requirements make it a highly accessible educational tool with great potential for therapeutic diabetes education. In this work, we explored the application potential for diabetes therapeutic education. However, there are several avenues for future improvement. Our study assessed participants&#x02019; knowledge retention two weeks after the activity. Future research should aim to include a larger sample size and conduct a larger longer-term evaluation, assessing knowledge retention at intervals such as one month, six months, and one year. This would provide deeper insights into the long-term effectiveness of the AR application. Additionally, it should examine how this knowledge acquisition and retention, and consequently carbohydrate counting, affect the patient&#x02019;s health, for example, by examining glycated hemoglobin (HbA1c) levels. Base on all of those findings, integrating AR applications like ours or other technology-based tools into therapeutic education could be considered, especially to support patients in carbohydrate counting and improve dietary self-management. Furthermore, a larger sample size could support the application of machine learning models to complement the statistical analyses, enabling more advanced insights, such as predictive modeling and pattern recognition in participant behavior and learning outcomes. The application could be improved by adding support for multiple languages and adjusting the carbohydrate choice equivalents based on the user&#x02019;s country.</p></sec></body><back><ack><title>Acknowledgments</title><p>We thank Gabriela Molina-Ochoa, the Diabetes House (Ecuador), and all of the patients who participated in the study. We would like to thank the editor and reviewers for their valuable suggestions.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, M.C. and M.-C.J.; data curation, M.C. and M.-C.J.; formal analysis, M.C. and M.-C.J.; funding acquisition, M.-C.J.; investigation, M.C., F.A. and M.-C.J.; methodology, M.C., F.A. and M.-C.J.; project administration, M.-C.J.; resources, M.-C.J.; software, M.C., F.A. and M.-C.J.; supervision, M.-C.J.; validation, M.C. and M.-C.J.; visualization, M.C., F.A. and M.-C.J.; writing&#x02014;original draft, M.C., F.A. and M.-C.J.; writing&#x02014;review and editing, M.C., F.A. and M.-C.J. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>This study was conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of the University of Cuenca (Ecuador).</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in this study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available upon reasonable request from the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01017"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ak&#x000e7;ay&#x00131;r</surname><given-names>M.</given-names></name>
<name><surname>Ak&#x000e7;ay&#x00131;r</surname><given-names>G.</given-names></name>
</person-group><article-title>Advantages and Challenges Associated with Augmented Reality for Education: A Systematic Review of the Literature</article-title><source>Educ. Res. Rev.</source><year>2017</year><volume>20</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.edurev.2016.11.002</pub-id></element-citation></ref><ref id="B2-sensors-25-01017"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Garz&#x000f3;n</surname><given-names>J.</given-names></name>
<name><surname>Pav&#x000f3;n</surname><given-names>J.</given-names></name>
<name><surname>Baldiris</surname><given-names>S.</given-names></name>
</person-group><article-title>Systematic Review and Meta-Analysis of Augmented Reality in Educational Settings</article-title><source>Virtual Real.</source><year>2019</year><volume>23</volume><fpage>447</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1007/s10055-019-00379-9</pub-id></element-citation></ref><ref id="B3-sensors-25-01017"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Milgram</surname><given-names>P.</given-names></name>
<name><surname>Kishino</surname><given-names>F.</given-names></name>
</person-group><article-title>A Taxonomy of Mixed Reality Visual Displays</article-title><source>IEICE Trans. Inf. Syst.</source><year>1994</year><volume>77</volume><fpage>1321</fpage><lpage>1329</lpage></element-citation></ref><ref id="B4-sensors-25-01017"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>&#x000d6;z&#x000e7;elik</surname><given-names>N.P.</given-names></name>
<name><surname>Ek&#x0015f;i</surname><given-names>G.</given-names></name>
<name><surname>Baturay</surname><given-names>M.H.</given-names></name>
</person-group><article-title>Augmented Reality (AR) in Language Learning: A Principled Review of 2017-2021</article-title><source>Particip. Educ. Res.</source><year>2022</year><volume>9</volume><fpage>131</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.17275/per.22.83.9.4</pub-id></element-citation></ref><ref id="B5-sensors-25-01017"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bulut</surname><given-names>M.</given-names></name>
<name><surname>Borromeo Ferri</surname><given-names>R.</given-names></name>
</person-group><article-title>A Systematic Literature Review on Augmented Reality in Mathematics Education</article-title><source>Eur. J. Sci. Math. Educ.</source><year>2023</year><volume>11</volume><fpage>556</fpage><lpage>572</lpage><pub-id pub-id-type="doi">10.30935/scimath/13124</pub-id></element-citation></ref><ref id="B6-sensors-25-01017"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mazzuco</surname><given-names>A.</given-names></name>
<name><surname>Krassmann</surname><given-names>A.L.</given-names></name>
<name><surname>Reategui</surname><given-names>E.</given-names></name>
<name><surname>Gomes</surname><given-names>R.S.</given-names></name>
</person-group><article-title>A Systematic Review of Augmented Reality in Chemistry Education</article-title><source>Rev. Educ.</source><year>2022</year><volume>10</volume><fpage>e3325</fpage><pub-id pub-id-type="doi">10.1002/rev3.3325</pub-id></element-citation></ref><ref id="B7-sensors-25-01017"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lampropoulos</surname><given-names>G.</given-names></name>
</person-group><article-title>Teaching and Learning Natural Sciences Using Augmented Reality in Preschool and Primary Education: A Literature Review</article-title><source>Adv. Mob. Learn. Educ. Res.</source><year>2024</year><volume>4</volume><fpage>1021</fpage><lpage>1037</lpage><pub-id pub-id-type="doi">10.25082/AMLER.2024.01.013</pub-id></element-citation></ref><ref id="B8-sensors-25-01017"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fern&#x000e1;ndez-Batanero</surname><given-names>J.M.</given-names></name>
<name><surname>Montenegro-Rueda</surname><given-names>M.</given-names></name>
<name><surname>Fern&#x000e1;ndez-Cerero</surname><given-names>J.</given-names></name>
</person-group><article-title>Use of Augmented Reality for Students with Educational Needs: A Systematic Review (2016&#x02013;2021)</article-title><source>Societies</source><year>2022</year><volume>12</volume><elocation-id>36</elocation-id><pub-id pub-id-type="doi">10.3390/soc12020036</pub-id></element-citation></ref><ref id="B9-sensors-25-01017"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fernandes</surname><given-names>N.</given-names></name>
<name><surname>Leite Junior</surname><given-names>A.J.M.</given-names></name>
<name><surname>Mar&#x000e7;al</surname><given-names>E.</given-names></name>
<name><surname>Viana</surname><given-names>W.</given-names></name>
</person-group><article-title>Augmented Reality in Education for People Who Are Deaf or Hard of Hearing: A Systematic Literature Review</article-title><source>Univers. Access Inf. Soc.</source><year>2024</year><volume>23</volume><fpage>1483</fpage><lpage>1502</lpage><pub-id pub-id-type="doi">10.1007/s10209-023-00994-z</pub-id></element-citation></ref><ref id="B10-sensors-25-01017"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Virca</surname><given-names>I.</given-names></name>
<name><surname>B&#x000e2;rsan</surname><given-names>G.</given-names></name>
<name><surname>Oancea</surname><given-names>R.</given-names></name>
<name><surname>Vesa</surname><given-names>C.</given-names></name>
</person-group><article-title>Applications of Augmented Reality Technology in the Military Educational Field</article-title><source>Land Forces Acad. Rev.</source><year>2021</year><volume>26</volume><fpage>337</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.2478/raft-2021-0044</pub-id></element-citation></ref><ref id="B11-sensors-25-01017"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hao</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Yao</surname><given-names>Z.</given-names></name>
<name><surname>Remis</surname><given-names>A.</given-names></name>
<name><surname>Huang</surname><given-names>B.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group><article-title>Effects of Virtual Reality on Balance in People with Diabetes: A Systematic Review and Meta-Analysis</article-title><source>J. Diabetes Metab. Disord.</source><year>2024</year><volume>23</volume><fpage>417</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1007/s40200-024-01413-7</pub-id><pub-id pub-id-type="pmid">38932876</pub-id>
</element-citation></ref><ref id="B12-sensors-25-01017"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vaughan</surname><given-names>N.</given-names></name>
</person-group><article-title>Virtual Reality Meets Diabetes</article-title><source>J. Diabetes Sci. Technol.</source><year>2024</year><pub-id pub-id-type="doi">10.1177/19322968231222022</pub-id><pub-id pub-id-type="pmid">38193465</pub-id>
</element-citation></ref><ref id="B13-sensors-25-01017"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Burkholz</surname><given-names>J.</given-names></name>
<name><surname>von Mammen</surname><given-names>S.</given-names></name>
</person-group><article-title>Empathy &#x00026; Information: Ingredients for a Children&#x02019;s Game on Diabetes</article-title><source>Proceedings of the 2019 11th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games)</source><conf-loc>Vienna, Austria</conf-loc><conf-date>4&#x02013;6 September 2019</conf-date><fpage>1</fpage><lpage>2</lpage></element-citation></ref><ref id="B14-sensors-25-01017"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lanning</surname><given-names>M.</given-names></name>
<name><surname>Shen</surname><given-names>J.</given-names></name>
<name><surname>Wasser</surname><given-names>D.</given-names></name>
<name><surname>Riddle</surname><given-names>S.</given-names></name>
<name><surname>Agustin</surname><given-names>B.</given-names></name>
<name><surname>Hood</surname><given-names>K.</given-names></name>
<name><surname>Naranjo</surname><given-names>D.</given-names></name>
</person-group><article-title>Exposure to Closed Loop Barriers Using Virtual Reality</article-title><source>J. Diabetes Sci. Technol.</source><year>2020</year><volume>14</volume><fpage>837</fpage><lpage>843</lpage><pub-id pub-id-type="doi">10.1177/1932296820902771</pub-id><pub-id pub-id-type="pmid">32019329</pub-id>
</element-citation></ref><ref id="B15-sensors-25-01017"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ruggiero</surname><given-names>L.</given-names></name>
<name><surname>Moadsiri</surname><given-names>A.</given-names></name>
<name><surname>Quinn</surname><given-names>L.T.</given-names></name>
<name><surname>Riley</surname><given-names>B.B.</given-names></name>
<name><surname>Danielson</surname><given-names>K.K.</given-names></name>
<name><surname>Monahan</surname><given-names>C.</given-names></name>
<name><surname>Bangs</surname><given-names>V.A.</given-names></name>
<name><surname>Gerber</surname><given-names>B.S.</given-names></name>
</person-group><article-title>Diabetes Island: Preliminary Impact of a Virtual World Self-Care Educational Intervention for African Americans with Type 2 Diabetes</article-title><source>JMIR Serious Games</source><year>2014</year><volume>2</volume><fpage>e10</fpage><pub-id pub-id-type="doi">10.2196/games.3260</pub-id><pub-id pub-id-type="pmid">25584346</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01017"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kempf</surname><given-names>K.</given-names></name>
<name><surname>Martin</surname><given-names>S.</given-names></name>
</person-group><article-title>Autonomous Exercise Game Use Improves Metabolic Control and Quality of Life in Type 2 Diabetes Patients&#x02014;A Randomized Controlled Trial</article-title><source>BMC Endocr. Disord.</source><year>2013</year><volume>13</volume><elocation-id>57</elocation-id><pub-id pub-id-type="doi">10.1186/1472-6823-13-57</pub-id><pub-id pub-id-type="pmid">24321337</pub-id>
</element-citation></ref><ref id="B17-sensors-25-01017"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ghosal</surname><given-names>S.</given-names></name>
<name><surname>Stanmore</surname><given-names>E.</given-names></name>
<name><surname>Sturt</surname><given-names>J.</given-names></name>
<name><surname>Bogosian</surname><given-names>A.</given-names></name>
<name><surname>Woodcock</surname><given-names>D.</given-names></name>
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Milne</surname><given-names>N.</given-names></name>
<name><surname>Mubita</surname><given-names>W.</given-names></name>
<name><surname>Robert</surname><given-names>G.</given-names></name>
<name><surname>O&#x02019;Connor</surname><given-names>S.</given-names></name>
</person-group><article-title>Using Artificial Intelligence-Informed Experience-Based Co-Design (AI-EBCD) to Create a Virtual Reality-Based Mindfulness Application to Reduce Diabetes Distress: Protocol for a Mixed-Methods Feasibility Study</article-title><source>BMJ Open</source><year>2024</year><volume>14</volume><fpage>e088576</fpage><pub-id pub-id-type="doi">10.1136/bmjopen-2024-088576</pub-id></element-citation></ref><ref id="B18-sensors-25-01017"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>Y.</given-names></name>
<name><surname>Hong</surname><given-names>J.</given-names></name>
<name><surname>Hur</surname><given-names>M.</given-names></name>
<name><surname>Seo</surname><given-names>E.</given-names></name>
</person-group><article-title>Effects of Virtual Reality Exercise Program on Blood Glucose, Body Composition, and Exercise Immersion in Patients with Type 2 Diabetes</article-title><source>Int. J. Environ. Res. Public Health</source><year>2023</year><volume>20</volume><elocation-id>4178</elocation-id><pub-id pub-id-type="doi">10.3390/ijerph20054178</pub-id><pub-id pub-id-type="pmid">36901191</pub-id>
</element-citation></ref><ref id="B19-sensors-25-01017"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mahmud</surname><given-names>M.R.</given-names></name>
<name><surname>Cordova</surname><given-names>A.</given-names></name>
<name><surname>Quarles</surname><given-names>J.</given-names></name>
</person-group><article-title>Auditory, Vibrotactile, or Visual? Investigating the Effective Feedback Modalities to Improve Standing Balance in Immersive Virtual Reality for People with Balance Impairments Due to Type 2 Diabetes</article-title><source>Proceedings of the 2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</source><conf-loc>Sydney, Australia</conf-loc><conf-date>16&#x02013;20 October 2023</conf-date><fpage>573</fpage><lpage>582</lpage></element-citation></ref><ref id="B20-sensors-25-01017"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Beverly</surname><given-names>E.A.</given-names></name>
<name><surname>Love</surname><given-names>C.</given-names></name>
<name><surname>Love</surname><given-names>M.</given-names></name>
<name><surname>Williams</surname><given-names>E.</given-names></name>
<name><surname>Bowditch</surname><given-names>J.</given-names></name>
</person-group><article-title>Using Virtual Reality to Improve Health Care Providers&#x02019; Cultural Self-Efficacy and Diabetes Attitudes: Pilot Questionnaire Study</article-title><source>JMIR Diabetes</source><year>2021</year><volume>6</volume><fpage>e23708</fpage><pub-id pub-id-type="doi">10.2196/23708</pub-id><pub-id pub-id-type="pmid">33502335</pub-id>
</element-citation></ref><ref id="B21-sensors-25-01017"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ahmadvand</surname><given-names>A.</given-names></name>
<name><surname>Drennan</surname><given-names>J.</given-names></name>
<name><surname>Burgess</surname><given-names>J.</given-names></name>
<name><surname>Clark</surname><given-names>M.</given-names></name>
<name><surname>Kavanagh</surname><given-names>D.</given-names></name>
<name><surname>Burns</surname><given-names>K.</given-names></name>
<name><surname>Howard</surname><given-names>S.</given-names></name>
<name><surname>Kelly</surname><given-names>F.</given-names></name>
<name><surname>Campbell</surname><given-names>C.</given-names></name>
<name><surname>Nissen</surname><given-names>L.</given-names></name>
</person-group><article-title>Novel Augmented Reality Solution for Improving Health Literacy around Antihypertensives in People Living with Type 2 Diabetes Mellitus: Protocol of a Technology Evaluation Study</article-title><source>BMJ Open</source><year>2018</year><volume>8</volume><fpage>e019422</fpage><pub-id pub-id-type="doi">10.1136/bmjopen-2017-019422</pub-id></element-citation></ref><ref id="B22-sensors-25-01017"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ullman</surname><given-names>D.</given-names></name>
<name><surname>Phillips</surname><given-names>E.</given-names></name>
<name><surname>Aladia</surname><given-names>S.</given-names></name>
<name><surname>Haas</surname><given-names>P.</given-names></name>
<name><surname>Fowler</surname><given-names>H.S.</given-names></name>
<name><surname>Iqbal</surname><given-names>I.S.</given-names></name>
<name><surname>Mi</surname><given-names>K.L.</given-names></name>
<name><surname>Riches</surname><given-names>I.W.</given-names></name>
<name><surname>Omori</surname><given-names>M.</given-names></name>
<name><surname>Malle</surname><given-names>B.F.</given-names></name>
</person-group><article-title>Evaluating Psychosocial Support Provided by an Augmented Reality Device for Children With Type 1 Diabetes</article-title><source>Proc. Int. Symp. Hum. Factors Ergon. Health Care</source><year>2021</year><volume>10</volume><fpage>126</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1177/2327857921101117</pub-id></element-citation></ref><ref id="B23-sensors-25-01017"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kurniawan</surname><given-names>A.H.</given-names></name>
<name><surname>Puspita</surname><given-names>N.</given-names></name>
<name><surname>Yusmaniar</surname></name>
<name><surname>Rajendra</surname><given-names>F.</given-names></name>
</person-group><article-title>The Effectiveness of Using Digital Applications for Diabetes Mellitus with Augmented Reality Models as Learning Media in Pharmacy Education</article-title><source>Pharm. Educ.</source><year>2023</year><volume>23</volume><fpage>53</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.46542/pe.2023.232.5359</pub-id></element-citation></ref><ref id="B24-sensors-25-01017"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fajriyah</surname><given-names>N.</given-names></name>
<name><surname>Susanti</surname></name>
<name><surname>Kristiani</surname><given-names>R.B.</given-names></name>
</person-group><article-title>Effectiveness of Augmented Reality-Based Therapeutic Patient Education on Health Locus of Control in Type 2 Diabetes Mellitus Patients</article-title><source>Nurse Health J. Keperawatan</source><year>2024</year><volume>13</volume><fpage>298</fpage><lpage>310</lpage><pub-id pub-id-type="doi">10.36720/nhjk.v13i2.700</pub-id></element-citation></ref><ref id="B25-sensors-25-01017"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Domhardt</surname><given-names>M.</given-names></name>
<name><surname>Tiefengrabner</surname><given-names>M.</given-names></name>
<name><surname>Dinic</surname><given-names>R.</given-names></name>
<name><surname>F&#x000f6;tschl</surname><given-names>U.</given-names></name>
<name><surname>Oostingh</surname><given-names>G.J.</given-names></name>
<name><surname>St&#x000fc;tz</surname><given-names>T.</given-names></name>
<name><surname>Stechemesser</surname><given-names>L.</given-names></name>
<name><surname>Weitgasser</surname><given-names>R.</given-names></name>
<name><surname>Ginzinger</surname><given-names>S.W.</given-names></name>
</person-group><article-title>Training of Carbohydrate Estimation for People with Diabetes Using Mobile Augmented Reality</article-title><source>J. Diabetes Sci. Technol.</source><year>2015</year><volume>9</volume><fpage>516</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1177/1932296815578880</pub-id><pub-id pub-id-type="pmid">25883165</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01017"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Calle-Bustos</surname><given-names>A.-M.</given-names></name>
<name><surname>Juan</surname><given-names>M.-C.</given-names></name>
<name><surname>Garc&#x000ed;a-Garc&#x000ed;a</surname><given-names>I.</given-names></name>
<name><surname>Abad</surname><given-names>F.</given-names></name>
</person-group><article-title>An Augmented Reality Game to Support Therapeutic Education for Children with Diabetes</article-title><source>PLoS ONE</source><year>2017</year><volume>12</volume><elocation-id>e0184645</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0184645</pub-id><pub-id pub-id-type="pmid">28957355</pub-id>
</element-citation></ref><ref id="B27-sensors-25-01017"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Laugwitz</surname><given-names>B.</given-names></name>
<name><surname>Held</surname><given-names>T.</given-names></name>
<name><surname>Schrepp</surname><given-names>M.</given-names></name>
</person-group><article-title>Construction and Evaluation of a User Experience Questionnaire</article-title><source>Proceedings of the 4th Symposium of the Workgroup Human-Computer Interaction and Usability Engineering of the Austrian Computer Society</source><conf-loc>Graz, Austria</conf-loc><conf-date>20&#x02013;21 November 2008</conf-date><person-group person-group-type="editor">
<name><surname>Holzinger</surname><given-names>A.</given-names></name>
</person-group><fpage>63</fpage><lpage>76</lpage></element-citation></ref><ref id="B28-sensors-25-01017"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Benjamin</surname><given-names>D.J.</given-names></name>
<name><surname>Berger</surname><given-names>J.O.</given-names></name>
<name><surname>Johannesson</surname><given-names>M.</given-names></name>
<name><surname>Nosek</surname><given-names>B.A.</given-names></name>
<name><surname>Wagenmakers</surname><given-names>E.-J.</given-names></name>
<name><surname>Berk</surname><given-names>R.</given-names></name>
<name><surname>Bollen</surname><given-names>K.A.</given-names></name>
<name><surname>Brembs</surname><given-names>B.</given-names></name>
<name><surname>Brown</surname><given-names>L.</given-names></name>
<name><surname>Camerer</surname><given-names>C.</given-names></name>
<etal/>
</person-group><article-title>Redefine Statistical Significance</article-title><source>Nat. Hum. Behav.</source><year>2018</year><volume>2</volume><fpage>6</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0189-z</pub-id><pub-id pub-id-type="pmid">30980045</pub-id>
</element-citation></ref><ref id="B29-sensors-25-01017"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Martin</surname><given-names>S.</given-names></name>
<name><surname>J&#x000f6;rg</surname><given-names>T.</given-names></name>
<name><surname>Andreas</surname><given-names>H.</given-names></name>
</person-group><article-title>Construction of a Benchmark for the User Experience Questionnaire (UEQ)</article-title><source>Int. J. Interact. Multimed. Artif. Intell.</source><year>2017</year><volume>4</volume><fpage>40</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.9781/ijimai.2017.445</pub-id></element-citation></ref><ref id="B30-sensors-25-01017"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<collab>Grupo de Trabajo Gu&#x000ed;as Cl&#x000ed;nicas y Consensos de la Sociedad Espa&#x000f1;ola de Diabetes</collab>
</person-group><article-title>Perfil Profesional Del Educador de Pacientes Con Diabetes</article-title><source>Av. Diabetol.</source><year>2012</year><volume>28</volume><fpage>38</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.avdiab.2012.03.003</pub-id></element-citation></ref><ref id="B31-sensors-25-01017"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Clerc</surname><given-names>A.</given-names></name>
</person-group><article-title>Nutrition Education to Type 1 Diabetes Patients: Few Changes over the Time</article-title><source>Front. Clin. Diabetes Health</source><year>2023</year><volume>4</volume><elocation-id>1243237</elocation-id><pub-id pub-id-type="doi">10.3389/fcdhc.2023.1243237</pub-id></element-citation></ref><ref id="B32-sensors-25-01017"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Furi&#x000f3;</surname><given-names>D.</given-names></name>
<name><surname>Gonz&#x000e1;lez-Gancedo</surname><given-names>S.</given-names></name>
<name><surname>Juan</surname><given-names>M.-C.</given-names></name>
<name><surname>Segu&#x000ed;</surname><given-names>I.</given-names></name>
<name><surname>Rando</surname><given-names>N.</given-names></name>
</person-group><article-title>Evaluation of Learning Outcomes Using an Educational IPhone Game vs. Traditional Game</article-title><source>Comput. Educ.</source><year>2013</year><volume>64</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.compedu.2012.12.001</pub-id></element-citation></ref><ref id="B33-sensors-25-01017"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Morales</surname><given-names>P.</given-names></name>
<name><surname>Valero-Moreno</surname><given-names>S.</given-names></name>
<name><surname>P&#x000e9;rez-Mar&#x000ed;n</surname><given-names>M.</given-names></name>
</person-group><article-title>New Technologies in Psychological Intervention for Adolescents with Type I Diabetes: A Systematic Review</article-title><source>Curr. Psychol.</source><year>2024</year><volume>43</volume><fpage>17577</fpage><lpage>17592</lpage><pub-id pub-id-type="doi">10.1007/s12144-024-05694-2</pub-id></element-citation></ref><ref id="B34-sensors-25-01017"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alhamad</surname><given-names>B.R.</given-names></name>
<name><surname>Agha</surname><given-names>S.</given-names></name>
</person-group><article-title>Comparing Knowledge Acquisition and Retention Between Mobile Learning and Traditional Learning in Teaching Respiratory Therapy Students: A Randomized Control Trial</article-title><source>Adv. Med. Educ. Pract.</source><year>2023</year><volume>14</volume><fpage>333</fpage><lpage>342</lpage><pub-id pub-id-type="doi">10.2147/AMEP.S390794</pub-id><pub-id pub-id-type="pmid">37051507</pub-id>
</element-citation></ref><ref id="B35-sensors-25-01017"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gargrish</surname><given-names>S.</given-names></name>
<name><surname>Mantri</surname><given-names>A.</given-names></name>
<name><surname>Kaur</surname><given-names>D.P.</given-names></name>
</person-group><article-title>Evaluation of Memory Retention among Students Using Augmented Reality Based Geometry Learning Assistant</article-title><source>Educ. Inf. Technol.</source><year>2022</year><volume>27</volume><fpage>12891</fpage><lpage>12912</lpage><pub-id pub-id-type="doi">10.1007/s10639-022-11147-9</pub-id></element-citation></ref><ref id="B36-sensors-25-01017"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Furi&#x000f3;</surname><given-names>D.</given-names></name>
<name><surname>Juan</surname><given-names>M.-C.</given-names></name>
<name><surname>Segu&#x000ed;</surname><given-names>I.</given-names></name>
<name><surname>Viv&#x000f3;</surname><given-names>R.</given-names></name>
</person-group><article-title>Mobile Learning vs. Traditional Classroom Lessons: A Comparative Study</article-title><source>J. Comput. Assist. Learn.</source><year>2015</year><volume>31</volume><fpage>189</fpage><lpage>201</lpage><pub-id pub-id-type="doi">10.1111/jcal.12071</pub-id></element-citation></ref><ref id="B37-sensors-25-01017"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chiang</surname><given-names>T.H.C.</given-names></name>
<name><surname>Yang</surname><given-names>S.J.H.</given-names></name>
<name><surname>Hwang</surname><given-names>G.-J.</given-names></name>
</person-group><article-title>An Augmented Reality-Based Mobile Learning System to Improve Students&#x02019; Learning Achievements and Motivations in Natural Science Inquiry Activities</article-title><source>J. Educ. Technol. Soc.</source><year>2014</year><volume>17</volume><fpage>352</fpage><lpage>365</lpage></element-citation></ref><ref id="B38-sensors-25-01017"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Squires</surname><given-names>D.</given-names></name>
<name><surname>Preece</surname><given-names>J.</given-names></name>
</person-group><article-title>Predicting Quality in Educational Software: Evaluating for Learning, Usability and the Synergy between Them</article-title><source>Interact. Comput.</source><year>1999</year><volume>11</volume><fpage>467</fpage><lpage>483</lpage><pub-id pub-id-type="doi">10.1016/S0953-5438(98)00063-0</pub-id></element-citation></ref><ref id="B39-sensors-25-01017"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>P.-C.</given-names></name>
<name><surname>Tsai</surname><given-names>R.J.</given-names></name>
<name><surname>Finger</surname><given-names>G.</given-names></name>
<name><surname>Chen</surname><given-names>Y.-Y.</given-names></name>
<name><surname>Yeh</surname><given-names>D.</given-names></name>
</person-group><article-title>What Drives a Successful E-Learning? An Empirical Investigation of the Critical Factors Influencing Learner Satisfaction</article-title><source>Comput. Educ.</source><year>2008</year><volume>50</volume><fpage>1183</fpage><lpage>1202</lpage><pub-id pub-id-type="doi">10.1016/j.compedu.2006.11.007</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01017-f001"><label>Figure 1</label><caption><p>A graphical summary of the steps to be followed for the correct use of the AR application.</p></caption><graphic xlink:href="sensors-25-01017-g001" position="float"/></fig><fig position="float" id="sensors-25-01017-f002"><label>Figure 2</label><caption><p>A flowchart detailing the functionality of the AR application.</p></caption><graphic xlink:href="sensors-25-01017-g002" position="float"/></fig><fig position="float" id="sensors-25-01017-f003"><label>Figure 3</label><caption><p>Box plot for the scores obtained by the patients after learning to use the AR application and the traditional method.</p></caption><graphic xlink:href="sensors-25-01017-g003" position="float"/></fig><fig position="float" id="sensors-25-01017-f004"><label>Figure 4</label><caption><p>Comparison of the patient&#x02019;s responses after using the AR application with the benchmark provided in the UEQ Data Analysis Tool.</p></caption><graphic xlink:href="sensors-25-01017-g004" position="float"/></fig><fig position="float" id="sensors-25-01017-f005"><label>Figure 5</label><caption><p>Box plots for the scores obtained by men and women after learning using (<bold>a</bold>) the traditional method and (<bold>b</bold>) the AR application.</p></caption><graphic xlink:href="sensors-25-01017-g005" position="float"/></fig><table-wrap position="float" id="sensors-25-01017-t001"><object-id pub-id-type="pii">sensors-25-01017-t001_Table 1</object-id><label>Table 1</label><caption><p>Mann&#x02013;Whitney U tests for the usability and satisfaction questions between the Ecuadorians and the Spanish patients after using the AR applications.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Variables</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ecuado.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Spanish</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">U</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">Z</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">p</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">r</italic>
</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">US#1. I found the AR application easy to use</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">4; 2</td><td align="center" valign="middle" rowspan="1" colspan="1">1078.5</td><td align="center" valign="middle" rowspan="1" colspan="1">2.909</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.004 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.314</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">US#2. I have gotten used to the application quickly</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 0</td><td align="center" valign="middle" rowspan="1" colspan="1">4; 2</td><td align="center" valign="middle" rowspan="1" colspan="1">1130.5</td><td align="center" valign="middle" rowspan="1" colspan="1">3.564</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.001 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.384</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">US#3. I have concentrated more on playing than on the Tablet</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 2</td><td align="center" valign="middle" rowspan="1" colspan="1">901.0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.088</td><td align="center" valign="middle" rowspan="1" colspan="1">0.279</td><td align="center" valign="middle" rowspan="1" colspan="1">0.117</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">US#4. I could get close enough to the food</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">4; 2</td><td align="center" valign="middle" rowspan="1" colspan="1">1076.5</td><td align="center" valign="middle" rowspan="1" colspan="1">2.815</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>&#x0003c;0.005 *</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.304</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">US#5. I could see the food from different positions</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 2</td><td align="center" valign="middle" rowspan="1" colspan="1">967.5</td><td align="center" valign="middle" rowspan="1" colspan="1">1.786</td><td align="center" valign="middle" rowspan="1" colspan="1">0.075</td><td align="center" valign="middle" rowspan="1" colspan="1">0.193</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SA#1. I have had a good time</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 0</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">863.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.804</td><td align="center" valign="middle" rowspan="1" colspan="1">0.425</td><td align="center" valign="middle" rowspan="1" colspan="1">0.087</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SA#2. I liked how the food looked on the dish</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 0</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">897.0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.184</td><td align="center" valign="middle" rowspan="1" colspan="1">0.239</td><td align="center" valign="middle" rowspan="1" colspan="1">0.128</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SA#3. I felt that the food on the dish could be real food</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">801.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.054</td><td align="center" valign="middle" rowspan="1" colspan="1">0.961</td><td align="center" valign="middle" rowspan="1" colspan="1">0.006</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SA#4. I think I have learned with this application</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 0</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">1035.0</td><td align="center" valign="middle" rowspan="1" colspan="1">2.598</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.010</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.280</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SA#5. I would like to use these applications to learn more about diabetes </td><td align="center" valign="middle" rowspan="1" colspan="1">5; 0</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">887.0</td><td align="center" valign="middle" rowspan="1" colspan="1">1.068</td><td align="center" valign="middle" rowspan="1" colspan="1">0.288</td><td align="center" valign="middle" rowspan="1" colspan="1">0.115</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SA#6. I would invite my friends to use the application</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">4; 2</td><td align="center" valign="middle" rowspan="1" colspan="1">1060.5</td><td align="center" valign="middle" rowspan="1" colspan="1">2.701</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.007</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.291</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SA#7. I would use this application again</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 0</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" rowspan="1" colspan="1">1030.5</td><td align="center" valign="middle" rowspan="1" colspan="1">2.498</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.013</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.269</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SA#8. Score the application from 1 to 5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5; 0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5; 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">972.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.077</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.038</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.224</td></tr></tbody></table><table-wrap-foot><fn><p>Statistically significant results (<italic toggle="yes">p</italic> &#x0003c; 0.005) are shown in bold with the symbol &#x02018;*&#x02019;. Suggestively significant results (0.005 &#x02264; <italic toggle="yes">p</italic> &#x0003c; 0.05) are shown in bold. US#1, US#2, etc. refer to Usability questions, while "SA#1, SA#2, etc. refer to Satisfaction questions. The numbers indicate the specific question number within each category.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-01017-t002"><object-id pub-id-type="pii">sensors-25-01017-t002_Table 2</object-id><label>Table 2</label><caption><p>Mann&#x02013;Whitney U tests to assess whether gender affects the scores obtained using the traditional method and the AR application.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Women</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Men</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">U</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">Z</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">p</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">r</italic>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Traditional method</td><td align="center" valign="middle" rowspan="1" colspan="1">4; 2</td><td align="center" valign="middle" rowspan="1" colspan="1">5; 2</td><td align="center" valign="middle" rowspan="1" colspan="1">60.0</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02212;1.530</td><td align="center" valign="middle" rowspan="1" colspan="1">0.132</td><td align="center" valign="middle" rowspan="1" colspan="1">0.294</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AR application</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8; 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8; 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">115.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.276</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.211</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.246</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01017-t003"><object-id pub-id-type="pii">sensors-25-01017-t003_Table 3</object-id><label>Table 3</label><caption><p>Mann&#x02013;Whitney U tests assess whether gender affects the UEQ variables.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Women</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Men</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">U</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">Z</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">p</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">r</italic>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pragmatic Quality</td><td align="center" valign="middle" rowspan="1" colspan="1">2.75; 0.25</td><td align="center" valign="middle" rowspan="1" colspan="1">3; 0.6875</td><td align="center" valign="middle" rowspan="1" colspan="1">86.0</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02212;0.260</td><td align="center" valign="middle" rowspan="1" colspan="1">0.815</td><td align="center" valign="middle" rowspan="1" colspan="1">0.050</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Hedonic Quality</td><td align="center" valign="middle" rowspan="1" colspan="1">3.00; 0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">2.625; 0.6875</td><td align="center" valign="middle" rowspan="1" colspan="1">105.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.752</td><td align="center" valign="middle" rowspan="1" colspan="1">0.468</td><td align="center" valign="middle" rowspan="1" colspan="1">0.145</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Overall</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.875; 0.375</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.75; 0.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.150</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.901</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.029</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01017-t004"><object-id pub-id-type="pii">sensors-25-01017-t004_Table 4</object-id><label>Table 4</label><caption><p>Kruskal&#x02013;Wallis tests for the knowledge scores considering age.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">d.f.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">H</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">p</italic>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pres-test Traditional</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">15.494</td><td align="center" valign="middle" rowspan="1" colspan="1">0.416</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Post-test Traditional</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">13.357</td><td align="center" valign="middle" rowspan="1" colspan="1">0.575</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pre-test AR application</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">13.315</td><td align="center" valign="middle" rowspan="1" colspan="1">0.578</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Post-test AR application</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.7032</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.838</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01017-t005"><object-id pub-id-type="pii">sensors-25-01017-t005_Table 5</object-id><label>Table 5</label><caption><p>Kruskal&#x02013;Wallis tests for the UEQ variables considering age.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">d.f.</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">H</italic>
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<italic toggle="yes">p</italic>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Pragmatic Quality</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">12.203</td><td align="center" valign="middle" rowspan="1" colspan="1">0.664</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Hedonic Quality</td><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">21.842</td><td align="center" valign="middle" rowspan="1" colspan="1">0.112</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Overall</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.992</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.214</td></tr></tbody></table></table-wrap></floats-group></article>