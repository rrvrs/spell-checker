<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006504</article-id><article-id pub-id-type="pmc">PMC11861537</article-id><article-id pub-id-type="doi">10.3390/s25041275</article-id><article-id pub-id-type="publisher-id">sensors-25-01275</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Sliding-Window CNN + Channel-Time Attention Transformer Network Trained with Inertial Measurement Units and Surface Electromyography Data for the Prediction of Muscle Activation and Motion Dynamics Leveraging IMU-Only Wearables for Home-Based Shoulder Rehabilitation</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-8019-5265</contrib-id><name><surname>Bai</surname><given-names>Aoyang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01275" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Song</surname><given-names>Hongyun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-25-01275" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Yan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af2-sensors-25-01275" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8715-7072</contrib-id><name><surname>Dong</surname><given-names>Shurong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01275" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Feng</surname><given-names>Gang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-01275" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9798-5149</contrib-id><name><surname>Jin</surname><given-names>Hao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01275" ref-type="aff">1</xref><xref rid="c1-sensors-25-01275" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Fischer</surname><given-names>Georg</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01275"><label>1</label>College of Information Science &#x00026; Electronic Engineering, Zhejiang University, Hangzhou 310027, China; <email>aoyang@zju.edu.cn</email> (A.B.); <email>dongshurong@zju.edu.cn</email> (S.D.)</aff><aff id="af2-sensors-25-01275"><label>2</label>2nd Affiliated Hospital, School of Medicine, Zhejiang University, Hangzhou 310009, China; <email>hedy.song@zju.edu.cn</email> (H.S.); <email>yanwu@zju.edu.cn</email> (Y.W.); <email>gangfeng@zju.edu.cn</email> (G.F.)</aff><author-notes><corresp id="c1-sensors-25-01275"><label>*</label>Correspondence: <email>hjin@zju.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>19</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1275</elocation-id><history><date date-type="received"><day>15</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>04</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Inertial Measurement Units (IMUs) are widely utilized in shoulder rehabilitation due to their portability and cost-effectiveness, but their reliance on spatial motion data restricts their use in comprehensive musculoskeletal analyses. To overcome this limitation, we propose SWCTNet (Sliding Window CNN + Channel-Time Attention Transformer Network), an advanced neural network specifically tailored for multichannel temporal tasks. SWCTNet integrates IMU and surface electromyography (sEMG) data through sliding window convolution and channel-time attention mechanisms, enabling the efficient extraction of temporal features. This model enables the prediction of muscle activation patterns and kinematics using exclusively IMU data. The experimental results demonstrate that the SWCTNet model achieves recognition accuracies ranging from 87.93% to 91.03% on public temporal datasets and an impressive 98% on self-collected datasets. Additionally, SWCTNet exhibits remarkable precision and stability in generative tasks: the normalized DTW distance was 0.12 for the normal group and 0.25 for the patient group when using the self-collected dataset. This study positions SWCTNet as an advanced tool for extracting musculoskeletal features from IMU data, paving the way for innovative applications in real-time monitoring and personalized rehabilitation at home. This approach demonstrates significant potential for long-term musculoskeletal function monitoring in non-clinical or home settings, advancing the capabilities of IMU-based wearable devices.</p></abstract><kwd-group><kwd>inertial measurement unit</kwd><kwd>surface electromyography</kwd><kwd>shoulder rehabilitation</kwd><kwd>deep learning</kwd><kwd>musculoskeletal analysis</kwd></kwd-group><funding-group><award-group><funding-source>Key Research and Development Program of Zhejiang Province</funding-source><award-id>2021C03108</award-id></award-group><award-group><funding-source>Dr. Li Dak Sum and Yip Yio Chin Development Fund for Regenerative Medicine, Zhejiang University</funding-source></award-group><funding-statement>This work was funded by grants from the Key Research and Development Program of Zhejiang Province (No. 2021C03108) and the Dr. Li Dak Sum and Yip Yio Chin Development Fund for Regenerative Medicine, Zhejiang University.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01275"><title>1. Introduction</title><p>The shoulder joint (SJ) is one of the most complex and functionally versatile joints in the human body, enabling a wide range of upper-limb movements [<xref rid="B1-sensors-25-01275" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01275" ref-type="bibr">2</xref>]. Its structure, comprising the scapula, clavicle, and humerus, is supported by the rotator cuff and scapular stabilizer muscles, ligaments, and joint capsules that maintain stability and mobility [<xref rid="B3-sensors-25-01275" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01275" ref-type="bibr">4</xref>]. The shoulder&#x02019;s range of motion (ROM) extends from 120&#x000b0; to 180&#x000b0;, surpassing that of all other joints, and it plays a vital role in essential tasks like lifting, grasping, and fine motor activities [<xref rid="B3-sensors-25-01275" ref-type="bibr">3</xref>,<xref rid="B5-sensors-25-01275" ref-type="bibr">5</xref>]. Over 65% of the kinetic energy required for upper-limb function comes from the interaction between the SJ and scapula [<xref rid="B6-sensors-25-01275" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01275" ref-type="bibr">7</xref>].</p><p>This highly mobile joint is subject to complex biomechanical loads, especially during dynamic movements. The rotator cuff and scapular stabilizer muscles cooperate to centralize and stabilize the SJ, but imbalances in muscle activity can result in dysfunction. Subacromial impingement syndrome (SAIS), a common cause of restricted mobility, often arises from insufficient coordination among these muscles [<xref rid="B8-sensors-25-01275" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01275" ref-type="bibr">9</xref>]. The prevalence of rotator cuff injuries increases with age, affecting over 20% of individuals over 40, and over 50% of those over 65, hindering functional recovery [<xref rid="B10-sensors-25-01275" ref-type="bibr">10</xref>]. The dysfunction of scapular stabilizers, such as in scapular dyskinesis (SD), can exacerbate shoulder instability, pain, and muscle weakness, significantly affecting daily life and work capacity [<xref rid="B11-sensors-25-01275" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01275" ref-type="bibr">12</xref>].</p><p>Shoulder dislocation is another prevalent condition, with an incidence rate of 23 cases per 100,000 individuals and a recurrence rate of as high as 90% in younger populations [<xref rid="B13-sensors-25-01275" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01275" ref-type="bibr">14</xref>]. Shoulder injuries are among the most common musculoskeletal disorders, with a prevalence of 55% and an annual incidence rate of 62 new cases per 1000 individuals [<xref rid="B1-sensors-25-01275" ref-type="bibr">1</xref>,<xref rid="B10-sensors-25-01275" ref-type="bibr">10</xref>]. These injuries not only affect a patient&#x02019;s quality of life but also restrict their ability to perform daily, occupational, and recreational activities, imposing significant psychological and economic burdens [<xref rid="B7-sensors-25-01275" ref-type="bibr">7</xref>,<xref rid="B15-sensors-25-01275" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-01275" ref-type="bibr">16</xref>].</p><p>Recent research on shoulder injuries has increasingly focused on understanding the biomechanical mechanisms that regulate shoulder stability and functional recovery. Studies have also supported the development of personalized rehabilitation strategies, including physical therapy, functional exercises, and biofeedback [<xref rid="B17-sensors-25-01275" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01275" ref-type="bibr">18</xref>]. With increasing use of wearable devices such as electromyography (EMG) and inertial measurement units (IMU), the real-time monitoring of muscle activity and joint motion has become feasible [<xref rid="B19-sensors-25-01275" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01275" ref-type="bibr">20</xref>]. When integrated with machine learning models, these devices can predict muscle activation patterns and joint motion trajectories, leading to more tailored and efficient rehabilitation strategies [<xref rid="B21-sensors-25-01275" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01275" ref-type="bibr">22</xref>].</p><p>Several studies have demonstrated the benefits of combining EMG and IMU data for shoulder rehabilitation. Hasan et al. [<xref rid="B22-sensors-25-01275" ref-type="bibr">22</xref>] introduced a neural network framework that integrates both data types to improve rehabilitation robotics, achieving robust predictions of upper-limb dynamics. Similarly, Lobo et al. [<xref rid="B23-sensors-25-01275" ref-type="bibr">23</xref>] reviewed wearable technologies and highlighted the importance of deep learning for multisensor data fusion in motion analysis. Hua et al. [<xref rid="B24-sensors-25-01275" ref-type="bibr">24</xref>] evaluated the use of machine learning models to classify upper limb movements using IMU data. While these models showeded high accuracy in controlled environments, their performance in real-world scenarios was challenged by the variability of patient movements. Brennan et al. [<xref rid="B25-sensors-25-01275" ref-type="bibr">25</xref>] developed a cost-effective method for shoulder rehabilitation motion segmentation using a single IMU, but sensor configuration limitations led to suboptimal tracking accuracy for complex shoulder movements.</p><p>These advancements highlight the growing potential of machine learning in healthcare, particularly in intelligent rehabilitation systems. However, balancing model robustness, real-time performance, and cost-effectiveness remains a significant challenge. Deep neural networks (DNNs) and convolutional neural networks (CNNs) have been widely used for processing temporal signals, including EMG and IMU data for activity recognition and joint angle prediction, showcasing their considerable potential. Mekruksavanich et al. [<xref rid="B9-sensors-25-01275" ref-type="bibr">9</xref>] proposed a residual deep learning model that integrates sensor data for fitness activity classification with exceptional accuracy, demonstrating deep learning&#x02019;s potential in dynamic motion classification for shoulder rehabilitation. Werner et al. [<xref rid="B21-sensors-25-01275" ref-type="bibr">21</xref>] used a linear neural network to estimate shoulder loading during wheelchair activities, further illustrating the versatility of deep learning across various rehabilitation contexts.</p><p>Shen et al. [<xref rid="B26-sensors-25-01275" ref-type="bibr">26</xref>] introduced a multimodal data fusion approach that combines IMU and EMG signals with a long short-term memory (LSTM) model to assess upper limb functionality. While this method improves analytical precision by integrating multisource data, its reliance on high-quality sensor data and computationally intensive processes limits its real-time applicability. Ren et al. [<xref rid="B27-sensors-25-01275" ref-type="bibr">27</xref>] combined IMU and EMG data using deep learning to classify fundamental shoulder movements during rehabilitation, although this method&#x02019;s robustness decreased for complex multi-step rehabilitation tasks. Elkholy et al. [<xref rid="B28-sensors-25-01275" ref-type="bibr">28</xref>] used DNNs to classify basic upper-limb activities, but the model&#x02019;s limited adaptability to unstructured data restricts its broader clinical use.</p><p>The integration of transfer learning and hybrid models has accelerated progress in this field. Choi et al. [<xref rid="B18-sensors-25-01275" ref-type="bibr">18</xref>] introduced a transfer learning-based system for detecting upper-limb movement intentions using EMG and IMU data, offering an adaptive solution to individual variability. This method illustrates the potential of transfer learning in addressing data distribution discrepancies. Wang et al. [<xref rid="B29-sensors-25-01275" ref-type="bibr">29</xref>] developed a hybrid multi-feature neural network that integrates traditional feature extraction with deep learning for upper-limb gesture classification. While it enhanced model robustness, its high computational demand limits its application in resource-limited environments. Teng et al. [<xref rid="B30-sensors-25-01275" ref-type="bibr">30</xref>] explored the use of graph convolutional networks (GCNs) to predict shoulder and elbow joint angles using EMG and IMU data. Although the model captured complex motion patterns effectively, its reliance on large-scale, high-quality training data presents challenges for real-world use.</p><p>Despite progress in shoulder rehabilitation technologies, developing robust models for real-time prediction remains a significant challenge. Issues include model adaptability to diverse patient data, conflicts between real-time and lightweight solutions, and concerns about multimodal data fusion robustness. Wei and Wu [<xref rid="B20-sensors-25-01275" ref-type="bibr">20</xref>] discussed the integration of wearable sensors and machine learning for rehabilitation, emphasizing the importance of reliable systems. Foroutannia et al. [<xref rid="B31-sensors-25-01275" ref-type="bibr">31</xref>] highlighted the need for lightweight, efficient models that maintain high accuracy across various patient populations to meet clinical demands.</p><p>This study introduces a novel neural network architecture for predicting EMG features from IMU data in shoulder rehabilitation systems. By incorporating advanced deep learning techniques and multimodal sensor fusion, the proposed system refines neural network architectures and data processing methods to achieve high-precision muscle activity prediction and real-time shoulder motion decoding. Compared to existing approaches, the model reduces computational overhead while improving real-time performance and adaptability, making it suitable for unstructured scenarios, such as home rehabilitation.</p><p>This research not only provides an efficient technical solution but also bridges the gap between innovation and clinical application. By enhancing the functionality of wearable devices, it offers clinicians more reliable assessment tools and lays the foundation for personalized rehabilitation strategies for shoulder injury patients. Ultimately, the findings aim to improve shoulder rehabilitation efficiency and promote the adoption of intelligent rehabilitation technologies.</p></sec><sec id="sec2-sensors-25-01275"><title>2. Materials and Methods</title><sec id="sec2dot1-sensors-25-01275"><title>2.1. Data Collection</title><p>To collect IMU and sEMG data from the SJ, this study developed an integrated experimental system designed to facilitate both the acquisition and processing of data. The system comprises two primary components: an sEMG data acquisition device and an IMU data acquisition device. The complete process of the acquisition system is illustrated in <xref rid="sensors-25-01275-f001" ref-type="fig">Figure 1</xref>.</p><p>The IMU acquistion device captures the kinematic and dynamic characteristics of the SJ. The single sensor device used is the WT9011DCL, manufactured by WitMotion Shenzhen Co. (Floor 3, Building 7, YunLi Park, Guangming District, Shenzhen, China). Its key components include a tri-axial accelerometer, a tri-axial gyroscope, and a tri-axial magnetometer, accompanied by modules for the acquisition, storage, and transmission of data. The accelerometer measures linear acceleration in three-dimensional space, the gyroscope records angular velocity changes, and the magnetometer detects magnetic field information to aid in posture recognition. The data sampling frequency is configured at 200 Hz, ensuring adequate temporal resolution to accurately capture SJ motion patterns. The collected data are wirelessly transmitted via a Bluetooth module to a computer, where they are synchronized with electromyography signals, facilitating the integrated analysis of dynamic motion characteristics and muscle activation patterns.</p><p>A custom EMG acquisition module was built, which is mainly composed of an AD1299 chip, which is a four-channel, 24 bit, Analog-to-Digital Converter for EEG and biopotential measurements. The device&#x02019;s key components include a four-channel sEMG signal acquisition unit, signal amplifiers, filters, an analog-to-digital (AD) converter, and modules for data storage and transmission. High-sensitivity electrodes accurately detect the electromyographic activity of shoulder-related muscle groups. Due to the low amplitude of sEMG signals (typically ranging from a few microvolts to millivolts), the system is equipped with high-gain amplifiers to boost signal strength. Signal preprocessing involves high-pass filtering (cut-off frequency: 20 Hz) and low-pass filtering (cut-off frequency: 500 Hz) to reduce environmental noise and mitigate power-line interference. The filtered analog signals are digitized at a sampling frequency of 2000 Hz using the AD converter, delivering high-temporal-resolution inputs for subsequent analysis. The digitized signals are transmitted to a computer in real time through a serial port, ensuring reliable and complete data transmission.</p><p>To ensure temporal synchronization between the sEMG and inertial data, the system timestamps the acquired signals and conducts preliminary cleaning and calibration using dedicated software after acquisition. <xref rid="sensors-25-01275-f001" ref-type="fig">Figure 1</xref>c shows the row IMU and sEMG signal. These steps enable the system to deliver high-quality input data, providing a robust foundation for subsequent modeling and analysis.</p><p>The experiment recruited 25 volunteers, comprising 10 healthy participants and 15 individuals with SJ movement injuries, aged 18 to 30 years (mean age: 25.3 &#x000b1; 3.1 years). Among the healthy participants, 7 were male and 3 were female, while the injury group consisted of 7 males and 8 females. The injuries included subacromial impingement syndrome, mild rotator cuff tears, and shoulder complex injuries related to fractures. Participant demographic information is summarized in <xref rid="sensors-25-01275-t001" ref-type="table">Table 1</xref>. All participants had stable symptoms and successfully completed the prescribed experimental movements. All participants were right-handed and had no history of neurological disorders or acute musculoskeletal injuries. The experimental design and procedures were approved by the ethics committee, and all participants gave written informed consent.</p><p>The experiments were conducted in a standardized indoor environment with temperatures maintained between 22 <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:math></inline-formula>C and 25 <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mrow></mml:math></inline-formula>C to minimize environmental noise and interference from large equipment, ensuring stable experimental conditions. Participants were instructed to avoid intense physical activity or strenuous training before the experiment to maintain natural muscle relaxation and prevent fatigue from compromising data quality. For participants with shoulder movement injuries, the research team tailored the intensity and guidance of experimental movements to individual conditions, ensuring safety and comfort throughout the experiment. Before data collection, the target muscle areas were cleaned and hydrogel electrodes were placed at designated locations to minimize skin impedance. EMG signals were collected from key shoulder muscles, including the Deltoid (Del), Trapezius (Trap), Biceps (Bicep), Triceps (Tricep), and Latissimus dorsi (Lat). Inertial sensors were securely fastened near the SJ using straps to record three-dimensional motion data. The specific collection check point is shown in the <xref rid="sensors-25-01275-f001" ref-type="fig">Figure 1</xref>b.</p><p>The experimental procedure comprised multiple steps to address both static and dynamic data collection requirements. Initially, participants underwent baseline testing in a static posture to record EMG and inertial data from the SJ without movement, providing a reference for noise correction. Dynamic testing included three standardized SJ movements: abduction, forward flexion, internal and external rotation motion, as shown in <xref rid="sensors-25-01275-f002" ref-type="fig">Figure 2</xref>. Under researcher guidance, participants performed the movements at a fixed pace, repeating each movement 10 times, with each repetition lasting 5 s and a 2 s interval between repetitions. For specific movements, lightweight dumbbells were introduced as external loads to investigate muscle activation patterns under varying load conditions. For participants with shoulder injuries, the range of motion and speed were adjusted to ensure safety and minimize pain or discomfort.</p><p>The entire experiment lasted approximately 60 min, during which each participant&#x02019;s movements were carefully guided and calibrated. After the experiment, all collected data were subjected to quality checks using specialized software, including signal cleaning, noise correction, and time alignment, to ensure compliance with the requirements for subsequent analysis. This high-quality dataset serves as a reliable foundation for investigating SJ movement characteristics and muscle activation patterns.</p></sec><sec id="sec2dot2-sensors-25-01275"><title>2.2. Data Process</title><p>Data processing in this study involves both EMG signals and IMU signals. Due to the significant differences in sampling frequencies between the two types of signals (EMG signals are sampled at a much higher rate than IMU signals), the EMG signals were downsampled to temporally align with the IMU data. To meet the structured input requirements of the model, the processed data were further segmented into fixed-length sliding windows, ensuring a consistent data format for subsequent analysis and modeling. The outcomes of the data processing are depicted in <xref rid="sensors-25-01275-f002" ref-type="fig">Figure 2</xref>.</p><sec id="sec2dot2dot1-sensors-25-01275"><title>2.2.1. IMU Signal Processing</title><p>IMUs comprise a core structure consisting of a three-axis accelerometer and a three-axis gyroscope. These components measure three-axis acceleration and the angular velocity of joints during spatial motion, enabling the real-time analysis of joint posture angle variations in three-dimensional space. To extract motion characteristics, spatial angle transformations and feature extraction techniques were applied to the IMU signals.</p><p>The raw data recorded by the IMU sensors are represented in the local coordinate system of the sensor. For unified analysis, the local coordinate data must be transformed into the global coordinate system to enable the integration and comparison of data from different sensors. This transformation is performed using a rotation matrix <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow></mml:math></inline-formula>, defined as Equation (<xref rid="FD1-sensors-25-01275" ref-type="disp-formula">1</xref>):<disp-formula id="FD1-sensors-25-01275"><label>(1)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">g</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi mathvariant="bold">g</mml:mi><mml:mi>local</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">g</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the global coordinate system, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">g</mml:mi><mml:mi>local</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the local coordinate system, and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow></mml:math></inline-formula> is derived from quaternions, calculated by integrating gyroscope and accelerometer data to estimate orientation. The quaternion is computed using sensor fusion algorithms, such as the extended Kalman filter, to ensure accuracy in the transformation to the global coordinate system.</p><p>By integrating accelerometer and gyroscope data, the orientation angles of the SJ in three-dimensional space&#x02014;roll (<inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:math></inline-formula>), pitch (<inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula>), and yaw (<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:math></inline-formula>)&#x02014;can be determined as follows:<disp-formula id="FD2-sensors-25-01275"><label>(2)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>&#x003d5;</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix">arctan</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:msub><mml:mi>a</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix">arcsin</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>g</mml:mi></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>&#x003c8;</mml:mi><mml:mo>=</mml:mo><mml:mo form="prefix">arctan</mml:mo><mml:mfenced separators="" open="{" close="}"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are accelerometer data, <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are magnetometer data, and <italic toggle="yes">g</italic> represents gravitational acceleration.</p></sec><sec id="sec2dot2dot2-sensors-25-01275"><title>2.2.2. sEMG Signal Processing</title><p>Surface-EMG is a low-amplitude bioelectric signal, with its magnitude typically proportional to muscle force and generally ranging from 0 to 1.5 mV. Its analyzable frequency range extends from 0 to 500 Hz, with the majority of energy concentrated in the 20&#x02013;150 Hzband. During acquisition, sEMG signals are vulnerable to noise interference, including baseline drift noise (1&#x02013;4 <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003bc;</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) and industrial frequency interference (0&#x02013;60 Hz). Therefore, filtering and denoising are crucial for extracting target features and enhancing signal quality.</p><p>The primary goal of preprocessing is to minimize industrial frequency noise and other interferences during acquisition, enhance the signal-to-noise ratio (SNR), and retain as much useful information as possible. Filtering is crucial in this process; this involves the design of specific frequency ranges that permit target signals to pass while attenuating unwanted frequencies. Among the software-based filtering techniques, the Butterworth filter is widely used. Renowned for its maximally flat amplitude response, this filter preserves signal integrity while avoiding the amplitude attenuation caused by filtering. Compared to first-order low-pass filters, the Butterworth filter delivers a superior performance and is available in various forms, including high-pass, low-pass, band-pass, and band-stop filters. It demonstrates stable amplitude-frequency characteristics both within and outside the passband, making it highly suitable for sEMG signal filtering. The transfer function of the Butterworth filter is presented in Equation (<xref rid="FD3-sensors-25-01275" ref-type="disp-formula">3</xref>):<disp-formula id="FD3-sensors-25-01275"><label>(3)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mi>&#x003c9;</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mi>&#x003c9;</mml:mi><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfrac></mml:mfenced><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">n</italic> represents the filter order, <italic toggle="yes">j</italic> is the imaginary unit, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:math></inline-formula> is the complex variable in the Laplace transform, and <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the cutoff frequency. This paper adopts a fourth-order Butterworth filter for the band-pass filtering, and the passband boundary is 10&#x02013;500 Hz.</p></sec></sec><sec id="sec2dot3-sensors-25-01275"><title>2.3. Dataset Preparation</title><sec id="sec2dot3dot1-sensors-25-01275"><title>2.3.1. Data Alignment</title><p>To construct a dataset for model training and testing, the collected high-frequency sEMG and IMU signals underwent downsampling, temporal alignment, sliding window segmentation, and feature extraction. The final dataset was split into training, validation, and test sets. This process ensured temporal synchronization, feature consistency, and balanced data splits, establishing a robust foundation for the generalization of subsequent models.</p><p>Because the EMG signal sampling frequency is 2000 Hz, which is significantly higher than the IMU signal sampling frequency of 200 Hz, downsampling of the EMG signals was required to achieve temporal alignment between the two signals. Downsampling the EMG signal inevitably results in the loss of some signal details, but it remains a reasonable and necessary operation. Key features of the EMG signal are extracted prior to downsampling, ensuring that essential information is preserved during the training process. Downsampling substantially reduces the time and computational cost associated with model training, effectively balancing efficiency and accuracy. This is particularly valuable in deep learning scenarios, where computational resources are intensive, underscoring the importance of downsampling [<xref rid="B32-sensors-25-01275" ref-type="bibr">32</xref>]. The downsampling formula is as follows:<disp-formula id="FD4-sensors-25-01275"><label>(4)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>down</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">M</italic> is the downsampling factor (<inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> in this study), <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the original EMG signal, and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>down</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the downsampled signal.</p><p>To ensure temporal alignment between the EMG and IMU signals, timestamps were employed to synchronize the two signals. Let the timestamps for the EMG and IMU signals be denoted as <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>EMG</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>IMU</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. The alignment condition is given by<disp-formula id="FD5-sensors-25-01275"><label>(5)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>EMG</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>IMU</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x0003c;</mml:mo></mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the permissible time error (<inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mspace width="0.166667em"/><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in this study). During alignment, signal samples not meeting the synchronization condition were discarded.</p><p>Signal segmentation was performed using a fixed-length sliding window approach, dividing the time series data into fixed intervals to generate training and testing samples. Let the sliding window length be <italic toggle="yes">L</italic> and the step size be <italic toggle="yes">S</italic>. The <italic toggle="yes">k</italic>-th window can be expressed as follows:<disp-formula id="FD6-sensors-25-01275"><label>(6)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>L</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">L</italic> is the window length, and <italic toggle="yes">S</italic> is the step size (<inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn><mml:mspace width="0.166667em"/><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, corresponding to 40 IMU data points; <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn><mml:mspace width="0.166667em"/><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in this study). The sliding window segmentation generated continuous time segments, with each segment containing temporally aligned EMG and IMU data.</p></sec><sec id="sec2dot3dot2-sensors-25-01275"><title>2.3.2. Feature Extraction from EMG Signals</title><p>To construct prediction targets for the model, this study extracted a variety of time-domain and frequency-domain features from the downsampled EMG signals. These features reflect the intensity, frequency, and variation trends of muscle activity, offering a reliable foundation for assessing muscle activation levels. The extracted features include RMS (Root Mean Square), MVA (Mean Absolute Value), ZC (Zero Crossing), WL (Waveform Length), SSC (Slope Sign Change), and MPF (Mean Power Frequency). The specific formulas for these features are given as follows:<list list-type="order"><list-item><p>Root Mean Square (RMS): Reflects the energy intensity of the signal and is calculated as follows:<disp-formula id="FD7-sensors-25-01275"><label>(7)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>RMS</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>x</mml:mi><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> is the number of samples in the signal, and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the signal amplitude at time <italic toggle="yes">n</italic>.</p></list-item><list-item><p>Mean Absolute Value (MAV): Represents the average level of signal amplitude:<disp-formula id="FD8-sensors-25-01275"><label>(8)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>MAV</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Zero Crossing (ZC): Counts the number of times the signal crosses zero within a given threshold <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></inline-formula>, reflecting the frequency characteristics:<disp-formula id="FD9-sensors-25-01275"><label>(9)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>ZC</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mo>(</mml:mo><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x0003c;</mml:mo><mml:mi>&#x003f5;</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is an indicator function.</p></list-item><list-item><p>Waveform Length (WL): Captures the complexity and rate of change of the signal:<disp-formula id="FD10-sensors-25-01275"><label>(10)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>WL</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Slope Sign Change (SSC): Counts the number of slope changes, reflecting signal fluctuations:<disp-formula id="FD11-sensors-25-01275"><label>(11)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>SSC</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>&#x003b4;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>]</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo><mml:mo>)</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo>[</mml:mo><mml:mi>n</mml:mi><mml:mo>]</mml:mo><mml:mo>)</mml:mo><mml:mo>&#x0003c;</mml:mo><mml:mn>0</mml:mn></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> equals 1 if the condition is met and 0 otherwise.</p></list-item><list-item><p>Mean Power Frequency (MPF): Assesses frequency shifts related to local muscle fatigue:<disp-formula id="FD12-sensors-25-01275"><label>(12)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>MPF</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mi>f</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the power spectral density at frequency <italic toggle="yes">f</italic>.</p></list-item></list></p><p>These features were calculated within each sliding window, and the resulting values were used to construct the prediction targets for the model. These targets not only quantify the magnitude, frequency, and variation trends of muscle activity but also offer valuable input information for optimizing the model&#x02019;s performance.</p></sec><sec id="sec2dot3dot3-sensors-25-01275"><title>2.3.3. Data Segmentation</title><p>After signal processing and feature extraction, the dataset was split into training, testing, and validation sets to ensure the model&#x02019;s robustness and generalization. During the data splitting process, different movements were split independently, with each repetition in each round treated as an independent sample to ensure the diversity and independence of the dataset.</p><p><bold>Data Splitting Principles</bold> To ensure the independence and representativeness of the data, data splitting was carried out according to the following principles:<list list-type="order"><list-item><p><bold>Independent Sample Splitting</bold>: Each participant performed three target movements, each with distinct motion characteristics. During the dataset splitting, each individual signal segment <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> from a repetition was treated as an independent sample and randomly assigned to the dataset. The structure of the dataset can be expressed as follows:<disp-formula id="FD13-sensors-25-01275"><label>(13)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x022c3;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x022c3;</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the signal segment of the <italic toggle="yes">i</italic>-th participant performing the <italic toggle="yes">j</italic>-th movement in the <italic toggle="yes">k</italic>-th round and the <italic toggle="yes">l</italic>-th repetition.</p></list-item><list-item><p><bold>Movement Group Splitting</bold>: Data from different target movements were independently split into training, validation, and testing sets to ensure the model&#x02019;s ability to generalize across distinct motion characteristics. Let the complete dataset be <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:math></inline-formula>, and the splitting is as follows:<disp-formula id="FD14-sensors-25-01275"><label>(14)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="script">D</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.166667em"/><mml:munderover><mml:mo>&#x022c3;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:munderover><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.166667em"/><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>train</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>val</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>test</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>train</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>val</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>test</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the training, validation, and testing sets for movement <italic toggle="yes">j</italic>, respectively.</p></list-item><list-item><p><bold>Proportional Splitting</bold>: All independent samples for each movement were allocated as <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>70</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to the training set, <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to the validation set, and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to the testing set:<disp-formula id="FD15-sensors-25-01275"><label>(15)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="{" close=""><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>train</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.166667em"/><mml:mn>0.70</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>val</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.166667em"/><mml:mn>0.20</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>test</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.166667em"/><mml:mn>0.10</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><bold>Independence Between Rounds and Movements</bold>: Data from different rounds were kept separate during splitting, ensuring that the samples from each round independently entered different dataset subsets.</p></list-item></list></p><p>During the splitting process, the proportions of different groups (healthy volunteers and participants with shoulder injuries) were preserved to ensure representativeness. Additionally, to prevent data leakage, consecutive data segments within the sliding window were confined to the same subset.</p><p><bold>Data Normalization</bold>: To eliminate amplitude differences across participants, all input data were normalized before modeling. Let the original data be denoted as <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and the normalized data as <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">&#x002dc;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The normalization formula is given by:<disp-formula id="FD16-sensors-25-01275"><label>(16)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">&#x002dc;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math></inline-formula> is the mean and <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula> is the standard deviation of the training set. Normalization was based solely on the training set&#x02019;s statistics to avoid information leakage from the validation and testing sets.</p><p><bold>Final Dataset Organization</bold>: Through the data processing workflow described above, a structured dataset was constructed, containing synchronized IMU and EMG data segments, along with corresponding EMG time-domain feature values. The structure of the dataset obtained after data cleaning is shown in <xref rid="sensors-25-01275-f003" ref-type="fig">Figure 3</xref>. The dataset consists of three subsets: the training set <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>train</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the validation set <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>val</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and the testing set <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mi>test</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Each subset is balanced in terms of sample distribution, encompassing different movements and rounds from all participants. This dataset organization effectively captures motion characteristic differences, providing high-quality data for model training and validation, as well as a solid foundation for subsequent modeling and prediction.</p></sec></sec><sec id="sec2dot4-sensors-25-01275"><title>2.4. Public Dataset</title><p>In addition to the dataset collected in this study, the SWIFTIES dataset (Subject and Wearables data for Investigation of Fatiguing Tasks in Extension/Flexion of Shoulder/Elbow) [<xref rid="B33-sensors-25-01275" ref-type="bibr">33</xref>] was incorporated to enhance the generalizability and applicability of the research and model. SWIFTIES is a high-quality dataset focused on fatigue tasks involving shoulder and elbow flexion/extension movements, containing rich physiological and motion data, making it highly suitable for studies on motion analysis and fatigue detection.</p><p>The SWIFTIES dataset includes experimental data from 32 participants (17 males and 15 females), all of whom performed tasks using their dominant hand. The tasks involved shoulder and elbow flexion/extension movements, performed under both static and dynamic conditions at 25% and 45% of Maximum Voluntary Contraction (MVC) force until noticeable fatigue was reached. Each participant completed all combinations of tasks. The experimental design comprehensively covered various motion types, joint areas, and loading conditions, offering diverse scenarios for analyzing muscle activity and movement states.</p><p>Following the data processing workflow established in this study, the SWIFTIES dataset underwent similar procedures, such as data alignment, feature extraction, dataset splitting, and standardization. The processed dataset, as shown in <xref rid="sensors-25-01275-t002" ref-type="table">Table 2</xref>, contains synchronized IMU and EMG data segments, along with EMG time-domain feature values. This dataset provided additional support for the study, effectively enhancing the model&#x02019;s generalization capabilities and practical value.</p></sec></sec><sec id="sec3-sensors-25-01275"><title>3. Network Architecture</title><p>To comprehensively enhance the feature extraction and sequence modeling capabilities of multi-channel IMU time-series data, this study introduces a novel Transformer-based model architecture, SWCTNet. Designed to address the complexity and diversity of time-series data, SWCTNet integrates the strengths of local feature extraction and global sequence modeling, making it particularly suitable for handling multi-channel, nonlinear, and high-dimensional time-series data. The SWCTNet architecture consists of three key modules: the Sliding Window Convolutional Neural Network Block (SW-CNN Block), the Channel-Time Attention Transformer Block (CTAT Block), and the Downstream Task Block.</p><p>The SW-CNN Block utilizes a sliding window mechanism combined with convolutional operations to effectively extract local features from time-series data, capturing short-term patterns and signal variation trends. The CTAT Block, built upon an enhanced multidimensional attention mechanism, focuses on global feature modeling across time steps and channels. The Downstream Task Block is flexibly configured to meet specific application requirements (e.g., classification, regression, or sequence generation), further processing the high-level semantic features output by the CTAT Block to make final predictions or perform generative tasks. The overall architecture of the model is depicted in <xref rid="sensors-25-01275-f004" ref-type="fig">Figure 4</xref>.</p><p>While CNN and attention mechanisms have been widely used in tasks such as EMG gesture recognition, the key innovation of SWCTNet lies in its combination of these mechanisms and a sliding window approach that enhances local feature extraction. Specifically, the SW-CNN Block integrates sliding window segmentation with convolution operations to capture fine-grained temporal patterns from multi-channel time-series data, while the CTAT Block introduces a novel channel-time attention mechanism, effectively modeling global dependencies across both time steps and channels. This unique synergy between local feature extraction and global sequence modeling enables SWCTNet to better handle the complexity and diversity of time-series data in applications like multi-channel IMU data analysis.</p><sec id="sec3dot1-sensors-25-01275"><title>3.1. Sliding Window CNN Block (SW-CNN Block)</title><p>The Sliding Window Convolutional Neural Network Block (SW-CNN Block) in SWCTNet is specifically designed to improve the model&#x02019;s ability to extract local temporal features from multi-channel time-series data, while incorporating inductive biases such as locality and translational invariance into the architecture. These properties are especially crucial for small-scale datasets, as they guide the model to focus on meaningful patterns and minimize the risk of overfitting. By combining a sliding window mechanism with convolutional operations, the SW-CNN Block effectively captures local dependencies, which are critical for both online and offline decoding tasks. Furthermore, the SW-CNN Block enables effective interaction with subsequent Transformer modules by highlighting relevant local temporal features and forwarding them for global dependency modeling. The SW-CNN Block includes the following key steps: sliding window segmentation, one-dimensional convolution (1D-CNN) operations, normalization layers, activation layers, mix pooling operations, and regularization strategies. The architecture of the SW-CNN Block is depicted in <xref rid="sensors-25-01275-f005" ref-type="fig">Figure 5</xref>.</p><p><bold>Sliding Window Partitioning</bold> As a preprocessing step, the sliding window mechanism partitions continuous time-series data into overlapping or non-overlapping local windows. Each window captures a small segment of the signal, allowing for the model to focus on localized temporal features while maintaining the overall structure of the input sequence. The sliding window mechanism also offers a natural form of data augmentation by increasing the number of samples, which is especially beneficial for small datasets. Moreover, this partitioned structure helps isolate transient signal characteristics, enhancing the effectiveness of subsequent convolutional operations.</p><p>For input data with dimensions <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the number of channels and <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> the number of time steps, the sliding window transforms <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula> into <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>window</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>num</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>win</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>num</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of windows and <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>win</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the window length. The formula for sliding window segmentation is given by<disp-formula id="FD17-sensors-25-01275"><label>(17)</label><mml:math id="mm67" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>window</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>S</mml:mi><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>win</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">S</italic> is the sliding window step size, and <italic toggle="yes">i</italic> is the window index.</p><p><bold>Convolutional Layer</bold> Based on the subsequences generated by the sliding window mechanism, the SW-CNN performs one-dimensional convolution operations on these segmented windows, using convolutional kernels to extract high-level temporal features [<xref rid="B34-sensors-25-01275" ref-type="bibr">34</xref>]. The convolution operation slides the kernel along the temporal axis, capturing signal patterns within the window, such as sudden changes, periodic characteristics, or noise variations. The convolution operation is given by<disp-formula id="FD18-sensors-25-01275"><label>(18)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>conv</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ELU</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mi>conv</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02217;</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>window</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>conv</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>conv</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">c</italic> represent the learnable kernel weights, <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>conv</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the bias term, and <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>ELU</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the activation function. The use of activation functions such as Exponential Linear Units (ELU) ensures a smooth response to negative inputs, overcoming the limitations of traditional activation functions like ReLU. The incorporation of ELU improves the model&#x02019;s ability to handle subtle variations in the input data.</p><p><bold>Normalization and Regularization</bold> The SW-CNN module incorporates two regularization strategies, Batch Normalization (BatchNorm) and Window Normalization (WindowNorm), to further improve the model&#x02019;s training stability and generalization capability. The combination of these techniques not only reduces the impact of non-stationary characteristics in the signal on model performance but also offers targeted optimization for varying input distributions and window-specific features.</p><p>BatchNorm is a commonly used normalization strategy in the SW-CNN module, primarily applied to normalize features following convolution operations. The core idea is to normalize each feature channel within a batch to have a mean of 0 and variance of 1, followed by the use of learnable parameters to scale and shift the normalized values [<xref rid="B35-sensors-25-01275" ref-type="bibr">35</xref>]. The formula for BatchNrom is given by<disp-formula id="FD19-sensors-25-01275"><label>(19)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>bn</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>bn</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>bn</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi>y</mml:mi><mml:mi>bn</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>bn</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>bn</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>bn</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">x</italic> represents the input features, <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>bn</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>bn</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the mean and standard deviation of the batch, and <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>bn</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>bn</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the learnable scale and shift parameters.</p><p>WindowNorm is a normalization strategy specifically designed for sliding windows, which aims to normalize the local features within each window. This approach mitigates the influence of local non-stationary properties on signal feature extraction. Unlike BatchNorm, WindowNorm limits its normalization scope to the features within individual sliding windows. The formula for WindowNorm is given by<disp-formula id="FD20-sensors-25-01275"><label>(20)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>win</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>win</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>win</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msub><mml:mi>y</mml:mi><mml:mi>win</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>win</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>win</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mi>win</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>win</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c3;</mml:mi><mml:mi>win</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the mean and standard deviation within the sliding window, and <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b3;</mml:mi><mml:mi>win</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>win</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the learnable scale and shift parameters for the window.</p><p>The SW-CNN module incorporates BatchNorm and WindowNorm, enabling it to adapt to global distribution variations while optimizing for local characteristics. This significantly improves the model&#x02019;s robustness when using small-scale datasets and high-dimensional time-series data.</p><p><bold>MixPooling Layer</bold> To further refine feature maps, the SW-CNN Block incorporates a pooling layer that reduces the spatial dimensions of features, compressing temporal information while retaining the most prominent patterns. A combination of max pooling and average pooling is typically applied, with kernel size and stride configured to effectively downsample the data, reducing the computational cost and preventing overfitting [<xref rid="B36-sensors-25-01275" ref-type="bibr">36</xref>].</p><p><bold>Activation and Regularization</bold> Regularization techniques, such as dropout and batch normalization, are essential components of the SW-CNN Block, ensuring robust training and generalization. Dropout is a regularization technique that randomly drops neurons and their connections during training to reduce reliance on specific neurons, thereby preventing overfitting. Mathematically, for the activation values of any layer <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:math></inline-formula>, the output after applying dropout becomes:<disp-formula id="FD21-sensors-25-01275"><label>(21)</label><mml:math id="mm83" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">h</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="bold">h</mml:mi><mml:mo>&#x02299;</mml:mo><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mi>p</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where &#x02299; denotes element-wise multiplication, <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">r</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>Bernoulli</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a random vector with each element sampled independently from a Bernoulli distribution with probability <italic toggle="yes">p</italic>, and <italic toggle="yes">p</italic> is the dropout rate. A dropout rate of 0.5 is applied to prevent overfitting, while spatial dropout is employed for highly spatially correlated signals, such as multi-channel time-series data (e.g., IMU signals, EMG signals) [<xref rid="B37-sensors-25-01275" ref-type="bibr">37</xref>]. Additionally, a maximum norm constraint is imposed on the weights of each temporal filter, preventing the model from overfitting to noise or irrelevant patterns in the input data.</p><p>The SW-CNN Block plays a crucial role in guiding the Transformer components of SWCTNet by providing high-quality local temporal features. These convolutional features not only capture the most relevant patterns within each sliding window but also lay the foundation for learning global dependencies in subsequent Transformer modules. This synergy between the SW-CNN Block and Transformer ensures that both local and global characteristics of time-series data are effectively captured, resulting in improved performance across downstream tasks such as classification, regression, or sequence generation.</p></sec><sec id="sec3dot2-sensors-25-01275"><title>3.2. Channel-Time Attention Transformer Block (CTAT Block)</title><p>The Channel-Time Attention Transformer Block (CTAT Block) in SWCTNet is a key module designed to capture global features and long-range dependencies in multi-channel time-series data. By introducing an enhanced self-attention mechanism, the CTAT Block identifies correlations between time steps and interactions across channels, generating high-level semantic feature representations. The collaboration between the CTAT Block and the SW-CNN Block forms the core of SWCTNet&#x02019;s feature extraction capabilities. The specific structure of the module is shown in <xref rid="sensors-25-01275-f006" ref-type="fig">Figure 6</xref>.</p><p>The input to the CTAT Block comes from the feature map output by the SW-CNN Block, represented as <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mi>cnn</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">k</italic> denotes the number of sliding windows, <italic toggle="yes">H</italic> is the number of channels, and <italic toggle="yes">L</italic> represents the length of the time steps. To meet the input requirements of Transformers, the feature map is reshaped to flatten its dimensions, resulting in <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mi>flat</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which combines features from all channels and windows into a unified input sequence. This transformation provides the foundation for global modeling, enabling the Transformer to focus on both inter-channel and inter-time-step information.</p><p>The Channel-Time Attention Mechanism is the core component of the CTAT Block. Built on an improved self-attention mechanism, it consists of two parallel computational paths that focus on capturing dependencies both across channels and over time.</p><p>The first is the channel-dominant path, which aims to model the correlations across channels. Specifically, the attention weight matrix <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is computed by taking the inner product of the Query and Key, followed by a Softmax operation, as shown by the following:<disp-formula id="FD22-sensors-25-01275"><label>(22)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">K</mml:mi><mml:mi>c</mml:mi><mml:mo>&#x022a4;</mml:mo></mml:msubsup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is set to one-fourth of the number of channels to reduce computational complexity.</p><p>The second is the time-dominant path, which models long-range temporal dependencies. The attention weight matrix <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is computed by taking the inner product of the Query and Key along the time dimension, followed by a Softmax operation, as expressed by<disp-formula id="FD23-sensors-25-01275"><label>(23)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">K</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x022a4;</mml:mo></mml:msubsup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">Q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is chosen to be proportional to the original time dimension <italic toggle="yes">L</italic> to ensure sufficient modeling of temporal information.</p><p>Finally, the outputs from the two paths are fused using a gating mechanism, which dynamically adjusts the contribution of each attention path based on the input data. The fused attention matrix <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>attn</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is computed as follows:<disp-formula id="FD24-sensors-25-01275"><label>(24)</label><mml:math id="mm96" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>attn</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where the gating coefficient <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></inline-formula> is computed via a learnable sigmoid function as follows:<disp-formula id="FD25-sensors-25-01275"><label>(25)</label><mml:math id="mm98" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mo>=</mml:mo><mml:mi>sigmoid</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a learnable weight matrix. This gating mechanism allows the model to dynamically adjust the contribution of each attention path to the final attention map, thereby optimizing the feature learning process.</p><p>To further improve the model&#x02019;s ability to learn multi-dimensional dependencies, the CTAT Block employs a multi-head attention mechanism, extracting diverse feature subspaces through multiple parallel attention heads. The output of the multi-head attention is combined with the input features using residual connections, followed by layer normalization to stabilize training and enhance optimization efficiency. The fused feature representation is computed as follows:<disp-formula id="FD26-sensors-25-01275"><label>(26)</label><mml:math id="mm100" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>LayerNorm</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>attn</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>flat</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To address the absence of positional information in time-series data, the CTAT Block incorporates learnable positional encoding [<xref rid="B38-sensors-25-01275" ref-type="bibr">38</xref>]. By combining positional encoding with attention weight matrices, the Transformer captures the sequential characteristics of the time-series data more effectively. The final input sequence is formed by adding feature embeddings (Patch Embeddings) and positional encoding, further improving the model&#x02019;s perception of sequential data.</p><p>By introducing the channel-time attention mechanism, the CTAT Block allows for the more effective global modeling of multi-channel time-series data. Combined with the local features extracted by the SW-CNN Block, the CTAT Block plays a key role in constructing high-quality global feature representations, providing strong support for accurate predictions in downstream tasks.</p></sec><sec id="sec3dot3-sensors-25-01275"><title>3.3. Downstream Task Block</title><p>The Downstream Task Block is the final component of SWCTNet, designed to process features from the CTAT Block and produce the final task-specific outputs. The evaluation of shoulder rehabilitation systems often involves a combination of diverse tasks, including predicting the types of rehabilitation movements, assessing the accuracy of the movement execution, and generating evaluation metrics based on standard movement patterns. Therefore, this block is highly flexible and can be adapted to various tasks, including classification, regression, and sequence generation. With its configurable architecture, the Downstream Task Block enables SWCTNet to be applied in a variety of task scenarios.</p><p>The input to the Downstream Task Block consists of global feature representations from the CTAT Block, denoted as <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">N</italic> is the length of the feature sequence and <italic toggle="yes">d</italic> is the feature dimension. These features integrate the local features extracted by the SW-CNN Block and the global dependencies modeled by the CTAT Block, containing rich spatiotemporal information and inter-channel interactions.</p><p>The Downstream Task Block adopts various architectures tailored to specific task objectives. For classification tasks, the block is configured as a Multi-Layer Perceptron (MLP) head to produce class probability distributions. For regression tasks, it uses a linear mapping to predict target values. In sequence generation tasks, a decoder architecture is employed to generate output sequences. The configurations for each task type are outlined as follows:<list list-type="order"><list-item><p><bold>Classification Tasks</bold> In classification tasks, the Downstream Task Block employs a fully connected layer to map the global features to the class space, followed by a softmax activation function to produce class probabilities. The mathematical expression is given by<disp-formula id="FD27-sensors-25-01275"><label>(27)</label><mml:math id="mm102" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>fc</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>fc</mml:mi></mml:msub></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>fc</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>fc</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the weights and biases of the fully connected layer, respectively.</p></list-item><list-item><p><bold>Feature Prediction Tasks</bold> For prediction tasks, the module directly outputs predictions through a linear layer:<disp-formula id="FD28-sensors-25-01275"><label>(28)</label><mml:math id="mm105" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>reg</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>reg</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>reg</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>reg</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the weights and biases of the regression model. This setup is ideal for tasks such as cross-modal feature mapping and feature prediction.</p></list-item><list-item><p><bold>Sequence Generation Tasks</bold> In sequence generation tasks, the Downstream Task Block employs a decoder architecture to iteratively convert input features into output sequences. The decoder generates the next time-step&#x02019;s result based on the current input and previous outputs in an autoregressive manner. The expression is given by<disp-formula id="FD29-sensors-25-01275"><label>(29)</label><mml:math id="mm108" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Decoder</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi>fused</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>&#x003b8;</mml:mi></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the previous output, <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mi>fused</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the global feature, and <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> denotes the decoder parameters.</p></list-item></list></p><p>To prevent overfitting and improve model generalization, the Downstream Task Block uses dropout regularization. The dropout rate is set to 0.5 for classification and regression tasks and 0.7 for sequence generation tasks, depending on task complexity. Additionally, a maximum norm constraint of 0.25 is imposed on the weights in each layer to reduce overfitting to noise or irrelevant features.</p><p>The modular design of the Downstream Task Block allows for its seamless adaptation to various task objectives. The classification configuration emphasizes class distinction, making it ideal for recognizing fixed patterns. The regression configuration focuses on continuous value prediction, making it suitable for cross-modal feature mapping or time-series forecasting. The sequence generation configuration supports the generation of complex sequential outputs. By inheriting global features from the CTAT Block and leveraging task-specific designs, the Downstream Task Block enhances predictive capabilities, ensuring an excellent performance across diverse task scenarios.</p></sec></sec><sec id="sec4-sensors-25-01275"><title>4. Experiments and Results</title><p>Based on the model structure and downstream tasks defined above, three groups of experiments were conducted in this study:<list list-type="order"><list-item><p><bold>Classification Task</bold>: Conducted on the publicly available datasets DB1/DB2/DBA/SWIFITS and the self-constructed dataset, focusing on posture recognition tasks.</p></list-item><list-item><p><bold>Cross-Modal Feature Prediction Task</bold>: Conducted on the SWIFTIES dataset and the self-constructed dataset, aiming to predict EMG features from IMU data.</p></list-item><list-item><p><bold>Cross-Modal Time-Series Generation Task</bold>: Conducted on the SWIFTIES dataset and the self-constructed dataset, focusing on generating EMG time-series data from IMU inputs.</p></list-item></list></p><sec id="sec4dot1-sensors-25-01275"><title>4.1. Classification Task</title><p>This study proposes a multi-time-series model architecture primarily designed to integrate and analyze multi-channel IMU time-series data for predicting SJ EMG features. To validate the model&#x02019;s effectiveness and robustness across various multi-time-series tasks, we conducted experiments comparing its performance with baseline models in shoulder motion and posture recognition tasks. For this evaluation, we used three publicly available datasets: NinaPro DB1 (DB1) [<xref rid="B39-sensors-25-01275" ref-type="bibr">39</xref>], NinaPro DB2 (DB2) [<xref rid="B40-sensors-25-01275" ref-type="bibr">40</xref>], and CapgMyo DB-a (DBA) [<xref rid="B41-sensors-25-01275" ref-type="bibr">41</xref>]. Each dataset contains data from different numbers of participants and experimental trials.</p><p>Specifically:<list list-type="order"><list-item><p><bold>DB1 (Ninapro DB1)</bold>: Contains sEMG and kinematic data from 27 participants performing 52 hand movements and a resting position. sEMG signals were recorded using 10 Otto Bock MyoBock electrodes and kinematic data via Cyberglove 2. Movements included finger gestures, isometric/isotonic hand positions, and functional tasks. Each participant completed 10 repetitions, with synchronized sEMG, glove sensor signals, and labeled stimuli.</p></list-item><list-item><p><bold>DB2 (Ninapro DB2)</bold>: Features sEMG, kinematic, inertial, and force data from 40 participants performing 49 hand movements and a resting position. Data were recorded with 12 Delsys Trigno electrodes, Cyberglove 2, accelerometers, and a custom force sensor. The study included finger gestures, grasping motions, and force patterns, with six repetitions per movement.</p></list-item><list-item><p><bold>DBA (CapgMyo DB-a)</bold>: Includes HD-sEMG signals from 18 participants using a 128-channel electrode array (<inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) to capture eight hand gestures. This dataset focuses on high-density sEMG and real-time signal imaging.</p></list-item></list></p><p>A detailed summary of the dataset attributes is provided in <xref rid="sensors-25-01275-t003" ref-type="table">Table 3</xref>.</p><p>The evaluation metrics for the shoulder motion and posture recognition task included class recognition accuracy and overall classification accuracy. Class recognition accuracy is defined as the ratio of correct predictions to the total number of predictions for each category, expressed as follows:<disp-formula id="FD30-sensors-25-01275"><label>(30)</label><mml:math id="mm113" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Number</mml:mi><mml:mspace width="4.pt"/><mml:mi>of</mml:mi><mml:mspace width="4.pt"/><mml:mi>correct</mml:mi><mml:mspace width="4.pt"/><mml:mi>predictions</mml:mi></mml:mrow><mml:mrow><mml:mi>Total</mml:mi><mml:mspace width="4.pt"/><mml:mi>number</mml:mi><mml:mspace width="4.pt"/><mml:mi>of</mml:mi><mml:mspace width="4.pt"/><mml:mi>predictions</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x000d7;</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The overall mean recognition accuracy (<inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.166667em"/><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>) is defined as follows:<disp-formula id="FD31-sensors-25-01275"><label>(31)</label><mml:math id="mm115" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.166667em"/><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> represents the total number of participants. By analyzing these metrics, we evaluated the model&#x02019;s ability to classify SJ motion postures accurately.</p><p>The proposed SWCTNet model showed an exceptional performance in multi-channel time-series classification tasks. To evaluate its effectiveness and robustness in shoulder motion and posture recognition tasks, we conducted experiments using three publicly available datasets (DB1, DB2, and DBA), SWIFITS, and the Personal Dataset. The model&#x02019;s classification performance was thoroughly compared with several baseline models, as summarized in <xref rid="sensors-25-01275-t004" ref-type="table">Table 4</xref>.</p><p>SWCTNet achieved high accuracy across datasets, with 87.93% on DB1, 88.75% on DB2, and 91.03% on DBA, outperforming several baseline models. On DB1, it effectively balanced local feature extraction via SW-CNN with global dependency modeling through CTAT, outperforming models like CNN-RNN. For DB2, the model&#x02019;s stability and efficiency in handling complex multi-channel signals surpass state-of-the-art methods, including multi-scale Bi-LSTM and TDCT. On DBA, it excelled in high-dimensional tasks, outperforming methods such as multi-stream CNN and Wavelet Transformer.</p><p>For benchmark datasets like SWIFTIES (99.75%), the model exhibited exceptional generalization and adaptability, while on the Personal Dataset (98.00%), it demonstrated superior transferability in personalized scenarios. By integrating local and global feature modeling, the SWCTNet model offers a robust and efficient solution for diverse multi-channel time-series classification tasks, setting new benchmarks for accuracy and robustness.</p><p>The SWCTNet excelled in time-series classification, delivering superior accuracy across datasets. By integrating local feature extraction via SW-CNN with global modeling through CTAT, it generated semantically rich representations. This modular design ensures robustness and adaptability, excelling on public datasets, in personalized tasks, and across diverse scenarios. Its performance surpasses state-of-the-art methods, providing a robust foundation for multi-channel analysis and future applications, while delivering efficient solutions for complex time-series tasks with strong generalization capabilities.</p></sec><sec id="sec4dot2-sensors-25-01275"><title>4.2. Ablation Study</title><p>To further investigate the contribution of individual components within the proposed SWCTNet architecture, we conducted an ablation study. The study focused on analyzing the roles of the SW-CNN Block and CTAT Block in overall model performance. The ablation experiments involved systematically removing or modifying specific components to create different variants of the model. The variants included the Transformer Baseline Model (M1), a model with the SW-CNN Block combined with the base Transformer (M2), a model with the CTAT Block combined with the base Transformer (M3), and a model integrating both the SW-CNN Block and the CTAT Block with the base Transformer (M4). Each variant was evaluated on the SJ motion posture recognition task using the same datasets and metrics, allowing for a direct comparison of how each component affected performance. The experimental results for the classification tasks and the ablation study are summarized in <xref rid="sensors-25-01275-f007" ref-type="fig">Figure 7</xref>, offering insights into the contributions of individual components to overall model performance.</p><p>The experimental results emphasize the effectiveness of combining the SW-CNN and CTAT Blocks in enhancing classification performance. The baseline model (M1) achieved limited accuracy (e.g., 80.05% on DB1 and 98.31% on SWIFTIES), highlighting the inadequacy of pure global modeling for capturing local features and inter-channel interactions in complex multi-channel time-series data. Incorporating the SW-CNN Block (M2) significantly improved performance by enhancing local feature extraction. For instance, accuracy on DBA increased from 87.39% (M1) to 89.95%, and on SWIFTIES, it improved to 98.92%. The SW-CNN Block effectively captures local dependencies and temporal patterns, demonstrating excellence in fine-grained feature recognition. Integrating the CTAT Block (M3) further enhanced accuracy across datasets by improving global dependency modeling. On DB2, accuracy rose from 80.53% (M1) to 83.91%, and on SWIFTIES, it improved to 98.89%. The CTAT Block leverages channel-time attention to model inter-channel interactions, significantly improving performance on complex signals. The complete model (M4), which combines SW-CNN and CTAT Blocks, achieved the highest accuracy, at 91.03% on DBA and 99.75% on SWIFTIES. This synergy between local and global modeling ensures superior classification accuracy and robustness, enabling the effective handling of diverse and complex time-series tasks.</p><p>The ablation study results clearly demonstrate the critical roles of the SW-CNN Block and CTAT Block in local feature extraction and global dependency modeling, respectively. The SW-CNN Block captures local signal features through sliding window convolution, significantly enhancing the model&#x02019;s sensitivity to fine-grained characteristics. Meanwhile, the CTAT Block strengthens global dependency modeling across channels and time steps via the channel-time attention mechanism. The combination of these modules enables the model to efficiently handle complex multi-channel time-series data, enhancing both classification accuracy and robustness.</p></sec><sec id="sec4dot3-sensors-25-01275"><title>4.3. Feature Forecast Task</title><p>To evaluate the model&#x02019;s performance in feature prediction, we employed a widely used metric: Root Mean Squared Error (RMSE). RMSE is the square root of the average squared differences between the predicted and true values. A smaller RMSE indicates that the model&#x02019;s predictions are closer to the observed values. RMSE is calculated as follows:<disp-formula id="FD32-sensors-25-01275"><label>(32)</label><mml:math id="mm116" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>RMSE</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the true value, <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the predicted value, and <italic toggle="yes">n</italic> is the number of samples.</p><p>Based on the sliding window design of the SW-Block, we segmented the aligned EMG data into smaller segments according to the window size. The segmented EMG time-series signals were then preprocessed to compute feature values. These feature values served as inputs to the Decoder, while the encoder&#x02013;decoder structure of the model was used to predict future feature values. For different pattern categories, the predicted feature values were compared with the true values to compute the RMSE. This evaluation process ensured that the model accurately captured and predicted future EMG feature values across various motion patterns. The radar chart in <xref rid="sensors-25-01275-f008" ref-type="fig">Figure 8</xref> offers a visual comparison of the model&#x02019;s performance in the feature prediction task.</p><p>In the feature prediction task, aligned EMG data were segmented into multiple time windows according to the SW-CNN Block&#x02019;s sliding window mechanism. Each window&#x02019;s signals were preprocessed to compute various feature values, including RMS (Root Mean Square), MAV (Mean Absolute Value), ZC (Zero Crossing Rate), WL (Waveform Length), SSC (Slope Sign Change), and MPF (Mean Power Frequency). These features reflect the signal&#x02019;s amplitude, frequency, and dynamic variation characteristics.</p><p>The model employs an encoder&#x02013;decoder structure to process input features. The encoder captures both local and global patterns from historical features, while the decoder uses these encoded features to predict future feature values. Discrepancies between predicted and true values are quantified using RMSE.</p><p>To provide an intuitive representation of prediction performance, the feature errors were normalized and visualized via radar charts. <xref rid="sensors-25-01275-f008" ref-type="fig">Figure 8</xref>a,b show the feature prediction results for the SWIFTIES dataset and the personal dataset, respectively. In the radar charts, different feature values (e.g., RMS, MAV, ZC) and muscle groups (e.g., Deltoid, Trapezius, Bicep) were normalized to the range [0, 1], where values closer to the circular boundary indicate fewer errors.</p><p>On the SWIFTIES dataset, the model exhibited the fewest prediction errors for RMS and MAV features, indicating high accuracy when capturing signal amplitude characteristics. This demonstrates the model&#x02019;s ability to accurately represent the overall strength and energy of the signal. In contrast, there were more prediction errors for ZC and MPF, indicating that frequency-related features are harder to predict, likely due to transient variations and noise interference. Additionally, prediction performance varied across muscle groups. For example, the Trapezius and Bicep muscles exhibited a better prediction performance than others, likely due to the increased signal stability.</p><p>On the personal dataset, the model&#x02019;s overall prediction performance differed from its performance on the SWIFTIES dataset. Specifically, RMS and MAV prediction errors increased, while ZC and MPF errors slightly decreased. This indicates that amplitude-related features are harder to model on the personal dataset, whereas frequency features demonstrate better adaptability.</p><p>Variations in dataset characteristics drive differences in predictions. The personal dataset&#x02019;s consistent signal acquisition improved its frequency feature predictions (e.g., MPF, ZC) but the model struggled with diverse amplitude features, such as RMS and MAV, compared to the SWIFTIES dataset. Prediction also varied by muscle group; the Bicep and Tricep muscles exhibited better results, while the Latissimus Dorsi showed higher errors in WL and MPF. The model&#x02019;s sliding window and multi-channel approach effectively captured amplitude, frequency, and dynamic characteristics, ensuring a robust performance across datasets. However, optimizing predictions for specific features and muscle groups under varying conditions remains a key focus for future improvements.</p></sec><sec id="sec4dot4-sensors-25-01275"><title>4.4. Sequence Generation Task</title><p>To demonstrate the effectiveness of our proposed model in multi-time-series sequence generation tasks, we conducted experiments using a personal dataset and the SWIFITS dataset. In these experiments, we fed the actual EMG time-series signals directly into the model as input. The model&#x02019;s encoder&#x02013;decoder structure was then used to predict future EMG time-series signals. To evaluate the model&#x02019;s performance, we employed Dynamic Time Warping (DTW) as an evaluation metric.</p><p>DTW is a technique for measuring the similarity between two sequences by finding their optimal alignment. The DTW output represents the distance between two sequences, with a smaller distance indicating higher similarity. The formula for DTW is given by<disp-formula id="FD33-sensors-25-01275"><label>(33)</label><mml:math id="mm119" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>DTW</mml:mi><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">&#x000af;</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">&#x000af;</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the two sequences, and <italic toggle="yes">n</italic> represents the length of the sequences. Since the path lengths between different signal pairs may be different, to make the DTW distance comparable, the DTW distance is normalized by dividing the path length, as shown in Equation (<xref rid="FD34-sensors-25-01275" ref-type="disp-formula">34</xref>):<disp-formula id="FD34-sensors-25-01275"><label>(34)</label><mml:math id="mm122" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Normalized</mml:mi><mml:mspace width="4.pt"/><mml:mi>DTW</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>DTW</mml:mi><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">n</italic> is the length of the sequences.</p><p>By employing DTW as evaluation metric, we assessed the model&#x02019;s ability to generate accurate future EMG time-series signals. These metrics offered insights into the similarity and correlation between the predicted and actual sequences. The visualization in <xref rid="sensors-25-01275-f009" ref-type="fig">Figure 9</xref> depicts the sequence generation task, presenting partial computational results of time-domain (RMS) and frequency-domain (MPF) features for both the actual and predicted EMG time-series signals.</p><p>The figures demonstrate that the model effectively captures overall feature trends, though notable differences exist among different data types. As shown in <xref rid="sensors-25-01275-f009" ref-type="fig">Figure 9</xref>a,c, the model exhibits a superior performance in predicting RMS and MPF features for healthy individuals. The predicted values closely align with the true signals, exhibiting minimal uncertainty and highlighting the model&#x02019;s strong adaptability to healthy data. For the RMS feature (<xref rid="sensors-25-01275-f009" ref-type="fig">Figure 9</xref>a), the low normalized DTW distance as 0.12 reflects a high level of alignment between predicted and true signals. These results can be attributed to the smoother signal distribution, lower noise levels, and higher correlations in healthy individual data. The model utilizes these characteristics to achieve higher predictive accuracy.</p><p>In contrast, the RMS and MPF feature predictions for patient data (<xref rid="sensors-25-01275-f009" ref-type="fig">Figure 9</xref>b,d) exhibit more uncertainty, particularly for the MPF feature (<xref rid="sensors-25-01275-f009" ref-type="fig">Figure 9</xref>d). Patient EMG data often contain more transient changes and irregular fluctuations, resulting in higher DTW distances. For instance, in RMS feature prediction (<xref rid="sensors-25-01275-f009" ref-type="fig">Figure 9</xref>b), while the model captures the overall trend, notable deviations exist in local variations, such as signal peaks. The MPF feature prediction (<xref rid="sensors-25-01275-f009" ref-type="fig">Figure 9</xref>d) demonstrates an even greater error range, underscoring the challenges the model faces in modeling frequency-related features. This phenomenon likely arises from the more complex signal characteristics in patient data, including non-stationary muscle activity, abnormal waveforms, and higher noise levels. Additionally, frequency features in patient data are significantly affected by pathological conditions, complicating the model&#x02019;s ability to capture consistent patterns and thereby reducing its predictive performance.</p><p>The experimental results show that the proposed SWCTNet model performs well in time-series prediction tasks on the personal dataset, but its adaptability differs between healthy and patient data. The predictions for healthy data are more accurate, reflecting the model&#x02019;s strong ability to handle stationary signals. However, the larger RMS and MPF errors for patient data suggest that complex fluctuations pose greater challenges to the model&#x02019;s robustness. Future optimization efforts should focus on feature modeling and dynamic prediction for patient data to enhance the model&#x02019;s adaptability and generalization across diverse tasks.</p></sec><sec sec-type="discussion" id="sec4dot5-sensors-25-01275"><title>4.5. Discussion</title><p>The proposed SWCTNet model demonstrates an exceptional performance in predicting EMG signals from IMU data by effectively capturing both local and global features through its innovative integration of the SW-CNN and CTAT modules. The SW-CNN module extracts local temporal features via sliding window convolution, while the CTAT module employs channel-time attention mechanisms to model the global dependencies across channels and time steps. This synergy enables the robust analysis of multi-channel time-series data, yielding high accuracy for amplitude-related features (e.g., RMS, MAV), as validated by radar charts and time-series generation results that show low errors and strong correlations with actual EMG signals.</p><p>A notable strength of SWCTNet is its ability to seamlessly integrate local and global modeling, consistently outperforming baseline models in classification and prediction tasks. Its modular design also allows for it to be adapted to personalized applications across diverse multi-channel datasets. However, while the model performs well with healthy datasets&#x02014;characterized by stable muscle activation patterns and strong IMU-to-EMG correlations&#x02014;it encounters challenges with patient data. The increased variability and noise in patient signals lead to higher prediction errors for frequency-related features (e.g., MPF and SSC), highlighting the need for advanced denoising techniques and enhanced frequency-domain modeling to address signal non-stationarity. Moreover, further optimization of computational efficiency and lightweight implementation are necessary to allow for real-time applications on portable devices.</p><p>Future research directions include integrating multi-modal data, enhancing frequency-domain modeling through methods such as wavelet transforms or FFT, developing task-specific submodels for personalized predictions, and optimizing hardware adaptation for wearable devices. These enhancements will further solidify the robustness and adaptability of SWCTNet for multi-channel time-series analysis.</p></sec><sec sec-type="conclusions" id="sec4dot6-sensors-25-01275"><title>4.6. Conclusions</title><p>This study introduces a novel model architecture based on SW-CNN and a CTAT-Transformer that successfully predicts EMG signal features from multi-channel IMU data. The proposed framework offers a new approach for intelligent evaluation and the personalized rehabilitation of shoulder motion systems. Notably, the findings demonstrate that muscle activation patterns can be accurately predicted using only portable, cost-effective, and easy-to-configure IMU wearable devices&#x02014;eliminating the need for complex EMG measurement setups. This innovation substantially lowers the barriers for rehabilitation monitoring, providing a convenient and practical solution for home-based rehabilitation. With further improvements in patient data adaptation and multi-modal data integration, the method holds great promise for widespread application in intelligent rehabilitation, motion monitoring, and medical signal processing.</p></sec></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, A.B., H.S., and H.J.; methodology, Y.W., S.D., and H.J.; investigation, A.B., H.S., and Y.W.; data curation, A.B., H.S. and H.J.; writing&#x02014;original draft preparation, A.B.; writing&#x02014;review and editing, H.J., G.F., and S.D.; supervision, G.F.; funding acquisition, G.F. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>All subjects gave their informed consent for inclusion before they participated in the study. The study was conducted in accordance with the Declaration of Helsinki, and the protocol was approved by the Ethics Committee of Human Research, The Second Affiliated Hospital, Zhejiang University School of Medicine (protocol code: 20240041, date of approval: 9 January 2024).</p></notes><notes><title>Informed Consent Statement</title><p>All subjects gave informed consent to participate in this investigation after a full explanation of the study design and experimental procedures.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available on request from the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">AD</td><td align="left" valign="middle" rowspan="1" colspan="1">Analog-to-Digital</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Bicep</td><td align="left" valign="middle" rowspan="1" colspan="1">Biceps</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Del</td><td align="left" valign="middle" rowspan="1" colspan="1">Deltoid</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">EMG</td><td align="left" valign="middle" rowspan="1" colspan="1">Electromyography</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FC</td><td align="left" valign="middle" rowspan="1" colspan="1">Fully Connected</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GCN</td><td align="left" valign="middle" rowspan="1" colspan="1">Graph Convolutional Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GRU</td><td align="left" valign="middle" rowspan="1" colspan="1">Gated Recurrent Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IMU</td><td align="left" valign="middle" rowspan="1" colspan="1">Inertial Measurement Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Lat</td><td align="left" valign="middle" rowspan="1" colspan="1">Latissimus Dorsi</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LSTM</td><td align="left" valign="middle" rowspan="1" colspan="1">Long Short-Term Memory</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MAE</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Absolute Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MPF</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Power Frequency</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MVA</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Absolute Value</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ReLU</td><td align="left" valign="middle" rowspan="1" colspan="1">Rectified Linear Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RMS</td><td align="left" valign="middle" rowspan="1" colspan="1">Root Mean Square</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RMSE</td><td align="left" valign="middle" rowspan="1" colspan="1">Root Mean Square Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Recurrent Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SAIS</td><td align="left" valign="middle" rowspan="1" colspan="1">Subacromial Impingement Syndrome</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SD</td><td align="left" valign="middle" rowspan="1" colspan="1">Scapular Dyskinesis</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">sEMG</td><td align="left" valign="middle" rowspan="1" colspan="1">Surface Electromyography</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SNR</td><td align="left" valign="middle" rowspan="1" colspan="1">Signal-to-Noise Ratio</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SJ</td><td align="left" valign="middle" rowspan="1" colspan="1">Shoulder Joint</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">SSC</td><td align="left" valign="middle" rowspan="1" colspan="1">Slope Sign Change</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Trap</td><td align="left" valign="middle" rowspan="1" colspan="1">Trapezius</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Tricep</td><td align="left" valign="middle" rowspan="1" colspan="1">Triceps</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WL</td><td align="left" valign="middle" rowspan="1" colspan="1">Waveform Length</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ZC</td><td align="left" valign="middle" rowspan="1" colspan="1">Zero Crossing</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-01275"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Luime</surname><given-names>J.J.</given-names></name>
<name><surname>Koes</surname><given-names>B.W.</given-names></name>
<name><surname>Hendriksen</surname><given-names>I.J.M.</given-names></name>
<name><surname>Burdorf</surname><given-names>A.</given-names></name>
<name><surname>Verhagen</surname><given-names>A.P.</given-names></name>
<name><surname>Miedema</surname><given-names>H.S.</given-names></name>
<name><surname>Verhaar</surname><given-names>J.A.N.</given-names></name>
</person-group><article-title>Prevalence and Incidence of Shoulder Pain in the General Population; A Systematic Review</article-title><source>Scand. J. Rheumatol.</source><year>2004</year><volume>33</volume><fpage>73</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1080/03009740310004667</pub-id><pub-id pub-id-type="pmid">15163107</pub-id>
</element-citation></ref><ref id="B2-sensors-25-01275"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Matsen</surname><given-names>F.A.</given-names></name>
<name><surname>Lippitt</surname><given-names>S.B.</given-names></name>
</person-group><source>Shoulder Surgery: Principles and Procedures</source><edition>1st ed.</edition><publisher-name>Saunders</publisher-name><publisher-loc>Philadelphia, PA, USA</publisher-loc><year>2003</year></element-citation></ref><ref id="B3-sensors-25-01275"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Giovannetti De Sanctis</surname><given-names>E.</given-names></name>
<name><surname>Ciolli</surname><given-names>G.</given-names></name>
<name><surname>Mocini</surname><given-names>F.</given-names></name>
<name><surname>Cerciello</surname><given-names>S.</given-names></name>
<name><surname>Maccauro</surname><given-names>G.</given-names></name>
<name><surname>Franceschi</surname><given-names>F.</given-names></name>
</person-group><article-title>Evaluation of the Range of Motion of Scapulothoracic, Acromioclavicular and Sternoclavicular Joints: State of the Art</article-title><source>Shoulder Elb.</source><year>2023</year><volume>15</volume><fpage>132</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1177/17585732221090226</pub-id><pub-id pub-id-type="pmid">37035616</pub-id>
</element-citation></ref><ref id="B4-sensors-25-01275"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Karduna</surname><given-names>A.R.</given-names></name>
<name><surname>Williams</surname><given-names>G.R.</given-names></name>
<name><surname>Williams</surname><given-names>J.L.</given-names></name>
<name><surname>Iannotti</surname><given-names>J.P.</given-names></name>
</person-group><article-title>Kinematics of the Glenohumeral Joint: Influences of Muscle Forces, Ligamentous Constraints, and Articular Geometry</article-title><source>J. Orthop. Res. Off. Publ. Orthop. Res. Soc.</source><year>1996</year><volume>14</volume><fpage>986</fpage><lpage>993</lpage><pub-id pub-id-type="doi">10.1002/jor.1100140620</pub-id><pub-id pub-id-type="pmid">8982143</pub-id>
</element-citation></ref><ref id="B5-sensors-25-01275"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yamaguchi</surname><given-names>K.</given-names></name>
<name><surname>Sher</surname><given-names>J.S.</given-names></name>
<name><surname>Andersen</surname><given-names>W.K.</given-names></name>
<name><surname>Garretson</surname><given-names>R.</given-names></name>
<name><surname>Uribe</surname><given-names>J.W.</given-names></name>
<name><surname>Hechtman</surname><given-names>K.</given-names></name>
<name><surname>Neviaser</surname><given-names>R.J.</given-names></name>
</person-group><article-title>Glenohumeral Motion in Patients with Rotator Cuff Tears: A Comparison of Asymptomatic and Symptomatic Shoulders</article-title><source>J. Shoulder Elb. Surg.</source><year>2000</year><volume>9</volume><fpage>6</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/S1058-2746(00)90002-8</pub-id></element-citation></ref><ref id="B6-sensors-25-01275"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamnik</surname><given-names>H.</given-names></name>
<name><surname>Spevak</surname><given-names>M.K.</given-names></name>
</person-group><article-title>Shoulder Pain and Disability Index: Validation of Slovene Version</article-title><source>Int. J. Rehabil. Res.</source><year>2008</year><volume>31</volume><fpage>337</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1097/MRR.0b013e3282fcae09</pub-id><pub-id pub-id-type="pmid">19008683</pub-id>
</element-citation></ref><ref id="B7-sensors-25-01275"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Largacha</surname><given-names>M.</given-names></name>
<name><surname>Parsons</surname><given-names>I.M.</given-names></name>
<name><surname>Campbell</surname><given-names>B.</given-names></name>
<name><surname>Titelman</surname><given-names>R.M.</given-names></name>
<name><surname>Smith</surname><given-names>K.L.</given-names></name>
<name><surname>Matsen</surname><given-names>F.</given-names></name>
</person-group><article-title>Deficits in Shoulder Function and General Health Associated with Sixteen Common Shoulder Diagnoses: A Study of 2674 Patients</article-title><source>J. Shoulder Elb. Surg.</source><year>2006</year><volume>15</volume><fpage>30</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.jse.2005.04.006</pub-id></element-citation></ref><ref id="B8-sensors-25-01275"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Escamilla</surname><given-names>R.F.</given-names></name>
<name><surname>Yamashiro</surname><given-names>K.</given-names></name>
<name><surname>Paulos</surname><given-names>L.</given-names></name>
<name><surname>Andrews</surname><given-names>J.R.</given-names></name>
</person-group><article-title>Shoulder Muscle Activity and Function in Common Shoulder Rehabilitation Exercises</article-title><source>Sport. Med.</source><year>2009</year><volume>39</volume><fpage>663</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.2165/00007256-200939080-00004</pub-id><pub-id pub-id-type="pmid">19769415</pub-id>
</element-citation></ref><ref id="B9-sensors-25-01275"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mekruksavanich</surname><given-names>S.</given-names></name>
<name><surname>Jitpattanakul</surname><given-names>A.</given-names></name>
</person-group><article-title>A Residual Deep Learning Method for Accurate and Efficient Recognition of Gym Exercise Activities Using Electromyography and IMU Sensors</article-title><source>Appl. Syst. Innov.</source><year>2024</year><volume>7</volume><elocation-id>59</elocation-id><pub-id pub-id-type="doi">10.3390/asi7040059</pub-id></element-citation></ref><ref id="B10-sensors-25-01275"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lucas</surname><given-names>J.</given-names></name>
<name><surname>van Doorn</surname><given-names>P.</given-names></name>
<name><surname>Hegedus</surname><given-names>E.</given-names></name>
<name><surname>Lewis</surname><given-names>J.</given-names></name>
<name><surname>van der Windt</surname><given-names>D.</given-names></name>
</person-group><article-title>A Systematic Review of the Global Prevalence and Incidence of Shoulder Pain</article-title><source>BMC Musculoskelet. Disord.</source><year>2022</year><volume>23</volume><fpage>1073</fpage><pub-id pub-id-type="doi">10.1186/s12891-022-05973-8</pub-id><pub-id pub-id-type="pmid">36476476</pub-id>
</element-citation></ref><ref id="B11-sensors-25-01275"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kibler</surname><given-names>B.W.</given-names></name>
<name><surname>Sciascia</surname><given-names>A.</given-names></name>
<name><surname>Wilkes</surname><given-names>T.</given-names></name>
</person-group><article-title>Scapular Dyskinesis and Its Relation to Shoulder Injury</article-title><source>J. Am. Acad. Orthop. Surg.</source><year>2012</year><volume>20</volume><fpage>364</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.5435/JAAOS-20-06-364</pub-id><pub-id pub-id-type="pmid">22661566</pub-id>
</element-citation></ref><ref id="B12-sensors-25-01275"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Seyres</surname><given-names>M.</given-names></name>
<name><surname>Postans</surname><given-names>N.</given-names></name>
<name><surname>Freeman</surname><given-names>R.</given-names></name>
<name><surname>Pandyan</surname><given-names>A.</given-names></name>
<name><surname>Chadwick</surname><given-names>E.K.</given-names></name>
<name><surname>Philp</surname><given-names>F.</given-names></name>
</person-group><article-title>Children and Adolescents with All Forms of Shoulder Instability Demonstrate Differences in Their Movement and Muscle Activity Patterns When Compared to Age- and Sex-Matched Controls</article-title><source>J. Shoulder Elb. Surg.</source><year>2024</year><volume>33</volume><fpage>e478</fpage><lpage>e491</lpage><pub-id pub-id-type="doi">10.1016/j.jse.2024.01.043</pub-id></element-citation></ref><ref id="B13-sensors-25-01275"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cibulas</surname><given-names>A.</given-names></name>
<name><surname>Leyva</surname><given-names>A.</given-names></name>
<name><surname>Cibulas</surname><given-names>G.</given-names></name>
<name><surname>Foss</surname><given-names>M.</given-names></name>
<name><surname>Boron</surname><given-names>A.</given-names></name>
<name><surname>Dennison</surname><given-names>J.</given-names></name>
<name><surname>Gutterman</surname><given-names>B.</given-names></name>
<name><surname>Kani</surname><given-names>K.</given-names></name>
<name><surname>Porrino</surname><given-names>J.</given-names></name>
<name><surname>Bancroft</surname><given-names>L.W.</given-names></name>
<etal/>
</person-group><article-title>Acute Shoulder Injury</article-title><source>Radiol. Clin. N. Am.</source><year>2019</year><volume>57</volume><fpage>883</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.1016/j.rcl.2019.03.004</pub-id><pub-id pub-id-type="pmid">31351539</pub-id>
</element-citation></ref><ref id="B14-sensors-25-01275"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cagle</surname><given-names>P.J.</given-names><suffix>Jr.</suffix></name>
</person-group><article-title>Shoulder Injury after Vaccination: A Systematic Review</article-title><source>Rev. Bras. Ortop.</source><year>2021</year><volume>56</volume><fpage>299</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1055/s-0040-1719086</pub-id></element-citation></ref><ref id="B15-sensors-25-01275"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ackerman</surname><given-names>I.N.</given-names></name>
<name><surname>Fotis</surname><given-names>K.</given-names></name>
<name><surname>Pearson</surname><given-names>L.</given-names></name>
<name><surname>Schoch</surname><given-names>P.</given-names></name>
<name><surname>Broughton</surname><given-names>N.</given-names></name>
<name><surname>Brennan-Olsen</surname><given-names>S.L.</given-names></name>
<name><surname>Bucknill</surname><given-names>A.</given-names></name>
<name><surname>Cross</surname><given-names>E.</given-names></name>
<name><surname>Bunting-Frame</surname><given-names>N.</given-names></name>
<name><surname>Page</surname><given-names>R.S.</given-names></name>
</person-group><article-title>Impaired Health-Related Quality of Life, Psychological Distress, and Productivity Loss in Younger People with Persistent Shoulder Pain: A Cross-Sectional Analysis</article-title><source>Disabil. Rehabil.</source><year>2022</year><volume>44</volume><fpage>3785</fpage><lpage>3794</lpage><pub-id pub-id-type="pmid">33620022</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01275"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fonseca Fialho</surname><given-names>H.R.</given-names></name>
<name><surname>Gava</surname><given-names>V.</given-names></name>
<name><surname>Fonseca</surname><given-names>R.N.S.</given-names></name>
<name><surname>Kamonseki</surname><given-names>D.H.</given-names></name>
<name><surname>Barbosa</surname><given-names>G.M.</given-names></name>
</person-group><article-title>Thinking Outside the Shoulder: A Systematic Review and Metanalysis of Kinetic Chain Characteristics in Non-Athletes with Shoulder Pain</article-title><source>PLoS ONE</source><year>2024</year><volume>19</volume><elocation-id>e0314909</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0314909</pub-id><pub-id pub-id-type="pmid">39652591</pub-id>
</element-citation></ref><ref id="B17-sensors-25-01275"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wright</surname><given-names>A.A.</given-names></name>
<name><surname>Ness</surname><given-names>B.M.</given-names></name>
<name><surname>Donaldson</surname><given-names>M.</given-names></name>
<name><surname>Hegedus</surname><given-names>E.J.</given-names></name>
<name><surname>Salamh</surname><given-names>P.</given-names></name>
<name><surname>Cleland</surname><given-names>J.A.</given-names></name>
</person-group><article-title>Effectiveness of Shoulder Injury Prevention Programs in an Overhead Athletic Population: A Systematic Review</article-title><source>Phys. Ther. Sport</source><year>2021</year><volume>52</volume><fpage>189</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.ptsp.2021.09.004</pub-id><pub-id pub-id-type="pmid">34560586</pub-id>
</element-citation></ref><ref id="B18-sensors-25-01275"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Choi</surname><given-names>A.</given-names></name>
<name><surname>Hyong Kim</surname><given-names>T.</given-names></name>
<name><surname>Chae</surname><given-names>S.</given-names></name>
<name><surname>Hwan Mun</surname><given-names>J.</given-names></name>
</person-group><article-title>Improved Transfer Learning for Detecting Upper-Limb Movement Intention Using Mechanical Sensors in an Exoskeletal Rehabilitation System</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2024</year><volume>32</volume><fpage>3953</fpage><lpage>3965</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2024.3486444</pub-id><pub-id pub-id-type="pmid">39453796</pub-id>
</element-citation></ref><ref id="B19-sensors-25-01275"><label>19.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Diaz</surname><given-names>C.</given-names></name>
</person-group><article-title>Combination of IMU and EMG for Object Mass Estimation Using Machine Learning and Musculoskeletal Modeling</article-title><source>Master&#x02019;s Thesis</source><publisher-name>KTH Royal Institute of Technology</publisher-name><publisher-loc>Stockholm, Sweden</publisher-loc><year>2020</year></element-citation></ref><ref id="B20-sensors-25-01275"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wei</surname><given-names>S.</given-names></name>
<name><surname>Wu</surname><given-names>Z.</given-names></name>
</person-group><article-title>The Application of Wearable Sensors and Machine Learning Algorithms in Rehabilitation Training: A Systematic Review</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>7667</elocation-id><pub-id pub-id-type="doi">10.3390/s23187667</pub-id><pub-id pub-id-type="pmid">37765724</pub-id>
</element-citation></ref><ref id="B21-sensors-25-01275"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Amrein</surname><given-names>S.</given-names></name>
<name><surname>Werner</surname><given-names>C.</given-names></name>
<name><surname>Arnet</surname><given-names>U.</given-names></name>
<name><surname>de Vries</surname><given-names>W.H.K.</given-names></name>
</person-group><article-title>Machine-Learning-Based Methodology for Estimation of Shoulder Load in Wheelchair-Related Activities Using Wearables</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>1577</elocation-id><pub-id pub-id-type="doi">10.3390/s23031577</pub-id><pub-id pub-id-type="pmid">36772617</pub-id>
</element-citation></ref><ref id="B22-sensors-25-01275"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hasan</surname><given-names>S.</given-names></name>
<name><surname>Alam</surname><given-names>N.</given-names></name>
<name><surname>Mashud</surname><given-names>G.A.</given-names></name>
<name><surname>Bhujel</surname><given-names>S.</given-names></name>
</person-group><article-title>Neural Network for Enhancing Robot Assisted Rehabilitation: A Systematic Review</article-title><source>Actuators</source><year>2025</year><volume>14</volume><elocation-id>16</elocation-id><pub-id pub-id-type="doi">10.3390/act14010016</pub-id></element-citation></ref><ref id="B23-sensors-25-01275"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lobo</surname><given-names>P.</given-names></name>
<name><surname>Morais</surname><given-names>P.</given-names></name>
<name><surname>Murray</surname><given-names>P.</given-names></name>
<name><surname>Vila&#x000e7;a</surname><given-names>J.L.</given-names></name>
</person-group><article-title>Trends and Innovations in Wearable Technology for Motor Rehabilitation, Prediction, and Monitoring: A Comprehensive Review</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>7973</elocation-id><pub-id pub-id-type="doi">10.3390/s24247973</pub-id><pub-id pub-id-type="pmid">39771710</pub-id>
</element-citation></ref><ref id="B24-sensors-25-01275"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hua</surname><given-names>A.</given-names></name>
<name><surname>Chaudhari</surname><given-names>P.</given-names></name>
<name><surname>Johnson</surname><given-names>N.</given-names></name>
<name><surname>Quinton</surname><given-names>J.</given-names></name>
<name><surname>Schatz</surname><given-names>B.</given-names></name>
<name><surname>Buchner</surname><given-names>D.</given-names></name>
<name><surname>Hernandez</surname><given-names>M.E.</given-names></name>
</person-group><article-title>Evaluation of Machine Learning Models for Classifying Upper Extremity Exercises Using Inertial Measurement Unit-Based Kinematic Data</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2020</year><volume>24</volume><fpage>2452</fpage><lpage>2460</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2020.2999902</pub-id><pub-id pub-id-type="pmid">32750927</pub-id>
</element-citation></ref><ref id="B25-sensors-25-01275"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brennan</surname><given-names>L.</given-names></name>
<name><surname>Bevilacqua</surname><given-names>A.</given-names></name>
<name><surname>Kechadi</surname><given-names>T.</given-names></name>
<name><surname>Caulfield</surname><given-names>B.</given-names></name>
</person-group><article-title>Segmentation of Shoulder Rehabilitation Exercises for Single and Multiple Inertial Sensor Systems</article-title><source>J. Rehabil. Assist. Technol. Eng.</source><year>2020</year><volume>7</volume><fpage>2055668320915377</fpage><pub-id pub-id-type="doi">10.1177/2055668320915377</pub-id><pub-id pub-id-type="pmid">32913661</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01275"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shen</surname><given-names>C.</given-names></name>
<name><surname>Ning</surname><given-names>X.</given-names></name>
<name><surname>Zhu</surname><given-names>Q.</given-names></name>
<name><surname>Miao</surname><given-names>S.</given-names></name>
<name><surname>Lv</surname><given-names>H.</given-names></name>
</person-group><article-title>Application and Comparison of Deep Learning Approaches for Upper Limb Functionality Evaluation Based on Multi-Modal Inertial Data</article-title><source>Sustain. Comput. Inform. Syst.</source><year>2022</year><volume>33</volume><fpage>100624</fpage><pub-id pub-id-type="doi">10.1016/j.suscom.2021.100624</pub-id></element-citation></ref><ref id="B27-sensors-25-01275"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ren</surname><given-names>J.L.</given-names></name>
<name><surname>Chien</surname><given-names>Y.H.</given-names></name>
<name><surname>Chia</surname><given-names>E.Y.</given-names></name>
<name><surname>Fu</surname><given-names>L.C.</given-names></name>
<name><surname>Lai</surname><given-names>J.S.</given-names></name>
</person-group><article-title>Deep Learning Based Motion Prediction for Exoskeleton Robot Control in Upper Limb Rehabilitation</article-title><source>Proceedings of the 2019 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>20&#x02013;24 May 2019</conf-date><fpage>5076</fpage><lpage>5082</lpage><pub-id pub-id-type="doi">10.1109/ICRA.2019.8794187</pub-id></element-citation></ref><ref id="B28-sensors-25-01275"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Elkholy</surname><given-names>H.A.</given-names></name>
<name><surname>Azar</surname><given-names>A.T.</given-names></name>
<name><surname>Magd</surname><given-names>A.</given-names></name>
<name><surname>Marzouk</surname><given-names>H.</given-names></name>
<name><surname>Ammar</surname><given-names>H.H.</given-names></name>
</person-group><article-title>Classifying Upper Limb Activities Using Deep Neural Networks</article-title><source>Proceedings of the International Conference on Artificial Intelligence and Computer Vision (AICV2020)</source><conf-loc>Cairo, Egypt</conf-loc><conf-date>8&#x02013;10 April 2020</conf-date><person-group person-group-type="editor">
<name><surname>Hassanien</surname><given-names>A.E.</given-names></name>
<name><surname>Azar</surname><given-names>A.T.</given-names></name>
<name><surname>Gaber</surname><given-names>T.</given-names></name>
<name><surname>Oliva</surname><given-names>D.</given-names></name>
<name><surname>Tolba</surname><given-names>F.M.</given-names></name>
</person-group><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>268</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-44289-7_26</pub-id></element-citation></ref><ref id="B29-sensors-25-01275"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Peng</surname><given-names>Y.</given-names></name>
<name><surname>Liao</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>L.</given-names></name>
</person-group><article-title>Recognizing Wearable Upper-Limb Rehabilitation Gestures by a Hybrid Multi-Feature Neural Network</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>127</volume><fpage>107424</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2023.107424</pub-id></element-citation></ref><ref id="B30-sensors-25-01275"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Teng</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>G.</given-names></name>
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Rahman</surname><given-names>S.M.A.</given-names></name>
<name><surname>Frisoli</surname><given-names>A.</given-names></name>
</person-group><article-title>Estimation of Elbow Joint Angle from EMG and IMU Measurements Based on Graph Convolution Neural Network</article-title><source>Proceedings of the 2024 30th International Conference on Mechatronics and Machine Vision in Practice (M2VIP)</source><conf-loc>Leeds, UK</conf-loc><conf-date>3&#x02013;5 October 2024</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/M2VIP62491.2024.10746173</pub-id></element-citation></ref><ref id="B31-sensors-25-01275"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Foroutannia</surname><given-names>A.</given-names></name>
<name><surname>Akbarzadeh-T</surname><given-names>M.R.</given-names></name>
<name><surname>Akbarzadeh</surname><given-names>A.</given-names></name>
</person-group><article-title>A Deep Learning Strategy for EMG-based Joint Position Prediction in Hip Exoskeleton Assistive Robots</article-title><source>Biomed. Signal Process. Control</source><year>2022</year><volume>75</volume><elocation-id>103557</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2022.103557</pub-id></element-citation></ref><ref id="B32-sensors-25-01275"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>R.</given-names></name>
<name><surname>Xue</surname><given-names>X.</given-names></name>
<name><surname>Xiao</surname><given-names>R.</given-names></name>
<name><surname>Bu</surname><given-names>F.</given-names></name>
</person-group><article-title>A Novel Method for ECG Signal Compression and Reconstruction: Down-Sampling Operation and Signal-Referenced Network</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>1760</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12081760</pub-id></element-citation></ref><ref id="B33-sensors-25-01275"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>O&#x02019;Sullivan</surname><given-names>P.</given-names></name>
</person-group><article-title>SWIFTIES (Subject and Wearables Data for Investigation of Fatiguing Tasks in Extension/Flexion of Shoulder/Elbow)</article-title><source>Zenodo</source><year>2024</year><pub-id pub-id-type="doi">10.5281/zenodo.13773233</pub-id></element-citation></ref><ref id="B34-sensors-25-01275"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Szegedy</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>W.</given-names></name>
<name><surname>Jia</surname><given-names>Y.</given-names></name>
<name><surname>Sermanet</surname><given-names>P.</given-names></name>
<name><surname>Reed</surname><given-names>S.</given-names></name>
<name><surname>Anguelov</surname><given-names>D.</given-names></name>
<name><surname>Erhan</surname><given-names>D.</given-names></name>
<name><surname>Vanhoucke</surname><given-names>V.</given-names></name>
<name><surname>Rabinovich</surname><given-names>A.</given-names></name>
</person-group><article-title>Going Deeper with Convolutions</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="doi">10.48550/arXiv.1409.4842</pub-id><pub-id pub-id-type="arxiv">cs/1409.4842</pub-id></element-citation></ref><ref id="B35-sensors-25-01275"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ioffe</surname><given-names>S.</given-names></name>
<name><surname>Szegedy</surname><given-names>C.</given-names></name>
</person-group><article-title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="doi">10.48550/arXiv.1502.03167</pub-id><pub-id pub-id-type="arxiv">cs/1502.03167</pub-id></element-citation></ref><ref id="B36-sensors-25-01275"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>H.</given-names></name>
<name><surname>Kim</surname><given-names>H.E.</given-names></name>
<name><surname>Nam</surname><given-names>H.</given-names></name>
</person-group><article-title>SRM: A Style-Based Recalibration Module for Convolutional Neural Networks</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="doi">10.48550/arXiv.1903.10829</pub-id><pub-id pub-id-type="arxiv">cs/1903.10829</pub-id></element-citation></ref><ref id="B37-sensors-25-01275"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tompson</surname><given-names>J.</given-names></name>
<name><surname>Goroshin</surname><given-names>R.</given-names></name>
<name><surname>Jain</surname><given-names>A.</given-names></name>
<name><surname>LeCun</surname><given-names>Y.</given-names></name>
<name><surname>Bregler</surname><given-names>C.</given-names></name>
</person-group><article-title>Efficient Object Localization Using Convolutional Networks</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="doi">10.48550/arXiv.1411.4280</pub-id><pub-id pub-id-type="arxiv">cs/1411.4280</pub-id></element-citation></ref><ref id="B38-sensors-25-01275"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Si</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>G.</given-names></name>
<name><surname>Hsieh</surname><given-names>C.J.</given-names></name>
<name><surname>Bengio</surname><given-names>S.</given-names></name>
</person-group><article-title>Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="doi">10.48550/arXiv.2106.02795</pub-id><pub-id pub-id-type="arxiv">cs/2106.02795</pub-id></element-citation></ref><ref id="B39-sensors-25-01275"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Atzori</surname><given-names>M.</given-names></name>
<name><surname>Gijsberts</surname><given-names>A.</given-names></name>
<name><surname>Castellini</surname><given-names>C.</given-names></name>
<name><surname>Caputo</surname><given-names>B.</given-names></name>
<name><surname>Hager</surname><given-names>A.G.M.</given-names></name>
<name><surname>Elsig</surname><given-names>S.</given-names></name>
<name><surname>Giatsidis</surname><given-names>G.</given-names></name>
<name><surname>Bassetto</surname><given-names>F.</given-names></name>
<name><surname>M&#x000fc;ller</surname><given-names>H.</given-names></name>
</person-group><article-title>Electromyography Data for Non-Invasive Naturally-Controlled Robotic Hand Prostheses</article-title><source>Sci. Data</source><year>2014</year><volume>1</volume><fpage>140053</fpage><pub-id pub-id-type="doi">10.1038/sdata.2014.53</pub-id><pub-id pub-id-type="pmid">25977804</pub-id>
</element-citation></ref><ref id="B40-sensors-25-01275"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Atzori</surname><given-names>M.</given-names></name>
<name><surname>Cognolato</surname><given-names>M.</given-names></name>
<name><surname>M&#x000fc;ller</surname><given-names>H.</given-names></name>
</person-group><article-title>Deep Learning with Convolutional Neural Networks Applied to Electromyography Data: A Resource for the Classification of Movements for Prosthetic Hands</article-title><source>Front. Neurorobot.</source><year>2016</year><volume>10</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.3389/fnbot.2016.00009</pub-id><pub-id pub-id-type="pmid">27656140</pub-id>
</element-citation></ref><ref id="B41-sensors-25-01275"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Geng</surname><given-names>W.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
<name><surname>Jin</surname><given-names>W.</given-names></name>
<name><surname>Wei</surname><given-names>W.</given-names></name>
<name><surname>Hu</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>Data from: Gesture Recognition by Instantaneous Surface EMG Images CapgMyo-DBa</article-title><source>figshare</source><year>2018</year><pub-id pub-id-type="doi">10.6084/M9.FIGSHARE.7210397.V1</pub-id></element-citation></ref><ref id="B42-sensors-25-01275"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Geng</surname><given-names>W.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
<name><surname>Jin</surname><given-names>W.</given-names></name>
<name><surname>Wei</surname><given-names>W.</given-names></name>
<name><surname>Hu</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>Gesture Recognition by Instantaneous Surface EMG Images</article-title><source>Sci. Rep.</source><year>2016</year><volume>6</volume><elocation-id>36571</elocation-id><pub-id pub-id-type="doi">10.1038/srep36571</pub-id><pub-id pub-id-type="pmid">27845347</pub-id>
</element-citation></ref><ref id="B43-sensors-25-01275"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tsagkas</surname><given-names>N.</given-names></name>
<name><surname>Tsinganos</surname><given-names>P.</given-names></name>
<name><surname>Skodras</surname><given-names>A.</given-names></name>
</person-group><article-title>On the Use of Deeper CNNs in Hand Gesture Recognition Based on sEMG Signals</article-title><source>Proceedings of the 2019 10th International Conference on Information, Intelligence, Systems and Applications (IISA)</source><conf-loc>Patras, Greece</conf-loc><conf-date>15&#x02013;17 July 2019</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/IISA.2019.8900709</pub-id></element-citation></ref><ref id="B44-sensors-25-01275"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>Y.</given-names></name>
<name><surname>Wong</surname><given-names>Y.</given-names></name>
<name><surname>Wei</surname><given-names>W.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
<name><surname>Kankanhalli</surname><given-names>M.</given-names></name>
<name><surname>Geng</surname><given-names>W.</given-names></name>
</person-group><article-title>A Novel Attention-Based Hybrid CNN-RNN Architecture for sEMG-based Gesture Recognition</article-title><source>PLoS ONE</source><year>2018</year><volume>13</volume><elocation-id>e0206049</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0206049</pub-id><pub-id pub-id-type="pmid">30376567</pub-id>
</element-citation></ref><ref id="B45-sensors-25-01275"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tsinganos</surname><given-names>P.</given-names></name>
<name><surname>Cornelis</surname><given-names>B.</given-names></name>
<name><surname>Cornelis</surname><given-names>J.</given-names></name>
<name><surname>Jansen</surname><given-names>B.</given-names></name>
<name><surname>Skodras</surname><given-names>A.</given-names></name>
</person-group><article-title>Hilbert sEMG Data Scanning for Hand Gesture Recognition Based on Deep Learning</article-title><source>Neural Comput. Appl.</source><year>2021</year><volume>33</volume><fpage>2645</fpage><lpage>2666</lpage><pub-id pub-id-type="doi">10.1007/s00521-020-05128-7</pub-id></element-citation></ref><ref id="B46-sensors-25-01275"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Huang</surname><given-names>L.</given-names></name>
<name><surname>Jiang</surname><given-names>D.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Jiang</surname><given-names>G.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Zou</surname><given-names>C.</given-names></name>
<name><surname>Fan</surname><given-names>H.</given-names></name>
<name><surname>Xie</surname><given-names>Y.</given-names></name>
<name><surname>Xiong</surname><given-names>H.</given-names></name>
<etal/>
</person-group><article-title>Improved Multi-Stream Convolutional Block Attention Module for sEMG-based Gesture Recognition</article-title><source>Front. Bioeng. Biotechnol.</source><year>2022</year><volume>10</volume><elocation-id>909023</elocation-id><pub-id pub-id-type="doi">10.3389/fbioe.2022.909023</pub-id><pub-id pub-id-type="pmid">35747495</pub-id>
</element-citation></ref><ref id="B47-sensors-25-01275"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Moslhi</surname><given-names>A.M.</given-names></name>
<name><surname>Aly</surname><given-names>H.H.</given-names></name>
<name><surname>ElMessiery</surname><given-names>M.</given-names></name>
</person-group><article-title>The Impact of Feature Extraction on Classification Accuracy Examined by Employing a Signal Transformer to Classify Hand Gestures Using Surface Electromyography Signals</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>1259</elocation-id><pub-id pub-id-type="doi">10.3390/s24041259</pub-id><pub-id pub-id-type="pmid">38400416</pub-id>
</element-citation></ref><ref id="B48-sensors-25-01275"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wei</surname><given-names>W.</given-names></name>
<name><surname>Wong</surname><given-names>Y.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>Y.</given-names></name>
<name><surname>Kankanhalli</surname><given-names>M.</given-names></name>
<name><surname>Geng</surname><given-names>W.</given-names></name>
</person-group><article-title>A Multi-Stream Convolutional Neural Network for sEMG-based Gesture Recognition in Muscle-Computer Interface</article-title><source>Pattern Recognit. Lett.</source><year>2019</year><volume>119</volume><fpage>131</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2017.12.005</pub-id></element-citation></ref><ref id="B49-sensors-25-01275"><label>49.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Said</surname><given-names>S.</given-names></name>
<name><surname>Albarakeh</surname><given-names>Z.</given-names></name>
<name><surname>Beyrouthy</surname><given-names>T.</given-names></name>
<name><surname>Alkork</surname><given-names>S.</given-names></name>
<name><surname>Nait-ali</surname><given-names>A.</given-names></name>
</person-group><article-title>Machine-Learning Based Wearable Multi-Channel sEMG Biometrics Modality for User&#x02019;s Identification</article-title><source>Proceedings of the 2021 4th International Conference on Bio-Engineering for Smart Technologies (BioSMART)</source><conf-loc>Paris/Creteil, France</conf-loc><conf-date>8&#x02013;10 December 2021</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/BioSMART54244.2021.9677744</pub-id></element-citation></ref><ref id="B50-sensors-25-01275"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wei</surname><given-names>W.</given-names></name>
<name><surname>Hong</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>X.</given-names></name>
</person-group><article-title>A Hierarchical View Pooling Network for Multichannel Surface Electromyography-Based Gesture Recognition</article-title><source>Comput. Intell. Neurosci.</source><year>2021</year><volume>2021</volume><fpage>6591035</fpage><pub-id pub-id-type="doi">10.1155/2021/6591035</pub-id><pub-id pub-id-type="pmid">34484323</pub-id>
</element-citation></ref><ref id="B51-sensors-25-01275"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>J.S.</given-names></name>
<name><surname>Kim</surname><given-names>M.G.</given-names></name>
<name><surname>Pan</surname><given-names>S.B.</given-names></name>
</person-group><article-title>Two-Step Biometrics Using Electromyogram Signal Based on Convolutional Neural Network-Long Short-Term Memory Networks</article-title><source>Appl. Sci.</source><year>2021</year><volume>11</volume><elocation-id>6824</elocation-id><pub-id pub-id-type="doi">10.3390/app11156824</pub-id></element-citation></ref><ref id="B52-sensors-25-01275"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shen</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Mao</surname><given-names>F.</given-names></name>
<name><surname>Sun</surname><given-names>L.</given-names></name>
<name><surname>Gu</surname><given-names>M.</given-names></name>
</person-group><article-title>Movements Classification through sEMG with Convolutional Vision Transformer and Stacking Ensemble Learning</article-title><source>IEEE Sens. J.</source><year>2022</year><volume>22</volume><fpage>13318</fpage><lpage>13325</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3179535</pub-id></element-citation></ref><ref id="B53-sensors-25-01275"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Huang</surname><given-names>W.</given-names></name>
<name><surname>Qi</surname><given-names>Z.</given-names></name>
<name><surname>Yin</surname><given-names>S.</given-names></name>
</person-group><article-title>MS-CLSTM: Myoelectric Manipulator Gesture Recognition Based on Multi-Scale Feature Fusion CNN-LSTM Network</article-title><source>Biomimetics</source><year>2024</year><volume>9</volume><elocation-id>784</elocation-id><pub-id pub-id-type="doi">10.3390/biomimetics9120784</pub-id><pub-id pub-id-type="pmid">39727788</pub-id>
</element-citation></ref><ref id="B54-sensors-25-01275"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Yao</surname><given-names>J.</given-names></name>
<name><surname>Xu</surname><given-names>M.</given-names></name>
<name><surname>Jiang</surname><given-names>M.</given-names></name>
<name><surname>Su</surname><given-names>J.</given-names></name>
</person-group><article-title>Transformer-based network with temporal depthwise convolutions for sEMG recognition</article-title><source>Pattern Recognit.</source><year>2024</year><volume>145</volume><fpage>109967</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2023.109967</pub-id></element-citation></ref><ref id="B55-sensors-25-01275"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>R.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
</person-group><article-title>Hand Gesture Recognition Based on Surface Electromyography Using Convolutional Neural Network with Transfer Learning Method</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2021</year><volume>25</volume><fpage>1292</fpage><lpage>1304</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2020.3009383</pub-id><pub-id pub-id-type="pmid">32750962</pub-id>
</element-citation></ref><ref id="B56-sensors-25-01275"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>K.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>X.</given-names></name>
<name><surname>Hu</surname><given-names>C.</given-names></name>
</person-group><article-title>Iterative Self-Training Based Domain Adaptation for Cross-User sEMG Gesture Recognition</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><year>2023</year><volume>31</volume><fpage>2974</fpage><lpage>2987</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2023.3293334</pub-id><pub-id pub-id-type="pmid">37418414</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01275-f001"><label>Figure 1</label><caption><p>Experimental setup for IMU and sEMG data acquisition: (<bold>a</bold>) placement of the IMU sensor; (<bold>b</bold>) data acquisition checkpoints for IMU and sEMG sensors; (<bold>c</bold>) a subset of raw signals captured by the system.</p></caption><graphic xlink:href="sensors-25-01275-g001" position="float"/></fig><fig position="float" id="sensors-25-01275-f002"><label>Figure 2</label><caption><p>Data process result. The Euler angles obtained from IMU transformation and the processed multi-channel EMG signals are plotted for three preset SJ movements.</p></caption><graphic xlink:href="sensors-25-01275-g002" position="float"/></fig><fig position="float" id="sensors-25-01275-f003"><label>Figure 3</label><caption><p>Dataset organization structure.</p></caption><graphic xlink:href="sensors-25-01275-g003" position="float"/></fig><fig position="float" id="sensors-25-01275-f004"><label>Figure 4</label><caption><p><bold>SWCTNet model architecture.</bold> The model consists of the SW-CNN Block, CTAT Block, and Downstream Task Block.</p></caption><graphic xlink:href="sensors-25-01275-g004" position="float"/></fig><fig position="float" id="sensors-25-01275-f005"><label>Figure 5</label><caption><p>Structure of the SW-CNN Block.</p></caption><graphic xlink:href="sensors-25-01275-g005" position="float"/></fig><fig position="float" id="sensors-25-01275-f006"><label>Figure 6</label><caption><p>Structure of the CTAT Block.</p></caption><graphic xlink:href="sensors-25-01275-g006" position="float"/></fig><fig position="float" id="sensors-25-01275-f007"><label>Figure 7</label><caption><p>Results of the ablation study conducted on four public datasets.</p></caption><graphic xlink:href="sensors-25-01275-g007" position="float"/></fig><fig position="float" id="sensors-25-01275-f008"><label>Figure 8</label><caption><p>Radar chart comparing the performance of different models on the feature prediction task, normalized to the range [0, 1]: (<bold>a</bold>) results for SWIFTIES dataset; (<bold>b</bold>) results for personal dataset.</p></caption><graphic xlink:href="sensors-25-01275-g008" position="float"/></fig><fig position="float" id="sensors-25-01275-f009"><label>Figure 9</label><caption><p>Visualization of the sequence generation task, showing the actual and predicted EMG feature time-series signals, where (<bold>a</bold>,<bold>b</bold>) are the results of the RMS feature, (<bold>c</bold>,<bold>d</bold>) are the results of the MPF feature, (<bold>a</bold>,<bold>c</bold>) are the results for healthy individuals, and (<bold>b</bold>,<bold>d</bold>) are the results for patients.</p></caption><graphic xlink:href="sensors-25-01275-g009" position="float"/></fig><table-wrap position="float" id="sensors-25-01275-t001"><object-id pub-id-type="pii">sensors-25-01275-t001_Table 1</object-id><label>Table 1</label><caption><p>Participant information statistics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Group (n)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Age (Years)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Gender (M/F)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Others</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Healthy (10)</td><td align="center" valign="middle" rowspan="1" colspan="1">25.1 &#x000b1; 2.9</td><td align="center" valign="middle" rowspan="1" colspan="1">7/3</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Injured (15)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25.5 &#x000b1; 3.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7/8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">fractures<break/>
mild rotator cuff tears<break/>
subacromial impingement syndrome</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01275-t002"><object-id pub-id-type="pii">sensors-25-01275-t002_Table 2</object-id><label>Table 2</label><caption><p>Processed SWIFTIES dataset with all task combinations and participant information.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Participant</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Personal Info</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Task Combination</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">IMU Data (Aligned)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">EMG Data (Aligned)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">EMG Features (Extracted)</th></tr></thead><tbody><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">1</td><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Age: 25<break/>
Sex: Male<break/>
Height: 175 cm<break/>
Weight: 70 kg</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dy/Sh/25%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm123" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>IMU</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm125" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dy/Sh/45%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm126" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>IMU</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm127" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm128" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dy/El/25%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>IMU</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm130" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm131" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dy/El/45%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm132" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>IMU</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm133" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm134" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02026;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02026;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02026;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02026;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02026;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02026;</td></tr><tr><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">32</td><td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Age: 30<break/>
Sex: Female<break/>
Height: 165 cm<break/>
Weight: 60 kg</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dy/Sh/25%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm135" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>IMU</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm136" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm137" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dy/Sh/45%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm138" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>IMU</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm139" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm140" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DySh</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dy/El/25%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm141" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>IMU</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm142" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm143" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>25</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dy/El/45%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm144" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>IMU</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm145" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm146" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>DyEl</mml:mi><mml:mn>45</mml:mn></mml:mrow><mml:mi>EMG</mml:mi></mml:msubsup></mml:mrow></mml:math>
</inline-formula>
</td></tr></tbody></table><table-wrap-foot><fn><p>St = Static; Dy = Dynamic; Sh = Shoulder; El = Elbow; <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>25</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> = <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>25</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> MVC; <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>45</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> = <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>45</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> MVC.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-01275-t003"><object-id pub-id-type="pii">sensors-25-01275-t003_Table 3</object-id><label>Table 3</label><caption><p>Summary of public dataset information. used.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensor</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sampling Rate (Hz)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Channels</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Actions (Repetitions)</th></tr></thead><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">NinaPro</td><td align="center" valign="middle" rowspan="1" colspan="1">DB-1</td><td align="center" valign="middle" rowspan="1" colspan="1">Otto Bock MyoBock</td><td align="center" valign="middle" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="left" valign="middle" rowspan="1" colspan="1">52 hand movements (10 repetitions)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DB-2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Delsys Trigno</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49 hand movements (10 repetitions)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CapgMyo</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DB-A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HD-EMG (<inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8 finger movements (10 single trials)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01275-t004"><object-id pub-id-type="pii">sensors-25-01275-t004_Table 4</object-id><label>Table 4</label><caption><p>Accuracy (%) comparison among different models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Reference</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Year</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Classifier</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy (%)</th></tr></thead><tbody><tr><td rowspan="7" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">DB1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Geng [<xref rid="B42-sensors-25-01275" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2016</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ConvNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.90</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tsagkas [<xref rid="B43-sensors-25-01275" ref-type="bibr">43</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71.85</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hu [<xref rid="B44-sensors-25-01275" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-RNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tsinganos [<xref rid="B45-sensors-25-01275" ref-type="bibr">45</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.75</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wang [<xref rid="B46-sensors-25-01275" ref-type="bibr">46</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN+GRU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moslhi [<xref rid="B47-sensors-25-01275" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single transformer (FFT)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.30</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SWCTNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>87.93</bold>
</td></tr><tr><td rowspan="10" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">DB2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Geng [<xref rid="B42-sensors-25-01275" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2016</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ConvNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.10</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hu [<xref rid="B44-sensors-25-01275" ref-type="bibr">44</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2018</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-RNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.20</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wei [<xref rid="B48-sensors-25-01275" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-stream CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.80</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Said [<xref rid="B49-sensors-25-01275" ref-type="bibr">49</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TL + MLP-TL + CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wei [<xref rid="B50-sensors-25-01275" ref-type="bibr">50</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-view CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.70</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kim [<xref rid="B51-sensors-25-01275" ref-type="bibr">51</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-LSTM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.91</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shen [<xref rid="B52-sensors-25-01275" ref-type="bibr">52</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CViT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80.02</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wang [<xref rid="B53-sensors-25-01275" ref-type="bibr">53</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-scale feature+Bi-LSTM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.66</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wang [<xref rid="B54-sensors-25-01275" ref-type="bibr">54</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">TDCT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.39</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SWCTNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>88.75</bold>
</td></tr><tr><td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">DBA</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Geng [<xref rid="B42-sensors-25-01275" ref-type="bibr">42</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2016</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ConvNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.30</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wei [<xref rid="B48-sensors-25-01275" ref-type="bibr">48</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2019</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-stream CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.50</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Chen [<xref rid="B55-sensors-25-01275" ref-type="bibr">55</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN + LSTM + TL</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.77</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shen [<xref rid="B52-sensors-25-01275" ref-type="bibr">52</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2022</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CViT</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.83</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wang [<xref rid="B56-sensors-25-01275" ref-type="bibr">56</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2023</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Self-learning</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.31</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moslhi [<xref rid="B47-sensors-25-01275" ref-type="bibr">47</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single transformer (Wavelet)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77.90</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wang [<xref rid="B54-sensors-25-01275" ref-type="bibr">54</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2024</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Position weight</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SWCTNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>91.03</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SWIFITS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SWCTNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>99.75</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Personal Dataset</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Ours</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">-</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SWCTNet</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>98.00</bold>
</td></tr></tbody></table></table-wrap></floats-group></article>