<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39730754</article-id><article-id pub-id-type="pmc">PMC11681068</article-id><article-id pub-id-type="publisher-id">82272</article-id><article-id pub-id-type="doi">10.1038/s41598-024-82272-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Potato late blight leaf detection in complex environments</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Li</surname><given-names>Jingtao</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Jiawei</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Rui</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Shu</surname><given-names>Guofeng</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Liu</surname><given-names>Xia</given-names></name><address><email>coosso@qq.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhu</surname><given-names>Kun</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Changyi</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Zhu</surname><given-names>Tong</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00xyeez13</institution-id><institution-id institution-id-type="GRID">grid.218292.2</institution-id><institution-id institution-id-type="ISNI">0000 0000 8571 108X</institution-id><institution>Faculty of Information Engineering and Automation, </institution><institution>Kunming University of Science and Technology, </institution></institution-wrap>Kunming, 650504 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04dpa3g90</institution-id><institution-id institution-id-type="GRID">grid.410696.c</institution-id><institution-id institution-id-type="ISNI">0000 0004 1761 2898</institution-id><institution>College of Plant Protection, </institution><institution>Yunnan Agricultural University, </institution></institution-wrap>Kunming, 650201 China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/038d7ve10</institution-id><institution-id institution-id-type="GRID">grid.459704.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 6473 2841</institution-id><institution>School of Physics and Electrical Engineering, </institution><institution>Liupanshui Normal University, </institution></institution-wrap>Liupanshui, 553004 China </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04facbs33</institution-id><institution-id institution-id-type="GRID">grid.443274.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2237 1871</institution-id><institution>School of Animation and Digital Arts, </institution><institution>Communication University of China, </institution></institution-wrap>Beijing, 100024 China </aff></contrib-group><pub-date pub-type="epub"><day>28</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>28</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>31046</elocation-id><history><date date-type="received"><day>21</day><month>9</month><year>2024</year></date><date date-type="accepted"><day>4</day><month>12</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Potato late blight is a common disease affecting crops worldwide. To help detect this disease in complex environments, an improved YOLOv5 algorithm is proposed. First, ShuffleNetV2 is used as the backbone network to reduce the number of parameters and computational load, making the model more lightweight. Second, the coordinate attention mechanism is added to reduce missed detection for leaves that are overlapping, damaged, or hidden, thereby increasing detection accuracy under challenging conditions. Lastly, a bidirectional feature pyramid network is employed to fuse feature information of different scales. The study results show a significant improvement in the model&#x02019;s performance. The number of parameters was reduced from 7.02 to 3.87 M, and the floating point operations dropped from 15.94 to 8.4 G. These reductions make the model lighter and more efficient. The detection speed increased by 16 %, enabling faster detection of potato late blight leaves. Additionally, the average precision improved by 3.22 %, indicating better detection accuracy. Overall, the improved model provides a robust solution for detecting potato late blight in complex environments. The study&#x02019;s findings can be useful for applications and further research in controlling potato late blight in similar environments.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Potato late blight</kwd><kwd>Disease leaf detection</kwd><kwd>Lightweight network</kwd><kwd>Attention mechanism</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Plant sciences</kwd><kwd>Mathematics and computing</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100018531</institution-id><institution>Major Science and Technology Projects in Yunnan Province</institution></institution-wrap></funding-source><award-id>202402AE090017</award-id><award-id>202402AE090017</award-id><award-id>202402AE090017</award-id><award-id>202402AE090017</award-id><award-id>202402AE090017</award-id><award-id>202402AE090017</award-id><award-id>202402AE090017</award-id><award-id>202402AE090017</award-id><principal-award-recipient><name><surname>Li</surname><given-names>Jingtao</given-names></name><name><surname>Wu</surname><given-names>Jiawei</given-names></name><name><surname>Liu</surname><given-names>Rui</given-names></name><name><surname>Shu</surname><given-names>Guofeng</given-names></name><name><surname>Liu</surname><given-names>Xia</given-names></name><name><surname>Zhu</surname><given-names>Kun</given-names></name><name><surname>Wang</surname><given-names>Changyi</given-names></name><name><surname>Zhu</surname><given-names>Tong</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Potato, one of the most crucial food crops worldwide, faces significant losses each year due to plant diseases<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. Among the various plant diseases, late blight is one of the most prevalent and serious, caused by pathogenic molds that infect potato leaves, leading to localized wilting and significantly hindering potato growth<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. This not only affects crop yield but also results in substantial economic losses for the potato industry. Therefore, the rapid identification of late blight-infected leaves in agricultural fields are crucial for the effective prevention and management of potato diseases<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>.</p><p id="Par3">Traditionally, identifying potato late blight has relied heavily on manual labor, with plant pathology experts needing to physically visit fields to identify the disease<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. Unfortunately, this method has proven inefficient. However, advances in machine learning technology<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup> and improvements in computer hardware have led to significant progress in using image processing techniques to recognize plant diseases. Many researchers have successfully utilized clustering<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, random forest<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and other methods for disease recognition, reducing both cost and labor. Nonetheless, these methods still require manual design of disease extraction features and face challenges such as limited adaptability to different situations and susceptibility to environmental factors.</p><p id="Par4">With the rapid advancements in computer hardware and significant breakthroughs in deep learning, image processing techniques utilizing convolutional neural networks (CNN)<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>, have emerged as a prominent area of research. Numerous scholars have achieved notable success in target classification, detection, and segmentation using these neural network models. In the context of plant leaf disease recognition, the PlantVillage dataset created by Mohanty et al.<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> has greatly facilitated research efforts, prompting many researchers to explore deep learning methodologies using this dataset. However, the images collected in laboratory settings often feature simple backgrounds and under constant lighting conditions. While some researchers have attempted to enhance the dataset through various data augmentation techniques, the effectiveness of models trained on these datasets remains constrained when applied to images captured in complex agricultural environments.</p><p id="Par5">To fundamentally enhance the diversity of datasets, some researchers<sup><xref ref-type="bibr" rid="CR14">14</xref>&#x02013;<xref ref-type="bibr" rid="CR16">16</xref></sup>, have developed plant disease leaf datasets that include complex backgrounds and have studied the recognition and detection of associated leaf diseases based on these datasets. However, these researchers often focus on addressing the low accuracy of plant leaf recognition and detection in complex environments, prioritizing improvements in model accuracy while overlooking the importance of model lightweighting. These models can achieve the desired accuracy when processing plant leaf images in complex environments. However, they are not suitable for deployment in resource-constrained edge settings because of their large number of parameters and high computational requirements.</p><p id="Par6">In summary, detecting potato late blight leaves in complex environments presents several key challenges. First, the images of potato late blight leaves in publicly available datasets were primarily taken in simple laboratory settings. As a result, detection models trained on this data often perform poorly in real farmland conditions. Second, potato disease leaves exhibit significant diversity in complex environments, including variations in leaf size, capture angles, occlusions, and leaf damage. These factors make it harder to identify and detect the disease, increasing the demands on the model&#x02019;s robustness and accuracy. Third, many existing detection models require substantial computational resources, which limits their use on resource-constrained edge devices. Therefore, it is essential to develop an efficient and lightweight model for detecting potato late blight leaves in complex environments.</p><p id="Par7">To address the challenges mentioned above, this paper develops a YOLOv5-based lightweight model specifically designed for detecting potato late blight leaves in complex environments. This model effectively balances complexity and detection efficiency. The main contributions of this work are as follows:<list list-type="bullet"><list-item><p id="Par8">A potato late blight leaf image dataset was constructed that reflects complex environments. This dataset was created by collecting images from real farmland, along with employing multiple data augmentation techniques. This ensures the robustness of the proposed model in real scenario applications.</p></list-item><list-item><p id="Par9">The backbone network of the original YOLOv5 was enhanced by integrating ShuffleNetV2<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, which employs a coordinate attention<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> mechanism. This change greatly decreases the number of parameters and the computational burden of the model, while also improving its ability to extract features. Consequently, the model is better suited for use on embedded devices and reduces the rates of missed detections for potato late blight leaves in challenging environments.</p></list-item><list-item><p id="Par10">The feature fusion network of the original model was improved by incorporating bidirectional feature pyramid network<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, which enhances the model&#x02019;s ability to extract features of potato late blight leaves at different scales. This improvement also boosts the model&#x02019;s generalization when dealing with potato leaves at various growth stages and from different shooting distances.</p></list-item></list>The following sections of this paper are organized as follows: Section &#x0201c;Related works&#x0201d; discusses related works on plant disease identification. Section &#x0201c;Materials and methods&#x0201d; outlines the materials used and provides a detailed description of the proposed methodology. Section &#x0201c;Experiments&#x0201d; describes the experimental environment and parameter settings. Section &#x0201c;Experimental results&#x0201d; analyzes and validates the experimental results in detail. Finally, Section &#x0201c;Conclusion&#x0201d; summarizes the main findings of this paper and offers future perspectives.</p></sec><sec id="Sec2"><title>Related works</title><p id="Par11">In traditional plant image processing research, various feature extraction techniques and machine learning algorithms have been used for plant image recognition studies. For example, Nebojsa et al.<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> improved the firefly algorithm and used it to optimize the parameters of the support vector machine algorithm, which achieved better results than the original algorithm in the task of classifying four categories of images: apples, cherries, peppers, and tomatoes. Nidhis et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> applied a clustering algorithm to categorize common rice leaf diseases, calculated the diseased area&#x02019;s percentage, and thereby classified the disease severity. Govardhan et al.<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> experimented on tomato leaf images from eight categories, comparing six machine learning methods including K-nearest neighbors, decision tree, and random forest. They found that the random forest method achieved the best results with a classification accuracy of 95%. Bukumir et al.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> extracted 12 features from carrots, such as length, mean diameter, and RGB color, and used a Bayesian-optimized cascade graph convolution neural network for grading. These methods based on traditional machine learning for plant image processing have shown some success, but they rely on manual feature extraction, are sensitive to environmental changes, and suffer from complex operation and poor generalization.</p><p id="Par12">Compared with traditional methods, deep learning techniques are increasingly studied and utilized in plant leaf disease recognition<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> due to their advantages like automatic feature extraction and excellent performance. For example, Eunice et al.<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> employed various popular deep learning models for disease classification of 38 crop leaf categories in the PlantVillage dataset, finding that DenseNet-121 achieved the best performance with a mean average precision (mAP) of 99.81%. Similarly, Deng et al.<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> constructed a classification network combining residual structure and attention mechanism, achieving 87.77% accuracy on 16 disease datasets across four plant species. Asif et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> designed a convolutional neural network model for diagnosing early blight, late blight and healthy potato leaves, attaining a maximum classification accuracy of 97%. These deep learning methods demonstrate high accuracy in classifying plant leaves and diseases, significantly aiding rapid identification and diagnosis. However, image classification alone does not fully address the complexity of diseases, such as the location of infected leaves and infection severity. Thus, employing object detection techniques is essential for a more comprehensive diagnosis of diseased leaves, providing more precise data to support disease management.</p><p id="Par13">Deep learning based object detection methods are classified into two types: two-stage and one-stage. Two-stage algorithms, like Faster R-CNN, were initially dominant in target detection due to their high accuracy. However, their long training times and slow detection speeds limit their applicability in real-time industrial scenarios. In contrast, one-stage algorithms like SSD, EfficientDet, and the YOLO series have gained significant attention for their superior real-time performance and ease of use. Notably, the YOLO series has shown impressive results in both research and various industry applications. For instance, Zhong et al.<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> developed an automated road defect detection model using YOLOv5, achieving an average accuracy of 81.6%. Qu et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> proposed a YOLOv8-LA model for underwater small target detection by improving the convolutional module and overall architecture of YOLOv8, achieving the highest mAP of 84.7% on a public dataset. Jovanovic et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> utilized both nano and small models of YOLOv8 to accomplish real-time detection of rocket bodies and engine flames, showcasing the promising applications of computer vision techniques in the field of passive rocket detection and tracking. It can be seen that the YOLO series algorithms have demonstrated effective applications across a variety of industries</p><p id="Par14">In the field of plant leaf disease diagnosis, the YOLO family of algorithms has also been heavily researched and applied. For example, By incorporating an attention mechanism into YOLOv5, Zhang et al.<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> achieved 85.59% precision and 83.70% recall in detecting grape downy mildew leaves. This advancement offers a quick and accurate deep learning approach for the automatic detection of grape leaf diseases. Zhao et al.<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> proposed a novel loss function and integrated it into the YOLOv7 model, combining it with transfer learning. This approach achieved a mAP of 96.75% in detecting five common plant leaf diseases. Abdullah et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> enhanced the original YOLOv5 model by incorporating dilated convolution, global attention mechanism and the EIoU loss function. This modified model successfully detected healthy tomato leaves as well as nine types of diseased tomato leaves from the PlantVillage dataset. The detection results demonstrated a higher mAP of 91.40%, outperforming the original YOLOv5, as well as the YOLOv7 and YOLOv8 algorithms. Although these studies show excellent detection accuracy, the datasets were mostly collected in controlled environments with simple backgrounds and constant light. This limits the effectiveness of models trained on these datasets when applied to real farmland situations.</p><p id="Par15">Some researchers have improved model performance for practical applications using images from real plant environments. Liu et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> captured images at a greenhouse tomato planting site to create a dataset of tomato brown rot with real backgrounds, achieving a detection accuracy of 94.6% using the YOLOv5 method. Yan et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> collected images of apple leaves in natural settings from the internet, creating a dataset called ALDD that includes eight typical diseased and healthy apple leaves. They achieved an average detection accuracy of 88.6% with their proposed FSM-YOLO method. Leng et al.<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> conducted experiments using the YOLOv5 model with images of maize leaf blight taken in farmland, achieving a mAP of 87.5%. These studies are valuable for improving the practical application of leaf disease detection by building datasets in complex environments. However, these studies have primarily concentrated on improving model accuracy to tackle the challenges presented by complex environmental images, while overlooking the importance of lightweight solutions. As a result, the models may be too complex to deploy effectively on resource-constrained embedded devices.</p><p id="Par16">In conclusion, recent studies mostly focus on datasets from controlled environments, overlooking the complexity of real farmland environments, which limits the model&#x02019;s generalization. Several studies have achieved high detection accuracy using datasets from real farmland environments. However, the complexity of these models presents challenges when deploying them on edge devices with limited computational resources. The work in this paper should focus on improving detection accuracy in complex settings, while also enhancing model efficiency and lightweight design to meet computational limits and real-time needs in practical applications.</p></sec><sec id="Sec3"><title>Materials and methods</title><sec id="Sec4"><title>Data collection and processing</title><p id="Par17">To create a dataset specifically for potato late blight leaf detection in complex environments, a total of 2592 images were collected from modernized potato planting bases in Yunnan Province and various internet sources. Due to limited storage space, the size of these images was reduced from 7.72 GB to 342 MB using the compression tool docosmall. Some of the processed pictures are shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.</p><p id="Par18">The collected images were then labeled using LabelImg, resulting in 27,540 instances of late blight leaves. To expand the dataset, random rotation, mirroring, and scaling operations were applied to the original images. Additionally, brightness adjustments, color variations, and noise were introduced to further augment the dataset. These processes produced a total of 20,736 images. After manually filtering out poor-quality images, a final dataset of 19,950 images with 215,856 labeled instances was obtained.</p><p id="Par19">The dataset was then divided into three sets: training, validation, and testing, in a ratio of 7:2:1. Care was taken to ensure that the average number of annotations per image was similar across all three sets, guaranteeing balanced representation. During the training process, the Mosaic data augmentation method included in YOLOv5 was disabled. This decision was made to prevent any potential negative impact from overlapping data augmentation techniques and to conserve computational resources.<fig id="Fig1"><label>Figure 1</label><caption><p>Potato late blight leaves in complex environments.</p></caption><graphic xlink:href="41598_2024_82272_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec5"><title>Improved YOLOv5-based detection</title><p id="Par20">YOLOv5 is a algorithm derived from the real-time, single-stage target detection algorithm YOLO (You Only Look Once). In October 2021, Ultralytics introduced the sixth iteration of YOLOv5, which is comprised of four versions: YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x. The primary distinction between these versions lies in the depth and width of the network, which influences the quantity of parameters and the training duration of the model. Over time, newer YOLO versions like YOLOv6, YOLOv7, and YOLOv8 have been proposed. However, YOLOv5 still has a strong accumulation of community technologies, which provide a large number of solutions for development and deployment. It will take time for new versions to gain similar support. Additionally, YOLOv5, due to its earlier introduction and proven advantages, has been successfully applied in various industrial scenarios, demonstrating mature experience and positive outcomes. In contrast, while YOLOv8 has shown superior performance on standard test datasets, its application across diverse environments in different scenarios still requires extensive validation. Therefore, this paper chooses YOLOv5 as the foundational research framework for detecting potato late blight in complex environments, aiming for efficient development and stable deployment.</p><p id="Par21">To adapt the original YOLOv5 model to detect potato late blight leaves in complex farm environments, this study incorporates several key improvements. Firstly, ShuffleNetV2 is employed as the backbone network to fulfill the lightweight requirements. Secondly, a coordinate attention mechanism is integrated into the backbone to enhance detection accuracy and reduce false negatives. Thirdly, BiFPN is used to merge features of different scales, thereby enhancing the model&#x02019;s feature expression capability and improving the detection of small targets. The improved YOLOv5 is illustrated in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Figure 2</label><caption><p>Improved YOLOv5 network structure.</p></caption><graphic xlink:href="41598_2024_82272_Fig2_HTML" id="MO2"/></fig></p><sec id="Sec6"><title>Lightweight backbone network</title><p id="Par22">In large potato fields, implementing disease detection models on edge devices such as UAVs (Unmanned Aerial Vehicles) offers a practical solution<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> for automated disease detection. However, the limited storage and performance of these edge devices make traditional detection models unsuitable<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Therefore, this paper uses ShuffleNetV2 to lighten the YOLOv5 backbone network, making it more suitable for edge device deployment.</p><p id="Par23">ShuffleNetV2 is a lightweight network architecture designed for edge devices. Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> illustrates two main unit structures of ShuffleNetV2. Stage 1 is the basic unit of ShuffleNetV2. It starts by dividing the input feature map into two segments via channel splitting. One segment is processed through three convolution operations, while the other remains unprocessed. These segments are then concatenated, followed by a channel shuffle operation to facilitate information exchange among channels. Stage 2 is the downsampling unit and differs from Stage 1 by not splitting the input feature map. Instead, it processes the input through deep convolution operations on two separate routes. The resulting feature maps are concatenated and subjected to a channel shuffle operation, which doubles the number of channels in the final output. This stage is designed to increase the network&#x02019;s capacity without a significant increase in computational complexity.<fig id="Fig3"><label>Figure 3</label><caption><p>Two unit structures for ShuffleNetV2.</p></caption><graphic xlink:href="41598_2024_82272_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec7"><title>Coordinate attention</title><p id="Par24">In image processing tasks, attention mechanisms<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> assigns weights to different features of the input data like channels and locations, enabling the model to focus on key areas and thus improve its ability to select and weight features. This dynamic weight allocation not only enhances the model&#x02019;s accuracy in complex scenarios but also strengthens its robustness, allowing it to better adapt to various environments. In the task of detecting potato late blight in complex environments, factors such as occlusion, complex backgrounds, and lighting changes can interfere with the model&#x02019;s judgment. Attention mechanisms can automatically adjust the importance of each position or channel, highlighting relevant areas for disease leaf recognition and localization while ignoring irrelevant backgrounds. This process enhances the model&#x02019;s ability to extract features related to diseased leaves<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> and ultimately improves the accuracy of disease leaf detection.</p><p id="Par25">To address the challenges of detecting potato late blight in complex environments, this paper introduces the coordinate attention (CA) mechanism into the YOLOv5 network architecture. Coordinate attention is a hybrid attention model that integrates position information into channel attention and encodes it bidirectionally. This approach enables the model to capture both channel relationships and long-distance dependencies more effectively. The CA mechanism consists of two main steps: coordinate information embedding and coordinate attention generation. Specifically, for a feature map <italic>x</italic> with an input size of <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C{\times }H{\times }W$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq1.gif"/></alternatives></inline-formula>, a pooling kernel of size (<italic>H</italic>,1) is used to encode along the horizontal direction. This operation results an output <inline-formula id="IEq2"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z^h$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq2.gif"/></alternatives></inline-formula> with the number of channels <italic>C</italic>, height <italic>H</italic>, and width 1. The output <inline-formula id="IEq3"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z^h$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq3.gif"/></alternatives></inline-formula> for the <italic>h</italic>-th row of the <italic>c</italic>-th channel can be described as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} z_c^h\left( h\right) =\frac{1}{W}\sum _{0\le i&#x0003c;W}{x_c(h,i)} \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82272_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>In the above equation, <inline-formula id="IEq4"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_c(h, i)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq4.gif"/></alternatives></inline-formula> represents the value of the feature map <italic>x</italic> at the <italic>c</italic>-th channel, located in the <italic>h</italic>-th row and <italic>i</italic>-th column. In a similar manner, the output <inline-formula id="IEq5"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z^w$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq5.gif"/></alternatives></inline-formula> for the <italic>w</italic>-th column of the <italic>c</italic>-th channel in the vertically encoded result can be defined as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} z_c^w\left( w\right) =\frac{1}{H}\sum _{0\le j&#x0003c;H}{x_c\left( j,w\right) } \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82272_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>Likewise, <inline-formula id="IEq6"><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_c(j, w)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq6.gif"/></alternatives></inline-formula> has the same meaning as <inline-formula id="IEq7"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_c(h, i)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq7.gif"/></alternatives></inline-formula> in Eq. (<xref rid="Equ1" ref-type="disp-formula">1</xref>). The two output feature maps mentioned above are concatenated into a new feature map of size <inline-formula id="IEq8"><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C{\times }1{\times }$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq8.gif"/></alternatives></inline-formula>(<italic>H</italic>+<italic>W</italic>). This concatenated feature is then downsampled using a 1<inline-formula id="IEq9"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq9.gif"/></alternatives></inline-formula>1 convolution, denoted as <inline-formula id="IEq10"><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq10.gif"/></alternatives></inline-formula>. Finally, a non-linear activation function <inline-formula id="IEq11"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\delta$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq11.gif"/></alternatives></inline-formula> is applied to obtain the final output, <italic>f</italic>. This can be represented as follows:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} f=\delta (F_1([z^h,z^w])) \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82272_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where [,] denotes the concatenation operation along the spatial dimension. Following the above steps, the output <italic>f</italic> is split into two components <inline-formula id="IEq12"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f^h$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq12.gif"/></alternatives></inline-formula> and <inline-formula id="IEq13"><alternatives><tex-math id="M16">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f^w$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq13.gif"/></alternatives></inline-formula>. These two feature maps are then transformed into two new feature maps, respectively, using two separate 1<inline-formula id="IEq14"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq14.gif"/></alternatives></inline-formula>1 convolutional kernels, <inline-formula id="IEq15"><alternatives><tex-math id="M18">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_h$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq15.gif"/></alternatives></inline-formula> and <inline-formula id="IEq16"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_w$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq16.gif"/></alternatives></inline-formula>. At this point, the horizontal and vertical attention weights <inline-formula id="IEq17"><alternatives><tex-math id="M20">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g^w$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq17.gif"/></alternatives></inline-formula> and <inline-formula id="IEq18"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g^h$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq18.gif"/></alternatives></inline-formula> are obtained. This can be described as:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M22">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} g^h&#x00026;=\sigma (F_h(f^h))\end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82272_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} g^w&#x00026;=\sigma (F_w(f^w)) \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82272_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>The <inline-formula id="IEq19"><alternatives><tex-math id="M24">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq19.gif"/></alternatives></inline-formula> in the above is the sigmoid activation function. In the final step, the feature information of <inline-formula id="IEq20"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g^h$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq20.gif"/></alternatives></inline-formula> and <inline-formula id="IEq21"><alternatives><tex-math id="M26">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g^w$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq21.gif"/></alternatives></inline-formula> are fused and then multiplied with the original feature map <italic>x</italic> to yield the final coordinate attention module, denoted as <italic>y</italic>. The weight value for the <italic>c</italic>-th channel in this module is defined as:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} y_c\left( i,j\right) =x_c\left( i,j\right) \times g_c^h\left( i\right) \times g_c^w\left( j\right) \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82272_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>As mentioned in the above equation, <inline-formula id="IEq22"><alternatives><tex-math id="M28">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g_c^h\left( i\right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq22.gif"/></alternatives></inline-formula> represents the attention value for the <italic>c</italic>-th channel at <italic>i</italic>-th row, while <inline-formula id="IEq23"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g_c^w\left( j\right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq23.gif"/></alternatives></inline-formula> denotes the attention value for the <italic>c</italic>-th channel at <italic>j</italic>-th column. By consecutively multiplying them with the value <inline-formula id="IEq24"><alternatives><tex-math id="M30">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_c\left( i,j\right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq24.gif"/></alternatives></inline-formula> at the i-th row and j-th column of the c-th channel in the feature map <italic>x</italic>, the final weight <inline-formula id="IEq25"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_c\left( i,j\right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq25.gif"/></alternatives></inline-formula>, incorporating the coordinate attention mechanism, is obtained. In this study, three coordinate attention modules are incorporated after each stage 1 of ShuffleNetV2. The placement of these additional coordinate attention modules is illustrated in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>.<fig id="Fig4"><label>Figure 4</label><caption><p>Backbone with added CAs.</p></caption><graphic xlink:href="41598_2024_82272_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec8"><title>Bidirectional feature pyramid network</title><p id="Par26">In practical applications, such as detecting potato leaves in a field, the size of potato leaves can vary with the growth stages. Moreover, leaves from the same plant may differ in size, and their apparent size can also change depending on the shooting distance. Therefore, fusing features from different scales is crucial to improve the model&#x02019;s ability to detect potato late blight leaves of various sizes.</p><p id="Par27">The Feature Pyramid Network<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> (FPN) enhances by merging feature information from feature maps of various scales within the backbone network. This process allows larger-scale feature maps to retain higher dimensional feature information while compensating for any potential loss during the downsampling of smaller-scale feature maps.</p><p id="Par28">In the original YOLOv5, a Path Aggregation Network (PAN) based on improved FPN is used as the feature fusion network. This network enables bidirectional fusion of features across different scales. However, PAN simply integrates features through a cascading structure without considering the weighted contributions of feature maps at different scales during fusion, which leads to high computational complexity and less efficient. To address these issues, Tan et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> proposed the Bidirectional Feature Pyramid Network (BiFPN) in EfficientDet. BiFPN employs a weighted feature fusion technique where different scales of features are combined using learnable weights. This adaptive mechanism helps in effectively merging features of varying resolutions and importance, leading to better feature representation. Compared to PAN, BiFPN performs feature fusion more efficiently, significantly enhancing the model&#x02019;s performance. The structural comparison of these two feature fusion networks is illustrated Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>.<fig id="Fig5"><label>Figure 5</label><caption><p>PAN and BiFPN.</p></caption><graphic xlink:href="41598_2024_82272_Fig5_HTML" id="MO5"/></fig></p></sec></sec></sec><sec id="Sec9"><title>Experiments</title><sec id="Sec10"><title>Experiment settings</title><p id="Par29">In the experiments conducted in this paper, consistency across all hardware and software environments was ensured to maintain the comparability and reliability of the results. The hardware configuration includes a CPU setup of 6 <inline-formula id="IEq26"><alternatives><tex-math id="M32">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq26.gif"/></alternatives></inline-formula> Xeon E5-2678 v3 and an NVIDIA GeForce RTX 2080 Ti GPU with 16GB of memory. On the software side, Ubuntu 18.04 was used as the operating system, Python 3.8 as the programming language, PyTorch 1.10 as the deep learning framework, and CUDA 11.3 for GPU acceleration.</p><p id="Par30">The input size for all training and testing images was standardized to 640 pixels to ensure consistency in image scale across different model configurations. During training, a batch size of 16 and 300 epochs were utilized. The initial learning rate was set at 0.01 to facilitate rapid convergence in the early stages of training, while a cyclic learning rate of 0.002 was employed to dynamically adjust according to changes in the loss curve. Stochastic gradient descent (SGD) was employed for backpropagation, with a momentum of 0.94 and a weight decay of 0.0005. Additionally, a confidence threshold of 0.5, and a non-maximum suppression threshold was established at 0.3.</p></sec><sec id="Sec11"><title>Evaluation metrics</title><p id="Par31">In this experiment, the evaluation metrics were primarily chosen based on the requirements for deploying the detection model in real-world agricultural settings. The main focus was on the model&#x02019;s detection performance and its lightweight nature. To evaluate the detection performance of the model, Precision (P), Recall (R), and Average Precision (AP) were used as the evaluation metrics. The three evaluation metrics are formulated as:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} P&#x00026;=\frac{TP}{TP+FP}\times 100\%\end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82272_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M34">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} R&#x00026;=\frac{TP}{TP+FN}\times 100\%\end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82272_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} AP&#x00026;=\int _{0}^{1}P\left( R\right) dR \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82272_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>In the formulas above, TP (True Positive) refers to the number of positive samples correctly identified as positive. FP (False Positive) denotes the number of negative samples incorrectly identified as positive. FN (False Negative) represents the number of positive samples incorrectly identified as negative. Furthermore, this study employs the number of parameters, floating point operations (FLOPs), and frames per second (FPS) as evaluation metrics for model lightweighting. These metrics are critical for assessing model efficiency, particularly given the limited computational resources in practical applications and the necessity to measure the degree of model lightweighting.</p></sec></sec><sec id="Sec12"><title>Experimental results</title><sec id="Sec13"><title>Comparison of backbone networks</title><p id="Par32">To demonstrate the feasibility of the lightweight improvement method proposed in this paper, this section employs CSPDarknet53, MobileNetV3<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, and ShuffleNetV2 as the respective backbone networks. The experiments compare the differences in AP, parameter count, and FLOPs among these different backbone network models. Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> presents the model detection performance of YOLOv5 under different backbone networks. The results indicate that both MobileNetV3 and ShuffleNetV2 significantly reduce the parameters and FLOPs compared to the original YOLOv5 model with CSPDarknet53 as the backbone network. Notably, the YOLOv5 model with MobileNetV3 as the backbone network exhibits the most pronounced lightweighting effect, with a reduction of 3.48M parameters and FLOPs decreasing from 15.9<inline-formula id="IEq27"><alternatives><tex-math id="M36">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 10^{9}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq27.gif"/></alternatives></inline-formula> to 6.3<inline-formula id="IEq28"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 10^{9}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq28.gif"/></alternatives></inline-formula>. However, this comes at the cost of a nearly 6% decrease in AP. Considering the objective of lightweighting the backbone network while minimizing accuracy loss, this paper selects ShuffleNetV2 as the lightweight backbone network due to its superior overall performance. Although the YOLOv5 model with ShuffleNetV2 as the backbone network has slightly higher parameters and FLOPs compared to MobileNetV3, its AP is 2.66% higher. This choice aligns better with the requirements of potato late blight leaf detection in complex environments addressed in this study.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of different backbone networks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Backbone</th><th align="left">AP@0.5 / %</th><th align="left">Params /<inline-formula id="IEq29"><alternatives><tex-math id="M38">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{6}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq29.gif"/></alternatives></inline-formula></th><th align="left">FLOPs / <inline-formula id="IEq30"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{9}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq30.gif"/></alternatives></inline-formula></th></tr></thead><tbody><tr><td align="left" rowspan="3">YOLOv5</td><td align="left">CSPDarknet</td><td align="left">74.43</td><td align="left">7.02</td><td align="left">15.94</td></tr><tr><td align="left">ShuffleNetV2</td><td align="left">72.87</td><td align="left">3.76</td><td align="left">8.16</td></tr><tr><td align="left">MobileNetV3</td><td align="left">70.21</td><td align="left">3.54</td><td align="left">6.34</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec14"><title>Comparison of attention mechanisms</title><p id="Par33">In this section, the introduction of attention mechanisms into the lightweight backbone network is discussed. The attention mechanisms incorporated include SE<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> (Squeeze-and-Excitation networks), CBAM<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> (Convolutional Block Attention Module), and CA. Experiments are conducted to compare their potential enhancements on the detection performance of the YOLOv5-ShuffleNetV2 network. The results of these experiments are presented in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> and Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Comparison of different backbone networks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Attention mechanism</th><th align="left">AP@0.5 / %</th><th align="left">Precision/ %</th><th align="left">Recall / %</th><th align="left">Params / <inline-formula id="IEq31"><alternatives><tex-math id="M40">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{6}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq31.gif"/></alternatives></inline-formula></th></tr></thead><tbody><tr><td align="left">&#x02013;</td><td align="left">72.87</td><td align="left">75.42</td><td align="left">71.38</td><td align="left">3.76</td></tr><tr><td align="left">SE</td><td align="left">75.16</td><td align="left">77.25</td><td align="left">74.25</td><td align="left">3.81</td></tr><tr><td align="left">CBAM</td><td align="left">75.91</td><td align="left">76.80</td><td align="left">74.84</td><td align="left">3.81</td></tr><tr><td align="left">CA</td><td align="left">76.51</td><td align="left">77.42</td><td align="left">76.14</td><td align="left">3.80</td></tr></tbody></table></table-wrap></p><p id="Par34">Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> provides a detailed comparison of various attention mechanisms on the model&#x02019;s performance metrics, including AP, precision, recall, and parameter count. Without introducing any attention mechanism, the network exhibits significantly lower recall compared to precision, leading to frequent misses in detecting potato late blight leaves in complex environments. When the SE mechanism is introduced, both precision and recall see improvements, with precision increasing by 1.83% and recall by 2.87 %. The CBAM further enhances AP and recall,although its precision is slightly lower than that achieved with SE. The addition of the CA leads to the greatest improvement in AP, precision, and recall, showing AP increasing by 3.64%, precision by 1.93%, and recall by 4.76%, compared to the original model without any attention mechanism. As illustrated in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, the P-R curves also demonstrates improved precision and recall upon incorporating CA. Additionally, the CA attention mechanism adds the fewest parameters, aligning well with this study&#x02019;s goal for a lightweight model. In conclusion, the model incorporating CA most effectively meets the experimental expectations by significantly enhancing the original network&#x02019;s low precision and recall for detecting potato late blight leaves, all while minimally increasing the parameter count.<fig id="Fig6"><label>Figure 6</label><caption><p>Comparison of P-R curves with and without CA.</p></caption><graphic xlink:href="41598_2024_82272_Fig6_HTML" id="MO6"/></fig></p></sec><sec id="Sec15"><title>Ablation experiments</title><p id="Par35">To validate that the enhancement strategies proposed in this paper effectively improve the detection of potato late blight leaves in complex environments. Based on the YOLOv5-ShuffleNetV2, this section includes CA and BiFPN into the network as independent validation modules for ablation experimental analysis. The experimental results are presented in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> and Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>. In the table, a &#x0201c;<inline-formula id="IEq32"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq32.gif"/></alternatives></inline-formula>&#x0201d; means the module is used, while a &#x0201c;&#x02014;&#x0201d; means it is not.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Results of ablation experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="2">Modules</th><th align="left" rowspan="2">AP@0.5 / %</th><th align="left" rowspan="2">Params/<inline-formula id="IEq33"><alternatives><tex-math id="M42">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{6}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq33.gif"/></alternatives></inline-formula></th><th align="left" rowspan="2">FLOPs /<inline-formula id="IEq34"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{6}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq34.gif"/></alternatives></inline-formula></th><th align="left" rowspan="2">Box loss</th></tr><tr><th align="left">CA</th><th align="left">BiFPN</th></tr></thead><tbody><tr><td align="left">&#x02014;</td><td align="left">&#x02014;</td><td align="left">72.87</td><td align="left">3.76</td><td align="left">8.16</td><td align="left">0.0270</td></tr><tr><td align="left"><inline-formula id="IEq35"><alternatives><tex-math id="M44">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq35.gif"/></alternatives></inline-formula></td><td align="left">&#x02014;</td><td align="left">76.51</td><td align="left">3.80</td><td align="left">8.22</td><td align="left">0.0208</td></tr><tr><td align="left">&#x02014;</td><td align="left"><inline-formula id="IEq36"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq36.gif"/></alternatives></inline-formula></td><td align="left">75.40</td><td align="left">3.83</td><td align="left">8.37</td><td align="left">0.0226</td></tr><tr><td align="left"><inline-formula id="IEq37"><alternatives><tex-math id="M46">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq37.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq38"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq38.gif"/></alternatives></inline-formula></td><td align="left">77.65</td><td align="left">3.87</td><td align="left">8.43</td><td align="left">0.0192</td></tr></tbody></table></table-wrap></p><p id="Par36">
<fig id="Fig7"><label>Figure 7</label><caption><p>Comparison of loss with different improvement modules.</p></caption><graphic xlink:href="41598_2024_82272_Fig7_HTML" id="MO7"/></fig>
</p><p id="Par37">As shown in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>, the AP of the original lightweight network improves after adding either CA or BiFPN individually, with CA showing an additional 1.11% increase compared to BiFPN. When both modules are added simultaneously, the model&#x02019;s detection performance sees further improvement, achieving a 4.78% increase in AP with only a 0.11M rise in parameters, maintaining the model&#x02019;s lightweight nature. Fig&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> illustrates the comparison of loss values during model training with different improvement modules. The results show that the model converges the fastest and achieves the lowest final loss value when both modules are implemented together. This outcome aligns with the study&#x02019;s expectations for enhancing potato late blight leaf detection in complex environments, effectively accomplishing the detection task in such scenarios.</p></sec><sec id="Sec16"><title>Comparison of different size models</title><p id="Par38">This section compares the proposed method with YOLOv5 and YOLOv8 to evaluate the balance between detection accuracy, number of parameters, and computational complexity for different model sizes. The goal is to choose the best potato late blight leaf detection model for the scenarios discussed in this paper.</p><p id="Par39">Table <xref rid="Tab4" ref-type="table">4</xref> shows that larger models improve detection accuracy for all models, especially for YOLOv5 and YOLOv8, highlighting the benefits of larger models. However, these improvements come with more parameters and higher computational needs, which may limit the deployment of the model in resource-constrained devices. YOLOv8 outperforms YOLOv5 in detection accuracy for same sizes, although models of the same size generally have more parameters and require more computation, these enhancements are manageable on devices with sufficient resources.</p><p id="Par40">The proposed method achieves a lightweight model comparable to the original YOLOv5 while also improving average precision across all size configurations. Specifically, it enhances AP by 3.22% for small size, 1.51% for medium size, and 0.86% for large size. This suggests that the proposed improvement strategy is especially effective for smaller YOLOv5 models, offering a more optimized solution for lightweight models used in detecting potato late blight on leaves. Compared to YOLOv8, the proposed model maintains higher accuracy in small size while being more lightweight. Although its detection accuracy for medium and large configurations is slightly lower than that of YOLOv8, the proposed model clearly outperforms in terms of reduced parameter count and computational load. Overall, the proposed method optimally balances detection accuracy, parameter count, and computational complexity in the small size model. It reduces parameters and computation while maintaining high accuracy, making it more suitable for applications in resource-limited edge devices.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Comparison of different size models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Size</th><th align="left">AP@0.5 / %</th><th align="left">Params / <inline-formula id="IEq39"><alternatives><tex-math id="M48">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{6}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq39.gif"/></alternatives></inline-formula></th><th align="left">FLOPs / <inline-formula id="IEq40"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{9}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq40.gif"/></alternatives></inline-formula></th></tr></thead><tbody><tr><td align="left" rowspan="3">YOLOv5</td><td align="left">S</td><td align="left">74.43</td><td align="left">7.02</td><td align="left">15.94</td></tr><tr><td align="left">M</td><td align="left">76.72</td><td align="left">20.87</td><td align="left">48.22</td></tr><tr><td align="left">L</td><td align="left">77.86</td><td align="left">46.13</td><td align="left">108.23</td></tr><tr><td align="left" rowspan="3">YOLOv8</td><td align="left">S</td><td align="left">76.11</td><td align="left">11.17</td><td align="left">28.81</td></tr><tr><td align="left">M</td><td align="left">78.46</td><td align="left">25.90</td><td align="left">79.32</td></tr><tr><td align="left">L</td><td align="left">79.58</td><td align="left">43.69</td><td align="left">165.74</td></tr><tr><td align="left" rowspan="3">Proposed method</td><td align="left">S</td><td align="left">77.65</td><td align="left">3.87</td><td align="left">8.43</td></tr><tr><td align="left">M</td><td align="left">78.23</td><td align="left">10.06</td><td align="left">21.58</td></tr><tr><td align="left">L</td><td align="left">78.72</td><td align="left">20.34</td><td align="left">43.20</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec17"><title>Comparison with other methods</title><p id="Par41">In this section, the proposed method is rigorously compared with other lightweight algorithms to substantiate its superior efficacy in detecting potato late blight leaves in complex environments. The experiments meticulously control variables and utilize the same dataset to ensure uniform experimental conditions. The outcomes of the experiment are presented in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Comparison of different lightweight networks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Modules</th><th align="left">AP@0.5 / %</th><th align="left">Params/<inline-formula id="IEq41"><alternatives><tex-math id="M50">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{6}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq41.gif"/></alternatives></inline-formula></th><th align="left">FLOPs /<inline-formula id="IEq42"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{6}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82272_Article_IEq42.gif"/></alternatives></inline-formula></th><th align="left">FPS</th></tr></thead><tbody><tr><td align="left">YOLOv5s</td><td align="left">74.43</td><td align="left">7.02</td><td align="left">15.94</td><td align="left">44</td></tr><tr><td align="left">YOLOv3-tiny</td><td align="left">69.31</td><td align="left">8.67</td><td align="left">12.99</td><td align="left"><bold>87</bold></td></tr><tr><td align="left">YOLOv5-ghost<sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td><td align="left">73.89</td><td align="left"><bold>3.46</bold></td><td align="left"><bold>7.21</bold></td><td align="left">48</td></tr><tr><td align="left">Proposed method</td><td align="left"><bold>77.65</bold></td><td align="left">3.87</td><td align="left">8.43</td><td align="left">51</td></tr></tbody></table></table-wrap></p><p id="Par42">Based on the comparison data in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>, it is evident that the YOLOv5s algorithm has the highest FLOPs and the slowest detection speed. In contrast, the YOLOv3-tiny algorithm achieves the fastest detection speed at 87 FPS but has the lowest AP. The YOLOv5-ghost algorithm, while being the lightest, has lower AP and FPS compared to the method proposed in this paper. The proposed method exhibits the highest AP, with parameters and FLOPs slightly higher than YOLOv5-ghost but significantly lower than other methods. It achieves an AP of 77.65% and a detection speed of 51 FPS, both surpassing YOLOv5s and YOLOv5-ghost. In conjunction with Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>, the method proposed in this paper achieves an optimal balance among detection accuracy, model complexity, and detection speed, resulting in superior overall performance. It proves to be an effective approach for detecting late blight on potato leaves in complex environments.<fig id="Fig8"><label>Figure 8</label><caption><p>Detection results of different lightweight networks.</p></caption><graphic xlink:href="41598_2024_82272_Fig8_HTML" id="MO8"/></fig></p></sec></sec><sec id="Sec18"><title>Conclusion</title><p id="Par43">In this work, an enhanced YOLOv5 model designed for deployment in farmland settings was introduced, with the aim of detecting potato late blight leaves in complex environments. A specialized dataset was created, followed by extensive experiments. The main work is in the following aspects: Firstly, various backbone networks were compared, and ShuffleNetV2 was selected as the lightweight backbone. This choice significantly reduces the model&#x02019;s parameters and computation while maintaining detection accuracy, facilitating deployment in resource-constrained embedded devices. Secondly, building on lightweighting, the study examined various attention mechanisms&#x02019; effects on model performance. Results indicated that introducing the CA mechanism significantly improved the model&#x02019;s AP value and recall rate, effectively reducing missed detections of potato late blight leaves in the original model. Next, ablation experiments verified that combining CA and BiFPN further enhances the model&#x02019;s performance while preserving its lightweight feature. Finally, the proposed method is compared with other methods. Experiments show that the proposed improvement strategy achieved the highest increase in AP at small size, reaching a value of 3.22%, while also offering the most lightweight solution. Compared to other lightweight detection methods, it excels in detection accuracy and speed, achieving an optimal balance in overall performance. In summary, this paper&#x02019;s key contribution is the development of an efficient and lightweight method for detecting potato late blight in complex environments. This method balances detection accuracy, model complexity, and speed, offering effective technical support for diagnosing late blight in real potato farmland scenarios.</p><p id="Par44">The proposed method accurately and quickly detects potato late blight leaves in complex environments but has limitations. It can identify diseased leaves but cannot measure how severe the disease is. This is important in potato farming because different severity levels need different treatments to keep plants healthy and productive. The inability of the current method to assess severity could potentially reduce control effectiveness and lead to resource waste or unnecessary chemical use. Therefore, future research should include area calculations for leaves and diseases based on the proposed detection model. Additionally, it should precisely assess potato late blight severity through diseased area percentage. This severity grading will offer more effective decision support and a comprehensive diagnostic information for the control of potato late blight.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This work has received support from the Yunnan Provincial Department of Science and Technology under the Major Science and Technology Special Project, Grant No.202402AE090017. Sincere gratitude is extended for this assistance.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>J.L. conceived the experiments, J.W. conducted the experiments, R.L. collected and processed the data, G.S. and K.Z. analyzed the results, X.L. wrote the main content of the manuscript, C.W. and T.Z. provided critical revisions on the experiments. All authors reviewed the manuscript and provided feedback.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets generated and analyzed during the current study are available from the corresponding author on reasonable request.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par48">The authors declare that they have no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>S</given-names></name><name><surname>Kooner</surname><given-names>R</given-names></name><name><surname>Arora</surname><given-names>R</given-names></name></person-group><article-title>Insect pests and crop losses</article-title><source>Breeding Insect Resistant Crops Sustain. Agricul.</source><year>2017</year><pub-id pub-id-type="doi">10.1007/978-981-10-6056-4_2</pub-id></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Sharma, S., Kooner, R. &#x00026; Arora, R. Insect pests and crop losses. <italic>Breeding Insect Resistant Crops Sustain. Agricul.</italic>[SPACE]10.1007/978-981-10-6056-4_2 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Zaheer</surname><given-names>K</given-names></name><name><surname>Akhtar</surname><given-names>MH</given-names></name></person-group><article-title>Potato production, usage, and nutrition-a review</article-title><source>Crit. Rev. Food Sci. Nutr.</source><year>2016</year><volume>56</volume><fpage>711</fpage><lpage>721</lpage><pub-id pub-id-type="doi">10.1080/10408398.2012.724479</pub-id><pub-id pub-id-type="pmid">24925679</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Zaheer, K. &#x00026; Akhtar, M. H. Potato production, usage, and nutrition-a review. <italic>Crit. Rev. Food Sci. Nutr.</italic><bold>56</bold>, 711&#x02013;721. 10.1080/10408398.2012.724479 (2016).<pub-id pub-id-type="pmid">24925679</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Chakraborty</surname><given-names>K</given-names></name><name><surname>Mukherjee</surname><given-names>R</given-names></name><name><surname>Chakroborty</surname><given-names>C</given-names></name><name><surname>Bora</surname><given-names>K</given-names></name></person-group><article-title>Automated recognition of optical image based potato leaf blight diseases using deep learning</article-title><source>Physiol. Molecular Plant Pathol.</source><year>2022</year><volume>117</volume><fpage>101781</fpage><pub-id pub-id-type="doi">10.1016/j.pmpp.2022.101781</pub-id></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Chakraborty, K., Mukherjee, R., Chakroborty, C. &#x00026; Bora, K. Automated recognition of optical image based potato leaf blight diseases using deep learning. <italic>Physiol. Molecular Plant Pathol.</italic><bold>117</bold>, 101781. 10.1016/j.pmpp.2022.101781 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Fern&#x000e1;ndez</surname><given-names>C</given-names></name><name><surname>Leblon</surname><given-names>B</given-names></name><name><surname>Haddadi</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name></person-group><article-title>Potato late blight detection at the leaf and canopy levels based in the red and red-edge spectral regions</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><fpage>1292</fpage><pub-id pub-id-type="doi">10.3390/rs12081292</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Fern&#x000e1;ndez, C., Leblon, B., Haddadi, A., Wang, K. &#x00026; Wang, J. Potato late blight detection at the leaf and canopy levels based in the red and red-edge spectral regions. <italic>Remote Sens.</italic><bold>12</bold>, 1292. 10.3390/rs12081292 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Sharma, R., Singh, A., Dutta, M., Riha, K. &#x00026; Kriz, P. Image processing based automated identification of late blight disease from leaf images of potato crops. In <italic>2017 40th international conference on telecommunications and signal processing (TSP)</italic>, 758&#x02013;762, 10.1109/TSP.2017.8076116 (IEEE, 2017).</mixed-citation></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Paluchowska</surname><given-names>P</given-names></name><name><surname>&#x0015a;liwka</surname><given-names>J</given-names></name><name><surname>Yin</surname><given-names>Z</given-names></name></person-group><article-title>Late blight resistance genes in potato breeding</article-title><source>Planta</source><year>2022</year><volume>255</volume><fpage>127</fpage><pub-id pub-id-type="doi">10.1007/s00425-022-03984-2</pub-id><pub-id pub-id-type="pmid">35576021</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Paluchowska, P., &#x0015a;liwka, J. &#x00026; Yin, Z. Late blight resistance genes in potato breeding. <italic>Planta</italic><bold>255</bold>, 127. 10.1007/s00425-022-03984-2 (2022).<pub-id pub-id-type="pmid">35576021</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Domingues</surname><given-names>T</given-names></name><name><surname>Brand&#x000e3;o</surname><given-names>T</given-names></name><name><surname>Ferreira</surname><given-names>J</given-names></name></person-group><article-title>Machine learning for detection and prediction of crop diseases and pests: A comprehensive survey</article-title><source>Agriculture</source><year>2022</year><volume>12</volume><fpage>1350</fpage><pub-id pub-id-type="doi">10.3390/agriculture12091350</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Domingues, T., Brand&#x000e3;o, T. &#x00026; Ferreira, J. Machine learning for detection and prediction of crop diseases and pests: A comprehensive survey. <italic>Agriculture</italic><bold>12</bold>, 1350. 10.3390/agriculture12091350 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Fenu, G. &#x00026; Malloci, F. An application of machine learning technique in forecasting crop disease. In <italic>Proceedings of the 3rd International Conference on Big Data Research</italic>, 76&#x02013;82, 10.1145/3365224.3365237 (2019).</mixed-citation></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="book"><person-group person-group-type="author"><name><surname>Nidhis</surname><given-names>A</given-names></name><name><surname>Pardhu</surname><given-names>C</given-names></name><name><surname>Reddy</surname><given-names>K</given-names></name><name><surname>Deepa</surname><given-names>K</given-names></name></person-group><source>Cluster based paddy leaf disease detection, classification and diagnosis in crop health monitoring unit, 281&#x02013;291</source><year>2019</year><publisher-name>Springer International Publishing</publisher-name></element-citation><mixed-citation id="mc-CR9" publication-type="book">Nidhis, A., Pardhu, C., Reddy, K. &#x00026; Deepa, K. <italic>Cluster based paddy leaf disease detection, classification and diagnosis in crop health monitoring unit, 281&#x02013;291</italic> (Springer International Publishing, 2019).</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Govardhan, M. &#x00026; Veena, M. Diagnosis of tomato plant diseases using random forest. In <italic>2019 Global Conference for Advancement in Technology (GCAT)</italic>, 1&#x02013;5, 10.1109/GCAT47503.2019.8978443 (IEEE, 2019).</mixed-citation></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Eunice</surname><given-names>J</given-names></name><name><surname>Popescu</surname><given-names>D</given-names></name><name><surname>Chowdary</surname><given-names>M</given-names></name><name><surname>Hemanth</surname><given-names>J</given-names></name></person-group><article-title>Deep learning-based leaf disease detection in crops using images for agricultural applications</article-title><source>Agronomy</source><year>2022</year><volume>12</volume><fpage>2395</fpage><pub-id pub-id-type="doi">10.3390/agronomy12102395</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Eunice, J., Popescu, D., Chowdary, M. &#x00026; Hemanth, J. Deep learning-based leaf disease detection in crops using images for agricultural applications. <italic>Agronomy</italic><bold>12</bold>, 2395. 10.3390/agronomy12102395 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Luo</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name></person-group><article-title>Ddvc-yolov5: An improved yolov5 model for road defect detection</article-title><source>IEEE Access</source><year>2024</year><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3453914</pub-id></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Zhong, S., Chen, C., Luo, W. &#x00026; Chen, S. Ddvc-yolov5: An improved yolov5 model for road defect detection. <italic>IEEE Access</italic>[SPACE]10.1109/ACCESS.2024.3453914 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Mohanty</surname><given-names>S</given-names></name><name><surname>Hughes</surname><given-names>D</given-names></name><name><surname>Salath&#x000e9;</surname><given-names>M</given-names></name></person-group><article-title>Using deep learning for image-based plant disease detection</article-title><source>Front. Plant Sci.</source><year>2016</year><volume>7</volume><fpage>1419</fpage><pub-id pub-id-type="doi">10.3389/fpls.2016.01419</pub-id><pub-id pub-id-type="pmid">27713752</pub-id>
</element-citation><mixed-citation id="mc-CR13" publication-type="journal">Mohanty, S., Hughes, D. &#x00026; Salath&#x000e9;, M. Using deep learning for image-based plant disease detection. <italic>Front. Plant Sci.</italic><bold>7</bold>, 1419. 10.3389/fpls.2016.01419 (2016).<pub-id pub-id-type="pmid">27713752</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Zhu</surname><given-names>Q</given-names></name><name><surname>Miao</surname><given-names>W</given-names></name></person-group><article-title>Tomato brown rot disease detection using improved yolov5 with attention mechanism</article-title><source>Front. Plant Sci.</source><year>2023</year><volume>14</volume><fpage>1289464</fpage><pub-id pub-id-type="doi">10.3389/fpls.2023.1289464</pub-id><pub-id pub-id-type="pmid">38053763</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Liu, J., Wang, X., Zhu, Q. &#x00026; Miao, W. Tomato brown rot disease detection using improved yolov5 with attention mechanism. <italic>Front. Plant Sci.</italic><bold>14</bold>, 1289464. 10.3389/fpls.2023.1289464 (2023).<pub-id pub-id-type="pmid">38053763</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name></person-group><article-title>Fsm-yolo: Apple leaf disease detection network based on adaptive feature capture and spatial context awareness</article-title><source>Digital Signal Process.</source><year>2024</year><volume>155</volume><fpage>104770</fpage><pub-id pub-id-type="doi">10.1016/j.dsp.2024.104770</pub-id></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Yan, C. &#x00026; Yang, K. Fsm-yolo: Apple leaf disease detection network based on adaptive feature capture and spatial context awareness. <italic>Digital Signal Process.</italic><bold>155</bold>, 104770. 10.1016/j.dsp.2024.104770 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Leng</surname><given-names>S</given-names></name><name><surname>Musha</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name></person-group><article-title>Cemlb-yolo: efficient detection model of maize leaf blight in complex field environments</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><fpage>9285</fpage><pub-id pub-id-type="doi">10.3390/app13169285</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Leng, S., Musha, Y., Yang, Y. &#x00026; Feng, G. Cemlb-yolo: efficient detection model of maize leaf blight in complex field environments. <italic>Appl. Sci.</italic><bold>13</bold>, 9285. 10.3390/app13169285 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Ma, N., Zhang, X. &#x00026; Zhang, H. Shufflenet v2: practical guidelines for efficient cnn architecture design. In <italic>European Conference on Computer Vision (ECCV)</italic>, 10.1007/978-3-030-01264-9_1 (Springer, Cham, 2018).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Hou, Q., Zhou, D. &#x00026; Feng, J. Coordinate attention for efficient mobile network design. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, 13713&#x02013;13722, 10.1109/CVPR46437.2021.01349 (2021).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Tan, M., Pang, R. &#x00026; Le, Q. Efficientdet: Scalable and efficient object detection. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, 10781&#x02013;10790, 10.1109/CVPR42600.2020.01080 (2020).</mixed-citation></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="book"><person-group person-group-type="author"><name><surname>Bacanin</surname><given-names>N</given-names></name><etal/></person-group><source>A novel multiswarm firefly algorithm: An application forplant classification</source><year>2022</year><publisher-name>Springer</publisher-name></element-citation><mixed-citation id="mc-CR20" publication-type="book">Bacanin, N. et al. <italic>A novel multiswarm firefly algorithm: An application forplant classification</italic> (Springer, 2022). 10.1007/978-3-031-09173-5_115.</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Bukumira</surname><given-names>M</given-names></name><etal/></person-group><article-title>Carrot grading system using computer vision feature parameters and a cascaded graph convolutional neural network</article-title><source>J. Electron. Imaging</source><year>2022</year><volume>31</volume><fpage>061815</fpage><lpage>061815</lpage><pub-id pub-id-type="doi">10.1117/1.JEI.31.6.061815</pub-id></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Bukumira, M. et al. Carrot grading system using computer vision feature parameters and a cascaded graph convolutional neural network. <italic>J. Electron. Imaging</italic><bold>31</bold>, 061815&#x02013;061815. 10.1117/1.JEI.31.6.061815 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Wongchai</surname><given-names>A</given-names></name><etal/></person-group><article-title>Farm monitoring and disease prediction by classification based on deep learning architectures in sustainable agriculture</article-title><source>Ecol. Modell.</source><year>2022</year><volume>474</volume><fpage>110167</fpage><pub-id pub-id-type="doi">10.1016/j.ecolmodel.2022.110167</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Wongchai, A. et al. Farm monitoring and disease prediction by classification based on deep learning architectures in sustainable agriculture. <italic>Ecol. Modell.</italic><bold>474</bold>, 110167. 10.1016/j.ecolmodel.2022.110167 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>H</given-names></name><etal/></person-group><article-title>Leaf disease recognition based on channel information attention network</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>83</volume><fpage>6601</fpage><lpage>6619</lpage><pub-id pub-id-type="doi">10.3923/itj.2011.267.275</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Deng, H. et al. Leaf disease recognition based on channel information attention network. <italic>Multimed. Tools Appl.</italic><bold>83</bold>, 6601&#x02013;6619. 10.3923/itj.2011.267.275 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Asif, K., Rahman, M. &#x00026; Hena, M. Cnn based disease detection approach on potato leaves. In <italic>2020 3rd International Conference on Intelligent Sustainable Systems (ICISS)</italic>, 428&#x02013;432, 10.1109/ICISS49785.2020.9315921 (IEEE, 2020).</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Qu</surname><given-names>S</given-names></name><name><surname>Cui</surname><given-names>C</given-names></name><name><surname>Duan</surname><given-names>J</given-names></name><name><surname>Lu</surname><given-names>Y</given-names></name><name><surname>Pang</surname><given-names>Z</given-names></name></person-group><article-title>Underwater small target detection under yolov8-la model</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>16108</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-66950-w</pub-id><pub-id pub-id-type="pmid">38997415</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Qu, S., Cui, C., Duan, J., Lu, Y. &#x00026; Pang, Z. Underwater small target detection under yolov8-la model. <italic>Sci. Rep.</italic><bold>14</bold>, 16108. 10.1038/s41598-024-66950-w (2024).<pub-id pub-id-type="pmid">38997415</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Jovanovic</surname><given-names>L</given-names></name><etal/></person-group><article-title>Computer vision based areal photographic rocket detection using yolov8 models</article-title><source>Int. J. Robot. Automation Technol.</source><year>2024</year><volume>11</volume><fpage>37</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.31875/2409-9694</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Jovanovic, L. et al. Computer vision based areal photographic rocket detection using yolov8 models. <italic>Int. J. Robot. Automation Technol.</italic><bold>11</bold>, 37&#x02013;49. 10.31875/2409-9694 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Qiao</surname><given-names>Y</given-names></name><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>He</surname><given-names>D</given-names></name></person-group><article-title>Deep learning based automatic grape downy mildew detection</article-title><source>Front. Plant Sci.</source><year>2022</year><volume>13</volume><fpage>872107</fpage><pub-id pub-id-type="doi">10.3389/fpls.2022.872107</pub-id><pub-id pub-id-type="pmid">35755646</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Zhang, Z., Qiao, Y., Guo, Y. &#x00026; He, D. Deep learning based automatic grape downy mildew detection. <italic>Front. Plant Sci.</italic><bold>13</bold>, 872107. 10.3389/fpls.2022.872107 (2022).<pub-id pub-id-type="pmid">35755646</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Y</given-names></name><name><surname>Lin</surname><given-names>C</given-names></name><name><surname>Wu</surname><given-names>N</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name></person-group><article-title>Apeiou integration for enhanced yolov7: Achieving efficient plant disease detection</article-title><source>Agriculture</source><year>2024</year><volume>14</volume><fpage>820</fpage><pub-id pub-id-type="doi">10.3390/agriculture14060820</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Zhao, Y., Lin, C., Wu, N. &#x00026; Xu, X. Apeiou integration for enhanced yolov7: Achieving efficient plant disease detection. <italic>Agriculture</italic><bold>14</bold>, 820. 10.3390/agriculture14060820 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Abdullah</surname><given-names>A</given-names></name><etal/></person-group><article-title>A deep-learning-based model for the detection of diseased tomato leaves</article-title><source>Agronomy</source><year>2024</year><volume>14</volume><fpage>1593</fpage><pub-id pub-id-type="doi">10.3390/agronomy14071593</pub-id></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Abdullah, A. et al. A deep-learning-based model for the detection of diseased tomato leaves. <italic>Agronomy</italic><bold>14</bold>, 1593. 10.3390/agronomy14071593 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Pine wilt disease detection in high-resolution uav images using object-oriented classification</article-title><source>J. Forestry Res.</source><year>2021</year><pub-id pub-id-type="doi">10.1007/s11676-021-01357-y</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Sun, Z. et al. Pine wilt disease detection in high-resolution uav images using object-oriented classification. <italic>J. Forestry Res.</italic>[SPACE]10.1007/s11676-021-01357-y (2021).</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Ran</surname><given-names>X</given-names></name></person-group><article-title>Deep learning with edge computing: A review</article-title><source>Proc. IEEE</source><year>2019</year><volume>107</volume><fpage>1655</fpage><lpage>1674</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2019.2921977</pub-id></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Chen, J. &#x00026; Ran, X. Deep learning with edge computing: A review. <italic>Proc. IEEE</italic><bold>107</bold>, 1655&#x02013;1674. 10.1109/JPROC.2019.2921977 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Niu</surname><given-names>Z</given-names></name><name><surname>Zhong</surname><given-names>G</given-names></name><name><surname>Yu</surname><given-names>H</given-names></name></person-group><article-title>A review on the attention mechanism of deep learning</article-title><source>Neurocomputing</source><year>2021</year><volume>452</volume><fpage>48</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2021.03.091</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Niu, Z., Zhong, G. &#x00026; Yu, H. A review on the attention mechanism of deep learning. <italic>Neurocomputing</italic><bold>452</bold>, 48&#x02013;62. 10.1016/j.neucom.2021.03.091 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Karthik</surname><given-names>R</given-names></name><etal/></person-group><article-title>Attention embedded residual cnn for disease detection in tomato leaves</article-title><source>Appl. Soft Comput.</source><year>2020</year><volume>86</volume><fpage>105933</fpage><pub-id pub-id-type="doi">10.1016/j.asoc.2019.105933</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Karthik, R. et al. Attention embedded residual cnn for disease detection in tomato leaves. <italic>Appl. Soft Comput.</italic><bold>86</bold>, 105933. 10.1016/j.asoc.2019.105933 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Lin, T. <italic>et al.</italic> Feature pyramid networks for object detection. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, 2117&#x02013;2125, 10.1109/CVPR.2017.106 (2017).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Howard, A. <italic>et al.</italic> Searching for mobilenetv3. In <italic>Proceedings of the IEEE/CVF international conference on computer vision</italic>, 1314&#x02013;1324, 10.1109/ICCV.2019.00142 (2019).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Hu, J., Shen, L. &#x00026; Sun, G. Squeeze-and-excitation networks. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, 7132&#x02013;7141, 10.1109/CVPR.2018.00745 (2018).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Woo, S., Park, J., Lee, J. &#x00026; Kweon, I. Cbam: Convolutional block attention module. In <italic>Proceedings of the European conference on computer vision (ECCV)</italic>, 3&#x02013;19, 10.1007/978-3-030-01234-2_1 (2018).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Dou, X., Wang, T. &#x00026; Shao, S. A lightweight yolov5 model integrating ghostnet and attention mechanism. <italic>2023 4th International Conference on Computer Vision, Image and Deep Learning (CVIDL)</italic> 348&#x02013;352, 10.1109/CVIDL58838.2023.10166155 (2023).</mixed-citation></ref></ref-list></back></article>