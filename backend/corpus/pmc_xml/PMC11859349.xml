<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006219</article-id><article-id pub-id-type="pmc">PMC11859349</article-id><article-id pub-id-type="doi">10.3390/s25040991</article-id><article-id pub-id-type="publisher-id">sensors-25-00991</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Light Field Angular Super-Resolution via Spatial-Angular Correlation Extracted by Deformable Convolutional Network</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-0861-5745</contrib-id><name><surname>Li</surname><given-names>Daichuan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0126-7543</contrib-id><name><surname>Zhong</surname><given-names>Rui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="c1-sensors-25-00991" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Yungang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Ledoux-Rak</surname><given-names>Isabelle</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-00991">School of Computer Science, Central China Normal University, Wuhan 430079, China; <email>dcli@mails.ccnu.edu.cn</email> (D.L.); <email>yungangyang@mails.ccnu.edu.cn</email> (Y.Y.)</aff><author-notes><corresp id="c1-sensors-25-00991"><label>*</label>Correspondence: <email>zhongr@ccnu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>07</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>991</elocation-id><history><date date-type="received"><day>20</day><month>11</month><year>2024</year></date><date date-type="rev-recd"><day>17</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>27</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Light Field Angular Super-Resolution (LFASR) addresses the issue where Light Field (LF) images can not simultaneously achieve both high spatial and angular resolution due to the limited resolution of optical sensors. Since Spatial-Angular Correlation (SAC) features are closely related to the structure of LF images, its accurate and complete extraction is crucial for the quality of LF images reconstructed by the LFASR method based on Deep Neural Networks (DNNs). In low-angular resolution LF images, SAC features must be extracted from a limited number of pixels that are at a great distance from each other and exhibit strong correlations. However, existing LFASR methods based on DNNs fail to extract SAC features accurately and completely. Due to the limited receptive field, methods based on regular Convolutional Neural Networks (CNNs) are unable to capture SAC features from distant pixels, leading to incomplete SAC feature extraction. On the other hand, methods based on large convolution kernels and attention mechanisms use an excessive number of pixels to extract SAC features, resulting in insufficient accuracy in extracted SAC features. To solve this problem, we introduce Deformable Convolutional Network (DCN), which adaptively changes the position of limited sampling point using offsets, so as to extract SAC from distant pixels. In addition, in order to make the offset of DCN more accurate and further improve the accuracy of SAC features, we also propose a Multi-Maximum-Offsets Fusion DCN (MMOF-DCN). MMOF-DCN can reduce the exploration range of finding the desired offset, thereby improving the offset finding efficiency. Experiment results show that our proposed method has advantages in real-world dataset and synthetic dataset. The PSNR value in synthetic dataset which have large disparity is improved by 0.45 dB compared to existing methods.</p></abstract><kwd-group><kwd>light field</kwd><kwd>angular super-resolution</kwd><kwd>optical sensors</kwd><kwd>reconstruct</kwd><kwd>deep neural network</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62002130</award-id><award-id>62201222</award-id></award-group><award-group><funding-source>Fundamental Research Funds for the Central Universities</funding-source><award-id>CCNU22QN014</award-id><award-id>CCNU22XJ034</award-id><award-id>CCNU22JC007</award-id></award-group><award-group><funding-source>National Key Research and Development Program of China</funding-source><award-id>2022YFD1700204</award-id></award-group><award-group><funding-source>Fonds Wetenschappelijk Onderzoek (FWO)</funding-source><award-id>G094122N</award-id></award-group><funding-statement>This research was funded in part by the National Natural Science Foundation of China under Grant 62002130 and Grant 62201222; in part by the Fundamental Research Funds for the Central Universities under Grant CCNU22QN014, Grant CCNU22XJ034, and Grant CCNU22JC007; in part by the National Key Research and Development Program of China under Grant 2022YFD1700204; and in part by Fonds Wetenschappelijk Onderzoek (FWO), Vlaanderen, under Project G094122N.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-00991"><title>1. Introduction</title><p>High-resolution Light Field (LF) image possess the unique capability to encapsulate both comprehensive spatial and angular information, making them highly valuable in applications such as Virtual Reality [<xref rid="B1-sensors-25-00991" ref-type="bibr">1</xref>], depth estimation [<xref rid="B2-sensors-25-00991" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-00991" ref-type="bibr">3</xref>], and 3D reconstruction [<xref rid="B4-sensors-25-00991" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-00991" ref-type="bibr">5</xref>]. However, acquiring high-resolution LF image directly remains a significant challenge. Specifically, LF images are captured using microlens arrays, but the limited number of optical sensor pixels in these arrays poses a trade-off: recording light direction (angular resolution) necessitates dedicating some optical sensor pixels to capture light from different directions, leading to reduced spatial resolution. To overcome this limitation, Light Field Angular Super-Resolution (LFASR) techniques have been developed to reconstruct High Angular Resolution (HAR) LF images from Low Angular Resolution (LAR) LF images, which preserve high spatial resolution but have LAR.</p><p>In recent years, Deep Neural Networks (DNNs) have shown strong reconstructive ability in LFASR [<xref rid="B6-sensors-25-00991" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-00991" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-00991" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>]. Accurate and complete extraction of the main features in LAR LF images is crucial for improving the quality of HAR LF images reconstructed using DNN-based LFASR methods. As shown in <xref rid="sensors-25-00991-f001" ref-type="fig">Figure 1</xref>, these main features typically include spatial features extracted from Sub-Aperture Image (SAI), angular features extracted from Macro-Pixel Image (MacPI), and Epipolar Plane Image (EPI) features extracted from EPI.</p><p>Although the existing DNN-based LFASR methods are highly effective, they overlook the fact that EPI contains Spatial-Angular Correlation (SAC) in LF images and fail to accurately and completely extract SAC features. This results in artifacts and blurring in the reconstructed HAR LF images. SAC features are closely related to the disparity in LF images [<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>], and their accurate and complete extraction can significantly improve the quality of reconstructed HAR LF images. As shown in <xref rid="sensors-25-00991-f002" ref-type="fig">Figure 2</xref>, in the EPIs of LAR LF images, SAC feature must be extracted from a limited number of strongly correlated pixels at greater distances due to the loss of angular resolution. Due to the limited receptive field, existing methods based on regular Convolutional Neural Networks (CNNs) [<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>] cannot completely extract SAC features from pixels at greater distances. Methods using large convolution kernels [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] and attention mechanisms [<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>] fail to accurately extract SAC features, as they establish relationships across a large number of pixels and introduce many weakly correlated pixels for feature extraction. Therefore, it is essential to design an LFASR method capable of extracting SAC features using a limited number of highly correlated pixels at greater distances.</p><p>To solve this problem, we introduce Deformable Convolutional Network (DCN) [<xref rid="B13-sensors-25-00991" ref-type="bibr">13</xref>] on the EPI to extract SAC features, enabling adaptive adjustment of sampling point positions through learnable offsets. Unlike regular CNNs constrained by small fixed convolutional kernels, DCNs dynamically adapt their receptive fields based on the distances between pixels needed for SAC feature extraction. Additionally, unlike methods that use large convolution kernels or attention mechanisms, DCNs focus only on a limited number of strongly correlated pixels, effectively preventing SAC feature extraction from being compromised by the inclusion of weakly correlated pixels.</p><p>Although DCN is capable of efficiently selecting a limited number of the most relevant points over a large range to extract SAC features, its fixed maximum offset imposes limitations, leading to inaccuracies and incomplete SAC feature extraction when SAC features that involve varying pixel distances. As shown in <xref rid="sensors-25-00991-f002" ref-type="fig">Figure 2</xref>b, the distances between pixels required for SAC feature extraction in LF scenes vary due to the non-uniform disparities of objects in the scene. This characteristic leads to the following issues when directly using a DCN with a fixed maximum offset: if a DCN with a larger maximum offset is used to extract SAC features with shorter distances, the accuracy of SAC extraction decreases; conversely, if a DCN with a smaller maximum offset is used to extract SAC features with great distances, the SAC cannot be extracted.</p><p>To solve this problem, we design a Multi-Maximum-Offsets Fusion DCN (MMOF-DCN). In this design, we first analyzed the distribution of sampling point offsets needed to extract different SAC features from LF images. Based on this analysis, we created multiple DCNs with different maximum offsets, which are then fused using an Adaptive Weight Mechanism (AWM). This design allows the DCN to more accurately focus on the appropriate offset range for each SAC feature, improving the precision of the extracted offsets and, ultimately, enhancing the accuracy of SAC feature extraction.</p><p>In summary, the contributions of this paper are as follows:<list list-type="bullet"><list-item><p>To address the difficulty in extracting SAC features on the EPI of LAR LF images due to the increased distance between pixels required for SAC extraction, we introduce DCN to the EPI to adaptively adjust the sampling point positions according to the distance between the required pixels for SAC extraction.</p></list-item><list-item><p>To address the issue that DCN with a fixed maximum offset cannot accurately and completely extract SAC features with varying pixel distances in LF scenes, we employ the Canny operator to compute and analyze the offsets required by DCN for extracting different SAC features. Based on this analysis, we propose a MMOF-DCN method. This method adaptively selects more precise sampling point offsets for different SAC features, thereby significantly enhancing the accuracy of SAC feature extraction.</p></list-item><list-item><p>We conducted extensive experiments on both real-world and synthetic datasets. Experimental results show that our method outperforms existing methods under both quantitative and qualitative conditions. The PSNR on synthetic dataset with large disparity is 0.45 dB higher than that of existing methods.</p></list-item></list></p></sec><sec id="sec2-sensors-25-00991"><title>2. Related&#x000a0;Work</title><p>LF angular super-resolution methods based on DNNs are usually divided into depend disparity estimation methods and non-depend disparity estimation methods. The depend disparity estimation methods [<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-00991" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-00991" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-00991" ref-type="bibr">18</xref>] usually use deep learning techniques to estimate the disparity of LF images and combine it with warp operations to reconstruct novel views. Kalantari et al. [<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>] used CNN to implement disparity estimation and color estimation. Jin et al. [<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>] predicted a disparity map for each view and introduced a blending strategy to recover the geometry structure of the EPIs. Ko et al. [<xref rid="B16-sensors-25-00991" ref-type="bibr">16</xref>] extracted multi-view features by disparity estimation and refined the features using an adaptive blending method. In LF scenes without occlusion, disparity estimation is generally accurate, allowing the depend disparity estimation methods to effectively reconstruct novel views. However, when occlusion occurs in LF scenes, the accuracy of disparity estimation significantly decreases. This results in degraded image quality of the reconstructed HAR LF images.</p><p>The non-depend disparity estimation methods [<xref rid="B6-sensors-25-00991" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-00991" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-00991" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>,<xref rid="B19-sensors-25-00991" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-00991" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-00991" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-00991" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-00991" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-00991" ref-type="bibr">24</xref>] usually use deep learning techniques to extract features from each feature domain of LF images and combine these features with upsampling methods to reconstruct HAR LF images. Yeung et al. [<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>] alternately use spatial convolution and angular convolution to simulate 4D convolution to extract features from LF images. Wang et al. [<xref rid="B19-sensors-25-00991" ref-type="bibr">19</xref>] proposed a pseudo 4DCNN to achieve LFASR task in horizontal and vertical directions step by step. Wu et al. [<xref rid="B8-sensors-25-00991" ref-type="bibr">8</xref>] proposed an anti-aliasing framework based on Laplacian pyramid to optimize the output ASR LF image. Wang et al. [<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>] proposed a decoupling mechanism for LF images. They proposed different convolutions on MacPIs to extract spatial features, angular features and EPI features. Liu et al. [<xref rid="B11-sensors-25-00991" ref-type="bibr">11</xref>] used CNN to achieve ASR, and then used angular Transformer and deblurring network to optimize the blurred ASR LF image. Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] replace global modeling in Transformer with large convolutional kernels to build local but broader relationships that represent non-local spatial features, angular features, and EPI features. However, these methods fail to recognize that the features on the EPI are SAC features associated with disparity. Additionally, the distance between the pixels required to extract SAC features becomes distant in LAR LF images, and is only related to a limited number of points. Without accurate extraction of SAC features, it is impossible to preserve a consistent disparity structure in the reconstructed HAR LF image, resulting in noticeable artifacts and blurring.</p></sec><sec id="sec3-sensors-25-00991"><title>3. SAC Feature&#x000a0;Analysis</title><sec id="sec3dot1-sensors-25-00991"><title>3.1. The Definition and Significance of the SAC&#x000a0;Feature</title><p>For any point (a given object point) <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in 3D scene, its imaging point on the sensor is recorded as <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> triplet, where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the imaging position coordinate and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the pixel value. At <italic toggle="yes">n</italic> different views, disparity means that <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is projected or imaged to different positions on <italic toggle="yes">n</italic> SAIs. At the same time, the pixel values at these <italic toggle="yes">n</italic> different imaging positions are correlated, called SAC.</p><p>SAC is closely related to disparity. The cause of disparity is related to the imaging principle of a camera which projects a 3D object from the world coordinate system onto the camera&#x02019;s focal plane. This process essentially involves coordinate transformations through a series of transformation matrix multiplications, ultimately yielding the pixel coordinates in the image. Since the transformation matrices change with the camera position, different views will result in different pixel coordinates [<xref rid="B25-sensors-25-00991" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-00991" ref-type="bibr">26</xref>]. When creating a pair of images from two adjacent cameras locations on a certain plane, a given object point <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> will project to different pixel locations in the two images, potentially several pixels apart. The distance between these two projected locations is related to disparity. Moreover, the pixel values at these two locations are correlated. In LF images, this correlation of pixel values aligns with SAC.</p><p>Using the EPI to study SAC features is highly beneficial, as the projections of an object from different views in the EPI form a distinct slash line. As shown in <xref rid="sensors-25-00991-f001" ref-type="fig">Figure 1</xref>a, the EPI is typically created by cutting the LF image along fixed spatial and angular directions according to the view sequence and then stacking the slices. In the EPI, an object is projected onto views in a fixed direction. Due to the consistent disparity of the object, the projection coordinates change in a regular pattern. When these projections are stacked in view order, as shown in <xref rid="sensors-25-00991-f002" ref-type="fig">Figure 2</xref>, the EPI of LF image reveals that pixels sharing the same SAC align along a obvious slash line. The slope of this slash line directly corresponds to the disparity of objects in the LF scene [<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>]. Since the disparity of objects within an LF scene remains constant, the SAC of the same object is invariant across both HAR LF images and LAR LF images. Consequently, accurately extracting SAC features from LAR LF images is critical for ensuring that the reconstructed HAR LF images maintain a complete and consistent disparity structure.</p></sec><sec id="sec3dot2-sensors-25-00991"><title>3.2. Limitations of Existing Methods in SAC Feature Extraction</title><p>As shown in <xref rid="sensors-25-00991-f002" ref-type="fig">Figure 2</xref>a, in the EPI of HAR LF images, the imaging coordinates of pixels with the same SAC across adjacent views tend to be closely aligned. This proximity can be effectively utilized for extracting SAC features. However, as shown in <xref rid="sensors-25-00991-f002" ref-type="fig">Figure 2</xref>b, reducing angular resolution causes adjacent views in the EPI of an LAR LF image to correspond to views that are distant apart in the HAR LF image. In this case, the sampling point positions need to be extended to extract the complete SAC features.</p><p>Existing methods do not account for the unique characteristics of SAC features in LAR LF images, which often results in incomplete SAC feature extraction. When the distance between the pixels required for extracting SAC features increases, methods based on regular CNNs [<xref rid="B8-sensors-25-00991" ref-type="bibr">8</xref>,<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>] are limited by their small convolutional kernels, making it difficult to accurately extract SAC features from distant pixels. To address this, some researchers have used large convolution kernels [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] and attention mechanisms [<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>] to establish connections among a large set of pixels&#x02014;or even all pixels&#x02014;within the EPI, including those pixels that are distant. However, establishing such extensive pixel connections presents two main challenges. First, it significantly increases computational complexity and memory requirements. Second, for any object in LF scenes, typically only a single pixel is projected in each view. While adjacent pixels within the same view may exhibit spatial correlation due to the sufficient spatial resolution of LAR LF images, most other pixels show weak correlations with this pixel. Including these weakly correlated pixels can reduce the accuracy of SAC feature extraction.</p><p>Therefore, we propose using DCN to extract SAC features. This approach can extract SAC features from distant pixels while ensuring that these pixels are the most relevant for the task.</p></sec></sec><sec id="sec4-sensors-25-00991"><title>4. Method</title><sec id="sec4dot1-sensors-25-00991"><title>4.1. Overview</title><p>In network design, we focus primarily on the accurate SAC feature extraction. However, inspired by previous studies [<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>,<xref rid="B23-sensors-25-00991" ref-type="bibr">23</xref>] that use LF essential features, we also introduce angular feature extraction and spatial feature extraction. As shown in <xref rid="sensors-25-00991-f003" ref-type="fig">Figure 3</xref>, for the LFASR task of <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, we use an LAR LF image <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>A</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> with an angular resolution of <italic toggle="yes">a</italic> as the input and reconstruct an HAR LF image <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> with an angular resolution of <italic toggle="yes">A</italic>. The proposed network consists of three main components: Initial Angular Feature Extraction (IAFE), Deep Feature Extraction (DFE), and Angular Upsampling.</p><p>Firstly, the <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>LAR</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is input into IAFE, which consists of an initial convolution and an angular Transformer. In the IAFE, <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>LAR</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> realizes channel expansion through the initial convolution and then use the angular Transformer to capture underlying angular information, thereby obtaining the initial feature map <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>init</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Secondly, <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>init</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is passed through DFE, which contains three DFE Blocks. As shown in <xref rid="sensors-25-00991-f003" ref-type="fig">Figure 3</xref>b, each DFE Block includes a SAC Feature Extraction (SACFE) unit&#x02014;built with DCNs fused with multiple maximum offsets and a Spatial Feature Extraction (SFE) unit, comprising convolutional layers with residual connections. This design extracts SAC features and spatial features from the LAR LF image. After passing through the DFE, we obtain the deep LF feature map <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>DFE</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>DFE</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is input into Angular Upsampling. Angular Upsampling combines multiple convolutions and PixelShuffle to establish a connection between <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>DFE</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the HAR LF image, then reconstructing the HAR LF image <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>HAR</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. </p></sec><sec id="sec4dot2-sensors-25-00991"><title>4.2. Initial Angular Feature&#x000a0;Extraction</title><p>IAFE takes <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>LAR</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as input and outputs the initial feature map <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>init</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. As shown in <xref rid="sensors-25-00991-f003" ref-type="fig">Figure 3</xref>b, <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>LAR</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is processed by the initial convolution, which includes a convolution with a kernel size of <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and a LeakyReLU activation. This operation expands the channel dimensions of input to <italic toggle="yes">C</italic>, producing <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>init</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Next, <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>init</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is reshaped into the MacPI, enabling more efficient operations in the angular domain. The reshaped <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>init</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is then passed into the Angular Feature Extraction (AFE) module. Here, we use the angular Transformer proposed by Cong et al. [<xref rid="B27-sensors-25-00991" ref-type="bibr">27</xref>], which effectively utilizes dynamic view position encoding to capture global angular information. Finally, the angular feature map produced by the AFE is reshaped back into the SAI, resulting in the initial feature map <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>init</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> enriched with angular features.</p></sec><sec id="sec4dot3-sensors-25-00991"><title>4.3. Deep Feature&#x000a0;Extraction</title><p>DFE takes <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>init</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as input and outputs the LF deep feature map <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>DFE</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. As shown in <xref rid="sensors-25-00991-f003" ref-type="fig">Figure 3</xref>a, the DFE is composed of three cascaded DFE Blocks. Each DFE block consists of SFE and SACFE. The SFE, as shown in <xref rid="sensors-25-00991-f003" ref-type="fig">Figure 3</xref>d, consists of three convolutions with kernels size of <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, each followed by an LReLU activation. Each convolution is activated by an LReLU layer and outputs via a residual connection. As shown in <xref rid="sensors-25-00991-f003" ref-type="fig">Figure 3</xref>c, in SACFE, we employed our proposed MMOF-DCN to extract SAC features. Given that the EPI has both horizontal and vertical directions with similar characteristics, we adopt a similar structure for extracting SAC features in both directions within each DFE Block. Specifically, for each DFE Block, the input feature map alternates between SACFE and SFE to extract SAC features and spatial features, which are then combined with the input via residual connections. After passing through three DFE Blocks, DFE extracts spatial and SAC features from <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>init</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and integrates them with the angular features contained in <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>init</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to form the LF deep feature map <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>DFE</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec4dot4-sensors-25-00991"><title>4.4. SAC Feature&#x000a0;Extraction</title><sec id="sec4dot4dot1-sensors-25-00991"><title>4.4.1. DCN for SAC Feature&#x000a0;Extraction</title><p>In order to extract SAC features using the limited pixel points on EPI that are most relevant to SAC, inspired by Wang et al. [<xref rid="B13-sensors-25-00991" ref-type="bibr">13</xref>], we introduced DCN. As shown in <xref rid="sensors-25-00991-f004" ref-type="fig">Figure 4</xref>, DCN differs from regular convolution by adaptively adjusting the sampling point positions using offsets, which allows for a flexible change in the receptive field of convolution and extended sampling point distance. With this design, DCN can not only extract SAC features from pixels at greater distances but also utilize only the most strongly correlated pixels, avoiding issues introduced by irrelevant points.</p><p>Since horizontal SAC features and vertical SAC features have similar properties, differing only in direction, we describe the process of DCN extracting SAC features in the horizontal direction as a general case. As SAC features are extracted from the EPI, the input is first reshaped into horizontal EPI, denoted as <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>HEPI</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, to facilitate the extraction of horizontal SAC feature. For any pixel at coordinate <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>HEPI</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, SAC features are extracted via DCN can be formulated as:<disp-formula id="FD1-sensors-25-00991"><label>(1)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>E</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> denotes SAC features extracted at the coordinate <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. <italic toggle="yes">T</italic> represents the total number of groups into which the features are divided along the channel dimension. This helps improve the efficiency of feature extraction [<xref rid="B28-sensors-25-00991" ref-type="bibr">28</xref>]. <italic toggle="yes">K</italic> represents the total number of sampling points. <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> respectively represent the channel attention weight and the sampling point positions attention weight. This mechanism adaptively assigns weights to channels and the importance of each sampling points, optimizing the utilization of information from the sampled points. The set <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the initial position of each sampling point. <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the offset for the <italic toggle="yes">k</italic>-th sampling point in channel <italic toggle="yes">t</italic>.</p></sec><sec id="sec4dot4dot2-sensors-25-00991"><title>4.4.2. Multi-Maximum-Offsets Fusion&#x000a0;DCN</title><p>In order to enable DCN to extract more accurate SAC features of various objects in LF scenes, we introduce the MMOF-DCN. The aim is to enhance the precision of the offsets by reducing the search space for the necessary offsets in the DCN when handling different SAC, thereby improving the overall accuracy of the SAC features. We have designed MMOF-DCN through three steps: offset distribution pattern statistics, setting multiple-maximum-offsets DCN using the distribution patterns, and DCN adaptive weight fusion.</p><p>The first step is offset distribution pattern statistics. We describe this process using the <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task. Based on previous work [<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>,<xref rid="B23-sensors-25-00991" ref-type="bibr">23</xref>], we selected real-world dataset (including 30 Scenes [<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>], Occlusion [<xref rid="B29-sensors-25-00991" ref-type="bibr">29</xref>], and Reflective [<xref rid="B29-sensors-25-00991" ref-type="bibr">29</xref>]) and synthetic dataset (including HCInew [<xref rid="B30-sensors-25-00991" ref-type="bibr">30</xref>] and HCIold [<xref rid="B31-sensors-25-00991" ref-type="bibr">31</xref>]) for the study. Specifically, we randomly selected 20 HAR LF images from the training set of the real-world dataset and 5 from the synthetic dataset, and constructed LAR LF images by removing several adjacent views. In LF scenes, the disparity of points on the same object is generally similar, and since our goal is to analyze the distribution range of the maximum offsets, precise calculation of the offsets for all points is unnecessary. As shown in <xref rid="sensors-25-00991-f005" ref-type="fig">Figure 5</xref>, we applied the Canny operator to perform edge detection on each randomly selected LAR LF image, which helped separate the different objects in the LF scene. We then extracted the horizontal EPI and vertical EPI from these edge-detected LF scenes. For any edge point in a specific view of the EPI, we identified the closest matching edge point in the neighboring views based on pixel value. Finally, as shown in <xref rid="sensors-25-00991-f006" ref-type="fig">Figure 6</xref>a,b, we performed a statistical analysis of the offsets required for the LF images in both datasets.</p><p>The second step is setting multiple-maximum-offsets DCN using the distribution patterns. As shown in <xref rid="sensors-25-00991-f006" ref-type="fig">Figure 6</xref>a,b, in both datasets, the offsets required to capture pixels with identical SAC features generally fall into three intervals: the most common offset range <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="3.33333pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the moderately common offset range <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mspace width="3.33333pt"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and the least common offset range <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mspace width="3.33333pt"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. For the <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task, in the real-world dataset, <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mspace width="3.33333pt"/><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; while in the synthetic dataset, <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mspace width="3.33333pt"/><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>17</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Based on these analyses, we configure three DCNs&#x02014;<inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="3.33333pt"/><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>&#x02014;each with a maximum offset of <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mspace width="3.33333pt"/><mml:msub><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, respectively, to extract SAC features using different maximum offsets.</p><p>The third step is DCN adaptive weight fusion. In this phase, SAC features are first extracted using DCNs with different maximum offsets, and then weighted through an AWM before being fused. Specifically, for the feature maps <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, we first use <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to extract the initial SAC features, resulting in <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Secondly, we applying a convolution with a kernel size of <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for channel-wise interaction, producing <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mspace width="3.33333pt"/><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. It is important to note that the convolution weights used in this interaction are shared, which ensures consistency in the subsequent steps when utilizing <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Thirdly, <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are input into the <italic toggle="yes">i</italic>-th AWM. As shown in <xref rid="sensors-25-00991-f007" ref-type="fig">Figure 7</xref>, in the AWM, <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are concatenated along the channel dimension, and a learnable weight mechanism is employed to adaptively adjust the accuracy of <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. The AWM consists of two convolutions with kernels size of <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, a ReLU activation layer, and a sigmoid function. The first convolution maps from <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> channels to 2 channels, and the second maps from 2 to <italic toggle="yes">C</italic> channels. The two convolutions allow for determining the SAC feature weights based solely on the pixel information, without interference from other pixels in the EPI. After obtaining the weights from the AWM, we multiply them with <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> to yield the weighted SAC features <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Finally, all <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are summed, and a convolution with a kernel size of <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">C</italic> output channels is applied for channel-wise interaction, producing the precise SAC feature map <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD2-sensors-25-00991"><label>(2)</label><mml:math id="mm79" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>&#x02217;</mml:mo><mml:mi>A</mml:mi><mml:mi>W</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>P</mml:mi><mml:mi>I</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec4dot5-sensors-25-00991"><title>4.5. Angular&#x000a0;Upsampling</title><p>After extracting the deep features <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>DDF</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from the LAR LF image, we reconstruct the HAR LF image using a combination of convolutional operations and PixelShuffle, following previously methods [<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>,<xref rid="B23-sensors-25-00991" ref-type="bibr">23</xref>]. Specifically, <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>DDF</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is first reshaped into the MacPI format. Secondly, it is processed by a full-angular convolution with a kernel size of <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> to integrate features in the angular domain. Thirdly, a convolution with a kernel size of <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is applied to expand the channel dimensions to <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, followed by PixelShuffle for redistribution. Fourthly, a spatial convolution with a kernel size of <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> maps the channels back to their original dimension, generating an initial high angular resolution LF image <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>H</mml:mi><mml:mi>A</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Finally, bicubic interpolation is applied to <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>A</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for blurry angular super-resolution, and the result is added to <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>H</mml:mi><mml:mi>A</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to produce the final HAR LF image:<disp-formula id="FD3-sensors-25-00991"><label>(3)</label><mml:math id="mm89" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>A</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>H</mml:mi><mml:mi>A</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>A</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="sec5-sensors-25-00991"><title>5. Experiment</title><p>To evaluate the performance of our proposed method across diverse LF scenes, we followed previous works [<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>,<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>] and conducted training and testing on both real-world dataset and synthetic dataset. We have compared our proposed method with existing methods. In addition, we also conducted a number of ablation studies to verify the effectiveness of the various components of our proposed method.</p><sec id="sec5dot1-sensors-25-00991"><title>5.1. Setup</title><sec id="sec5dot1dot1-sensors-25-00991"><title>5.1.1. Datasets</title><p>In real-world dataset, three public datasets are used, which are 30Scenes [<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>], Occlusion [<xref rid="B29-sensors-25-00991" ref-type="bibr">29</xref>] and Reflective [<xref rid="B29-sensors-25-00991" ref-type="bibr">29</xref>]. We use 100 LF images from 30Scenes for training and 70 LF images for testing. These test LF images include 30 LF images from 30Scenes, 25 LF images from Occlusion, and 15 LF images from Reflective.</p><p>In the synthetic dataset, two public datasets are used, which are HCInew [<xref rid="B30-sensors-25-00991" ref-type="bibr">30</xref>] and HCIold [<xref rid="B31-sensors-25-00991" ref-type="bibr">31</xref>]. We use 20 LF images from HCInew for training, and 4 LF images from HCInew and 5 LF images from HCIold for testing. It is worth noting that the distance between the cameras used to obtain HCInew and HCIold is relatively large, so the distance between each view in the obtained LF image is large. Therefore, compared to the real-world dataset, synthetic dataset has larger disparity and is more challenging.</p></sec><sec id="sec5dot1dot2-sensors-25-00991"><title>5.1.2. Implement&#x000a0;Details</title><p>We implement our method in PyTorch. Our method is trained and tested on a single NVIDIA 3090 GPU. For the channel number <italic toggle="yes">C</italic> of our proposed network, except for the version <italic toggle="yes">C</italic> is equal to 32 introduced in <xref rid="sec5dot2dot3-sensors-25-00991" ref-type="sec">Section 5.2.3</xref>, the conventional version <italic toggle="yes">C</italic> is equal to 32 was used in all the other experiments.</p><p>During training, the training images are cropped to <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>64</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> size and the batch size is set to 1. Since <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> Loss is more robust to outliers [<xref rid="B32-sensors-25-00991" ref-type="bibr">32</xref>], we use it as the loss function for training. The initial learning rate is set to 1e-4, decayed every 25 epochs, and the decay rate is 0.5. Similar to previous works [<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>,<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>], we represent LF images with YCBCR and evaluate the quality of LF angular super-resolution on the Y channel. For the CBCR channel, we apply Bicubic interpolation to upsample them.</p></sec></sec><sec id="sec5dot2-sensors-25-00991"><title>5.2. Comparison with Existing&#x000a0;Methods</title><p>We choose six representative existing methods as the comparison methods. They are Kalantari et al. [<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>], Yeung et al. [<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>], Jin et al. [<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>], SAANet [<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>], DistgASR [<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>], and Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>]. We use <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task and <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task to evaluate the superiority of our proposed approach. To compare the difficulty of the LFASR tasks, we can compare the upsampling scale <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></inline-formula> of the angular dimension in the LFASR tasks. Specifically, for the <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task, the upsampling scale <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></inline-formula> is defined as <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>]. A larger value of <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></inline-formula> means that each view in the LAR LF image must reconstruct more views from the HAR LF image, which increases the complexity of the LFASR task. This means that the <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task (<inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>6</mml:mn><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> Upsampling) is more challenging to reconstruction than the <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task (<inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> Upsampling). Therefore, we validate the superiority of our method for challenging large disparity LFASR tasks with the former, and the generality of our method for routine LFASR tasks with the latter. For each task, we evaluated it separately using quantitative analysis and qualitative analysis. In quantitative analysis, PSNR and SSIM are used as evaluation criteria. In qualitative analysis, we conducted three presentations to evaluate, error map, two details and EPI reconstruction, respectively. In addition, we also compare the number of parameters to evaluate the efficiency of parameter use of our proposed method.</p><sec id="sec5dot2dot1-sensors-25-00991"><title>5.2.1. <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR&#x000a0;Task</title><p>The comparative experiment commences with a systematic quantitative analysis. As shown in <xref rid="sensors-25-00991-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-25-00991-t002" ref-type="table">Table 2</xref>, our proposed method achieves higher PSNR and SSIM on both real-world and synthetic datasets compared to existing methods. Because Yeung et al. [<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>] only used spatial and angular features without SAC features, they could not use all essential LF features to reconstruct HAR LF images. In occluded LF scenes, methods that depend on disparity estimation, such as those by Kalantari et al. [<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>] and Jin et al. [<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>], exhibit noticeably lower reconstruction quality compared to our approach. This is due to the reduced accuracy of disparity estimation caused by occlusions, which significantly degrades the reconstruction results.</p><p>On synthetic dataset with large disparity, our proposed method achieves an average PSNR that is 0.45 dB higher than existing method. SAANet rely on global attention mechanisms for SAC feature extraction leads to the inclusion of numerous irrelevant points, thereby reducing the precision of SAC features. Additionally, their method overlooks other essential LF image features. These have led to poor reconstruction quality of their methods. While DistgASR and Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] incorporate essential LF features, their methods fail to effectively extract SAC features. DistgASR is limited by regular CNN, which captures overly localized features on EPIs and struggles with accurate SAC feature extraction in large-disparity LF scenes. Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] use convolutions with large kernel introduces excessive irrelevant points, causing significant errors. In contrast, our proposed method not only expands the receptive field for SAC feature extraction but also selectively focuses on a limited set of the most relevant points. This enables accurate SAC feature extraction even in LF scenes with large disparity, leading to superior reconstruction performance.</p><p>Furthermore, we augment our comparative experiment through comprehensive qualitative analysis. As shown in <xref rid="sensors-25-00991-f008" ref-type="fig">Figure 8</xref>, <xref rid="sensors-25-00991-f009" ref-type="fig">Figure 9</xref> and <xref rid="sensors-25-00991-f010" ref-type="fig">Figure 10</xref>, we illustrate the visual comparisons between our proposed method and other methods on both real-world and synthetic LF scenes. Since synthetic LF scenes are more challenging, we selected one image each from the HCInew and HCIold datasets for analysis. In real-world LF scenes, methods that depend on disparity estimation, such as Kalantari et al. [<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>] and Jin et al. [<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>], struggle to determine whether the rear vehicle is occluded by grass. Compared to Yeung et al. [<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>], our method produces significantly more accurate error maps, indicating clearer reconstructed images.</p><p>In synthetic LF scenes, DistgASR relies on regular CNNs to extract SAC features based on adjacent pixels, which overemphasizes local neighborhood information. This leads to noticeable artifacts and blurring in areas containing letters and numbers in the images reconstructed from HCInew. For EPI reconstruction, both Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] using convolutions with large kernel, and SAANet employing global attention, introduce excessive pixel points. Consequently, the disparity structure is poorly preserved in the EPIs reconstructed from HCInew and HCIold, leading to extensive blurry regions. In contrast, our method effectively preserves the EPI structure by utilizing only a limited number of the most relevant pixels for SAC features.</p></sec><sec id="sec5dot2dot2-sensors-25-00991"><title>5.2.2. <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR&#x000a0;Task</title><p>In the <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task, we only choose SAANet, DistgASR, and Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] as the comparison methods for two primary reasons. First, we previously analyzed the limitations of Kalantari et al. [<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>], Yeung et al. [<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>], and Jin et al. [<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>] during the <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task. These limitations, which arise from factors such as LF scene composition (e.g., occlusion) and insufficient feature utilization, are not affected by the degree of angular resolution reduction. Second, SAANet, DistgASR, and Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] each represent different strategies for extracting SAC features from EPIs&#x02014;global attention mechanisms, regular CNNs, and convolutions with large kernel. Given the characteristics of SAC features in LAR LF images, as discussed earlier, their feature extraction capabilities are influenced by the reduction of angular resolution.</p><p>We also employ quantitative analysis to conduct our comparative experiments. As shown in <xref rid="sensors-25-00991-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-00991-t004" ref-type="table">Table 4</xref>, our method achieves higher PSNR and SSIM on both real-world and synthetic datasets compared to existing methods. In the <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task, the limited loss of views results in shorter distances between pixels belonging to the same SAC on the EPI. This makes CNNs-based SAC extraction methods like DistgASR more likely to accurately capture SAC features, leading to a substantial performance improvement compared to the <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task. However, DistgASR still struggles with capturing points exhibiting larger disparities, which limits its reconstruction quality compared to our method that effectively explores a broader receptive field. While SAANet and Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] also benefit from the increased number of views, allowing them to capture more pixels belonging to the same SAC and thereby improving reconstruction performance, these methods also incorporate more irrelevant pixels. As a result, their reconstruction quality falls short of our method, which focuses solely on the most relevant pixels for SAC feature extraction.</p><p>The qualitative analysis substantiates the superiority of our proposed method over existing methods. As shown in <xref rid="sensors-25-00991-f011" ref-type="fig">Figure 11</xref> and <xref rid="sensors-25-00991-f012" ref-type="fig">Figure 12</xref>, we illustrate the visual comparisons between our method and other methods on both real-world and synthetic LF scenes. In the detailed display of the real-world LF scene, the reconstruction by DistgASR shows noticeable blurring, particularly at the junction of the branches and the background on the left, as well as in adjacent regions. In the synthetic LF scene, the EPI reconstruction by SAANet and Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] exhibits significant directional errors. This problem stems from the SAC features they extracted being influenced by an excessive number of irrelevant points, which hampers the accurate extraction of SAC features.</p></sec><sec id="sec5dot2dot3-sensors-25-00991"><title>5.2.3. Parameter Number&#x000a0;Comparison</title><p>To better analysis the use of parameters and make a fair comparison with other methods, we evaluate the effectiveness of our proposed method using two versions with different parameter sizes: <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. As shown in <xref rid="sensors-25-00991-t005" ref-type="table">Table 5</xref>, we present the parameter size and average evaluation results for the synthetic dataset in the <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task. Compared to the methods with smaller parameter sizes such as Yeung et al. [<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>], Jin et al. [<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>], and SAANet, our <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> version not only has a similar parameter size but also achieves a 1.02 dB increase in PSNR. When compared to methods with moderate parameter sizes, such as DistgASR, our <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> version uses fewer parameters while still achieving a 0.3 dB improvement in PSNR. In the comparison with larger models, our <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> version, with similar parameters count to Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>], achieves a 0.45 dB increase in PSNR. These parameter comparisons highlight the efficient utilization of parameters in our proposed method.</p></sec></sec><sec id="sec5dot3-sensors-25-00991"><title>5.3. Ablation&#x000a0;Study</title><p>In this section, we verify the validity of the various components of our proposed method through different ablation studies. We ensured the consistency of the number of parameters by adjusting the number of channels. In addition, our ablation studies were performed on challenging <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task, as well as on synthetic dataset with large disparity.</p><sec id="sec5dot3dot1-sensors-25-00991"><title>5.3.1. Effectiveness of MMOF-DCN in SAC Feature Extraction Compared with Other&#x000a0;Models</title><p>To verify the effectiveness of MMOF-DCN in accurately extracting SAC feature, we introduced three representative models that are commonly used on EPI and proposed three variants based on these models, which are named SAC-R-CNN, SAC-Transformer, and SAC-B-CNN. Specifically, each variant substitutes the MMOF-DCN with a different model: in SAC-R-CNN, we replace it with the Regular CNN used in DistgASR [<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>] to capture EPI features on MacPI; SAC-Transformer substitutes the EPI Transformer proposed by Liang et al. [<xref rid="B33-sensors-25-00991" ref-type="bibr">33</xref>]; in SAC-B-CNN, we use the convolution with large kernel for EPI to extract features proposed by Xia et al. [<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>].</p><p>As shown in <xref rid="sensors-25-00991-t006" ref-type="table">Table 6</xref>, the PSNR values of these three variants decreased by 1.25 dB, 0.88 dB, and 0.65 dB, respectively, compared to our proposed method. In addition, as shown in <xref rid="sensors-25-00991-f013" ref-type="fig">Figure 13</xref>, the H-EPI and V-EPI reconstructed by the three variants are less clear than those reconstructed by our proposed method. Both quantitative and qualitative results demonstrate that the DCN-based approach is more effective in addressing the issue of varying and inconsistent pixel distances required for extracting SAC features.</p></sec><sec id="sec5dot3dot2-sensors-25-00991"><title>5.3.2. Effectiveness of Multi-Maximum-Offsets Fusion (MMOF) Design for DCN</title><p>To verify the effectiveness of the MMOF design for DCN, we introduced a variant named O-DCN. Specifically, O-DCN follows the original design by Wang [<xref rid="B13-sensors-25-00991" ref-type="bibr">13</xref>], where a DCN is first applied, followed by a Linear layer to enhance extracted features. In O-DCN, we use the maximum offset value from the LF images in the synthetic dataset to ensure the exploration of SAC features at the greatest possible distance.</p><p>As shown in <xref rid="sensors-25-00991-t006" ref-type="table">Table 6</xref>, the PSNR value of O-DCN is lower by 0.33 dB compared to our proposed method. This quantitative result suggests that the MMOF design effectively extracts SAC features not only at greater distances but also accurately at shorter and medium distances. In addition, as shown in <xref rid="sensors-25-00991-f013" ref-type="fig">Figure 13</xref>, the reconstruction of spots on the wall by O-DCN is not as clear as that of our proposed method, which proves that the MMOF design can reconstruct more details.</p></sec><sec id="sec5dot3dot3-sensors-25-00991"><title>5.3.3. The Number of DFE Blocks&#x000a0;Used</title><p>The DFE is composed of <italic toggle="yes">m</italic> DFE Blocks, where in our proposed method, <inline-formula><mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. To validate the appropriateness of this strategy, we experimented with different values of <italic toggle="yes">m</italic> and analyzed the results in terms of both parameter and quality.</p><p>As shown in <xref rid="sensors-25-00991-t007" ref-type="table">Table 7</xref>, when <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the PSNR values decrease significantly compared to when <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. For values of <italic toggle="yes">m</italic> greater than 3, the increase in PSNR is minimal, while the parameter rises substantially. In contrast, when <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the PSNR value reaches its optimal level, and the parameter count remains moderate. In addition, as shown in <xref rid="sensors-25-00991-f014" ref-type="fig">Figure 14</xref>, when <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the errors in the error map is very obvious, and the shape of the circular projection in the local details is incomplete. When <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the quality of the reconstructed image is significantly improved, and the quality becomes nearly identical. Therefore, we conclude that using <inline-formula><mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-00991"><title>6. Conclusions and Future&#x000a0;Work</title><p>In this paper, we address the limitations of existing LFASR methods, where inaccuracies in SAC feature extraction lead to insufficient reconstruction of disparity structures, resulting in blurred regions and artifacts. To overcome these challenges, we proposed MMOF-DCN. Our approach begins with an analysis of two key challenges in SAC feature extraction for LAR LF images: (1) the distance between pixels required for SAC feature extraction increases and the number of pixels is limited, and (2) the inconsistent distances necessary for extracting different SAC features. These challenges make it difficult to achieve accurate and comprehensive SAC feature extraction. To address the first issue, we employ DCN to adaptively adjust sampling point positions, thereby expanding the sampling range to handle larger pixel distances. For the second issue, we introduce a multi-maximal offset fusion mechanism within DCN to accommodate the varying pixel distances required for different SAC features. We conducted extensive experiments on both real-world and synthetic datasets. The results demonstrate that our proposed method achieves superior quantitative and qualitative performance compared to existing methods. Notably, in challenging tasks and synthetic LF scenes with large disparities, our proposed method achieves a PSNR improvement of 0.45 dB over existing methods. Ablation study further confirm the effectiveness of each module in our proposed method.</p><p>Despite our proposed method has shown the best reconstruction of HAR LF images, our work primarily focuses on accurate SAC feature extraction with limited exploration of spatial and angular features. We recognize that leveraging spatial and angular features more effectively can enhance detail and texture clarity in reconstructed LF images. In future work, we plan to investigate the properties of spatial and angular features in LAR LF images more comprehensively and design improved network architectures to optimize feature extraction.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, D.L. and R.Z.; methodology, D.L. and R.Z.; software, D.L. and Y.Y.; validation, D.L. and R.Z.; formal analysis, D.L. and R.Z.; investigation, D.L. and Y.Y.; resources, D.L.; data curation, D.L.; writing&#x02014;original draft preparation, D.L. and R.Z.; writing&#x02014;review and editing, D.L. and R.Z.; visualization, D.L.; supervision, R.Z.; project administration, R.Z.; funding acquisition, R.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in this study are included in the article. Further inquiries can be directed to the corresponding author.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">SSIM</td><td align="left" valign="middle" rowspan="1" colspan="1">Structural Similarity</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PSNR</td><td align="left" valign="middle" rowspan="1" colspan="1">Peak Signal Noise Ratio</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-00991"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Luo</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Semmen</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>S.T.</given-names></name>
</person-group><article-title>Achromatic diffractive liquid-crystal optics for VR displays</article-title><source>Proceedings of the Optical Architectures for Displays and Sensing in Augmented, Virtual, and Mixed Reality (AR, VR, MR) V. SPIE</source><conf-loc>Online</conf-loc><conf-date>28&#x02013;30 March 2021</conf-date><volume>Volume 12913</volume><fpage>17</fpage><lpage>25</lpage></element-citation></ref><ref id="B2-sensors-25-00991"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Han</surname><given-names>K.</given-names></name>
<name><surname>Xiang</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>E.</given-names></name>
<name><surname>Huang</surname><given-names>T.</given-names></name>
</person-group><article-title>A novel occlusion-aware vote cost for light field depth estimation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>44</volume><fpage>8022</fpage><lpage>8035</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2021.3105523</pub-id><pub-id pub-id-type="pmid">34406938</pub-id>
</element-citation></ref><ref id="B3-sensors-25-00991"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Aleksandrov</surname><given-names>M.</given-names></name>
<name><surname>Hu</surname><given-names>Z.</given-names></name>
<name><surname>Meng</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Zlatanova</surname><given-names>S.</given-names></name>
<name><surname>Ai</surname><given-names>H.</given-names></name>
<name><surname>Tao</surname><given-names>P.</given-names></name>
</person-group><article-title>Accurate light field depth estimation under occlusion</article-title><source>Pattern Recognit.</source><year>2023</year><volume>138</volume><fpage>109415</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2023.109415</pub-id></element-citation></ref><ref id="B4-sensors-25-00991"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Lu</surname><given-names>Z.</given-names></name>
<name><surname>Jiang</surname><given-names>D.</given-names></name>
<name><surname>Guo</surname><given-names>Y.</given-names></name>
<name><surname>Qiao</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Zhu</surname><given-names>T.</given-names></name>
<name><surname>Cai</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Zhanghao</surname><given-names>K.</given-names></name>
<etal/>
</person-group><article-title>Iterative tomography with digital adaptive optics permits hour-long intravital observation of 3D subcellular dynamics at millisecond scale</article-title><source>Cell</source><year>2021</year><volume>184</volume><fpage>3318</fpage><lpage>3332</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2021.04.029</pub-id><pub-id pub-id-type="pmid">34038702</pub-id>
</element-citation></ref><ref id="B5-sensors-25-00991"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>R.</given-names></name>
<name><surname>Yang</surname><given-names>Q.</given-names></name>
<name><surname>Chang</surname><given-names>A.S.</given-names></name>
<name><surname>Hu</surname><given-names>G.</given-names></name>
<name><surname>Greene</surname><given-names>J.</given-names></name>
<name><surname>Gabel</surname><given-names>C.V.</given-names></name>
<name><surname>You</surname><given-names>S.</given-names></name>
<name><surname>Tian</surname><given-names>L.</given-names></name>
</person-group><article-title>EventLFM: Event camera integrated Fourier light field microscopy for ultrafast 3D imaging</article-title><source>Light Sci. Appl.</source><year>2024</year><volume>13</volume><fpage>144</fpage><pub-id pub-id-type="doi">10.1038/s41377-024-01502-5</pub-id><pub-id pub-id-type="pmid">38918363</pub-id>
</element-citation></ref><ref id="B6-sensors-25-00991"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yoon</surname><given-names>Y.</given-names></name>
<name><surname>Jeon</surname><given-names>H.G.</given-names></name>
<name><surname>Yoo</surname><given-names>D.</given-names></name>
<name><surname>Lee</surname><given-names>J.Y.</given-names></name>
<name><surname>So Kweon</surname><given-names>I.</given-names></name>
</person-group><article-title>Learning a deep convolutional network for light-field image super-resolution</article-title><source>Proceedings of the IEEE International Conference on Computer Vision Workshops</source><conf-loc>Santiago, Chile</conf-loc><conf-date>11&#x02013;18 December 2015</conf-date><fpage>24</fpage><lpage>32</lpage></element-citation></ref><ref id="B7-sensors-25-00991"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yeung</surname><given-names>H.W.F.</given-names></name>
<name><surname>Hou</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Chung</surname><given-names>Y.Y.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
</person-group><article-title>Fast light field reconstruction with deep coarse-to-fine modeling of spatial-angular clues</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#x02013;14 September 2018</conf-date><fpage>137</fpage><lpage>152</lpage></element-citation></ref><ref id="B8-sensors-25-00991"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>G.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Fang</surname><given-names>L.</given-names></name>
<name><surname>Dai</surname><given-names>Q.</given-names></name>
<name><surname>Chai</surname><given-names>T.</given-names></name>
</person-group><article-title>Light field reconstruction using convolutional network on EPI and extended applications</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2018</year><volume>41</volume><fpage>1681</fpage><lpage>1694</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2018.2845393</pub-id><pub-id pub-id-type="pmid">29994195</pub-id>
</element-citation></ref><ref id="B9-sensors-25-00991"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>G.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Fang</surname><given-names>L.</given-names></name>
<name><surname>Chai</surname><given-names>T.</given-names></name>
</person-group><article-title>Spatial-angular attention network for light field reconstruction</article-title><source>IEEE Trans. Image Process.</source><year>2021</year><volume>30</volume><fpage>8999</fpage><lpage>9013</lpage><pub-id pub-id-type="doi">10.1109/TIP.2021.3122089</pub-id><pub-id pub-id-type="pmid">34705646</pub-id>
</element-citation></ref><ref id="B10-sensors-25-00991"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Wu</surname><given-names>G.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>An</surname><given-names>W.</given-names></name>
<name><surname>Yu</surname><given-names>J.</given-names></name>
<name><surname>Guo</surname><given-names>Y.</given-names></name>
</person-group><article-title>Disentangling Light Fields for Super-Resolution and Disparity Estimation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2023</year><volume>45</volume><fpage>425</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2022.3152488</pub-id><pub-id pub-id-type="pmid">35180076</pub-id>
</element-citation></ref><ref id="B11-sensors-25-00991"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>D.</given-names></name>
<name><surname>Mao</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>X.</given-names></name>
<name><surname>An</surname><given-names>P.</given-names></name>
<name><surname>Fang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Learning a Multilevel Cooperative View Reconstruction Network for Light Field Angular Super-Resolution</article-title><source>Proceedings of the 2023 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Brisbane, Australia</conf-loc><conf-date>10&#x02013;14 July 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>1271</fpage><lpage>1276</lpage></element-citation></ref><ref id="B12-sensors-25-00991"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xia</surname><given-names>P.</given-names></name>
<name><surname>Lu</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Xia</surname><given-names>W.</given-names></name>
</person-group><article-title>Revisiting Large Kernel Convolution for Light Field Image Angular Super-Resolution</article-title><source>Proceedings of the 2024 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Niagara Falls, ON, Canada</conf-loc><conf-date>15&#x02013;19 July 2024</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2024</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B13-sensors-25-00991"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Dai</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Huang</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Hu</surname><given-names>X.</given-names></name>
<name><surname>Lu</surname><given-names>T.</given-names></name>
<name><surname>Lu</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<etal/>
</person-group><article-title>Internimage: Exploring large-scale vision foundation models with deformable convolutions</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#x02013;24 June 2023</conf-date><fpage>14408</fpage><lpage>14419</lpage></element-citation></ref><ref id="B14-sensors-25-00991"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kalantari</surname><given-names>N.K.</given-names></name>
<name><surname>Wang</surname><given-names>T.C.</given-names></name>
<name><surname>Ramamoorthi</surname><given-names>R.</given-names></name>
</person-group><article-title>Learning-based view synthesis for light field cameras</article-title><source>ACM Trans. Graph. (TOG)</source><year>2016</year><volume>35</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1145/2980179.2980251</pub-id></element-citation></ref><ref id="B15-sensors-25-00991"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jin</surname><given-names>J.</given-names></name>
<name><surname>Hou</surname><given-names>J.</given-names></name>
<name><surname>Yuan</surname><given-names>H.</given-names></name>
<name><surname>Kwong</surname><given-names>S.</given-names></name>
</person-group><article-title>Learning light field angular super-resolution via a geometry-aware network</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>New York, NY, USA</conf-loc><conf-date>7&#x02013;12 February 2020</conf-date><volume>Volume 34</volume><fpage>11141</fpage><lpage>11148</lpage></element-citation></ref><ref id="B16-sensors-25-00991"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ko</surname><given-names>K.</given-names></name>
<name><surname>Koh</surname><given-names>Y.J.</given-names></name>
<name><surname>Chang</surname><given-names>S.</given-names></name>
<name><surname>Kim</surname><given-names>C.S.</given-names></name>
</person-group><article-title>Light field super-resolution via adaptive feature remixing</article-title><source>IEEE Trans. Image Process.</source><year>2021</year><volume>30</volume><fpage>4114</fpage><lpage>4128</lpage><pub-id pub-id-type="doi">10.1109/TIP.2021.3069291</pub-id><pub-id pub-id-type="pmid">33798082</pub-id>
</element-citation></ref><ref id="B17-sensors-25-00991"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Shi</surname><given-names>J.</given-names></name>
<name><surname>Jiang</surname><given-names>X.</given-names></name>
<name><surname>Guillemot</surname><given-names>C.</given-names></name>
</person-group><article-title>Learning fused pixel and feature-based view reconstructions for light fields</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date><fpage>2555</fpage><lpage>2564</lpage></element-citation></ref><ref id="B18-sensors-25-00991"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>D.</given-names></name>
<name><surname>Mao</surname><given-names>Y.</given-names></name>
<name><surname>Huang</surname><given-names>Y.</given-names></name>
<name><surname>Cao</surname><given-names>L.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Fang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Optical flow-assisted multi-level fusion network for Light Field image angular reconstruction</article-title><source>Signal Process. Image Commun.</source><year>2023</year><volume>119</volume><fpage>117031</fpage><pub-id pub-id-type="doi">10.1016/j.image.2023.117031</pub-id></element-citation></ref><ref id="B19-sensors-25-00991"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>F.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Hou</surname><given-names>G.</given-names></name>
<name><surname>Sun</surname><given-names>Z.</given-names></name>
<name><surname>Tan</surname><given-names>T.</given-names></name>
</person-group><article-title>End-to-end view synthesis for light field imaging with pseudo 4DCNN</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#x02013;14 September 2018</conf-date><fpage>333</fpage><lpage>348</lpage></element-citation></ref><ref id="B20-sensors-25-00991"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>D.</given-names></name>
<name><surname>Huang</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>Q.</given-names></name>
<name><surname>Ma</surname><given-names>R.</given-names></name>
<name><surname>An</surname><given-names>P.</given-names></name>
</person-group><article-title>Multi-angular epipolar geometry based light field angular reconstruction network</article-title><source>IEEE Trans. Comput. Imaging</source><year>2020</year><volume>6</volume><fpage>1507</fpage><lpage>1522</lpage><pub-id pub-id-type="doi">10.1109/TCI.2020.3037413</pub-id></element-citation></ref><ref id="B21-sensors-25-00991"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>G.</given-names></name>
<name><surname>Yue</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
</person-group><article-title>Intra-inter view interaction network for light field image super-resolution</article-title><source>IEEE Trans. Multimed.</source><year>2021</year><volume>25</volume><fpage>256</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1109/TMM.2021.3124385</pub-id></element-citation></ref><ref id="B22-sensors-25-00991"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>G.</given-names></name>
<name><surname>Yue</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
</person-group><article-title>Efficient Light Field Angular Super-Resolution With Sub-Aperture Feature Learning and Macro-Pixel Upsampling</article-title><source>IEEE Trans. Multimed.</source><year>2023</year><volume>25</volume><fpage>6588</fpage><lpage>6600</lpage><pub-id pub-id-type="doi">10.1109/TMM.2022.3211402</pub-id></element-citation></ref><ref id="B23-sensors-25-00991"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Ren</surname><given-names>L.</given-names></name>
<name><surname>Cao</surname><given-names>Y.</given-names></name>
<name><surname>Cao</surname><given-names>Y.</given-names></name>
</person-group><article-title>Light field angular super-resolution based on structure and scene information</article-title><source>Appl. Intell.</source><year>2023</year><volume>53</volume><fpage>4767</fpage><lpage>4783</lpage><pub-id pub-id-type="doi">10.1007/s10489-022-03759-y</pub-id></element-citation></ref><ref id="B24-sensors-25-00991"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>D.</given-names></name>
<name><surname>Tong</surname><given-names>Z.</given-names></name>
<name><surname>Huang</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Zuo</surname><given-names>Y.</given-names></name>
<name><surname>Fang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Geometry-assisted multi-representation view reconstruction network for light field image angular super-resolution</article-title><source>Knowl.-Based Syst.</source><year>2023</year><volume>267</volume><fpage>110390</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2023.110390</pub-id></element-citation></ref><ref id="B25-sensors-25-00991"><label>25.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Hearn</surname><given-names>D.</given-names></name>
</person-group><source>Computer Graphics with OpenGL</source><publisher-name>Pearson Education India</publisher-name><publisher-loc>Delhi, India</publisher-loc><year>2004</year></element-citation></ref><ref id="B26-sensors-25-00991"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Levoy</surname><given-names>M.</given-names></name>
<name><surname>Hanrahan</surname><given-names>P.</given-names></name>
</person-group><article-title>Light field rendering</article-title><source>Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>4&#x02013;9 August 1996</conf-date><fpage>31</fpage><lpage>42</lpage></element-citation></ref><ref id="B27-sensors-25-00991"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cong</surname><given-names>R.</given-names></name>
<name><surname>Sheng</surname><given-names>H.</given-names></name>
<name><surname>Yang</surname><given-names>D.</given-names></name>
<name><surname>Cui</surname><given-names>Z.</given-names></name>
<name><surname>Chen</surname><given-names>R.</given-names></name>
</person-group><article-title>Exploiting spatial and angular correlations with deep efficient transformers for light field image super-resolution</article-title><source>IEEE Trans. Multimed.</source><year>2023</year><volume>26</volume><fpage>1421</fpage><lpage>1435</lpage><pub-id pub-id-type="doi">10.1109/TMM.2023.3282465</pub-id></element-citation></ref><ref id="B28-sensors-25-00991"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dosovitskiy</surname><given-names>A.</given-names></name>
</person-group><article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2010.11929</pub-id></element-citation></ref><ref id="B29-sensors-25-00991"><label>29.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>University</surname><given-names>S.</given-names></name>
</person-group><article-title>The Stanford Light Field Archive</article-title><comment>Available online: <ext-link xlink:href="http://lightfield.stanford.edu/lfs.html" ext-link-type="uri">http://lightfield.stanford.edu/lfs.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-11-11">(accessed on 11 November 2024)</date-in-citation></element-citation></ref><ref id="B30-sensors-25-00991"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Honauer</surname><given-names>K.</given-names></name>
<name><surname>Johannsen</surname><given-names>O.</given-names></name>
<name><surname>Kondermann</surname><given-names>D.</given-names></name>
<name><surname>Goldluecke</surname><given-names>B.</given-names></name>
</person-group><article-title>A dataset and evaluation methodology for depth estimation on 4D light fields</article-title><source>Proceedings of the Computer Vision&#x02013;ACCV 2016: 13th Asian Conference on Computer Vision</source><conf-loc>Taipei, Taiwan</conf-loc><conf-date>20&#x02013;24 November 2016</conf-date><comment>Revised Selected Papers, Part III 13</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2017</year><fpage>19</fpage><lpage>34</lpage></element-citation></ref><ref id="B31-sensors-25-00991"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wanner</surname><given-names>S.</given-names></name>
<name><surname>Meister</surname><given-names>S.</given-names></name>
<name><surname>Goldluecke</surname><given-names>B.</given-names></name>
</person-group><article-title>Datasets and benchmarks for densely sampled 4D light fields</article-title><source>Proceedings of the VMV</source><conf-loc>Saarbr&#x000fc;cken, Germany</conf-loc><conf-date>3&#x02013;6 September 2013</conf-date><volume>Volume 13</volume><fpage>225</fpage><lpage>226</lpage></element-citation></ref><ref id="B32-sensors-25-00991"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Gallo</surname><given-names>O.</given-names></name>
<name><surname>Frosio</surname><given-names>I.</given-names></name>
<name><surname>Kautz</surname><given-names>J.</given-names></name>
</person-group><article-title>Loss functions for image restoration with neural networks</article-title><source>IEEE Trans. Comput. Imaging</source><year>2016</year><volume>3</volume><fpage>47</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1109/TCI.2016.2644865</pub-id></element-citation></ref><ref id="B33-sensors-25-00991"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liang</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Zhou</surname><given-names>S.</given-names></name>
<name><surname>Guo</surname><given-names>Y.</given-names></name>
</person-group><article-title>Learning non-local spatial-angular correlation for light field image super-resolution</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#x02013;6 October 2023</conf-date><fpage>12376</fpage><lpage>12386</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-00991-f001"><label>Figure 1</label><caption><p>The three representations of LF image. (<bold>a</bold>) shows the LF image representation of Sub-Aperture Image (SAI) and Epipolar Plane Image (EPI). (<bold>b</bold>) shows the LF image representation of Macro-Pixel Image (MacPI). Since each pixel in an SAI comes from the same angular position, there is only a spatial relationship between the pixels within a SAI, not an angular relationship. Similarly, since the pixels in each MacPI come from the same spatial position, there is only an angular relationship between the pixels within a MacPI, not a spatial relationship. The Epipolar Plane Image (EPI) is generated by extracting and stacking SAIs along designated spatial and angular dimensions. This imaging modality inherently encapsulates both spatial and angular information, thereby exhibiting spatial-angular correlation.</p></caption><graphic xlink:href="sensors-25-00991-g001" position="float"/></fig><fig position="float" id="sensors-25-00991-f002"><label>Figure 2</label><caption><p>The EPIs of HAR LF image and LAR LF image. (<bold>a</bold>) shows the EPIs of the HAR LF images. In (<bold>b</bold>), we simulate the EPIs of the LAR LF image by obscuring the views in the EPIs of HAR LF image with black areas. We use three different colored slashes in the EPIs to represent three different SACs. Arranged in descending order of length, their corresponding colors are red, yellow, and blue, respectively. The pixels within each rectangle in the SAC can be used to extract SAC features. As observed, extracting SAC features from LAR LF images necessitates the involvement of more distant pixels. Additionally, the distance between the pixels required to extract different SAC feature varies.</p></caption><graphic xlink:href="sensors-25-00991-g002" position="float"/></fig><fig position="float" id="sensors-25-00991-f003"><label>Figure 3</label><caption><p>Our proposed network architecture. (<bold>a</bold>) shows the overview of our proposed network. (<bold>b</bold>) displays the detail of the Deep Feature Extraction (DFE) block. (<bold>c</bold>) demonstrates the basic framework of the Spatial-Angular Correlation (SAC) Extraction with Multi-Maximum-Offsets Fusion Deformable Convolutional Network (MMOF-DCN). Furthermore, AWM means Adaptive Weight Mechanism. (<bold>d</bold>) shows the detail of the Spatial Feature Extraction (SFE).</p></caption><graphic xlink:href="sensors-25-00991-g003" position="float"/></fig><fig position="float" id="sensors-25-00991-f004"><label>Figure 4</label><caption><p>The process by which DCN adaptively changes the sampling point positions of the CNN using offset. (<bold>a</bold>&#x02013;<bold>d</bold>) show the sampling point positions from unable to cover related pixel points to covering all related pixel points step by step.</p></caption><graphic xlink:href="sensors-25-00991-g004" position="float"/></fig><fig position="float" id="sensors-25-00991-f005"><label>Figure 5</label><caption><p>The process of edge detection of LAR LF image by Canny operator. (<bold>a</bold>,<bold>b</bold>) shows the edge detection process in the Synthetic dataset, and (<bold>c</bold>,<bold>d</bold>) shows the edge detection process in the real world dataset.</p></caption><graphic xlink:href="sensors-25-00991-g005" position="float"/></fig><fig position="float" id="sensors-25-00991-f006"><label>Figure 6</label><caption><p>The proportional of offset required for accurate extraction of SAC of LARLF images randomly sampled by training sets of Real-world dataset and synthetic dataset data sets. We have marked three main divisions in the diagram.</p></caption><graphic xlink:href="sensors-25-00991-g006" position="float"/></fig><fig position="float" id="sensors-25-00991-f007"><label>Figure 7</label><caption><p>Details of the <italic toggle="yes">i</italic> branch of MMOF-DCN. In the figure, we assume that the number of input channels for EPI and output channels for DCN is 64.</p></caption><graphic xlink:href="sensors-25-00991-g007" position="float"/></fig><fig position="float" id="sensors-25-00991-f008"><label>Figure 8</label><caption><p>Visual comparison of the of <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task in real-world LF scene. We choose the <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>30</mml:mn><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mtext>_</mml:mtext><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>] for comparison and show it with the central view of <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The comparison shows the error map, two local details, a H-EPI and a V-EPI ([<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>,<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>,<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>]).</p></caption><graphic xlink:href="sensors-25-00991-g008" position="float"/></fig><fig position="float" id="sensors-25-00991-f009"><label>Figure 9</label><caption><p>Visual comparison of the of <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task in synthetic LF scene. We select the <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mtext>_</mml:mtext><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B30-sensors-25-00991" ref-type="bibr">30</xref>] for comparison and show it with the central view of <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The comparison shows the error map, two local details, a H-EPI and a V-EPI ([<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>,<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>,<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>]).</p></caption><graphic xlink:href="sensors-25-00991-g009" position="float"/></fig><fig position="float" id="sensors-25-00991-f010"><label>Figure 10</label><caption><p>Visual comparison of the of <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task in synthetic LF scene. We select the <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi><mml:mtext>_</mml:mtext><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>L</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B31-sensors-25-00991" ref-type="bibr">31</xref>] for comparison and show it with the central view of <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The comparison shows the error map, two local details, a H-EPI and a V-EPI ([<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>,<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>,<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>]).</p></caption><graphic xlink:href="sensors-25-00991-g010" position="float"/></fig><fig position="float" id="sensors-25-00991-f011"><label>Figure 11</label><caption><p>Visual comparison of the of <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task in real-world LF scene. We select the <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext>_</mml:mtext><mml:mn>44</mml:mn><mml:mtext>_</mml:mtext><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B29-sensors-25-00991" ref-type="bibr">29</xref>] for comparison and show it with the central view of <inline-formula><mml:math id="mm137" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The comparison shows the error map, two local details, a H-EPI and a V-EPI ([<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>]).</p></caption><graphic xlink:href="sensors-25-00991-g011" position="float"/></fig><fig position="float" id="sensors-25-00991-f012"><label>Figure 12</label><caption><p>Visual comparison of the of <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task in synthetic LF scene. We select the <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mtext>_</mml:mtext><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B30-sensors-25-00991" ref-type="bibr">30</xref>] for comparison and show it with the central view of <inline-formula><mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The comparison shows the error map, two local details, a H-EPI and a V-EPI ([<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>,<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>]).</p></caption><graphic xlink:href="sensors-25-00991-g012" position="float"/></fig><fig position="float" id="sensors-25-00991-f013"><label>Figure 13</label><caption><p>The qualitative results of the ablation study to verify the effectiveness of the MMOF-DCN and the MMOF design for DCN are shown. We select the <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mtext>_</mml:mtext><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B30-sensors-25-00991" ref-type="bibr">30</xref>] and show it with the central view of <inline-formula><mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The results show the error map, two local details, a H-EPI and a V-EPI.</p></caption><graphic xlink:href="sensors-25-00991-g013" position="float"/></fig><fig position="float" id="sensors-25-00991-f014"><label>Figure 14</label><caption><p>The qualitative results of the ablation study validate the most reasonable number of DFE blocks. We select the <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mtext>_</mml:mtext><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> [<xref rid="B30-sensors-25-00991" ref-type="bibr">30</xref>] and show it with the central view of <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The results show the error map, two local details, a H-EPI and a V-EPI. Here, <italic toggle="yes">m</italic> represents the number of DFE blocks, and we propose that <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> be highlighted in red.</p></caption><graphic xlink:href="sensors-25-00991-g014" position="float"/></fig><table-wrap position="float" id="sensors-25-00991-t001"><object-id pub-id-type="pii">sensors-25-00991-t001_Table 1</object-id><label>Table 1</label><caption><p>For the <inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task, the results of the comparative experiment on real-world dataset. We use PSNR/SSIM to evaluate. The best result in red and the suboptimal result in blue.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">30Scenes</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Occlusion</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Reflective</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Kanlantari et&#x000a0;al.&#x000a0;[<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">41.40/0.982</td><td align="center" valign="middle" rowspan="1" colspan="1">37.25/0.972</td><td align="center" valign="middle" rowspan="1" colspan="1">38.09/0.953</td><td align="center" valign="middle" rowspan="1" colspan="1">38.91/0.969</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Yeung et&#x000a0;al.&#x000a0;[<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">42.77/0.986</td><td align="center" valign="middle" rowspan="1" colspan="1">38.88/0.980</td><td align="center" valign="middle" rowspan="1" colspan="1">38.33/0.960</td><td align="center" valign="middle" rowspan="1" colspan="1">39.99/0.975</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Jin et&#x000a0;al.&#x000a0;[<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">42.54/0.986</td><td align="center" valign="middle" rowspan="1" colspan="1">38.53/0.979</td><td align="center" valign="middle" rowspan="1" colspan="1">38.46/0.959</td><td align="center" valign="middle" rowspan="1" colspan="1">39.84/0.974</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SAANet&#x000a0;[<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">40.95/0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">36.75/0.971</td><td align="center" valign="middle" rowspan="1" colspan="1">37.85/0.955</td><td align="center" valign="middle" rowspan="1" colspan="1">38.51/0.969</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DistgASR&#x000a0;[<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">43.67/0.995</td><td align="center" valign="middle" rowspan="1" colspan="1">39.46/0.991</td><td align="center" valign="middle" rowspan="1" colspan="1">39.11/0.978</td><td align="center" valign="middle" rowspan="1" colspan="1">40.74/0.988</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Xia et&#x000a0;al.&#x000a0;[<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">44.35/0.996</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">40.03/0.992</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">39.73/0.982</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">41.37/0.990</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">44.41/0.997</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">40.07/0.992</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">39.85/0.983</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">41.44/0.991</named-content>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00991-t002"><object-id pub-id-type="pii">sensors-25-00991-t002_Table 2</object-id><label>Table 2</label><caption><p>For the <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task, the results of the comparative experiment on synthetic dataset. We use PSNR/SSIM to evaluate. The best result in red and the suboptimal result in blue.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">HCInew</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">HCIold</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Kanlantari et&#x000a0;al.&#x000a0;[<xref rid="B14-sensors-25-00991" ref-type="bibr">14</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">32.85/0.909</td><td align="center" valign="middle" rowspan="1" colspan="1">38.58/0.944</td><td align="center" valign="middle" rowspan="1" colspan="1">35.71/0.926</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Yeung et&#x000a0;al.&#x000a0;[<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">32.30/0.900</td><td align="center" valign="middle" rowspan="1" colspan="1">39.69/0.941</td><td align="center" valign="middle" rowspan="1" colspan="1">35.99/0.920</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Jin et&#x000a0;al.&#x000a0;[<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">34.60/0.937</td><td align="center" valign="middle" rowspan="1" colspan="1">40.84/0.960</td><td align="center" valign="middle" rowspan="1" colspan="1">37.72/0.948</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SAANet&#x000a0;[<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">31.39/0.882</td><td align="center" valign="middle" rowspan="1" colspan="1">39.03/0.926</td><td align="center" valign="middle" rowspan="1" colspan="1">35.21/0.904</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DistgASR&#x000a0;[<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">34.70/0.974</td><td align="center" valign="middle" rowspan="1" colspan="1">42.18/0.978</td><td align="center" valign="middle" rowspan="1" colspan="1">42.18/0.978</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Xia et&#x000a0;al.&#x000a0;[<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">36.35/0.981</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">42.56/0.983</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">39.46/0.982</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">37.07/0.985</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">42.85/0.984</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">39.91/0.985</named-content>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00991-t003"><object-id pub-id-type="pii">sensors-25-00991-t003_Table 3</object-id><label>Table 3</label><caption><p>For the <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task, the results of the comparative experiment on real-world dataset. We use PSNR/SSIM to evaluate. The best result in red and the suboptimal result in blue.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">30Scenes</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Occlusion</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Reflective</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SAANet&#x000a0;[<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">43.85/0.994</td><td align="center" valign="middle" rowspan="1" colspan="1">38.78/0.978</td><td align="center" valign="middle" rowspan="1" colspan="1">42.37/0.988</td><td align="center" valign="middle" rowspan="1" colspan="1">41.67/0.987</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DistgASR&#x000a0;[<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">44.53/0.996</td><td align="center" valign="middle" rowspan="1" colspan="1">43.20/0.995</td><td align="center" valign="middle" rowspan="1" colspan="1">43.12/0.991</td><td align="center" valign="middle" rowspan="1" colspan="1">43.45/0.994</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Xia et&#x000a0;al.&#x000a0;[<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">44.96/0.997</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">43.35/0.995</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">43.20/0.991</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">43.84/0.994</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">45.04/0.998</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">43.39/0.995</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">43.34/0.991</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">43.92/0.994</named-content>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00991-t004"><object-id pub-id-type="pii">sensors-25-00991-t004_Table 4</object-id><label>Table 4</label><caption><p>For the <inline-formula><mml:math id="mm149" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>9</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task, the results of the comparative experiment on synthetic dataset. We use PSNR/SSIM to evaluate. The best result in red and the suboptimal result in blue.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">HCInew</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">HCIold</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SAANet&#x000a0;[<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">33.68/0.925</td><td align="center" valign="middle" rowspan="1" colspan="1">39.57/0.946</td><td align="center" valign="middle" rowspan="1" colspan="1">36.63/0.936</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DistgASR&#x000a0;[<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">37.53/0.983</td><td align="center" valign="middle" rowspan="1" colspan="1">42.51/0.977</td><td align="center" valign="middle" rowspan="1" colspan="1">40.02/0.980</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Xia et&#x000a0;al.&#x000a0;[<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">37.85/0.984</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">42.76/0.980</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">40.31/0.982</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">38.22/0.987</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">42.92/0.981</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">40.57/0.984</named-content>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00991-t005"><object-id pub-id-type="pii">sensors-25-00991-t005_Table 5</object-id><label>Table 5</label><caption><p>The number of parameters and reconstruction quality of our method are compared with existing methods. We used <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02192;</mml:mo><mml:mn>7</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> LFASR task and synthetic dataset for comparative experiments. The best result in red.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params.</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Synthetic Average</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Yeung et&#x000a0;al.&#x000a0;[<xref rid="B7-sensors-25-00991" ref-type="bibr">7</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1.50 M</td><td align="center" valign="middle" rowspan="1" colspan="1">35.99/0.920</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Jin et&#x000a0;al.&#x000a0;[<xref rid="B15-sensors-25-00991" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">1.11 M</td><td align="center" valign="middle" rowspan="1" colspan="1">37.72/0.948</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SAANet&#x000a0;[<xref rid="B9-sensors-25-00991" ref-type="bibr">9</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">0.34 M</named-content>
</td><td align="center" valign="middle" rowspan="1" colspan="1">35.21/0.904</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DistgASR&#x000a0;[<xref rid="B10-sensors-25-00991" ref-type="bibr">10</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">2.68M</td><td align="center" valign="middle" rowspan="1" colspan="1">38.44/0.976</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Xia et&#x000a0;al.&#x000a0;[<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>](<inline-formula><mml:math id="mm151" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" rowspan="1" colspan="1">1.53 M</td><td align="center" valign="middle" rowspan="1" colspan="1">38.41/0.975</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Xia et&#x000a0;al.&#x000a0;[<xref rid="B12-sensors-25-00991" ref-type="bibr">12</xref>] (<inline-formula><mml:math id="mm152" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" rowspan="1" colspan="1">5.98 M</td><td align="center" valign="middle" rowspan="1" colspan="1">39.46/0.982</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ours (<inline-formula><mml:math id="mm153" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" rowspan="1" colspan="1">1.47 M</td><td align="center" valign="middle" rowspan="1" colspan="1">38.74/0.977</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours (<inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.79 M</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">39.91/0.985</named-content>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00991-t006"><object-id pub-id-type="pii">sensors-25-00991-t006_Table 6</object-id><label>Table 6</label><caption><p>The results of the ablation study to verify the effectiveness of the MMOF-DCN and the MMOF design for DCN. We use PSNR/SSIM to evaluate. The best result in red and the suboptimal result in blue.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Variant</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">HCInew</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">HCIold</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SAC-R-CNN</td><td align="center" valign="middle" rowspan="1" colspan="1">35.30/0.977</td><td align="center" valign="middle" rowspan="1" colspan="1">42.01/0.977</td><td align="center" valign="middle" rowspan="1" colspan="1">38.66/0.977</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SAC-Transformer</td><td align="center" valign="middle" rowspan="1" colspan="1">35.87/0.979</td><td align="center" valign="middle" rowspan="1" colspan="1">42.19/0.980</td><td align="center" valign="middle" rowspan="1" colspan="1">39.03/0.980</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SAC-B-CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">36.20/0.980</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.32/0.982</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">39.26/0.981</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">O-DCN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">36.66/0.982</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">42.50/0.983</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #0000FF">39.58/0.983</named-content>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">37.07/0.985</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">42.85/0.984</named-content>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<named-content content-type="color: #FF0000">39.91/0.985</named-content>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-00991-t007"><object-id pub-id-type="pii">sensors-25-00991-t007_Table 7</object-id><label>Table 7</label><caption><p>Parameters and qualities of different number of DFE blocks. We use PSNR/SSIM to evaluate. Our proposed <inline-formula><mml:math id="mm155" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is shown in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Variant</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Params</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">HCInew</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">HCIold</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Average</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">m = 1</td><td align="center" valign="middle" rowspan="1" colspan="1">2.97 M</td><td align="center" valign="middle" rowspan="1" colspan="1">35.49/0.978</td><td align="center" valign="middle" rowspan="1" colspan="1">42.09/0.978</td><td align="center" valign="middle" rowspan="1" colspan="1">38.79/0.978</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">m = 2</td><td align="center" valign="middle" rowspan="1" colspan="1">4.31 M</td><td align="center" valign="middle" rowspan="1" colspan="1">36.24/0.981</td><td align="center" valign="middle" rowspan="1" colspan="1">42.31/0.982</td><td align="center" valign="middle" rowspan="1" colspan="1">39.28/0.982</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>m = 3</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>5.79 M</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>37.07</bold>/<bold>0.985</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>42.85</bold>/<bold>0.984</bold></td><td align="center" valign="middle" rowspan="1" colspan="1"><bold>39.91</bold>/<bold>0.985</bold></td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">m = 4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.10 M</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.16/0.985</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.91/0.984</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40.03/0.985</td></tr></tbody></table></table-wrap></floats-group></article>