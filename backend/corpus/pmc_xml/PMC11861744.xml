<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006411</article-id><article-id pub-id-type="pmc">PMC11861744</article-id><article-id pub-id-type="doi">10.3390/s25041182</article-id><article-id pub-id-type="publisher-id">sensors-25-01182</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>HGF-MiLaG: Hierarchical Graph Fusion for Emotion Recognition in Conversation with Mid-Late Gender-Aware Strategy</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-2695-3173</contrib-id><name><surname>Wang</surname><given-names>Yihan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-sensors-25-01182" ref-type="aff">1</xref><xref rid="fn1-sensors-25-01182" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-4034-1400</contrib-id><name><surname>Hao</surname><given-names>Rongrong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="af1-sensors-25-01182" ref-type="aff">1</xref><xref rid="fn1-sensors-25-01182" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Ziheng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af1-sensors-25-01182" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Kuang</surname><given-names>Xinhe</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01182" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Dong</surname><given-names>Jiacheng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-01182" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-3277-9061</contrib-id><name><surname>Zhang</surname><given-names>Qi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-01182" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Qian</surname><given-names>Fengkui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-01182" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Fu</surname><given-names>Changzeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-01182" ref-type="aff">1</xref><xref rid="af2-sensors-25-01182" ref-type="aff">2</xref><xref rid="af3-sensors-25-01182" ref-type="aff">3</xref><xref rid="c1-sensors-25-01182" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Sato</surname><given-names>Wataru</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01182"><label>1</label>Sydney Smart Technology College, Northeastern University, Qinhuangdao Campus, Qinhuangdao 066004, China; <email>202219206@stu.neuq.edu.cn</email> (Y.W.); <email>202219259@stu.neuq.edu.cn</email> (R.H.); <email>202219039@stu.neuq.edu.cn</email> (Z.L.); <email>202319257@stu.neuq.edu.cn</email> (X.K.); <email>202219071@stu.neuq.edu.cn</email> (J.D.); <email>202212078@stu.neu.edu.cn</email> (Q.Z.); <email>2372410@stu.neu.edu.cn</email> (F.Q.)</aff><aff id="af2-sensors-25-01182"><label>2</label>Osaka University, Toyonaka Campus, Osaka 560-0043, Japan</aff><aff id="af3-sensors-25-01182"><label>3</label>Hebei Key Laboratory of Marine Perception Network and Data Processing, Northeastern University, Qinhuangdao Campus, Qinhuangdao 066004, China</aff><author-notes><corresp id="c1-sensors-25-01182"><label>*</label>Correspondence: <email>changzeng.fu@irl.sys.es.osaka-u.ac.jp</email></corresp><fn id="fn1-sensors-25-01182"><label>&#x02020;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>14</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1182</elocation-id><history><date date-type="received"><day>08</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>29</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>12</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Emotion recognition in conversation (ERC) is an important research direction in the field of human-computer interaction (HCI), which recognizes emotions by analyzing utterance signals to enhance user experience and plays an important role in several domains. However, existing research on ERC mainly focuses on constructing graph networks by directly modeling interactions on multimodal fused features, which cannot adequately capture the complex dialog dependency based on time, speaker, modalities, etc. In addition, existing multi-task learning frameworks for ERC do not systematically investigate how and where gender information is injected into the model to optimize ERC performance. To address the above problems, this paper proposes a Hierarchical Graph Fusion for ERC with Mid-Late Gender-aware Strategy (HGF-MiLaG). HGF-MiLaG uses hierarchical fusion graph to adequately capture intra-modal and inter-modal speaker dependency and temporal dependency. In addition, HGF-MiLaG explores the effect of the location of gender information injections on ERC performance, and ultimately employs a Mid-Late multilevel gender-aware strategy in order to allow the hierarchical graph network to determine the proportion of emotion and gender information in the classifier. Empirical results on two public multimodal datasets(i.e.,IEMOCAP and MELD), demonstrate that HGF-MiLaG outperforms existing methods.</p></abstract><kwd-group><kwd>emotion recognition</kwd><kwd>hierarchical graph fusion</kwd><kwd>mid-late multilevel gender-aware strategy</kwd><kwd>multi-task learning</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62306068</award-id></award-group><award-group><funding-source>Natural Science Foundation of Hebei Province</funding-source><award-id>F2024501002</award-id></award-group><funding-statement>This research was funded by the National Natural Science Foundation of China (Grant No. 62306068) Project and the Natural Science Foundation of Hebei Province, China (Grant No. F2024501002).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01182"><title>1. Introduction</title><p>With the rapid development of AI technology, the application of emotion recognition in conversation (ERC) in human-computer interaction (HCI) has gained widespread attention. ERC technology is capable of recognizing the emotion of speakers by analyzing signals (i.e., audio, video, text, etc.) in utterance. By recognizing and responding to emotional state of users, machines can provide a more personalized and empathetic interaction experience, deepening the human emotional connection between humans and machines, thus enhancing user experience [<xref rid="B1-sensors-25-01182" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01182" ref-type="bibr">2</xref>]. Additionally, ERC has played a significant role in the fields of opinion mining in social media [<xref rid="B3-sensors-25-01182" ref-type="bibr">3</xref>], doctor-patient interaction in clinical practice [<xref rid="B4-sensors-25-01182" ref-type="bibr">4</xref>], depression diagnosis [<xref rid="B5-sensors-25-01182" ref-type="bibr">5</xref>], and electronic learning environments [<xref rid="B6-sensors-25-01182" ref-type="bibr">6</xref>].</p><p>In ERC, understanding and utilizing contextual information is particularly important [<xref rid="B7-sensors-25-01182" ref-type="bibr">7</xref>]. Capture of speaker dependency and temporal dependency can provide rich contextual understanding. Speaker dependency refers to emotional interactions between individuals in a conversation [<xref rid="B8-sensors-25-01182" ref-type="bibr">8</xref>]. This dependency includes changes in not only the individual&#x02019;s own emotional state, but also emotional interactions between different speakers. For example, as shown in <xref rid="sensors-25-01182-f001" ref-type="fig">Figure 1</xref>, an example of intra-speaker dependency is that in u7, the son expresses frustration, an emotional state that may have been influenced by his own previous utterances (u3, u5). An example of inter-speaker dependency is that in u10, the son expresses concern about his mother&#x02019;s angry feelings, which may have been influenced by his mother&#x02019;s angry utterance in u8 and u9. Temporal dependency involves changes in emotional states over time in a conversation. Past utterance influences the trajectory of future utterance, and future utterance may fill in missing information in the past utterance [<xref rid="B9-sensors-25-01182" ref-type="bibr">9</xref>]. For example, as shown in <xref rid="sensors-25-01182-f001" ref-type="fig">Figure 1</xref>, &#x02018;frustrated&#x02019; in u5 is influenced by &#x02018;sad&#x02019; in u4, and it is also supplemented by the information of the emotion state of &#x02018;sad&#x02019; of u6 at the next time step. Therefore, the ability to adequately capture speaker dependency and temporal dependency in a conversation is crucial to improve the performance of ERC. HiGRU [<xref rid="B10-sensors-25-01182" ref-type="bibr">10</xref>] combined attention and recurrent neural networks to capture neighboring utterance through three main components, but it failed to take speaker dependency into account. To this end, DialogueGCN [<xref rid="B11-sensors-25-01182" ref-type="bibr">11</xref>] captured inter-speaker dependence and intra-speaker dependence through a graph network, but it used only textual information and failed to take full advantage of complementarities between different modalities of information, which led to poor performance in complex emotion recognition tasks. Therefore, researchers started to focus on how to integrate information from different modalities using graph networks, proposing cross-modal multi-head attention mechanisms [<xref rid="B12-sensors-25-01182" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01182" ref-type="bibr">13</xref>] and cross-modal feature complementation [<xref rid="B14-sensors-25-01182" ref-type="bibr">14</xref>]. However, existing approaches mainly focus on fusing multimodal features and directly modeling the relationships, which cannot adequately capture speaker dependency and temporal dependency within and across modalities. This may lead to a degradation of the overall performance of the model when there are differences in the quality of information across unimodality.</p><p>Additionally, adding auxiliary tasks to ERC can improve the performance of the model significantly. Existing multi-task learning, such as the auxiliary tasks in speaker recognition [<xref rid="B15-sensors-25-01182" ref-type="bibr">15</xref>], emotion shift detection (ESD-ERC model) [<xref rid="B16-sensors-25-01182" ref-type="bibr">16</xref>], facial expression perception (Facial MMT framework) [<xref rid="B17-sensors-25-01182" ref-type="bibr">17</xref>], provided additional supervised signals to the ERC model, which helped to improve the model&#x02019;s understanding and prediction of emotional states. However, most multitask learning fails to utilize gender information effectively, or does not systematically examine how and where gender information is injected into the model to optimize ERC performance. Studies have shown that males and females differ in emotion expression, and these differences are not only reflected in the audio characteristics of speech, such as pitch, intensity, and sound quality, but may also affect the accuracy of emotion recognition [<xref rid="B18-sensors-25-01182" ref-type="bibr">18</xref>]. Ignoring gender information can lead to models that fail to model individual differences effectively, thus affecting the personalization and accuracy of emotion recognition.</p><p>To address these limitations, this paper introduces Hierarchical Graph Fusion and Mid-Late Gender-aware Strategy (HGF-MiLaG). HGF-MiLaG captures the emotional dynamics of a conversation by constructing Hierarchical Fusion Graph in order to enhance the model&#x02019;s understanding of semantics and emotions. Specifically, the hierarchical fusion graph is constructed by first constructing unimodal graphs for each modality to capture intra-modal dependency, and then building multimodal graph that enables inter-modal information integration and capture inter-modal speaker dependency, the combination of which is the basis of the hierarchical graph fusion strategy. For the construction of each graph, we model conversations with the help of directed graphs in order to model speaker dependency and temporal dependency. Each node in the graph represents a utterance and the edges represent dependency between utterances. We input the built graph into a graph convolutional network [<xref rid="B9-sensors-25-01182" ref-type="bibr">9</xref>] to propagate contextual information. Meanwhile, to address the problem of ignored gender information, we introduce the Mid-Late multilevel gender-aware strategy, and the gender prediction subtask was designed as an auxiliary task to enhance contextual understanding and individual difference recognition for emotion recognition. Considering that mid-stage fusion with feature interaction and fusion at the middle layer of the model can more utilize complementary information between tasks effectively, while late-stage fusion can fuse data from different tasks flexibly. Therefore, in the process of optimizing the multitasking mechanism, we propose an innovative approach of injecting gender-auxiliary information into both the unimodal graph structure with speaker dependency and temporal dependency (mid-stage), and the multimodal graph structure (late-stage), in order to allow the hierarchical graph network to decide how much gender and emotion information to use.</p><p>In particular, it should be noted that due to the involvement of gender labels, we need to face up to the potential bias brought about by the use of gender information for modelling. In the data processing process, if there is gender imbalance in the training data or under-representation of emotion data of certain genders, it may lead to bias in the learning process of the model, which in turn may lead to unfair results in practical applications. For example, if the amount of female emotion data is much larger than that of male, the model may identify female emotions more accurately, while the judgement of male emotions is biased. Therefore, in subsequent studies, we will always be highly cautious to circumvent potential biases and ensure the fairness and social value of the model.</p><p>General speaking, our main contributions and innovations of this work are as follows:</p><p>1. Hierarchical graph fusion: In order to capture and integrate complex dependency in multimodal data, we propose a hierarchical graph fusion strategy. This strategy first constructs three unimodal graphs to capture speaker dependency and temporal dependency within modalities, and then constructs a multimodal graph to represent inter-modal dependency, which ultimately enhances the model&#x02019;s ability to recognize emotionally relevant features, and provides a richer and more dynamic perspective for the ERC task.</p><p>2. Mid-Late multi-Level gender-aware strategy: In order to improve the accuracy of the model in emotion node classification and prediction tasks, we adopt a Mid-Late multi-level gender-aware strategy approach to inject gender information. Taking into account the advantages of the Mid-Late fusion strategy that feature interaction and fusion at the middle layer of the model can utilize the complementary information between tasks more effectively, and the late-stage fusion that can flexibly fuse the data from different tasks, we finally adopt the simultaneous injection of gender-auxiliary information in both the unimodal graph structure with speaker-dependent and temporal-dependent features (mid-stage) and the multimodal graph structure (late-stage) to improve the model&#x02019;s performance on a emotion classification task. We further explore the effect of the location of gender information injection on ERC performance in <xref rid="sec5-sensors-25-01182" ref-type="sec">Section 5</xref>, and the results corroborate our ideas.</p><p>We evaluated the proposed model on the IEMOCAP and MELD dialog datasets and compared it with existing methods. The experimental results show that the model has competitive performance over the chosen competitors on both datasets. The rest of the paper is organized as follows: we present related work in <xref rid="sec2-sensors-25-01182" ref-type="sec">Section 2</xref>. The details of our proposed methodology are given in <xref rid="sec3-sensors-25-01182" ref-type="sec">Section 3</xref>. The experiments are detailed in <xref rid="sec4-sensors-25-01182" ref-type="sec">Section 4</xref>. Results and discussions are presented in <xref rid="sec5-sensors-25-01182" ref-type="sec">Section 5</xref>, and <xref rid="sec6-sensors-25-01182" ref-type="sec">Section 6</xref> briefly summarizes our work.</p></sec><sec id="sec2-sensors-25-01182"><title>2. Related Works</title><sec id="sec2dot1-sensors-25-01182"><title>2.1. Graph Neural Network</title><p>In the field of conversational emotion recognition, graph neural networks (GNNs) provide a powerful framework for modeling speaker dependency in conversations due to their excellent relational modeling capabilities. Ghosal et al. [<xref rid="B11-sensors-25-01182" ref-type="bibr">11</xref>] applied graph neural networks to ERC for the first time by proposing the DialogueGCN model, which solved the context propagation problem that existed in recurrent neural network (RNN)-based models. However, the method mainly focused on textual information and does not integrate multimodal data. To solve this problem, Hu et al. [<xref rid="B12-sensors-25-01182" ref-type="bibr">12</xref>] proposed a multimodal graph convolutional network (MMGCN) based on DialogueGCN, which improves the accuracy of emotion recognition by exploiting the dependency between modalities. However, MMGCN crudely connected the utterance to all other utterances within the modality, which brought additional noise. For this reason, Li et al. [<xref rid="B19-sensors-25-01182" ref-type="bibr">19</xref>] proposed a novel multimodal fusion method (GraphMFT) based on graph neural networks, which reduced the noise by constructing multiple heterogeneous graphs and introducing an improved graph attention network. However, the above GNN-based method establishes utterance dependency under a fixed window and thus tends to over-consider contextual information that is weakly or irrelevantly related to the current utterance. Therefore, Gan et al. [<xref rid="B20-sensors-25-01182" ref-type="bibr">20</xref>] proposed a model for recognizing emotions in conversations using graph neural networks, supplemented with novel context filters and feature correction mechanisms, which yielded superior performance in the task of conversational emotion recognition. The graph neural network model of HAM-GNN proposed by Fu et al. [<xref rid="B21-sensors-25-01182" ref-type="bibr">21</xref>] efficiently models conversational behavior labels by capturing interactions between speakers and contextual semantics, enabling efficient conversational behavior classification. It can be concluded that graph neural networks (GNNs) have made significant progress in the field of ERC. In this paper, we propose the hierarchical graph fusion strategy to adequately capture intra-modal and inter-modal dependency.</p></sec><sec id="sec2dot2-sensors-25-01182"><title>2.2. Multi-Task Learning</title><p>Multi-task learning is an efficient machine learning strategy [<xref rid="B22-sensors-25-01182" ref-type="bibr">22</xref>] that enhances the generalization ability of a model by training multiple related tasks simultaneously. The core advantage of this approach is the ability to share knowledge points and utilize the correlation between tasks to enhance the learning efficiency of each task. Ruder [<xref rid="B23-sensors-25-01182" ref-type="bibr">23</xref>] pointed out that the use of a multi-task learning approach can simultaneously optimize multiple emotion-related subtasks during the training process, which not only helps to reduce the overfitting problem of the model, but also improves the recognition effect. In addition, collaboration between the tasks also promotes complementary information between modalities and effectively avoids affecting the performance of the whole model due to insufficient data or noise in one modality [<xref rid="B24-sensors-25-01182" ref-type="bibr">24</xref>]. A typical example is the multimodal emotion recognition model based on multi-task learning proposed by Wang et al. [<xref rid="B25-sensors-25-01182" ref-type="bibr">25</xref>], which improved modal fusion by setting auxiliary tasks to learn more emotionally inclined visual and audio representations. Xue et al. [<xref rid="B26-sensors-25-01182" ref-type="bibr">26</xref>] investigated and proposed a self-supervised dynamic fusion model for multi-task multimodal interactive learning, centered on text modality and supplemented by audio modality and video modality, using distribution similarity loss function and heterogeneity loss function to learn commonality characterization and characteristic characterization of modalities. Based on this, multi-task learning is used to obtain the consistency and difference characterization of the modalities. These studies have fully demonstrated the potential of multitask learning in multimodal emotion recognition. In this paper, we further introduce a multitask learning module for Mid-Late multilevel gender-aware strategy to enhance contextual understanding and individual difference recognition for emotion recognition, thus further improving the performance of the model.</p></sec><sec id="sec2dot3-sensors-25-01182"><title>2.3. Auxiliary Information Fusion</title><p>Individual difference modeling is one of the key directions for optimization of ERC, which belongs to the subtask of auxiliary information fusion [<xref rid="B27-sensors-25-01182" ref-type="bibr">27</xref>]. In the process of individual difference modeling, the location factor of fusion is crucial, and different fusion locations can significantly affect the model performance. Depending on the location of the fusion, the research methods are primarily divided into early-stage fusion, mid-stage fusion, and late-stage fusion.</p><p>Early-stage fusion strategies mainly inject auxiliary information at the input layer [<xref rid="B28-sensors-25-01182" ref-type="bibr">28</xref>]. For example, in the field of multimodal emotion analysis, research in Tian et al. [<xref rid="B29-sensors-25-01182" ref-type="bibr">29</xref>] improved model performance by incorporating weakly labeled emotion information based on emoji filtering into word vector representations and introducing external feature extraction algorithms. This approach simplifies the subsequent processing by merging data from different modalities right at the data preprocessing stage. However, the disadvantage of early-stage fusion is that it may lead to under-modeling of high-dimensional feature space and complex relationships between modalities [<xref rid="B30-sensors-25-01182" ref-type="bibr">30</xref>]. Late-stage fusion strategies merge information from different tasks in the final stage of the model. For example, Zadeh et al. [<xref rid="B31-sensors-25-01182" ref-type="bibr">31</xref>] proposed the Multi-Attention Recurrent Network (MARN). MARN recognizes emotions by merging information from different modalities in the final stage of the model. The disadvantage of late fusion is that it may not adequately capture information about interactions between different modalities because each modality is processed independently, which may lead to information loss [<xref rid="B32-sensors-25-01182" ref-type="bibr">32</xref>]. The mid-stage fusion strategy, on the other hand, performs feature interaction and fusion at the middle layer of the model [<xref rid="B31-sensors-25-01182" ref-type="bibr">31</xref>]. Wu et al. [<xref rid="B33-sensors-25-01182" ref-type="bibr">33</xref>] proposed a multimodal emotion recognition method based on the assistance of affective information, which simultaneously improves the performance of emotion classification and emotion recognition tasks by means of joint learning. Specifically, the method encoded modality internal information through a private network layer and achieved mid-stage fusion by jointly learning the main and auxiliary tasks through a shared network layer. The mid-stage fusion strategy provides a way to balance the disadvantages of early-stage and late-stage fusion due to its ability to perform feature interaction and fusion at the middle layer of the model, which can utilize the complementary information between the tasks more effectively, improving the accuracy and robustness of emotion recognition.</p><p>Therefore, in the process of optimizing the multitasking mechanism, we propose an innovative approach of injecting both gender-auxiliary information into the unimodal graph structure with speaker dependency and temporal dependency (mid-stage) and the multimodal graph structure (late-stage), which is done in order to allow the hierarchical graph network to decide on the proportion of gender information and emotion information utilized.</p></sec></sec><sec id="sec3-sensors-25-01182"><title>3. Method</title><p>This section describes the proposed method in this paper in detail. The overall architecture of our method is shown in <xref rid="sensors-25-01182-f002" ref-type="fig">Figure 2</xref>, including context encoding, construction of hierarchical graph based on graph networks, injection of gender information and emotion prediction.</p><sec id="sec3dot1-sensors-25-01182"><title>3.1. Task Definition</title><p>In the ERC scenario, each dialog contains M utterances <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and each utterance incorporates information from three modalities: audio (<inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>), text (<inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>), and video (<inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>). The core task of ERC is to assign each utterance in the dialog a emotion label <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Our model exploits the speaker dependency and temporal dependency within and across modalities, and proposes a hierarchical graph fusion strategy. In addition, in order to improve the accuracy of emotion recognition, a Mid-Late multilevel gender-aware strategy is adopted to incorporate gender information as an auxiliary task in the model training.</p></sec><sec id="sec3dot2-sensors-25-01182"><title>3.2. Unimodal Feature Extraction</title><p>AUDIO: We used the OpenSmile toolkit and its IS10 configuration [<xref rid="B34-sensors-25-01182" ref-type="bibr">34</xref>] for audio feature extraction. During feature extraction, the window length was set to 25 ms, the step size to 10 ms, and the frames were windowed using a Hamming window. Finally, the acoustic features of each speech are represented as a 1582-dimensional feature vector.</p><p>TEXT: We use RoBERTa model [<xref rid="B10-sensors-25-01182" ref-type="bibr">10</xref>] to extract the context independent utterance level feature vector. Specifically, we fine-tune the RoBERTa Large model so that each utterance can be generated by the model as a 1024-dimensional feature vector representation.</p><p>VISUAL: In line with previous methods [<xref rid="B35-sensors-25-01182" ref-type="bibr">35</xref>], we extract visual features using the DenseNet model [<xref rid="B36-sensors-25-01182" ref-type="bibr">36</xref>] to generate a feature vector of dimension 342 for each utterance.</p></sec><sec id="sec3dot3-sensors-25-01182"><title>3.3. Hierarchical Graph Fusion</title><sec id="sec3dot3dot1-sensors-25-01182"><title>3.3.1. Context Encoding</title><p>To obtain the contextual information of each modality, we refer to DialogueGCN [<xref rid="B11-sensors-25-01182" ref-type="bibr">11</xref>] and use bidirectional gated recurrent unit (<inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>GRU</mml:mi><mml:mo>&#x02194;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>).<disp-formula id="FD1-sensors-25-01182"><label>(1)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mo form="prefix">GRU</mml:mo><mml:mo>&#x02194;</mml:mo></mml:mover><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b1;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the context-independent feature representation of utterance i from the audio, textual, and visual modalities, respectively. <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the sequential contextual utterance of the output of each modal encoder.</p></sec><sec id="sec3dot3dot2-sensors-25-01182"><title>3.3.2. Hierarchical Fusion Graph Construction</title><p>We construct directed graphs for each modality to dynamically capture speaker dependency and temporal dependency between utterances. A conversation with M utterances is represented as three unimodal graphs <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="script">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="script">V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="script">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="script">T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> stands for vertices, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> stands for edges, and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> stands for edge types.</p><p>Vertex: Each utterance in a conversation contains three modalities, so each utterance is represented as a vertex <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> in the directed graph of audio, textual and visual modalities, respectively. Where each vertex is initialized with the feature vector <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> encoded in the corresponding context.</p><p>Edge: In a graph, an edge represents a connection between vertices. In our model, it is assumed that each utterance in a set of conversations has a direct dependency on all other utterances in the conversation, i.e., a conversation is constructed as a fully connected graph.</p><p>Edge Types: In dialog analysis, the connectivity relations between edges need to be considered in terms of speaker dependency and temporal dependency. Specifically, speaker dependency includes intra-speaker (<inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) dependency and inter-speaker (<inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) dependency. In this paper, only two speakers (<inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) are considered for conversational emotion recognition, so there are four relationship types for speaker dependency, i.e., speaker 1 self-dependency, speaker 2 self-dependency, speaker 1&#x02019;s dependency on speaker 2, and speaker 2&#x02019;s dependency on speaker 1. Temporal dependency considers the order in which utterance appears in a conversation, i.e., there are two types of relations: dependency of present utterance on future utterance and dependency of future utterance on present utterance. We use <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> to encode speaker dependency in three unimodal graphs: when there is some speaker dependency between two vertices, <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is 1; otherwise, <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is 0. Similarly, we encode the temporal dependence by <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus for each modality there are at most 8 different relation types <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="script">T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Graph Transformer: We introduce the Graph Transformer model [<xref rid="B37-sensors-25-01182" ref-type="bibr">37</xref>] to update the feature vectors of vertices and the weights of edges in unimodal graphs to obtain speaker-dependent and temporal-dependent vertex features. The model incorporates the classical multi-head attention mechanism, making it applicable to graph-structured data.<disp-formula id="FD2-sensors-25-01182"><label>(2)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="script">T</mml:mi></mml:msub></mml:mfrac></mml:mstyle><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the trainable matrix, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="script">T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the total number of relation types of edges, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the total number of utterances, and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the attention coefficients computed by the multi-head dot product attention mechanism.<disp-formula id="FD3-sensors-25-01182"><label>(3)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x022a4;</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mn>4</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mi>D</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where D is the dimension of the model.</p><p>Inspired by the studies of Chudasama et al. [<xref rid="B37-sensors-25-01182" ref-type="bibr">37</xref>] and Deng et al. [<xref rid="B38-sensors-25-01182" ref-type="bibr">38</xref>], we introduce a multi-modal cross-modal attention mechanism to fuse three unimodal features. The multimodal feature is finally represented by <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>Similar to the construction of per-modal graphs, we utilize the output multimodal feature representation <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to construct the multimodal graph structure <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="script">G</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="script">V</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="script">E</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="script">T</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> via speaker dependency and temporal dependency, where <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">V</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are the nodes, <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">E</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are the edges, and <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="script">T</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> are the type of relationship of the edges. Our goal is to utilize the graph structure to more fully integrate the multimodal features of different information dynamics.</p><p>Based on the multimodal graph construction, we utilize the graph convolutional network RGCN to deal with different types of relationships between different types of interactions such as intra-speaker and inter-speaker interactions. The capability of RGCN lies in its ability to learn unique representations of different types of relationships, which provides richer contextual information for our model.</p><p>First, the multimodal feature representation <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is input to the RGCN, which combines information from neighboring nodes as well as specific relationship types to update the feature representation of a node.<disp-formula id="FD4-sensors-25-01182"><label>(4)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003d1;</mml:mi><mml:mi>root</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mi>&#x003d1;</mml:mi><mml:mi>&#x003c4;</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003d1;</mml:mi><mml:mi>root</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a learnable parameter of the RGCN for combining the features of the node itself and its neighboring nodes. <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the set of neighboring nodes of node <italic toggle="yes">i</italic> under relation type <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003d1;</mml:mi><mml:mi>&#x003c4;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the learnable weight matrix associated with relation type <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mo>(</mml:mo><mml:mo>.</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the ReLU activation function.</p><p>After RGCN processing, we use the Weisfeiler-Lehman algorithm [<xref rid="B39-sensors-25-01182" ref-type="bibr">39</xref>] to further refine and summarize graph-related features. The WL algorithm encodes the topology of the graph by iteratively updating the feature representations of the nodes, thus generating a feature vector for each node containing information about the global graph structure. The WL processing can increase the model&#x02019;s ability to perceive the overall structure of the graph.<disp-formula id="FD5-sensors-25-01182"><label>(5)</label><mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>Mult</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>Mult</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the edge weight from source node <italic toggle="yes">j</italic> to target node <italic toggle="yes">i</italic>.</p></sec></sec><sec id="sec3dot4-sensors-25-01182"><title>3.4. Mid-Late Gender-Aware Emotion Feature Classifier</title><p>In this section, we introduce a gender prediction task that aims to integrate gender information as an auxiliary feature into our model. The goal of the auxiliary task is to predict the gender attributes of the dialog participants, thus enriching the model&#x02019;s understanding of user characteristics. In this way, our model is able to more accurately capture and reflect gender-related emotional features in conversations, which ultimately improves the accuracy of the model&#x02019;s main task of emotion recognition. To achieve this goal, we design a multi-task learning framework [<xref rid="B23-sensors-25-01182" ref-type="bibr">23</xref>] inspired by existing hard parameter sharing strategies [<xref rid="B40-sensors-25-01182" ref-type="bibr">40</xref>] and customized for our research goals. In particular, we adopt a Mid-Late multilevel gender-aware approach to inject gender information, which is a way to improve the performance of the model&#x02019;s emotion classification task by simultaneously injecting gender-auxiliary information in both the speaker-dependent and temporal-dependent unimodal graph structure (mid-stage), and in the fused multimodal graph structure (late-stage). Based on this, we constructed a lightweight multi-task GNN model, HGF-MiLaG, which shares all the convolutional layers between gender prediction and emotion prediction tasks to improve the accuracy of emotion recognition.</p><sec id="sec3dot4dot1-sensors-25-01182"><title>3.4.1. Middle Gender-Aware Emotion Classifier</title><p>In a unimodal gender-aware emotion classification task, feature vectors <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:msubsup></mml:mrow></mml:math></inline-formula> of each modal utterance are fed into respective mid-stage gender-aware emotion classifiers to predict emotion labels and gender labels. Specifically, the classifiers first compute the unnormalized emotion or gender category labels <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> via a ReLU activation function, and then apply a Softmax function to obtain the probability distributions <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. Ultimately, the model selects the category with the highest probability as the emotion or gender labels <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> for each modal prediction. The features of the two tasks are represented as: <disp-formula id="FD6-sensors-25-01182"><label>(6)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>W</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD7-sensors-25-01182"><label>(7)</label><mml:math id="mm55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD8-sensors-25-01182"><label>(8)</label><mml:math id="mm56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">argmax</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-01182"><label>(9)</label><mml:math id="mm57" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>W</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD10-sensors-25-01182"><label>(10)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-01182"><label>(11)</label><mml:math id="mm59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">argmax</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> denote the score vectors for emotion or gender categorization after processing by ReLU activation function, <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are the obtained emotion and gender probability distributions, <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are the emotion and gender labels predicted by the model, which is the most probable category in the probability distribution.</p></sec><sec id="sec3dot4dot2-sensors-25-01182"><title>3.4.2. Late Gender-Aware Emotion Classifier</title><p>To avoid omitting the initial multimodal representation information from the final emotion output, the processed multimodal representation <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the unprocessed multimodal representation <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> were concatenated together as a joint multimodal representation input into a late gender-aware emotion classifier, going through the following steps:<disp-formula id="FD12-sensors-25-01182"><label>(12)</label><mml:math id="mm68" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD13-sensors-25-01182"><label>(13)</label><mml:math id="mm69" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mn>0</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>t</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In order to capture the temporal dynamics of emotions in the dialog, i.e., to take into account the influence of the emotional dynamics of previous speeches on the emotions of current speeches in the model, we constructed the module of temporal attention, and input the feature sequences <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> into the formula of temporal attention to get the attention weights <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003be;</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. After processing, the model can reflect the evolution and dynamics of the emotional state in the dialog.<disp-formula id="FD14-sensors-25-01182"><label>(14)</label><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003be;</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>&#x02032;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>W</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>&#x02032;</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a trainable weight matrix used to map features to a new space to capture temporal relationships.</p><p><inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003be;</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is fed into the multimodal classifier to obtain the multimodal part of the output <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD15-sensors-25-01182"><label>(15)</label><mml:math id="mm77" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>&#x003be;</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-01182"><label>(16)</label><mml:math id="mm78" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD17-sensors-25-01182"><label>(17)</label><mml:math id="mm79" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi>&#x003be;</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:msubsup><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD18-sensors-25-01182"><label>(18)</label><mml:math id="mm80" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>Softmax</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Ultimately, the probability distribution <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> for emotion category and <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> for gender category obtained by the mid-state gender-aware emotion classifier as well as the probability distribution <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> for each emotion category and <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> for each gender category obtained by the late-state gender-aware emotion classifier are combined as the final prediction of each utterance&#x02019;s emotion label or gender labeling tool.<disp-formula id="FD19-sensors-25-01182"><label>(19)</label><mml:math id="mm85" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD20-sensors-25-01182"><label>(20)</label><mml:math id="mm86" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">arg</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD21-sensors-25-01182"><label>(21)</label><mml:math id="mm87" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD22-sensors-25-01182"><label>(22)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">arg</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the emotion and gender labels predicted by the utterance <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> respectively. <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a pre-determined hyperparameter.</p><p>In the Mid-Late multilevel gender-aware strategy, the model learns both emotion and gender features simultaneously by jointly optimizing the loss function, and the parameter update of the model is guided by the jointly optimized loss function, which enables the model to capture the complex interactions between the gender features and the emotion expression, and thus largely improves the overall performance of HGF-MiLaG.</p></sec><sec id="sec3dot4dot3-sensors-25-01182"><title>3.4.3. Loss Function</title><p>Throughout the process, we use a single loss function to train all classifiers simultaneously. Given that categorical cross-entropy is well-suited for classification tasks and L2 regularization helps mitigate overfitting [<xref rid="B41-sensors-25-01182" ref-type="bibr">41</xref>], we employ categorical cross-entropy as the loss function and incorporate L2 regularization to train our model in an end-to-end manner via backpropagation. Since there is a gender task involved, the total loss function is the sum of the loss functions of the gender classification task and the emotion classification task. It is composed as: <disp-formula id="FD23-sensors-25-01182"><label>(23)</label><mml:math id="mm93" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Loss</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>Loss</mml:mi><mml:mi>emo</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>Loss</mml:mi><mml:mi>gen</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula> is the weight of emotion information and <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula> is the weight of gender information, the sum of the weights is 1. The weights of the two pieces of information are determined through a hierarchical graph network.<disp-formula id="FD24-sensors-25-01182"><label>(24)</label><mml:math id="mm96" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Loss</mml:mi><mml:mrow><mml:mi>emo</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo form="prefix">log</mml:mo><mml:msubsup><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>emo</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:msubsup><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>emo</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD25-sensors-25-01182"><label>(25)</label><mml:math id="mm97" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Loss</mml:mi><mml:mi>emo</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>Loss</mml:mi><mml:mrow><mml:mi>emo</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD26-sensors-25-01182"><label>(26)</label><mml:math id="mm98" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Loss</mml:mi><mml:mrow><mml:mi>gen</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo form="prefix">log</mml:mo><mml:msubsup><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>gen</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:mfenced separators="" open="[" close="]"><mml:msubsup><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>gen</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msub><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD27-sensors-25-01182"><label>(27)</label><mml:math id="mm99" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Loss</mml:mi><mml:mi>gen</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>Loss</mml:mi><mml:mrow><mml:mi>gen</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where N is the number of utterances in the conversation, <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>emo</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>gen</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are the base truth labels for a single emotion prediction or gender prediction task respectively. Specifically, <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> if the emotion or gender type of a sample utterance <italic toggle="yes">i</italic> belongs to the <italic toggle="yes">i</italic>th class, and <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> otherwise. <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></inline-formula> is the L2-regularity weight, <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">&#x00398;</mml:mi></mml:mrow></mml:math></inline-formula> is the set of trainable parameters, and <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow></mml:math></inline-formula> are the weights of the respective modality (including multimodality) associated with emotion and gender, respectively.</p></sec></sec></sec><sec id="sec4-sensors-25-01182"><title>4. Experiment</title><sec id="sec4dot1-sensors-25-01182"><title>4.1. Dataset</title><p>We evaluated our proposed HGF-MiLaG in two public datasets: IEMOCAP [<xref rid="B42-sensors-25-01182" ref-type="bibr">42</xref>] and MELD [<xref rid="B43-sensors-25-01182" ref-type="bibr">43</xref>].</p><p>IEMOCAP: This dataset records the performances of 10 actors in 5 binary sessions covering 12 h of audio-visual data as well as textual transcriptions. Each session involved 2 actors and was segmented into individual utterances. Each utterance was labeled with one of the following six emotion labels: happy, sad, neutral, excited, frustrated, and angry.</p><p>MELD: This dataset is a large multimodal multi-party affective dialogue dataset that contains more than 13,000 utterance fragments extracted from the classic American TV show &#x0201c;Friends&#x0201d;, which are organized into approximately 1400 dialogues. Each utterance in the dialog is labeled with one of the following seven emotions: neutral, anger, disgust, fear, joy, sadness, surprise.</p></sec><sec id="sec4dot2-sensors-25-01182"><title>4.2. Baseline</title><p>To validate the effectiveness of our proposed hierarchical graph fusion with Mid-Late gender-aware strategy, we compared it with several previous baselines. The baselines include DiagueGCN [<xref rid="B11-sensors-25-01182" ref-type="bibr">11</xref>], DiagueCRN [<xref rid="B44-sensors-25-01182" ref-type="bibr">44</xref>], MMGCN [<xref rid="B12-sensors-25-01182" ref-type="bibr">12</xref>], COGMEN [<xref rid="B8-sensors-25-01182" ref-type="bibr">8</xref>], GraphCFC [<xref rid="B14-sensors-25-01182" ref-type="bibr">14</xref>], GA2MIF [<xref rid="B13-sensors-25-01182" ref-type="bibr">13</xref>], GraphMFT [<xref rid="B19-sensors-25-01182" ref-type="bibr">19</xref>], GCCL [<xref rid="B45-sensors-25-01182" ref-type="bibr">45</xref>], and HiMul-LGG [<xref rid="B46-sensors-25-01182" ref-type="bibr">46</xref>]. The details of these models are shown below.</p><p>DiagueGCN improves the accuracy of emotion recognition by constructing a directed graph to capture the dependency between individual speeches in a conversation, including inter-dependency and intra-dependency between speakers, and utilizing graph convolutional networks to propagate the contextual information in these dependency.</p><p>DiagueCRN mimics unique human cognitive thinking by designing a multi-round reasoning module in the cognitive stage to iteratively perform intuitive retrieval processes and conscious reasoning processes, thus providing a deeper understanding of the conversational context and identifying the key cues that trigger the current emotion.</p><p>MMGCN is a graph convolutional network model that fuses audio, visual, and textual modalities to enable information interaction by constructing graphs and establishing inter-modal edge connections, and injecting speaker embeddings to capture speaker dependency. The model employs a spectral domain graph convolutional network and extends to deep layers to enhance emotion recognition performance.</p><p>COGMEN is a cognitive graph-based emotion recognition model focused on modeling the process of human emotion understanding. The model captures the emotional dynamics in a conversation by combining local information (internal and external dependency between speakers) and global information (conversation context) using Graph Convolutional Networks (GCN) and Graph Transformers.</p><p>GraphCFC effectively mitigates the heterogeneity gap problem in multimodal fusion by using multiple subspace extractors and the pairwise cross-modal complementation (PairCC) strategy. By extracting multiple edges from the graph, the GNN can capture key contextual and interaction information in the message delivery more accurately. In addition, the model designs the GAT-MLP structure, which provides a new unified framework for multimodal learning.</p><p>GA2MIF focuses on contextual modeling and cross-modal modeling by utilizing multi-head directed graph attention networks (MDGATs) and multi-head paired cross-modal attention networks (MPCATs), respectively, and is able to capture the long-term contextual information within modalities and the complementary information between modalities efficiently.</p><p>GraphMFT integrates data objects from different modalities by constructing a graph that utilizes multiple improved graph attention networks to capture intra-modal contextual information and inter-modal complementary information.</p><p>GCCL is a multimodal emotion recognition framework that captures speaker, temporal, and inter-modal dependency and integrates multimodal information through a graph-based module. The framework includes a emotion consensus learning unit and a consensus-aware unit with an attention mechanism that ensures individual diversity and inter-modal semantic consistency as well as maintains category-level semantic associations across samples.</p><p>COLD Fusion quantifies modality-specific probability or data uncertainty to predict emotion through calibration and ordinal latent distribution fusion. It learns unimodal temporal contextual latent distributions by limiting variance and designs softmax distribution matching loss for uncertainty-weighted fusion. The method significantly improves the generalisation performance and robustness of emotion recognition on multiple datasets.</p><p>HiMul-LGG employs a hierarchical decision fusion strategy to ensure cross-modal feature consistency and a local-global graph neural network architecture to enhance inter-modality and intra-modality speaker dependency. In addition, HiMul-LGG utilizes a cross-modal multi-head attention mechanism to facilitate inter-modal interactions.</p></sec><sec id="sec4dot3-sensors-25-01182"><title>4.3. Experiment Setup</title><p>In this study, all experiments were run on NVIDIA GeForce RTX 4060 laptop Gpus (NVIDIA Corporation, Santa Clara, CA, USA). The training framework uses PyTorch 2.5.1 (developed by Facebook AI Research Team), Python version 3.9.0, CUDA Toolkit version 12.4 (NVIDIA Corporation, Santa Clara, CA, USA). During training, the Adam optimizer is used to train our network with the dropout rate of 0.1 and the learning rate set to 0.0003. And the batch size is set to 32 for all datasets. For context encoding, the number of units of the GRUs used is set to 160. The hidden dimension of the hidden layer of the hierarchical fusion graph and the hidden dimension of the temporal attention are also set to 160. The weights of the emotion information and the gender information are set to 0.7 and 0.3, respectively. The pre-determined hyper-parameters <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are 0.1, 0.7, and 0.2, respectively. Our evaluation method is the same as that of the chosen baseline article method, where the weighted average F1 scores and the average accuracies are used to evaluate the performance of HGF-MiLaG.</p></sec></sec><sec sec-type="results" id="sec5-sensors-25-01182"><title>5. Results and Discussion</title><p>In this section, experimental results will be reported to evaluate the proposed HGF-MiLaG. Firstly, an overall comparison of HGF-MiLaG with all baseline methods will be made. Then, the effects of different components in the ablation experiments on HGF-MiLaG are discussed. Further, we specifically discuss the effect of injecting gender information at different locations on the effectiveness of model implementation. Next, we explore the effects of different time window settings on HGF-MiLaG. Finally, we also performed an error analysis of our method.</p><sec id="sec5dot1-sensors-25-01182"><title>5.1. Comparison with Baseline Models</title><p>Our model obtained optimal weighted average F1 scores on both the IEMOCAP and MELD datasets. <xref rid="sensors-25-01182-t001" ref-type="table">Table 1</xref> compares the experimental results of HGF-MiLaG with other baseline models (mentioned in <xref rid="sec4dot2-sensors-25-01182" ref-type="sec">Section 4.2</xref>). As can be seen from <xref rid="sensors-25-01182-t001" ref-type="table">Table 1</xref>, for the IEMOCAP dataset, the accuracy and F1 score of HGF-MiLaG are 70.98% and 71.92%, respectively, which are 0.86% and 1.11% more accurate than the current state-of-the-art models HiMul-LGG and GCCL, respectively, and the F1 scores are 0.80% and 1.73% more accurate, respectively. In addition, we show the corresponding F1 scores for each emotion label in detail in the table, and HGF-MiLaG shows significant improvement on the emotion label of Happy. Notably, the F1 scores of HGF-MiLaG are significantly higher than the current state-of-the-art models GCCL and AVL COLD Fusion for all six emotion categories.For the MELD dataset, the accuracy and F1 scores of HGF-MiLaG are 66.22% and 65.26%, respectively, and similarly, compared to the accuracies of HiMul-LGG and GCCL models by 0.01% and 3.40%, respectively, and F1 scores by 0.08% and 4.98% over the HiMul-LGG and GCCL models, respectively.</p><p>Based on these results, the HGF-MiLaG model exhibits superior performance on both datasets, which is mainly attributed to the synergistic effect of hierarchical graph fusion and Mid-Late multilevel gender-aware strategy. The hierarchical graph fusion strategy effectively captures speaker dependency and temporal dependency within and across modalities by constructing unimodal and cross-modal graph structures. This enables the model to better understand the complex modal relationships and temporal dynamics in emotional expressions. The Mid-Late multilevel gender-aware strategy further enhances the model&#x02019;s contextual understanding of emotion recognition and its ability to recognise individual differences by simultaneously injecting gender-auxiliary information in both the middle and late stages of the model. Gender plays a key role in emotion expression, and there are often differences in the way males and females express their emotions. By introducing gender information, the model is able to better distinguish these differences, thus improving the overall recognition accuracy. The combination of these two approaches enables the model to not only capture rich modal and temporal information, but also deeply understand gender differences in emotion expressions, thus achieving better performance in complex emotion recognition tasks.</p></sec><sec id="sec5dot2-sensors-25-01182"><title>5.2. Ablation Study</title><p>To better understand the role of our model components, we performed ablation experiments on key components of HGF-MiLaG. The results are shown in <xref rid="sensors-25-01182-t002" ref-type="table">Table 2</xref>.</p><p>Effect of MiLaG. We first explored the impact of the Mid-Late multilevel gender-aware strategy on the model&#x02019;s emotion categorization task. For this purpose, we removed the gender-auxiliary information injection module, and what can be seen is that the accuracy decreased by 1.11% and the F1-weighted average score by 1.01% in the IEMOCAP dataset, and the accuracy decreased by 0.31% and the F1-weighted average score by 0.35% in the MELD dataset. Introducing gender-auxiliary information can provide the model with additional a priori knowledge to help it better capture individual differences in emotion expression. This strategy can enhance the model&#x02019;s contextual understanding of emotion recognition and make it more accurate in handling emotion classification tasks. This fully demonstrates the effectiveness of introducing gender-auxiliary information in ERC. It can enhance the model&#x02019;s contextual understanding of emotion recognition and individual difference recognition.</p><p>Impact of the MG. We then explored the impact of multimodal graphs on the model emotion classification task. For this purpose, we removed the multimodal fusion graph and relied solely on the three unimodal graph classifiers for emotion classification, in which case we can see a decrease of 5.55% in accuracy and 5.57% in F1-weighted average score for the IEMOCAP dataset, and a decrease of 0.58% in accuracy and 0.37% in F1-weighted average score for the MELD dataset. The reason for this is that there is complementarity and correlation between different modes, for example, intonation and rhythm of speech can enhance the intensity of emotion in text, while visual information (such as facial expressions) can further assist the recognition of emotion. As an effective intermodal information integration tool, multimodal graph can fuse information of different modes and capture intermodal speaker dependency and temporal dependency. This cross-modal fusion can make up for the deficiency of unimodal information and improve the accuracy and robustness of emotion classification. The fact that the classification performance decreases drastically proves the effectiveness of the multimodal map and the introduction of the multimodal map, which enables cross-modal information integration and thus captures inter-modal speaker dependency and temporal dependency.</p><p>Impact of UG. another core of HGF-MiLaG is the unimodal graph. It is responsible for modeling intra-modal speaker dependency and temporal dependency. For this reason, we removed the three unimodal graphs, and the final emotion classification results were determined by the multimodal classifier only, as can be seen by the fact that in the IEMOCAP dataset, the accuracy was reduced by 1.97% and the F1-weighted average score by 1.93%, and in the MELD dataset, the accuracy was reduced by 0.19% and the F1-weighted average score was reduced by 0.51%. According to the dynamic theory of emotion, the expression and transmission of emotion are sequential and dependent. Unimodal graphs can capture this dynamic change of emotion by modeling the temporal dependencies within the modes. In addition, modeling speaker dependence can further enhance the accuracy of emotion recognition, since the emotional expression styles and habits of different speakers may differ. The effectiveness of the unimodal graph lies in its ability to dig deep into the complex relationships within the modes and provide richer semantic and structural information for emotion classification. This demonstrates the validity of the unimodal graphs.</p></sec><sec id="sec5dot3-sensors-25-01182"><title>5.3. Comparison with Different Position of Gender Injection</title><p>In this section, we explore the effect of the location of gender injection on the IEMOCAP dataset, i.e., early, middle, late, and mid-late injection, on the model performance. The results in <xref rid="sensors-25-01182-t003" ref-type="table">Table 3</xref> clearly show that injecting gender information in the Mid-Late stage significantly improves the accuracy and weighted F1 scores of the model. Specifically, the F1 scores of injecting gender information in the Mid-Late stage are improved by 1.33%, 1.38%, and 1.81% compared to the F1 scores of injecting gender information in the early, middle, and late stages, respectively. This is due to the unique advantages of Mid-Late multilevel gender aware strategies: Middle stage fusion excels in capturing information about mid-level associations between different modalities, effectively balancing the advantages and disadvantages of early and late fusion. While early fusion facilitates the integration of information at the initial stage, it may lead to information loss or excessive smoothing due to premature merging of features from different modalities. On the other hand, late fusion allows for more flexible processing of data from different tasks in later stages of the model, but may not effectively capture inter-modal associations. The Mid-Late multilevel gender-aware strategy exploits mid-level correlation information by combining the advantages of middle stage fusion and late stage fusion, while maintaining the flexibility and robustness of late-stage fusion. This multilevel gender-aware strategy ensures that the model can better adapt to the complexity of multimodal data, thereby improving its overall performance.</p><p>To better understand the effect of gender injection location on the model, we used t-SNE to visualize the potential representations learned by different gender information injection locations on the IEMOCAP test set. <xref rid="sensors-25-01182-f003" ref-type="fig">Figure 3</xref>a&#x02013;c show the visualization results for gender information injected alone in the early, middle, and late stages, respectively, and <xref rid="sensors-25-01182-f003" ref-type="fig">Figure 3</xref>d shows the visualization results for gender information injected simultaneously in the middle and late stages. Compared with injecting gender information at early, middle, and late stages, our Mid-Late multilevel gender-aware strategy has significantly optimized the clustering of the same emotional category and increased the distance and clarity of boundaries between different emotional categories. Specifically, the representations of sad and happy are clearly distinguishable. Moreover, the overlapping areas of excited, frustrated, and neutral with other emotional categories have been significantly reduced. These results show that MiLaG is able to learn structured representations with clustered levels, improving the model&#x02019;s ability to recognize emotion categories.</p></sec><sec id="sec5dot4-sensors-25-01182"><title>5.4. Effect of Window Size</title><p>In this section, we discuss the effect of different window sizes on the performance of HGF-MiLaG. <xref rid="sensors-25-01182-f004" ref-type="fig">Figure 4</xref>a,b show the changes in accuracy and weighted F1 values of HGF-MiLaG corresponding to different window sizes on the IEMOCAP and MELD datasets, respectively.</p><p>IEMOCAP dataset sees a trend that both F1-w scores and accuracy rates show an increasing and then decreasing trend. The highest values of both F1-w score and accuracy are reached at a window size of 10, both of which are 0.712. while the lowest values of these two metrics are found at window sizes of 0 and 1, which are about 0.679 and 0.677, respectively. the model performance is best at a window size of 10. The model&#x02019;s performance first improves as the size of the contextual window increases, but beyond a certain size, the effect of the model&#x02019;s enhancement gradually diminishes, the even to the point of showing a downward trend. The possible reason for this is that shorter context windows cannot capture these long-term dependency, while longer windows help the model to capture these dependency, but beyond a certain length, the increased context information may contain too much noise or irrelevant data, which in turn reduces the model&#x02019;s recognition ability. Specifically, the optimal window on the IEMOCAP dataset is 10.</p><p>The same phenomenon also occurs in the MELD dataset, but with a lower degree of variability and more volatility, where the optimal window is 12. The F1-w scores and accuracy of HGF-MiLaG fluctuate over a small range with window size, and the overall scores are low.The F1-w scores do not vary much between a window size of 0 and 20, with the highest value being about 0.651 and the lowest about 0.645.The accuracy similarly does not vary much between a window size of 0 and 20, with the highest value being about 0.664 and the lowest about 0.657.This indicates that in the second graph, the window size has less effect on the model performance and the performance is more stable. The possible reason for this variability is that the MELD dataset is constructed based on dialogues from the Old Friends TV series, where some of the neighboring utterances are not contiguous in the actual scenarios, making the results somewhat uncertain and requiring longer distances for contextual modeling. In this section, we conclude that the number of windows (i.e., the number of nodes) in graph construction has a significant impact on the model, and that choosing appropriate forward and backward time steps can maximize the model performance and improve the model&#x02019;s ability for emotion classification tasks.</p></sec><sec id="sec5dot5-sensors-25-01182"><title>5.5. Insight from Output</title><p>To further evaluate the performance of the model, we show in <xref rid="sensors-25-01182-f005" ref-type="fig">Figure 5</xref> the confusion matrix of HGF-MiLaG on the IEMOCAP and MELD test sets, which is used to assess the quality of the model&#x02019;s prediction output. With the confusion matrix on the IEMOCAP test set (<xref rid="sensors-25-01182-f005" ref-type="fig">Figure 5</xref>a), we observe that the HGF-MiLaG model has some limitations in distinguishing emotional nuances. For example, the model exhibits low discrimination in recognizing anger vs. frustrated and happy vs. excited. This suggests that the model has room for improvement in capturing subtle differences in emotion.</p><p>We have further analysed the MELD test set to compare the model output distribution with the true distribution of the dataset. The results show that the model output distribution is highly consistent with the true distribution of the MELD dataset, and there is no data imbalance that causes the model to over-concentrate on outputting dominant labels.</p><p>Specifically, although the emotion labels Anger, Disgust and Fear have a small number of samples and can be easily misclassified as the Neutral label with the largest number of samples, the overall prediction results show that the model&#x02019;s predicted probability distributions of the emotion labels match with the actual distribution of the dataset. For example, the model&#x02019;s higher prediction probability for the Neutral label matches the actual situation in the MELD dataset where neutral emotion expressions are more common. This is not a flaw in the model, but rather a reflection of its ability to accurately reflect real-world emotion distributions. In real-world emotion analysis scenarios, the distribution of emotions is inherently uneven, and neutral emotion expressions are usually more common, with strong emotions (e.g., anger, disgust, etc.) being relatively less common, which is related to the natural distribution of people&#x02019;s daily language expressions and emotional states.</p><p>For visual presentation, we draw a histogram of the distribution of the model&#x02019;s output labels (see <xref rid="sensors-25-01182-f006" ref-type="fig">Figure 6</xref>) and compare it with the true distribution of the MELD dataset. As can be seen from the figure, the model predictions are highly consistent with the real distribution, further confirming the model&#x02019;s adaptability to real-world emotion distribution.</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-01182"><title>6. Conclusions</title><p>In this study, we present the innovative HGF-MiLaG model for analyzing conversational emotional states, and the model adequately combines the hierarchical graph fusion and the Mid-Late multilevel gender-aware strategy to improve the performance of the model&#x02019;s affective classification task. We provide insights into the performance of our model by comparing it to other baselines and related works in the IEMOCAP and MELD datasets suitable for our experiments. The results reported on both datasets establish the superiority of the HGF-MiLaG model performance. The results of the ablation experiments validate the contribution of the key components of HGF-MiLaG to the model, and show that the hierarchical fusion graph is able to capture speaker-dependency and temporal dependency within and across modalities, and that the introduction of gender-auxiliary information in the ERC strengthens the model&#x02019;s contextual understanding of emotion recognition and the identification of individual differences. Further, we explored the effect of the location of gender information injection on the performance of ERC, and finally adopted the Mid-Late multilevel gender-aware strategy to realize the ability to let the hierarchical graph network have the ability to decide the proportion of emotion and gender information in the classifier by itself. We believe that this study fully reveals the importance of graph-based modeling techniques and the incorporation of gender-auxiliary tasks at optimal locations for emotion recognition tasks, and that our work has taken an important step forward in the field of modeling the dynamics of complex information for multimodal emotion recognition in conversations. In future work, we will continue to push multimodal learning with a focus on solving the previously mentioned problem of similar emotion discrimination, as well as improving HGF-MiLaG to enhance the model&#x02019;s ability to deal with sample label imbalance. In addition, we would like to apply our model to more practical settings, similar to the multimodal dialog generation domain, etc., to understand the utility of our model. </p></sec></body><back><ack><title>Acknowledgments</title><p>The authors extend their thanks for the creators of the IEMOCAP and MELD datasets for making their data publicly available, which greatly supported their research.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Y.W. and R.H.; Methodology, Y.W. and R.H.; Software, Z.L.; Validation, X.K.; Formal analysis, F.Q.; Investigation, Z.L.; Data curation, J.D. and Q.Z.; Writing&#x02014;review &#x00026; editing, C.F.; Visualization, X.K.; Supervision, F.Q. and C.F.; Project administration, J.D. and Q.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are openly available.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01182"><label>1.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Pereira</surname><given-names>R.</given-names></name>
<name><surname>Mendes</surname><given-names>C.</given-names></name>
<name><surname>Costa</surname><given-names>N.</given-names></name>
<name><surname>Fraz&#x000e3;o</surname><given-names>L.</given-names></name>
<name><surname>Fern&#x000e1;ndez-Caballero</surname><given-names>A.</given-names></name>
<name><surname>Pereira</surname><given-names>A.</given-names></name>
</person-group><article-title>Human-Computer Interaction Approach with Empathic Conversational Agent and Computer Vision</article-title><source>Artificial Intelligence for Neuroscience and Emotional Systems, Proceedings of the 10th International Work-Conference on the Interplay Between Natural and Artificial Computation, Olh&#x000e2;o, Portugal, 4&#x02013;7 June 2024</source><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year><fpage>431</fpage><lpage>440</lpage></element-citation></ref><ref id="B2-sensors-25-01182"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Votintseva</surname><given-names>A.</given-names></name>
<name><surname>Johnson</surname><given-names>R.</given-names></name>
<name><surname>Villa</surname><given-names>I.</given-names></name>
</person-group><article-title>Emotionally Intelligent Conversational User Interfaces: Bridging Empathy and Technology in Human-Computer Interaction</article-title><source>Human-Computer Interaction, Proceedings of the International Conference on Human-Computer Interaction, Washington DC, USA, 29 June 2024</source><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year><fpage>404</fpage><lpage>422</lpage></element-citation></ref><ref id="B3-sensors-25-01182"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Messaoudi</surname><given-names>C.</given-names></name>
<name><surname>Guessoum</surname><given-names>Z.</given-names></name>
<name><surname>Ben Romdhane</surname><given-names>L.</given-names></name>
</person-group><article-title>Opinion mining in online social media: A survey</article-title><source>Soc. Netw. Anal. Min.</source><year>2022</year><volume>12</volume><fpage>25</fpage><pub-id pub-id-type="doi">10.1007/s13278-021-00855-8</pub-id></element-citation></ref><ref id="B4-sensors-25-01182"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>C.W.</given-names></name>
<name><surname>Wu</surname><given-names>B.C.</given-names></name>
<name><surname>Nguyen</surname><given-names>P.A.</given-names></name>
<name><surname>Wang</surname><given-names>H.H.</given-names></name>
<name><surname>Kao</surname><given-names>C.C.</given-names></name>
<name><surname>Lee</surname><given-names>P.C.</given-names></name>
<name><surname>Rahmanti</surname><given-names>A.R.</given-names></name>
<name><surname>Hsu</surname><given-names>J.C.</given-names></name>
<name><surname>Yang</surname><given-names>H.C.</given-names></name>
<name><surname>Li</surname><given-names>Y.C.J.</given-names></name>
</person-group><article-title>Emotion recognition in doctor-patient interactions from real-world clinical video database: Initial development of artificial empathy</article-title><source>Comput. Methods Programs Biomed.</source><year>2023</year><volume>233</volume><elocation-id>107480</elocation-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2023.107480</pub-id><pub-id pub-id-type="pmid">36965299</pub-id>
</element-citation></ref><ref id="B5-sensors-25-01182"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>LiMember</surname><given-names>W.J.</given-names></name>
<name><surname>Fu</surname><given-names>C.</given-names></name>
<name><surname>Lian</surname><given-names>C.</given-names></name>
<name><surname>Shan</surname><given-names>P.</given-names></name>
</person-group><article-title>Image Encoding and Fusion of Multi-modal Data Enhance Depression Diagnosis in Parkinson&#x02019;s Disease Patients</article-title><source>IEEE Trans. Affect. Comput.</source><year>2024</year><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2024.3418415</pub-id></element-citation></ref><ref id="B6-sensors-25-01182"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Imani</surname><given-names>M.</given-names></name>
<name><surname>Montazer</surname><given-names>G.A.</given-names></name>
</person-group><article-title>A survey of emotion recognition methods with emphasis on E-Learning environments</article-title><source>J. Netw. Comput. Appl.</source><year>2019</year><volume>147</volume><fpage>102423</fpage><pub-id pub-id-type="doi">10.1016/j.jnca.2019.102423</pub-id></element-citation></ref><ref id="B7-sensors-25-01182"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Poria</surname><given-names>S.</given-names></name>
<name><surname>Majumder</surname><given-names>N.</given-names></name>
<name><surname>Mihalcea</surname><given-names>R.</given-names></name>
<name><surname>Hovy</surname><given-names>E.</given-names></name>
</person-group><article-title>Emotion recognition in conversation: Research challenges, datasets, and recent advances</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>100943</fpage><lpage>100953</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2929050</pub-id></element-citation></ref><ref id="B8-sensors-25-01182"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Joshi</surname><given-names>A.</given-names></name>
<name><surname>Bhat</surname><given-names>A.</given-names></name>
<name><surname>Jain</surname><given-names>A.</given-names></name>
<name><surname>Singh</surname><given-names>A.V.</given-names></name>
<name><surname>Modi</surname><given-names>A.</given-names></name>
</person-group><article-title>COGMEN: COntextualized GNN based multimodal emotion recognition</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2205.02455</pub-id></element-citation></ref><ref id="B9-sensors-25-01182"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Defferrard</surname><given-names>M.</given-names></name>
<name><surname>Bresson</surname><given-names>X.</given-names></name>
<name><surname>Vandergheynst</surname><given-names>P.</given-names></name>
</person-group><article-title>Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</article-title><source>Proceedings of the 30th Annual Conference on Neural Information Processing Systems (NIPS)</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>5&#x02013;10 December 2016</conf-date></element-citation></ref><ref id="B10-sensors-25-01182"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiao</surname><given-names>W.</given-names></name>
<name><surname>Yang</surname><given-names>H.</given-names></name>
<name><surname>King</surname><given-names>I.</given-names></name>
<name><surname>Lyu</surname><given-names>M.R.</given-names></name>
</person-group><article-title>HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1904.04446</pub-id></element-citation></ref><ref id="B11-sensors-25-01182"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ghosal</surname><given-names>D.</given-names></name>
<name><surname>Majumder</surname><given-names>N.</given-names></name>
<name><surname>Poria</surname><given-names>S.</given-names></name>
<name><surname>Chhaya</surname><given-names>N.</given-names></name>
<name><surname>Gelbukh</surname><given-names>A.</given-names></name>
</person-group><article-title>DialogueGCN: A graph convolutional neural network for emotion recognition in conversation</article-title><source>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP-IJCNLP)</source><conf-loc>Hong Kong, China</conf-loc><conf-date>3&#x02013;7 November 2019</conf-date><fpage>154</fpage><lpage>164</lpage></element-citation></ref><ref id="B12-sensors-25-01182"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Jin</surname><given-names>Q.</given-names></name>
</person-group><article-title>MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation</article-title><source>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics &#x00026; 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</source><conf-loc>Online</conf-loc><conf-date>1&#x02013;6 August 2021</conf-date><fpage>5666</fpage><lpage>5675</lpage></element-citation></ref><ref id="B13-sensors-25-01182"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Lv</surname><given-names>G.</given-names></name>
<name><surname>Zeng</surname><given-names>Z.</given-names></name>
</person-group><article-title>GA2MIF: Graph and attention based two-stage multi-source information fusion for conversational emotion detection</article-title><source>IEEE Trans. Affect. Comput.</source><year>2024</year><volume>15</volume><fpage>130</fpage><lpage>143</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2023.3261279</pub-id></element-citation></ref><ref id="B14-sensors-25-01182"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Lv</surname><given-names>G.</given-names></name>
<name><surname>Zeng</surname><given-names>Z.</given-names></name>
</person-group><article-title>GraphCFC: A Directed Graph Based Cross-Modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition</article-title><source>IEEE Trans. Multimed.</source><year>2024</year><volume>26</volume><fpage>77</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1109/TMM.2023.3260635</pub-id></element-citation></ref><ref id="B15-sensors-25-01182"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Ji</surname><given-names>D.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Multi-task learning with auxiliary speaker identification for conversational emotion recognition</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2003.01478</pub-id></element-citation></ref><ref id="B16-sensors-25-01182"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>Q.</given-names></name>
<name><surname>Cao</surname><given-names>B.</given-names></name>
<name><surname>Guan</surname><given-names>X.</given-names></name>
<name><surname>Gu</surname><given-names>T.</given-names></name>
<name><surname>Bao</surname><given-names>X.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>B.</given-names></name>
<name><surname>Cao</surname><given-names>J.</given-names></name>
</person-group><article-title>Emotion recognition in conversations with emotion shift detection based on multi-task learning</article-title><source>Knowl.-Based Syst.</source><year>2022</year><volume>248</volume><fpage>108861</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2022.108861</pub-id></element-citation></ref><ref id="B17-sensors-25-01182"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>W.</given-names></name>
<name><surname>Yu</surname><given-names>J.</given-names></name>
<name><surname>Xia</surname><given-names>R.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
</person-group><article-title>A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations</article-title><source>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</source><conf-loc>Toronto, ON, Canada</conf-loc><conf-date>9&#x02013;14 July 2023</conf-date><fpage>15445</fpage><lpage>15459</lpage></element-citation></ref><ref id="B18-sensors-25-01182"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cao</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
</person-group><article-title>A study on gender differences in speech emotion recognition based on corpus</article-title><source>J. Nanjing Univ. (Nat. Sci.)</source><year>2019</year><volume>55</volume><fpage>758</fpage><lpage>764</lpage></element-citation></ref><ref id="B19-sensors-25-01182"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Lv</surname><given-names>G.</given-names></name>
<name><surname>Zeng</surname><given-names>Z.</given-names></name>
</person-group><article-title>GraphMFT: A graph network based multimodal fusion technique for emotion recognition in conversation</article-title><source>Neurocomputing</source><year>2023</year><volume>550</volume><fpage>126427</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2023.126427</pub-id></element-citation></ref><ref id="B20-sensors-25-01182"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gan</surname><given-names>C.</given-names></name>
<name><surname>Zheng</surname><given-names>J.</given-names></name>
<name><surname>Zhu</surname><given-names>Q.</given-names></name>
<name><surname>Jain</surname><given-names>D.K.</given-names></name>
<name><surname>&#x00160;truc</surname><given-names>V.</given-names></name>
</person-group><article-title>A graph neural network with context filtering and feature correction for conversational emotion recognition</article-title><source>Inf. Sci.</source><year>2024</year><volume>658</volume><fpage>120017</fpage><pub-id pub-id-type="doi">10.1016/j.ins.2023.120017</pub-id></element-citation></ref><ref id="B21-sensors-25-01182"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fu</surname><given-names>C.</given-names></name>
<name><surname>Su</surname><given-names>Y.</given-names></name>
<name><surname>Su</surname><given-names>K.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Shi</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>B.</given-names></name>
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Ishi</surname><given-names>C.T.</given-names></name>
<name><surname>Ishiguro</surname><given-names>H.</given-names></name>
</person-group><article-title>HAM-GNN: A hierarchical attention-based multi-dimensional edge graph neural network for dialogue act classification</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>261</volume><fpage>125459</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.125459</pub-id></element-citation></ref><ref id="B22-sensors-25-01182"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Caruana</surname><given-names>R.</given-names></name>
</person-group><article-title>Multitask Learning</article-title><source>Mach. Learn.</source><year>1997</year><volume>28</volume><fpage>41</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1023/A:1007379606734</pub-id></element-citation></ref><ref id="B23-sensors-25-01182"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ruder</surname><given-names>S.</given-names></name>
</person-group><article-title>An Overview of Multi-Task Learning in Deep Neural Networks</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1706.05098</pub-id></element-citation></ref><ref id="B24-sensors-25-01182"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xie</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>K.</given-names></name>
<name><surname>Sun</surname><given-names>C.J.</given-names></name>
<name><surname>Liu</surname><given-names>B.</given-names></name>
<name><surname>Ji</surname><given-names>Z.</given-names></name>
</person-group><article-title>Knowledge-interactive network with sentiment polarity intensity-aware multi-task learning for emotion recognition in conversations</article-title><source>Proceedings of the Findings of the Association for Computational Linguistics: EMNLP 2021</source><conf-loc>Punta Cana, Dominican Republic</conf-loc><conf-date>16&#x02013;20 November 2021</conf-date><fpage>2879</fpage><lpage>2889</lpage></element-citation></ref><ref id="B25-sensors-25-01182"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Wei</surname><given-names>D.</given-names></name>
<name><surname>Shao</surname><given-names>Y.</given-names></name>
</person-group><article-title>Dynamic weighted multitask learning and contrastive learning for multimodal sentiment analysis</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>2986</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12132986</pub-id></element-citation></ref><ref id="B26-sensors-25-01182"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xue</surname><given-names>P.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Liao</surname><given-names>J.</given-names></name>
<name><surname>Zheng</surname><given-names>J.</given-names></name>
<name><surname>Fu</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>D.</given-names></name>
</person-group><article-title>Sentiment classification method based on multitasking and multimodal interactive learning</article-title><source>Proceedings of the 22nd China National Conference on Computational Linguistics</source><conf-loc>Harbin, China</conf-loc><conf-date>3&#x02013;5 August 2023</conf-date><fpage>315</fpage><lpage>327</lpage></element-citation></ref><ref id="B27-sensors-25-01182"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ma</surname><given-names>Z.</given-names></name>
<name><surname>Jia</surname><given-names>W.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>B.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>Z.</given-names></name>
</person-group><article-title>Personality Enhanced Emotion Generation Modeling for Dialogue Systems</article-title><source>Cogn. Comput.</source><year>2024</year><volume>16</volume><fpage>293</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1007/s12559-023-10204-w</pub-id></element-citation></ref><ref id="B28-sensors-25-01182"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cai</surname><given-names>C.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Sun</surname><given-names>L.</given-names></name>
<name><surname>Lian</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>B.</given-names></name>
<name><surname>Tao</surname><given-names>J.</given-names></name>
<name><surname>Xu</surname><given-names>M.</given-names></name>
<name><surname>Wang</surname><given-names>K.</given-names></name>
</person-group><article-title>Multimodal sentiment analysis based on recurrent neural network and multimodal attention</article-title><source>Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge</source><conf-loc>Online</conf-loc><conf-date>24 October 2021</conf-date><fpage>61</fpage><lpage>67</lpage></element-citation></ref><ref id="B29-sensors-25-01182"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tian</surname><given-names>H.</given-names></name>
<name><surname>Gao</surname><given-names>C.</given-names></name>
<name><surname>Xiao</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>He</surname><given-names>B.</given-names></name>
<name><surname>Wu</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>F.</given-names></name>
</person-group><article-title>SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2005.05635</pub-id></element-citation></ref><ref id="B30-sensors-25-01182"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>F.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Geng</surname><given-names>B.</given-names></name>
</person-group><article-title>Deep Multimodal Data Fusion</article-title><source>ACM Comput. Surv.</source><year>2024</year><volume>56</volume><fpage>216</fpage><pub-id pub-id-type="doi">10.1145/3649447</pub-id></element-citation></ref><ref id="B31-sensors-25-01182"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zadeh</surname><given-names>A.</given-names></name>
<name><surname>Liang</surname><given-names>P.P.</given-names></name>
<name><surname>Poria</surname><given-names>S.</given-names></name>
<name><surname>Vij</surname><given-names>P.</given-names></name>
<name><surname>Cambria</surname><given-names>E.</given-names></name>
<name><surname>Morency</surname><given-names>L.P.</given-names></name>
</person-group><article-title>Multi-attention recurrent network for human communication comprehension</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>2&#x02013;7 February 2018</conf-date><volume>Volume 32</volume></element-citation></ref><ref id="B32-sensors-25-01182"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nagrani</surname><given-names>A.</given-names></name>
<name><surname>Yang</surname><given-names>S.</given-names></name>
<name><surname>Arnab</surname><given-names>A.</given-names></name>
<name><surname>Jansen</surname><given-names>A.</given-names></name>
<name><surname>Schmid</surname><given-names>C.</given-names></name>
<name><surname>Sun</surname><given-names>C.</given-names></name>
</person-group><article-title>Attention bottlenecks for multimodal fusion</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>14200</fpage><lpage>14213</lpage></element-citation></ref><ref id="B33-sensors-25-01182"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Zhou</surname><given-names>G.</given-names></name>
</person-group><article-title>Multimodal emotion recognition with auxiliary sentiment information</article-title><source>Beijing Da Xue Xue Bao</source><year>2020</year><volume>56</volume><fpage>75</fpage><lpage>81</lpage></element-citation></ref><ref id="B34-sensors-25-01182"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schuller</surname><given-names>B.</given-names></name>
<name><surname>Batliner</surname><given-names>A.</given-names></name>
<name><surname>Steidl</surname><given-names>S.</given-names></name>
<name><surname>Seppi</surname><given-names>D.</given-names></name>
</person-group><article-title>Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge</article-title><source>Speech Commun.</source><year>2011</year><volume>53</volume><fpage>1062</fpage><lpage>1087</lpage><pub-id pub-id-type="doi">10.1016/j.specom.2011.01.011</pub-id></element-citation></ref><ref id="B35-sensors-25-01182"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fu</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Ishi</surname><given-names>C.T.</given-names></name>
<name><surname>Yoshikawa</surname><given-names>Y.</given-names></name>
<name><surname>Iio</surname><given-names>T.</given-names></name>
<name><surname>Ishiguro</surname><given-names>H.</given-names></name>
</person-group><article-title>Using an android robot to improve social connectedness by sharing recent experiences of group members in human-robot conversations</article-title><source>IEEE Robot. Autom. Lett.</source><year>2021</year><volume>6</volume><fpage>6670</fpage><lpage>6677</lpage><pub-id pub-id-type="doi">10.1109/LRA.2021.3094779</pub-id></element-citation></ref><ref id="B36-sensors-25-01182"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>G.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Van Der Maaten</surname><given-names>L.</given-names></name>
<name><surname>Weinberger</surname><given-names>K.Q.</given-names></name>
</person-group><article-title>Densely connected convolutional networks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date><fpage>4700</fpage><lpage>4708</lpage></element-citation></ref><ref id="B37-sensors-25-01182"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chudasama</surname><given-names>V.</given-names></name>
<name><surname>Kar</surname><given-names>P.</given-names></name>
<name><surname>Gudmalwar</surname><given-names>A.</given-names></name>
<name><surname>Shah</surname><given-names>N.</given-names></name>
<name><surname>Wasnik</surname><given-names>P.</given-names></name>
<name><surname>Onoe</surname><given-names>N.</given-names></name>
</person-group><article-title>M2fnet: Multi-modal fusion network for emotion recognition in conversation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#x02013;24 June 2022</conf-date><fpage>4652</fpage><lpage>4661</lpage></element-citation></ref><ref id="B38-sensors-25-01182"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Deng</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>B.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
</person-group><article-title>Multimodal Sentiment Analysis Based on a Cross-Modal Multihead Attention Mechanism</article-title><source>Comput. Mater. Contin.</source><year>2024</year><volume>78</volume><fpage>1</fpage><pub-id pub-id-type="doi">10.32604/cmc.2023.042150</pub-id></element-citation></ref><ref id="B39-sensors-25-01182"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Morris</surname><given-names>C.</given-names></name>
<name><surname>Ritzert</surname><given-names>M.</given-names></name>
<name><surname>Fey</surname><given-names>M.</given-names></name>
<name><surname>Hamilton</surname><given-names>W.L.</given-names></name>
<name><surname>Lenssen</surname><given-names>J.E.</given-names></name>
<name><surname>Rattan</surname><given-names>G.</given-names></name>
<name><surname>Grohe</surname><given-names>M.</given-names></name>
</person-group><article-title>Weisfeiler and Leman go neural: Higher-order graph neural networks</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>29&#x02013;31 January 2019</conf-date><volume>Volume 33</volume><fpage>4602</fpage><lpage>4609</lpage></element-citation></ref><ref id="B40-sensors-25-01182"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Foggia</surname><given-names>P.</given-names></name>
<name><surname>Greco</surname><given-names>A.</given-names></name>
<name><surname>Saggese</surname><given-names>A.</given-names></name>
<name><surname>Vento</surname><given-names>M.</given-names></name>
</person-group><article-title>Multi-task learning on the edge for effective gender, age, ethnicity and emotion recognition</article-title><source>Eng. Appl. Artif. Intell.</source><year>2023</year><volume>118</volume><fpage>105651</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2022.105651</pub-id></element-citation></ref><ref id="B41-sensors-25-01182"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ciampiconi</surname><given-names>L.</given-names></name>
<name><surname>Elwood</surname><given-names>A.</given-names></name>
<name><surname>Leonardi</surname><given-names>M.</given-names></name>
<name><surname>M&#x02019;ohamed</surname><given-names>A.</given-names></name>
<name><surname>Rozza</surname><given-names>A.</given-names></name>
</person-group><article-title>A survey and taxonomy of loss functions in machine learning</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2301.05579</pub-id></element-citation></ref><ref id="B42-sensors-25-01182"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Busso</surname><given-names>C.</given-names></name>
<name><surname>Bulut</surname><given-names>M.</given-names></name>
<name><surname>Lee</surname><given-names>C.C.</given-names></name>
<name><surname>Kazemzadeh</surname><given-names>A.</given-names></name>
<name><surname>Mower</surname><given-names>E.</given-names></name>
<name><surname>Kim</surname><given-names>S.</given-names></name>
<name><surname>Chang</surname><given-names>J.N.</given-names></name>
<name><surname>Lee</surname><given-names>S.</given-names></name>
<name><surname>Narayanan</surname><given-names>S.S.</given-names></name>
</person-group><article-title>IEMOCAP: Interactive emotional dyadic motion capture database</article-title><source>Lang. Resour. Eval.</source><year>2008</year><volume>42</volume><fpage>335</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1007/s10579-008-9076-6</pub-id></element-citation></ref><ref id="B43-sensors-25-01182"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Poria</surname><given-names>S.</given-names></name>
<name><surname>Hazarika</surname><given-names>D.</given-names></name>
<name><surname>Majumder</surname><given-names>N.</given-names></name>
<name><surname>Naik</surname><given-names>G.</given-names></name>
<name><surname>Cambria</surname><given-names>E.</given-names></name>
<name><surname>Mihalcea</surname><given-names>R.</given-names></name>
</person-group><article-title>MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</article-title><source>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</source><conf-loc>Florence, Italy</conf-loc><conf-date>28 July&#x02013;2 August 2019</conf-date><publisher-name>Association for Computational Linguistics</publisher-name><publisher-loc>Florence, Italy</publisher-loc><year>2019</year><fpage>527</fpage><lpage>536</lpage></element-citation></ref><ref id="B44-sensors-25-01182"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>D.</given-names></name>
<name><surname>Wei</surname><given-names>L.</given-names></name>
<name><surname>Huai</surname><given-names>X.</given-names></name>
</person-group><article-title>DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2106.01978</pub-id></element-citation></ref><ref id="B45-sensors-25-01182"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dai</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Lu</surname><given-names>G.</given-names></name>
</person-group><article-title>Multi-modal graph context extraction and consensus-aware learning for emotion recognition in conversation</article-title><source>Knowl. Based Syst.</source><year>2024</year><volume>298</volume><fpage>111954</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2024.111954</pub-id></element-citation></ref><ref id="B46-sensors-25-01182"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fu</surname><given-names>C.</given-names></name>
<name><surname>Qian</surname><given-names>F.</given-names></name>
<name><surname>Su</surname><given-names>K.</given-names></name>
<name><surname>Su</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Shi</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Ishi</surname><given-names>C.T.</given-names></name>
</person-group><article-title>HiMul-LGG: A hierarchical decision fusion-based local&#x02013;global graph neural network for multimodal emotion recognition in conversation</article-title><source>Neural Netw.</source><year>2025</year><volume>181</volume><fpage>106764</fpage><pub-id pub-id-type="doi">10.1016/j.neunet.2024.106764</pub-id><pub-id pub-id-type="pmid">39368277</pub-id>
</element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01182-f001"><label>Figure 1</label><caption><p>Example of a multimodal dialog scene: The dialog is drawn from textual, audio, and visual modalities. The dialog demonstrates the complexity of communication and emotional relationships faced by the mother and the son when dealing with the loss of a loved one as an intra-familial issue, reflecting speaker-dependent and temporal-dependent relationships.</p></caption><graphic xlink:href="sensors-25-01182-g001" position="float"/></fig><fig position="float" id="sensors-25-01182-f002"><label>Figure 2</label><caption><p>The overall architecture of the proposed HGF-MiLaG, including context encoding, creation of hierarchical graph based on graph networks, auxiliary tasks gender information injection and emotion prediction.</p></caption><graphic xlink:href="sensors-25-01182-g002" position="float"/></fig><fig position="float" id="sensors-25-01182-f003"><label>Figure 3</label><caption><p>T-SNE visualization of the learned features using different methods on the IEMOCAP test set. In these figures, we use blue, green, red, purple, brown and pink to represent happy, sad, neutral, anger, excited and frustrated, respectively.</p></caption><graphic xlink:href="sensors-25-01182-g003" position="float"/></fig><fig position="float" id="sensors-25-01182-f004"><label>Figure 4</label><caption><p>T-SNE visualization of the learned features using different methods on the IEMOCAP test set. In these figures, we use blue, green, red, purple, brown and pink to represent happy, sad, neutral, anger, excited and frustrated, respectively.</p></caption><graphic xlink:href="sensors-25-01182-g004" position="float"/></fig><fig position="float" id="sensors-25-01182-f005"><label>Figure 5</label><caption><p>Normalized confusion matrix of HGF-MiLaG on IEMOCAP and MELD test sets. Rows indicate predicted labels and columns indicate true labels.</p></caption><graphic xlink:href="sensors-25-01182-g005" position="float"/></fig><fig position="float" id="sensors-25-01182-f006"><label>Figure 6</label><caption><p>Comparison of model predictions and true emotion label distributions on the MELD test set.</p></caption><graphic xlink:href="sensors-25-01182-g006" position="float"/></fig><table-wrap position="float" id="sensors-25-01182-t001"><object-id pub-id-type="pii">sensors-25-01182-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison with the baseline model: assessment metrics include Acc., F1 and Wa-F1, representing accuracy (%), F1 score (%) and weighted average F1 score (%), respectively. The best performance is indicated in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model</th><th colspan="8" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">IEMOCAP</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MELD</th></tr><tr><th align="center" valign="middle" rowspan="1" colspan="1">Happy</th><th align="center" valign="middle" rowspan="1" colspan="1">Sad</th><th align="center" valign="middle" rowspan="1" colspan="1">Neutral</th><th align="center" valign="middle" rowspan="1" colspan="1">Angry</th><th align="center" valign="middle" rowspan="1" colspan="1">Excited</th><th align="center" valign="middle" rowspan="1" colspan="1">Frustrated</th><th align="center" valign="middle" rowspan="1" colspan="1">Acc. </th><th align="center" valign="middle" rowspan="1" colspan="1">Wa-F1</th><th align="center" valign="middle" rowspan="1" colspan="1">Acc.</th><th align="center" valign="middle" rowspan="1" colspan="1">Wa-F1</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">DiagueGCN</td><td align="center" valign="middle" rowspan="1" colspan="1">41.28</td><td align="center" valign="middle" rowspan="1" colspan="1">82.52</td><td align="center" valign="middle" rowspan="1" colspan="1">64.33</td><td align="center" valign="middle" rowspan="1" colspan="1">65.18</td><td align="center" valign="middle" rowspan="1" colspan="1">73.18</td><td align="center" valign="middle" rowspan="1" colspan="1">64.46</td><td align="center" valign="middle" rowspan="1" colspan="1">66.85</td><td align="center" valign="middle" rowspan="1" colspan="1">66.78</td><td align="center" valign="middle" rowspan="1" colspan="1">59.58</td><td align="center" valign="middle" rowspan="1" colspan="1">58.17</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DiagueCRN</td><td align="center" valign="middle" rowspan="1" colspan="1">53.23</td><td align="center" valign="middle" rowspan="1" colspan="1">83.37</td><td align="center" valign="middle" rowspan="1" colspan="1">62.96</td><td align="center" valign="middle" rowspan="1" colspan="1">66.09</td><td align="center" valign="middle" rowspan="1" colspan="1">75.40</td><td align="center" valign="middle" rowspan="1" colspan="1">66.07</td><td align="center" valign="middle" rowspan="1" colspan="1">67.16</td><td align="center" valign="middle" rowspan="1" colspan="1">67.21</td><td align="center" valign="middle" rowspan="1" colspan="1">61.11</td><td align="center" valign="middle" rowspan="1" colspan="1">58.67</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MMGCN</td><td align="center" valign="middle" rowspan="1" colspan="1">37.61</td><td align="center" valign="middle" rowspan="1" colspan="1">79.84</td><td align="center" valign="middle" rowspan="1" colspan="1">62.26</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold> 74.29</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">75.00</td><td align="center" valign="middle" rowspan="1" colspan="1">63.68</td><td align="center" valign="middle" rowspan="1" colspan="1">67.28</td><td align="center" valign="middle" rowspan="1" colspan="1">66.67</td><td align="center" valign="middle" rowspan="1" colspan="1">61.07</td><td align="center" valign="middle" rowspan="1" colspan="1">57.33</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">COGMEN</td><td align="center" valign="middle" rowspan="1" colspan="1">51.90</td><td align="center" valign="middle" rowspan="1" colspan="1">81.70</td><td align="center" valign="middle" rowspan="1" colspan="1">68.60</td><td align="center" valign="middle" rowspan="1" colspan="1">66.00</td><td align="center" valign="middle" rowspan="1" colspan="1">75.30</td><td align="center" valign="middle" rowspan="1" colspan="1">58.20</td><td align="center" valign="middle" rowspan="1" colspan="1">68.20</td><td align="center" valign="middle" rowspan="1" colspan="1">67.60</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GraphCFC</td><td align="center" valign="middle" rowspan="1" colspan="1">43.08</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>84.99</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">64.70</td><td align="center" valign="middle" rowspan="1" colspan="1">71.35</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>78.86</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">63.70</td><td align="center" valign="middle" rowspan="1" colspan="1">69.13</td><td align="center" valign="middle" rowspan="1" colspan="1">68.91</td><td align="center" valign="middle" rowspan="1" colspan="1">61.42</td><td align="center" valign="middle" rowspan="1" colspan="1">58.86</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GA2MIF</td><td align="center" valign="middle" rowspan="1" colspan="1">46.15</td><td align="center" valign="middle" rowspan="1" colspan="1">84.50</td><td align="center" valign="middle" rowspan="1" colspan="1">68.38</td><td align="center" valign="middle" rowspan="1" colspan="1">70.29</td><td align="center" valign="middle" rowspan="1" colspan="1">75.99</td><td align="center" valign="middle" rowspan="1" colspan="1">66.49</td><td align="center" valign="middle" rowspan="1" colspan="1">69.75</td><td align="center" valign="middle" rowspan="1" colspan="1">70.00</td><td align="center" valign="middle" rowspan="1" colspan="1">61.65</td><td align="center" valign="middle" rowspan="1" colspan="1">58.94</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GraphMFT</td><td align="center" valign="middle" rowspan="1" colspan="1">45.99</td><td align="center" valign="middle" rowspan="1" colspan="1">83.12</td><td align="center" valign="middle" rowspan="1" colspan="1">63.08</td><td align="center" valign="middle" rowspan="1" colspan="1">70.30</td><td align="center" valign="middle" rowspan="1" colspan="1">76.92</td><td align="center" valign="middle" rowspan="1" colspan="1">63.84</td><td align="center" valign="middle" rowspan="1" colspan="1">67.90</td><td align="center" valign="middle" rowspan="1" colspan="1">68.07</td><td align="center" valign="middle" rowspan="1" colspan="1">61.30</td><td align="center" valign="middle" rowspan="1" colspan="1">58.37</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">GCCL</td><td align="center" valign="middle" rowspan="1" colspan="1">54.05</td><td align="center" valign="middle" rowspan="1" colspan="1">81.10</td><td align="center" valign="middle" rowspan="1" colspan="1">70.28</td><td align="center" valign="middle" rowspan="1" colspan="1">68.21</td><td align="center" valign="middle" rowspan="1" colspan="1">72.17</td><td align="center" valign="middle" rowspan="1" colspan="1">64.00</td><td align="center" valign="middle" rowspan="1" colspan="1">69.87</td><td align="center" valign="middle" rowspan="1" colspan="1">69.29</td><td align="center" valign="middle" rowspan="1" colspan="1">62.82</td><td align="center" valign="middle" rowspan="1" colspan="1">60.28</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">AVL COLD Fusion</td><td align="center" valign="middle" rowspan="1" colspan="1">43.70</td><td align="center" valign="middle" rowspan="1" colspan="1">60.20</td><td align="center" valign="middle" rowspan="1" colspan="1">48.90</td><td align="center" valign="middle" rowspan="1" colspan="1">58.40</td><td align="center" valign="middle" rowspan="1" colspan="1">61.60</td><td align="center" valign="middle" rowspan="1" colspan="1">57.90</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>82.70</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">55.10</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td><td align="center" valign="middle" rowspan="1" colspan="1">-</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HiMul-LGG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.95</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">79.92</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold> 71.66</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>68.46</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.12</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.22</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.18</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HGF-MiLaG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold> 59.16</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.94</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">68.60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.21</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>70.98</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold> 71.02</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold> 66.22</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>65.26</bold>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01182-t002"><object-id pub-id-type="pii">sensors-25-01182-t002_Table 2</object-id><label>Table 2</label><caption><p>Ablation experiments of HGF-MiLaG: assessment metrics include Acc. and Wa-F1. The best performance is indicated in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Method</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">IEMOCAP</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MELD</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wa-F1</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wa-F1</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">HGF-MiLaG</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>70.98</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>71.02</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold> 66.22</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold> 65.26</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">w/o MiLaG</td><td align="center" valign="middle" rowspan="1" colspan="1">69.87</td><td align="center" valign="middle" rowspan="1" colspan="1">70.01</td><td align="center" valign="middle" rowspan="1" colspan="1">65.91</td><td align="center" valign="middle" rowspan="1" colspan="1">64.91</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">w/o MG</td><td align="center" valign="middle" rowspan="1" colspan="1">65.43</td><td align="center" valign="middle" rowspan="1" colspan="1">65.45</td><td align="center" valign="middle" rowspan="1" colspan="1">65.64</td><td align="center" valign="middle" rowspan="1" colspan="1">64.89</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">w/o UG</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.01</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.09</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.03</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64.75</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01182-t003"><object-id pub-id-type="pii">sensors-25-01182-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of effects across different gender injection locations on the IEMOCAP dataset. The best performance is indicated in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Position</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">IEMOCAP</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Acc.
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
Wa-F1
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Early</td><td align="center" valign="middle" rowspan="1" colspan="1">69.50</td><td align="center" valign="middle" rowspan="1" colspan="1">69.69</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Middle</td><td align="center" valign="middle" rowspan="1" colspan="1">69.38</td><td align="center" valign="middle" rowspan="1" colspan="1">69.64</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Late</td><td align="center" valign="middle" rowspan="1" colspan="1">69.25</td><td align="center" valign="middle" rowspan="1" colspan="1">69.21</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mid-Late</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>70.98</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>71.02</bold>
</td></tr></tbody></table></table-wrap></floats-group></article>