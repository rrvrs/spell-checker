<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006339</article-id><article-id pub-id-type="pmc">PMC11859858</article-id><article-id pub-id-type="doi">10.3390/s25041111</article-id><article-id pub-id-type="publisher-id">sensors-25-01111</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Quality Evaluation for Colored Point Clouds Produced by Autonomous Vehicle Sensor Fusion Systems</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Schaefer</surname><given-names>Colin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01111" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Kootbally</surname><given-names>Zeid</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af2-sensors-25-01111" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4617-0892</contrib-id><name><surname>Nguyen</surname><given-names>Vinh</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-01111" ref-type="aff">1</xref><xref rid="c1-sensors-25-01111" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Aparicio-Navarro</surname><given-names>Francisco J.</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Adnane</surname><given-names>Asma</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Jadidbonab</surname><given-names>Hesam</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Shakarchi</surname><given-names>Ali Safaa Sadiq Al</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01111"><label>1</label>Department of Mechanical and Aerospace Engineering, Michigan Technological University, 1400 Townsend Drive, Houghton, MI 49931, USA; <email>colinsch@mtu.edu</email></aff><aff id="af2-sensors-25-01111"><label>2</label>Intelligent Systems Division, National Institute of Standards and Technology, 100 Bureau Drive, Gaithersburg, MD 20899, USA; <email>zeid.kootbally@nist.gov</email></aff><author-notes><corresp id="c1-sensors-25-01111"><label>*</label>Correspondence: <email>vinhn@mtu.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1111</elocation-id><history><date date-type="received"><day>29</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>28</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>11</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Perception systems for autonomous vehicles (AVs) require various types of sensors, including light detection and ranging (LiDAR) and cameras, to ensure their robustness in driving scenarios and weather conditions. The data from these sensors are fused together to generate maps of the surrounding environment and provide information for the detection and tracking of objects. Hence, evaluation methods are necessary to compare existing and future sensor systems through quantifiable measurements given the wide range of sensor models and design choices. This paper presents an evaluation method to compare colored point clouds, a common fused data type, among two LiDAR&#x02013;camera fusion systems and a stereo camera setup. The evaluation approach uses a test artifact measured by the fusion system&#x02019;s colored point cloud through the spread, area coverage, and color difference of the colored points within the computed space. The test results showed the evaluation approach was able to rank the sensor fusion systems based on its metrics and complement the experimental observations. The proposed evaluation methodology is, therefore, suitable towards the comparison of generated colored point clouds by sensor fusion systems.</p></abstract><kwd-group><kwd>LiDAR</kwd><kwd>camera</kwd><kwd>sensor fusion</kwd><kwd>colored point clouds</kwd><kwd>automated driving systems</kwd><kwd>metrology</kwd></kwd-group><funding-group><award-group><funding-source>National Institute of Standards and Technology (NIST)</funding-source><award-id>70NANB23H020</award-id></award-group><funding-statement>This research was funded by the National Institute of Standards and Technology (NIST) Grant 70NANB23H020.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01111"><title>1. Introduction</title><p>One of the core aspects that differentiate autonomous vehicles (AVs) from manually driven vehicles is their perception system. As AV development continues, it has become apparent that their perception system will need to be composed of multiple and distinct types of sensors to ensure AVs can perceive their surroundings for all possible scenarios and weather events [<xref rid="B1-sensors-25-01111" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01111" ref-type="bibr">2</xref>]. Such a sensor fusion system needs to gather data from the surrounding environment for complex decision-making [<xref rid="B3-sensors-25-01111" ref-type="bibr">3</xref>]. There are numerous approaches to a perception fusion system for an AV [<xref rid="B4-sensors-25-01111" ref-type="bibr">4</xref>] due to the plethora of design choices, including what types of perception sensors to use, how many to use, and their placement on the AV. Hence, there is a need to evaluate the performance of all these unique perception systems under standardized methods. Currently, no standardized test methods exist for evaluating AV sensor fusion systems.</p><p>Light detection and ranging (LiDAR) sensors and cameras are commonly used sensor fusion systems in AVs [<xref rid="B5-sensors-25-01111" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01111" ref-type="bibr">6</xref>]. However, LiDAR and cameras generate different data, resulting in difficulties of individually quantifying and comparing perception performance. In addition, LiDAR&#x02013;camera sensor fusion architecture can vary from system to system [<xref rid="B5-sensors-25-01111" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01111" ref-type="bibr">6</xref>]. However, they all share the primary purpose of classifying objects in the environment, whether for mapping or detection [<xref rid="B7-sensors-25-01111" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01111" ref-type="bibr">8</xref>]. Hence, the goal of a LiDAR&#x02013;camera fusion perception system is to provide data on the driving environment for automated driving functionality. Therefore, a standard evaluation method would need to test an AV&#x02019;s LiDAR&#x02013;camera sensor fusion ability to accurately observe the environment and address the nature of perception observations.</p><p>One fused data type in LiDAR&#x02013;camera sensor fusion is colored point clouds, which are used as a representation of objects and for capturing differences in appearance. Colored point clouds are the closest data type to how humans see their surroundings because they contain color and distance values of an environment. While test artifacts have been developed for camera images [<xref rid="B9-sensors-25-01111" ref-type="bibr">9</xref>] and stereo camera setups [<xref rid="B10-sensors-25-01111" ref-type="bibr">10</xref>], the assessment of colored point clouds is still an active area of study. Point Cloud Quality Assessment (PCQA) is an evaluation technique to measure the distortion an object within a point cloud. PCQA can be implemented through a subjective or objective approach. Subjective PCQA involves using human subjects to grade point cloud objects based on their discretion, while objective PCQA uses computations and calculated metrics to compare point clouds of the same object [<xref rid="B11-sensors-25-01111" ref-type="bibr">11</xref>]. Objective PCQA is further categorized by the presences of a reference model of the object. Objective PCQA techniques can either compare a distorted model to a highly dense reference model and measure the differences [<xref rid="B12-sensors-25-01111" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01111" ref-type="bibr">13</xref>], use some features of a reference model for comparison [<xref rid="B14-sensors-25-01111" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01111" ref-type="bibr">15</xref>], or use no reference model and develop a different method to measure the distortion [<xref rid="B16-sensors-25-01111" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01111" ref-type="bibr">17</xref>]. These techniques are known as full-reference, reduced-reference, and no-reference methods, respectively. There also exist prior methods for full-reference PCQA using public datasets [<xref rid="B12-sensors-25-01111" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01111" ref-type="bibr">13</xref>]. For instance, test objects have been integrated into public datasets for the evaluation of point clouds [<xref rid="B18-sensors-25-01111" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01111" ref-type="bibr">19</xref>]. However, such an approach is expensive and no standard exists to ensure the point cloud data are consistently accurate between research groups. Though no-reference methods have been proposed, most of the methods used machine learning techniques [<xref rid="B20-sensors-25-01111" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01111" ref-type="bibr">21</xref>], which can be computationally expensive and subjective to the training data. Recent methods also involve injecting features within point clouds [<xref rid="B16-sensors-25-01111" ref-type="bibr">16</xref>] or using private point cloud data to prove validity [<xref rid="B17-sensors-25-01111" ref-type="bibr">17</xref>]. Unfortunately, most PCQA methods evaluate the distortion in point clouds using the entire point cloud representation of an object. Nevertheless, objective PCQA is seen as more suited for the evaluation of point clouds because subjective PCQA has been shown to lead to variable scores based on the subject&#x02019;s preference [<xref rid="B22-sensors-25-01111" ref-type="bibr">22</xref>].</p><p>The flaws of subjective PCQA can be alleviated through applying standardized image quality assessment test methods [<xref rid="B23-sensors-25-01111" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-01111" ref-type="bibr">24</xref>]. For instance, camera standards can be leveraged to consistently determine image quality by providing observers with three images at a time to evaluate their quality with characteristics including color difference and sharpness [<xref rid="B25-sensors-25-01111" ref-type="bibr">25</xref>]. In addition, LiDAR standards have been implicitly covered in test methods and documentation guidelines for evaluating perception stacks of automated driving capabilities [<xref rid="B26-sensors-25-01111" ref-type="bibr">26</xref>]. While these standards include detectability, which is inferred from spread and area coverage, these test methods do not specifically quantify LiDAR. Furthermore, these standard test methods do not apply to colored point clouds.</p><p>The research presented in this paper aims to fill the gap in the literature with respect to a repeatable method to evaluate colored point cloud data from different AV sensor fusion perception systems using a novel application of metrics (spread, area coverage, and color difference) and test artifacts. The proposed evaluation method uses a test artifact developed by the authors to evaluate multiple aspects of sensor fusion systems. The research approach in this paper is as follows: First, data were acquired by performing a scan test at varying distances from the different sensor fusion systems simultaneously. The resulting data were reviewed to find clear distinctions between the colored point clouds from the different fusion systems. These observations served as benchmarks for the evaluation method to recreate using quantitative metrics. The calculations and the metrics used within the proposed method were refined until they produced desirable results. <xref rid="sensors-25-01111-f001" ref-type="fig">Figure 1</xref> shows an overview of the research approach used to develop the proposed metrics. Note that the main goal of this research is the evaluation process and not to determine which of the presented sensor fusion systems is best for an AV application.</p></sec><sec sec-type="methods" id="sec2-sensors-25-01111"><title>2. Methodology</title><p>This section provides a description of the general experimental setup. First, the setup for test artifact scan tests is described along with the sensor hardware and the sensor fusion. Afterwards, the metrics used within the proposed evaluation method are presented.</p><sec id="sec2dot1-sensors-25-01111"><title>2.1. Experimental Setup</title><p>For these static object tests, a Velodyne VLP-16 Puck (Velodyne, San Jose, CA, USA) [<xref rid="B27-sensors-25-01111" ref-type="bibr">27</xref>] was used for the LiDAR, and an Intel D435i Realsense Depth Camera (Intel, Santa Clara, CA, USA [<xref rid="B28-sensors-25-01111" ref-type="bibr">28</xref>] was used to obtain two-dimensional (2D) images of the test environment for the LiDAR&#x02013;camera fusion systems. The Realsense camera produces a colored point cloud created by its internal stereo camera system in addition to the 2D image output. To obtain the raw data from these sensors, the respective open-source Robotic Operating System (ROS) drivers for each of the sensors were used. For data collection, the ROS drivers ran simultaneously, and the sensors were fixed to a mount that placed the LiDAR 0.114 m above the camera. To develop the proposed evaluation method, three separate sensor fusion systems were used to compare their colored point cloud representation of the test artifact. These three fusion systems were the Matrix Laboratory from MATLAB 2023a (MathWorks, Natick, MA, USA) fusion function, a publicly available sensor fusion ROS driver (lidar-camera-fusion 11.1.1, University of Alicante, Alicante, Spain) [<xref rid="B29-sensors-25-01111" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-01111" ref-type="bibr">30</xref>], and the output of the stereo camera setup within the Realsense camera. The MATLAB function used was the &#x0201c;fuseCameraToLidar()&#x0201d; function, which requires a point cloud, a 2D image, the camera&#x02019;s intrinsic parameters, and the extrinsic transformation matrix between the LiDAR and camera. These same arguments were needed for the LiDAR&#x02013;camera fusion ROS drivers. The difference between these fusion techniques is the MATLAB fusion function was performed after the data were collected as compared to the live data fusion from the ROS drivers. In addition, the MATLAB fusion function only overlays the image onto the point cloud giving color values to each point, while the ROS drivers interpolates new points between the LiDAR&#x02019;s laser scan layers in addition to giving color values [<xref rid="B29-sensors-25-01111" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-01111" ref-type="bibr">30</xref>]. The extrinsic transformation matrix between the LiDAR and camera was calibrated using the Lidar-Camera Calibration MATLAB 2023a app. While these two sensor fusion methods used a 3D point cloud and a 2D image, the Realsense stereo camera setup uses two identical cameras to measure depth. An optional infrared projector exists within the Realsense unit that can improve the stereo camera&#x02019;s depth accuracy, but this light source was not used during testing. In this work, each sensor fusion system is given a short-hand reference: the MATLAB 2023a fusion function, ROS drivers, and the Realsense stereo camera are referred to as sensor fusions &#x0201c;Without Interpolation&#x0201d;, &#x0201c;With Interpolation&#x0201d;, and using a &#x0201c;Stereoscopic&#x0201d; design, respectively. (Certain equipment, instruments, software, or materials are identified in this paper in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement of any product or service by the National Institute of Standards and Technology, nor is it intended to imply that the materials or equipment identified are necessarily the best available for the purpose).</p><p>The testing for data collection involved a static artifact placed a distance away from the sensor mount. The artifact used for testing consisted of two sides: a detectable and undetectable side, which were composed of two plates to be either &#x0201c;detectable&#x0201d; or &#x0201c;undetectable&#x0201d; to a LiDAR when the corresponding side is in view. Hence, the perception sensor&#x02019;s ability to detect the artifact is hampered or enhanced by the physical design of the side of the artifact. For the detectable side, the artifact exhibits a white color for higher reflection intensity for LiDARs, a rough texture to ensure light is diffused equally in all directions, and a concave shape to direct the reflected light back towards its source. The undetectable side has a black color for low reflection intensity for LiDARs, a smooth surface finish to better reflect the emitted light in one direction, and a convex shape to ensure the reflected light never returns to the sensors. These two distinct sides can be used to evaluate a sensor fusion system between LiDARs and cameras to evaluate their perception capabilities under ideal and difficult conditions. Using a single artifact ensures ease of manufacturing and simple implementation for a test to evaluate the sensor fusion systems under the same conditions. The detectable side flat plates had dimensions of 0.61 m by 0.21 m arranged in a 45&#x000b0; 0.01 cm concave shape towards the sensors. The undetectable side had 0.61 m by 0.30 m plates arranged in a 60&#x000b0; 0.01 cm convex shape towards the perception sensors. Colored point cloud data were collected for both sides at 1 m, 2 m, 3 m, 4 m, and 5 m away from the sensor mount&#x02019;s center. <xref rid="sensors-25-01111-f002" ref-type="fig">Figure 2</xref> provides a visual display of the endpoints of these distances and an example of one of these scan tests. All the tests presented were performed in one location: a general lab space with dimensions of approximately 10.72 m &#x000d7; 8.99 m &#x000d7; 2.74 m. Data shown in this work were collected at night with the overhead lights in the lab space acting as the only source of visible light.</p></sec><sec id="sec2dot2-sensors-25-01111"><title>2.2. Evaluation Method and Metrics</title><p>The calculations performed within the proposed evaluation method are discussed in this section. First, an explanation of how the points representing the artifact were separated from the rest of the environment is presented. This section discusses the process of splitting the artifact point cloud into a left and right side for ease of analysis and obtaining their reference planes. The point cloud coverage analysis is then introduced. Next, the spread of the point cloud around the expected location of the test artifact&#x02019;s side is discussed. Finally, the color difference between the artifact points and the reference or expected color of the artifact is presented.</p><sec id="sec2dot2dot1-sensors-25-01111"><title>2.2.1. Preliminary Evaluation Steps</title><p>The points representing the test artifact must first be isolated and stored in a separate dataset from the rest of the environment. After data collection, the artifact point cloud was split into two separate sets of points corresponding to the left side and the right side. From these two point cloud sets, a reference plane was created to estimate where the two halves of the visible side of the test artifact are located within the colored point cloud.</p><p>The artifact points within the data from the fusion system without interpolation were isolated from the rest of the environment. The following convention was used when isolating the artifact points in the fused data with interpolation fusion: points that were above the ground and were between the sensors and the artifact were included in the artifact set, while single-line trailing points emitting from the back of the artifact were excluded. <xref rid="sensors-25-01111-f003" ref-type="fig">Figure 3</xref> provides visual examples of this convention. The 1 m and 2 m scan tests of the stereoscopic data produced clear artifact point sets with the only encountered obstacle separating the ground points from the artifact ones. The remaining tests did not contain a distinguishable artifact. These issues were expected because the maximum range listed within the Realsense camera&#x02019;s manual is 3 m. For these tests, the artifact sets were cropped by viewing the full point cloud from the camera&#x02019;s position and creating a 2D bounding box covering the estimated location of the visible artifact&#x02019;s side. A visual demonstration of this process is shown in <xref rid="sensors-25-01111-f004" ref-type="fig">Figure 4</xref>. These cropped point clouds were analyzed, and their performance was compared to the other fusion system for completeness.</p><p>Before continuing with the evaluation, the ground plane for each sensor fusion system was established for future calculations. A single, random point cloud result was taken from the test set for each of the fusion systems. Within these point clouds, the ground points were isolated from the rest of the environment. The ground plane was estimated from these points and its parameters were stored for evaluation.</p><p>Once the artifact point cloud sets were established, each set needed to be split into two to represent the left and right plates that compose one of the test artifact&#x02019;s sides. A splitting plane formation process was created to ensure consistency throughout the analysis. Three points were calculated from the artifact point cloud to define this plane: the centroid of the artifact points was determined by calculating the average location value, the normal point which is a point that is in line with the centroid point calculated by using a directional vector equal ground plane&#x02019;s normal vector and is located near the ground plane, and the centroid point (referred to as the vertex point) of a bounding box encompassing the vertex of the artifact. The splitting plane and its point-normal equation were derived from these three points. Once the plane was formed, the coordinates for each artifact point were inserted into the splitting plane&#x02019;s point-normal equation. The sign of the result would indicate on which side of the splitting plane the points were located. <xref rid="sensors-25-01111-f005" ref-type="fig">Figure 5</xref> illustrates this splitting process.</p><p>With the two halves of the visible artifact&#x02019;s side separated, the reference plane for each was determined. Similar to creating the splitting plane, three points were found to form each reference plane. To ensure that the reference planes intersected at their vertex, both reference planes shared the vertex of the splitting plane. The second point was calculated from finding the average location of a subset of points in the center of the two sides. The final point was calculated by using this middle point and the normal vector of the ground plane to find a point that lies near the ground plane. These points controlled the angle between their corresponding reference plane and the other reference plane. Also, these points ensured that the reference planes were normal to the ground plane. With these two perpendicular lines and the shared vertex point, the reference planes and their point-normal equations were derived and used in the point coverage and spread metrics. <xref rid="sensors-25-01111-f006" ref-type="fig">Figure 6</xref> demonstrates the reference plane creation process. The process for determining this middle point worked well with clear artifact point clouds. However, the process was difficult to implement with the noisy stereoscopic data. Through the rigorous iteration of the subset limits for the middle points, usable reference planes were produced for the noisy artifact point clouds.</p></sec><sec id="sec2dot2dot2-sensors-25-01111"><title>2.2.2. Point Coverage</title><p>With the reference planes, the artifact points of both sides can be projected onto their respective reference plane to obtain a 2D representation of the artifact point cloud. The following were used to calculate the coordinates of each point relative to the reference plane&#x02019;s coordinate system (<inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>x</mml:mi><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>y</mml:mi><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>) in their corresponding point cloud reference frame:<disp-formula id="FD1-sensors-25-01111"><label>(1)</label><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>x</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-01111"><label>(2)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>y</mml:mi><mml:mi>R</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the <italic toggle="yes">x</italic>-axis of the reference plane, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the reference plane&#x02019;s <italic toggle="yes">y</italic>-axis, which is equal to the ground plane&#x02019;s normal vector, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the origin point of the reference plane&#x02019;s coordinate system, and <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is an artifact point&#x02019;s location relative to the full point cloud. Note that <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was determined by finding the vector that was perpendicular to its respective reference plane&#x02019;s <italic toggle="yes">y</italic>-axis vector and normal vector (<italic toggle="yes">z</italic>-axis). The middle normal point used in creating the reference plane acted as the origin for its corresponding reference plane because the point was always located below the artifact points.</p><p>The artifact points within the reference planes&#x02019; coordinate systems allowed for easier analysis of these points to investigate how well they covered the expected area of the test artifact. Finding the point coverage area that was within the expected test artifact area relative to the reference planes determined the quality of the point cloud coverage for each test. The total point coverage of the point cloud relative to their respective reference plane was first found. A neighborhood set for each artifact point was determined by using the Radius Nearest Neighbors method with increasing radius values of 0.01 m, 0.03 m, 0.05 m, 0.07 m, and 0.10 m corresponding to the data with test distances of 1 m, 2 m, 3 m, 4 m, and 5 m. With each neighborhood set, a boundary was placed around the points using the &#x0201c;boundary()&#x0201d; MATLAB function. A shrink value of 0.01 was implemented to ensure the neighborhood sets had convex shapes, to cover the most area, and to output a cleaner appearance. With these boundary sets, a polygon (&#x0201c;polyshape()&#x0201d; object in MATLAB) could be created by using the boundary points as its vertices. Removing the overlapping area between the entire set of polygons results in the total point coverage area of the artifact points with respect to the reference planes. An expression for the total point cloud coverage of the artifact set (<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mo>&#x003a3;</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>) is given in the following:<disp-formula id="FD3-sensors-25-01111"><label>(3)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mo>&#x003a3;</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>&#x0222a;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the area of a neighborhood set of one of the artifact points and <italic toggle="yes">n</italic> is the number of points within the artifact set. Creating the expected artifact area in the reference planes was performed as follows. First, the bottom left or right corner of the artifact&#x02019;s side was projected onto the reference plane. This point was determined by first finding intersection lines between the two reference planes and both intersection lines between the reference planes and the ground plane. With these three lines, their intersection point was calculated and projected onto the reference planes as the bottom corner of the visible test artifact&#x02019;s sides. Using the physical dimensions of the test artifact, the area of each half was overlaid on the reference planes with the projected artifact points&#x02019; total point coverage shape. The intersection between these two shapes provides the point coverage of the expected area for the artifact points, while the leftover area of the total coverage shape represents the error or the incorrect point coverage that lies outside the expected area. The following equation represents the point coverage of the expected area (<italic toggle="yes">P</italic>):<disp-formula id="FD4-sensors-25-01111"><label>(4)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mo>&#x003a3;</mml:mo></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the estimated area of coverage of the artifact in the reference plane. <xref rid="sensors-25-01111-f007" ref-type="fig">Figure 7</xref> provides a sequence of plots visually describing the process of obtaining the point coverage of the projected points. The point coverage of the expected area expressed in a percentage value (<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mo>%</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>) was found by dividing the intersection area by the surface area of the test artifact&#x02019;s side, as shown in the following equation:<disp-formula id="FD5-sensors-25-01111"><label>(5)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mo>%</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>P</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The other percentage term is the point coverage error (<inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>&#x003f5;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) calculated as:<disp-formula id="FD6-sensors-25-01111"><label>(6)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>&#x003f5;</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>P</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mo>&#x003a3;</mml:mo></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot2dot3-sensors-25-01111"><title>2.2.3. Spread</title><p>The equation shown below was used to calculate the shortest distance (<inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) between an artifact point and the reference plane.<disp-formula id="FD7-sensors-25-01111"><label>(7)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the normal vector of the reference plane. These distance values help describe the spread of the artifact points relative to one of their corresponding reference planes. Once this distance value was determined for each point, the Root-Mean-Square (RMS) variable was calculated for each half.</p></sec><sec id="sec2dot2dot4-sensors-25-01111"><title>2.2.4. Color</title><p>The color metric in this paper evaluates the red&#x02013;green&#x02013;blue (RGB) value of the points disregarding the reference planes of the artifact. The RGB value of each point is compared to a reference RGB value (<italic toggle="yes">r</italic>) using the color difference (<italic toggle="yes">CR</italic>) variable as follows:<disp-formula id="FD8-sensors-25-01111"><label>(8)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the measured colors and reference colors, respectively, on the red, green, and blue scales. An average for the full artifact set was then calculated. Reference RGB values of (146, 145, 143) and (189, 188, 186) were visually determined for the left and right detectable sides. These were calculated by finding the average RGB value of 10 randomly selected pixels from an image of the test setup. It was assumed the undetectable side was pure black color, which corresponds to a reference value of (0, 0, 0) for both sides.</p></sec></sec></sec><sec sec-type="results" id="sec3-sensors-25-01111"><title>3. Results</title><p>The results for each of the evaluation aspects are presented and discussed to determine if the quantitative results follow the reported observations. For each metric, the observations are discussed first followed by the results produced by the evaluation method. The corresponding analysis for the point cloud coverage, spread, and the color difference of the points are discussed. <xref rid="sensors-25-01111-f008" ref-type="fig">Figure 8</xref> provides exemplary data of the point clouds.</p><sec id="sec3dot1-sensors-25-01111"><title>3.1. Point Cloud Coverage</title><p>To summarize the observations relating to point cloud coverage, coverage was the best at close distances and at tests with the detectable side. Generally, as distance increased, the greater the error and the less the coverage of the expected area. Also, for most of the tests, there was a greater amount of points for the detectable tests as compared with the undetectable side. With fewer points, the undetectable side had less coverage than the detectable tests. The stereoscopic data consistently had the most coverage because they had the largest amount of points, while the interpolation fusion system had the next best amount of coverage. As expected, the fusion data without interpolation had the least coverage area because they always had the least amount of points. These relationships can clearly seen within <xref rid="sensors-25-01111-f008" ref-type="fig">Figure 8</xref>.</p><fig position="anchor" id="sensors-25-01111-f008"><label>Figure 8</label><caption><p>The raw point cloud of each testing configuration at the test distance of 1 m.</p></caption><graphic xlink:href="sensors-25-01111-g008" position="float"/></fig><p><xref rid="sensors-25-01111-f009" ref-type="fig">Figure 9</xref> provides the results for point coverage of the expected area while <xref rid="sensors-25-01111-f010" ref-type="fig">Figure 10</xref> contains the error in coverage plots. The point coverage results are similar to the observations: the stereoscopic data with the detectable side at 1 m were the best overall, without interpolation, fusion points covered very little of the area, and all configurations generally decreased with increasing testing distance. In addition, for most of the tests, the undetectable side performed worse than the detectable side tests. The increase in point coverage at 2 m for undetectable was expected because the LiDAR sensors performed best at that test distance when scanning the artifact&#x02019;s undetectable side. This phenomenon was encountered during the development of the artifact&#x02019;s design. Investigating the point coverage error plots also generally followed the prior observations. As the test distance increased, the coverage error increased for all tests. The errors for undetectable side tests were mostly greater than their counterparts. Also, the error results illustrate that despite having little point coverage, the fusion without interpolation for the detectable side was relatively error-free. An unanticipated behavior within the evaluation results was that some of the right side error values were far larger than the left side values. This difference is most likely caused by the subjective nature of the artifact isolation process and the imperfect estimation of determining where the artifact&#x02019;s expected area region lies relative to the artifact&#x02019;s point set.</p></sec><sec id="sec3dot2-sensors-25-01111"><title>3.2. Spread</title><p>The artifact points closely clustered around the reference planes for 1 m and 2 m for all fusion techniques. However, as the test distance increased, the distance between the artifact points and the reference planes increased, with some points reaching almost a meter from where the test artifact&#x02019;s expected location. This increase of spread was only present in the interpolation and stereoscopic data, while the artifact points from the fusion without interpolation were consistently clustered near the reference planes. <xref rid="sensors-25-01111-f008" ref-type="fig">Figure 8</xref> provides little insight in this point spread behavior. However, <xref rid="sensors-25-01111-f003" ref-type="fig">Figure 3</xref> and <xref rid="sensors-25-01111-f004" ref-type="fig">Figure 4</xref> imply how the point spread increased with the test distance because the point clouds presented in both figures were included in the full dataset for the evaluation method.</p><fig position="anchor" id="sensors-25-01111-f009"><label>Figure 9</label><caption><p>The point coverage results of the test represented as the percentage of the artifact&#x02019;s expected area the point cloud covers within its enclosed area.</p></caption><graphic xlink:href="sensors-25-01111-g009" position="float"/></fig><fig position="anchor" id="sensors-25-01111-f010"><label>Figure 10</label><caption><p>The error in point coverage for each test represented as the percentage of the total point coverage located outside the expected area region of the test artifact.</p></caption><graphic xlink:href="sensors-25-01111-g010" position="float"/></fig><p>The results shown in <xref rid="sensors-25-01111-f011" ref-type="fig">Figure 11</xref> match the general observations: the without interpolation fusion points consistently ranked the highest with the shortest RMS values as compared to the other fusion methods. As with the other two fusion methods, as the test distance increased, the point spread increased their artifact sets. Interestingly, the detectable side ranked the lowest as compared to their undetectable counterpart. This behavior could be attributed to subjective error from the isolation process of the artifact points. It was easier to isolate the light-colored detectable side points than the dark-colored points of the undetectable side because of the better visibility. Also, as seen during the development of the test artifact, there were fewer points in the undetectable artifact point clouds as compared to the detectable side point clouds. The points further away from the reference planes were most likely not registered in the raw-colored point clouds because of their low-intensity backscattering values.</p><p>In addition, a comparative analysis was conducted with prior literature metrics for point cloud analysis. Specifically, the local point density (LPD) was computed [<xref rid="B31-sensors-25-01111" ref-type="bibr">31</xref>]. The LPD refers to the number of points surrounding a query point in a given volume. Hence, the LPD was calculated for every point in the point cloud and then the average of the LPD values was determined. The radius of the spherical volume surrounding each point was 0.05 m. <xref rid="sensors-25-01111-f012" ref-type="fig">Figure 12</xref> shows the results of the average LPD values. As expected, as the distances from the artifact increased, the LPD decreased. However, unlike the RMS values, the LPD is intrinsic and does not reference an artifact. Hence, the results in <xref rid="sensors-25-01111-f011" ref-type="fig">Figure 11</xref> are more intuitive towards determining accuracy than the LPD since it quantifies the spacing of the point cloud to the reference rather than the spacing between points within the point cloud.</p></sec><sec id="sec3dot3-sensors-25-01111"><title>3.3. Color</title><p>The main trends noticed were that both the fusion systems with and without interpolation had significant color error close to the sensors as shown in <xref rid="sensors-25-01111-f008" ref-type="fig">Figure 8</xref>, but this error decreased as the distance increased. These inaccuracies were most likely caused by the calibration error within the LiDAR and camera fusion system. The stereoscopic data generally performed better in representing the artifact&#x02019;s color because of the use of a stereo camera configuration and the process of isolating the artifact points benefited the system greatly. Within <xref rid="sensors-25-01111-f008" ref-type="fig">Figure 8</xref>, the stereoscopic data had little color variation as compared to the other fusion systems.</p><p><xref rid="sensors-25-01111-f013" ref-type="fig">Figure 13</xref> matches with some of the expected behavior found in the observations. Both the fusion with and without interpolation performed worse at close distances than the stereoscopic data for the left side, but as the test distances increased, all the data seemed to converge to the same range of values for both the left and right sides. This behavior is valid because as the distance increased, the color within the pixels had less variation since the test artifact was smaller relative to the camera. Despite capturing the LiDAR&#x02013;camera calibration error, the color difference results did not capture all the observations related to the color quality of the point clouds.</p><p>In addition, a comparative analysis of the average color distance with the traditional peak signal-to-noise ratio (PSNR) for image quality was conducted [<xref rid="B32-sensors-25-01111" ref-type="bibr">32</xref>]. The PSNR is expressed as the ratio between the maximum possible value (power) of the reference image and the power of corrupting noise, which is expressed as the mean square error in the color difference. Hence, the PSNR is expressed as the following equation:<disp-formula id="FD9-sensors-25-01111"><label>(9)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>10</mml:mn></mml:msub><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi></mml:mrow><mml:mi>I</mml:mi></mml:msub><mml:msqrt><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the mean squared error of the color difference and <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>X</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the max intensity of the reference value. <xref rid="sensors-25-01111-f014" ref-type="fig">Figure 14</xref> shows the results of the PSNR analysis. As expected, <xref rid="sensors-25-01111-f014" ref-type="fig">Figure 14</xref> exhibits the inverse behavior of <xref rid="sensors-25-01111-f013" ref-type="fig">Figure 13</xref>. This is because the noise increased as the color difference increased, which resulted in a lower PSNR value. However, note that the theoretical peak image value of the PSNR for the undetectable side was (0, 0, 0). Therefore, the PSNR could not be calculated for the undetectable side. Hence, the color difference metric seems more sufficient for testing with the reference artifact used in this work.</p><p>To summarize the results, the stereoscopic fusion system had the best performance of all the results at 1 m with the detectable side. However, the error and spread of the points greatly increased with the test distance. The fusion system without interpolation had the lowest point coverage and generally had the most color variation, but the error in the point coverage and its spread were small as compared to the other two fusion systems. The fusion system with interpolation had the least extreme results since it did not have the worst or best values for each evaluation metric, regardless of the test parameters. Note that the methods in this work are interlinked with the proposed test artifact. Automated driving environments encounter a large variety of obstacles including pedestrians, signage, and foliage. To apply this research towards objects detected in the driving environment, this work would have to be either adapted or extrapolated to such objects. With respect to adapting this work for objects in the driving environment, a representative 3D model of the objects should be constructed. Equations (<xref rid="FD1-sensors-25-01111" ref-type="disp-formula">1</xref>)&#x02013;(<xref rid="FD8-sensors-25-01111" ref-type="disp-formula">8</xref>) can then be used based on the 3D model as a reference to compute the point cloud coverage, the spread, and the color difference. Note that to compute the color difference, the 3D model should also have reference color values that can be acquired using a colorimeter. Alternatively, another approach would be to conduct these tests using this test artifact, and then extrapolate the results to a driving environment object. For instance, the differences in the geometry and color between the test artifact in this work and the driving environment object would be identified and examined to approximate the LiDAR&#x02013;camera sensor fusion performance for the driving environment object.</p></sec></sec><sec sec-type="conclusions" id="sec4-sensors-25-01111"><title>4. Conclusions and Future Work</title><p>The proposed evaluation method in this work can be applied to colored point clouds produced by AV sensor fusion perception systems. The results from this method lead to similar conclusions as observations of the colored point cloud data. Due to the simplicity of the evaluation metrics, they are effective in analyzing the colored point clouds and provide quantifiable measurements to determine the strengths and weaknesses in the perception accuracy of sensor fusion systems. Point cloud coverage metrics distinguish which colored point cloud accurately represents the artifact shape in the point cloud space. The spread value provides an insight into the spacing of points relative to the test artifact&#x02019;s location compared to the LPD, which intrinsically determines a point cloud&#x02019;s spacing. Finally, averaging the color difference value of the point cloud helps determine whether the 2D image pixels are calibrated to the artifact, while the PSNR is insufficient for reference artifacts that are a black color. The next steps for this evaluation method are to improve the processes implemented, especially refining the preliminary steps to ensure consistency between the sensor fusion systems. These steps are the most subjective and are prone to cause errors within the analysis. In addition, developing a more sophisticated approach, such as with machine learning, to evaluate the point&#x02019;s color can be conducted because this metric is the weakest of the three in capturing the accuracy of the point cloud. Also, future work may involve testing other sensor fusion systems using this method to ensure consistency in results and confirm that the method is applicable to other sensors.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to thank Omar Aboul-Enein and Ninad Harishchandrakar for the guidance they provided to the project.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, C.S., Z.K. and V.N.; methodology, C.S.; software, C.S.; validation, C.S.; formal analysis, C.S.; investigation, C.S., Z.K. and V.N.; resources, Z.K. and V.N.; data curation, C.S. and V.N.; writing&#x02014;original draft preparation, C.S.; writing&#x02014;review and editing, Z.K. and V.N.; visualization, C.S.; supervision, Z.K. and V.N.; project administration, Z.K. and V.N.; funding acquisition, V.N. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01111"><label>1.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fremont</surname><given-names>D.J.</given-names></name>
<name><surname>Kim</surname><given-names>E.</given-names></name>
<name><surname>Pant</surname><given-names>Y.V.</given-names></name>
<name><surname>Seshia</surname><given-names>S.A.</given-names></name>
<name><surname>Acharya</surname><given-names>A.</given-names></name>
<name><surname>Bruso</surname><given-names>X.</given-names></name>
<name><surname>Wells</surname><given-names>P.</given-names></name>
<name><surname>Lemke</surname><given-names>S.</given-names></name>
<name><surname>Lu</surname><given-names>Q.</given-names></name>
<name><surname>Mehta</surname><given-names>S.</given-names></name>
</person-group><article-title>Formal Scenario-Based Testing of Autonomous Vehicles: From Simulation to the Real World</article-title><source>Proceedings of the 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Rhodes, Greece</conf-loc><conf-date>20&#x02013;23 September 2020</conf-date><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/ITSC45102.2020.9294368</pub-id></element-citation></ref><ref id="B2-sensors-25-01111"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vargas</surname><given-names>J.</given-names></name>
<name><surname>Alsweiss</surname><given-names>S.</given-names></name>
<name><surname>Toker</surname><given-names>O.</given-names></name>
<name><surname>Razdan</surname><given-names>R.</given-names></name>
<name><surname>Santos</surname><given-names>J.</given-names></name>
</person-group><article-title>An Overview of Autonomous Vehicles Sensors and Their Vulnerability to Weather Conditions</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>5397</elocation-id><pub-id pub-id-type="doi">10.3390/s21165397</pub-id><pub-id pub-id-type="pmid">34450839</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01111"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schwarting</surname><given-names>W.</given-names></name>
<name><surname>Alonso-Mora</surname><given-names>J.</given-names></name>
<name><surname>Rus</surname><given-names>D.</given-names></name>
</person-group><article-title>Planning and Decision-Making for Autonomous Vehicles</article-title><source>Annu. Rev. Control. Robot. Auton. Syst.</source><year>2018</year><volume>1</volume><fpage>187</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1146/annurev-control-060117-105157</pub-id></element-citation></ref><ref id="B4-sensors-25-01111"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Van Brummelen</surname><given-names>J.</given-names></name>
<name><surname>O&#x02019;Brien</surname><given-names>M.</given-names></name>
<name><surname>Gruyer</surname><given-names>D.</given-names></name>
<name><surname>Najjaran</surname><given-names>H.</given-names></name>
</person-group><article-title>Autonomous Vehicle Perception: The Technology of Today and Tomorrow</article-title><source>Transp. Res. Part C Emerg. Technol.</source><year>2018</year><volume>89</volume><fpage>384</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1016/j.trc.2018.02.012</pub-id></element-citation></ref><ref id="B5-sensors-25-01111"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Koci&#x00107;</surname><given-names>J.</given-names></name>
<name><surname>Jovi&#x0010d;i&#x00107;</surname><given-names>N.</given-names></name>
<name><surname>Drndarevi&#x00107;</surname><given-names>V.</given-names></name>
</person-group><article-title>Sensors and Sensor Fusion in Autonomous Vehicles</article-title><source>Proceedings of the 2018 26th Telecommunications Forum (TELFOR)</source><conf-loc>Belgrade, Serbia</conf-loc><conf-date>20&#x02013;21 November 2018</conf-date><fpage>420</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1109/TELFOR.2018.8612054</pub-id></element-citation></ref><ref id="B6-sensors-25-01111"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Campbell</surname><given-names>S.</given-names></name>
<name><surname>O&#x02019;Mahony</surname><given-names>N.</given-names></name>
<name><surname>Krpalcova</surname><given-names>L.</given-names></name>
<name><surname>Riordan</surname><given-names>D.</given-names></name>
<name><surname>Walsh</surname><given-names>J.</given-names></name>
<name><surname>Murphy</surname><given-names>A.</given-names></name>
<name><surname>Ryan</surname><given-names>C.</given-names></name>
</person-group><article-title>Sensor Technology in Autonomous Vehicles: A review</article-title><source>Proceedings of the 2018 29th Irish Signals and Systems Conference (ISSC)</source><conf-loc>Belfast, UK</conf-loc><conf-date>21&#x02013;22 June 2018</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/ISSC.2018.8585340</pub-id></element-citation></ref><ref id="B7-sensors-25-01111"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>H.</given-names></name>
<name><surname>Cheng</surname><given-names>B.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>K.</given-names></name>
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>D.</given-names></name>
</person-group><article-title>Object Classification Using CNN-Based Fusion of Vision and LIDAR in Autonomous Vehicle Environment</article-title><source>IEEE Trans. Ind. Inform.</source><year>2018</year><volume>14</volume><fpage>4224</fpage><lpage>4231</lpage><pub-id pub-id-type="doi">10.1109/TII.2018.2822828</pub-id></element-citation></ref><ref id="B8-sensors-25-01111"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Maddern</surname><given-names>W.</given-names></name>
<name><surname>Stewart</surname><given-names>A.</given-names></name>
<name><surname>McManus</surname><given-names>C.</given-names></name>
<name><surname>Upcroft</surname><given-names>B.</given-names></name>
<name><surname>Churchill</surname><given-names>W.</given-names></name>
<name><surname>Newman</surname><given-names>P.</given-names></name>
</person-group><article-title>Illumination Invariant Imaging: Applications in Robust Vision-Based Localisation, Mapping and Classification for Autonomous Vehicles</article-title><source>Proceedings of the Visual Place Recognition in Changing Environments Workshop, IEEE International Conference on Robotics and Automation (ICRA)</source><conf-loc>Hong Kong, China</conf-loc><conf-date>31 May&#x02013;7 June 2014</conf-date><volume>Volume 2</volume><fpage>5</fpage></element-citation></ref><ref id="B9-sensors-25-01111"><label>9.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Einecke</surname><given-names>N.</given-names></name>
<name><surname>Gandhi</surname><given-names>H.</given-names></name>
<name><surname>Deigm&#x000f6;ller</surname><given-names>J.</given-names></name>
</person-group><article-title>Detection of camera artifacts from camera images</article-title><source>Proceedings of the 17th International IEEE Conference on Intelligent Transportation Systems (ITSC)</source><conf-loc>Qingdao, China</conf-loc><conf-date>8&#x02013;11 October 2014</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2014</year><fpage>603</fpage><lpage>610</lpage></element-citation></ref><ref id="B10-sensors-25-01111"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Boev</surname><given-names>A.</given-names></name>
<name><surname>Hollosi</surname><given-names>D.</given-names></name>
<name><surname>Gotchev</surname><given-names>A.</given-names></name>
<name><surname>Egiazarian</surname><given-names>K.</given-names></name>
</person-group><article-title>Classification and simulation of stereoscopic artifacts in mobile 3DTV content</article-title><source>Proceedings of the Stereoscopic Displays and Applications XX</source><conf-loc>San Jose, CA, USA</conf-loc><conf-date>18&#x02013;22 January 2009</conf-date><publisher-name>SPIE</publisher-name><publisher-loc>Bellingham, WA, USA</publisher-loc><year>2009</year><volume>Volume 7237</volume><fpage>462</fpage><lpage>473</lpage></element-citation></ref><ref id="B11-sensors-25-01111"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dumic</surname><given-names>E.</given-names></name>
<name><surname>da Silva Cruz</surname><given-names>L.A.</given-names></name>
</person-group><article-title>Point Cloud Coding Solutions, Subjective Assessment and Objective Measures: A Case Study</article-title><source>Symmetry</source><year>2020</year><volume>12</volume><elocation-id>1955</elocation-id><pub-id pub-id-type="doi">10.3390/sym12121955</pub-id></element-citation></ref><ref id="B12-sensors-25-01111"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Meynet</surname><given-names>G.</given-names></name>
<name><surname>Nehm&#x000e9;</surname><given-names>Y.</given-names></name>
<name><surname>Digne</surname><given-names>J.</given-names></name>
<name><surname>Lavou&#x000e9;</surname><given-names>G.</given-names></name>
</person-group><article-title>PCQM: A Full-Reference Quality Metric for Colored 3D Point Clouds</article-title><source>Proceedings of the 2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX)</source><conf-loc>Athlone, Ireland</conf-loc><conf-date>26&#x02013;28 May 2020</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/QoMEX48832.2020.9123147</pub-id></element-citation></ref><ref id="B13-sensors-25-01111"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Javaheri</surname><given-names>A.</given-names></name>
<name><surname>Brites</surname><given-names>C.</given-names></name>
<name><surname>Pereira</surname><given-names>F.</given-names></name>
<name><surname>Ascenso</surname><given-names>J.</given-names></name>
</person-group><article-title>A Point-to-Distribution Joint Geometry and Color Metric for Point Cloud Quality Assessment</article-title><source>Proceedings of the 2021 IEEE 23rd International Workshop on Multimedia Signal Processing (MMSP)</source><conf-loc>Tampere, Finland</conf-loc><conf-date>6&#x02013;8 October 2021</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/MMSP53017.2021.9733670</pub-id></element-citation></ref><ref id="B14-sensors-25-01111"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Yuan</surname><given-names>H.</given-names></name>
<name><surname>Hamzaoui</surname><given-names>R.</given-names></name>
<name><surname>Su</surname><given-names>H.</given-names></name>
<name><surname>Hou</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>H.</given-names></name>
</person-group><article-title>Reduced Reference Perceptual Quality Model With Application to Rate Control for Video-Based Point Cloud Compression</article-title><source>IEEE Trans. Image Process.</source><year>2021</year><volume>30</volume><fpage>6623</fpage><lpage>6636</lpage><pub-id pub-id-type="doi">10.1109/TIP.2021.3096060</pub-id><pub-id pub-id-type="pmid">34280099</pub-id>
</element-citation></ref><ref id="B15-sensors-25-01111"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Viola</surname><given-names>I.</given-names></name>
<name><surname>Cesar</surname><given-names>P.</given-names></name>
</person-group><article-title>A Reduced Reference Metric for Visual Quality Evaluation of Point Cloud Contents</article-title><source>IEEE Signal Process. Lett.</source><year>2020</year><volume>27</volume><fpage>1660</fpage><lpage>1664</lpage><pub-id pub-id-type="doi">10.1109/LSP.2020.3024065</pub-id></element-citation></ref><ref id="B16-sensors-25-01111"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mu</surname><given-names>B.</given-names></name>
<name><surname>Shao</surname><given-names>F.</given-names></name>
<name><surname>Chen</surname><given-names>H.</given-names></name>
<name><surname>Jiang</surname><given-names>Q.</given-names></name>
<name><surname>Xu</surname><given-names>L.</given-names></name>
<name><surname>Ho</surname><given-names>Y.S.</given-names></name>
</person-group><article-title>Hallucinated-PQA: No Reference Point Cloud Quality Assessment via Injecting Pseudo-Reference Features</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>243</volume><fpage>122953</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2023.122953</pub-id></element-citation></ref><ref id="B17-sensors-25-01111"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qu</surname><given-names>C.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Ma</surname><given-names>F.</given-names></name>
<name><surname>Huang</surname><given-names>K.</given-names></name>
</person-group><article-title>Parameter Optimization for Point Clouds Denoising based on No-Reference Quality Assessment</article-title><source>Measurement</source><year>2023</year><volume>211</volume><fpage>112592</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.112592</pub-id></element-citation></ref><ref id="B18-sensors-25-01111"><label>18.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Krivoku&#x00107;a</surname><given-names>M.</given-names></name>
<name><surname>Chou</surname><given-names>P.A.</given-names></name>
<name><surname>Savill</surname><given-names>P.</given-names></name>
</person-group><article-title>8i Voxelized Surface Light Field (8iVSLF) Dataset</article-title><year>2018</year><comment>Available online: <ext-link xlink:href="https://mpeg-pcc.org/index.php/pcc-content-database/8i-voxelized-surface-light-field-8ivslf-dataset" ext-link-type="uri">https://mpeg-pcc.org/index.php/pcc-content-database/8i-voxelized-surface-light-field-8ivslf-dataset</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-01-20">(accessed on 20 January 2024)</date-in-citation></element-citation></ref><ref id="B19-sensors-25-01111"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Su</surname><given-names>H.</given-names></name>
<name><surname>Duanmu</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
</person-group><article-title>Perceptual Quality Assessment of Colored 3D Point Clouds</article-title><source>IEEE Trans. Vis. Comput. Graph.</source><year>2022</year><volume>29</volume><fpage>3642</fpage><lpage>3655</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2022.3167151</pub-id><pub-id pub-id-type="pmid">35417349</pub-id>
</element-citation></ref><ref id="B20-sensors-25-01111"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>Q.</given-names></name>
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>L.</given-names></name>
</person-group><article-title>Point Cloud Quality Assessment: Dataset Construction and Learning-based No-reference Metric</article-title><source>ACM Trans. Multimed. Comput. Commun. Appl.</source><year>2023</year><volume>19</volume><fpage>80</fpage><pub-id pub-id-type="doi">10.1145/3550274</pub-id></element-citation></ref><ref id="B21-sensors-25-01111"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Abouelaziz</surname><given-names>I.</given-names></name>
<name><surname>Chetouani</surname><given-names>A.</given-names></name>
<name><surname>El Hassouni</surname><given-names>M.</given-names></name>
<name><surname>Latecki</surname><given-names>L.J.</given-names></name>
<name><surname>Cherifi</surname><given-names>H.</given-names></name>
</person-group><article-title>No-Reference Mesh Visual Quality Assessment via Ensemble of Convolutional Neural Networks and Compact Multi-Linear Pooling</article-title><source>Pattern Recognit.</source><year>2020</year><volume>100</volume><fpage>107174</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2019.107174</pub-id></element-citation></ref><ref id="B22-sensors-25-01111"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Javaheri</surname><given-names>A.</given-names></name>
<name><surname>Brites</surname><given-names>C.</given-names></name>
<name><surname>Pereira</surname><given-names>F.</given-names></name>
<name><surname>Ascenso</surname><given-names>J.</given-names></name>
</person-group><article-title>Subjective and Objective Quality Evaluation of 3D Point Cloud Denoising Algorithms</article-title><source>Proceedings of the 2017 IEEE International Conference on Multimedia &#x00026; Expo Workshops (ICMEW)</source><conf-loc>Hong Kong, China</conf-loc><conf-date>10&#x02013;14 July 2017</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ICMEW.2017.8026263</pub-id></element-citation></ref><ref id="B23-sensors-25-01111"><label>23.</label><element-citation publication-type="book"><std>ITU-RBT.500-11</std><source>Methodology for the Subjective Assessment of the Quality of Television Pictures</source><publisher-name>International Telecommunication Union</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2002</year></element-citation></ref><ref id="B24-sensors-25-01111"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mohammadi</surname><given-names>P.</given-names></name>
<name><surname>Ebrahimi-Moghadam</surname><given-names>A.</given-names></name>
<name><surname>Shirani</surname><given-names>S.</given-names></name>
</person-group><article-title>Subjective and Objective Quality Assessment of Image: A Survey</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1406.7799v1</pub-id></element-citation></ref><ref id="B25-sensors-25-01111"><label>25.</label><element-citation publication-type="book"><std>ISO-20462-3:2012</std><source>Photography&#x02014;Psychophysical Experimental Methods for Estimating Image Quality</source><publisher-name>International Organization for Standardization</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2012</year></element-citation></ref><ref id="B26-sensors-25-01111"><label>26.</label><element-citation publication-type="book"><std>ISO-15622:2018</std><source>Adaptive Cruise Control Systems</source><publisher-name>International Organization for Standardization</publisher-name><publisher-loc>Geneva, Switzerland</publisher-loc><year>2018</year></element-citation></ref><ref id="B27-sensors-25-01111"><label>27.</label><element-citation publication-type="book"><source>LiDAR Puck</source><publisher-name>Velodyne Lidar</publisher-name><publisher-loc>San Jose, CA, USA</publisher-loc><year>2019</year><comment>Available online: <ext-link xlink:href="https://web.archive.org/web/20240603174709/https://velodynelidar.com/wp-content/uploads/2019/12/63-9229_Rev-K_Puck-_Datasheet_Web.pdf" ext-link-type="uri">https://web.archive.org/web/20240603174709/https:
//velodynelidar.com/wp-content/uploads/2019/12/63-9229_Rev-K_Puck-_Datasheet_Web.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-01-20">(accessed on 20 January 2024)</date-in-citation></element-citation></ref><ref id="B28-sensors-25-01111"><label>28.</label><element-citation publication-type="book"><article-title>Intel RealSense Product Family D400 Series</article-title><publisher-name>Intel(R) Corporation</publisher-name><publisher-loc>Santa Clara, CA, USA</publisher-loc><year>2023</year><comment>Available online: <ext-link xlink:href="https://www.intel.com/content/dam/support/us/en/documents/emerging-technologies/intel-realsense-technology/Intel-RealSense-D400-Series-Datasheet.pdf" ext-link-type="uri">https://www.intel.com/content/dam/support/us/en/documents/emerging-technologies/intel-realsense-technology/Intel-RealSense-D400-Series-Datasheet.pdf</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-01-20">(accessed on 20 January 2024)</date-in-citation></element-citation></ref><ref id="B29-sensors-25-01111"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Velasco-S&#x000e1;nchez</surname><given-names>E.</given-names></name>
<name><surname>Loyola P&#x000e1;ez-Ubieta</surname><given-names>I.d.</given-names></name>
<name><surname>Candelas</surname><given-names>F.A.</given-names></name>
<name><surname>Puente</surname><given-names>S.T.</given-names></name>
</person-group><article-title>LiDAR Data Augmentation by Interpolation on Spherical Range Image</article-title><source>Proceedings of the 2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)</source><conf-loc>Sinaia, Romania</conf-loc><conf-date>12&#x02013;15 September 2023</conf-date><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/ETFA54631.2023.10275512</pub-id></element-citation></ref><ref id="B30-sensors-25-01111"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>P&#x000e1;ez-Ubieta</surname><given-names>I.d.L.</given-names></name>
<name><surname>Velasco-S&#x000e1;nchez</surname><given-names>E.</given-names></name>
<name><surname>Puente</surname><given-names>S.T.</given-names></name>
<name><surname>Candelas</surname><given-names>F.A.</given-names></name>
</person-group><article-title>Detection and Depth Estimation for Domestic Waste in Outdoor Environments by Sensors Fusion</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2211.04085</pub-id><pub-id pub-id-type="doi">10.1016/j.ifacol.2023.10.211</pub-id></element-citation></ref><ref id="B31-sensors-25-01111"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Weinmann</surname><given-names>M.</given-names></name>
<name><surname>Jutzi</surname><given-names>B.</given-names></name>
<name><surname>Mallet</surname><given-names>C.</given-names></name>
</person-group><article-title>Semantic 3D scene interpretation: A framework combining optimal neighborhood size selection with relevant features</article-title><source>ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.</source><year>2014</year><volume>2</volume><fpage>181</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.5194/isprsannals-II-3-181-2014</pub-id></element-citation></ref><ref id="B32-sensors-25-01111"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Korhonen</surname><given-names>J.</given-names></name>
<name><surname>You</surname><given-names>J.</given-names></name>
</person-group><article-title>Peak signal-to-noise ratio revisited: Is simple beautiful?</article-title><source>Proceedings of the 2012 Fourth International Workshop on Quality of Multimedia Experience</source><conf-loc>Melbourne, VIC, Australia</conf-loc><conf-date>5&#x02013;7 July 2012</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2012</year><fpage>37</fpage><lpage>38</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01111-f001"><label>Figure 1</label><caption><p>The overview of the development of the colored point cloud evaluation method. The research approach starts with performing the tests to obtain raw data consisting of colored point clouds representations of the test artifact.</p></caption><graphic xlink:href="sensors-25-01111-g001" position="float"/></fig><fig position="float" id="sensors-25-01111-f002"><label>Figure 2</label><caption><p>A schematic showing a top view of the test setup for each artifact side with the sensor shown in grey (<bold>left</bold>). An example of a detectable test at 2 m with the artifact and sensor mount circled in yellow (<bold>right</bold>).</p></caption><graphic xlink:href="sensors-25-01111-g002" position="float"/></fig><fig position="float" id="sensors-25-01111-f003"><label>Figure 3</label><caption><p>Images of sample results visually describing the conventions followed when isolating the points of the test artifact within the interpolation fusion data.</p></caption><graphic xlink:href="sensors-25-01111-g003" position="float"/></fig><fig position="float" id="sensors-25-01111-f004"><label>Figure 4</label><caption><p>Results from a sample test showing an example of how a bounding box was used to isolate the test artifacts points in stereoscopic data for tests at distances greater than 3 m.</p></caption><graphic xlink:href="sensors-25-01111-g004" position="float"/></fig><fig position="float" id="sensors-25-01111-f005"><label>Figure 5</label><caption><p>Developing the splitting plane to classify the artifact points based on their location. Coordinates are in meters.</p></caption><graphic xlink:href="sensors-25-01111-g005" position="float"/></fig><fig position="float" id="sensors-25-01111-f006"><label>Figure 6</label><caption><p>The process of creating the reference planes for both sides of the test artifact points. Coordinates are in meters.</p></caption><graphic xlink:href="sensors-25-01111-g006" position="float"/></fig><fig position="float" id="sensors-25-01111-f007"><label>Figure 7</label><caption><p>A series of plots visually describing the sequence of determining the point coverage of a set of artifact points. For these images, the stereoscopic data of the left&#x02212;undetectable side at 1 m test was used, with the projected artifact point on the left reference plane shown in black (<bold>left</bold>), forming the shape of the total point coverage shown in yellow (<bold>middle</bold>), and determining the point coverage of the test artifact&#x02019;s expected area with the artifact&#x02019;s bounding box in blue, the centroid normal in pink x, point coverage in green, and error in the point coverage in red (<bold>right</bold>).</p></caption><graphic xlink:href="sensors-25-01111-g007" position="float"/></fig><fig position="float" id="sensors-25-01111-f011"><label>Figure 11</label><caption><p>The RMS value of the distance between the artifact points and their respective reference plane.</p></caption><graphic xlink:href="sensors-25-01111-g011" position="float"/></fig><fig position="float" id="sensors-25-01111-f012"><label>Figure 12</label><caption><p>The average LPD of the artifact point clouds.</p></caption><graphic xlink:href="sensors-25-01111-g012" position="float"/></fig><fig position="float" id="sensors-25-01111-f013"><label>Figure 13</label><caption><p>The average color difference values for each test.</p></caption><graphic xlink:href="sensors-25-01111-g013" position="float"/></fig><fig position="float" id="sensors-25-01111-f014"><label>Figure 14</label><caption><p>The PSNR image values for each test. Note that the PSNR could not be calculated for the undetectable side since the reference color was (0, 0, 0).</p></caption><graphic xlink:href="sensors-25-01111-g014" position="float"/></fig></floats-group></article>