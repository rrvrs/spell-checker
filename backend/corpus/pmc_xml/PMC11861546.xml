<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Aging Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Aging Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Aging Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Aging Neuroscience</journal-title></journal-title-group><issn pub-type="epub">1663-4365</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40013095</article-id><article-id pub-id-type="pmc">PMC11861546</article-id><article-id pub-id-type="doi">10.3389/fnagi.2025.1527323</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Multimodal fusion model for diagnosing mild cognitive impairment in unilateral middle cerebral artery steno-occlusive disease</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yuan</surname><given-names>Ziyi</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Zhaodi</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/resources/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Chaojun</given-names></name><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/2903623/overview"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Shengrong</given-names></name><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Ren</surname><given-names>Qingguo</given-names></name><xref rid="aff4" ref-type="aff">
<sup>4</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/825920/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Xia</surname><given-names>Xiaona</given-names></name><xref rid="aff4" ref-type="aff">
<sup>4</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/2811660/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Jiang</surname><given-names>Qingjun</given-names></name><xref rid="aff4" ref-type="aff">
<sup>4</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Daoqiang</given-names></name><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/550122/overview"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhu</surname><given-names>Qi</given-names></name><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/654168/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/resources/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Meng</surname><given-names>Xiangshui</given-names></name><xref rid="aff4" ref-type="aff">
<sup>4</sup>
</xref><xref rid="aff5" ref-type="aff">
<sup>5</sup>
</xref><xref rid="c002" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/2779048/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/resources/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>School of Medicine, Cheeloo College of Medicine, Shandong University</institution>, <addr-line>Jinan</addr-line>, <country>China</country></aff><aff id="aff2"><sup>2</sup><institution>Department of Radiology, Meng Chao Hepatobiliary Hospital of Fujian Medical University, Fuzhou</institution>, <addr-line>Fujian</addr-line>, <country>China</country></aff><aff id="aff3"><sup>3</sup><institution>College of Artificial Intelligence, Nanjing University of Aeronautics and Astronautics, Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education</institution>, <addr-line>Nanjing</addr-line>, <country>China</country></aff><aff id="aff4"><sup>4</sup><institution>Department of Radiology, Qilu Hospital (Qingdao), Cheeloo College of Medicine, Shandong University</institution>, <addr-line>Qingdao</addr-line>, <country>China</country></aff><aff id="aff5"><sup>5</sup><institution>Medical Imaging and Engineering Intersection Key Laboratory of Qingdao</institution>, <addr-line>Qingdao</addr-line>, <country>China</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Lidia Ghosh, RCC Institute of Information Technology, India</p></fn><fn fn-type="edited-by"><p>Reviewed by: Yingwei Guo, Northeast Petroleum University, China</p><p>Amiyangshu De, Indian Institute of Technology Kharagpur, India</p></fn><corresp id="c001">*Correspondence: Qi Zhu, <email>zhuqi@nuaa.edu.cn</email></corresp><corresp id="c002">Xiangshui Meng, <email>mengxiangshui2021@126.com</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>17</volume><elocation-id>1527323</elocation-id><history><date date-type="received"><day>13</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>29</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025 Yuan, Huang, Li, Li, Ren, Xia, Jiang, Zhang, Zhu, Meng.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Yuan, Huang, Li, Li, Ren, Xia, Jiang, Zhang, Zhu, Meng</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><sec><title>Objectives</title><p>To propose a multimodal functional brain network (FBN) and structural brain network (SBN) topological feature fusion technique based on resting-state functional magnetic resonance imaging (rs-fMRI), diffusion tensor imaging (DTI), 3D-T1-weighted imaging (3D-T1WI), and demographic characteristics to diagnose mild cognitive impairment (MCI) in patients with unilateral middle cerebral artery (MCA) steno-occlusive disease.</p></sec><sec><title>Methods</title><p>The performances of different algorithms on the MCI dataset were evaluated using 5-fold cross-validation. The diagnostic results of the multimodal performance were evaluated using t-distributed stochastic neighbor embedding (t-SNE) analysis. The four-modal analysis method proposed in this study was applied to identify brain regions and connections associated with MCI, thus confirming its validity.</p></sec><sec><title>Results</title><p>Based on the fusion of the topological features of the multimodal FBN and SBN, the accuracy for the diagnosis of MCI in patients with unilateral MCA steno-occlusive disease reached 90.00%. The accuracy, recall, sensitivity, and F1-score were higher than those of the other methods, as was the diagnostic efficacy (AUC = 0.9149).</p></sec><sec><title>Conclusion</title><p>The multimodal FBN and SBN topological feature fusion technique, which incorporates rs-fMRI, DTI, 3D-T1WI, and demographic characteristics, obtains the most discriminative features of MCI in patients with unilateral MCA steno-occlusive disease and can effectively identify disease-related brain areas and connections. Efficient automated diagnosis facilitates the early and accurate detection of MCI and timely intervention and treatment to delay or prevent disease progression.</p></sec></abstract><kwd-group><kwd>middle cerebral artery</kwd><kwd>stenosis</kwd><kwd>multimodality imaging</kwd><kwd>mild cognitive impairment</kwd><kwd>Montreal cognitive assessment</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that financial support was received for the research, authorship, and/or publication of this article. This research was supported by the Natural Science Foundation of Shandong province (ZR2023MH042), Qingdao key clinical specialty project fund (QDZDZK-2022-097), and the Natural Science Foundation of Qingdao (23-2-1-201-zyyd-jch, 24-4-4-zrjj-153-jch).</funding-statement></funding-group><counts><fig-count count="7"/><table-count count="6"/><equation-count count="10"/><ref-count count="70"/><page-count count="15"/><word-count count="10273"/></counts><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Neurocognitive Aging and Behavior</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="S1"><title>1 Introduction</title><p>Intracranial arterial stenosis (ICAS) is an independent risk factor for cerebral ischaemia. Stenosis of the middle cerebral artery (MCA) is the most frequent subtype of ICAS (<xref rid="B66" ref-type="bibr">Zhang L. et al., 2022</xref>). Persistent arterial stenosis can lead to intracranial ischaemic damage, resulting in brain atrophy and secondary neurodegeneration, which may affect cognitive function. Mild cognitive impairment (MCI) is an intermediate state between normal aging and dementia (<xref rid="B42" ref-type="bibr">Petersen et al., 2018</xref>; <xref rid="B8" ref-type="bibr">Cheng et al., 2017</xref>; <xref rid="B26" ref-type="bibr">Langa and Levine, 2014</xref>). Various scales are used to diagnose MCI, such as the Mini-Mental State Examination (MMSE), Montreal cognitive assessment (MoCA), and AD8 (<xref rid="B43" ref-type="bibr">Pinto et al., 2019</xref>; <xref rid="B37" ref-type="bibr">Nasreddine et al., 2005</xref>; <xref rid="B54" ref-type="bibr">Usarel et al., 2019</xref>). Different scales use slightly different criteria and methods to assess cognitive function, which may result in different diagnostic results (<xref rid="B70" ref-type="bibr">Zhuang et al., 2021</xref>). In addition, the scale assessment process is limited by a certain degree of subjectivity because the scoring criteria and results may be affected by the assessor&#x02019;s personal experience and bias, leading to incorrect or missed diagnoses. The characteristics and symptoms of MCI vary among different populations. For example, factors such as age, cultural background, and education level may affect the accuracy of assessment results. Therefore, it is of great clinical significance to investigate intelligent diagnostic and analytical methods for MCI as crucial interventions to ensure early diagnosis and timely treatment.</p><p>With the development of imaging technology, multimodal magnetic resonance imaging (MRI) has provided objective supplementary disease biomarkers for the computer-aided diagnosis of MCI. Structural MRI shows specific cerebral gray and white matter atrophy (<xref rid="B41" ref-type="bibr">Pennanen et al., 2004</xref>). Resting-state functional MRI (rs-fMRI) indirectly detects neural activity in the brain based on blood oxygen level-dependent (BOLD) signals and can detect abnormalities in brain function in patients with MCI (<xref rid="B32" ref-type="bibr">Li et al., 2016</xref>; <xref rid="B33" ref-type="bibr">Liu et al., 2016</xref>). Diffusion tensor imaging (DTI) is widely used to study the orientation and integrity of white matter fiber tracts by measuring the Brownian motion of water molecules in neural tissues (<xref rid="B27" ref-type="bibr">Le Bihan, 2003</xref>), indirectly reflecting tissue microstructure and pathological changes (<xref rid="B23" ref-type="bibr">Jiang et al., 2006</xref>). Significant differences in anisotropy scores (FA) and mean diffusivity (MD) have been found in the white matter of patients with MCI compared to normal subjects (<xref rid="B47" ref-type="bibr">Sexton et al., 2011</xref>; <xref rid="B63" ref-type="bibr">Yu et al., 2017</xref>; <xref rid="B49" ref-type="bibr">Shim et al., 2017</xref>).</p><p>Despite the utility of the techniques, learning disease features and identifying imaging markers by using a single modality have limitations. Multimodal MRI can integrate complementary information from different modalities, thereby improving disease diagnosis by detecting subtle structural alterations in the brain more accurately than with a single modality. Therefore, several studies investigating MCI have used combinations of functional and structural connectivity networks, with results indicating that network features based on multimodal images are advantageous for the diagnosis of MCI (<xref rid="B68" ref-type="bibr">Zhu et al., 2014</xref>; <xref rid="B64" ref-type="bibr">Zhang et al., 2011</xref>; <xref rid="B50" ref-type="bibr">Song et al., 2023</xref>). For example, studies have shown that the integration of multiple modalities, such as genetic, epigenomic, and neuroimaging data, using hyper-graph-based sparse canonical correlation analysis (HGSCCA) can extract meaningful biomarkers related to MCI (<xref rid="B48" ref-type="bibr">Shao et al., 2021</xref>). Joint neuroimaging synthesis representation learning (JSRL) has been proposed for conversion using incomplete multi-modal neuroimaging data and has shown superior performance for MCI cross-database synthesis compared to several state-of-the-art methods (<xref rid="B34" ref-type="bibr">Liu et al., 2022</xref>). In addition, feature selection methods, such as a multi-classification prediction model based on fusing multi-modal features, have been developed to accurately diagnose and predict the progression of MCI. The proposed feature selection method with a multikernel support vector machine (MK-SVM) showed better classification performance than state-of-the-art multimodality-based methods (<xref rid="B17" ref-type="bibr">Hao et al., 2020</xref>). In another study, <xref rid="B30" ref-type="bibr">Lei et al. (2021)</xref> constructed a functional brain network (FBN) and structural brain network (SBN) based on rs-fMRI and DTI, respectively, and used an automatic weighted centralized multitasking learning framework to integrate these structural and functional connectivity features, achieving diagnostic accuracies higher than 84.80% between normal controls and patients with subjective cognitive impairment with MCI. This suggests that connected networks based on multimodal images have significant potential for MCI classification and diagnosis.</p><p>Studies have demonstrated that extracranial arterial stenosis is an independent risk factor for cognitive dysfunction (<xref rid="B20" ref-type="bibr">Huang et al., 2018</xref>; <xref rid="B13" ref-type="bibr">Dempsey et al., 2018</xref>; <xref rid="B56" ref-type="bibr">Wang et al., 2017</xref>). However, few studies have investigated the correlation between intracranial arterial stenosis, particularly MCA stenosis, and cognitive impairment. In additionally, uniform diagnostic imaging criteria for cognitive impairment caused by intracranial vascular stenosis are lacking. Therefore, this study tested a novel fusion technique based on the topological features of multimodal FBN and SBN to improve early diagnosis of MCI in patients with unilateral MCA steno-occlusive disease. The main contributions of the proposed method are summarized as follows:</p><list list-type="simple"><list-item><label>&#x02022;</label><p>We developed a multimodal framework that combines rs-fMRI, DTI, three-dimensional-T1-weighted imaging (3D-T1WI), and demographic data, leveraging the complementary strengths of these modalities for diagnosing MCI in patients with unilateral MCA steno-occlusive disease.</p></list-item><list-item><label>&#x02022;</label><p>A topological feature fusion technique was introduced that preserves SBN and FBN properties while using attention mechanisms to integrate multi-channel topological features and identify MCI-related brain regions and connections.</p></list-item><list-item><label>&#x02022;</label><p>Our method achieved superior performance, with an accuracy (ACC) of 90.00% and an area under the receiver operating characteristic (ROC) curve (AUC) of 0.9149, surpassing existing techniques and providing an effective tool for early MCI detection and intervention.</p></list-item></list></sec><sec sec-type="materials|methods" id="S2"><title>2 Materials and methods</title><sec id="S2.SS1"><title>2.1 Participants</title><p>Forty patients with unilateral MCA steno-occlusive disease, diagnosed based on magnetic resonance angiography (MRA) between January 2017 and August 2023, were recruited and divided into two groups based on MoCA results. The MCI and non-MCI (NMCI) groups comprised 11 and 29 patients, respectively. According to Chinese MoCA norms (<xref rid="B35" ref-type="bibr">Lu et al., 2011</xref>), the level of cognitive impairment is defined by the following scores: &#x02264; 13 for illiterate individuals, &#x02264; 19 for individuals with 1&#x02013;6 years of education, and &#x02264; 24 for those with 7 or more years of education. The inclusion criteria were as follows: (a) asymptomatic or subjective memory decline; (b) unilateral MCA stenosis &#x0003e; 70%; (c) absence of stroke, TIA, or dementia; (d) right-handed; (e) ability to complete the MRI examination with a qualifying high-resolution MRI image; (f) no history of drug use that could affect cognitive function; and (g) normal-appearing white matter (normal brain parenchymal signals or lacunar infarcts &#x0003c; 3 mm in diameter on T2-weighted and fluid attenuated inversion recovery [FLAIR] sequences). The exclusion criteria were as follows: (a) other cerebral artery stenosis &#x02265; 30%; (b) severe visual or auditory impairment preventing completion of cognitive function assessment; (c) history of severe systemic or neuropsychiatric diseases; (d) history of frequent dizziness and headache; (e) history of acute or chronic cerebral infarction, bleeding, tumor, infectious disease, or metabolic disease detected by MRI; (f) history of drug or alcohol dependence during the last 6 months; and (g) contraindications for MRI. This study was approved by the Medical Ethics Committee of Q Hospital, and informed consent was obtained from all participants. The detailed demographic characteristics of the participants are presented in <xref rid="T1" ref-type="table">Table 1</xref>.</p><table-wrap position="float" id="T1"><label>TABLE 1</label><caption><p>Demographic characteristics of the subjects.</p></caption><table frame="box" rules="all" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1"/><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">NMCI (<italic>n</italic> = 29)</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">MCI (<italic>n</italic> = 11)</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">&#x003c7; 2/T/Z</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1"><italic>p</italic>-value</td></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Age (years, median, IQR)</td><td valign="top" align="center" rowspan="1" colspan="1">59.3 (16.00)</td><td valign="top" align="center" rowspan="1" colspan="1">64.2 (2.00)</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;1.352</td><td valign="top" align="center" rowspan="1" colspan="1">0.176<xref rid="t1fnc" ref-type="table-fn"><sup>c</sup></xref></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Sex (female, %)</td><td valign="top" align="center" rowspan="1" colspan="1">16 (55.17%)</td><td valign="top" align="center" rowspan="1" colspan="1">6 (54.55%)</td><td valign="top" align="center" rowspan="1" colspan="1">0.000</td><td valign="top" align="center" rowspan="1" colspan="1">1.000<xref rid="t1fna" ref-type="table-fn"><sup>a</sup></xref></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Education (years, mean &#x000b1; SD)</td><td valign="top" align="center" rowspan="1" colspan="1">9.03 &#x000b1; 4.64</td><td valign="top" align="center" rowspan="1" colspan="1">9.18 &#x000b1; 2.04</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02212;0.139</td><td valign="top" align="center" rowspan="1" colspan="1">0.890<xref rid="t1fnb" ref-type="table-fn"><sup>b</sup></xref></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Hypertension (%)</td><td valign="top" align="center" rowspan="1" colspan="1">24 (82.76%)</td><td valign="top" align="center" rowspan="1" colspan="1">8 (72.73%)</td><td valign="top" align="center" rowspan="1" colspan="1">0.071</td><td valign="top" align="center" rowspan="1" colspan="1">0.791<xref rid="t1fna" ref-type="table-fn"><sup>a</sup></xref></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Diabetes (%)</td><td valign="top" align="center" rowspan="1" colspan="1">13 (44.83%)</td><td valign="top" align="center" rowspan="1" colspan="1">3 (27.27%)</td><td valign="top" align="center" rowspan="1" colspan="1">0.423</td><td valign="top" align="center" rowspan="1" colspan="1">0.515<xref rid="t1fna" ref-type="table-fn"><sup>a</sup></xref></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Hyperlipidemia (%)</td><td valign="top" align="center" rowspan="1" colspan="1">13 (44.83%)</td><td valign="top" align="center" rowspan="1" colspan="1">7 (63.64%)</td><td valign="top" align="center" rowspan="1" colspan="1">1.129</td><td valign="top" align="center" rowspan="1" colspan="1">0.288<xref rid="t1fna" ref-type="table-fn"><sup>a</sup></xref></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Current smoker (%)</td><td valign="top" align="center" rowspan="1" colspan="1">6 (20.69%)</td><td valign="top" align="center" rowspan="1" colspan="1">2 (18.18%)</td><td valign="top" align="center" rowspan="1" colspan="1">0.000</td><td valign="top" align="center" rowspan="1" colspan="1">1.000<xref rid="t1fna" ref-type="table-fn"><sup>a</sup></xref></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Current drinker (%)</td><td valign="top" align="center" rowspan="1" colspan="1">6 (20.69%)</td><td valign="top" align="center" rowspan="1" colspan="1">3 (27.27%)</td><td valign="top" align="center" rowspan="1" colspan="1">0.000</td><td valign="top" align="center" rowspan="1" colspan="1">0.983<xref rid="t1fna" ref-type="table-fn"><sup>a</sup></xref></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Severe stenosis or occlusion side (right, %)</td><td valign="top" align="center" rowspan="1" colspan="1">13 (44.83%)</td><td valign="top" align="center" rowspan="1" colspan="1">7 (63.64%)</td><td valign="top" align="center" rowspan="1" colspan="1">1.129</td><td valign="top" align="center" rowspan="1" colspan="1">0.288<xref rid="t1fna" ref-type="table-fn"><sup>a</sup></xref></td></tr></tbody></table><table-wrap-foot><fn id="t1fna"><p><italic><sup>a</sup></italic>Fisher&#x02019;s exact test.</p></fn><fn id="t1fnb"><p><italic><sup>b</sup></italic>Independent-samples <italic>t</italic>-test.</p></fn><fn id="t1fnc"><p><italic><sup>c</sup></italic>Mann&#x02013;Whitney U test. <italic>p</italic>-value significant cut-off 0.05. IQR, interquartile range; SD, standard deviation.</p></fn></table-wrap-foot></table-wrap><p>Hypertension was defined as a self-reported diagnosis by a physician, antihypertensive medication use, or systolic or diastolic blood pressure &#x02265; 140 or &#x02265; 90 mmHg, respectively. Diabetes was defined as a self-reported history of antidiabetic medication use or glycated hemoglobin A1C level &#x02265; 6.5%. Hyperlipidaemia was defined as a history of hyperlipidemia, a clinical diagnosis of hyperlipidaemia during hospitalization, or the use of lipid-lowering medication (<xref rid="B2" ref-type="bibr">American Diabetes Association, 2020</xref>; <xref rid="B21" ref-type="bibr">Israelsson et al., 2017</xref>; <xref rid="B22" ref-type="bibr">Jaraj et al., 2016</xref>). A current smoker was defined as someone who smoked &#x0003e;100 cigarettes in their lifetime and was currently smoking cigarettes at the time of the survey (<xref rid="B1" ref-type="bibr">Adeloye et al., 2019</xref>). A current drinker was defined as someone who consumed at least one alcoholic beverage per week during the past month.</p></sec><sec id="S2.SS2"><title>2.2 MRI data acquisition</title><p>Brain MRI was performed using a 3.0T MRI scanner (Ingenia; Philips Medical Systems, Netherlands). A matched head coil with foam padding and earplugs were used to reduce head motion and scanner noise. The scanning sessions were performed using the following parameters: (1) T2-weighted imaging (T2WI): 18 axial slices, 6-mm slice thickness with a 1-mm gap, repetition time/time to echo (TR/TE) = 2,369/107 ms, matrix = 352 &#x000d7; 352; (2) T2WI- FLAIR: 18 axial slices, 6-mm slice thickness with a 1-mm gap, TR/TE = 7,000/125 ms, matrix = 288 &#x000d7; 163; (3) diffusion weighted imaging (DWI): 18 axial slices, 6-mm slice thickness with a 1-mm gap, TR/TE = 2,235/76 ms, matrix = 176 &#x000d7; 134; (4) 3D-T1WI: 170 sagittal slices, 1-mm slice thickness with no gap, TR/TE = 6.7/3.0 ms; (5) DTI: (70 axial slices, 2 mm slice thickness with no gap, TR/TE = 4,900/95 ms, matrix = 122 &#x000d7; 110, b values = 1,000 s/mm<sup>2</sup>) in 32 directions; and (6) rs-fMRI: 32 axial slices, 4-mm slice thickness with a 0.5-mm gap, 240 time points, TR/TE = 2,000/30 ms, flip angle = 90&#x000b0;, field of view = 230 mm<sup>2</sup> &#x000d7; 230 mm<sup>2</sup>, data matrix = 68 &#x000d7; 66, voxel = 3.5 mm<sup>3</sup> &#x000d7; 3.5 mm<sup>3</sup> &#x000d7; 4 mm<sup>3</sup>.</p></sec><sec id="S2.SS3"><title>2.3 Data preprocessing</title><p>(1) rs-fMRI processing: All rs-fMRI data were pre-processed using the DPARSF toolbox (<xref rid="B6" ref-type="bibr">Chao-Gan and Yu-Feng, 2010</xref>). Following the general fMRI preprocessing pipeline, the serialized data were split into several pieces and adjusted to the echo-planar imaging template to correct and rectify the initial image (<xref rid="B65" ref-type="bibr">Zhang J. et al., 2022</xref>; <xref rid="B69" ref-type="bibr">Zhu et al., 2024</xref>). Detrending was used to reduce the effects of head motion and interference from the cerebrospinal fluid (CSF) and white matter. After linear detrending, the data were filtered using a typical time bandpass filter to reduce low-frequency drift and high-frequency physiological noise. Next, the motion parameters, global mean signal, white matter, and CSF were employed as interference covariates to reduce the effects of head movement and non-neuronal blood oxygenation level dependent fluctuations. After processing, we employed an automated anatomical labeling (AAL) atlas (<xref rid="B11" ref-type="bibr">Craddock et al., 2012</xref>) to partition the rs-fMRI date into 90 brain regions, each containing blood oxygen level signals at 240 time points, with feature dimensions of 90 &#x000d7; 240.</p><p>(2) DTI processing: DTI distortions were first corrected using the FSL-based PANDA toolbox (<xref rid="B12" ref-type="bibr">Cui et al., 2013</xref>). The processing procedure inclouded the following steps: b0-based brain extraction utilizing the bet function and correction for eddy currents and head motion employing the eddy_correct function with b0 serving as the reference volume. Based on each subject&#x02019;s co-registered T1 images, TrackVis was used to obtain fiber images using a deterministic tracking method, and the anatomic areas were defined using AAL conventions. Finally, the number of fibers was used to measure the structural connectivity with feature dimensions of 90 &#x000d7; 90.</p><p>(3) 3D-T1WI processing: The CAT12 toolbox of Statistical Parametric Mapping (SPM12) (<xref rid="B3" ref-type="bibr">Ashburner and Friston, 2005</xref>) was used to segment the 3D-T1WI. Regions of interest (ROIs) refer to specific areas of the brain selected for detailed analysis, which are often based on prior knowledge or anatomical templates. In this case, the AAL template was used to extract the volume of the 116 ROIs from the segmented gray matter. Given that we focused exclusively on the brain, brain regions 91&#x02013;116 were removed as features, resulting in feature dimensions of 90 &#x000d7; 1. The last 26 brain regions were removed to exclude those that were less relevant to the specific analysis, thus ensuring a more focused and meaningful feature set for the given task.</p><p>(4) Demographic characteristic processing: Participant information was coded with dimensions of 90 &#x000d7; 9.</p></sec><sec id="S2.SS4"><title>2.4 Multimodal imaging technique based on rs-fMRI, DTI, 3D-T1WI, and demographic characteristics</title><p>In this study, we proposed a framework that fuses multimodal (four-modality) FBN and SBN topological features, focusing on multimodal classification using four modalities: rs-fMRI, DTI, 3D-T1WI, and demographic characteristics. <xref rid="F1" ref-type="fig">Figure 1</xref> shows a schematic of the proposed multimodal data fusion and classification system. The construction of FBNs and SBNs plays a key role in generating brain network data with topological properties. A multichannel graph attention network was utilized to extract spatial features from multichannel graph-structured data. The attention mechanism effectively fuses features from different channels, and a multilayer perceptron (MLP) (<xref rid="B46" ref-type="bibr">Rosenblatt, 1958</xref>; <xref rid="B67" ref-type="bibr">Zhou et al., 2023</xref>) classifier was then applied to classify the extracted features.</p><fig position="float" id="F1"><label>FIGURE 1</label><caption><p>The framework of the proposed multimodal brain networks fusion for brain disease diagnosis. Our framework can be divided into three parts: construction of FBN and SBN, multi-channel graph attention network and classifier.</p></caption><graphic xlink:href="fnagi-17-1527323-g001" position="float"/></fig><p>As illustrated in <xref rid="F2" ref-type="fig">Figure 2</xref>, our framework initiated a comprehensive data processing phase that standardizes the fMRI, DTI, and 3D-T1WI data. These processes include distortion correction, brain extraction, eddy current and head motion correction, image segmentation, ROI extraction, and time-point signal extraction, along with the integration of encoded demographic characteristics. Subsequently, FBNs and SBNs were constructed to reveal dynamic functional connectivity and physical connections between brain regions, respectively. A multichannel graph attention network was then employed to extract the topological features from these networks. The network utilizes an attention mechanism to enhance the representation of brain regions associated with MCI and integrates multimodal information to improve the comprehensiveness and accuracy of features. The extracted features were then fed into an optimized classifier, which was designed to enhance the accuracy, recall, specificity, and F1 score for MCI diagnosis, and the model&#x02019;s diagnostic performance was further assessed through the ROC curve. <xref rid="F2" ref-type="fig">Figure 2</xref> provides a clear visual representation of the entire process, from data preprocessing to brain network construction to feature extraction and classification, offering a transparent view of how the various components of the multimodal brain network analysis workflow interact and collaborate to effectively diagnose MCI. This integrated approach allows a more comprehensive capture of brain network changes related to MCI, thereby providing a scientific basis for early diagnosis and intervention.</p><fig position="float" id="F2"><label>FIGURE 2</label><caption><p>Application flow chart of multimodal brain network construction and feature fusion in MCI diagnosis.</p></caption><graphic xlink:href="fnagi-17-1527323-g002" position="float"/></fig><sec id="S2.SS4.SSS1"><title>2.4.1 Construction of functional brain networks and structural brain networks</title><p>In the diagnosis of diseases, single-modal brain imaging data such as 3D-T1WI, rs-fMRI, and DTI contain complex and unique discriminative information. A vector representation constructed as an Euclidean space is not conducive to data fusion or information extraction. 3D-T1WI can generate static images to obtain information regarding the patient&#x02019;s body, rs-fMRI reflects changes in brain activity in the temporal dimension, and DTI reflects the physical connectivity of brain intervals in the spatial dimension. In addition, demographic characteristics, including years of education, type of MCA stenosis, sex, age, disease history, and other information comprehensively reflect the background of the subjects. Changes in the brain connectivity patterns are important features of brain disorders. Constructing the brain networks of subjects is a common method for intelligent diagnosis of brain diseases. For each subject, we defined the demographic characteristics matrix <italic>E</italic> = (<italic>e</italic>, <italic>e</italic>,&#x022ef;,<italic>e</italic><sub><italic>P</italic></sub>)<sup><italic>T</italic></sup> &#x02208; &#x0211d;<sup><italic>P</italic></sup>, where <italic>P</italic> represents the amount of background information in demographic characteristics. The feature matrix of 3D-T1WI was defined as <italic>T</italic> = (<italic>t</italic><sub>1</sub>, <italic>t</italic>,&#x022ef;,<italic>t</italic><sub><italic>N</italic></sub>)<sup><italic>T</italic></sup> &#x02208; &#x0211d;<sup><italic>N</italic></sup>, where <italic>N</italic> denotes the number of ROIs. We further introduced a multilayer perceptron to transform the dimensions of the demographic characteristic matrix. The process is as follows:</p><disp-formula id="S2.E1">
<label>(1)</label>
<mml:math id="M1" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mpadded width="+3.3pt"><mml:mi>E</mml:mi><mml:mi/><mml:mo>&#x02032;</mml:mo></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><p>where <italic>f</italic><sub><italic>1</italic></sub> is parameterised by the network weights <italic>w</italic><sub>1</sub> &#x02208; &#x0211d;<sup><italic>N</italic> &#x000d7; <italic>P</italic></sup> and the learnable bias term <italic>b</italic><sub>1</sub> &#x02208; &#x0211d;<sup><italic>N</italic></sup>. We further performed a preliminary fusion of <italic>E</italic>&#x02032; and <italic>T</italic> to obtain the feature F: <italic>F</italic> = <italic>E</italic>&#x02032; + <italic>T</italic>. Next, we transformed the dimensions of the fused feature matrix as follows:</p><disp-formula id="S2.E2">
<label>(2)</label>
<mml:math id="M2" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mpadded width="+3.3pt"><mml:mi>R</mml:mi></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><p>where <italic>f</italic><sub><italic>2</italic></sub> is parameterised by the network weights <italic>w</italic><sub>2</sub> &#x02208; &#x0211d;<sup><italic>N</italic>&#x000d7;<italic>N</italic></sup> and <italic>b</italic><sub>2</sub> &#x02208; &#x0211d;<sup><italic>N</italic></sup> is the learnable bias term, <italic>R</italic> &#x02208; &#x0211d;<sup><italic>N</italic></sup>. At the same time, we defined the rs-fMRI time-series data for each subject as <italic>X</italic> = (<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>,&#x022ef;,<italic>x</italic><sub><italic>N</italic></sub>)<sup><italic>T</italic></sup> &#x02208; &#x0211d;<sup><italic>N</italic>&#x000d7;<italic>M</italic></sup>, where <italic>N</italic> denotes the number of brain regions and <italic>M</italic> represents the number of consecutive time series points collected. Pearson&#x02019;s correlation coefficients were calculated for the paired ROIs to measure functional connectivity.</p><disp-formula id="S2.E3">
<label>(3)</label>
<mml:math id="M3" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>o</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>v</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">&#x003c3;</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>&#x02062;</mml:mo><mml:msub><mml:mi mathvariant="normal">&#x003c3;</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><p>where <inline-formula><mml:math id="INEQ12" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>o</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>v</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the covariance of <inline-formula><mml:math id="INEQ13" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="INEQ14" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> and &#x003c3; denotes the standard deviation. Thus, we obtained the functional connectivity matrix <italic>C</italic> = (<italic>c</italic><sub>1</sub>, <italic>c</italic><sub>2</sub>,&#x022ef;,<italic>c</italic><sub><italic>N</italic></sub>)<sup><italic>T</italic></sup> &#x02208; &#x0211d;<sup><italic>N</italic> &#x000d7; <italic>N</italic></sup>, where vector <inline-formula><mml:math id="INEQ16" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> denotes functional connectivity feature of the <italic>i<sup>th</sup></italic> brain region. The DTI feature matrix is <italic>D</italic> = (<italic>d</italic><sub>1</sub>, <italic>d</italic><sub>2</sub>,&#x022ef;,<italic>d</italic><sub><italic>N</italic></sub>)<sup><italic>T</italic></sup>, <italic>d</italic><sub><italic>i</italic></sub> &#x02208; &#x0211d;<sup><italic>N</italic></sup>, and the values in the matrix reflect the strength of brain interval connectivity. To achieve further fusion of different modalities, we defined the FBN as <italic>G</italic><sub><italic>F</italic></sub> = (<italic>R</italic>, <italic>C</italic>), and the SBN as <italic>G</italic><sub><italic>S</italic></sub> = (<italic>R</italic>, <italic>D</italic>).</p></sec><sec id="S2.SS4.SSS2"><title>2.4.2 Multichannel graph attention network</title><p>To maintain the topological information of FBNs and SBNs, we developed a method to extract the topological features of brain networks using multi-channel graph attention networks. A multi-channel graph attention network is primarily composed of two graph-attention networks, as shown in <xref rid="F1" ref-type="fig">Figure 1</xref>. The input to each channel is a series of feature vectors of the brain regions and connections. Taking the FBN<italic>G</italic><sub><italic>F</italic></sub> = (<italic>R</italic>, <italic>C</italic>) as an example, its node feature can be expressed as R = (<italic>r</italic><sub>1</sub>, <italic>r</italic><sub>2</sub>,&#x022ef;, <italic>r</italic><sub><italic>N</italic></sub>)<sup><italic>T</italic></sup>,<italic>r</italic><sub><italic>i</italic></sub> &#x02208; &#x0211d;<sup><italic>N</italic></sup>, where the number of brain regions and feature dimension are <italic>N</italic>. If brain regions <italic>i</italic> to brain regions <italic>j</italic> have an edge, the brain area concentration coefficient <inline-formula><mml:math id="INEQ23" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>R</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>L</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>U</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02062;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt">&#x02225;</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02062;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where a &#x02208; &#x0211d;<sup>2<italic>N</italic></sup> denotes a learnable attention vector, <italic>W</italic> denotes a learnable weight matrix, and &#x02225; represents the concatenation operation. <italic>LeakyReLU</italic> is the LeakyReLU activation function, where the parameter &#x003b1; is typically set to 0.2, allowing a small gradient for negative input values to prevent neurones from becoming inactive.</p><p>We employed a masking mechanism to embed the graph structure from the adjacency matrix <italic>C</italic> into the attention coefficients. Subsequently, attention coefficient <italic>e</italic><sub><italic>ij</italic></sub> is updated as follows:</p><disp-formula id="S2.E4">
<label>(4)</label>
<mml:math id="M4" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo rspace="52.5pt">,</mml:mo><mml:mpadded width="+3.3pt"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mpadded></mml:mrow><mml:mo rspace="5.8pt">=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo rspace="57.5pt">,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><p>We further applied the SoftMax function to normalize <italic>e</italic><sub><italic>ij</italic></sub> for neighboring brain regions <italic>j</italic> &#x02208; <italic>N</italic><sub><italic>i</italic></sub> of the <italic>i<sup>th</sup></italic> brain region. The SoftMax function converts the input values into a probability distribution, ensuring that the normalized values sum to 1 across the neighboring regions. The normalized attention coefficient can then be obtained as follows:</p><disp-formula id="S2.E5">
<label>(5)</label>
<mml:math id="M5" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mi mathvariant="normal">&#x003b1;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>o</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>f</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>R</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>L</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>U</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02062;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt">&#x02225;</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02062;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>R</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>L</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>U</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02062;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt">&#x02225;</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02062;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><p>The normalized attention coefficient is used to update the brain network features, and the updated features of the <italic>i<sup>th</sup></italic> brain region are expressed as follows.</p><disp-formula id="S2.E6">
<label>(6)</label>
<mml:math id="M6" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msubsup><mml:mi>z</mml:mi><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi mathvariant="normal">&#x003c3;</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">&#x1d4a9;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi mathvariant="normal">&#x003b1;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02062;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x02062;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula></sec><sec id="S2.SS4.SSS3"><title>2.4.3 Attention mechanism</title><p>The attention mechanism plays a pivotal role in feature fusion, because it enables the model to selectively prioritize the most informative features from each embedding. By assigning adaptive weights to different features, the attention mechanism enhances the capacity of the model to discern and leverage complex interactions between brain regions, leading to more refined and accurate classifications. Furthermore, this approach increases the interpretability of the model, as it provides insight into which specific features or regions contribute most significantly to the decision-making process. Using the multi-channel graph attention network, we obtained two feature embeddings: <italic>Z<sub>F</sub></italic> and <italic>Z<sub>S</sub></italic>. Considering that the labels of the brain network are related to their pair combinations, we used the attention mechanism (&#x003b3;<sub>F</sub>, &#x003b3;<sub>S</sub>) = <italic>att</italic>(Z<sub>F</sub>, Z<sub>S</sub>) to fuse them, where &#x003b3;<sub><italic>F</italic></sub>, &#x003b3;<sub><italic>S</italic></sub> &#x02208; &#x0211d;<sup><italic>N</italic>&#x000d7;1</sup> represent the attention values of the <italic>i<sup>th</sup></italic> brain embedded regions <italic>Z<sub>F</sub></italic> and <italic>Z<sub>S</sub></italic>, respectively. For the brain region <italic>i</italic>, its embedding in <italic>Z</italic> was <italic>z</italic>. We first transformed the embedding by nonlinear transformation, and subsequently used a shared attention algorithm <italic>q</italic> &#x02208; &#x0211d;<sup><italic>N</italic>&#x000d7;1</sup> to obtain the attention value <inline-formula><mml:math id="INEQ32" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b4;</mml:mi><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula> as follows:</p><disp-formula id="S2.Ex1">
<mml:math id="M7" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msubsup><mml:mi mathvariant="normal">&#x003b4;</mml:mi><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>&#x02062;</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>h</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02062;</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula><p>where <italic>W</italic> is the weight matrix, and <italic>b</italic> is the bias vector. We normalized the attention values using the SoftMax function:</p><disp-formula id="S2.E7">
<label>(7)</label>
<mml:math id="M8" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msubsup><mml:mi>w</mml:mi><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>o</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>f</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="normal">&#x003b4;</mml:mi><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo rspace="5.8pt">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.8pt">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="normal">&#x003b4;</mml:mi><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="normal">&#x003b4;</mml:mi><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="normal">&#x003b4;</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><p>Similarly, <inline-formula><mml:math id="INEQ33" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msubsup><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>o</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>f</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="normal">&#x003b4;</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, a larger attention weight indicated that the corresponding embedding is more important. For <italic>N</italic> brain regions, there were learnable weights <italic>w</italic><sub>F</sub>, <italic>w</italic><sub>S</sub> &#x02208; &#x0211d;<sup><italic>N</italic>&#x000d7;1</sup>, and &#x003b3;<sub>F</sub> = <italic>diag</italic>(<italic>w</italic><sub>F</sub>), &#x003b3;<sub>S</sub> = <italic>diag</italic>(<italic>w</italic><sub>S</sub>). We then combined the embedding output from the multi-channel graph attention network to obtain the final embedding:</p><disp-formula id="S2.E8">
<label>(8)</label>
<mml:math id="M9" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mpadded width="+3.3pt"><mml:mi>Z</mml:mi></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">&#x003b3;</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">&#x003b3;</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula></sec><sec id="S2.SS4.SSS4"><title>2.4.4 MLP classifier</title><p>MLP, also known as an artificial neural network (ANN), contains an input layer, output layer, and several hidden layers. The hidden layer is fully connected to the input layer. Assuming that the vector of the input layer is x and h(x) is selected as a linear function, the hidden layer is: <italic>g</italic> = <italic>Hx</italic> + &#x022c5;<italic>k</italic>, and the vector y of the output layeris:</p><disp-formula id="S2.E9">
<label>(9)</label>
<mml:math id="M10" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mpadded width="+3.3pt"><mml:mi>y</mml:mi></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.8pt">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x02062;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><p>where <italic>H</italic> denotes the weight coefficient, <italic>k</italic> is the bias term, and the function <italic>f</italic> mostly uses the sigmoid, tanh, and ReLU functions. Core complex multilayer perceptron can contain several hidden layers. After the experiments, an MLP model with the following structure was selected: each hidden layer used a linear function, the input was the final embedding <italic>Z</italic>, and the output was the probability vector of MCI and NMCI. In the experiments, we adopted ReLU as the activation function and employed cross-entropy loss to supervise the learning of the multimodal brain network topological features.</p></sec></sec><sec id="S2.SS5"><title>2.5 Validation</title><p>To evaluate the performance of the different classification methods, we used a 5-fold cross-validation strategy to compute the classification accuracy (ACC), sensitivity (SEN), specificity (SPE), precision (PRE), recall (REC), F-measure (F1), and AUC. Specifically, five approximately equally sized, mutually exclusive subsets were partitioned from the entire dataset, four of which were used for training and the remaining for testing. Each algorithm was applied to the MCI recognition task, where the MCI dataset contained two classification labels (MCI group and NMCI group), which we considered a binary task to determine whether the subject had cognitive impairment.</p></sec><sec id="S2.SS6"><title>2.6 Statistical analysis</title><p>Fisher&#x02019;s exact test, independent samples <italic>t</italic>-test, and Mann&#x02013;Whitney U test were used to determine whether there were statistically significant differences between the groups. Statistical analyses of demographic characteristics and neurobehavioural assessment results were performed using the Statistical Package for the Social Sciences, version 25 (SPSS 25, Chicago, Illinois, USA), with a significance level set at <italic>p</italic> &#x0003c; 0.05.</p></sec></sec><sec id="S3"><title>3 Experiment results</title><sec id="S3.SS1"><title>3.1 Multimodal classification</title><p>To validate the effectiveness of the proposed method, we compared it with the following ten FBN methods: GraphSAGE (<xref rid="B16" ref-type="bibr">Hamilton et al., 2017</xref>), GCN (<xref rid="B25" ref-type="bibr">Kipf and Welling, 2017</xref>), GAT (<xref rid="B55" ref-type="bibr">Veli&#x0010d;kovi&#x00107; et al., 2018</xref>), MLP (<xref rid="B46" ref-type="bibr">Rosenblatt, 1958</xref>, <xref rid="B67" ref-type="bibr">Zhou et al., 2023</xref>), BrainNetCNN (<xref rid="B24" ref-type="bibr">Kawahara et al., 2017</xref>), SAGPool (<xref rid="B29" ref-type="bibr">Lee et al., 2019</xref>), AM-GCN (<xref rid="B57" ref-type="bibr">Wang et al., 2020</xref>), PageRank (<xref rid="B38" ref-type="bibr">Page et al., 1999</xref>), SVM (<xref rid="B10" ref-type="bibr">Cortes and Vapnik, 1995</xref>), and CNN (<xref rid="B28" ref-type="bibr">Lecun et al., 1998</xref>), Cross-GNN (<xref rid="B59" ref-type="bibr">Yang et al., 2024</xref>), and RH-BrainFS (<xref rid="B61" ref-type="bibr">Ye et al., 2023</xref>). The classification performance results of the different brain network construction methods are presented in <xref rid="T2" ref-type="table">Tables 2</xref>, <xref rid="T3" ref-type="table">3</xref>. The evaluation metrics were ACC, PRE, REC, F1, AUC, SEN, and SPE. The best results are shown in bold. In addition, we plotted the ROC curves of the proposed methods and compared them (<xref rid="F3" ref-type="fig">Figures 3</xref>, <xref rid="F4" ref-type="fig">4</xref>).</p><table-wrap position="float" id="T2"><label>TABLE 2</label><caption><p>Comparative experimental results based on rs-fMRI, DTI, and 3D-T1WI data.</p></caption><table frame="box" rules="all" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Method</td><td valign="top" align="center" colspan="7" style="color:#ffffff;background-color: #7f8080;" rowspan="1">MCI vs. NMCI</td></tr></thead><tbody><tr><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1"/><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>ACC</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>PRE</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>REC</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>F1</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>AUC</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>SEN</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>SPE</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">GraphSAGE</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">46.67</td><td valign="top" align="center" rowspan="1" colspan="1">46.00</td><td valign="top" align="center" rowspan="1" colspan="1">70.50</td><td valign="top" align="center" rowspan="1" colspan="1">46.67</td><td valign="top" align="center" rowspan="1" colspan="1">96.67</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">GCN</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">59.24</td><td valign="top" align="center" rowspan="1" colspan="1">83.33</td><td valign="top" align="center" rowspan="1" colspan="1">61.33</td><td valign="top" align="center" rowspan="1" colspan="1">57.57</td><td valign="top" align="center" rowspan="1" colspan="1">83.33</td><td valign="top" align="center" rowspan="1" colspan="1">65.24</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">GAT</td><td valign="top" align="center" rowspan="1" colspan="1">82.50</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">75.00</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">96.00</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MLP</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">63.33</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">54.00</td><td valign="top" align="center" rowspan="1" colspan="1">69.67</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">92.67</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">BrainNetCNN</td><td valign="top" align="center" rowspan="1" colspan="1">75.00</td><td valign="top" align="center" rowspan="1" colspan="1">50.00</td><td valign="top" align="center" rowspan="1" colspan="1">56.67</td><td valign="top" align="center" rowspan="1" colspan="1">51.33</td><td valign="top" align="center" rowspan="1" colspan="1">57.00</td><td valign="top" align="center" rowspan="1" colspan="1">56.67</td><td valign="top" align="center" rowspan="1" colspan="1">80.67</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SAGPool</td><td valign="top" align="center" rowspan="1" colspan="1">67.50</td><td valign="top" align="center" rowspan="1" colspan="1">52.00</td><td valign="top" align="center" rowspan="1" colspan="1">46.67</td><td valign="top" align="center" rowspan="1" colspan="1">40.48</td><td valign="top" align="center" rowspan="1" colspan="1">59.00</td><td valign="top" align="center" rowspan="1" colspan="1">46.67</td><td valign="top" align="center" rowspan="1" colspan="1">76.67</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">AM-GCN</td><td valign="top" align="center" rowspan="1" colspan="1">77.50</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">46.67</td><td valign="top" align="center" rowspan="1" colspan="1">48.10</td><td valign="top" align="center" rowspan="1" colspan="1">61.00</td><td valign="top" align="center" rowspan="1" colspan="1">46.67</td><td valign="top" align="center" rowspan="1" colspan="1">84.67</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">PageRank</td><td valign="top" align="center" rowspan="1" colspan="1">85.00</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">40.00</td><td valign="top" align="center" rowspan="1" colspan="1">52.67</td><td valign="top" align="center" rowspan="1" colspan="1">65.67</td><td valign="top" align="center" rowspan="1" colspan="1">40.00</td><td valign="top" align="center" rowspan="1" colspan="1">92.67</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SVM</td><td valign="top" align="center" rowspan="1" colspan="1">60.00</td><td valign="top" align="center" rowspan="1" colspan="1">43.75</td><td valign="top" align="center" rowspan="1" colspan="1">63.64</td><td valign="top" align="center" rowspan="1" colspan="1">51.85</td><td valign="top" align="center" rowspan="1" colspan="1">42.67</td><td valign="top" align="center" rowspan="1" colspan="1">63.64</td><td valign="top" align="center" rowspan="1" colspan="1">68.97</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CNN</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">59.24</td><td valign="top" align="center" rowspan="1" colspan="1">83.33</td><td valign="top" align="center" rowspan="1" colspan="1">61.33</td><td valign="top" align="center" rowspan="1" colspan="1">57.57</td><td valign="top" align="center" rowspan="1" colspan="1">83.33</td><td valign="top" align="center" rowspan="1" colspan="1">65.24</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cross-GNN</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">66.67</td><td valign="top" align="center" rowspan="1" colspan="1">43.75</td><td valign="top" align="center" rowspan="1" colspan="1">46.67</td><td valign="top" align="center" rowspan="1" colspan="1">63.91</td><td valign="top" align="center" rowspan="1" colspan="1">43.75</td><td valign="top" align="center" rowspan="1" colspan="1">96.88</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">RH-BrainFS</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">66.67</td><td valign="top" align="center" rowspan="1" colspan="1">39.58</td><td valign="top" align="center" rowspan="1" colspan="1">42.50</td><td valign="top" align="center" rowspan="1" colspan="1">69.49</td><td valign="top" align="center" rowspan="1" colspan="1">39.58</td><td valign="top" align="center" rowspan="1" colspan="1">96.88</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="center" rowspan="1" colspan="1">87.50</td><td valign="top" align="center" rowspan="1" colspan="1">91.67</td><td valign="top" align="center" rowspan="1" colspan="1">83.33</td><td valign="top" align="center" rowspan="1" colspan="1">82.50</td><td valign="top" align="center" rowspan="1" colspan="1">87.50</td><td valign="top" align="center" rowspan="1" colspan="1">83.33</td><td valign="top" align="center" rowspan="1" colspan="1">96.88</td></tr></tbody></table><table-wrap-foot><fn><p>ACC, accuracy; PRE, precision; REC, recall; F1, F-measure; AUC, area under the curve; SEN, sensitivity; SPE, specificity.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="T3"><label>TABLE 3</label><caption><p>Comparative experimental results based on rs-fMRI, DTI, 3D-T1WI, and demographic characteristic data.</p></caption><table frame="box" rules="all" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Method</td><td valign="top" align="center" colspan="7" style="color:#ffffff;background-color: #7f8080;" rowspan="1">MCI vs. NMCI</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1"/></tr></thead><tbody><tr><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1"/><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>ACC</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>PRE</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>REC</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>F1</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>AUC</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>SEN</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>SPE</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold><italic>p</italic>-value</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">GraphSAGE</td><td valign="top" align="center" rowspan="1" colspan="1">85.00</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">62.67</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">96.00</td><td valign="top" align="center" rowspan="1" colspan="1">0.542</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">GCN</td><td valign="top" align="center" rowspan="1" colspan="1">75.00<xref rid="t3fns2" ref-type="table-fn">**</xref></td><td valign="top" align="center" rowspan="1" colspan="1">68.00</td><td valign="top" align="center" rowspan="1" colspan="1">63.33</td><td valign="top" align="center" rowspan="1" colspan="1">56.10</td><td valign="top" align="center" rowspan="1" colspan="1">67.14</td><td valign="top" align="center" rowspan="1" colspan="1">63.33</td><td valign="top" align="center" rowspan="1" colspan="1">83.14</td><td valign="top" align="center" rowspan="1" colspan="1">0.048</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">GAT</td><td valign="top" align="center" rowspan="1" colspan="1">82.50<xref rid="t3fns1" ref-type="table-fn">*</xref></td><td valign="top" align="center" rowspan="1" colspan="1">48.33</td><td valign="top" align="center" rowspan="1" colspan="1">60.00</td><td valign="top" align="center" rowspan="1" colspan="1">53.14</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">60.00</td><td valign="top" align="center" rowspan="1" colspan="1">92.67</td><td valign="top" align="center" rowspan="1" colspan="1">0.133</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MLP</td><td valign="top" align="center" rowspan="1" colspan="1">82.50<xref rid="t3fns1" ref-type="table-fn">*</xref></td><td valign="top" align="center" rowspan="1" colspan="1">66.67</td><td valign="top" align="center" rowspan="1" colspan="1">56.67</td><td valign="top" align="center" rowspan="1" colspan="1">60.00</td><td valign="top" align="center" rowspan="1" colspan="1">77.00</td><td valign="top" align="center" rowspan="1" colspan="1">56.67</td><td valign="top" align="center" rowspan="1" colspan="1">92.00</td><td valign="top" align="center" rowspan="1" colspan="1">0.140</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">BrainNetCNN</td><td valign="top" align="center" rowspan="1" colspan="1">72.50<xref rid="t3fns2" ref-type="table-fn">**</xref></td><td valign="top" align="center" rowspan="1" colspan="1">41.90</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">54.67</td><td valign="top" align="center" rowspan="1" colspan="1">66.14</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">65.81</td><td valign="top" align="center" rowspan="1" colspan="1">0.033</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SAGPool</td><td valign="top" align="center" rowspan="1" colspan="1">77.50<xref rid="t3fns2" ref-type="table-fn">**</xref></td><td valign="top" align="center" rowspan="1" colspan="1">50.00</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">56.67</td><td valign="top" align="center" rowspan="1" colspan="1">70.33</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">78.00</td><td valign="top" align="center" rowspan="1" colspan="1">0.043</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">AM-GCN</td><td valign="top" align="center" rowspan="1" colspan="1">82.50</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">50.00</td><td valign="top" align="center" rowspan="1" colspan="1">52.67</td><td valign="top" align="center" rowspan="1" colspan="1">59.33</td><td valign="top" align="center" rowspan="1" colspan="1">50.00</td><td valign="top" align="center" rowspan="1" colspan="1">93.33</td><td valign="top" align="center" rowspan="1" colspan="1">0.176</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">PageRank</td><td valign="top" align="center" rowspan="1" colspan="1">85.00</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">45.00</td><td valign="top" align="center" rowspan="1" colspan="1">54.67</td><td valign="top" align="center" rowspan="1" colspan="1">73.15</td><td valign="top" align="center" rowspan="1" colspan="1">45.00</td><td valign="top" align="center" rowspan="1" colspan="1">87.14</td><td valign="top" align="center" rowspan="1" colspan="1">0.542</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SVM</td><td valign="top" align="center" rowspan="1" colspan="1">75.00<xref rid="t3fns2" ref-type="table-fn">**</xref></td><td valign="top" align="center" rowspan="1" colspan="1">62.50</td><td valign="top" align="center" rowspan="1" colspan="1">45.45</td><td valign="top" align="center" rowspan="1" colspan="1">52.63</td><td valign="top" align="center" rowspan="1" colspan="1">78.00</td><td valign="top" align="center" rowspan="1" colspan="1">45.45</td><td valign="top" align="center" rowspan="1" colspan="1">89.66</td><td valign="top" align="center" rowspan="1" colspan="1">0.047</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">CNN</td><td valign="top" align="center" rowspan="1" colspan="1">85.00</td><td valign="top" align="center" rowspan="1" colspan="1">60.00</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">67.62</td><td valign="top" align="center" rowspan="1" colspan="1">73.33</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">85.33</td><td valign="top" align="center" rowspan="1" colspan="1">0.360</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cross-GNN</td><td valign="top" align="center" rowspan="1" colspan="1">85.00</td><td valign="top" align="center" rowspan="1" colspan="1">66.67</td><td valign="top" align="center" rowspan="1" colspan="1">52.08</td><td valign="top" align="center" rowspan="1" colspan="1">53.93</td><td valign="top" align="center" rowspan="1" colspan="1">84.89</td><td valign="top" align="center" rowspan="1" colspan="1">52.08</td><td valign="top" align="center" rowspan="1" colspan="1">96.87</td><td valign="top" align="center" rowspan="1" colspan="1">0.255</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">RH-BrainFS</td><td valign="top" align="center" rowspan="1" colspan="1">81.25<xref rid="t3fns1" ref-type="table-fn">*</xref></td><td valign="top" align="center" rowspan="1" colspan="1">16.67</td><td valign="top" align="center" rowspan="1" colspan="1">16.67</td><td valign="top" align="center" rowspan="1" colspan="1">16.67</td><td valign="top" align="center" rowspan="1" colspan="1">24.05</td><td valign="top" align="center" rowspan="1" colspan="1">16.67</td><td valign="top" align="center" rowspan="1" colspan="1">95.00</td><td valign="top" align="center" rowspan="1" colspan="1">0.149</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="center" rowspan="1" colspan="1">90.00</td><td valign="top" align="center" rowspan="1" colspan="1">91.67</td><td valign="top" align="center" rowspan="1" colspan="1">81.67</td><td valign="top" align="center" rowspan="1" colspan="1">83.75</td><td valign="top" align="center" rowspan="1" colspan="1">91.49</td><td valign="top" align="center" rowspan="1" colspan="1">81.67</td><td valign="top" align="center" rowspan="1" colspan="1">96.88</td><td valign="top" align="center" rowspan="1" colspan="1">&#x02013;</td></tr></tbody></table><table-wrap-foot><fn><p>ACC, accuracy; PRE, precision; REC, recall; F1, F-measure; AUC, area under the curve; SEN, sensitivity; SPE, specificity. <italic>p</italic>-value between the comparison methods and the proposed method:</p></fn><fn id="t3fns1"><p>*indicating <italic>p</italic> &#x02264; 0.15,</p></fn><fn id="t3fns2"><p>**indicating <italic>p</italic> &#x02264; 0.05.</p></fn></table-wrap-foot></table-wrap><fig position="float" id="F3"><label>FIGURE 3</label><caption><p>ROC curve analysis of different modes and different methods. <bold>(A)</bold> Three-modality analysis (fMRI, DTI, and 3D-T1WI data). <bold>(B)</bold> Four-modality analysis (rs-fMRI, DTI, 3D-T1WI, and demographic characteristics).</p></caption><graphic xlink:href="fnagi-17-1527323-g003" position="float"/></fig><fig position="float" id="F4"><label>FIGURE 4</label><caption><p>Parameter analysis of the model&#x02019;s performance: <bold>(A)</bold> Accuracy (ACC%) with varying learning rates. <bold>(B)</bold> Accuracy (ACC%) with different numbers of GAT layers, showing optimal configurations for improved performance.</p></caption><graphic xlink:href="fnagi-17-1527323-g004" position="float"/></fig><p>First, we tested the performance of our multimodal classification method using rs-fMRI, DTI, and 3D-T1WI for differentiating MCI from NMCI. <xref rid="T2" ref-type="table">Table 2</xref> shows the classification with our multimodal method compared with other methods. In the MCI and NMCI classification tasks, the ACC of the three-modal approach was 87.50% (SEN: 83.33%; SPE: 96.88%). The AUC value was 0.8750, which was higher than the AUC values of the other ten methods, indicating that the three-modal approach is effective and has good generalis ability in MCI diagnosis.</p><p>The performances of the different methods in the MCI and NMCI classification tasks after adding demographic characteristics are shown in <xref rid="T3" ref-type="table">Table 3</xref>. When demographic characteristics were combined with the three-modal approach, the ACC of the four-modal classification reached 90.00% (sensitivity = 81.67% and specificity = 96.88%) and the AUC reached 0.9149, which were higher than those of the other 10 methods. Compared with using only the three-modal approach, except for a slight decrease in SEN and REC, the ACC, F1, and AUC values improved, with a 3.99% increase in the AUC value (<xref rid="T1" ref-type="table">Table 1</xref>). The <italic>p</italic>-values presented in <xref rid="T3" ref-type="table">Table 3</xref> highlight the statistical significance of the proposed method compared with the existing approaches. Specifically, the <italic>p</italic>-values for comparisons with the GCN (<italic>p</italic> = 0.048), BrainNetCNN (<italic>p</italic> = 0.033), SAGPool (<italic>p</italic> = 0.043), and SVM (<italic>p</italic> = 0.047) were all less than 0.05, indicating that the improvements achieved by the proposed method were statistically significant. Furthermore, when compared with the RH-BrainFS (<italic>p</italic> = 0.149), GAT (<italic>p</italic> = 0.133), and MLP (<italic>p</italic> = 0.140), the <italic>p</italic>-values are &#x0003c; 0.15, demonstrating a trend toward significance. These results substantiate the superior performance of the proposed method, while underscoring the reliability of the observed improvements across various metrics. The experimental results indicated that the four-modal approach had superior classification ability for MCI. In addition, from the experimental results, we can found that rs-fMRI, DTI, 3D-T1WI, and demographic features complemented each other and jointly improved ACC of MCI diagnosis. Extensive experimental results further demonstrate that our method is effective and outperforms other algorithms.</p><p><xref rid="F3" ref-type="fig">Figure 3</xref> shows the ROC curves of the proposed method and the comparison methods, where the proposed method is represented by a thick red curve. These graphs show that the ROC curves of the comparison methods are mostly located below and to the right of the ROC curve of our method, whereas the area under the ROC curve of the comparative methods is significantly smaller than that of our method.</p><p>In general, from these tables, we find that our method achieved the optimal performance in all metrics compared with the comparison methods. This is because the proposed method effectively captured the complex topological information of brain networks and fused complementary information from the different modalities.</p><p><xref rid="F4" ref-type="fig">Figure 4</xref> illustrates the impact of the learning rate and number of GAT layers on the model performance (measured by percentage accuracy, ACC%). In panel (a), the accuracy initially improves as the learning rate increases, before stabilizing or declining, indicating that selecting an appropriate learning rate is crucial for effective model optimisation. The optimal learning rate was determined using grid search over the range {1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10}. Panel (b) shows that the accuracy increases with the number of GAT layers up to a certain point, after which it starts to decline. The optimal number of GAT layers was determined through a grid search within the range of {1, 2, 3, 4, 5}. These results suggest that while additional layers can enhance the representational capacity of the model, an excessive number of layers may lead to overfitting or gradient-related issues. These findings emphasize the importance of systematically tuning hyperparameters such as the learning rate and layer number to balance model performance and complexity and ensure optimal outcomes during training.</p><p>In addition, we evaluated the computational efficiency of the proposed approach to highlight its practicality. The experimental results indicated that the model incorporated 9.1579 million trainable parameters and required 9.8855 million floating-point operations per forward pass. These values reflect the lightweight nature of the framework, allowing it to deliver high diagnostic accuracy with minimal computational demands. This balance between performance and efficiency underscores its potential for real-world applications, particularly in resource-constrained environments.</p></sec><sec id="S3.SS2"><title>3.2 Ablation experiments</title><p>Different modalities provide complementary information for the diagnosis of cognitive impairment and other diseases, allowing the distinguishing features of the disease to be captured from multiple perspectives. We further studied the effectiveness of using discriminative information provided by multimodal data to enhance the diagnostic performance for diseases involving cognitive impairment. First, the existing architecture of the model was maintained throughout the analysis to ensure consistency. We then removed the rs-fMRI, DTI, 3D-T1WI, and demographic characteristic data to diagnose diseases with cognitive impairment while retaining the rs-fMRI, DTI, 3D-T1WI, and demographic characteristics for diagnosis. Finally, we integrated the four modal datasets to validate the improvement in multimodal information and the role of each modality. The results of the ablation experiments are listed in <xref rid="T4" ref-type="table">Table 4</xref>, which shows an improvement in the classification performance compared to the fusion of all four modalities.</p><table-wrap position="float" id="T4"><label>TABLE 4</label><caption><p>Results of the ablation experiments.</p></caption><table frame="box" rules="all" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Modality</td><td valign="top" align="center" colspan="7" style="color:#ffffff;background-color: #7f8080;" rowspan="1">MCI vs. NMCI</td></tr></thead><tbody><tr><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1"/><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>ACC</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>PRE</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>REC</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>F1</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>AUC</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>SEN</bold>
</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">
<bold>SPE</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">w/o rs-fMRI</td><td valign="top" align="center" rowspan="1" colspan="1">82.50</td><td valign="top" align="center" rowspan="1" colspan="1">53.33</td><td valign="top" align="center" rowspan="1" colspan="1">45.00</td><td valign="top" align="center" rowspan="1" colspan="1">44.00</td><td valign="top" align="center" rowspan="1" colspan="1">65.95</td><td valign="top" align="center" rowspan="1" colspan="1">45.00</td><td valign="top" align="center" rowspan="1" colspan="1">96.67</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">w/o DTI</td><td valign="top" align="center" rowspan="1" colspan="1">87.50</td><td valign="top" align="center" rowspan="1" colspan="1">85.42</td><td valign="top" align="center" rowspan="1" colspan="1">77.08</td><td valign="top" align="center" rowspan="1" colspan="1">80.42</td><td valign="top" align="center" rowspan="1" colspan="1">88.39</td><td valign="top" align="center" rowspan="1" colspan="1">77.08</td><td valign="top" align="center" rowspan="1" colspan="1">92.26</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">w/o 3D-T1WI</td><td valign="top" align="center" rowspan="1" colspan="1">87.50</td><td valign="top" align="center" rowspan="1" colspan="1">73.33</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">69.33</td><td valign="top" align="center" rowspan="1" colspan="1">81.67</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">90.00</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">w/o demographic characteristics</td><td valign="top" align="center" rowspan="1" colspan="1">87.50</td><td valign="top" align="center" rowspan="1" colspan="1">75.00</td><td valign="top" align="center" rowspan="1" colspan="1">63.33</td><td valign="top" align="center" rowspan="1" colspan="1">66.48</td><td valign="top" align="center" rowspan="1" colspan="1">75.67</td><td valign="top" align="center" rowspan="1" colspan="1">63.33</td><td valign="top" align="center" rowspan="1" colspan="1">96.00</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">rs-fMRI</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">46.67</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">54.29</td><td valign="top" align="center" rowspan="1" colspan="1">76.14</td><td valign="top" align="center" rowspan="1" colspan="1">70.00</td><td valign="top" align="center" rowspan="1" colspan="1">82.95</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">DTI</td><td valign="top" align="center" rowspan="1" colspan="1">85.00</td><td valign="top" align="center" rowspan="1" colspan="1">66.67</td><td valign="top" align="center" rowspan="1" colspan="1">50.00</td><td valign="top" align="center" rowspan="1" colspan="1">52.50</td><td valign="top" align="center" rowspan="1" colspan="1">53.13</td><td valign="top" align="center" rowspan="1" colspan="1">50.00</td><td valign="top" align="center" rowspan="1" colspan="1">91.67</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">3D-T1WI</td><td valign="top" align="center" rowspan="1" colspan="1">87.50</td><td valign="top" align="center" rowspan="1" colspan="1">75.00</td><td valign="top" align="center" rowspan="1" colspan="1">47.92</td><td valign="top" align="center" rowspan="1" colspan="1">55.00</td><td valign="top" align="center" rowspan="1" colspan="1">77.43</td><td valign="top" align="center" rowspan="1" colspan="1">47.92</td><td valign="top" align="center" rowspan="1" colspan="1">96.67</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Demographic characteristics</td><td valign="top" align="center" rowspan="1" colspan="1">85.00</td><td valign="top" align="center" rowspan="1" colspan="1">92.00</td><td valign="top" align="center" rowspan="1" colspan="1">66.67</td><td valign="top" align="center" rowspan="1" colspan="1">71.67</td><td valign="top" align="center" rowspan="1" colspan="1">68.67</td><td valign="top" align="center" rowspan="1" colspan="1">66.67</td><td valign="top" align="center" rowspan="1" colspan="1">92.00</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Ours</td><td valign="top" align="center" rowspan="1" colspan="1">90.00</td><td valign="top" align="center" rowspan="1" colspan="1">91.67</td><td valign="top" align="center" rowspan="1" colspan="1">81.67</td><td valign="top" align="center" rowspan="1" colspan="1">83.75</td><td valign="top" align="center" rowspan="1" colspan="1">91.49</td><td valign="top" align="center" rowspan="1" colspan="1">81.67</td><td valign="top" align="center" rowspan="1" colspan="1">96.88</td></tr></tbody></table><table-wrap-foot><fn><p>w/o, without.</p></fn></table-wrap-foot></table-wrap></sec><sec id="S3.SS3"><title>3.3 t-SNE visualization</title><p>To validate the feature extraction capability of the proposed method, we compared the learned features extracted using different methods. First, we reduced the dimensionality of the representation vectors using the t-SNE method (<xref rid="B36" ref-type="bibr">Maaten and Hinton, 2008</xref>), which is commonly used to visualize high-dimensional data by mapping it into a lower-dimensional space, typically two or three dimensions, while preserving the local structure of the data. We projected the two-dimensional vectors onto a public space for visualization, as shown in <xref rid="F5" ref-type="fig">Figure 5</xref>. Further, we visualized the results of the original sample data and comparison methods, as shown in <xref rid="F5" ref-type="fig">Figures 5A&#x02013;G</xref>, respectively. According to the experimental results, our method demonstrates a higher clustering of samples from the same class and clear boundaries between different categories of samples.</p><fig position="float" id="F5"><label>FIGURE 5</label><caption><p>t-SNE visualizations of the different methods: <bold>(A)</bold> The scattered distribution of the original sample in the representation space. <bold>(B&#x02013;G)</bold> The representation space of different methods on tasks MCI vs. NMCI, and <bold>(H)</bold> is the distribution of the features of our proposed methods in the representation space. In particular, the t-SNE scatter plot in our method exhibits a clearer separation between classes, demonstrating superior clustering and boundary delineation compared to the other methods.</p></caption><graphic xlink:href="fnagi-17-1527323-g005" position="float"/></fig></sec><sec id="S3.SS4"><title>3.4 Discriminative ROIs</title><p>In addition to the diagnostic performance of the classification, significant changes in brain regions and connections can be used to evaluate the performance of the brain network. Because not all ROIs are closely related to cognitive impairment, we used our proposed method to identify the most discriminative ROIs to understand brain abnormalities. By calculating the significant alterations in connectivity (SAC), we demonstrated local differences in brain networks. SAC quantifies the changes in connectivity strength between specific brain regions by comparing the brain network structures of different groups. Specifically, SAC is calculated by measuring the difference in connectivity values between corresponding brain regions across groups. Specifically, we applied a non-negative elastic net to measure the important brain regions in the brain network embedded prior to classification for each subject. We subsequent visualized the 20 most relevant ROIs and the top ten connections between them in the NMCI and MCI task.</p><p>The top 20 brain regions with significant weights in the MCI and NMCI classifications are presented in <xref rid="T5" ref-type="table">Table 5</xref> and <xref rid="F6" ref-type="fig">Figure 6</xref>. The English abbreviations corresponding to the Chinese and English names of the brain regions are listed in the <xref rid="DS1" ref-type="supplementary-material">Supplementary Appendix 1</xref>. The larger the weight of a brain region, the more likely it was to undergo significant changes. Brain regions with significant structural changes found using this method were confirmed to be associated with MCI, including Olfactory_L, Frontal_Sup_Medial_L, Parietal_Sup_L, Temporal_Pole_Mid_L, Frontal_Inf_Tri_R, Amygdala_L, Hippocampus_L, Frontal_Sup_Orb_R, and Paracentral_Lobule_L.</p><table-wrap position="float" id="T5"><label>TABLE 5</label><caption><p>Top 20 SACs between the MCI and NMCI groups.</p></caption><table frame="box" rules="all" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">No</td><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Brain region</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Weight</td></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">1</td><td valign="top" align="left" rowspan="1" colspan="1">Olfactory_L</td><td valign="top" align="center" rowspan="1" colspan="1">2.0813</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">2</td><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Sup_Medial_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.8685</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">3</td><td valign="top" align="left" rowspan="1" colspan="1">Calcarine_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.8178</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">4</td><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Inf_Oper_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.8059</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">5</td><td valign="top" align="left" rowspan="1" colspan="1">Temporal_Pole_Mid_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.7753</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">Amygdala_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.7066</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">Parietal_Sup_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.6641</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">8</td><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Sup_Orb_R</td><td valign="top" align="center" rowspan="1" colspan="1">0.5934</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">9</td><td valign="top" align="left" rowspan="1" colspan="1">Precentral_R</td><td valign="top" align="center" rowspan="1" colspan="1">0.5196</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">10</td><td valign="top" align="left" rowspan="1" colspan="1">Hippocampus_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.5095</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">11</td><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Sup_Orb_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.4984</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">12</td><td valign="top" align="left" rowspan="1" colspan="1">Occipital_Sup_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.4941</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">13</td><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Inf_Tri_R</td><td valign="top" align="center" rowspan="1" colspan="1">0.4638</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">14</td><td valign="top" align="left" rowspan="1" colspan="1">Paracentral_Lobule_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.4350</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">15</td><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Mid_R</td><td valign="top" align="center" rowspan="1" colspan="1">0.4264</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">16</td><td valign="top" align="left" rowspan="1" colspan="1">Caudate_R</td><td valign="top" align="center" rowspan="1" colspan="1">0.3770</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">17</td><td valign="top" align="left" rowspan="1" colspan="1">Parietal_Sup_R</td><td valign="top" align="center" rowspan="1" colspan="1">0.3687</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">18</td><td valign="top" align="left" rowspan="1" colspan="1">Temporal_Pole_Sup_R</td><td valign="top" align="center" rowspan="1" colspan="1">0.3671</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">19</td><td valign="top" align="left" rowspan="1" colspan="1">Cuneus_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.3148</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">20</td><td valign="top" align="left" rowspan="1" colspan="1">Frontal_Mid_L</td><td valign="top" align="center" rowspan="1" colspan="1">0.2635</td></tr></tbody></table></table-wrap><fig position="float" id="F6"><label>FIGURE 6</label><caption><p>Top 20 SACs between the MCI and NMCI groups. Brain regions with significant structural changes include Olfactory_L, Frontal_Sup_Medial_L, Parietal_Sup_L, Temporal_Pole_Mid_L, Frontal_Inf_Tri_R, Amygdala_L, Hippocampus_L, Frontal_Sup_Orb_R, and Paracentral_Lobule_L. Different brain regions are represented by different colors.</p></caption><graphic xlink:href="fnagi-17-1527323-g006" position="float"/></fig><p>In the classification of MCI and NMCI, the 10 brain connections with higher weights, as shown in <xref rid="T6" ref-type="table">Table 6</xref> and <xref rid="F7" ref-type="fig">Figure 7</xref>, the abnormal connections were distributed throughout the brain, showing asymmetry between the left and right hemispheres. Specifically, the connections between the Fusiform_R, Cuneus_R, and Supp_Motor_Area_L, and other brain regions showed significant changes.</p><table-wrap position="float" id="T6"><label>TABLE 6</label><caption><p>Top 10 brain connections between the MCI and NMCI groups.</p></caption><table frame="box" rules="all" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">NO</td><td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Connection</td><td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Weight</td></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">1</td><td valign="top" align="left" rowspan="1" colspan="1">Fusiform_R&#x02014;Precentral_R</td><td valign="top" align="center" rowspan="1" colspan="1">91.2125</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">2</td><td valign="top" align="left" rowspan="1" colspan="1">Fusiform_R&#x02014;Occipital_Sup_L</td><td valign="top" align="center" rowspan="1" colspan="1">89.9365</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">3</td><td valign="top" align="left" rowspan="1" colspan="1">Fusiform_R&#x02014;Temporal_Mid_L</td><td valign="top" align="center" rowspan="1" colspan="1">85.2252</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">4</td><td valign="top" align="left" rowspan="1" colspan="1">Cuneus_R&#x02014;Supp_Motor_Area_R</td><td valign="top" align="center" rowspan="1" colspan="1">82.1950</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">5</td><td valign="top" align="left" rowspan="1" colspan="1">Cuneus_R&#x02014;Frontal_Sup_Orb_L</td><td valign="top" align="center" rowspan="1" colspan="1">79.3418</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">Fusiform_R&#x02014;Frontal_Inf_Orb_L</td><td valign="top" align="center" rowspan="1" colspan="1">75.7217</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">Cuneus_R&#x02014;Putamen_R</td><td valign="top" align="center" rowspan="1" colspan="1">74.0230</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">8</td><td valign="top" align="left" rowspan="1" colspan="1">Cuneus_R&#x02014;Insula_R</td><td valign="top" align="center" rowspan="1" colspan="1">61.9906</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">9</td><td valign="top" align="left" rowspan="1" colspan="1">Supp_Motor_Area_L&#x02014;Putamen_R</td><td valign="top" align="center" rowspan="1" colspan="1">39.3506</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">10</td><td valign="top" align="left" rowspan="1" colspan="1">Supp_Motor_Area_L&#x02014;Frontal_Sup_Orb_L</td><td valign="top" align="center" rowspan="1" colspan="1">39.1733</td></tr></tbody></table></table-wrap><fig position="float" id="F7"><label>FIGURE 7</label><caption><p>Top 10 brain connections between the MCI and NMCI groups. The 10 brain connections with higher weights in the MCI and NMCI classifications are represented by yellow dots. These connections include Fusiform_R&#x02014;Precentral_R, Fusiform_R&#x02014;Occipital_Sup_L, Fusiform_R&#x02014;Temporal_Mid_L, Cuneus_R&#x02014;Supp_Motor_Area_R, Cuneus_R&#x02014;Frontal_Sup_Orb_L, Fusiform_R&#x02014;Frontal_Inf_Orb_L, Cuneus_R&#x02014;Putamen_R, Cuneus_R&#x02014;Insula_R, Supp_Motor_Area_L&#x02014;Putamen_R, and Supp_Motor_Area_L&#x02014;Frontal_Sup_Orb_L.</p></caption><graphic xlink:href="fnagi-17-1527323-g007" position="float"/></fig></sec></sec><sec sec-type="discussion" id="S4"><title>4 Discussion</title><p>MCI increasingly poses a significant economic and social burden on patients with unilateral MCA steno-occlusive disease. Early diagnosis of MCI is of paramount importance for maximizing treatment effectiveness. Therefore, in this study, we proposed a multimodal imaging technique based on rs-fMRI, DTI, 3D-T1WI, and demographic characteristics for the identification of MCI in patients with unilateral MCA steno-occlusive disease. This technique obtains the most discriminative features of MCI and NMCI by combining the information of multiple modalities, obtains a better classification result, and improves the ACC of diagnosis. Efficient automated diagnosis facilitates early and accurate detection of MCI and timely intervention and treatment to delay or prevent disease progression.</p><p>Overall, the results of the present study indicate that the discrepant regions are widely distributed throughout the brain, including the Olfactory_L, Frontal_Sup_Medial_L, Calcarine_L, Frontal_Inf_Oper_L, Temporal_Pole_Mid_L, and Amygdala_L (<xref rid="B58" ref-type="bibr">Whitwell et al., 2007</xref>; <xref rid="B5" ref-type="bibr">Bozzali et al., 2006</xref>; <xref rid="B15" ref-type="bibr">H&#x000e4;m&#x000e4;l&#x000e4;inen et al., 2007</xref>; <xref rid="B7" ref-type="bibr">Chen et al., 2020</xref>). Previous studies have shown that intracranial stenosis (ICS) is associated with cognitive impairment, independent of vascular risk factors. This association may be attributed to subtle cortical and subcortical ischaemic damage, including increased resistance and reduced vascular reactivity of small vessels, or to reduction in anatomic connectivity and perfusion deficits secondary to ICS (<xref rid="B18" ref-type="bibr">Hilal et al., 2015</xref>; <xref rid="B19" ref-type="bibr">Hilal et al., 2017</xref>). All of above regions are commonly involved in MCI-related pathology; as such, their involvement may represent the early features of MCI pathology. The frontal lobe has the largest number of differential brain regions, which primarily include the centers of higher cortical nerves and motor speech centers. Several studies have shown that reduced frontal lobe volume compared to the normal group can be found at the MCI stage (<xref rid="B62" ref-type="bibr">Yener et al., 2016</xref>). In addition, the present study found that the inner olfactory cortex had the greatest weight and showed the most significant differences. Consistent with our findings, the olfactory decline caused by lesions in this region was previously found to be highly discriminatory for MCI (<xref rid="B45" ref-type="bibr">Roberts et al., 2016</xref>). Previous studies have shown that volume loss in the internal olfactory cortex is greater than hippocampal volume loss in patients with MCI (<xref rid="B41" ref-type="bibr">Pennanen et al., 2004</xref>), suggesting that the volume of the internal olfactory cortex is better able to differentiate between patients with MCI and controls than the hippocampal volume. One meta-analysis found a reduced gray matter volumes in patients with MCI compared to controls, most notably in the hippocampus, parahippocampal gyrus, and amygdala (<xref rid="B44" ref-type="bibr">Raine and Rao, 2022</xref>). In addition, at the neuropsychological level, an 18F-AV-1451 PET imaging study showed that tau proteins accumulated only in the internal olfactory cortex of patients with MCI (<xref rid="B9" ref-type="bibr">Cho et al., 2016</xref>).</p><p>This study also found that the brain connections that distinguished MCI from NMCI were primarily located in the fusiform R, cuneus R, and supplementary motor area L. The fusiform gyrus, the largest component of the ventral temporal cortex, plays a key role in visual categorization and is associated with high-level tasks related to visual processing (<xref rid="B14" ref-type="bibr">Grill-Spector and Weiner, 2014</xref>; <xref rid="B40" ref-type="bibr">Palejwala et al., 2020</xref>). In one analysis based on fMRI data, <xref rid="B51" ref-type="bibr">Spagna et al. (2021)</xref> found that the fusiform gyrus is associated with visual images, and that damage to it causes deficits in the construction of visual images, which in turn leads to a decrease in visual memory capacity (<xref rid="B4" ref-type="bibr">Bartolomeo et al., 2020</xref>; <xref rid="B52" ref-type="bibr">Tabi et al., 2022</xref>). The cuneus is a part of the occipital lobe, forming the primary visual cortex along with the surrounding cortex, which is involved in the integration of visual space and visual motion, and plays an important role in non-visual functions such as language and memory (<xref rid="B53" ref-type="bibr">Tanglay et al., 2022</xref>; <xref rid="B39" ref-type="bibr">Palejwala et al., 2021</xref>). The supplementary motor area forms part of the secondary motor system and is mainly responsible for somatosensory motor functions, with extensive fiber connections to the cingulate gyrus and frontal lobe, playing a key role in the integration of functions as well as emotions, behaviors, and cognitive functions (<xref rid="B31" ref-type="bibr">Leisman et al., 2016</xref>). <xref rid="B60" ref-type="bibr">Ye et al. (2019)</xref> used multivariate distance matrix regression to investigate abnormal connectivity patterns of the SBN, identifying abnormalities in the supplementary motor area in patients with MCI. These findings are consistent with those of previous studies, showing that brain regions and connections may play important roles in the development of cognitive impairment in patients with unilateral MCA occlusion. Exploring the exact mechanism of these regions in cognitive dysfunction will help to enrich our knowledge of the developmental process of this disease, and to provide a scientific basis for future clinical practice.</p><p>This study adopted a fully automatic brain segmentation software, which has high automation, high data consistency, fast analysis, and high accuracy and can ensure data analysis of large sample sizes, thereby ensuring the reliability of the research results. Further, we propose a framework integrating the topological features of a multimodal (four modalities) FBN and SBN, and design a multi-channel graph attention network to extract the topological features of multimodal brain networks to allow using attention mechanisms. This proposed model can detect subtle pathological physiological abnormalities in the brain more accurately than a single modality, thus improving diagnostic efficiency. Intracranial arterial stenosis has attracted significant attention owing to its high incidence in the Asian population and the potential for long-term adverse events such as stroke. Many studies have further focused on the effects of carotid artery stenosis on cognition. However, studies on the impact of intracranial vessels on cognition are rare, and no unified conclusions have yet been reached. As such, this study is innovative.</p><p>This study has some limitations. First, the small sample size and imbalance between the MCI and NMCI groups are limitations of the current study, limitations commonly attributed to single-center studies and disease specificity. We used statistical methods such as 5-fold cross-validation to minimize the impact and ensure the accuracy of the results. Second, we recognize the limitations of single-center studies and plan to increase the sample size in the future through collaborative multi-center studies to ensure that device parameters are consistent across centers to further improve the reliability and generalisability of the data. Third, we plan to conduct a long-term follow-up study to validate and extend the results of the current study. With a long-term follow-up, we can gain a deeper understanding of disease progression, observe the performance of the model at different time points, comprehensively assess its validity and generalisability, and predict future cognitive decline.</p></sec><sec sec-type="conclusion" id="S5"><title>5 Conclusion</title><p>In this study, we propose a multimodality (four-modalities) FBN and SBN fusion framework that can effectively fuse rs-fMRI, DTI, 3D-T1WI, and demographic characteristic data. This framework can also embed the characteristics of different modalities into the fusion model, which can effectively extract complex and complementary topological structural information from the brain network. The experimental results show that our method not only achieves good performance in the diagnosis of MCI, but can also effectively identify disease-related brain areas and connections, which provides a promising prospect for the diagnosis of auxiliary brain diseases. Temporal changes in brain activity are extremely important in the analysis brain networks. Therefore, the investigation of brain networks&#x02019; spatiotemporal evolution, leveraging their dynamic transformational properties, stands as a promising research pathway for deepening our understanding of brain disease mechanisms. However, the proposed method primarily studies static brain networks and ignores the dynamic attributes of the brain. In future research, we will design methods to extract the spatio-temporal features of dynamic brain networks to comprehensively capture the evolutionary information of brain networks.</p></sec></body><back><sec sec-type="data-availability" id="S6"><title>Data availability statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.</p></sec><sec sec-type="ethics-statement" id="S7"><title>Ethics statement</title><p>The studies involving humans were approved by the Ethics Committee of Qilu Hospital of Shandong University. The studies were conducted in accordance with the local legislation and institutional requirements. Informed consent was obtained from all participants.</p></sec><sec sec-type="author-contributions" id="S8"><title>Author contributions</title><p>ZY: Validation, Writing &#x02013; original draft, Writing &#x02013; review and editing, Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration. ZH: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Resources, Validation, Writing &#x02013; original draft. CL: Formal analysis, Investigation, Methodology, Project administration, Software, Supervision, Validation, Visualization, Writing &#x02013; original draft. SL: Investigation, Methodology, Project administration, Software, Supervision, Validation, Visualization, Writing &#x02013; original draft. QR: Conceptualization, Data curation, Formal analysis, Methodology, Project administration, Supervision, Writing &#x02013; original draft. XX: Conceptualization, Data curation, Methodology, Project administration, Writing &#x02013; original draft. QJ: Conceptualization, Funding acquisition, Project administration, Supervision, Writing &#x02013; review and editing. DZ: Methodology, Project administration, Software, Supervision, Validation, Writing &#x02013; review and editing. QZ: Conceptualization, Funding acquisition, Methodology, Project administration, Resources, Supervision, Validation, Visualization, Writing &#x02013; review and editing. XM: Conceptualization, Data curation, Funding acquisition, Methodology, Project administration, Resources, Supervision, Writing &#x02013; review and editing.</p></sec><sec sec-type="COI-statement" id="S10"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec id="S11"><title>Generative AI statement</title><p>The authors declare that no Generative AI was used in the creation of this manuscript.</p></sec><sec sec-type="disclaimer" id="S12"><title>Publisher&#x02019;s note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><sec sec-type="supplementary-material" id="S13"><title>Supplementary material</title><p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fnagi.2025.1527323/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fnagi.2025.1527323/full#supplementary-material</ext-link></p><supplementary-material id="DS1" position="float" content-type="local-data"><media xlink:href="Data_Sheet_1.docx"/></supplementary-material></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adeloye</surname><given-names>D.</given-names></name><name><surname>Auta</surname><given-names>A.</given-names></name><name><surname>Fawibe</surname><given-names>A.</given-names></name><name><surname>Gadanya</surname><given-names>M.</given-names></name><name><surname>Ezeigwe</surname><given-names>N.</given-names></name><name><surname>Mpazanje</surname><given-names>R. G.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Current prevalence pattern of tobacco smoking in Nigeria: A systematic review and meta-analysis.</article-title>
<source><italic>BMC Public Health</italic></source>
<volume>19</volume>:<issue>1719</issue>. <pub-id pub-id-type="doi">10.1186/s12889-019-8010-8</pub-id>
<pub-id pub-id-type="pmid">31864324</pub-id>
</mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><collab>American Diabetes Association</collab> (<year>2020</year>). <article-title>2. classification and diagnosis of diabetes: standards of medical care in diabetes-2020.</article-title>
<source><italic>Diabetes Care</italic></source>
<volume>43</volume>
<fpage>S14</fpage>&#x02013;<lpage>S31</lpage>. <pub-id pub-id-type="doi">10.2337/dc20-S002</pub-id>
<pub-id pub-id-type="pmid">31862745</pub-id>
</mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>J.</given-names></name><name><surname>Friston</surname><given-names>K. J.</given-names></name></person-group> (<year>2005</year>). <article-title>Unified segmentation.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>26</volume>
<fpage>839</fpage>&#x02013;<lpage>851</lpage>.<pub-id pub-id-type="pmid">15955494</pub-id>
</mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartolomeo</surname><given-names>P.</given-names></name><name><surname>Hajhajate</surname><given-names>D.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Spagna</surname><given-names>A.</given-names></name></person-group> (<year>2020</year>). <article-title>Assessing the causal role of early visual areas in visual mental imagery.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>21</volume>:<issue>517</issue>.</mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bozzali</surname><given-names>M.</given-names></name><name><surname>Filippi</surname><given-names>M.</given-names></name><name><surname>Magnani</surname><given-names>G.</given-names></name><name><surname>Cercignani</surname><given-names>M.</given-names></name><name><surname>Franceschi</surname><given-names>M.</given-names></name><name><surname>Schiatti</surname><given-names>E.</given-names></name><etal/></person-group> (<year>2006</year>). <article-title>The contribution of voxel-based morphometry in staging patients with mild cognitive impairment.</article-title>
<source><italic>Neurology</italic></source>
<volume>67</volume>
<fpage>453</fpage>&#x02013;<lpage>460</lpage>.<pub-id pub-id-type="pmid">16894107</pub-id>
</mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao-Gan</surname><given-names>Y.</given-names></name><name><surname>Yu-Feng</surname><given-names>Z.</given-names></name></person-group> (<year>2010</year>). <article-title>Dparsf: A MATLAB toolbox for &#x0201c;pipeline&#x0201d; data analysis of resting-state fMRI.</article-title>
<source><italic>Front. Syst. Neurosci.</italic></source>
<volume>4</volume>:<issue>13</issue>. <pub-id pub-id-type="doi">10.3389/fnsys.2010.00013</pub-id>
<pub-id pub-id-type="pmid">20577591</pub-id>
</mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Yan</surname><given-names>Y.</given-names></name><name><surname>Gu</surname><given-names>L.</given-names></name><name><surname>Gao</surname><given-names>L.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name></person-group> (<year>2020</year>). <article-title>Electrophysiological processes on motor imagery mediate the association between increased gray matter volume and cognition in amnestic mild cognitive impairment.</article-title>
<source><italic>Brain Topogr.</italic></source>
<volume>33</volume>
<fpage>255</fpage>&#x02013;<lpage>266</lpage>. <pub-id pub-id-type="doi">10.1007/s10548-019-00742-8</pub-id>
<pub-id pub-id-type="pmid">31691911</pub-id>
</mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Y. W.</given-names></name><name><surname>Chen</surname><given-names>T. F.</given-names></name><name><surname>Chiu</surname><given-names>M. J.</given-names></name></person-group> (<year>2017</year>). <article-title>From mild cognitive impairment to subjective cognitive decline: Conceptual and methodological evolution.</article-title>
<source><italic>Neuropsychiatr. Dis. Treat.</italic></source>
<volume>13</volume>
<fpage>491</fpage>&#x02013;<lpage>498</lpage>. <pub-id pub-id-type="doi">10.2147/NDT.S123428</pub-id>
<pub-id pub-id-type="pmid">28243102</pub-id>
</mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>H.</given-names></name><name><surname>Choi</surname><given-names>J. Y.</given-names></name><name><surname>Hwang</surname><given-names>M. S.</given-names></name><name><surname>Lee</surname><given-names>J. H.</given-names></name><name><surname>Kim</surname><given-names>Y. J.</given-names></name><name><surname>Lee</surname><given-names>H. M.</given-names></name></person-group> (<year>2016</year>). <article-title>Tau PET in Alzheimer disease and mild cognitive impairment.</article-title>
<source><italic>Neurology</italic></source>
<volume>87</volume>
<fpage>375</fpage>&#x02013;<lpage>383</lpage>.<pub-id pub-id-type="pmid">27358341</pub-id>
</mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortes</surname><given-names>C.</given-names></name><name><surname>Vapnik</surname><given-names>V.</given-names></name></person-group> (<year>1995</year>). <article-title>Support-vector networks.</article-title>
<source><italic>Mach. Learn.</italic></source>
<volume>20</volume>
<fpage>273</fpage>&#x02013;<lpage>297</lpage>.</mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craddock</surname><given-names>R. C.</given-names></name><name><surname>James</surname><given-names>G. A.</given-names></name><name><surname>Holtzheimer</surname><given-names>P. E.</given-names></name><name><surname>Hu</surname><given-names>X. P.</given-names></name><name><surname>Mayberg</surname><given-names>H. S.</given-names></name></person-group> (<year>2012</year>). <article-title>A whole brain fMRI atlas generated via spatially constrained spectral clustering.</article-title>
<source><italic>Hum. Brain Mapp.</italic></source>
<volume>33</volume>
<fpage>1914</fpage>&#x02013;<lpage>1928</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.21333</pub-id>
<pub-id pub-id-type="pmid">21769991</pub-id>
</mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cui</surname><given-names>Z.</given-names></name><name><surname>Zhong</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>P.</given-names></name><name><surname>He</surname><given-names>Y.</given-names></name><name><surname>Gong</surname><given-names>G.</given-names></name></person-group> (<year>2013</year>). <article-title>PANDA: A pipeline toolbox for analyzing brain diffusion images.</article-title>
<source><italic>Front. Hum. Neurosci.</italic></source>
<volume>7</volume>:<issue>42</issue>. <pub-id pub-id-type="doi">10.3389/fnhum.2013.00042</pub-id>
<pub-id pub-id-type="pmid">23439846</pub-id>
</mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dempsey</surname><given-names>R. J.</given-names></name><name><surname>Varghese</surname><given-names>T.</given-names></name><name><surname>Jackson</surname><given-names>D. C.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Meshram</surname><given-names>N. H.</given-names></name><name><surname>Mitchell</surname><given-names>C. C.</given-names></name></person-group> (<year>2018</year>). <article-title>Carotid atherosclerotic plaque instability and cognition determined by ultrasound-measured plaque strain in asymptomatic patients with significant stenosis.</article-title>
<source><italic>J. Neurosurg.</italic></source>
<volume>128</volume>
<fpage>111</fpage>&#x02013;<lpage>119</lpage>. <pub-id pub-id-type="doi">10.3171/2016.10.JNS161299</pub-id>
<pub-id pub-id-type="pmid">28298048</pub-id>
</mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K.</given-names></name><name><surname>Weiner</surname><given-names>K. S.</given-names></name></person-group> (<year>2014</year>). <article-title>The functional architecture of the ventral temporal cortex and its role in categorization.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>15</volume>
<fpage>536</fpage>&#x02013;<lpage>548</lpage>.<pub-id pub-id-type="pmid">24962370</pub-id>
</mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>H&#x000e4;m&#x000e4;l&#x000e4;inen</surname><given-names>A.</given-names></name><name><surname>Tervo</surname><given-names>S.</given-names></name><name><surname>Grau-Olivares</surname><given-names>M.</given-names></name><name><surname>Niskanen</surname><given-names>E.</given-names></name><name><surname>Pennanen</surname><given-names>C.</given-names></name><name><surname>Huuskonen</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2007</year>). <article-title>Voxel-based morphometry to detect brain atrophy in progressive mild cognitive impairment.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>37</volume>
<fpage>1122</fpage>&#x02013;<lpage>1131</lpage>.<pub-id pub-id-type="pmid">17683950</pub-id>
</mixed-citation></ref><ref id="B16"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>W. L.</given-names></name><name><surname>Ying</surname><given-names>R.</given-names></name><name><surname>Leskovec</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>). &#x0201c;<article-title>Inductive representation learning on large graphs</article-title>,&#x0201d; in <source><italic>Proceedings of the 31st International Conference on Neural Information Processing Systems</italic></source>, (<publisher-loc>Long Beach, CA</publisher-loc>: <publisher-name>Curran Associates Inc</publisher-name>).</mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>X.</given-names></name><name><surname>Bao</surname><given-names>Y.</given-names></name><name><surname>Guo</surname><given-names>Y.</given-names></name><name><surname>Yu</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>D.</given-names></name><name><surname>Risacher</surname><given-names>S. L.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Multi-modal neuroimaging feature selection with consistent metric constraint for diagnosis of Alzheimer&#x02019;s disease.</article-title>
<source><italic>Med. Image Anal.</italic></source>
<volume>60</volume>:<issue>101625</issue>. <pub-id pub-id-type="doi">10.1016/j.media.2019.101625</pub-id>
<pub-id pub-id-type="pmid">31841947</pub-id>
</mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hilal</surname><given-names>S.</given-names></name><name><surname>Saini</surname><given-names>M.</given-names></name><name><surname>Tan</surname><given-names>C. S.</given-names></name><name><surname>Catindig</surname><given-names>J. A.</given-names></name><name><surname>Dong</surname><given-names>Y. H.</given-names></name><name><surname>Holandez</surname><given-names>R. L.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>Intracranial stenosis, cerebrovascular diseases, and cognitive impairment in chinese.</article-title>
<source><italic>Alzheimer Dis. Assoc. Disord.</italic></source>
<volume>29</volume>
<fpage>12</fpage>&#x02013;<lpage>17</lpage>.<pub-id pub-id-type="pmid">24731981</pub-id>
</mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hilal</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name><name><surname>Ikram</surname><given-names>M. K.</given-names></name><name><surname>Vrooman</surname><given-names>H.</given-names></name><name><surname>Venketasubramanian</surname><given-names>N.</given-names></name><name><surname>Chen</surname><given-names>C.</given-names></name></person-group> (<year>2017</year>). <article-title>Intracranial stenosis in cognitive impairment and dementia.</article-title>
<source><italic>J. Cereb. Blood Flow Metab.</italic></source>
<volume>37</volume>
<fpage>2262</fpage>&#x02013;<lpage>2269</lpage>.<pub-id pub-id-type="pmid">27488908</pub-id>
</mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>K. L.</given-names></name><name><surname>Chang</surname><given-names>T. Y.</given-names></name><name><surname>Ho</surname><given-names>M. Y.</given-names></name><name><surname>Chen</surname><given-names>W. H.</given-names></name><name><surname>Yeh</surname><given-names>M. Y.</given-names></name><name><surname>Chang</surname><given-names>Y. J.</given-names></name></person-group> (<year>2018</year>). <article-title>The correlation of asymmetrical functional connectivity with cognition and reperfusion in carotid stenosis patients.</article-title>
<source><italic>Neuroimage Clin.</italic></source>
<volume>20</volume>
<fpage>476</fpage>&#x02013;<lpage>484</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2018.08.011</pub-id>
<pub-id pub-id-type="pmid">30128286</pub-id>
</mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Israelsson</surname><given-names>H.</given-names></name><name><surname>Carlberg</surname><given-names>B.</given-names></name><name><surname>Wikkels&#x000f6;</surname><given-names>C.</given-names></name><name><surname>Laurell</surname><given-names>K.</given-names></name><name><surname>Kahlon</surname><given-names>B.</given-names></name><name><surname>Leijon</surname><given-names>G.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Vascular risk factors in INPH: A prospective case-control study (the INPH-CRasH study).</article-title>
<source><italic>Neurology</italic></source>
<volume>88</volume>
<fpage>577</fpage>&#x02013;<lpage>585</lpage>.<pub-id pub-id-type="pmid">28062721</pub-id>
</mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaraj</surname><given-names>D.</given-names></name><name><surname>Agerskov</surname><given-names>S.</given-names></name><name><surname>Rabiei</surname><given-names>K.</given-names></name><name><surname>Marlow</surname><given-names>T.</given-names></name><name><surname>Jensen</surname><given-names>C.</given-names></name><name><surname>Guo</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Vascular factors in suspected normal pressure hydrocephalus: A population-based study.</article-title>
<source><italic>Neurology</italic></source>
<volume>86</volume>
<fpage>592</fpage>&#x02013;<lpage>599</lpage>. <pub-id pub-id-type="doi">10.1212/WNL.0000000000002369</pub-id>
<pub-id pub-id-type="pmid">26773072</pub-id>
</mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>H.</given-names></name><name><surname>van Zijl</surname><given-names>P. C.</given-names></name><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Pearlson</surname><given-names>G. D.</given-names></name><name><surname>Mori</surname><given-names>S.</given-names></name></person-group> (<year>2006</year>). <article-title>DtiStudio: resource program for diffusion tensor computation and fiber bundle tracking.</article-title>
<source><italic>Comput. Methods Programs Biomed.</italic></source>
<volume>81</volume>
<fpage>106</fpage>&#x02013;<lpage>116</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2005.08.004</pub-id>
<pub-id pub-id-type="pmid">16413083</pub-id>
</mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawahara</surname><given-names>J.</given-names></name><name><surname>Brown</surname><given-names>C. J.</given-names></name><name><surname>Miller</surname><given-names>S. P.</given-names></name><name><surname>Booth</surname><given-names>B. G.</given-names></name><name><surname>Chau</surname><given-names>V.</given-names></name><name><surname>Grunau</surname><given-names>R. E.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>BrainNetCNN: Convolutional neural networks for brain networks; towards predicting neurodevelopment.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>146</volume>
<fpage>1038</fpage>&#x02013;<lpage>1049</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.09.046</pub-id>
<pub-id pub-id-type="pmid">27693612</pub-id>
</mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kipf</surname><given-names>T. N.</given-names></name><name><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Semi-supervised classification with graph convolutional networks.</article-title>
<source><italic>arXiv [Preprint]</italic></source>
<pub-id pub-id-type="doi">10.48550/arXiv.1609.02907</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langa</surname><given-names>K. M.</given-names></name><name><surname>Levine</surname><given-names>D. A.</given-names></name></person-group> (<year>2014</year>). <article-title>The diagnosis and management of mild cognitive impairment: A clinical review.</article-title>
<source><italic>JAMA</italic></source>
<volume>312</volume>
<fpage>2551</fpage>&#x02013;<lpage>2561</lpage>.<pub-id pub-id-type="pmid">25514304</pub-id>
</mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Bihan</surname><given-names>D.</given-names></name></person-group> (<year>2003</year>). <article-title>Looking into the functional architecture of the brain with diffusion MRI.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>4</volume>
<fpage>469</fpage>&#x02013;<lpage>480</lpage>.<pub-id pub-id-type="pmid">12778119</pub-id>
</mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Y.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Haffner</surname><given-names>P.</given-names></name></person-group> (<year>1998</year>). <article-title>Gradient-based learning applied to document recognition.</article-title>
<source><italic>Proc. IEEE</italic></source>
<volume>86</volume>
<fpage>2278</fpage>&#x02013;<lpage>2324</lpage>.</mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J.</given-names></name><name><surname>Lee</surname><given-names>I.</given-names></name><name><surname>Kang</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>Self-attention graph pooling.</article-title>
<source><italic>arXiv [Preprint]</italic></source>
<pub-id pub-id-type="doi">10.48550/arXiv.1904.08082</pub-id></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lei</surname><given-names>B.</given-names></name><name><surname>Cheng</surname><given-names>N.</given-names></name><name><surname>Frangi</surname><given-names>A. F.</given-names></name><name><surname>Wei</surname><given-names>Y.</given-names></name><name><surname>Yu</surname><given-names>B.</given-names></name><name><surname>Liang</surname><given-names>L.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Auto-weighted centralised multi-task learning via integrating functional and structural connectivity for subjective cognitive decline diagnosis.</article-title>
<source><italic>Med. Image Anal.</italic></source>
<volume>74</volume>:<issue>102248</issue>. <pub-id pub-id-type="doi">10.1016/j.media.2021.102248</pub-id>
<pub-id pub-id-type="pmid">34597938</pub-id>
</mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leisman</surname><given-names>G.</given-names></name><name><surname>Moustafa</surname><given-names>A. A.</given-names></name><name><surname>Shafir</surname><given-names>T.</given-names></name></person-group> (<year>2016</year>). <article-title>Thinking, walking, talking: integratory motor and cognitive brain function.</article-title>
<source><italic>Front. Public Health</italic></source>
<volume>4</volume>:<issue>94</issue>. <pub-id pub-id-type="doi">10.3389/fpubh.2016.00094</pub-id>
<pub-id pub-id-type="pmid">27252937</pub-id>
</mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Sun</surname><given-names>Y.</given-names></name><name><surname>Sheng</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>H.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Abnormal resting-state functional connectivity strength in mild cognitive impairment and its conversion to Alzheimer&#x02019;s disease.</article-title>
<source><italic>Neural Plast.</italic></source>
<volume>2016</volume>:<issue>4680972</issue>. <pub-id pub-id-type="doi">10.1155/2016/4680972</pub-id>
<pub-id pub-id-type="pmid">26843991</pub-id>
</mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Yu</surname><given-names>C.</given-names></name><name><surname>Duan</surname><given-names>Y.</given-names></name><name><surname>Zhuo</surname><given-names>J.</given-names></name><name><surname>Cui</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Impaired parahippocampus connectivity in mild cognitive impairment and Alzheimer&#x02019;s disease.</article-title>
<source><italic>J. Alzheimers Dis.</italic></source>
<volume>49</volume>
<fpage>1051</fpage>&#x02013;<lpage>1064</lpage>.<pub-id pub-id-type="pmid">26599055</pub-id>
</mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Yue</surname><given-names>L.</given-names></name><name><surname>Xiao</surname><given-names>S.</given-names></name><name><surname>Yang</surname><given-names>W.</given-names></name><name><surname>Shen</surname><given-names>D.</given-names></name><name><surname>Liu</surname><given-names>M.</given-names></name></person-group> (<year>2022</year>). <article-title>Assessing clinical progression from subjective cognitive decline to mild cognitive impairment with incomplete multi-modal neuroimages.</article-title>
<source><italic>Med. Image Anal.</italic></source>
<volume>75</volume>:<issue>102266</issue>. <pub-id pub-id-type="doi">10.1016/j.media.2021.102266</pub-id>
<pub-id pub-id-type="pmid">34700245</pub-id>
</mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>D.</given-names></name><name><surname>Li</surname><given-names>F.</given-names></name><name><surname>Zhou</surname><given-names>A.</given-names></name><name><surname>Wang</surname><given-names>F.</given-names></name><name><surname>Zuo</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Montreal cognitive assessment in detecting cognitive impairment in Chinese elderly individuals: A population-based study.</article-title>
<source><italic>J. Geriatr. Psychiatry Neurol.</italic></source>
<volume>24</volume>
<fpage>184</fpage>&#x02013;<lpage>190</lpage>.<pub-id pub-id-type="pmid">22228824</pub-id>
</mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maaten</surname><given-names>L. V. D.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2008</year>). <article-title>Visualizing data using t-SNE.</article-title>
<source><italic>J. Mach. Learn. Res.</italic></source>
<volume>9</volume>
<fpage>2579</fpage>&#x02013;<lpage>2605</lpage>.</mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasreddine</surname><given-names>Z. S.</given-names></name><name><surname>Phillips</surname><given-names>N. A.</given-names></name><name><surname>B&#x000c9;dirian</surname><given-names>V.</given-names></name><name><surname>Charbonneau</surname><given-names>S.</given-names></name><name><surname>Whitehead</surname><given-names>V.</given-names></name><name><surname>Collin</surname><given-names>I.</given-names></name><etal/></person-group> (<year>2005</year>). <article-title>The montreal cognitive assessment, MoCA: A brief screening tool for mild cognitive impairment.</article-title>
<source><italic>J. Am. Geriatr. Soc.</italic></source>
<volume>53</volume>
<fpage>695</fpage>&#x02013;<lpage>699</lpage>. <pub-id pub-id-type="doi">10.1111/j.1532-5415.2005.53221.x</pub-id>
<pub-id pub-id-type="pmid">15817019</pub-id>
</mixed-citation></ref><ref id="B38"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Page</surname><given-names>L.</given-names></name><name><surname>Brin</surname><given-names>S.</given-names></name><name><surname>Motwani</surname><given-names>R.</given-names></name><name><surname>Winograd</surname><given-names>T.</given-names></name></person-group> (<year>1999</year>). &#x0201c;<article-title>The pagerank citation ranking: Bringing order to the web</article-title>,&#x0201d; in <source><italic>Proceedings of the The Web Conference</italic></source>, (<publisher-name>ACM</publisher-name>).</mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palejwala</surname><given-names>A. H.</given-names></name><name><surname>Dadario</surname><given-names>N. B.</given-names></name><name><surname>Young</surname><given-names>I. M.</given-names></name><name><surname>O&#x02019;connor</surname><given-names>K.</given-names></name><name><surname>Briggs</surname><given-names>R. G.</given-names></name><name><surname>Conner</surname><given-names>A. K.</given-names></name></person-group> (<year>2021</year>). <article-title>Anatomy and white matter connections of the lingual gyrus and cuneus.</article-title>
<source><italic>World Neurosurg.</italic></source>
<volume>151</volume>
<fpage>e426</fpage>&#x02013;<lpage>e437</lpage>.<pub-id pub-id-type="pmid">33894399</pub-id>
</mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palejwala</surname><given-names>A. H.</given-names></name><name><surname>O&#x02019;Connor</surname><given-names>K. P.</given-names></name><name><surname>Milton</surname><given-names>C. K.</given-names></name><name><surname>Anderson</surname><given-names>C.</given-names></name><name><surname>Pelargos</surname><given-names>P.</given-names></name><name><surname>Briggs</surname><given-names>R. G.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Anatomy and white matter connections of the fusiform gyrus.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>10</volume>:<issue>13489</issue>.</mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pennanen</surname><given-names>C.</given-names></name><name><surname>Kivipelto</surname><given-names>M.</given-names></name><name><surname>Tuomainen</surname><given-names>S.</given-names></name><name><surname>Hartikainen</surname><given-names>P.</given-names></name><name><surname>H&#x000e4;nninen</surname><given-names>T.</given-names></name><name><surname>Laakso</surname><given-names>M. P.</given-names></name></person-group> (<year>2004</year>). <article-title>Hippocampus and entorhinal cortex in mild cognitive impairment and early AD.</article-title>
<source><italic>Neurobiol. Aging</italic></source>
<volume>25</volume>
<fpage>303</fpage>&#x02013;<lpage>310</lpage>. <pub-id pub-id-type="doi">10.1016/S0197-4580(03)00084-8</pub-id>
<pub-id pub-id-type="pmid">15123335</pub-id>
</mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>R. C.</given-names></name><name><surname>Lopez</surname><given-names>O.</given-names></name><name><surname>Armstrong</surname><given-names>M. J.</given-names></name><name><surname>Getchius</surname><given-names>T. S. D.</given-names></name><name><surname>Ganguli</surname><given-names>M.</given-names></name><name><surname>Gloss</surname><given-names>D.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Practice guideline update summary: Mild cognitive impairment: report of the guideline development, dissemination, and implementation subcommittee of the american academy of neurology.</article-title>
<source><italic>Neurology</italic></source>
<volume>90</volume>
<fpage>126</fpage>&#x02013;<lpage>135</lpage>.<pub-id pub-id-type="pmid">29282327</pub-id>
</mixed-citation></ref><ref id="B43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname><given-names>T. C. C.</given-names></name><name><surname>Machado</surname><given-names>L.</given-names></name><name><surname>Bulgacov</surname><given-names>T. M.</given-names></name><name><surname>Rodrigues-J&#x000fa;nior</surname><given-names>A. L.</given-names></name><name><surname>Costa</surname><given-names>M. L. G.</given-names></name><name><surname>Ximenes</surname><given-names>R. C. C.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Is the montreal cognitive assessment (MoCA) screening superior to the mini-mental state examination (MMSE) in the detection of mild cognitive impairment (MCI) and Alzheimer&#x02019;s disease (AD) in the elderly?</article-title>
<source><italic>Int. Psychogeriatr.</italic></source>
<volume>31</volume>
<fpage>491</fpage>&#x02013;<lpage>504</lpage>. <pub-id pub-id-type="doi">10.1017/S1041610218001370</pub-id>
<pub-id pub-id-type="pmid">30426911</pub-id>
</mixed-citation></ref><ref id="B44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raine</surname><given-names>P. J.</given-names></name><name><surname>Rao</surname><given-names>H.</given-names></name></person-group> (<year>2022</year>). <article-title>Volume, density, and thickness brain abnormalities in mild cognitive impairment: an ALE meta-analysis controlling for age and education.</article-title>
<source><italic>Brain Imaging Behav.</italic></source>
<volume>16</volume>
<fpage>2335</fpage>&#x02013;<lpage>2352</lpage>. <pub-id pub-id-type="doi">10.1007/s11682-022-00659-0</pub-id>
<pub-id pub-id-type="pmid">35416608</pub-id>
</mixed-citation></ref><ref id="B45"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>R. O.</given-names></name><name><surname>Christianson</surname><given-names>T. J.</given-names></name><name><surname>Kremers</surname><given-names>W. K.</given-names></name><name><surname>Mielke</surname><given-names>M. M.</given-names></name><name><surname>Machulda</surname><given-names>M. M.</given-names></name><name><surname>Vassilaki</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Association between olfactory dysfunction and amnestic mild cognitive impairment and Alzheimer disease Dementia.</article-title>
<source><italic>JAMA Neurol.</italic></source>
<volume>73</volume>
<fpage>93</fpage>&#x02013;<lpage>101</lpage>.<pub-id pub-id-type="pmid">26569387</pub-id>
</mixed-citation></ref><ref id="B46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblatt</surname><given-names>F.</given-names></name></person-group> (<year>1958</year>). <article-title>The perceptron: A probabilistic model for information storage and organization in the brain.</article-title>
<source><italic>Psychol. Rev.</italic></source>
<volume>65</volume>
<fpage>386</fpage>&#x02013;<lpage>408</lpage>.<pub-id pub-id-type="pmid">13602029</pub-id>
</mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sexton</surname><given-names>C. E.</given-names></name><name><surname>Kalu</surname><given-names>U. G.</given-names></name><name><surname>Filippini</surname><given-names>N.</given-names></name><name><surname>Mackay</surname><given-names>C. E.</given-names></name><name><surname>Ebmeier</surname><given-names>K. P.</given-names></name></person-group> (<year>2011</year>). <article-title>A meta-analysis of diffusion tensor imaging in mild cognitive impairment and Alzheimer&#x02019;s disease.</article-title>
<source><italic>Neurobiol. Aging</italic></source>
<volume>32</volume>:<issue>2322.e5-18</issue>.</mixed-citation></ref><ref id="B48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shao</surname><given-names>W.</given-names></name><name><surname>Xiang</surname><given-names>S.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Huang</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>). <article-title>Hyper-graph based sparse canonical correlation analysis for the diagnosis of Alzheimer&#x02019;s disease from multi-dimensional genomic data.</article-title>
<source><italic>Methods</italic></source>
<volume>189</volume>
<fpage>86</fpage>&#x02013;<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1016/j.ymeth.2020.04.008</pub-id>
<pub-id pub-id-type="pmid">32360353</pub-id>
</mixed-citation></ref><ref id="B49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shim</surname><given-names>G.</given-names></name><name><surname>Choi</surname><given-names>K. Y.</given-names></name><name><surname>Kim</surname><given-names>D.</given-names></name><name><surname>Suh</surname><given-names>S.</given-names></name><name><surname>Lee</surname><given-names>S.</given-names></name><name><surname>Jeong</surname><given-names>H. G.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Predicting neurocognitive function with hippocampal volumes and DTI metrics in patients with Alzheimer&#x02019;s dementia and mild cognitive impairment.</article-title>
<source><italic>Brain Behav.</italic></source>
<volume>7</volume>:<issue>e00766</issue>. <pub-id pub-id-type="doi">10.1002/brb3.766</pub-id>
<pub-id pub-id-type="pmid">28948070</pub-id>
</mixed-citation></ref><ref id="B50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>X.</given-names></name><name><surname>Zhou</surname><given-names>F.</given-names></name><name><surname>Frangi</surname><given-names>A. F.</given-names></name><name><surname>Cao</surname><given-names>J.</given-names></name><name><surname>Xiao</surname><given-names>X.</given-names></name><name><surname>Lei</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2023</year>). <article-title>Multicenter and multichannel pooling GCN for Early AD diagnosis based on dual-modality fused brain network.</article-title>
<source><italic>IEEE Trans. Med. Imaging</italic></source>
<volume>42</volume>
<fpage>354</fpage>&#x02013;<lpage>367</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2022.3187141</pub-id>
<pub-id pub-id-type="pmid">35767511</pub-id>
</mixed-citation></ref><ref id="B51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spagna</surname><given-names>A.</given-names></name><name><surname>Hajhajate</surname><given-names>D.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Bartolomeo</surname><given-names>P.</given-names></name></person-group> (<year>2021</year>). <article-title>Visual mental imagery engages the left fusiform gyrus, but not the early visual cortex: A meta-analysis of neuroimaging evidence.</article-title>
<source><italic>Neurosci. Biobehav. Rev.</italic></source>
<volume>122</volume>
<fpage>201</fpage>&#x02013;<lpage>217</lpage>.<pub-id pub-id-type="pmid">33422567</pub-id>
</mixed-citation></ref><ref id="B52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabi</surname><given-names>Y. A.</given-names></name><name><surname>Maio</surname><given-names>M. R.</given-names></name><name><surname>Attaallah</surname><given-names>B.</given-names></name><name><surname>Dickson</surname><given-names>S.</given-names></name><name><surname>Drew</surname><given-names>D.</given-names></name><name><surname>Idris</surname><given-names>M. I.</given-names></name><etal/></person-group> (<year>2022</year>). <article-title>Vividness of visual imagery questionnaire scores and their relationship to visual short-term memory performance.</article-title>
<source><italic>Cortex</italic></source>
<volume>146</volume>
<fpage>186</fpage>&#x02013;<lpage>199</lpage>. <pub-id pub-id-type="doi">10.1016/j.cortex.2021.10.011</pub-id>
<pub-id pub-id-type="pmid">34894605</pub-id>
</mixed-citation></ref><ref id="B53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanglay</surname><given-names>O.</given-names></name><name><surname>Young</surname><given-names>I. M.</given-names></name><name><surname>Dadario</surname><given-names>N. B.</given-names></name><name><surname>Briggs</surname><given-names>R. G.</given-names></name><name><surname>Fonseka</surname><given-names>R. D.</given-names></name><name><surname>Dhanaraj</surname><given-names>V.</given-names></name><etal/></person-group> (<year>2022</year>). <article-title>Anatomy and white-matter connections of the precuneus.</article-title>
<source><italic>Brain Imaging Behav.</italic></source>
<volume>16</volume>
<fpage>574</fpage>&#x02013;<lpage>586</lpage>.<pub-id pub-id-type="pmid">34448064</pub-id>
</mixed-citation></ref><ref id="B54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Usarel</surname><given-names>C.</given-names></name><name><surname>Dokuzlar</surname><given-names>O.</given-names></name><name><surname>Aydin</surname><given-names>A. E.</given-names></name><name><surname>Soysal</surname><given-names>P.</given-names></name><name><surname>Isik</surname><given-names>A. T.</given-names></name></person-group> (<year>2019</year>). <article-title>The AD8 (Dementia Screening Interview) is a valid and reliable screening scale not only for dementia but also for mild cognitive impairment in the Turkish geriatric outpatients.</article-title>
<source><italic>Int. Psychogeriatr.</italic></source>
<volume>31</volume>
<fpage>223</fpage>&#x02013;<lpage>229</lpage>.<pub-id pub-id-type="pmid">29923472</pub-id>
</mixed-citation></ref><ref id="B55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veli&#x0010d;kovi&#x00107;</surname><given-names>P.</given-names></name><name><surname>Cucurull</surname><given-names>G.</given-names></name><name><surname>Casanova</surname><given-names>A.</given-names></name><name><surname>Romero</surname><given-names>A.</given-names></name><name><surname>Li&#x000f2;</surname><given-names>P.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group> (<year>2018</year>). <article-title>Graph attention networks.</article-title>
<source><italic>arXiv [Preprint]</italic></source>
<pub-id pub-id-type="doi">10.48550/arXiv.1710.10903</pub-id></mixed-citation></ref><ref id="B56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>T.</given-names></name><name><surname>Xiao</surname><given-names>F.</given-names></name><name><surname>Wu</surname><given-names>G.</given-names></name><name><surname>Fang</surname><given-names>J.</given-names></name><name><surname>Sun</surname><given-names>Z.</given-names></name><name><surname>Feng</surname><given-names>H.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Impairments in brain perfusion, metabolites, functional connectivity, and cognition in severe asymptomatic carotid stenosis patients: An integrated MRI study.</article-title>
<source><italic>Neural Plast.</italic></source>
<volume>2017</volume>:<issue>8738714</issue>. <pub-id pub-id-type="doi">10.1155/2017/8738714</pub-id>
<pub-id pub-id-type="pmid">28255464</pub-id>
</mixed-citation></ref><ref id="B57"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Zhu</surname><given-names>M.</given-names></name><name><surname>Bo</surname><given-names>D.</given-names></name><name><surname>Cui</surname><given-names>P.</given-names></name><name><surname>Shi</surname><given-names>C.</given-names></name><name><surname>Pei</surname><given-names>J.</given-names></name></person-group> (<year>2020</year>). &#x0201c;<article-title>AM-GCN: Adaptive multi-channel graph convolutional networks</article-title>,&#x0201d; in <source><italic>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining</italic></source>, (<publisher-name>ACM</publisher-name>). <pub-id pub-id-type="doi">10.1155/2022/2389560</pub-id>
</mixed-citation></ref><ref id="B58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitwell</surname><given-names>J. L.</given-names></name><name><surname>Przybelski</surname><given-names>S. A.</given-names></name><name><surname>Weigand</surname><given-names>S. D.</given-names></name><name><surname>Knopman</surname><given-names>D. S.</given-names></name><name><surname>Boeve</surname><given-names>B. F.</given-names></name><name><surname>Petersen</surname><given-names>R. C.</given-names></name><etal/></person-group> (<year>2007</year>). <article-title>3D maps from multiple MRI illustrate changing atrophy patterns as subjects progress from mild cognitive impairment to Alzheimer&#x02019;s disease.</article-title>
<source><italic>Brain</italic></source>
<volume>130</volume>
<fpage>1777</fpage>&#x02013;<lpage>1786</lpage>. <pub-id pub-id-type="doi">10.1093/brain/awm112</pub-id>
<pub-id pub-id-type="pmid">17533169</pub-id>
</mixed-citation></ref><ref id="B59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Ye</surname><given-names>C.</given-names></name><name><surname>Guo</surname><given-names>X.</given-names></name><name><surname>Wu</surname><given-names>T.</given-names></name><name><surname>Xiang</surname><given-names>Y.</given-names></name><name><surname>Ma</surname><given-names>T.</given-names></name></person-group> (<year>2024</year>). <article-title>Mapping multi-modal brain connectome for brain disorder diagnosis via cross-modal mutual learning.</article-title>
<source><italic>IEEE Trans. Med. Imaging</italic></source>
<volume>43</volume>
<fpage>108</fpage>&#x02013;<lpage>121</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2023.3294967</pub-id>
<pub-id pub-id-type="pmid">37440391</pub-id>
</mixed-citation></ref><ref id="B60"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>C.</given-names></name><name><surname>Mori</surname><given-names>S.</given-names></name><name><surname>Chan</surname><given-names>P.</given-names></name><name><surname>Ma</surname><given-names>T.</given-names></name></person-group> (<year>2019</year>). <article-title>Connectome-wide network analysis of white matter connectivity in Alzheimer&#x02019;s disease.</article-title>
<source><italic>Neuroimage Clin.</italic></source>
<volume>22</volume>:<issue>101690</issue>.</mixed-citation></ref><ref id="B61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>H.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>K.</given-names></name><name><surname>Kong</surname><given-names>Y.</given-names></name><name><surname>Yuan</surname><given-names>Y.</given-names></name></person-group> (<year>2023</year>). <article-title>RH-BrainFS: regional heterogeneous multimodal brain networks fusion strategy.</article-title>
<source><italic>Neural Inf. Processing Syst.</italic></source>
<volume>2589</volume>, <fpage>59286</fpage>&#x02013;<lpage>59303</lpage>.</mixed-citation></ref><ref id="B62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yener</surname><given-names>G. G.</given-names></name><name><surname>Emek-Sava&#x0015f;</surname><given-names>D. D.</given-names></name><name><surname>Lizio</surname><given-names>R.</given-names></name><name><surname>&#x000c7;avu&#x0015f;o&#x0011f;lu</surname><given-names>B.</given-names></name><name><surname>Carducci</surname><given-names>F.</given-names></name><name><surname>Ada</surname><given-names>E.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Frontal delta event-related oscillations relate to frontal volume in mild cognitive impairment and healthy controls.</article-title>
<source><italic>Int. J. Psychophysiol.</italic></source>
<volume>103</volume>
<fpage>110</fpage>&#x02013;<lpage>117</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijpsycho.2015.02.005</pub-id>
<pub-id pub-id-type="pmid">25660300</pub-id>
</mixed-citation></ref><ref id="B63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>J.</given-names></name><name><surname>Lam</surname><given-names>C. L. M.</given-names></name><name><surname>Lee</surname><given-names>T. M. C.</given-names></name></person-group> (<year>2017</year>). <article-title>White matter microstructural abnormalities in amnestic mild cognitive impairment: A meta-analysis of whole-brain and ROI-based studies.</article-title>
<source><italic>Neurosci. Biobehav. Rev.</italic></source>
<volume>83</volume>
<fpage>405</fpage>&#x02013;<lpage>416</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.10.026</pub-id>
<pub-id pub-id-type="pmid">29092777</pub-id>
</mixed-citation></ref><ref id="B64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Zhou</surname><given-names>L.</given-names></name><name><surname>Yuan</surname><given-names>H.</given-names></name><name><surname>Shen</surname><given-names>D.</given-names></name></person-group> (<year>2011</year>). <article-title>Multimodal classification of Alzheimer&#x02019;s disease and mild cognitive impairment.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>55</volume>
<fpage>856</fpage>&#x02013;<lpage>867</lpage>.<pub-id pub-id-type="pmid">21236349</pub-id>
</mixed-citation></ref><ref id="B65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Zhou</surname><given-names>L.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>M.</given-names></name><name><surname>Shen</surname><given-names>D.</given-names></name></person-group> (<year>2022</year>). <article-title>Diffusion kernel attention network for brain disorder classification.</article-title>
<source><italic>IEEE Trans. Med. Imaging</italic></source>
<volume>41</volume>
<fpage>2814</fpage>&#x02013;<lpage>2827</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2022.3170701</pub-id>
<pub-id pub-id-type="pmid">35471877</pub-id>
</mixed-citation></ref><ref id="B66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Wu</surname><given-names>C.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Mao</surname><given-names>L.</given-names></name></person-group> (<year>2022</year>). <article-title>Microsurgical treatment of middle cerebral artery stenosis or occlusion: A single center experience and literature review.</article-title>
<source><italic>BMC Surg.</italic></source>
<volume>22</volume>:<issue>87</issue>. <pub-id pub-id-type="doi">10.1186/s12893-022-01539-6</pub-id>
<pub-id pub-id-type="pmid">35255875</pub-id>
</mixed-citation></ref><ref id="B67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>T.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name></person-group> (<year>2023</year>). <article-title>Differentiable multi-granularity human parsing.</article-title>
<source><italic>IEEE Trans. Pattern. Anal. Mach Intell.</italic></source>
<volume>45</volume>
<fpage>8296</fpage>&#x02013;<lpage>8310</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2023.3239194</pub-id>
<pub-id pub-id-type="pmid">37022259</pub-id>
</mixed-citation></ref><ref id="B68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>D.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Terry</surname><given-names>D. P.</given-names></name><name><surname>Puente</surname><given-names>A. N.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Shen</surname><given-names>D.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Connectome-scale assessments of structural and functional connectivity in MCI.</article-title>
<source><italic>Hum. Brain Mapp.</italic></source>
<volume>35</volume>
<fpage>2911</fpage>&#x02013;<lpage>2923</lpage>.<pub-id pub-id-type="pmid">24123412</pub-id>
</mixed-citation></ref><ref id="B69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Q.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Meng</surname><given-names>X.</given-names></name><name><surname>Xu</surname><given-names>Q.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Shao</surname><given-names>W.</given-names></name><etal/></person-group> (<year>2024</year>). <article-title>Spatio-temporal graph hubness propagation model for dynamic brain network classification.</article-title>
<source><italic>IEEE Trans. Med. Imaging</italic></source>
<volume>43</volume>
<fpage>2381</fpage>&#x02013;<lpage>2394</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2024.3363014</pub-id>
<pub-id pub-id-type="pmid">38319754</pub-id>
</mixed-citation></ref><ref id="B70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>L.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Gao</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>). <article-title>Cognitive assessment tools for mild cognitive impairment screening.</article-title>
<source><italic>J. Neurol.</italic></source>
<volume>268</volume>
<fpage>1615</fpage>&#x02013;<lpage>1622</lpage>.<pub-id pub-id-type="pmid">31414193</pub-id>
</mixed-citation></ref></ref-list></back></article>