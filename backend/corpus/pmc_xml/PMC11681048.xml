<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39730714</article-id><article-id pub-id-type="pmc">PMC11681048</article-id><article-id pub-id-type="publisher-id">82253</article-id><article-id pub-id-type="doi">10.1038/s41598-024-82253-6</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Clustering and classification for dry bean feature imbalanced data</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Lee</surname><given-names>Chou-Yuan</given-names></name><address><email>lqy@fzfu.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Wei</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Jian-Qiong</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/011xvna82</institution-id><institution-id institution-id-type="GRID">grid.411604.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0130 6528</institution-id><institution>School of Big Data, </institution><institution>Fuzhou University of International Studies and Trade, </institution></institution-wrap>Fuzhou, 350202 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0040axw97</institution-id><institution-id institution-id-type="GRID">grid.440773.3</institution-id><institution-id institution-id-type="ISNI">0000 0000 9342 2456</institution-id><institution>School of Software, </institution><institution>Yunnan University, </institution></institution-wrap>Kunming, 650000 China </aff></contrib-group><pub-date pub-type="epub"><day>28</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>28</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>31058</elocation-id><history><date date-type="received"><day>1</day><month>4</month><year>2024</year></date><date date-type="accepted"><day>3</day><month>12</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">The traditional machine learning methods such as decision tree (DT), random forest (RF), and support vector machine (SVM) have low classification performance. This paper proposes an algorithm for the dry bean dataset and obesity levels dataset that can balance the minority class and the majority class and has a clustering function to improve the traditional machine learning classification accuracy and various performance indicators such as precision, recall, f1-score, and area under curve (AUC) for imbalanced data. The key idea is to use the advantages of borderline-synthetic minority oversampling technique (BLSMOTE) to generate new samples using samples on the boundary of minority class samples to reduce the impact of noise on model building, and the advantages of K-means clustering to divide data into different groups according to similarities or common features. The results show that the proposed algorithm BLSMOTE + K-means + SVM is superior to other traditional machine learning methods in classification and various performance indicators. The BLSMOTE + K-means + DT generates decision rules for the dry bean dataset and the the obesity levels dataset, and the BLSMOTE + K-means + RF ranks the importance of explanatory variables. These experimental results can provide scientific evidence for decision-makers.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>K-means</kwd><kwd>BLSMOTE</kwd><kwd>Decision tree</kwd><kwd>Random forest</kwd><kwd>Support vector machine</kwd><kwd>Imbalanced data</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Plant sciences</kwd><kwd>Mathematics and computing</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">The dry beans are important food crops that can play an important role in addressing global food security and environmental challenges, while also contributing to healthy diets. The dry beans are an important and inexpensive source of plant protein, vitamins and minerals for people around the world. They are low in fat, cholesterol-free and an important source of dietary fiber. In addition, they are gluten-free and rich in minerals and B vitamins, which are important elements for a healthy life. With the rapid development of agricultural economic globalization, the economic exchange of dry beans among countries is becoming increasingly close. The dry beans are important edible beans for humans, they as a part of traditional diets have played an important role around the world. They are rich in nutrients and are a near perfect food that helps control weight, provide necessary nutrients for the body, and thus achieve disease prevention<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Although we have benefited greatly from dry beans, we must realize that as a common agricultural product, dry beans have a large market demand, and there is still a need to strengthen the information construction of various types of dry bean foods and promote economic development among trading countries. From an agricultural perspective, multiple cropping systems that include dry beans can increase soil fertility, improve yields and contribute to more sustainable food systems. Significantly, dry beans require much less water than other protein sources and can be grown in soils too poor to support other crops. The dry beans can also be used as animal feed, thus improving the quality of animal diets. In addition, dry beans can play an important role in climate change adaptation as they are rich in genetic diversity and can be used to breed climate-resilient varieties. If governments around the world increase their investment in the dry bean economy, it will surely promote the development of the dry bean industry. In view of the continuous growth of dry bean production, the increase in product varieties and the rapid development of the world economy, it is necessary to strengthen the research and analysis of dry beans, find potential factors to promote the development of the dry bean industry, strengthen dry bean production and promote technological innovation<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>.</p><p id="Par3">The dry bean datasets are generally approached from both a data perspective and an algorithmic perspective, and a combination of both. For example, in 2022, Shahoveisi et al. used traditional machine learning methods to model the risk of disease development caused by sclerotinia sclerotiorum on rapeseed and dry beans<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. In 2021, Mendigoria predicted the morphological characteristics and variety classification of dry beans through traditional machine learning methods<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. However, the classification effect is not very good. This is because the dry bean dataset is an imbalanced data, which may cause the above scholars to have low classification accuracy and performance indicators such as precision, recall, f1-score, receiver operating characteristic (ROC) curve and area under the curve (AUC) in traditional machine learning methods such as decision tree (DT), random forest (RF), and support vector machine (SVM).</p><p id="Par4">The imbalanced data refers to an imbalanced distribution of sample label values &#x0200b;&#x0200b;in machine learning tasks. In classification problems, if the number of samples classified as negative (the majority class) far exceeds the number of samples classified as positive (the minority class), then the dataset can be considered imbalanced<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. In many fields such as medicine, agriculture, and daily life, if minorities are ignored or misclassified, it will cause serious harm and negative impacts to individuals and society. The synthetic minority oversampling technique (SMOTE) is a method of randomly generating sample points based on sample distribution to reduce the imbalance of the dataset. Some scholars have used SMOTE to study sampling methods for imbalanced data. For example, in 2021, Wang et al. used SMOTE to expand and classify imbalanced data<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. In 2022, Sun et al. et al. an ensemble model using K-means combined with SMOTE was used to predict stacking rockburst<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Although these scholars used SMOTE to process imbalanced data, the minority class samples on the borderline could not be processed, which easily caused the influence of noise during modeling, thus affecting the performance of data classification.</p><p id="Par5">Since the dry bean dataset is an imbalanced data, this may lead to low classification accuracy and various performance indicators such as precision, recall, f1-score, ROC- AUC of traditional machine learning methods such DT, RF, and SVM. This study proposes a method different from the traditional machine learning method. It uses the borderline-synthetic minority class oversampling technique (BLSMOTE) and K-means combined with machine learning algorithms to predict dry bean varieties, that is, BLSMOTE + K-means + DT, BLSMOTE + K-means + RF, and BLSMOTE + K-means + SVM are proposed to improve the classification accuracy and various performance indicators such as precision, recall, f1-score, ROC-AUC of traditional machine learning methods such as DT, RF, and SVM. The main idea is to first use BLSMOTE to generate new samples for the samples on the boundary of the minority class samples of the dry bean dataset to reduce the impact of noise on model building, and then use K-means to cluster the dry bean data and divide the data into different clusters according to similarity or common features. The proposed algorithm BLSMOTE + K-means + SVM has better classification performance than other traditional machine learning methods, the BLSMOTE + K-means + DT provides decision rules for the dry bean dataset, and the BLSMOTE + K-means + RF is used to find the importance ranking of the factors affecting the dry bean features. In addition to the dry bean dataset, this study also used the obesity levels imbalanced dataset to test the performance of the proposed algorithm.</p><p id="Par6">This paper collected University of California Irvine (UCI) dry beans dataset and shared by Selkuk University in Turkey, through R 4.3.2 software, and used BLSMOTE + K-means combined with machine learning methods, namely BLSMOTE + K-means + SVM, BLSMOTE + K-means + DT, and BLSMOTE + K-means + RF, to improve the classification performance such as classification accuracy, precision, recall, f1-score, and AUC of traditional machine algorithms, and also reflected the information closely related to dry bean characteristics and dry bean varieties. Furthermore, another obesity levels dataset is also used to test the performance of the proposed method.</p><p id="Par7">The remainder of this paper is divided into four parts. Section&#x000a0;2 reviews the algorithms relevant to this study. The proposed method is presented in Sect.&#x000a0;3. Experimental results and discussions are presented in Sect.&#x000a0;4. Finally, Sect.&#x000a0;5 draws the conclusions of this study.</p></sec><sec id="Sec2"><title>Review of related algorithms</title><sec id="Sec3"><title>Decision tree</title><p id="Par8">The decision tree (DT) is a very important approach for providing decision-making in machine learning approaches. It is supervised learning, that is, the algorithm needs to be given certain samples. Each sample point instance has its own features and categories. Through these sample point information, the supervised learning algorithm can obtain the classification rules, and through the classification rules it can pass Features of new sample points to correctly classify them. Each leaf node corresponds to a decision, while the internal nodes and root nodes correspond to a test of a feature attribute; the samples contained in each node can be divided into sub-nodes based on the results of the attribute test; The complete set of samples is included in the root node of the decision tree, and the path from the root node to each leaf node corresponds to a set of attribute tests<sup><xref ref-type="bibr" rid="CR8">8</xref>&#x02013;<xref ref-type="bibr" rid="CR10">10</xref></sup>. The information gain is an extremely important amount of data in the DT algorithm. Choosing the attribute with the highest information gain as the splitting attribute can promote the result partition to classify tuples with the smallest amount of information in the optional range, and the result is the most accurate. The calculation formula of information gain as shown in Eq.&#x000a0;(<xref rid="Equ1" ref-type="disp-formula">1</xref>).<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:Info\left(D\right)=\left[-\sum\:_{i=1}^{m}{P}_{i}{{log}}_{2}\left({P}_{i}\right)\right]$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equa.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equb"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{Info}_{A}\left(D\right)=\sum\:_{j=1}^{v}\frac{\left|{D}_{j}\right|}{\left|D\right|}\times\:Info\left(D\right)$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equb.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Gain\: (A) =Info\:(D)-\:{Info}_{A}\left(D\right)$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par9">where <inline-formula id="IEq1"><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq1.gif"/></alternatives></inline-formula> represents the probability that the <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i^{th}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq2.gif"/></alternatives></inline-formula> class appears in the entire training set, the Info (D) is the average amount of information needed to identify the class of a case in D, the <inline-formula id="IEq5"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Info_A\;(D)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq5.gif"/></alternatives></inline-formula> is the expected information value for feature A to the partition D, the D is the number of cases in the training set, <inline-formula id="IEq9"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D_j$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq9.gif"/></alternatives></inline-formula> is a class, j = 1, 2, &#x02026;, v and v is the number of classes,&#x000a0;<inline-formula id="IEq13"><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D_j$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq13.gif"/></alternatives></inline-formula>&#x000a0;is a subset of D corresponding to the <inline-formula id="IEq15"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j^{th}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq15.gif"/></alternatives></inline-formula> output, and&#x000a0;<inline-formula id="IEq16"><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mid{D}_{j}\mid$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq16.gif"/></alternatives></inline-formula>&#x000a0;is the number of cases of the subset <inline-formula id="IEq17"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D_j$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq17.gif"/></alternatives></inline-formula>. It is necessary to set the complexity parameter (CP) and minimum split in DT to achieve a balance between accuracy and brevity.</p></sec><sec id="Sec4"><title>Random forest</title><p id="Par10">The random forest (RF) is an algorithm that integrates multiple DT. It originates from the idea of classifier integration, which is to combine multiple classifiers to complete the classification task. Because the DT has poor generalization capabilities, the emergence of RF solves this problem. Because a decision tree has only one tree, its generalization ability is poor. Because random forest is composed of multiple DT, it has better generalization ability.</p><p id="Par11">Selecting the best feature from the feature set of the current node is the rule for classifying features by the decision tree. However, in the random forest, the rule for classifying features is to select a single feature that contains m features for each node based on the decision tree. A subset of the node is randomly extracted from the feature set of the node, and then a single best feature is selected from the subset for partitioning<sup><xref ref-type="bibr" rid="CR11">11</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref></sup>. The RF adopts a random approach in the feature selection process of each node, which randomly selects a portion of features from all features as candidate features, and then selects the optimal features for partitioning. In the prediction stage, each classification tree predicts new samples and generates a prediction result. The final prediction result is obtained through voting.</p></sec><sec id="Sec5"><title>Support vector machine</title><p id="Par12">The support vector machine (SVM) is a machine learning approach that classifies a dataset into a hyperplane and solves the classification problem. The SVM solves the problem of Eq.&#x000a0;(<xref rid="Equ2" ref-type="disp-formula">2</xref>) with a given training patterns&#x000a0;<inline-formula id="IEq19"><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\left({x}_{i},\:{y}_{i}\right),\:i\:=\:1,\:2,\dots\:\:n,x\in\:{S}^{t},{y}_{i}\in\:\:\left\{-1,+1\right\}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq19.gif"/></alternatives></inline-formula>, the feature input of a multi-dimensional feature vector of <inline-formula id="IEq20"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{x}_{i}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq20.gif"/></alternatives></inline-formula> in the <inline-formula id="IEq21"><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{i}^{th}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq21.gif"/></alternatives></inline-formula> pattern, the number of patterns of&#x000a0;<inline-formula id="IEq22"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq22.gif"/></alternatives></inline-formula>, the t-dimensional real number space of&#x000a0;<inline-formula id="IEq25"><alternatives><tex-math id="M16">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S^t$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq25.gif"/></alternatives></inline-formula>, and the output of <inline-formula id="IEq26"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq26.gif"/></alternatives></inline-formula>. The SVM solves the problem shown in Eq.&#x000a0;(2).<disp-formula id="Equc"><alternatives><tex-math id="M18">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{Max}\;L(Q)\:=\sum\:_{i=1}^{n}{Q}_{i\:}-\:\frac{1}{2}\sum\:_{i,j=1}^{n}{Q}_{i}{Q}_{j}{y}_{i}{y}_{j}\langle{x}_{i}{,x}_{j}\rangle$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equc.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:s.t.\:\:0\le\:\:{Q}_{i}\:\le\:C,\:and\sum\:_{i=1}^{n}{Q}_{i}{y}_{i}=0$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par13">where <inline-formula id="IEq27"><alternatives><tex-math id="M20">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Q_i\;\geq$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq27.gif"/></alternatives></inline-formula> 0 denotes the Lagrange multiplier and <inline-formula id="IEq28"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq28.gif"/></alternatives></inline-formula> is a parameter of the cost of penalty. The feature space vectors <inline-formula id="IEq29"><alternatives><tex-math id="M22">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i,x_j$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq29.gif"/></alternatives></inline-formula> are constructed in terms of the kernel <inline-formula id="IEq30"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq30.gif"/></alternatives></inline-formula> where<inline-formula id="IEq31"><alternatives><tex-math id="M24">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k\left({x}_{i}{,x}_{j}\right)=\:\langle{x}_{i}{,x}_{j}\rangle.$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq31.gif"/></alternatives></inline-formula> Using the feature space<inline-formula id="IEq32"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k\left({x}_{i}{,x}_{j}\right)=\:\langle{x}_{i}{,x}_{j}\rangle$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq32.gif"/></alternatives></inline-formula>, the SVM can be expressed in Eq. (<xref rid="Equ3" ref-type="disp-formula">3</xref>).<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M26">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{Max}\;L\left(Q\right)=\sum\:_{i=1}^{n}{Q}_{i\:}-\:\frac{1}{2}\sum\:_{i,j=1}^{n}{Q}_{i}{Q}_{j}{y}_{i}{y}_{j}k\left({x}_{i}{,x}_{j}\right)$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par14">For the radial basis function, it can be expressed as <inline-formula id="IEq33"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:k\left({x}_{i}{,x}_{j}\right)={exp}(-\gamma\:{\parallel{x}_{i}{-x}_{j}\parallel}^{2})$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq33.gif"/></alternatives></inline-formula>. Two parameters <inline-formula id="IEq34"><alternatives><tex-math id="M28">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq34.gif"/></alternatives></inline-formula> and <inline-formula id="IEq35"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq35.gif"/></alternatives></inline-formula> must be appropriately set in SVM. It is necessary to set <inline-formula id="IEq36"><alternatives><tex-math id="M30">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq36.gif"/></alternatives></inline-formula> and the <inline-formula id="IEq37"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq37.gif"/></alternatives></inline-formula>parameters in the SVM to achieve a balance between accuracy and brevity<sup><xref ref-type="bibr" rid="CR15">15</xref>&#x02013;<xref ref-type="bibr" rid="CR17">17</xref></sup>.</p></sec><sec id="Sec6"><title>BLSMOTE</title><p id="Par15">The borderline-synthetic minority oversampling technique (BLSMOTE) is an oversampling method that is improved on the basis of synthetic minority oversampling technique (SMOTE) and uses samples on the boundary of minority class samples to generate new samples. The basic idea of &#x0200b;&#x0200b;SMOTE is to generate new synthetic samples to balance the dataset based on the similarity between minority class samples. The basic principle is: first, calculate the Euclidean distance between the minority class sample and its neighboring samples, and then select the <inline-formula id="IEq38"><alternatives><tex-math id="M32">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:k$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq38.gif"/></alternatives></inline-formula> nearest neighbor samples; then, set the sample magnification to <inline-formula id="IEq39"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:M$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq39.gif"/></alternatives></inline-formula> according to the sample ratio, randomly select a sample from the sample and name it <inline-formula id="IEq40"><alternatives><tex-math id="M34">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{X}_{i}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq40.gif"/></alternatives></inline-formula>, <inline-formula id="IEq41"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i=1,\:2,\:3,\:...,a$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq41.gif"/></alternatives></inline-formula>, and then randomly select a sample from its <inline-formula id="IEq42"><alternatives><tex-math id="M36">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:k$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq42.gif"/></alternatives></inline-formula> nearest neighbors and name it <inline-formula id="IEq43"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{X}_{in}\:,\:\:n=\text{1,2},3,\dots\:,b$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq43.gif"/></alternatives></inline-formula>; finally, connect <inline-formula id="IEq44"><alternatives><tex-math id="M38">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{X}_{i}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq44.gif"/></alternatives></inline-formula> with <inline-formula id="IEq45"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{X}_{in},$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq45.gif"/></alternatives></inline-formula> and perform linear interpolation on the connecting line to generate a new artificial synthetic sample point <inline-formula id="IEq46"><alternatives><tex-math id="M40">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{X}_{newj}\:,\:\:j=1,\:2,\:3,\:...,t$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq46.gif"/></alternatives></inline-formula>. As shown in Eq.&#x000a0;(<xref rid="Equ4" ref-type="disp-formula">4</xref>), the<inline-formula id="IEq47"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\:rand\left(\text{0,1}\right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq47.gif"/></alternatives></inline-formula> means to extract a random number between (0, 1), and <inline-formula id="IEq48"><alternatives><tex-math id="M42">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:({X}_{in}-{X}_{i})$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq48.gif"/></alternatives></inline-formula>is the distance in the feature space<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{X}_{newj}={X}_{i}+rand\left(\text{0,1}\right)\cdot\:\left({X}_{in}-{X}_{i}\right)$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par16">Because SMOTE is a method of randomly generating sample points based on sample distribution to reduce the imbalance of the dataset. It only considers a minority of samples and easily ignores the influence of surrounding samples. This can easily lead to the problem of repeated generated samples, thereby causing over-fitting. Compared with SMOTE, the BLSMOTE uses samples on the boundary of minority class samples to generate new samples, which can reduce the impact of noise on the model building.</p></sec><sec id="Sec7"><title>K-means</title><p id="Par17">The K-means algorithm is a clustering algorithm. It is a representative of a typical objective function clustering approach. It uses a certain distance from a specific sample point in the sample space to the prototype as a reference for clustering, and uses functions to find the maximum and minimum values, and iteratively operates to finally obtain the clustering rules<sup><xref ref-type="bibr" rid="CR19">19</xref>&#x02013;<xref ref-type="bibr" rid="CR21">21</xref></sup>.</p><p id="Par18">For a certain sample in the sample space, calculate the Euclidean distance between sample points in different clusters and divide the sample space into <inline-formula id="IEq49"><alternatives><tex-math id="M44">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq49.gif"/></alternatives></inline-formula> clusters. Assuming that the clusters are divided into <inline-formula id="IEq50"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_1,\;C_2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq50.gif"/></alternatives></inline-formula>,&#x000a0;<inline-formula id="IEq51"><alternatives><tex-math id="M46">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_k$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq51.gif"/></alternatives></inline-formula>, the expression of the Euclidean distance <inline-formula id="IEq52"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq52.gif"/></alternatives></inline-formula>&#x000a0;as Eq.&#x000a0;(<xref rid="Equ5" ref-type="disp-formula">5</xref>)<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>.<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M48">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:E\:=\:\sum\:_{i=1}^{k}\sum\:_{x\in\:{C}_{i}}^{\:}{\parallel{x-u}_{i}\parallel}^{2}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par19">The heuristic approach process of K-means is divided into the following steps: the first step is to select <inline-formula id="IEq53"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq53.gif"/></alternatives></inline-formula> center points in the sample space; the second step is to calculate all the sample points in the sample space and calculate the distance to each center point. Euclidean distance, and classify each sample point and the nearest center point into the same category; the third step, recalculate the center point of each cluster in the sample space; the fourth step, based on the new center point, classify each sample in the sample space again; in the fifth step, repeat the third and fourth steps until the center point in the sample space no longer changes.</p><p id="Par20">The process of the heuristic approach is shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>. All the samples in (a) select the initial center point to form (b). The distances between the sample points in (b) and the center point are calculated respectively, and further identified and classified. The cluster is formed (c), and then the center point is recalculated for each cluster to form (d). The above steps are repeated again to form a new cluster, which is formed (e). The (f) is formed when all center points no longer change.<fig id="Fig1"><label>Fig. 1</label><caption><p>The process of K-means heuristic approach.</p></caption><graphic xlink:href="41598_2024_82253_Fig1_HTML" id="d33e600"/></fig></p></sec></sec><sec id="Sec8"><title> Proposed methodology</title><sec id="Sec9"><title>Data preprocessing</title><p id="Par21">The dry bean dataset comes from the UCI dataset (<ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset">http://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset</ext-link>), which is collected and shared by Selkuk University in Turkey. This dataset has a total of 13,611 data, including 16 explanatory variables and 1 target variable. The 16 explanatory variables are Area, Perimeter, MajorAxisLength, MinorAxisLength, AspectRation, Eccentricity, ConvexArea, EquivDiameter, Extent, Solidity, Roundness, Compactness, ShapeFactor1, ShapeFactor2, ShapeFactor3, and ShapeFactor4. The dry bean dataset variables and their meanings are shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The dry bean dataset variable names and their meanings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">NO.</th><th align="left">Name</th><th align="left">Meanings</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">Area</td><td align="left">The area of the dry bean region and the number of pixels within its boundaries</td></tr><tr><td align="left">2</td><td align="left">Perimeter</td><td align="left">The dry bean border length</td></tr><tr><td align="left">3</td><td align="left">MajorAxisLength</td><td align="left">The distance between the ends of the longest straight line that can be drawn in dry beans</td></tr><tr><td align="left">4</td><td align="left">MinorAxisLength</td><td align="left">The longest distance that can be drawn when dry beans are perpendicular to the main axis</td></tr><tr><td align="left">5</td><td align="left">AspectRation</td><td align="left">Define the relationship between MajorAxisLength and MinorAxisLength</td></tr><tr><td align="left">6</td><td align="left">Eccentricity</td><td align="left">The ellipse has the same moment as the eccentricity of the region</td></tr><tr><td align="left">7</td><td align="left">ConvexArea</td><td align="left">The number of pixels in the smallest convex polygon that can contain a dry bean.</td></tr><tr><td align="left">8</td><td align="left">EquivDiameter</td><td align="left">The diameter of a circle equal to the area of a dry bean.</td></tr><tr><td align="left">9</td><td align="left">Extent</td><td align="left">The ratio of pixels in the dry bean bounding box to the dry bean area</td></tr><tr><td align="left">10</td><td align="left">Solidity</td><td align="left">The ratio of pixels in the convex shell of dry beans to the pixels in dry beans</td></tr><tr><td align="left">11</td><td align="left">Roundness</td><td align="left"><inline-formula id="IEq54"><alternatives><tex-math id="M50">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{4\cdot\pi\:\cdot\text{Area}}{\text{Perimeter}^{2}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq54.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">12</td><td align="left">Compactness</td><td align="left"><inline-formula id="IEq55"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{\text{EquivDiameter}}{\text{MajorAxisLength}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq55.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">13</td><td align="left">ShapeFactor1</td><td align="left"><inline-formula id="IEq56"><alternatives><tex-math id="M52">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\frac{\text{MajorAxisLength}}{Area}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq56.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">14</td><td align="left">ShapeFactor2</td><td align="left"><inline-formula id="IEq57"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\frac{\text{MinorAxisLength}}{Area}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq57.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">15</td><td align="left">ShapeFactor3</td><td align="left"><inline-formula id="IEq58"><alternatives><tex-math id="M54">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\frac{4\cdot\text{Area}}{{{\uppi\:}\cdot\text{MajorAxisLength}}^{2}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq58.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">16</td><td align="left">ShapeFactor4</td><td align="left"><inline-formula id="IEq59"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\frac{4\cdot{Area}}{\pi\:\cdot\text{MajorAxisLength}\cdot\text{MinorAxisLength}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq59.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">17</td><td align="left">Class</td><td align="left">The target variable has seven categories: Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira.</td></tr></tbody></table></table-wrap></p><p id="Par22">The target variable of this dataset has seven different types of dry beans, taking into account their morphology, shape, type, structure and other features, namely Barbunya, Bombay, Cali, Dermason, Horoz, Seker and Sira. In the classification model, a high-resolution camera was used to image 13,611 grains of 7 different types of dry beans. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>is a sampling diagram of categories 1&#x02013;7 of the target variable<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>.<fig id="Fig2"><label>Fig. 2</label><caption><p>The category 1&#x02013;7 sampling diagram of the target variable.</p></caption><graphic xlink:href="41598_2024_82253_Fig2_HTML" id="d33e800"/></fig></p><p id="Par23">Furthermore, the obesity levels dataset collected from UCI repository (<ext-link ext-link-type="uri" xlink:href="https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition">https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition</ext-link> is also used to test the performance for the proposed method. The obesity levels dataset contains 2111 records, consisting of 16 explanatory variables and 1 target variable. The target variable consists of seven different obesity levels, including Insufficiency weight, normal weight, overweight level I, overweight level II, obesity type I, obesity type II, and obesity type III. The obesity levels dataset variables and their meanings are shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>The obesity levels dataset variable names and their meanings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">No.</th><th align="left">name</th><th align="left">Meanings</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">Gender</td><td align="left">Male and female</td></tr><tr><td align="left">2</td><td align="left">Age</td><td align="left">Age, taken between 14 and 61 years old</td></tr><tr><td align="left">3</td><td align="left">Height</td><td align="left">Participants&#x02019; height (meters)</td></tr><tr><td align="left">4</td><td align="left">Weight</td><td align="left">Participant&#x02019;s weight (kg)</td></tr><tr><td align="left">5</td><td align="left">Family_history_with_over</td><td align="left">Are family members overweight?</td></tr><tr><td align="left">6</td><td align="left">FAVC</td><td align="left">Do you often eat high calorie foods?</td></tr><tr><td align="left">7</td><td align="left">FCVC</td><td align="left">Will you eat vegeles during meals?</td></tr><tr><td align="left">8</td><td align="left">NCP</td><td align="left">How many main meals do you eat every day?</td></tr><tr><td align="left">9</td><td align="left">CAEC</td><td align="left">The frequency of eating food between meals</td></tr><tr><td align="left">10</td><td align="left">SMOKE</td><td align="left">Smoking Status</td></tr><tr><td align="left">11</td><td align="left">CH2O</td><td align="left">How much water do you drink every day?</td></tr><tr><td align="left">12</td><td align="left">SCC</td><td align="left">Are calories monitored daily?</td></tr><tr><td align="left">13</td><td align="left">FAF</td><td align="left">How often do I engage in physical activity?</td></tr><tr><td align="left">14</td><td align="left">TUE</td><td align="left">What is the time spent using technology devices such as mobile phones?</td></tr><tr><td align="left">15</td><td align="left">CALC</td><td align="left">How often do you drink alcohol?</td></tr><tr><td align="left">16</td><td align="left">MTRANS</td><td align="left">What kind of transportation is usually used?</td></tr><tr><td align="left">17</td><td align="left">NObeyesdad</td><td align="left">The target variable has seven grades such as insufficient_weight, normal_weight, overweight_level_I, overweight_level_II, obesity_type_I, obesity_type_II, and obesity_type_III</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec10"><title>Research process</title><p id="Par24">The dry bean dataset in this paper comes from UCI. In order to facilitate prediction, the value of the target variable of the dataset is converted from a string to a number. The data is divided into ten parts. Eight parts of the data are retrieved as training data and the other two parts are used as test data, and all comparisons are made under fair conditions.</p><p id="Par25">Since the dry bean dataset is an imbalanced data, this may lead to low classification accuracy and various performance indicators such as precision, recall, f1-score, ROC-AUC of traditional machine learning methods such as DT, RF, and SVM. In order to improve this shortcoming, this paper proposes a BLSMOTE and K-means algorithm combined with DT, RF and SVM, namely BLSMOTE + K-means + DT, BLSMOTE + K-means + RF, and BLSMOTE + K-means + SVM for clustering and classification of dry bean characteristics. In the proposed algorithm, BLSMOTE generates new samples for samples on the boundary of minority class samples in the dry bean dataset to reduce the impact of noise on model building, and K-means clusters the dry bean data and divides the data into different clusters according to similarity or common features. The flowchart of the proposed algorithm is shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>. In Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, after preprocessing the dry bean dataset, the Kvalue of K-means is parameterized, and the parameter of DT, RF, and SVM are set. After that, calculating the classification accuracy of DT, RF, and SVM, and then the classification accuracy of BLSMOTE + DT, BLSMOTE + RF, and BLSMOTE + SVM are calculated, and then the classification accuracy of BLSMOTE + K-means + DT, BLSMOTE + K-means + RF, and BLSMOTE + K-means + SVM are calculated, and the parameters were adjusted to improve the training set classification accuracy and test set classification accuracy without over-fitting<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. Finally output the classification accuracy and analyze the results.<fig id="Fig3"><label>Fig. 3</label><caption><p>The flow chart of BLSMOTE + K-means + machine learning approaches.</p></caption><graphic xlink:href="41598_2024_82253_Fig3_HTML" id="d33e970"/></fig></p></sec></sec><sec id="Sec11"><title>Experimental results and discussions</title><sec id="Sec12"><title>Data source and preprocessing</title><p id="Par26">The dry bean dataset collected by UCI provides information on 13,611 dry beans (target variable has seven different varieties) from Selkuk University in Turkey. This study uses digital transformation to convert the value of the target variable from a string to a numeric value so that the subsequent machine learning algorithm can classify the dry bean dataset. The target variable content, such as Seker, Barbunya, Bombay, Cali, Horoz, Sira, and Dermason, are replaced with 1 to 7 respectively.</p><p id="Par27">Furthermore, the textual content of the obesity levels dataset of 2,111 records was replaced with numerical content using digital conversion. The specific method is to convert the contents of the explanatory variables (CAEC) and (CALC) such as no, sometimes, frequently, and always to 1, 2, 3, and 4 respectively. Replace the contents of the explanatory variables (MTRANS) such as bike, motorbike, walking, automobile, and publicTransportation with 1, 2, 3, 4, and 5 respectively. Represent the contents of the explanation variables (family_history_with_overweight), (FAVC), (SMOKE), and (SCC) with 0 for no and 1 for yes. The content of Gender is represented by 0 for female and 1 for male. Replace the contents of the target variable such as insufficient_weight, normal_weight, overweight_level_I, overweight_level_II, obesity_type_I, obesity_type_II, and obesity_type_III with 0 to 6, respectively.</p></sec><sec id="Sec13"><title>The calculation of classification accuracy and other performance metrics</title><p id="Par28">The confusion matrix is a standard form of presenting accuracy estimates in matrix form. This paper uses a confusion matrix to calculate the classification accuracy of the algorithm results. The confusion matrix is shown in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>The confusion matrix.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Predicted positive</th><th align="left">Predicted negative</th></tr></thead><tbody><tr><td align="left">Actual Positive</td><td align="left">TP (true positive)</td><td align="left">FN (false negative)</td></tr><tr><td align="left">Actual Negative</td><td align="left">FP (false positive)</td><td align="left">TN (true negative)</td></tr></tbody></table></table-wrap></p><p id="Par29">In Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>, the TP and TN represent the results of correct prediction. The FP and FN represent the results of incorrect prediction. The classification accuracy is calculated as shown in Eq.&#x000a0;(<xref rid="Equ6" ref-type="disp-formula">6</xref>):.<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M56">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:Classification\:accuracy=\frac{TN+TP}{TN+TP+FN+FP}\cdot\:100{\%}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par30">The precision is proposed based on the prediction results, representing the correct probability of the classification model predicting a positive result. The calculation method is shown in Eq.&#x000a0;(<xref rid="Equ7" ref-type="disp-formula">7</xref>).<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:Precision=\frac{TP}{FP+TP}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par31">The recall represents the probability of being successfully predicted when the event we are concerned about actually occurs, calculated using Eq.&#x000a0;(<xref rid="Equ8" ref-type="disp-formula">8</xref>).<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M58">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:Recall=\frac{TP}{FN+TP}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par32">The f1-score is a comprehensive evaluation index that combines precision and recall, and is the harmonic mean of the two<sup><xref ref-type="bibr" rid="CR25">25</xref>&#x02013;<xref ref-type="bibr" rid="CR27">27</xref></sup>. The calculation method is shown in Eq.&#x000a0;(<xref rid="Equ9" ref-type="disp-formula">9</xref>).<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:F1-score=\frac{2\cdot\:Precision\cdot\:Recall}{Precision+Recall}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par33">The AUC value is expressed as the area under the ROC curve. The AUC ranges from 0 to 1. A higher value indicates better classifier performance. The ROC curve shows the relationship between false positive rate (FPR) and true positive rate (TPR). Among them, the horizontal axis is FPR, which represents the proportion of samples that are incorrectly predicted as positive categories in the actual negative categories.The calculation is shown in Eq.&#x000a0;(<xref rid="Equ10" ref-type="disp-formula">10</xref>). The vertical axis is TPR, which represents the proportion of correctly predicted positive categories in the actual positive category samples. The calculation is shown in Eq.&#x000a0;(<xref rid="Equ11" ref-type="disp-formula">11</xref>).<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M60">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:TPR=\frac{TP}{TP+FN}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:FPR=\frac{FP}{FP+TN}$$\end{document}</tex-math><graphic xlink:href="41598_2024_82253_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec14"><title>Comparative analysis of BLSMOTE + K-means + SVM and other methods</title><p id="Par34">The cross-validation is a very important technique in training models, which can avoid overfitting of models. It provides us with a more accurate way to estimate the prediction performance of models, and can also improve the generalization ability of models. In order to fairly compare the prediction accuracy of various algorithms, each method in this study utilized 10-fold cross-validation to calculate the prediction accuracy. The data is divided into 10 parts. Eight parts of the data are retrieved as training data and the other two parts are used as test data. The complexity parameter (CP) of DT is set to 0.005, and the minimum number of branch nodes (Minsplit) is set to 10<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. The number of trees in RF is set to 100. The settings of C and parameters will affect the eslishment of the support vector machine model. This article sets the C value to 1 and the value to 0.4<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The K value of K-means is set to 7, which means it is divided into 7 clusters. To illustrate the effectiveness of the proposed algorithm, the experimental results are shown from Tables&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>.<table-wrap id="Tab4"><label>Table 4</label><caption><p>The classification accuracy and performance indicators of dry bean dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Approaches</th><th align="left">Training set accuracy (%)</th><th align="left">Test set accuracy (%)</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">DT</td><td align="left">89.20</td><td align="left">88.83</td><td align="left">0.9042</td><td align="left">0.8959</td><td align="left">0.9000</td><td align="left">0.9088</td></tr><tr><td align="left">BLSMOTE + DT</td><td align="left">89.25</td><td align="left">89.03</td><td align="left">0.9071</td><td align="left">0.8973</td><td align="left">0.9022</td><td align="left">0.9016</td></tr><tr><td align="left">K-means + DT</td><td align="left">89.29</td><td align="left">88.61</td><td align="left">0.9076</td><td align="left">0.8989</td><td align="left">0.9032</td><td align="left">0.9085</td></tr><tr><td align="left">BLSMOTE + K-means + DT</td><td align="left">89.98</td><td align="left">89.81</td><td align="left">0.9098</td><td align="left">0.9069</td><td align="left">0.9083</td><td align="left">0.9102</td></tr><tr><td align="left">RF</td><td align="left">92.46</td><td align="left">83.25</td><td align="left">0.9053</td><td align="left">0.9071</td><td align="left">0.9062</td><td align="left">0.9245</td></tr><tr><td align="left">BLSMOTE + RF</td><td align="left">92.51</td><td align="left">84.95</td><td align="left">0.9033</td><td align="left">0.9031</td><td align="left">0.9032</td><td align="left">0.9259</td></tr><tr><td align="left">K-means + RF</td><td align="left">92.57</td><td align="left">84.31</td><td align="left">0.9073</td><td align="left">0.9023</td><td align="left">0.9048</td><td align="left">0.9266</td></tr><tr><td align="left">BLSMOTE + K-means + RF</td><td align="left">92.58</td><td align="left">86.06</td><td align="left">0.9001</td><td align="left">0.9183</td><td align="left">0.9091</td><td align="left">0.9277</td></tr><tr><td align="left">SVM</td><td align="left">94.01</td><td align="left">93.75</td><td align="left">0.9402</td><td align="left">0.9437</td><td align="left">0.9419</td><td align="left">0.9527</td></tr><tr><td align="left">BLSMOTE + SVM</td><td align="left">94.25</td><td align="left">93.09</td><td align="left">0.9336</td><td align="left">0.9395</td><td align="left">0.9365</td><td align="left">0.9541</td></tr><tr><td align="left">K-means + SVM</td><td align="left">98.79</td><td align="left">96.98</td><td align="left">0.9644</td><td align="left">0.9691</td><td align="left">0.9667</td><td align="left">0.9568</td></tr><tr><td align="left">BLSMOTE + K-means + SVM</td><td align="left">98.86</td><td align="left">97.54</td><td align="left">0.9736</td><td align="left">0.9743</td><td align="left">0.9739</td><td align="left">0.9831</td></tr></tbody></table></table-wrap></p><p id="Par35">
<table-wrap id="Tab5"><label>Table 5</label><caption><p>The classification accuracy and performance indicators of obesity levels dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Approaches</th><th align="left">Training set accuracy</th><th align="left">Test set accuracy (%)</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">DT</td><td char="." align="char">90.41</td><td char="." align="char">89.60</td><td char="." align="char">0.8052</td><td char="." align="char">0.8151</td><td char="." align="char">0.8101</td><td char="." align="char">0.9312</td></tr><tr><td align="left">BLSMOTE + DT</td><td char="." align="char">90.88</td><td char="." align="char">89.03</td><td char="." align="char">0.8055</td><td char="." align="char">0.8158</td><td char="." align="char">0.8106</td><td char="." align="char">0.9416</td></tr><tr><td align="left">K-means + DT</td><td char="." align="char">91.56</td><td char="." align="char">89.07</td><td char="." align="char">0.8193</td><td char="." align="char">0.8154</td><td char="." align="char">0.8173</td><td char="." align="char">0.9635</td></tr><tr><td align="left">BLSMOTE + K-means + DT</td><td char="." align="char">93.29</td><td char="." align="char">90.07</td><td char="." align="char">0.8060</td><td char="." align="char">0.8765</td><td char="." align="char">0.8398</td><td char="." align="char">0.9642</td></tr><tr><td align="left">RF</td><td char="." align="char">97.99</td><td char="." align="char">90.20</td><td char="." align="char">0.8668</td><td char="." align="char">0.8754</td><td char="." align="char">0.8711</td><td char="." align="char">0.9747</td></tr><tr><td align="left">BLSMOTE + RF</td><td char="." align="char">98.51</td><td char="." align="char">90.95</td><td char="." align="char">0.8751</td><td char="." align="char">0.8763</td><td char="." align="char">0.8757</td><td char="." align="char">0.9772</td></tr><tr><td align="left">K-means + RF</td><td char="." align="char">99.01</td><td char="." align="char">91.25</td><td char="." align="char">0.8713</td><td char="." align="char">0.8852</td><td char="." align="char">0.8782</td><td char="." align="char">0.9833</td></tr><tr><td align="left">BLSMOTE + K-means + RF</td><td char="." align="char">99.13</td><td char="." align="char">91.49</td><td char="." align="char">0.9046</td><td char="." align="char">0.9256</td><td char="." align="char">0.9150</td><td char="." align="char">0.9835</td></tr><tr><td align="left">SVM</td><td char="." align="char">98.99</td><td char="." align="char">92.62</td><td char="." align="char">0.9402</td><td char="." align="char">0.9437</td><td char="." align="char">0.9419</td><td char="." align="char">0.9807</td></tr><tr><td align="left">BLSMOTE + SVM</td><td char="." align="char">99.16</td><td char="." align="char">93.09</td><td char="." align="char">0.9436</td><td char="." align="char">0.9495</td><td char="." align="char">0.9465</td><td char="." align="char">0.9841</td></tr><tr><td align="left">K-means + SVM</td><td char="." align="char">99.31</td><td char="." align="char">93.58</td><td char="." align="char">0.9644</td><td char="." align="char">0.9691</td><td char="." align="char">0.9667</td><td char="." align="char">0.9868</td></tr><tr><td align="left">BLSMOTE + K-means + SVM</td><td char="." align="char">99.57</td><td char="." align="char">94.09</td><td char="." align="char">0.9736</td><td char="." align="char">0.9743</td><td char="." align="char">0.9739</td><td char="." align="char">0.9885</td></tr></tbody></table></table-wrap>
</p><p id="Par36">
<list list-type="order"><list-item><p id="Par37">The comparison results in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> show that the classification accuracy of the BLSMOTE + K-means + SVM training set in the dry bean dataset is 98.86%, which is better than the 94.01% of only SVM. Also from Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>, we can find that the classification accuracy of the training set of the BLSMOTE + K-means + SVM on the obesity levels dataset is 99.57%, which is better than the 98.99% of only SVM. Because the imbalanced data is processed by BLSMOTE, it has the advantage of using samples on the boundary of minority class samples to generate new samples, which can reduce the impact of noise on model building, and K-means clustering has the advantage of dividing data into different groups according to similarity or common features, so BLSMOTE + K-means + SVM has better classification performance than only SVM in Tables&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>.</p></list-item><list-item><p id="Par38">The comparison results in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> show that the classification accuracy of BLSMOTE + K-means + SVM on the dry bean dataset on the training set is 98.86%, which is better than 92.58% of BLSMOTE + K-means + RF and 89.98% of BLSMOTE + K-means + DT. Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref> shows the classification accuracy of BLSMOTE + K-means + SVM on the obesity levels dataset on the training set is 99.57%, which is better than 99.13% of BLSMOTE + K-means + RF and 93.29% of BLSMOTE + K-means + DT. Since SVM is a hyperplane classifier in BLSMOTE + K-means + SVM, while RF and DT are classifiers based on tree structures in BLSMOTE + K-means + RF and BLSMOTE + K-means + DT. The BLSMOTE + K-means + SVM has the advantage of hyperplane classification, so its performance is better than the tree classification of BLSMOTE + K-means + RF and BLSMOTE + K-means + DT in Tables&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>.</p></list-item><list-item><p id="Par39">Comparing the results in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>, in addition to the classification accuracy of BLSMOTE + K-means + SVM of the dry bean dataset, the performance indicators of precision, recall, f1-score, and AUC are relatively the best compared to other algorithms, with an AUC of 0.9831, and its ROC-AUC is shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>. Comparing the results in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>, the AUC of BLSMOTE + K-means + SVM for the obesity levels dataset is 0.9885, and its ROC- AUC is shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>.<fig id="Fig4"><label>Fig. 4</label><caption><p>The ROC curve and AUC value of BLSMOTE + K-means + SVM for dry bean dataset.</p></caption><graphic xlink:href="41598_2024_82253_Fig4_HTML" id="d33e1595"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>The ROC curve and AUC value of BLSMOTE + K-means + SVM for obesity levels dataset.</p></caption><graphic xlink:href="41598_2024_82253_Fig5_HTML" id="d33e1602"/></fig></p></list-item></list>
</p></sec><sec id="Sec15"><title>The analysis and discussion of K-means clustering</title><p id="Par40">After the dry bean data is balanced by BLSMOTE, the K-means is used for clustering, which can divide the data into different groups based on similarities or common features. Among the 16 explanatory variables of dry beans, this feature (Area) and other explanatory variables such as Roundness, ShapeFactor1, ShapeFactor2, ShapeFactor3, and ShapeFactor4 has a close relationship, so the feature (Area) is used to illustrate the seven clusters divided by K-means, as shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>. In Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, the horizontal axis is the seven clusters divided by K-means, and the vertical axis is the value of (Area). It can be found that the maximum value in the feature (Area) falls in the cluster 3, and the minimum value in the feature (Area) falls in the cluster 5.<fig id="Fig6"><label>Fig. 6</label><caption><p>The dry bean data is divided into 7 clusters using feature (Area) through K-means.</p></caption><graphic xlink:href="41598_2024_82253_Fig6_HTML" id="d33e1620"/></fig></p><p id="Par41">The dry bean dataset has a total of 13,611 data. After K-means clustering, seven clusters are formed. The number of records in each cluster is presented from most to least, as shown in Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>. From Fig.&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>, we can find the cluster 1 has 4,236 records, which is the largest number of records among the seven clusters, which means that features with more similarities or commonalities are drawn in the cluster 1. The cluster 2 has 3,261 records, the cluster 5 has 2,547 records, the cluster 7 has 1,918 records, the cluster 4 has 1,129 records, the cluster 3 has 303 records, and the cluster 6 has 217 records, which is the least number of records among the seven clusters, which means that the features with the least similarity or commonality are plotted in cluster 6.<fig id="Fig7"><label>Fig. 7</label><caption><p>The clustering diagram of dry bean dataset.</p></caption><graphic xlink:href="41598_2024_82253_Fig7_HTML" id="d33e1635"/></fig></p><p id="Par42">Among the 16 explanatory variables of the obesity levels dataset, since the feature (Weight) is closely related to the obesity level, the feature (Weight) is used to explain the seven clusters divided by K-means, as shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>. In Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>, the horizontal axis is the seven clusters divided by K-means, and the vertical axis is the value of the feature (Weight). It can be found that the maximum value in the feature (Weight) falls in the cluster 7, and the minimum value in the feature (Weight) falls in the cluster 1.<fig id="Fig8"><label>Fig. 8</label><caption><p>The obesity levels data are divided into 7 clusters using feature (Weight) through K-mean.</p></caption><graphic xlink:href="41598_2024_82253_Fig8_HTML" id="d33e1650"/></fig></p><p id="Par43">The obesity levels dataset has a total of 2,111 data. After K-means clustering, seven clusters are formed. The number of records in each cluster is presented from most to least, as shown in Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>. From Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>, we can find the cluster 5 has 351 records, which is the largest number of records among the seven clusters, which means that features with more similarities or commonalities are drawn in the cluster 5. The cluster 7 has 324 records, the cluster 6 has 297 records, the cluster 3 has 290 records, the cluster 4 has 290 records, the cluster 2 has 287 records, and the cluster 1 has 272 records, the least number among all the seven clusters, which means that the features with the least similarity or commonality are plotted in cluster 1.<fig id="Fig9"><label>Fig. 9</label><caption><p>The clustering diagram of obesity levels dataset.</p></caption><graphic xlink:href="41598_2024_82253_Fig9_HTML" id="d33e1665"/></fig></p></sec><sec id="Sec16"><title>The analysis and discussion of the BLSMOTE + K-means + DT</title><p id="Par44">The training set classification accuracy of the BLSMOTE + K-means + DT is 89.98% for the dry bean dataset, which is better than 89.20% of only DT. The BLSMOTE + K-means + DTt training set decision diagram is shown in Fig.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>.<fig id="Fig10"><label>Fig. 10</label><caption><p>The BLSMOTE + K-means + DT training set decision diagram of dry bean dataset.</p></caption><graphic xlink:href="41598_2024_82253_Fig10_HTML" id="d33e1679"/></fig></p><p id="Par45">From Fig.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>, it can be found that when MajorAxisLength &#x0003c; 281, the left half tree is selected, and when MajorAxisLength <inline-formula id="IEq64"><alternatives><tex-math id="M62">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\ge\:$$\end{document}</tex-math><inline-graphic xlink:href="41598_2024_82253_Article_IEq64.gif"/></alternatives></inline-formula> 281, the right half tree is selected, and the BLSMOTE + K-means + DT decision tree has a total of eleven decision rules, as shown in Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>.<table-wrap id="Tab6"><label>Table 6</label><caption><p>The eleven decision rules of the BLSMOTE + K-means + DT for dry bean dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Rule</th><th align="left">Explanation</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">When MajorAxisLength &#x0003c; 281, ShapeFactor1 &#x0003c; 0.0068, Compactness&#x000a0;&#x02265; 0.85, the result is 1, which is Seker, and the number of samples accounts for 13%.</td></tr><tr><td align="left">2</td><td align="left">When MajorAxisLength &#x0003c; 281, ShapeFactor1 &#x0003c; 0.0068, and Compactness &#x0003c; 0.85, the result is 7, which is Dermason, and the number of samples accounts for 2%.</td></tr><tr><td align="left">3</td><td align="left">When MajorAxisLength &#x0003c; 281, ShapeFactor1 &#x02265;&#x000a0;0.0068, the result is 5, which is Horoz, and the number of samples accounts for 27%.</td></tr><tr><td align="left">4</td><td align="left">When MajorAxisLength &#x02265;&#x000a0;281, Compactness &#x02265;&#x000a0;0.73, Perimeter &#x02265; 897, ShapeFactor1 &#x0003c; 0.0041, the result is 3, which is Bombay, and the number of samples accounts for 4%.</td></tr><tr><td align="left">5</td><td align="left">When MajorAxisLength &#x02265; 281, Compactness &#x02265; 0.73, Perimeter <bold>&#x02265;&#x000a0;</bold>897, ShapeFactor1 &#x02265; 0.0041, Compactness &#x02265; 0.79, the result is 2, that is, Barbunya, and the number of samples accounts for 7%.</td></tr><tr><td align="left">6</td><td align="left">When MajorAxisLength &#x02265; 281, Compactness &#x02265; 0.73, Perimeter &#x02265; 897, ShapeFactor1 &#x02265; 0.0041, Compactness &#x0003c; 0.79, roundness &#x0003c; 0.8, the result is 2, that is, Barbunya, and the number of samples accounts for 2%.</td></tr><tr><td align="left">7</td><td align="left">When MajorAxisLength &#x02265; 281, Compactness &#x02265; 0.73, Perimeter &#x02265; 897, ShapeFactor1 &#x02265; 0.0041, Compactness &#x0003c; 0.79, roundness &#x02265; 0.8, the result is 4, which is Cali, and the number of samples accounts for 12%.</td></tr><tr><td align="left">8</td><td align="left">When MajorAxisLength &#x02265; 281, Compactness &#x02265; 0.73, Perimeter &#x0003c; 897, roundness &#x02265; 0.92, the result is 1, that is, Serker, and the number of samples accounts for 1%.</td></tr><tr><td align="left">9</td><td align="left">When MajorAxisLength &#x02265; 281, Compactness &#x02265; 0.73, Perimeter &#x0003c; 897, roundness &#x0003c; 0.92, the result is 7, which is Dermason, and the number of samples accounts for 18%.</td></tr><tr><td align="left">10</td><td align="left">When MajorAxisLength &#x02265; 281, Compactness &#x0003c; 0.73, and MinorAxisLength &#x02265; 215, the result is 4, which is Cali, and the number of samples accounts for 1%.</td></tr><tr><td align="left">11</td><td align="left">When MajorAxisLength &#x02265; 281, Compactness &#x0003c; 0.73, and MinorAxisLength &#x0003c; 215, the result is 6, which is Sira, and the number of samples accounts for 13%.</td></tr></tbody></table></table-wrap></p><p id="Par46">From the decision rules of BLSMOTE + K-means + DT in Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>, it can be seen that the decision tree rules for Seker (the first type) are 1 and 8, the decision tree rules for Barbunya (the second type) are 5 and 6, the decision tree rules for Bombay (the third type) are only 4, the decision tree rules for Cali (the fourth type) are 7 and 10, the decision tree rules for Horoz (the fifth type) are only 3, the decision tree rules for Sira (the sixth type) are only 11, and the decision tree rules for Dermason (the seventh type) are 2 and 9.</p><p id="Par47">The training set classification accuracy of the BLSMOTE + K-means + DT is 93.29% for the obesity levels dataset, which is better than 90.41% of only DT. The BLSMOTE + K-means + DTt training set decision diagram is shown in Fig.&#x000a0;<xref rid="Fig11" ref-type="fig">11</xref>.<fig id="Fig11"><label>Fig. 11</label><caption><p>The BLSMOTE + K-means + DT training set decision diagram of obesity levels dataset.</p></caption><graphic xlink:href="41598_2024_82253_Fig11_HTML" id="d33e1784"/></fig></p><p id="Par48">It can be found from Fig.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref> that weight is the root node, and the value 100 is the split point. When Weight &#x0003c; 100, the left half of the tree is selected, and when Weight &#x02265; 100, the right half of the tree is selected, and then the leaf nodes are recursed. In Fig.&#x000a0;<xref rid="Fig11" ref-type="fig">11</xref>, the target variable content values &#x0200b;&#x0200b; (Insufficient_Weight, Normal_Weight, Overweight_Level_I, Overweight_Level_II, Obesity_Type_I, Obesity_Type_II, Obesity_Type_III) are replaced with 0 to 6 respectively using data transformation. The decision tree has a total of twenty-five decision rules, as shown in Table&#x000a0;<xref rid="Tab7" ref-type="table">7</xref>.<table-wrap id="Tab7"><label>Table 7</label><caption><p>The twenty five decision rules of the BLSMOTE + K-means + DT for obesity levels dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Rule</th><th align="left">Explanation</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">When Weight &#x0003c; 100, Weight &#x0003c; 60, Height &#x02265; 1.7, the result is 0, which means the population is underweight (Insufficient_Weight), and the sample size accounts for 9%.</td></tr><tr><td align="left">2</td><td align="left">When Weight &#x0003c; 100, Weight &#x0003c; 60, Height &#x0003c; 1.7, and Weight &#x0003c; 47, the result is 0, which means the population is underweight (Insufficient_Weight), and the sample size accounts for 4%.</td></tr><tr><td align="left">3</td><td align="left">When Weight &#x0003c; 100, Weigh t &#x0003c; 60, Height &#x0003c; 1.7, and Weight &#x02265; 47, the result is 1, which means the population is of normal weight (Normal Weight), and the sample size accounts for 6%.</td></tr><tr><td align="left">4</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x0003c; 76, and Height &#x02265; 1.7, the result is 1, which means the population is of normal weight (Normal Weight), and the sample size accounts for 4%.</td></tr><tr><td align="left">5</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x0003c; 76, Height &#x0003c; 1.7, FAVC &#x02265; 0.5, CAEC &#x02265; 2.5, the result is 1, which means the population is of normal weight (Normal_Weight), and the sample size accounts for 1%.</td></tr><tr><td align="left">6</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x0003c; 76, Height &#x0003c; 1.7, FAVC &#x02265; 0.5, and CAEC &#x0003c; 2.5, the result is 2, which means the overweight level I (Overweight_Level_I) population, and the sample size accounts for 9%.</td></tr><tr><td align="left">7</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x0003c; 76, Height &#x0003c; 1.7, FAVC &#x0003c; 0.5, Height &#x02265; 1.6, Weight &#x0003c; 67, the result is 1, which means the population is of normal weight (Normal_Weight), and the sample size accounts for 1%.</td></tr><tr><td align="left">8</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x0003c; 76, Height &#x0003c; 1.7, FAVC &#x0003c; 0.5, Height &#x02265; 1.6, Weight &#x02265; 67, the result is 2, which means the population is Overweight Level I (Overweight_Level_I), and the sample size accounts for 1%</td></tr><tr><td align="left">9</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x0003c; 76, Height &#x0003c; 1.7, FAVC &#x0003c; 0.5, Height &#x0003c; 1.6, the result is 3, which means the overweight level II (Overweight_Level_II) population, and the sample size accounts for 2%.</td></tr><tr><td align="left">10</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x02265; 76, Height &#x02265; 1.7, Weight &#x0003c; 90, Height &#x02265; 1.7, Height &#x02265; 1.8, Weight &#x0003c; 85, the result is 1, which means the population is of normal weight (Normal_Weight), and the sample size accounts for 1%.</td></tr><tr><td align="left">11</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x02265; 76, Height &#x02265; 1.7, Weight &#x0003c; 90, Height &#x02265; 1.7, Height &#x02265; 1.8, Weight &#x02265; 85, the result is 2, which means the overweight level I (Overweight_Level_I) population, and the sample number accounts for 3%.</td></tr><tr><td align="left">12</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x02265; 76, Height &#x02265; 1.7, Weight &#x0003c; 90, Height &#x02265; 1.7, Height &#x0003c; 1.8, Weight &#x0003c; 83, the result is 2, which means the overweight level I (Overweight_Level_I) population, and the sample number accounts for 2%.</td></tr><tr><td align="left">13</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x02265; 76, Height &#x02265; 1.7, Weight &#x0003c; 90, Height &#x02265; 1.7, Height &#x0003c; 1.8, Weight &#x02265; 83, the result is 3, which means the overweight level II (Overweight_Level_II) population, and the sample number accounts for 2%.</td></tr><tr><td align="left">14</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x02265; 76, Height &#x02265; 1.7, Weight &#x0003c; 90, Height &#x0003c; 1.7, the result is 3, which means the overweight level II (Overweight_Level_II) population, and the sample size accounts for 6%.</td></tr><tr><td align="left">15</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x02265; 76, Height &#x02265; 1.7, Weight &#x02265; 90, Height &#x02265; 1.8, the result is 3, which means the overweight level II (Overweight_Level_II) population, and the sample size accounts for 2%.</td></tr><tr><td align="left">16</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x02265; 76, Height &#x02265; 1.7, Weight &#x02265; 90, Height &#x0003c; 1.8, the result is 4, which means the obesity level I (Obesity_Type_I) population, and the sample size accounts for 3%.</td></tr><tr><td align="left">17</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x02265; 76, Height &#x0003c; 1.7, Height &#x02265; 1.6, Weight &#x0003c; 82, the result is 3, which means the overweight level II (Overweight_Level_II) population, and the sample size accounts for 2%.</td></tr><tr><td align="left">18</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x02265; 76, Height &#x0003c; 1.7, Height &#x02265; 1.6, Weight &#x02265; 82, the result is 4, which means the obesity level I (Obesity_Type_I) population, and the sample size accounts for 2%.</td></tr><tr><td align="left">19</td><td align="left">When Weight &#x0003c; 100, Weight &#x02265; 60, Weight &#x02265; 76, Height &#x0003c; 1.7, Height &#x0003c; 1.6, the result is 4, which means the obesity level I (Obesity_Type_I) population, and the sample size accounts for 6%.</td></tr><tr><td align="left">20</td><td align="left">When Weight &#x02265; 100, Gender &#x02265; 0.5, Age &#x0003c; 23, FCVC &#x0003c; 2.6, the result is 4, which means the obesity type I (Obesity_Type_I) population, and the sample size accounts for 3%.</td></tr><tr><td align="left">21</td><td align="left">When Weight &#x02265; 100, Gender &#x02265; 0.5, Age &#x0003c; 23, and FCVC &#x02265; 2.6, the result is 5, which means the obesity type II (Obesity_Type_II) population, and the sample size accounts for 1%.</td></tr><tr><td align="left">22</td><td align="left">When Weight &#x02265; 100, Gender &#x02265; 0.5, Age &#x02265; 23, Weight &#x0003c; 110, CALA &#x02265; 1.5, the result is 4, which means the obesity type I (Obesity_Type_I) population, and the sample size accounts for 2%.</td></tr><tr><td align="left">23</td><td align="left">When Weight &#x02265; 100, Gender &#x02265; 0.5, Age &#x02265; 23, Weight &#x0003c; 110, CALA &#x0003c; 1.5, the result is 5, which means the obesity type II (Obesity_Type_II) population, and the sample size accounts for 2%.</td></tr><tr><td align="left">24</td><td align="left">When Weight &#x02265; 100, Gender &#x02265; 0.5, Age &#x02265; 23, and Weight &#x02265; 110, the result is 5, which means the obesity type II (Obesity_Type_II) population, and the sample size accounts for 10%.</td></tr><tr><td align="left">25</td><td align="left">When Weight &#x02265; 100 and Gender &#x0003c; 0.5, the result is 6, which means the obesity level III (Obesity_Type_III) population, accounting for 16% of the sample size.</td></tr></tbody></table></table-wrap></p><p id="Par49">It can be found from Table&#x000a0;<xref rid="Tab7" ref-type="table">7</xref> that the decision rules for Insufficient_Weight (the first type) are 1 and 2, and the decision rules for Normal_Weight (the second type) are 3, 4, 5, 7 and 10. The decision rules that result in Overweight_Level_I (the third type) are 6, 8, 11, and 12, and the decision rules that result in Overweight_Level_II (the fourth type) are 9, 13, 14, 15, and 17. The decision rules that result in Obesity_Type_I (the fifth type) are 16, 18, 19, 20, and 22, and the decision rules that result in Obesity_Type_II (the sixth type) are 21, 23, and 24. The result is that the decision rule for Obesity_Type_III (the seventh type) is only 25.</p></sec><sec id="Sec17"><title>The analysis and discussion of the BLSMOTE + K-means + RF</title><p id="Par50">The dry bean dataset in the study of BLSMOTE + K-means + RF, ntree was set to 100, and the training set classification accuracy of the BLSMOTE + K-means + RF is 92.58% for the dry bean dataset, which is better than 92.46% of only RF. The importance ranking of dry bean features is based on the average impurity reduction as the indicator, which is the weighted average of the Gini impurity indicator reduction within the random forest range. The larger the indicator, the more important the attribute is. The average impurity reduction value of dry bean features by BLSMOTE + K-means + RF is shown in Fig.&#x000a0;<xref rid="Fig12" ref-type="fig">12</xref>. The dry bean feature importance ranking of BLSMOTE + K-means + RF is given, as shown in Fig.&#x000a0;<xref rid="Fig13" ref-type="fig">13</xref>.<fig id="Fig12"><label>Fig. 12</label><caption><p>The average impurity reduction value of dry bean features of the BLSMOTE + K-means + RF.</p></caption><graphic xlink:href="41598_2024_82253_Fig12_HTML" id="d33e1959"/></fig><fig id="Fig13"><label>Fig. 13</label><caption><p>The BLSMOTE + K-means + RF feature importance ranking diagram of dry bean.</p></caption><graphic xlink:href="41598_2024_82253_Fig13_HTML" id="d33e1966"/></fig></p><p id="Par51">As shown in Figs.&#x000a0;<xref rid="Fig12" ref-type="fig">12</xref> and <xref rid="Fig13" ref-type="fig">13</xref>, it can be found that the order of importance of dry bean features is: ShapeFactor3 &#x0003e; ShapeFactor1 &#x0003e; Compactness &#x0003e; AspectRation &#x0003e; MinorAxisLength &#x0003e; Eccentricity &#x0003e; roundness &#x0003e; ConvexArea &#x0003e; Perimeter &#x0003e; EquivDiameter &#x0003e; MajorAxisLength &#x0003e; Area &#x0003e; ShapeFactor4 &#x0003e; ShapeFactor2 &#x0003e; Solidity &#x0003e; Extent. Among them, ShapeFactor3 has the greatest impact on the dry bean classification results. The main reason is that the appearance shapes of different dry beans vary greatly, and it is easier to determine the category of dry beans through the ShapeFactor3 feature.</p><p id="Par52">The obesity levels dataset in the study of BLSMOTE + K-means + RF, ntree was set to 100, and the training set classification accuracy of the BLSMOTE + K-means + RF is 99.13% for the obesity levels dataset, which is better than 97.99% of only RF. The importance ranking of obesity levels features is based on the average impurity reduction as the indicator, which is the weighted average of the Gini impurity indicator reduction within the random forest range. The larger the indicator, the more important the attribute is. The average impurity reduction of obesity levels features by BLSMOTE + K-means + RF is shown in Fig.&#x000a0;<xref rid="Fig14" ref-type="fig">14</xref>. The obesity levels feature importance ranking of BLSMOTE + K-means + RF is given, as shown in Fig.&#x000a0;<xref rid="Fig15" ref-type="fig">15</xref>.<fig id="Fig14"><label>Fig. 14</label><caption><p>The average impurity reduction value of obesity levels features of the BLSMOTE +K-means + RF.</p></caption><graphic xlink:href="41598_2024_82253_Fig14_HTML" id="d33e1989"/></fig><fig id="Fig15"><label>Fig. 15</label><caption><p>The BLSMOTE + K-means + RF feature importance ranking diagram of obesity levels dataset.</p></caption><graphic xlink:href="41598_2024_82253_Fig15_HTML" id="d33e1996"/></fig></p><p id="Par53">As shown in Figs.&#x000a0;<xref rid="Fig14" ref-type="fig">14</xref> and <xref rid="Fig15" ref-type="fig">15</xref>, it can be found that the order of importance of obesity levels features is: Weight &#x0003e;family_history_with_over &#x0003e; Age &#x0003e; CAEC &#x0003e; FCVC &#x0003e; Height &#x0003e; NCP &#x0003e; Gender &#x0003e; FAF &#x0003e; TUE &#x0003e; MTRANS &#x0003e; CH2O &#x0003e; CALC &#x0003e; FAVC &#x0003e; SCC &#x0003e; SMOKE. Among them, the feature (Weight) has the greatest impact on the dry bean classification results.</p></sec></sec><sec id="Sec18" sec-type="conclusion"><title>Conclusions</title><p id="Par54">This study combined BLSMOTE + K-means with machine learning, namely BLSMOTE + K-means + SVM, BLSMOTE + K-means + DT, and BLSMOTE + K-means + RF, to eslish prediction models for dry bean datasets and obesity level datasets to improve the classification performance of traditional machine learning methods. The training set classification accuracy of BLSMOTE + K-means + SVM on the dry bean dataset is 98.86%, which is better than the 94.01% of only SVM. The training set classification accuracy of BLSMOTE + K-means + SVM on the obesity level dataset is 99.57%, which is better than the 98.99% of only SVM. The training set classification accuracy of BLSMOTE + K-means + RF on the dry bean dataset is 92.58%, which is better than the 92.46% of only RF. The training set classification accuracy of BLSMOTE + K-means + RF on the obesity level dataset is 99.13%, which is better than the 97.99% of only RF. The training set classification accuracy of BLSMOTE + K-means + DT on the dry bean dataset is 89.98%, which is better than the 89.20% of only DT. The training set classification accuracy of BLSMOTE + K-means + DT on the obesity level dataset is 93.29%, which is better than the 90.41% of only DT. In addition, the precision, recall, f1-score, and AUC performance indicators of BLSMOTE + K-means + SVM on the dry bean dataset are 0.9736, 0.9743, 09739, and 0.9831, respectively, which are better than 0.9402, 0.9437, 0.9419, and 0.9527 of only SVM. In addition to precision, the recall, f1-score, and AUC performance indicators of BLSMOTE + K-means + RF on the dry bean dataset are 0.9183, 0.9091, and 0.9277, respectively, which are better than 0.9071, 0.9062 and 0.9245, of only RF. The precision, recall, f1-score, and AUC performance indicators of BLSMOTE + K-means + DT on the dry bean dataset are 0.9098, 0.9069, 0.9083, and 0.9102, respectively, which are better than 0.9042, 0.8959, 0.9000 and 0.9088 of only DT. The precision, recall, f1-score, and AUC performance indicators of BLSMOTE + K-means + SVM on the obesity level dataset are 0.9736, 0.9743, 0.9739, and 0.9885, respectively, which are better than 0.9402, 0.9437, 0.9419, and 0.9807 of only SVM. The precision, recall, f1-score, and AUC performance indicators of BLSMOTE + K-means + RF on the obesity level dataset are 0.9046, 0.9256, 0.9150, and 0.9835, respectively, which are better than 0.8668, 0.8754, 0.8711, and 0.9747 of only RF. The precision, recall, f1- score, and AUC performance indicators of BLSMOTE + K-means + DT on the obesity level dataset are 0.8060, 0.8765, 0.8398, and 0.9642, respectively, which are better than 0.8052, 0.8151, 0.8101, and 0.9312 of only DT. The experimental results show that the BLSMOTE + K-means + SVM, BLSMOTE + K-means + RF, and BLSMOTE + K-means + DT proposed in this study have indeed improved the traditional only SVM, only RF, only DT in classification accuracy and precision, recall, f1-score, and AUC performance indicators. Because the imbalanced data is first processed by the BLSMOTE, its advantage is that it uses samples on the boundary of minority class samples to generate new samples, which can reduce the impact of noise on model building; and the advantage of K-means clustering is that it can divide data into different groups based on similarities or common features.</p><p id="Par55">In this study, the BLSMOTE + K-means + DT generated eleven decision rules for the dry bean dataset and twenty-five decision rules for the obesity levels dataset. BLSMOTE + K-means + RF also gave the importance ranking of the explanatory variables for the above two datasets. From the experimental results, the performance of the proposed algorithm can indeed effectively improve the traditional machine learning method. The following suggestions are made for future work:</p><p id="Par56">(1) In future work, we can consider using intelligent algorithms to optimize the parameters of SVM to improve classification accuracy.</p><p id="Par57">(2) In the future, we can use Adaboost, and XGB as a comparison with the BLSMOTE + K-means + SVM, BLSMOTE + K-means + DT, and BLSMOTE + K-means + RF proposed in this paper, which is believed to enrich the content of this study.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This research is supported by Fuzhou University of International Studies and Trade under Grant Nos. 2018KYTD-05, FJTPY-2020009, FJKTP-2021005 and BRJF-01.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>This manuscript was written and independently completed in research design and data analysis by C. Y. L. and W. W., data curation by J. Q. H., writing-review and editing by C. Y. L. and J. Q. H. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>All data generated or analyzed during this study are included in this published paper.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par58">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Carre&#x000f1;o Siqueira</surname><given-names>JA</given-names></name><etal/></person-group><article-title>The use of photosynthetic pigments and SPAD can help in the selection of bean genotypes under fertilization organic and mineral</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>22610</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-49582-4</pub-id><pub-id pub-id-type="pmid">38114650</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Carre&#x000f1;o Siqueira, J. A. et al. The use of photosynthetic pigments and SPAD can help in the selection of bean genotypes under fertilization organic and mineral. <italic>Sci. Rep.</italic><bold>13</bold>, 22610 (2023).<pub-id pub-id-type="pmid">38114650</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Rodr&#x00131;guez-Pulido</surname><given-names>FJ</given-names></name><etal/></person-group><article-title>Research progress in imaging technology for assessing quality in wine grapes and seeds</article-title><source>Foods</source><year>2022</year><volume>11</volume><fpage>254</fpage><pub-id pub-id-type="doi">10.3390/foods11030254</pub-id><pub-id pub-id-type="pmid">35159406</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Rodr&#x00131;guez-Pulido, F. J. et al. Research progress in imaging technology for assessing quality in wine grapes and seeds. <italic>Foods</italic><bold>11</bold>, 254 (2022).<pub-id pub-id-type="pmid">35159406</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Shahoveisi</surname><given-names>F</given-names></name><name><surname>Riahi Manesh</surname><given-names>M</given-names></name></person-group><article-title>Del R&#x000ed;o Mendoza, L.E. modeling risk of Sclerotinia sclerotiorum-induced disease development on canola and dry bean using machine learning algorithms</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>864</fpage><pub-id pub-id-type="doi">10.1038/s41598-021-04743-1</pub-id><pub-id pub-id-type="pmid">35039560</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Shahoveisi, F. &#x00026; Riahi Manesh, M. Del R&#x000ed;o Mendoza, L.E. modeling risk of Sclerotinia sclerotiorum-induced disease development on canola and dry bean using machine learning algorithms. <italic>Sci. Rep.</italic><bold>12</bold>, 864 (2022).<pub-id pub-id-type="pmid">35039560</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Mendigoria, C. H. et al. Seed architectural phenes prediction and variety classification of dry beans using machine learning algorithms. <italic>IEEE 9th Reg. 10 Humanitarian Technol. Conf.</italic>, 1&#x02013;6 (2021).</mixed-citation></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Debnath</surname><given-names>T</given-names></name><name><surname>Nakamoto</surname><given-names>T</given-names></name></person-group><article-title>Predicting individual perceptual scent impression from imbalanced dataset using mass spectrum of odorant molecules</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>3778</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-07802-3</pub-id><pub-id pub-id-type="pmid">35260669</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Debnath, T. &#x00026; Nakamoto, T. Predicting individual perceptual scent impression from imbalanced dataset using mass spectrum of odorant molecules. <italic>Sci. Rep.</italic><bold>12</bold>, 3778 (2022).<pub-id pub-id-type="pmid">35260669</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><etal/></person-group><article-title>Research on expansion and classification of imbalanced data based on SMOTE algorithm</article-title><source>Sci. Rep.</source><year>2021</year><volume>11</volume><fpage>24039</fpage><pub-id pub-id-type="doi">10.1038/s41598-021-03430-5</pub-id><pub-id pub-id-type="pmid">34912009</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Wang, S. et al. Research on expansion and classification of imbalanced data based on SMOTE algorithm. <italic>Sci. Rep.</italic><bold>11</bold>, 24039 (2021).<pub-id pub-id-type="pmid">34912009</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>L</given-names></name><etal/></person-group><article-title>Stacking rockburst based on Yeo&#x02013;Johnson, K-means SMOTE, and optimal rockburst feature dimension determination</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>15352</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-19669-5</pub-id><pub-id pub-id-type="pmid">36097043</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Sun, L. et al. Stacking rockburst based on Yeo&#x02013;Johnson, K-means SMOTE, and optimal rockburst feature dimension determination. <italic>Sci. Rep.</italic><bold>12</bold>, 15352 (2022).<pub-id pub-id-type="pmid">36097043</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Katz</surname><given-names>R</given-names></name><name><surname>Sridhar</surname><given-names>D</given-names></name></person-group><article-title>A decision-making tree for policy responses to a pathogen with pandemic potential</article-title><source>Nat. Med.</source><year>2024</year><volume>30</volume><fpage>327</fpage><lpage>329</lpage><pub-id pub-id-type="doi">10.1038/s41591-023-02755-0</pub-id><pub-id pub-id-type="pmid">38321221</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Katz, R. &#x00026; Sridhar, D. A decision-making tree for policy responses to a pathogen with pandemic potential. <italic>Nat. Med.</italic><bold>30</bold>, 327&#x02013;329 (2024).<pub-id pub-id-type="pmid">38321221</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Yan, Y. et al. Climate-induced tree-mortality pulses are obscured by broad-scale and long-term greening. <italic>Nat. Ecol. Evol.</italic>, 1&#x02013;12 (2024).</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">B&#x000fc;ntgen, U. et al. The influence of decision-making in tree ring-based climate reconstructions. <italic>Nat. Commun.</italic><bold>12</bold>, 3411 (2021).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Yang, X. et al. Multi-source information fusion-driven corn yield prediction using the Random Forest from the perspective of Agricultural and Forestry Economic Management. <italic>Sci. Rep.</italic><bold>14</bold>, 4052 (2024).</mixed-citation></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Managi</surname><given-names>S</given-names></name></person-group><article-title>Mental health and natural land cover: a global analysis based on random forest with geographical consideration</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>2894</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-53279-7</pub-id><pub-id pub-id-type="pmid">38316893</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Li, C. &#x00026; Managi, S. Mental health and natural land cover: a global analysis based on random forest with geographical consideration. <italic>Sci. Rep.</italic><bold>14</bold>, 2894 (2024).<pub-id pub-id-type="pmid">38316893</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazwani</surname><given-names>M</given-names></name><name><surname>Begum</surname><given-names>MY</given-names></name></person-group><article-title>Computational intelligence modeling of hyoscine drug solubility and solvent density in supercritical processing: gradient boosting, extra trees, and random forest models</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>10046</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-37232-8</pub-id><pub-id pub-id-type="pmid">37344621</pub-id>
</element-citation><mixed-citation id="mc-CR13" publication-type="journal">Ghazwani, M. &#x00026; Begum, M. Y. Computational intelligence modeling of hyoscine drug solubility and solvent density in supercritical processing: gradient boosting, extra trees, and random forest models. <italic>Sci. Rep.</italic><bold>13</bold>, 10046 (2023).<pub-id pub-id-type="pmid">37344621</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Niu</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Selection and prediction of metro station sites based on spatial data and random forest: a study of Lanzhou, China</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>22542</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-49877-6</pub-id><pub-id pub-id-type="pmid">38110563</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Niu, Q. et al. Selection and prediction of metro station sites based on spatial data and random forest: a study of Lanzhou, China. <italic>Sci. Rep.</italic><bold>13</bold>, 22542 (2023).<pub-id pub-id-type="pmid">38110563</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>X</given-names></name><etal/></person-group><article-title>Reconfigurable mixed-kernel heterojunction transistors for personalized support vector machine classification</article-title><source>Nat. Electron.</source><year>2023</year><volume>6</volume><fpage>862</fpage><lpage>869</lpage><pub-id pub-id-type="doi">10.1038/s41928-023-01042-7</pub-id></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Yan, X. et al. Reconfigurable mixed-kernel heterojunction transistors for personalized support vector machine classification. <italic>Nat. Electron.</italic><bold>6</bold>, 862&#x02013;869 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Che</surname><given-names>A</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name></person-group><article-title>Seismic landslide susceptibility assessment using principal component analysis and support vector machine</article-title><source>Sc i Rep.</source><year>2024</year><volume>14</volume><fpage>3734</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-48196-0</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Xu, Z., Che, A. &#x00026; Zhou, H. Seismic landslide susceptibility assessment using principal component analysis and support vector machine. <italic>Sc i Rep.</italic><bold>14</bold>, 3734 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Schreurs, M. et al. Predicting and improving complex beer flavor through machine learning. <italic>Nat. Commun.</italic><bold>15</bold>, 2368 (2024).</mixed-citation></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Self-learning spatial distribution-based intrusion detection for industrial cyber-physical systems</article-title><source>IEEE Trans. Comput. Social Syst.</source><year>2022</year><volume>9</volume><fpage>1693</fpage><lpage>1702</lpage><pub-id pub-id-type="doi">10.1109/TCSS.2021.3135586</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Gao, Y. et al. Self-learning spatial distribution-based intrusion detection for industrial cyber-physical systems. <italic>IEEE Trans. Comput. Social Syst.</italic><bold>9</bold>, 1693&#x02013;1702 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Premkumar</surname><given-names>M</given-names></name><etal/></person-group><article-title>Augmented weighted K-means grey wolf optimizer: an enhanced metaheuristic algorithm for data clustering problems</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>5434</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-55619-z</pub-id><pub-id pub-id-type="pmid">38443569</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Premkumar, M. et al. Augmented weighted K-means grey wolf optimizer: an enhanced metaheuristic algorithm for data clustering problems. <italic>Sci. Rep.</italic><bold>14</bold>, 5434 (2024).<pub-id pub-id-type="pmid">38443569</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><etal/></person-group><article-title>A hybrid interprele deep structure based on adaptive neuro-fuzzy inference system, decision tree, and K-means for intrusion detection</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>20770</fpage><pub-id pub-id-type="doi">10.1038/s41598-022-23765-x</pub-id><pub-id pub-id-type="pmid">36456582</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Liu, J. et al. A hybrid interprele deep structure based on adaptive neuro-fuzzy inference system, decision tree, and K-means for intrusion detection. <italic>Sci. Rep.</italic><bold>12</bold>, 20770 (2022).<pub-id pub-id-type="pmid">36456582</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Booth</surname><given-names>FG</given-names></name><name><surname>Bond</surname><given-names>R</given-names></name><name><surname>Mulvenna</surname><given-names>RD</given-names></name></person-group><article-title>Discovering and comparing types of general practitioner practices using geolocational features and prescribing behaviours by means of K-means clustering</article-title><source>Sci. Rep.</source><year>2021</year><volume>11</volume><fpage>18289</fpage><pub-id pub-id-type="doi">10.1038/s41598-021-97716-3</pub-id><pub-id pub-id-type="pmid">34521920</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Booth, F. G., Bond, R. &#x00026; Mulvenna, R. D. Discovering and comparing types of general practitioner practices using geolocational features and prescribing behaviours by means of K-means clustering. <italic>Sci. Rep.</italic><bold>11</bold>, 18289 (2021).<pub-id pub-id-type="pmid">34521920</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>S</given-names></name><etal/></person-group><article-title>Label-aware distance mitigates temporal and spatial variability for clustering and visualization of single-cell gene expression data</article-title><source>Commun. Biol.</source><year>2024</year><volume>7</volume><fpage>326</fpage><pub-id pub-id-type="doi">10.1038/s42003-024-05988-y</pub-id><pub-id pub-id-type="pmid">38486077</pub-id>
</element-citation><mixed-citation id="mc-CR22" publication-type="journal">Liang, S. et al. Label-aware distance mitigates temporal and spatial variability for clustering and visualization of single-cell gene expression data. <italic>Commun. Biol.</italic><bold>7</bold>, 326 (2024).<pub-id pub-id-type="pmid">38486077</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Sandstr&#x000f6;m</surname><given-names>V</given-names></name><etal/></person-group><article-title>Food system by-products upcycled in livestock and aquaculture feeds can increase global food supply</article-title><source>Nat. Food</source><year>2022</year><volume>3</volume><fpage>729</fpage><lpage>740</lpage><pub-id pub-id-type="doi">10.1038/s43016-022-00589-6</pub-id><pub-id pub-id-type="pmid">37118146</pub-id>
</element-citation><mixed-citation id="mc-CR23" publication-type="journal">Sandstr&#x000f6;m, V. et al. Food system by-products upcycled in livestock and aquaculture feeds can increase global food supply. <italic>Nat. Food</italic>. <bold>3</bold>, 729&#x02013;740 (2022).<pub-id pub-id-type="pmid">37118146</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Koklu</surname><given-names>M</given-names></name><name><surname>Ozkan</surname><given-names>LA</given-names></name></person-group><article-title>Multiclass classification of dry beans using computer vision and machine learning techniques</article-title><source>Comput. Electron. Agric.</source><year>2020</year><volume>174</volume><fpage>105507</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2020.105507</pub-id></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Koklu, M. &#x00026; Ozkan, L. A. Multiclass classification of dry beans using computer vision and machine learning techniques. <italic>Comput. Electron. Agric.</italic><bold>174</bold>, 105507 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Parlak</surname><given-names>B</given-names></name><name><surname>Uysal</surname><given-names>AK</given-names></name></person-group><article-title>A novel filter feature selection method for text classification: extensive feature selector</article-title><source>J. Inf. Sci.</source><year>2023</year><volume>49</volume><issue>1</issue><fpage>59</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1177/0165551521991037</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Parlak, B. &#x00026; Uysal, A. K. A novel filter feature selection method for text classification: extensive feature selector. <italic>J. Inf. Sci.</italic><bold>49</bold> (1), 59&#x02013;78 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Parlak</surname><given-names>B</given-names></name><name><surname>Uysal</surname><given-names>AK</given-names></name></person-group><article-title>The effects of globalisation techniques on feature selection for text classification</article-title><source>J. Inf. Sci.</source><year>2021</year><volume>47</volume><issue>6</issue><fpage>727</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1177/0165551520930897</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Parlak, B. &#x00026; Uysal, A. K. The effects of globalisation techniques on feature selection for text classification. <italic>J. Inf. Sci.</italic><bold>47</bold> (6), 727&#x02013;739 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Parlak</surname><given-names>B</given-names></name></person-group><article-title>A novel feature and class-based globalization technique for text classification</article-title><source>Multimedia Tools Appl.</source><year>2023</year><volume>82</volume><issue>24</issue><fpage>37635</fpage><lpage>37660</lpage><pub-id pub-id-type="doi">10.1007/s11042-023-15459-x</pub-id></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Parlak, B. A novel feature and class-based globalization technique for text classification. <italic>Multimedia Tools Appl.</italic><bold>82</bold> (24), 37635&#x02013;37660 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><etal/></person-group><article-title>Developing a machine learning model for accurate nucleoside hydrogels prediction based on descriptors</article-title><source>Nat. Commun.</source><year>2024</year><volume>15</volume><fpage>2603</fpage><pub-id pub-id-type="doi">10.1038/s41467-024-46866-9</pub-id><pub-id pub-id-type="pmid">38521777</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Li, W. et al. Developing a machine learning model for accurate nucleoside hydrogels prediction based on descriptors. <italic>Nat. Commun.</italic><bold>15</bold>, 2603 (2024).<pub-id pub-id-type="pmid">38521777</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Sharkas</surname><given-names>M</given-names></name><name><surname>Attallah</surname><given-names>O</given-names></name></person-group><article-title>Color-CADx: a deep learning approach for colorectal cancer classification through triple convolutional neural networks and discrete cosine transform</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>6914</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-56820-w</pub-id><pub-id pub-id-type="pmid">38519513</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Sharkas, M. &#x00026; Attallah, O. Color-CADx: a deep learning approach for colorectal cancer classification through triple convolutional neural networks and discrete cosine transform. <italic>Sci. Rep.</italic><bold>14</bold>, 6914 (2024).<pub-id pub-id-type="pmid">38519513</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>