<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006453</article-id><article-id pub-id-type="pmc">PMC11860563</article-id><article-id pub-id-type="doi">10.3390/s25041224</article-id><article-id pub-id-type="publisher-id">sensors-25-01224</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>GazeCapsNet: A Lightweight Gaze Estimation Framework</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6223-4502</contrib-id><name><surname>Muksimova</surname><given-names>Shakhnoza</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-01224" ref-type="aff">1</xref><xref rid="fn1-sensors-25-01224" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9418-591X</contrib-id><name><surname>Valikhujaev</surname><given-names>Yakhyokhuja</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af2-sensors-25-01224" ref-type="aff">2</xref><xref rid="fn1-sensors-25-01224" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Umirzakova</surname><given-names>Sabina</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><xref rid="af1-sensors-25-01224" ref-type="aff">1</xref><xref rid="c1-sensors-25-01224" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Baltayev</surname><given-names>Jushkin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af3-sensors-25-01224" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0184-7599</contrib-id><name><surname>Cho</surname><given-names>Young Im</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-01224" ref-type="aff">1</xref><xref rid="c1-sensors-25-01224" ref-type="corresp">*</xref></contrib></contrib-group><aff id="af1-sensors-25-01224"><label>1</label>Department of Computer Engineering, Gachon University, Sujeong-gu, Seongnam-si 461-701, Republic of Korea; <email>shakhnoza02@gachon.ac.kr</email></aff><aff id="af2-sensors-25-01224"><label>2</label>Aria Studios Co., Ltd., Seoul 06247, Republic of Korea; <email>yakhyo9696@gmail.com</email></aff><aff id="af3-sensors-25-01224"><label>3</label>Department of Information Systems and Technologies of the Tashkent State University of Economic, Tashkent 100066, Uzbekistan; <email>j_baltayev@tsue.uz</email></aff><author-notes><corresp id="c1-sensors-25-01224"><label>*</label>Correspondence: <email>sabinatuit@gachon.ac.kr</email> (S.U.); <email>yicho@gachon.ac.kr</email> (Y.I.C.)</corresp><fn id="fn1-sensors-25-01224"><label>&#x02020;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>17</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1224</elocation-id><history><date date-type="received"><day>27</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>05</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>14</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Gaze estimation is increasingly pivotal in applications spanning virtual reality, augmented reality, and driver monitoring systems, necessitating efficient yet accurate models for mobile deployment. Current methodologies often fall short, particularly in mobile settings, due to their extensive computational requirements or reliance on intricate pre-processing. Addressing these limitations, we present Mobile-GazeCapsNet, an innovative gaze estimation framework that harnesses the strengths of capsule networks and integrates them with lightweight architectures such as MobileNet v2, MobileOne, and ResNet-18. This framework not only eliminates the need for facial landmark detection but also significantly enhances real-time operability on mobile devices. Through the innovative use of Self-Attention Routing, GazeCapsNet dynamically allocates computational resources, thereby improving both accuracy and efficiency. Our results demonstrate that GazeCapsNet achieves competitive performance by optimizing capsule networks for gaze estimation through Self-Attention Routing (SAR), which replaces iterative routing with a lightweight attention-based mechanism, improving computational efficiency. Our results show that GazeCapsNet achieves state-of-the-art (SOTA) performance on several benchmark datasets, including ETH-XGaze and Gaze360, achieving a mean angular error (MAE) reduction of up to 15% compared to existing models. Furthermore, the model maintains a real-time processing capability of 20 milliseconds per frame while requiring only 11.7 million parameters, making it exceptionally suitable for real-time applications in resource-constrained environments. These findings not only underscore the efficacy and practicality of GazeCapsNet but also establish a new standard for mobile gaze estimation technologies.</p></abstract><kwd-group><kwd>eye appearance</kwd><kwd>capsule networks</kwd><kwd>self-attention routing mechanism</kwd><kwd>lightweight architectures</kwd><kwd>gaze estimation</kwd></kwd-group><funding-group><award-group><funding-source>Korean Agency for Technology and Standards under the Ministry of Trade, Industry, and Energy</funding-source><award-id>1415181629</award-id></award-group><funding-statement>This paper was supported by the Korean Agency for Technology and Standards under the Ministry of Trade, Industry, and Energy in 2023. The project number is 1415181629 (Development of International Standard Technologies based on AI Model Lightweighting Technologies).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01224"><title>1. Introduction</title><p>Estimating gaze from facial images is crucial for understanding human cognition and behavior. Currently, gaze estimation plays a vital role across various fields such as virtual reality (VR) [<xref rid="B1-sensors-25-01224" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01224" ref-type="bibr">2</xref>], human&#x02013;computer interaction [<xref rid="B3-sensors-25-01224" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01224" ref-type="bibr">4</xref>], semi-autonomous driving [<xref rid="B5-sensors-25-01224" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01224" ref-type="bibr">6</xref>], and psychological studies [<xref rid="B7-sensors-25-01224" ref-type="bibr">7</xref>]. Paper [<xref rid="B8-sensors-25-01224" ref-type="bibr">8</xref>] presents a mobile gaze estimation framework that utilizes a real-time algorithm operating at 30 Hz. It predicts eye movements from sequential frames with high accuracy and fewer parameters, enhancing AR/VR interactions. Achieving real-time performance in mobile environments poses significant challenges due to the limited processing capabilities of these platforms. Traditional convolutional neural networks (CNNs), while robust in various visual recognition tasks [<xref rid="B9-sensors-25-01224" ref-type="bibr">9</xref>], often struggle to capture the complex spatial hierarchies essential for accurate gaze prediction [<xref rid="B10-sensors-25-01224" ref-type="bibr">10</xref>]. While MobileNet v2 and ResNet-18 have previously been combined for various vision tasks [<xref rid="B11-sensors-25-01224" ref-type="bibr">11</xref>], their integration has not been extensively explored for gaze estimation. Previous studies utilizing this combination primarily focus on image classification [<xref rid="B12-sensors-25-01224" ref-type="bibr">12</xref>] and object detection [<xref rid="B13-sensors-25-01224" ref-type="bibr">13</xref>], where feature extraction is performed at fixed spatial scales without considering hierarchical facial feature relationships. In contrast, our GazeCapsNet framework leverages this hybrid feature extraction strategy within a capsule network-based architecture, where SAR dynamically prioritizes relevant gaze-related regions. This allows our model to retain spatial dependencies across different head poses and lighting conditions, improving generalization in real-time gaze-tracking applications.</p><p>At the core of our approach is the use of capsule networks (CapsNets) [<xref rid="B14-sensors-25-01224" ref-type="bibr">14</xref>] with Self-Attention Routing (SAR). CapsNets address some limitations of traditional CNNs by preserving spatial hierarchies between features, which is crucial for tasks like gaze estimation. The SAR mechanism further enhances the model&#x02019;s efficiency by focusing computational resources on the most relevant features for accurate gaze prediction, adapting dynamically to various real-world conditions. This study rigorously evaluates GazeCapsNet across multiple benchmark datasets, including ETH-XGaze, Gaze360, and MPIIFaceGaze, which feature various head poses, lighting conditions, and gaze directions. These datasets provide a comprehensive platform for demonstrating the robustness of our model in both controlled and uncontrolled environments.</p><p>Our work presents GazeCapsNet, a novel, lightweight deep learning framework for real-time gaze estimation. The key contributions of this study are as follows:<list list-type="bullet"><list-item><p>We introduce a capsule network-based architecture with SAR, which enhances gaze estimation accuracy while reducing computational overhead.</p></list-item><list-item><p>By integrating MobileNet v2 and ResNet-18, we achieve a balance between low-latency inference and robust feature extraction, enabling real-time performance.</p></list-item><list-item><p>Unlike traditional methods that rely on intermediate steps like facial landmark detection, GazeCapsNet predicts 3D gaze direction directly from raw images, improving efficiency and generalization.</p></list-item><list-item><p>With an inference time of 20 ms per frame and only 11.7 M parameters, our model is suitable for resource-constrained environments such as AR/VR and driver monitoring systems.</p></list-item><list-item><p>Extensive evaluations on benchmark datasets (ETH-XGaze, Gaze360, MPIIFaceGaze) demonstrate that GazeCapsNet achieves competitive accuracy while maintaining a lightweight architecture.</p></list-item></list></p><p>Our work introduces a highly efficient, real-time gaze estimation model that integrates capsule networks with Self-Attention Routing, optimized for mobile and resource-constrained devices. We provide a comprehensive solution that balances high accuracy, scalability, and deployment efficiency, setting a new standard for gaze estimation in both controlled and real-world environments.</p><p>The remainder of this paper is organized as follows: <xref rid="sec2-sensors-25-01224" ref-type="sec">Section 2</xref> reviews existing advancements in lightweight mobile models and the integration of capsule networks for gaze estimation. <xref rid="sec3-sensors-25-01224" ref-type="sec">Section 3</xref>, the methodology section, delves into the architecture of the proposed Mobile-GazeCapsNet, detailing its innovative features such as multi-scale feature aggregation and boundary refinement techniques. In <xref rid="sec4-sensors-25-01224" ref-type="sec">Section 4</xref>, the paper describes the experimental setup, including the datasets used, and evaluation metrics, and presents the performance results, comparing them to current state-of-the-art models. <xref rid="sec5-sensors-25-01224" ref-type="sec">Section 5</xref>, the Discussion and Limitations Section, evaluates the outcomes, acknowledges the model constraints, and suggests directions for future research. Finally, <xref rid="sec6-sensors-25-01224" ref-type="sec">Section 6</xref>, the conclusion, summarizes the contributions of the study and its implications for real-time applications in gaze estimation.</p></sec><sec id="sec2-sensors-25-01224"><title>2. Related Work</title><p>Traditional gaze estimation methods typically rely on CNNs for feature extraction, which are then followed by regression or classification models to predict gaze direction. Recent advancements in deep learning have led to the adoption of more sophisticated architectures such as ResNet and DenseNet. These models improve gaze prediction accuracy but are associated with significant computational costs. Capsule networks, introduced in [<xref rid="B14-sensors-25-01224" ref-type="bibr">14</xref>], have shown promise in accurately representing spatial hierarchies of image features, making them particularly suitable for gaze estimation tasks. Other promising models such as MobileNet v2 [<xref rid="B15-sensors-25-01224" ref-type="bibr">15</xref>] and MobileOne [<xref rid="B16-sensors-25-01224" ref-type="bibr">16</xref>] are prominent mobile-optimized architectures, designed to reduce computation and memory usage effectively. MobileNet v2 utilizes inverted residuals and linear bottlenecks to minimize computational overhead, while MobileOne is tailored to enhance real-time inference with minimal latency. These architectures enable the efficient performance of complex vision tasks, such as gaze estimation, on mobile devices.</p><p>GazeCaps [<xref rid="B17-sensors-25-01224" ref-type="bibr">17</xref>] introduced the use of capsule networks for gaze estimation. The model employs an SAR mechanism that dynamically allocates attention across various regions of the face, effectively handling nonlinear transformations due to head poses or lighting changes. This approach surpasses traditional CNN-based models in terms of accuracy and generalization across datasets. Recent innovations in mobile gaze estimation have incorporated novel deep learning architectures and lightweight models tailored for mobile devices. Recent studies have emphasized the emergence of mobile gaze estimation systems that leverage both hardware optimizations and advanced machine learning techniques. For instance, ref. [<xref rid="B18-sensors-25-01224" ref-type="bibr">18</xref>] presented a method that requires minimal user interaction for calibration and demonstrated its efficacy on mobile devices through efficient algorithm design. Similarly, ref. [<xref rid="B19-sensors-25-01224" ref-type="bibr">19</xref>] developed a gaze estimation framework that adapts dynamically to various user environments by adjusting its parameters.</p><p>Deep learning continues to significantly enhance gaze estimation accuracy. Notably, ref. [<xref rid="B20-sensors-25-01224" ref-type="bibr">20</xref>] introduced a multi-task learning framework that simultaneously predicts eye landmarks and gaze direction, considerably reducing the error in cross-dataset evaluations. This strategy has proven to be effective in improving the model&#x02019;s generalization capabilities across diverse populations and settings. Ref. [<xref rid="B21-sensors-25-01224" ref-type="bibr">21</xref>] discussed the deployment of quantized neural networks to substantially reduce computational requirements while maintaining high accuracy, essential for real-time applications on mobile platforms. Researchers [<xref rid="B22-sensors-25-01224" ref-type="bibr">22</xref>] explored the integration of gaze estimation into daily mobile applications, showcasing practical deployment scenarios and user interaction models. The use of edge computing in gaze tracking was pioneered by the authors of [<xref rid="B23-sensors-25-01224" ref-type="bibr">23</xref>], which proved the feasibility of processing gaze data directly on edge devices, thereby reducing latency and enhancing responsiveness. This method is particularly advantageous for interactive systems like augmented reality, where rapid processing is paramount.</p><p>Recent studies, such as [<xref rid="B24-sensors-25-01224" ref-type="bibr">24</xref>], have concentrated on the robustness of gaze estimation systems in natural settings. Their research includes developing algorithms capable of adapting to outdoor lighting and complex background variations, challenges that traditional systems often face. Generative Adversarial Networks (GANs) have been utilized to augment gaze estimation datasets, as noted in [<xref rid="B25-sensors-25-01224" ref-type="bibr">25</xref>]. This technique enhances the diversity and volume of training data, crucial for improving the model&#x02019;s accuracy and robustness. The integration of Recurrent Neural Networks (RNNs) with CNNs in [<xref rid="B26-sensors-25-01224" ref-type="bibr">26</xref>] captures temporal dependencies in video-based gaze estimation tasks, a critical factor for understanding dynamic gaze shifts in real-time video streams. Transfer learning has been increasingly applied to gaze estimation, as discussed in [<xref rid="B27-sensors-25-01224" ref-type="bibr">27</xref>], utilizing pre-trained models on large image datasets to bootstrap gaze estimation models, significantly reducing the required amount of gaze-specific data. Beyond conventional models, the use of attention mechanisms to selectively focus on the most relevant parts of the image for gaze prediction has been explored by the authors of [<xref rid="B28-sensors-25-01224" ref-type="bibr">28</xref>]; their findings suggest that attention improves model interpretability and efficiency by reducing the influence of noisy or irrelevant data.</p><p>Application-specific adaptations, such as those for driver monitoring systems, have been developed by the authors of [<xref rid="B29-sensors-25-01224" ref-type="bibr">29</xref>], who tailored gaze estimation models to assess driver alertness and gaze direction within the context of automotive safety. Comprehensive benchmarking of gaze estimation techniques, particularly in unconstrained environments, was performed by the authors of [<xref rid="B30-sensors-25-01224" ref-type="bibr">30</xref>], who provided insights into the performance variations across different settings and datasets. The work in [<xref rid="B31-sensors-25-01224" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01224" ref-type="bibr">32</xref>] addresses the ethical and privacy implications of gaze-tracking technologies, especially in public and semi-public spaces, highlighting the need for guidelines and regulations to govern the use of such sensitive biometric data.</p><p>Prior work in gaze estimation has primarily relied on convolutional neural networks (CNNs) for feature extraction, often coupled with additional modules such as recurrent layers or attention mechanisms to refine predictions. Some classic models are shown in <xref rid="sensors-25-01224-t001" ref-type="table">Table 1</xref>. While CNN-based models like FullFace [<xref rid="B33-sensors-25-01224" ref-type="bibr">33</xref>] employ a full-face representation for gaze prediction, improving robustness but at the cost of high inference time (50 ms per frame) and large model size (196.6 M parameters), RT-GENE [<xref rid="B34-sensors-25-01224" ref-type="bibr">34</xref>] focuses on real-time applications but still requires 40 ms per frame, limiting its deployment on mobile devices, and GazeTR-Pure [<xref rid="B35-sensors-25-01224" ref-type="bibr">35</xref>], a Transformer-based model, achieves strong generalization but demands significant computational power, making it impractical for embedded systems where computational complexity makes them unsuitable for real-time mobile applications. While knowledge transfer techniques like FSKT-GE [<xref rid="B36-sensors-25-01224" ref-type="bibr">36</xref>] provide strong results for specific low-resolution applications, our approach offers a more flexible, computationally efficient, and deployment-ready solution for real-world gaze estimation tasks. CapsNets have recently been explored as an alternative due to their ability to retain spatial hierarchies, but existing implementations suffer from high computational demands due to iterative routing. In contrast, GazeCapsNet introduces a novel SAR mechanism, which eliminates the need for multiple routing iterations, making capsule networks more efficient for gaze estimation. Additionally, our framework integrates MobileNet v2 and ResNet-18 for feature extraction, balancing speed and accuracy. Unlike previous works that require explicit facial landmark detection, GazeCapsNet operates in an end-to-end manner, directly predicting both 3D gaze direction and gaze origin, thereby reducing pipeline complexity and improving robustness in real-world conditions.</p><p>GazeCapsNet is the only approach that integrates capsule networks with a lightweight backbone (MobileNet v2 + ResNet-18), improving accuracy and efficiency. Compared to GazeCaps [<xref rid="B17-sensors-25-01224" ref-type="bibr">17</xref>], our SAR eliminates iterative routing overhead, reducing latency by 20%. Our model achieves an inference time of 20 ms per frame, making it more suitable for real-time applications than FullFace [<xref rid="B33-sensors-25-01224" ref-type="bibr">33</xref>] (50 ms) or RT-GENE [<xref rid="B34-sensors-25-01224" ref-type="bibr">34</xref>] (40 ms). This critical analysis clarifies how prior work has shaped gaze estimation research and highlights the unique advantages of GazeCapsNet over existing models.</p></sec><sec sec-type="methods" id="sec3-sensors-25-01224"><title>3. Proposed Methodology</title><p>Traditional CapsNets rely on iterative routing algorithms to determine the connections between lower- and higher-level capsules. These routing methods, such as Dynamic Routing and EM Routing, suffer from high computational overhead and are difficult to optimize in real-time applications. To address this, we propose SAR, which replaces iterative routing with an attention-based mechanism that dynamically assigns importance to capsule connections in a single-pass operation. <xref rid="sensors-25-01224-t002" ref-type="table">Table 2</xref> highlights that SAR removes iterative routing bottlenecks, making it a computationally efficient alternative to standard CapsNet methods.</p><p><xref rid="sensors-25-01224-f001" ref-type="fig">Figure 1</xref> illustrates the architecture of a gaze estimation system that integrates multiple deep learning models and capsule networks to analyze and predict gaze direction. The process begins with an input image undergoing face detection using a Single-Stage Efficient and Real-Time Face Detector (SCRFD) block, followed by feature extraction via a combination of MobileNet v2 and ResNet-18 blocks. These features are fed into a capsule network layer that produces primary capsule predictions, which are then refined through an attention-weighted SAR block to generate gaze-related capsule outputs. The gaze estimation culminates in a gaze regression head and a gaze classification module, outputting a 3D gaze vector and gaze classification, respectively, Algorithm 1. This sophisticated architecture enables precise and robust gaze estimation, making it suitable for applications in areas such as human&#x02013;computer interaction and behavioral analysis.</p><p>The GazeCapsNet architecture is designed to balance high performance with computational efficiency, making it suitable for real-time gaze estimation tasks on mobile and resource-constrained devices. This section outlines the components and innovations of the proposed method in detail. The GazeCapsNet architecture is divided into three major components: face detection, feature extraction, and gaze estimation. These components work in tandem to predict both the gaze origin and gaze direction in real time. The architecture is optimized for low computational overhead, ensuring rapid inference while maintaining high prediction accuracy.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold>. Main framework. </td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1. <bold>function</bold> Main(I): <break/>2. &#x02003;&#x02003;face_crop &#x02190; FaceDetection(I) <break/>3. &#x02003;&#x02003;<bold>if</bold> face_crop is <bold>None</bold>: <break/>4. &#x02003;&#x02003;&#x02003;&#x02003;<bold>return</bold> &#x0201c;No face detected&#x0201d; <break/>5. &#x02003;&#x02003;<bold>end if</bold>
<break/>6. &#x02003;&#x02003;combined_features &#x02190; FeatureExtraction(face_crop) <break/>7. &#x02003;&#x02003;gaze_direction &#x02190; GazeCapsModule(combined_features) <break/>8. &#x02003;&#x02003;gaze_class &#x02190; GazeClassification(gaze_direction) <break/>9. &#x02003;&#x02003;<bold>return</bold> gaze_direction, gaze_class</td></tr></tbody></array></p><sec id="sec3dot1-sensors-25-01224"><title>3.1. Face Detection with SCRFD</title><p>For detecting faces in the input images, we employ SCRFD. SCRFD is chosen for its optimized performance on mobile devices, providing high detection accuracy without requiring large computational resources, Algorithm 2. The face detection step is crucial, as it ensures that the model focuses on relevant regions of the image before proceeding to gaze estimation.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2</bold>. Face Detection.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1. <bold>function</bold> FaceDetection(I):<break/>2. &#x02003;&#x02003;&#x02003; face_bbox &#x02190; SCRFD_DetectFace(I)<break/>3. &#x02003;&#x02003;&#x02003; <bold>if</bold> face_bbox is None:<break/>4. &#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003; <bold>return</bold> None <break/>5. &#x02003;&#x02003;<bold>end if</bold><break/>6. &#x02003;&#x02003;face_crop &#x02190; CropAndResize(I, face_bbox, size = (224, 224))<break/>7. &#x02003;&#x02003;<bold>return</bold> face_crop<break/>8. <bold>function</bold> FeatureExtraction(face_crop): <break/>9. &#x02003;&#x02003;&#x02003; mobile_features &#x02190; MobileNet_v2(face_crop) <break/>10. &#x02003;&#x02003;&#x02003;resnet_features &#x02190; ResNet_18(face_crop) <break/>11. &#x02003;&#x02003;&#x02003;combined_features &#x02190; Concatenate (mobile_features, resnet_features) <break/>12. &#x02003;&#x02003;&#x02003;<bold>return</bold> combined_features</td></tr></tbody></array></p><p>SCRFD provides bounding boxes around faces, which are then cropped and resized for input into the feature extraction module. By using SCRFD, the model ensures that no significant pre-processing, such as landmark detection or eye region cropping, is needed&#x02014;this helps to simplify the pipeline and improve real-time capabilities. To extract features from the detected face regions, we utilize a combination of MobileNet v2 and ResNet-18 architectures. MobileNet v2, a lightweight neural network tailored for mobile and embedded systems, employs inverted residual blocks and linear bottlenecks. These mechanisms help reduce memory and computational costs while preserving the model&#x02019;s ability to capture essential facial features. This architecture ensures that the model operates with minimal latency, making it highly efficient even on low-power devices. MobileNet v2 is responsible for extracting hierarchical facial features, which are subsequently passed on for further processing. Its primary role in the overall architecture is to generate low-level representations from the input image. In addition to MobileNet v2, we incorporate ResNet-18 to capture deeper and more complex features from the face images. ResNet-18 uses residual connections, which prevent the degradation of gradients during backpropagation. This design allows for more robust learning, particularly in deeper networks. By integrating ResNet-18 with MobileNet v2, we achieve a balance between computational efficiency and the extraction of more complex facial features. This combination enables the model to efficiently handle both low-level and high-level representations, enhancing its overall performance. The combined outputs of MobileNet v2 and ResNet-18 serve as rich feature maps for gaze estimation, feeding both the classification and regression tasks.</p></sec><sec id="sec3dot2-sensors-25-01224"><title>3.2. Gaze Estimation Using GazeCaps</title><p>At the core of GazeCapsNet is the GazeCaps module, which is responsible for estimating the gaze direction and gaze origin from the feature maps generated by the MobileNet v2 and ResNet-18 backbones. The GazeCaps module utilizes capsule networks combined with SAR to model the complex spatial relationships between facial features and improve gaze estimation accuracy, Algorithm 3.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 3</bold>. Gaze Estimation Using GazeCaps with SAR.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1. <bold>function</bold> GazeCapsModule(combined_features): <break/>2. &#x02003;&#x02003;primary_capsules &#x02190; CreatePrimaryCapsules(combined_features) <break/>3. &#x02003;&#x02003;attention_matrix &#x02190; ComputeAttentionMatrix(primary_capsules) <break/>4. &#x02003;&#x02003;gaze_capsules &#x02190; RouteCapsules (primary_capsules, attention_matrix) <break/>5. &#x02003;&#x02003;output_capsules &#x02190; Squash(gaze_capsules) <break/>6. &#x02003;&#x02003;gaze_direction &#x02190; ExtractGazeDirection(output_capsules) <break/>7. &#x02003;&#x02003;<bold>return</bold> gaze_direction</td></tr></tbody></array></p><sec id="sec3dot2dot1-sensors-25-01224"><title>3.2.1. Capsule Networks for Gaze Estimation</title><p>CapsNets are a powerful extension of CNNs, designed to overcome the limitations of CNNs in capturing hierarchical relationships between parts of an image. In gaze estimation, this is particularly useful, as the spatial relationships between facial features (e.g., the eyes, nose, and head pose) are crucial for accurately determining where the person is looking. A capsule is a group of neurons that encapsulates not just the presence of a feature, but also the properties of that feature, such as its orientation, position, and scale. In GazeCaps, we use two types of capsules. The primary capsules represent low-level facial features, such as eye shapes and textures. These capsules are generated from the feature maps produced by MobileNet v2 and ResNet-18. Output of the primary capsule layer can be represented as<disp-formula id="FD1-sensors-25-01224"><label>(1)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the feature vector from the feature extraction backbone (MobileNet v2 or ResNet-18), <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the weight matrix for capsule <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the output vector of capsule <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The function <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> ensures that the length of the vector encodes the probability that the entity represented by the capsule is present, while the orientation of the vector encodes the feature properties. The squash function is defined as<disp-formula id="FD2-sensors-25-01224"><label>(2)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>&#x000b7;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the total input to capsule <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>&#x02225;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the vector norm. This nonlinear squashing function ensures that the output vector has a length between <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, reflecting the probability that the feature is detected. The output from the primary capsules is passed through the SAR mechanism to form high-level capsules. These gaze capsules encapsulate the more complex relationships needed to estimate gaze direction, such as the interaction between the eyes, head orientation, and overall face position.</p></sec><sec id="sec3dot2dot2-sensors-25-01224"><title>3.2.2. Self-Attention Routing</title><p>The SAR mechanism is a key component in GazeCaps, allowing the network to dynamically assign attention to different regions of the face, depending on their relevance to the task of gaze estimation. Unlike traditional routing methods that require multiple iterations to update coupling coefficients between capsule layers, SAR introduces an efficient attention-based routing mechanism. The steps in SAR can be described as follows: For each capsule in the lower layer, we compute a set of predictions for each higher-level capsule. This is performed using a weight matrix <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> that transforms the output of capsule <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> into a prediction for capsule <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.<disp-formula id="FD3-sensors-25-01224"><label>(3)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the output of capsule <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the lower layer; <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the prediction for capsule <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the higher layer. Instead of iterative routing, SAR uses self-attention to determine the coupling coefficients between capsules. The attention matrix <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is computed using a SoftMax function over the predictions:<disp-formula id="FD4-sensors-25-01224"><label>(4)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the attention score (or coupling coefficient) assigned to the connection between capsule <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the lower layer and capsule j in the higher layer. This score is based on the similarity between the predicted output <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the actual output <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Using the attention matrix <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the outputs of the higher-level capsules are computed as a weighted sum of the predictions:<disp-formula id="FD5-sensors-25-01224"><label>(5)</label><mml:math id="mm29" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">u</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the final output of capsule <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> in the higher layer. The squash function is applied to ensure that the length of the vector remains between <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, representing the probability that the capsule is active. The output of the gaze capsule layer is a set of vectors that encode the gaze direction. These vectors are passed to the regression head to compute the continuous gaze angles. The gaze regression problem is treated as a multi-dimensional angular regression task, where the angular loss function is minimized to match the predicted and ground truth gaze vectors.</p></sec><sec id="sec3dot2dot3-sensors-25-01224"><title>3.2.3. Gaze Classification and Regression</title><p>GazeCapsNet is designed to handle both classification and regression tasks related to gaze estimation, Algorithm 4.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 4</bold>. Gaze Classification.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1. <bold>function</bold> GazeClassification(output_capsules): <break/>2. gaze_class_probs &#x02190; Softmax(output_capsules) <break/>3. gaze_class &#x02190; Argmax(gaze_class_probs) <break/>4. &#x02003;&#x02003; <bold>return</bold> gaze_class</td></tr></tbody></array></p><p>The classification head predicts discrete gaze directions (e.g., left, right, up, down) by applying a SoftMax function to the capsule outputs. The output probabilities are computed as<disp-formula id="FD6-sensors-25-01224"><label>(6)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability that the gaze direction belongs to class <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the weight vector for class <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. For continuous gaze direction estimation, the regression head predicts a 3D gaze vector <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The angular loss is used to compute the error between the predicted gaze vector and the ground truth gaze vector <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD7-sensors-25-01224"><label>(7)</label><mml:math id="mm41" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02217;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced open="" close="&#x02016;" separators="|"><mml:mrow><mml:mfenced open="&#x02016;" close="" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mfenced open="" close="&#x02016;" separators="|"><mml:mrow><mml:mfenced open="&#x02016;" close="" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This loss ensures that the predicted gaze direction minimizes the angular difference between the true and predicted gaze vectors.</p></sec><sec id="sec3dot2dot4-sensors-25-01224"><title>3.2.4. Loss Functions</title><p>The overall loss function for training the GazeCaps module combines both classification and regression losses, as well as the routing loss for the SAR mechanism, Algorithm 5. The final loss function <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as<disp-formula id="FD8-sensors-25-01224"><label>(8)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:msub><mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the cross-entropy loss for gaze classification, <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the angular loss for gaze regression, <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the loss associated with the attention-based capsule routing, and <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are weighting factors that control the contributions of each loss term during training. This multi-task loss framework ensures that the model can handle both classification and regression tasks while maintaining optimal routing between capsules.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 5</bold>. Loss Function.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1. function ComputeLoss(gaze_direction_pred, gaze_direction_true, gaze_class_pred, gaze_class_true): <break/>2. &#x02003;&#x02003;angular_loss &#x02190; ArcCosineLoss(gaze_direction_pred, gaze_direction_true) <break/>3. &#x02003;&#x02003;classification_loss &#x02190; CrossEntropyLoss(gaze_class_pred, gaze_class_true) <break/>4. &#x02003;&#x02003;total_loss &#x02190; &#x003bb;_angular * angular_loss + &#x003bb;_class * classification_loss <break/>5. &#x02003;&#x02003;return total_loss</td></tr></tbody></array></p><p>The GazeCaps module in GazeCapsNet combines the representational power of capsule networks with the efficiency of SAR to predict gaze direction accurately. By leveraging capsule hierarchies and dynamically focusing attention on the most relevant facial regions, the model achieves SOTA performance in real-time gaze estimation tasks.</p></sec></sec></sec><sec id="sec4-sensors-25-01224"><title>4. Experimental Setup and Implementation</title><p>In this section, we describe the experiments conducted to evaluate the performance of GazeCapsNet for gaze estimation. The model was trained and tested on multiple benchmark datasets to assess its accuracy, efficiency, and generalization across diverse scenarios. We compare GazeCapsNet to SOTA gaze estimation methods and demonstrate its suitability for real-time applications on mobile and resource-constrained platforms.</p><sec id="sec4dot1-sensors-25-01224"><title>4.1. Datasets</title><p>We evaluate GazeCapsNet on three publicly available datasets for gaze estimation, <xref rid="sensors-25-01224-t001" ref-type="table">Table 1</xref>, each offering a wide range of head poses, lighting conditions, and gaze directions, which provides a comprehensive benchmark for performance assessment, <xref rid="sensors-25-01224-f002" ref-type="fig">Figure 2</xref>. While the datasets used in this study provide a diverse range of gaze variations, differences in dataset size and demographic representation could introduce biases that impact the generalization of our model.</p><p>The first dataset, ETH-XGaze [<xref rid="B38-sensors-25-01224" ref-type="bibr">38</xref>], consists of over 1.1 million images from 110 subjects, captured under various head poses and lighting conditions. This dataset presents a challenging environment due to the extreme gaze angles and diverse head orientations it includes. Given its large size and diversity, we use ETH-XGaze for pre-training the model, allowing the network to learn robust representations across a wide range of conditions.</p><p>The second dataset, Gaze360 [<xref rid="B39-sensors-25-01224" ref-type="bibr">39</xref>], contains 172,000 images from 238 subjects, with 360-degree gaze annotations. This dataset is collected in natural, uncontrolled environments, capturing real-world variations in head poses and gaze directions. Gaze360 is particularly useful for evaluating cross-domain generalization, as it reflects diverse, in-the-wild conditions, making it ideal for testing the model&#x02019;s robustness in unpredictable settings.</p><p>The third dataset are shown in <xref rid="sensors-25-01224-t003" ref-type="table">Table 3</xref>, MPIIFaceGaze [<xref rid="B40-sensors-25-01224" ref-type="bibr">40</xref>], includes 45,000 images from 15 subjects, recorded under natural lighting conditions with head movements. These images are captured using laptop webcams and come with continuous 3D gaze annotations. This dataset is used to evaluate the model&#x02019;s performance in more controlled, yet dynamic, environments, offering a different challenge in terms of head and gaze movement while maintaining relatively consistent lighting conditions. These datasets cover a broad range of conditions, from controlled laboratory environments to more challenging in-the-wild settings, allowing us to assess the robustness of the GazeCapsNet model.</p></sec><sec id="sec4dot2-sensors-25-01224"><title>4.2. Training and Evaluation</title><p>We pre-train GazeCapsNet on the ETH-XGaze dataset and fine-tune it on Gaze360 and MPIIFaceGaze, Algorithm 6. We perform cross-dataset evaluations to assess the generalization capability of the model.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 6</bold>. Training Process. </td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1. <bold>function</bold> TrainModel(dataset):<break/>2. &#x02003;&#x02003;<bold>for</bold> each epoch do:<break/>3. &#x02003;&#x02003;&#x02003;&#x02003;<bold>for</bold> each batch in dataset do:<break/>4. &#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;image, gaze_direction_true, gaze_class_true &#x02190; LoadBatch(batch)<break/>5. &#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;face_crop &#x02190; FaceDetection(image)<break/>6. &#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;<bold>if</bold> face_crop is None:<break/>7. &#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;continue<break/>8. &#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;<bold>end if</bold><break/>9. &#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;combined_features &#x02190; FeatureExtraction(face_crop)<break/>10. &#x02003;&#x02003;&#x02003;&#x02003;&#x02003;gaze_direction_pred &#x02190; GazeCapsModule(combined_features)<break/>11. &#x02003;&#x02003;&#x02003;&#x02003;&#x02003;gaze_class_pred &#x02190; GazeClassification(gaze_direction_pred)<break/>12. &#x02003;&#x02003;&#x02003; total_loss &#x02190; ComputeLoss(gaze_direction_pred, gaze_direction_true, gaze_class_pred, gaze_class_true)<break/>13. Backpropagate(total_loss)<break/>14. UpdateModelWeights()<break/>15. &#x02003;&#x02003;&#x02003;&#x02003;<bold>end for</bold><break/>16. &#x02003;&#x02003;<bold>end for</bold><break/>17. &#x02003;&#x02003;<bold>return</bold> TrainedModel</td></tr></tbody></array></p><sec id="sec4dot2dot1-sensors-25-01224"><title>4.2.1. Pre-Processing</title><p>For all datasets, face detection is performed using SCRFD to ensure consistent cropping of the face region across all images. The cropped face regions are resized to 224 &#x000d7; 224 pixels for input into the GazeCapsNet model. No additional pre-processing steps, such as eye or landmark cropping, are applied, as our model directly learns from the full-face image. To improve generalization, we apply several data augmentation techniques during training. Random rotations are used to simulate variations in head poses, allowing the model to handle a wider range of head orientations. Brightness and contrast adjustments help mimic changes in lighting conditions, ensuring that the model performs well under different illumination levels. Additionally, horizontal flipping is introduced to create symmetry in the gaze direction, enhancing the model&#x02019;s ability to generalize across different gaze orientations. These augmentation strategies collectively enhance the robustness of the model across diverse real-world scenarios.</p></sec><sec id="sec4dot2dot2-sensors-25-01224"><title>4.2.2. Model Training</title><p>The GazeCapsNet model is pre-trained on the ETH-XGaze dataset for 100 epochs using the Adam optimizer [<xref rid="B41-sensors-25-01224" ref-type="bibr">41</xref>], with an initial learning rate of 0.001 and a batch size of 64. After pre-training, the model is fine-tuned on Gaze360 and MPIIFaceGaze for 50 epochs, with a reduced learning rate of 0.0001. For gaze classification, the model uses cross-entropy loss, while for regression, the angular loss function is applied. A multi-task loss function, as described in <xref rid="sec3dot2dot4-sensors-25-01224" ref-type="sec">Section 3.2.4</xref>, is used to combine these objectives. The training process is conducted on an NVIDIA Tesla V100 GPU. <xref rid="sensors-25-01224-t004" ref-type="table">Table 4</xref> summarizes the key training hyperparameters used for training GazeCapsNet across different datasets.</p></sec></sec><sec id="sec4dot3-sensors-25-01224"><title>4.3. Performance Metrics</title><p>We evaluate the performance of GazeCapsNet using the following metrics: mean angular error (MAE) measures the angular difference between the predicted gaze vector and the ground truth gaze vector, providing an indication of how accurately the model can predict gaze direction.<disp-formula id="FD9-sensors-25-01224"><label>(9)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&#x02217;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced open="" close="&#x02016;" separators="|"><mml:mrow><mml:mfenced open="&#x02016;" close="" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mfenced open="" close="&#x02016;" separators="|"><mml:mrow><mml:mfenced open="&#x02016;" close="" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">g</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the predicted and ground truth gaze vectors for the <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>-th sample. We measure the time taken to process a single frame during inference to assess the real-time capabilities of GazeCapsNet on mobile and embedded devices. The total number of parameters in the model is reported, providing insight into the model&#x02019;s computational complexity and its suitability for resource-constrained environments.</p></sec><sec id="sec4dot4-sensors-25-01224"><title>4.4. Results</title><sec><title>Quantitative Results</title><p><xref rid="sensors-25-01224-t005" ref-type="table">Table 5</xref>, <xref rid="sensors-25-01224-t006" ref-type="table">Table 6</xref> and <xref rid="sensors-25-01224-t007" ref-type="table">Table 7</xref> summarize the performance of GazeCapsNet on the ETH-XGaze, Gaze360, and MPIIFaceGaze datasets, compared to SOTA methods. GazeCapsNet consistently outperforms other models in terms of gaze estimation accuracy while maintaining a smaller model size and faster inference time.</p><p>As shown in <xref rid="sensors-25-01224-t005" ref-type="table">Table 5</xref>, <xref rid="sensors-25-01224-t006" ref-type="table">Table 6</xref> and <xref rid="sensors-25-01224-t007" ref-type="table">Table 7</xref>, GazeCapsNet achieves a 5.10&#x000b0; MAE on the Gaze360 dataset, matching the accuracy of the GazeCaps model but with 20% faster inference time and a comparable model size. On the MPIIFaceGaze dataset, GazeCapsNet achieves a 4.06&#x000b0; MAE, demonstrating its robustness in more controlled environments. Our model matches the accuracy of GazeCaps (5.10&#x000b0; MAE) while reducing inference time by 20% through SAR, which eliminates redundant computations. The hybrid lightweight architecture (MobileNet v2 + ResNet-18) balances computational efficiency with robust feature extraction, ensuring fast and accurate gaze estimation. Unlike FSKT-GE, which relies on knowledge transfer, our model is optimized for end-to-end efficiency, making it more deployment-friendly. FullFace (6.53&#x000b0; MAE) struggles with extreme head poses due to its reliance on a full-face representation, leading to lower robustness in unconstrained settings. RT-GENE (6.02&#x000b0; MAE), while effective in real-world scenarios, lacks capsule-based spatial awareness, affecting generalization. GazeTR-Pure (5.33&#x000b0; MAE), despite leveraging Transformer-based architectures, suffers from higher latency (45 ms), making it less suitable for real-time applications. FSKT-GE (5.20&#x000b0; MAE), while strong in low-resolution conditions, introduces pre-training overhead, increasing computational cost and dependency on a teacher network. MPIIFaceGaze primarily consists of controlled environments, where Dilated-Net&#x02019;s local feature extraction is particularly effective. In contrast, GazeCapsNet is optimized for both controlled and real-world settings, balancing robustness and computational efficiency.</p><p>GazeCapsNet excels in real-time applications, with an inference time of 20 milliseconds per frame, making it well suited for real-time gaze estimation tasks such as driver monitoring systems or AR/VR applications. The model size is kept to 11.7 million parameters, making it lightweight and deployable on mobile devices with limited computational power.</p></sec></sec><sec id="sec4dot5-sensors-25-01224"><title>4.5. Ablation Study</title><p>To further understand the contributions of different components of the GazeCapsNet architecture, we conduct an ablation study where key modules (MobileNet v2, ResNet-18, and Self-Attention Routing) are removed or replaced with simpler alternatives. <xref rid="sensors-25-01224-t008" ref-type="table">Table 8</xref> shows the results of this ablation study.</p><p>As shown in <xref rid="sensors-25-01224-t008" ref-type="table">Table 8</xref>, removing ResNet-18 or SAR significantly degrades performance, highlighting the importance of these components in achieving accurate gaze estimation. Using a vanilla CNN as the backbone further reduces accuracy, confirming the effectiveness of the capsule network and mobile-optimized architectures.</p></sec><sec id="sec4dot6-sensors-25-01224"><title>4.6. Cross-Dataset Generalization</title><p>We assess the generalization ability of GazeCapsNet by performing cross-dataset evaluations. The model is trained on one dataset and tested on another without any fine-tuning. <xref rid="sensors-25-01224-t009" ref-type="table">Table 9</xref> presents the results of this evaluation.</p><p>The cross-dataset results show that GazeCapsNet generalizes well to unseen datasets, particularly when trained on diverse data from Gaze360, indicating the model robustness to real-world variations in gaze directions and head poses.</p><p>In <xref rid="sensors-25-01224-f003" ref-type="fig">Figure 3</xref>, we present qualitative results of gaze estimation using Mobile-GazeCapsNet. The model accurately predicts gaze direction even under challenging conditions such as varying lighting, head orientations, and occlusions. The experimental results demonstrate that GazeCapsNet achieves SOTA performance in both controlled and in-the-wild environments. Its efficient architecture, combining capsule networks with lightweight feature extraction, enables real-time gaze estimation on mobile devices while maintaining high accuracy. The ablation study highlights the importance of key components like ResNet-18 and SAR in ensuring robust performance. The experiments confirm that GazeCapsNet is a versatile, high-performance solution for real-time gaze estimation across diverse settings, making it suitable for applications in AR.</p></sec></sec><sec sec-type="discussion" id="sec5-sensors-25-01224"><title>5. Discussion and Limitations</title><p>GazeCapsNet demonstrates a significant advancement in mobile gaze estimation by integrating lightweight architectures with capsule networks and Self-Attention Routing. This model efficiently handles feature extraction and dynamic attention allocation, simplifying deployment and reducing computational demands. It has shown robust performance across several benchmark datasets, including ETH-XGaze and Gaze360, suggesting its utility for real-time applications like augmented reality and driver monitoring.</p><p>However, GazeCapsNet has several limitations. Its performance in adverse conditions such as low lighting or with subjects wearing eyewear remains insufficiently tested, potentially limiting its applicability. The model architecture may also restrict adaptation to newer neural network developments. Additionally, GazeCapsNet has been insufficiently evaluated on subjects wearing eyewear, which can introduce significant distortions in gaze estimation due to reflection, occlusion, and refraction artifacts. These effects degrade the visibility of critical eye features, leading to higher prediction errors in subjects who wear glasses. Its effectiveness across diverse demographic groups has not been thoroughly validated, which could impact its deployment in global markets. Although GazeCapsNet demonstrates strong performance, we acknowledge certain limitations that must be addressed for improved real-world deployment.</p><p>Future research should focus on enhancing the model robustness in various real-world conditions and expanding its adaptability to new neural network architectures. Efforts to reduce the model size and computational requirements will further align it with the constraints of mobile devices. Broadening the training datasets to include more diverse demographic data will also be crucial for improving its generalization capabilities.</p></sec><sec sec-type="conclusions" id="sec6-sensors-25-01224"><title>6. Conclusions</title><p>In this work, we presented Mobile-GazeCapsNet, a novel, lightweight framework for real-time gaze estimation that integrates capsule networks with Self-Attention Routing and mobile-optimized architectures. By leveraging MobileNet v2 and ResNet-18 for efficient feature extraction and GazeCaps for capturing complex spatial relationships between facial features, our model is able to deliver state-of-the-art accuracy with minimal computational overhead. The incorporation of SAR allows for the dynamic allocation of attention to key facial regions, significantly improving the model&#x02019;s ability to handle variations in the head pose, lighting, and occlusions. Our experimental results across diverse datasets, including ETH-XGaze, Gaze360, and MPIIFaceGaze, demonstrate that GazeCapsNet not only achieves high accuracy in both controlled and in-the-wild scenarios but also does so with an inference speed that is suitable for real-time applications. The model&#x02019;s compact size and low latency make it particularly well suited for deployment on mobile and resource-constrained devices, without sacrificing performance. GazeCapsNet represents a significant advancement in the field of gaze estimation, providing a scalable, efficient, and accurate solution for a range of real-time applications. Future work may explore extending this framework to multi-modal inputs, further optimizing its performance for specific domains such as AR/VR, or applying it to other human&#x02013;computer interaction tasks.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Methodology, S.M., Y.V., S.U., J.B. and Y.I.C.; software, S.M., Y.V. and S.U.; validation, J.B., S.M., Y.V. and S.U.; formal analysis, J.B., S.U. and Y.I.C.; resources, S.M., Y.V., S.U. and J.B.; data curation, J.B., S.U. and Y.I.C.; writing&#x02014;original draft, S.M., Y.V. and S.U.; writing&#x02014;review and editing, S.M., Y.V., S.U., J.B. and Y.I.C.; supervision, Y.I.C.; project administration, S.M., Y.V., S.U. and Y.I.C. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>All used datasets are available online with open access.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Mr. Yakhyokhuja Valikhujaev was employed by the company Aria Studios Co. Ltd., Seoul, Korea. The remaining authors declare that the research was conducted in the absence of any commerctial or financial relationships that could be construed as a potential confict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01224"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shi</surname><given-names>P.</given-names></name>
<name><surname>Billeter</surname><given-names>M.</given-names></name>
<name><surname>Eisemann</surname><given-names>E.</given-names></name>
</person-group><article-title>SalientGaze: Saliency-Based gaze correction in virtual reality</article-title><source>Comput. Graph.</source><year>2020</year><volume>91</volume><fpage>83</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.cag.2020.06.007</pub-id></element-citation></ref><ref id="B2-sensors-25-01224"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Stark</surname><given-names>P.</given-names></name>
<name><surname>Hasenbein</surname><given-names>L.</given-names></name>
<name><surname>Kasneci</surname><given-names>E.</given-names></name>
<name><surname>G&#x000f6;llner</surname><given-names>R.</given-names></name>
</person-group><article-title>Gaze-Based attention network analysis in a virtual reality classroom</article-title><source>MethodsX</source><year>2024</year><volume>12</volume><fpage>102662</fpage><pub-id pub-id-type="doi">10.1016/j.mex.2024.102662</pub-id><pub-id pub-id-type="pmid">38577409</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01224"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Smith</surname><given-names>M.L.</given-names></name>
<name><surname>Smith</surname><given-names>L.N.</given-names></name>
<name><surname>Farooq</surname><given-names>A.</given-names></name>
</person-group><article-title>Gender and gaze gesture recognition for human-computer interaction</article-title><source>Comput. Vis. Image Underst.</source><year>2016</year><volume>149</volume><fpage>32</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2016.03.014</pub-id></element-citation></ref><ref id="B4-sensors-25-01224"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Xie</surname><given-names>G.</given-names></name>
<name><surname>He</surname><given-names>H.</given-names></name>
</person-group><article-title>Real-Time precise human-computer interaction system based on gaze estimation and tracking</article-title><source>Wirel. Commun. Mob. Comput.</source><year>2021</year><volume>2021</volume><fpage>8213946</fpage><pub-id pub-id-type="doi">10.1155/2021/8213946</pub-id></element-citation></ref><ref id="B5-sensors-25-01224"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>Z.</given-names></name>
<name><surname>Lv</surname><given-names>C.</given-names></name>
<name><surname>Hang</surname><given-names>P.</given-names></name>
<name><surname>Huang</surname><given-names>C.</given-names></name>
<name><surname>Xing</surname><given-names>Y.</given-names></name>
</person-group><article-title>Data-Driven estimation of driver attention using calibration-free eye gaze and scene features</article-title><source>IEEE Trans. Ind. Electron.</source><year>2022</year><volume>69</volume><fpage>1800</fpage><lpage>1808</lpage><pub-id pub-id-type="doi">10.1109/TIE.2021.3057033</pub-id></element-citation></ref><ref id="B6-sensors-25-01224"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yuan</surname><given-names>G.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Yan</surname><given-names>H.</given-names></name>
<name><surname>Fu</surname><given-names>X.</given-names></name>
</person-group><article-title>Self-Calibrated driver gaze estimation via gaze pattern learning</article-title><source>Knowl. Based Syst.</source><year>2022</year><volume>235</volume><fpage>107630</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2021.107630</pub-id></element-citation></ref><ref id="B7-sensors-25-01224"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Valtakari</surname><given-names>N.V.</given-names></name>
<name><surname>Hessels</surname><given-names>R.S.</given-names></name>
<name><surname>Niehorster</surname><given-names>D.C.</given-names></name>
<name><surname>Viktorsson</surname><given-names>C.</given-names></name>
<name><surname>Nystrom</surname><given-names>P.</given-names></name>
<name><surname>Falck-Ytter</surname><given-names>T.</given-names></name>
<name><surname>Kemner</surname><given-names>C.</given-names></name>
<name><surname>Hooge</surname><given-names>I.T.C.</given-names></name>
</person-group><article-title>A field test of computer-vision-based gaze estimation in psychology</article-title><source>Behav. Res. Methods</source><year>2023</year><volume>56</volume><fpage>1900</fpage><lpage>1915</lpage><pub-id pub-id-type="doi">10.3758/s13428-023-02125-1</pub-id><pub-id pub-id-type="pmid">37101100</pub-id>
</element-citation></ref><ref id="B8-sensors-25-01224"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Feng</surname><given-names>Y.</given-names></name>
<name><surname>Goulding-Hotta</surname><given-names>N.</given-names></name>
<name><surname>Khan</surname><given-names>A.</given-names></name>
<name><surname>Reyserhove</surname><given-names>H.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
</person-group><article-title>A Real-Time Gaze Estimation Framework for Mobile Devices</article-title><source>Frameless</source><year>2021</year><volume>4</volume><fpage>3</fpage></element-citation></ref><ref id="B9-sensors-25-01224"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Muksimova</surname><given-names>S.</given-names></name>
<name><surname>Umirzakova</surname><given-names>S.</given-names></name>
<name><surname>Sultanov</surname><given-names>M.</given-names></name>
<name><surname>Cho</surname><given-names>Y.I.</given-names></name>
</person-group><article-title>Cross-Modal Transformer-Based Streaming Dense Video Captioning with Neural ODE Temporal Localization</article-title><source>Sensors</source><year>2025</year><volume>25</volume><elocation-id>707</elocation-id><pub-id pub-id-type="doi">10.3390/s25030707</pub-id><pub-id pub-id-type="pmid">39943345</pub-id>
</element-citation></ref><ref id="B10-sensors-25-01224"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yasir</surname><given-names>M.</given-names></name>
<name><surname>Ullah</surname><given-names>I.</given-names></name>
<name><surname>Choi</surname><given-names>C.</given-names></name>
</person-group><article-title>Depthwise channel attention network (DWCAN): An efficient and lightweight model for single image super-resolution and metaverse gaming</article-title><source>Expert Syst.</source><year>2024</year><volume>41</volume><fpage>e13516</fpage><pub-id pub-id-type="doi">10.1111/exsy.13516</pub-id></element-citation></ref><ref id="B11-sensors-25-01224"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Umirzakova</surname><given-names>S.</given-names></name>
<name><surname>Abdullaev</surname><given-names>M.</given-names></name>
<name><surname>Mardieva</surname><given-names>S.</given-names></name>
<name><surname>Latipova</surname><given-names>N.</given-names></name>
<name><surname>Muksimova</surname><given-names>S.</given-names></name>
</person-group><article-title>Simplified Knowledge Distillation for Deep Neural Networks Bridging the Performance Gap with a Novel Teacher&#x02013;Student Architecture</article-title><source>Electronics</source><year>2024</year><volume>13</volume><elocation-id>4530</elocation-id><pub-id pub-id-type="doi">10.3390/electronics13224530</pub-id></element-citation></ref><ref id="B12-sensors-25-01224"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Choi</surname><given-names>J.H.</given-names></name>
<name><surname>Kim</surname><given-names>J.H.</given-names></name>
<name><surname>Nasridinov</surname><given-names>A.</given-names></name>
<name><surname>Kim</surname><given-names>Y.S.</given-names></name>
</person-group><article-title>Three-Dimensional atrous inception module for crowd behavior classification</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>14390</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-65003-6</pub-id><pub-id pub-id-type="pmid">38909074</pub-id>
</element-citation></ref><ref id="B13-sensors-25-01224"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>J.H.</given-names></name>
<name><surname>Hao</surname><given-names>F.</given-names></name>
<name><surname>Leung</surname><given-names>C.K.S.</given-names></name>
<name><surname>Nasridinov</surname><given-names>A.</given-names></name>
</person-group><article-title>Cluster-Guided temporal modeling for action recognition</article-title><source>Int. J. Multimed. Inf. Retr.</source><year>2023</year><volume>12</volume><fpage>15</fpage><pub-id pub-id-type="doi">10.1007/s13735-023-00280-x</pub-id></element-citation></ref><ref id="B14-sensors-25-01224"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sabour</surname><given-names>S.</given-names></name>
<name><surname>Frosst</surname><given-names>N.</given-names></name>
<name><surname>Hinton</surname><given-names>G.E.</given-names></name>
</person-group><article-title>Dynamic routing between capsules</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2017</year><volume>30</volume><pub-id pub-id-type="doi">10.48550/arXiv.1710.09829</pub-id></element-citation></ref><ref id="B15-sensors-25-01224"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sandler</surname><given-names>M.</given-names></name>
<name><surname>Howard</surname><given-names>A.</given-names></name>
<name><surname>Zhu</surname><given-names>M.</given-names></name>
<name><surname>Zhmoginov</surname><given-names>A.</given-names></name>
<name><surname>Chen</surname><given-names>L.C.</given-names></name>
</person-group><article-title>MobileNetV2: Inverted residuals and linear bottlenecks</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;23 June 2018</conf-date><fpage>4510</fpage><lpage>4520</lpage></element-citation></ref><ref id="B16-sensors-25-01224"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Vasu</surname><given-names>P.K.A.</given-names></name>
<name><surname>Gabriel</surname><given-names>J.</given-names></name>
<name><surname>Zhu</surname><given-names>J.</given-names></name>
<name><surname>Tuzel</surname><given-names>O.</given-names></name>
<name><surname>Ranjan</surname><given-names>A.</given-names></name>
</person-group><article-title>MobileOne: An improved one-millisecond mobile backbone</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#x02013;24 June 2023</conf-date><fpage>7907</fpage><lpage>7917</lpage></element-citation></ref><ref id="B17-sensors-25-01224"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Oh</surname><given-names>J.O.</given-names></name>
<name><surname>Chang</surname><given-names>H.J.</given-names></name>
<name><surname>Na</surname><given-names>J.H.</given-names></name>
<name><surname>Tae</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Choi</surname><given-names>S.-I.</given-names></name>
</person-group><article-title>GazeCaps: Gaze estimation with self-attention-routed capsules</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#x02013;24 June 2023</conf-date><fpage>2669</fpage><lpage>2677</lpage></element-citation></ref><ref id="B18-sensors-25-01224"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Lu</surname><given-names>F.</given-names></name>
</person-group><article-title>Edge-Guided near-eye image analysis for head mounted displays</article-title><source>Proceedings of the IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</source><conf-loc>Bari, Italy</conf-loc><conf-date>4&#x02013;8 October 2021</conf-date><fpage>11</fpage><lpage>20</lpage></element-citation></ref><ref id="B19-sensors-25-01224"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wan</surname><given-names>Z.</given-names></name>
<name><surname>Xiong</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>S.</given-names></name>
</person-group><article-title>Pupil-Contour-Based gaze estimation with real pupil axes for head-mounted eye tracking</article-title><source>IEEE Trans. Ind. Inform.</source><year>2022</year><volume>18</volume><fpage>3640</fpage><lpage>3650</lpage><pub-id pub-id-type="doi">10.1109/TII.2021.3118022</pub-id></element-citation></ref><ref id="B20-sensors-25-01224"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Sugano</surname><given-names>Y.</given-names></name>
<name><surname>Fritz</surname><given-names>M.</given-names></name>
<name><surname>Bulling</surname><given-names>A.</given-names></name>
</person-group><article-title>Appearance-based gaze estimation in the wild</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date><fpage>4510</fpage><lpage>4520</lpage></element-citation></ref><ref id="B21-sensors-25-01224"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>Y.</given-names></name>
<name><surname>Huang</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>F.</given-names></name>
<name><surname>Qian</surname><given-names>C.</given-names></name>
<name><surname>Lu</surname><given-names>F.</given-names></name>
</person-group><article-title>A coarse-to-fine adaptive network for appearance-based gaze estimation</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>New York, NY, USA</conf-loc><conf-date>7&#x02013;12 February 2020</conf-date><volume>Volume 34</volume><fpage>10623</fpage><lpage>10630</lpage></element-citation></ref><ref id="B22-sensors-25-01224"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Lu</surname><given-names>F.</given-names></name>
<name><surname>Sato</surname><given-names>Y.</given-names></name>
</person-group><article-title>Gaze estimation by exploring two-eye asymmetry</article-title><source>IEEE Trans. Image Process.</source><year>2020</year><volume>29</volume><fpage>5259</fpage><lpage>5272</lpage><pub-id pub-id-type="doi">10.1109/TIP.2020.2982828</pub-id></element-citation></ref><ref id="B23-sensors-25-01224"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>G.</given-names></name>
<name><surname>Yu</surname><given-names>Y.</given-names></name>
<name><surname>Mora</surname><given-names>K.A.F.</given-names></name>
<name><surname>Odobez</surname><given-names>J.-M.</given-names></name>
</person-group><article-title>A differential approach for gaze estimation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>43</volume><fpage>1092</fpage><lpage>1099</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2019.2957373</pub-id><pub-id pub-id-type="pmid">31804927</pub-id>
</element-citation></ref><ref id="B24-sensors-25-01224"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lian</surname><given-names>D.</given-names></name>
<name><surname>Hu</surname><given-names>L.</given-names></name>
<name><surname>Luo</surname><given-names>W.</given-names></name>
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Duan</surname><given-names>L.</given-names></name>
<name><surname>Yu</surname><given-names>J.</given-names></name>
<name><surname>Gao</surname><given-names>S.</given-names></name>
</person-group><article-title>Multiview multitask gaze estimation with deep convolutional neural networks</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2019</year><volume>30</volume><fpage>3010</fpage><lpage>3023</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2018.2865525</pub-id><pub-id pub-id-type="pmid">30183647</pub-id>
</element-citation></ref><ref id="B25-sensors-25-01224"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Akinyelu</surname><given-names>A.A.</given-names></name>
<name><surname>Blignaut</surname><given-names>P.</given-names></name>
</person-group><article-title>Convolutional neural network-based technique for gaze estimation on mobile devices</article-title><source>Front. Artif. Intell.</source><year>2022</year><volume>4</volume><elocation-id>796825</elocation-id><pub-id pub-id-type="doi">10.3389/frai.2021.796825</pub-id><pub-id pub-id-type="pmid">35156012</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01224"><label>26.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wong</surname><given-names>E.T.</given-names></name>
<name><surname>Yean</surname><given-names>S.</given-names></name>
<name><surname>Hu</surname><given-names>Q.</given-names></name>
<name><surname>Lee</surname><given-names>B.S.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Deepu</surname><given-names>R.</given-names></name>
</person-group><article-title>Gaze estimation using residual neural network</article-title><source>Proceedings of the 2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)</source><conf-loc>Kyoto, Japan</conf-loc><conf-date>11&#x02013;15 March 2019</conf-date><fpage>411</fpage><lpage>414</lpage></element-citation></ref><ref id="B27-sensors-25-01224"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Vieira</surname><given-names>G.L.</given-names></name>
<name><surname>Oliveira</surname><given-names>L.</given-names></name>
</person-group><article-title>Gaze estimation via self-attention augmented convolutions</article-title><source>Proceedings of the 34th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</source><conf-loc>Gramado, Brazil</conf-loc><conf-date>18&#x02013;22 October 2021</conf-date><fpage>49</fpage><lpage>56</lpage></element-citation></ref><ref id="B28-sensors-25-01224"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Oh</surname><given-names>O.</given-names></name>
<name><surname>Chang</surname><given-names>H.J.</given-names></name>
<name><surname>Choi</surname><given-names>S.-I.</given-names></name>
</person-group><article-title>Self-Attention with Convolution and Deconvolution for Efficient Eye Gaze Estimation from a Full Face Image</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>19&#x02013;20 June 2022</conf-date><fpage>4988</fpage><lpage>4996</lpage></element-citation></ref><ref id="B29-sensors-25-01224"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Zhong</surname><given-names>Y.</given-names></name>
<name><surname>Lam</surname><given-names>H.-K.</given-names></name>
</person-group><article-title>Appearance-based gaze estimation for ASD diagnosis</article-title><source>IEEE Trans. Cybern.</source><year>2022</year><volume>52</volume><fpage>6504</fpage><lpage>6515</lpage><pub-id pub-id-type="doi">10.1109/TCYB.2022.3165063</pub-id><pub-id pub-id-type="pmid">35468077</pub-id>
</element-citation></ref><ref id="B30-sensors-25-01224"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>R.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Luo</surname><given-names>S.</given-names></name>
<name><surname>Shou</surname><given-names>S.</given-names></name>
<name><surname>Tang</surname><given-names>P.</given-names></name>
</person-group><article-title>Gaze-Swin: Enhancing Gaze Estimation with a Hybrid CNN-Transformer Network and Dropkey Mechanism</article-title><source>Electronics</source><year>2024</year><volume>13</volume><elocation-id>328</elocation-id><pub-id pub-id-type="doi">10.3390/electronics13020328</pub-id></element-citation></ref><ref id="B31-sensors-25-01224"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Toaiari</surname><given-names>A.</given-names></name>
<name><surname>Murino</surname><given-names>V.</given-names></name>
<name><surname>Cristani</surname><given-names>M.</given-names></name>
<name><surname>Beyan</surname><given-names>C.</given-names></name>
</person-group><article-title>Upper-Body pose-based gaze estimation for privacy-preserving 3D gaze target detection</article-title><source>Comput. Sci. Comput. Vis. Pattern Recognit.</source><year>2024</year><pub-id pub-id-type="arxiv">2409.17886</pub-id></element-citation></ref><ref id="B32-sensors-25-01224"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>D.</given-names></name>
<name><surname>Huang</surname><given-names>K.</given-names></name>
</person-group><article-title>Semi-Supervised multitask learning using gaze focus for gaze estimation</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2024</year><volume>34</volume><fpage>7935</fpage><lpage>7947</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2024.3383597</pub-id></element-citation></ref><ref id="B33-sensors-25-01224"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Sugano</surname><given-names>Y.</given-names></name>
<name><surname>Fritz</surname><given-names>M.</given-names></name>
<name><surname>Bulling</surname><given-names>A.</given-names></name>
</person-group><article-title>It&#x02019;s written all over your face: Full-Face appearance-based gaze estimation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date><fpage>2299</fpage><lpage>2308</lpage></element-citation></ref><ref id="B34-sensors-25-01224"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fischer</surname><given-names>T.</given-names></name>
<name><surname>Chang</surname><given-names>H.</given-names></name>
<name><surname>Demiris</surname><given-names>Y.</given-names></name>
</person-group><article-title>Rt-gene: Real-time eye gaze estimation in natural environments</article-title><source>Proceedings of the European Conference on Computer Vision (ECCV)</source><conf-loc>Munich, Germany</conf-loc><conf-date>8&#x02013;14 September 2018</conf-date><fpage>334</fpage><lpage>352</lpage></element-citation></ref><ref id="B35-sensors-25-01224"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>Y.</given-names></name>
<name><surname>Lu</surname><given-names>F.</given-names></name>
</person-group><article-title>Gaze Estimation using Transformer</article-title><source>Proceedings of the 2022 26th International Conference on Pattern Recognition (ICPR)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>21&#x02013;25 August 2022</conf-date><fpage>3341</fpage><lpage>3347</lpage><pub-id pub-id-type="doi">10.1109/ICPR56361.2022.9956687</pub-id></element-citation></ref><ref id="B36-sensors-25-01224"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yan</surname><given-names>C.</given-names></name>
<name><surname>Pan</surname><given-names>W.</given-names></name>
<name><surname>Dai</surname><given-names>S.</given-names></name>
<name><surname>Xu</surname><given-names>B.</given-names></name>
<name><surname>Xu</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
</person-group><article-title>FSKT-GE: Feature maps similarity knowledge transfer for low-resolution gaze estimation</article-title><source>IET Image Process.</source><year>2024</year><volume>18</volume><fpage>1642</fpage><lpage>1654</lpage><pub-id pub-id-type="doi">10.1049/ipr2.13056</pub-id></element-citation></ref><ref id="B37-sensors-25-01224"><label>37.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hinton</surname><given-names>G.E.</given-names></name>
<name><surname>Sabour</surname><given-names>S.</given-names></name>
<name><surname>Frosst</surname><given-names>N.</given-names></name>
</person-group><article-title>Matrix capsules with EM routing</article-title><source>Proceedings of the International Conference on Learning Representations</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>30 April&#x02013;3 May 2018</conf-date></element-citation></ref><ref id="B38-sensors-25-01224"><label>38.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Park</surname><given-names>S.</given-names></name>
<name><surname>Beeler</surname><given-names>T.</given-names></name>
<name><surname>Bradley</surname><given-names>D.</given-names></name>
<name><surname>Tang</surname><given-names>S.</given-names></name>
<name><surname>Hilliges</surname><given-names>O.</given-names></name>
</person-group><article-title>ETH-XGaze: A large-scale dataset for gaze estimation under extreme head pose and gaze variation</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#x02013;28 August 2020</conf-date><fpage>365</fpage><lpage>381</lpage></element-citation></ref><ref id="B39-sensors-25-01224"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kellnhofer</surname><given-names>P.</given-names></name>
<name><surname>Recasens</surname><given-names>A.</given-names></name>
<name><surname>Stent</surname><given-names>S.</given-names></name>
<name><surname>Matusik</surname><given-names>W.</given-names></name>
<name><surname>Torralba</surname><given-names>A.</given-names></name>
</person-group><article-title>Gaze360: Physically unconstrained gaze estimation in the wild</article-title><source>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</source><conf-loc>Seoul, Republic of Korea</conf-loc><conf-date>27 October&#x02013;2 November 2019</conf-date></element-citation></ref><ref id="B40-sensors-25-01224"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Sugano</surname><given-names>Y.</given-names></name>
<name><surname>Fritz</surname><given-names>M.</given-names></name>
<name><surname>Bulling</surname><given-names>A.</given-names></name>
</person-group><article-title>Mpiigaze: Real-world dataset and deep appearance-based gaze estimation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>41</volume><fpage>162</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2778103</pub-id><pub-id pub-id-type="pmid">29990057</pub-id>
</element-citation></ref><ref id="B41-sensors-25-01224"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kingma</surname><given-names>D.P.</given-names></name>
<name><surname>Ba</surname><given-names>J.</given-names></name>
</person-group><article-title>Adam: A Method for Stochastic Optimization</article-title><source>Proceedings of the 3rd International Conference on Learning Representations (ICLR)</source><conf-loc>San Diego, CA, USA</conf-loc><conf-date>7&#x02013;9 May 2015</conf-date></element-citation></ref><ref id="B42-sensors-25-01224"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Shi</surname><given-names>B.E.</given-names></name>
</person-group><article-title>Appearance-Based gaze estimation using dilated-convolutions</article-title><source>Proceedings of the Asian Conference on Computer Vision</source><conf-loc>Perth, Australia</conf-loc><conf-date>2&#x02013;6 December 2018</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><volume>Volume 11366</volume><fpage>309</fpage><lpage>324</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01224-f001"><label>Figure 1</label><caption><p>Architecture of an integrated gaze estimation system using deep learning and capsule networks.</p></caption><graphic xlink:href="sensors-25-01224-g001" position="float"/></fig><fig position="float" id="sensors-25-01224-f002"><label>Figure 2</label><caption><p>Comparative analysis of gaze estimation techniques across varied datasets: insights from ETH-XGaze, Gaze360, and MPIIFaceGaze.</p></caption><graphic xlink:href="sensors-25-01224-g002" position="float"/></fig><fig position="float" id="sensors-25-01224-f003"><label>Figure 3</label><caption><p>Examples of results obtained using the proposed method.</p></caption><graphic xlink:href="sensors-25-01224-g003" position="float"/></fig><table-wrap position="float" id="sensors-25-01224-t001"><object-id pub-id-type="pii">sensors-25-01224-t001_Table 1</object-id><label>Table 1</label><caption><p>Key differentiators of GazeCapsNet compared to prior methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Knowledge Transfer</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Capsule Networks</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Lightweight Architecture</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">End-to-End Gaze Estimation</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Real-Time Performance</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>FullFace</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (50 ms per frame)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>RT-GENE</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (40 ms per frame)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>GazeTR-Pure</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm65" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (45 ms per frame)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>FSKT-GE [<xref rid="B36-sensors-25-01224" ref-type="bibr">36</xref>]</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (requires teacher model)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (pre-training overhead)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>GazeCaps</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (25 ms per frame)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>GazeCapsNet (Ours)</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713; (SAR)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713; (MobileNet v2 + ResNet-18)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713; (20 ms per frame)</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01224-t002"><object-id pub-id-type="pii">sensors-25-01224-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison&#x02014;SAR vs. Standard Capsule Routing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Routing Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Iterations Required</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Computational Cost</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dynamic Attention Mechanism</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference Time (ms) &#x02193;</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dynamic Routing (Hinton et al., 2017 [<xref rid="B14-sensors-25-01224" ref-type="bibr">14</xref>])</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713; (3&#x02013;5)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35 ms</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EM Routing (Sabour et al., 2018 [<xref rid="B37-sensors-25-01224" ref-type="bibr">37</xref>])</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713; (4&#x02013;6)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38 ms</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GazeCaps (Wang et al., 2023 [<xref rid="B17-sensors-25-01224" ref-type="bibr">17</xref>])</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713; (3)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25 ms</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Self-Attention Routing (Ours)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (Single-Pass)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x02713;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20 ms</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01224-t003"><object-id pub-id-type="pii">sensors-25-01224-t003_Table 3</object-id><label>Table 3</label><caption><p>Overview of Datasets Used for Evaluating GazeCapsNet Performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Images</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Subjects</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ETH-XGaze [<xref rid="B38-sensors-25-01224" ref-type="bibr">38</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.1 million</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">110</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gaze360 [<xref rid="B39-sensors-25-01224" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">172,000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">238</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MPIIFaceGaze [<xref rid="B40-sensors-25-01224" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45,000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01224-t004"><object-id pub-id-type="pii">sensors-25-01224-t004_Table 4</object-id><label>Table 4</label><caption><p>Training Hyperparameters for GazeCapsNet.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameter</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Value</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Notes</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Batch Size</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">64</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimized for GPU memory efficiency</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning Rate (LR)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0001</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tuned via grid search</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimizer</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adam</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adaptive momentum-based</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weight Decay</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prevents overfitting</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning Rate Scheduler</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cosine Annealing</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Adjusts LR dynamically</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dropout Rate</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.2</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reduces overfitting in fully connected layers</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Capsule Routing Iterations</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1 (SAR)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Single-pass Self-Attention Routing</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01224-t005"><object-id pub-id-type="pii">sensors-25-01224-t005_Table 5</object-id><label>Table 5</label><caption><p>Comparison of Gaze Estimation Methods on ETH-XGaze.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MAE (&#x000b0;) &#x02193;</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference Time (ms) &#x02193;</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters (M) &#x02193;</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FullFace [<xref rid="B33-sensors-25-01224" ref-type="bibr">33</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.53</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">196.6</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RT-GENE [<xref rid="B34-sensors-25-01224" ref-type="bibr">34</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.02</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.0</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FSKT-GE [<xref rid="B36-sensors-25-01224" ref-type="bibr">36</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.91</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.2</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GazeCapsNet (Ours)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.75</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.7</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01224-t006"><object-id pub-id-type="pii">sensors-25-01224-t006_Table 6</object-id><label>Table 6</label><caption><p>Comparison of Gaze Estimation Methods on Gaze360.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MAE (&#x000b0;) &#x02193;</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference Time (ms) &#x02193;</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters (M) &#x02193;</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GazeTR-Pure [<xref rid="B35-sensors-25-01224" ref-type="bibr">35</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.33</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">227.3</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GazeCaps [<xref rid="B17-sensors-25-01224" ref-type="bibr">17</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.10</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.8</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GazeCapsNet (Ours)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.10</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.7</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01224-t007"><object-id pub-id-type="pii">sensors-25-01224-t007_Table 7</object-id><label>Table 7</label><caption><p>Comparison of Gaze Estimation Methods on MPIIFaceGaze.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MAE (&#x000b0;) &#x02193;</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference Time (ms) &#x02193;</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters (M) &#x02193;</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dilated-Net [<xref rid="B42-sensors-25-01224" ref-type="bibr">42</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.42</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.9</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FSKT-GE [<xref rid="B36-sensors-25-01224" ref-type="bibr">36</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.20</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">38</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.2</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GazeCapsNet (Ours)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.06</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.7</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01224-t008"><object-id pub-id-type="pii">sensors-25-01224-t008_Table 8</object-id><label>Table 8</label><caption><p>Impact of Model Configurations on Gaze Estimation Performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model Configuration</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MAE (&#x000b0;)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inference Time (ms)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GazeCapsNet (Full Model)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Without ResNet-18</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Without Self-Attention Routing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6.70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">19</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Using Vanilla CNN Backbone</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01224-t009"><object-id pub-id-type="pii">sensors-25-01224-t009_Table 9</object-id><label>Table 9</label><caption><p>Cross-Dataset Gaze Estimation Performance Comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Training Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MAE (&#x000b0;)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ETH-XGaze</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gaze360</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gaze360</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MPIIFaceGaze</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ETH-XGaze</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MPIIFaceGaze</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.4</td></tr></tbody></table></table-wrap></floats-group></article>