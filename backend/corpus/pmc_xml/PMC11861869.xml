<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006493</article-id><article-id pub-id-type="pmc">PMC11861869</article-id><article-id pub-id-type="doi">10.3390/s25041266</article-id><article-id pub-id-type="publisher-id">sensors-25-01266</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Federated Learning Framework for Real-Time Activity and Context Monitoring Using Edge Devices</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Alharbey</surname><given-names>Rania A.</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01266" ref-type="aff">1</xref><xref rid="c1-sensors-25-01266" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Jamil</surname><given-names>Faisal</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-25-01266" ref-type="aff">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Bellavista</surname><given-names>Paolo</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01266"><label>1</label>Department of Mathematics, Faculty of Science, King Abdulaziz University, Jeddah 21589, Saudi Arabia</aff><aff id="af2-sensors-25-01266"><label>2</label>School of Computing, Engineering and Intelligent Systems, Ulster University, Derry-Londonderry BT48 7JL, Northern Ireland, UK; <email>f.jamil@ulster.ac.uk</email></aff><author-notes><corresp id="c1-sensors-25-01266"><label>*</label>Correspondence: <email>rallehabi@kau.edu.sa</email></corresp></author-notes><pub-date pub-type="epub"><day>19</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1266</elocation-id><history><date date-type="received"><day>15</day><month>10</month><year>2024</year></date><date date-type="rev-recd"><day>31</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>13</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>With the increasing need for effective elderly care solutions, this paper presents a novel federated learning-based system that uses smartphones as edge devices to monitor and enhance elderly care in real-time. In this system, elderly individuals carry smartphones equipped with Inertial Measurement Unit (IMU) sensors, including an accelerometer for activity recognition, a barometer for altitude detection, and a combination of the accelerometer, gyrometer, and magnetometer for location tracking. The smartphones continuously collect real-time data as the elderly individuals go about their daily routines. These data are processed locally on each device to train personalized models for activity recognition and contextual monitoring. The locally trained models are then sent to a federated server, where the FedAvg algorithm is used to aggregate model parameters, creating an improved global model. This aggregated model is subsequently distributed back to the smartphones, enhancing their activity recognition capabilities. In addition to model updates, information on the users&#x02019; location, altitude, and context is sent to the server to enable the continuous monitoring and tracking of the elderly. By integrating activity recognition with location and altitude data, the system provides a comprehensive framework for tracking and supporting the well-being of elderly individuals across diverse environments. This approach offers a scalable and efficient solution for elderly care, contributing to enhanced safety and overall quality of life.</p></abstract><kwd-group><kwd>federated learning</kwd><kwd>elderly care monitoring</kwd><kwd>altitude detection</kwd><kwd>location tracking</kwd><kwd>contextual monitoring</kwd></kwd-group><funding-group><award-group><funding-source>Institutional Fund Projects</funding-source><award-id>IFPIP: 981-247-1443</award-id></award-group><award-group><funding-source>Ministry of Education and King Abdulaziz University, DSR, Jeddah, Saudi Arabia</funding-source></award-group><funding-statement>This research work was funded by Institutional Fund Projects under grant no. (IFPIP: 981-247-1443). The authors gratefully acknowledge technical and financial support provided by the Ministry of Education and King Abdulaziz University, DSR, Jeddah, Saudi Arabia.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01266"><title>1. Introduction</title><p>The global population of elderly individuals is growing rapidly, leading to increased demand for efficient monitoring and care solutions in order to improve their safety and quality of life&#x000a0;[<xref rid="B1-sensors-25-01266" ref-type="bibr">1</xref>]. One of the critical aspects of elderly care is real-time monitoring, which enables caregivers and healthcare professionals to track the physical activity, location, and&#x000a0;well-being of elderly individuals, particularly those living in care facilities or independently at home&#x000a0;[<xref rid="B2-sensors-25-01266" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01266" ref-type="bibr">3</xref>]. Technological advancements, such as the proliferation of smartphones and wearable devices, have introduced new possibilities for health monitoring, making it possible to collect data from various sensors in real-time and process it for valuable insights&#x000a0;[<xref rid="B4-sensors-25-01266" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01266" ref-type="bibr">5</xref>]. Smartphones, which are widely used across all age groups, have proven to be effective tools for monitoring individuals, as&#x000a0;they are equipped with several sensors capable of capturing critical data, including activity levels, location, and&#x000a0;environmental factors&#x000a0;[<xref rid="B6-sensors-25-01266" ref-type="bibr">6</xref>]. In&#x000a0;this context, smartphones can serve as edge devices for continuous monitoring in elderly care systems, where real-time data collection and analysis can offer significant benefits&#x000a0;[<xref rid="B7-sensors-25-01266" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01266" ref-type="bibr">8</xref>]. Specifically, smartphones are equipped with Inertial Measurement Unit (IMU) sensors, such as accelerometers, gyrometers, and&#x000a0;magnetometers, which can be used to detect movements and provide detailed information about a person&#x02019;s physical activities and location&#x000a0;[<xref rid="B9-sensors-25-01266" ref-type="bibr">9</xref>].</p><p>Federated learning has emerged as a powerful technique for training machine learning models in a decentralized manner, allowing data to remain on local devices while model updates are shared with a central server&#x000a0;[<xref rid="B10-sensors-25-01266" ref-type="bibr">10</xref>]. This approach is particularly useful in cases where data privacy is a concern, as&#x000a0;it ensures that raw data are not transmitted to the server&#x000a0;[<xref rid="B11-sensors-25-01266" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01266" ref-type="bibr">12</xref>]. However, in&#x000a0;the context of elderly care, continuous monitoring often requires certain data, such as location and altitude information, to&#x000a0;be transmitted to the server for effective tracking and oversight&#x000a0;[<xref rid="B13-sensors-25-01266" ref-type="bibr">13</xref>]. Despite this, federated learning still plays a crucial role in enhancing the performance of activity recognition models across multiple devices without the need for centralizing all the raw data&#x000a0;[<xref rid="B14-sensors-25-01266" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01266" ref-type="bibr">15</xref>].</p><p>This paper proposes a novel system for elderly care that utilizes federated learning with smartphones as edge devices, where elderly individuals carry smartphones equipped with IMU sensors for continuous data collection. The&#x000a0;proposed system focuses on monitoring the elderly through three key components&#x02014;activity recognition, altitude detection, and&#x000a0;location tracking, all of which are critical for understanding the day-to-day activities and safety of elderly individuals&#x000a0;[<xref rid="B16-sensors-25-01266" ref-type="bibr">16</xref>]. By&#x000a0;integrating federated learning into this system, we can improve the accuracy of activity recognition models while simultaneously tracking the users&#x02019; altitude and location data in real-time&#x000a0;[<xref rid="B17-sensors-25-01266" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01266" ref-type="bibr">18</xref>].</p><p>Activity recognition plays a crucial role in understanding the physical well-being of elderly individuals, as&#x000a0;changes in activity patterns can often indicate health issues such as falls, mobility problems, or&#x000a0;physical inactivity&#x000a0;[<xref rid="B19-sensors-25-01266" ref-type="bibr">19</xref>]. IMU sensors, particularly accelerometers, have been extensively used to detect human activities such as walking, sitting, and&#x000a0;standing, and&#x000a0;they provide an effective solution for non-intrusive monitoring&#x000a0;[<xref rid="B20-sensors-25-01266" ref-type="bibr">20</xref>]. Additionally, gyrometers and magnetometers complement accelerometers by providing information about the orientation and movement of the device, which helps in refining the detection of complex activities and distinguishing between different types of movements&#x000a0;[<xref rid="B21-sensors-25-01266" ref-type="bibr">21</xref>].</p><p>Altitude detection, another important aspect of elderly care, can be particularly relevant for individuals living in multi-story buildings or care facilities, where changes in altitude may indicate movement between floors&#x000a0;[<xref rid="B22-sensors-25-01266" ref-type="bibr">22</xref>]. The&#x000a0;barometer sensor in smartphones measures atmospheric pressure and can be used to estimate changes in altitude with high accuracy. This capability is critical for tracking the location and movement of elderly individuals within such environments, ensuring that caregivers are aware of their&#x000a0;whereabouts.</p><p>Location tracking is an essential feature for ensuring the safety and well-being of elderly individuals, especially for those who may suffer from cognitive impairments or wander off from their designated locations&#x000a0;[<xref rid="B23-sensors-25-01266" ref-type="bibr">23</xref>]. By&#x000a0;leveraging the combination of accelerometers, gyrometers, and&#x000a0;magnetometers, along with GPS when available, smartphones can provide reliable location information, even in environments where GPS signals may be weak, such as indoors. This capability allows for the continuous tracking of elderly individuals in care facilities or during their daily routines, enabling real-time alerts and interventions when necessary&#x000a0;[<xref rid="B24-sensors-25-01266" ref-type="bibr">24</xref>].</p><p>The use of federated learning in this system offers several advantages over traditional centralized approaches. First, federated learning allows the system to continually improve the accuracy of activity recognition models by aggregating updates from multiple devices while reducing the computational and communication burden on the central server. The&#x000a0;FedAvg algorithm, which averages the locally trained models from each smartphone, ensures that the global model benefits from the diversity of data collected across different users and environments. This process allows the model to generalize better to different scenarios, improving the system&#x02019;s overall&#x000a0;performance.</p><p>The proposed system enables continuous model updates without centralized data storage, minimizing the risk of data breaches and allowing seamless integration of new data without disrupting the monitoring process. This holistic approach captures elderly individuals&#x02019; activity patterns, health status, and&#x000a0;location, providing caregivers with actionable insights. By&#x000a0;incorporating real-time location and altitude data, the&#x000a0;system offers a comprehensive framework for monitoring movements and behaviors, enabling rapid responses to emergencies. For&#x000a0;example, sudden altitude changes coupled with inactivity could indicate a fall, while location tracking helps prevent wandering or&#x000a0;disorientation.</p><p>The notable contributions of the proposed study are as&#x000a0;follows:<list list-type="bullet"><list-item><p>Multi-Sensor Data Integration: The system combines activity recognition, location tracking, and&#x000a0;altitude detection using smartphone IMU sensors for real-time elderly&#x000a0;monitoring.</p></list-item><list-item><p>Federated Learning for Activity Recognition: Federated learning with FedAvg enhances activity recognition accuracy without centralizing raw data.</p></list-item><list-item><p>Real-Time Elderly Tracking: The system provides the real-time monitoring of location, altitude, and&#x000a0;context, enabling rapid emergency response.</p></list-item><list-item><p>Scalable and Non-Intrusive Solution: The use of smartphones as edge devices offers a scalable, non-intrusive monitoring system for elderly care.</p></list-item></list></p><p>The rest of the paper is divided into the following sections. <xref rid="sec2-sensors-25-01266" ref-type="sec">Section 2</xref> presents the related works. <xref rid="sec3-sensors-25-01266" ref-type="sec">Section 3</xref> details the Federated Learning-Based Elderly Monitoring System Using Smartphone Sensors, covering data collection, local model training, and&#x000a0;federated learning aggregation. <xref rid="sec4-sensors-25-01266" ref-type="sec">Section 4</xref> presents the experimental results and discussion, including performance evaluation, accuracy improvements, model compression effects, and&#x000a0;scalability analysis. Finally, <xref rid="sec5-sensors-25-01266" ref-type="sec">Section 5</xref> concludes the paper with a summary of the findings and future research&#x000a0;directions.</p></sec><sec id="sec2-sensors-25-01266"><title>2. Related&#x000a0;Work</title><p>The monitoring of elderly individuals for health and safety has garnered significant research attention due to the growing aging population and the increasing demand for non-intrusive and scalable monitoring solutions. Various approaches have been proposed for activity recognition, location tracking, real-time elderly care, and&#x000a0;network monitoring, leveraging different technologies and methodologies&#x000a0;[<xref rid="B25-sensors-25-01266" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01266" ref-type="bibr">26</xref>,<xref rid="B27-sensors-25-01266" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-01266" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-01266" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-01266" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-01266" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01266" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-01266" ref-type="bibr">33</xref>]. A&#x000a0;widely adopted technique in elderly monitoring is wearable-based activity recognition, which, despite its effectiveness, often lacks scalability and comfort for long-term deployment in elderly care settings. Given the ubiquity of smartphones equipped with advanced sensors, they have emerged as a viable alternative for activity recognition and elderly care applications&#x000a0;[<xref rid="B22-sensors-25-01266" ref-type="bibr">22</xref>].</p><sec id="sec2dot1-sensors-25-01266"><title>2.1. Activity Recognition Using Smartphone&#x000a0;Sensors</title><p>Inertial Measurement Unit sensors, including accelerometers, gyroscopes, and&#x000a0;magnetometers, have been extensively utilized for activity recognition, enabling the detection of common human movements such as walking, sitting, and&#x000a0;running&#x000a0;[<xref rid="B34-sensors-25-01266" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-01266" ref-type="bibr">35</xref>,<xref rid="B36-sensors-25-01266" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-01266" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-01266" ref-type="bibr">38</xref>,<xref rid="B39-sensors-25-01266" ref-type="bibr">39</xref>]. Given their integration into smartphones, these sensors provide a convenient platform for continuous monitoring. Prior studies, such as&#x000a0;[<xref rid="B40-sensors-25-01266" ref-type="bibr">40</xref>], have demonstrated the feasibility of using smartphone-based accelerometers for human activity recognition, underscoring their potential in real-world health monitoring systems. However, most existing approaches rely on centralized data collection, which poses challenges related to scalability and&#x000a0;flexibility.</p></sec><sec id="sec2dot2-sensors-25-01266"><title>2.2. Federated Learning in Elderly&#x000a0;Monitoring</title><p>Federated learning has gained traction as a decentralized machine learning paradigm, particularly in privacy-sensitive applications&#x000a0;[<xref rid="B41-sensors-25-01266" ref-type="bibr">41</xref>]. This technique enables devices to train models locally and share only model updates with a central server, thereby preserving data privacy&#x000a0;[<xref rid="B42-sensors-25-01266" ref-type="bibr">42</xref>]. Early implementations of federated learning focused on mobile environments, with&#x000a0;Google pioneering its use in predictive text applications without requiring centralized data storage&#x000a0;[<xref rid="B43-sensors-25-01266" ref-type="bibr">43</xref>]. Subsequent research, including&#x000a0;[<xref rid="B44-sensors-25-01266" ref-type="bibr">44</xref>], expanded federated learning to broader mobile and IoT applications, demonstrating its potential for elderly monitoring systems that demand scalability&#x000a0;[<xref rid="B45-sensors-25-01266" ref-type="bibr">45</xref>].</p><p>In healthcare, federated learning has been explored for collaborative medical diagnostics, allowing multiple institutions to train models on sensitive patient data without directly sharing information&#x000a0;[<xref rid="B46-sensors-25-01266" ref-type="bibr">46</xref>]. In&#x000a0;another study, the&#x000a0;researcher applied federated learning for medical image classification, showing that decentralized learning improves model performance while maintaining data privacy&#x000a0;[<xref rid="B47-sensors-25-01266" ref-type="bibr">47</xref>]. While privacy remains a key advantage, federated learning in the proposed system is primarily leveraged to enhance activity recognition accuracy by aggregating models from multiple devices&#x000a0;[<xref rid="B48-sensors-25-01266" ref-type="bibr">48</xref>].</p></sec><sec id="sec2dot3-sensors-25-01266"><title>2.3. Location Tracking for Elderly&#x000a0;Care</title><p>Location tracking is crucial for elderly individuals, particularly those with cognitive impairments, as&#x000a0;it aids in mitigating risks associated with wandering and disorientation&#x000a0;[<xref rid="B49-sensors-25-01266" ref-type="bibr">49</xref>]. Conventional GPS-based tracking systems often suffer from poor accuracy indoors due to signal attenuation&#x000a0;[<xref rid="B50-sensors-25-01266" ref-type="bibr">50</xref>]. To&#x000a0;address this limitation, researchers have incorporated IMU sensors, such as magnetometers and gyroscopes, to&#x000a0;complement GPS data, enhancing indoor localization accuracy. Ref.&#x000a0;[<xref rid="B51-sensors-25-01266" ref-type="bibr">51</xref>] demonstrated that fusing accelerometer and magnetometer data improves positioning accuracy in environments with weak GPS signals, a&#x000a0;strategy that aligns with the multi-sensor approach adopted in the proposed system&#x000a0;[<xref rid="B45-sensors-25-01266" ref-type="bibr">45</xref>].</p></sec><sec id="sec2dot4-sensors-25-01266"><title>2.4. Altitude Detection for Multi-Level&#x000a0;Tracking</title><p>Barometric sensors have been widely employed for altitude detection in multi-story buildings and uneven terrain&#x000a0;[<xref rid="B52-sensors-25-01266" ref-type="bibr">52</xref>]. These sensors, by&#x000a0;measuring atmospheric pressure variations, provide insights into vertical movement, enabling systems to track floor transitions in elderly care facilities&#x000a0;[<xref rid="B52-sensors-25-01266" ref-type="bibr">52</xref>]. Ref. [<xref rid="B53-sensors-25-01266" ref-type="bibr">53</xref>] utilized smartphone barometer readings to monitor the elevation of elderly individuals in care homes, ensuring caregivers remain informed about their precise location&#x000a0;[<xref rid="B54-sensors-25-01266" ref-type="bibr">54</xref>]. The&#x000a0;integration of altitude detection into real-time monitoring systems enhances caregiver awareness, adding an extra layer of safety for elderly individuals&#x000a0;[<xref rid="B55-sensors-25-01266" ref-type="bibr">55</xref>].</p></sec><sec id="sec2dot5-sensors-25-01266"><title>2.5. Existing Integrated Systems and&#x000a0;Limitations</title><p>The combination of activity recognition, location tracking, and&#x000a0;altitude detection has been investigated across multiple domains. However, few studies have integrated these components into a unified framework tailored for elderly care&#x000a0;[<xref rid="B56-sensors-25-01266" ref-type="bibr">56</xref>]. In another study, the author proposed a smartphone-based system for monitoring elderly individuals&#x02019; activities and locations but lacked altitude detection capabilities&#x000a0;[<xref rid="B57-sensors-25-01266" ref-type="bibr">57</xref>]. Similarly, ref.&#x000a0;[<xref rid="B58-sensors-25-01266" ref-type="bibr">58</xref>] introduced a wearable-smartphone hybrid system for activity recognition and location tracking, yet it lacked real-time federated aggregation. The&#x000a0;novelty of the proposed work lies in the seamless integration of federated learning with activity recognition, location tracking, and&#x000a0;altitude detection, creating a scalable and real-time monitoring framework for elderly&#x000a0;individuals.</p><p>Federated learning in elderly care remains an emerging research area, with&#x000a0;only a few studies exploring its full potential. Ref.&#x000a0;[<xref rid="B59-sensors-25-01266" ref-type="bibr">59</xref>] applied federated learning for fall detection using wearable sensors, highlighting its ability to enhance accuracy while accommodating diverse data distributions. However, their approach was predominantly focused on wearable devices rather than smartphone-based systems, thereby limiting scalability&#x000a0;[<xref rid="B60-sensors-25-01266" ref-type="bibr">60</xref>]. The&#x000a0;proposed system extends this work by leveraging federated learning in a smartphone-based environment, which is inherently more accessible and widely adopted in elderly care settings&#x000a0;[<xref rid="B61-sensors-25-01266" ref-type="bibr">61</xref>].</p><p>None of the existing systems provide a fully integrated federated learning-based framework that combines activity recognition, location tracking, and&#x000a0;altitude detection while maintaining scalability and privacy as&#x000a0;shown in <xref rid="sensors-25-01266-t001" ref-type="table">Table 1</xref>. Most approaches rely on centralized processing, leading to privacy risks, or&#x000a0;require additional infrastructure such as BLE beacons or RFID, limiting real-world deployment. The&#x000a0;proposed model leverages federated learning to ensure privacy-preserving, real-time monitoring while integrating multi-sensor data for accurate and scalable elderly care. Additionally, model compression minimizes the communication overhead, ensuring efficient deployment in resource-constrained&#x000a0;environments.</p></sec></sec><sec id="sec3-sensors-25-01266"><title>3. Federated Learning-Based Elderly Monitoring System Using Smartphone&#x000a0;Sensors</title><p>This study builds upon prior advancements in federated learning, activity recognition, and&#x000a0;indoor localization, employing methodologies that enhance human activity monitoring and smartphone-based tracking. The&#x000a0;PDR-BLE compensation mechanism has demonstrated improvements in indoor localization through sensor fusion, particularly by utilizing accelerometer data for activity detection&#x000a0;[<xref rid="B64-sensors-25-01266" ref-type="bibr">64</xref>]. Furthermore, an&#x000a0;optimal ensemble scheme for human activity recognition (HAR) and floor detection was developed using AutoML, thereby enhancing HAR accuracy through smartphone sensor data&#x000a0;[<xref rid="B65-sensors-25-01266" ref-type="bibr">65</xref>]. Similarly, a&#x000a0;fusion-based localization approach was proposed for tracking smartphone users in multistory buildings, integrating advanced methodologies to address the complexities of indoor environments&#x000a0;[<xref rid="B66-sensors-25-01266" ref-type="bibr">66</xref>]. Recent advancements in evolutionary-enhanced particle filters have focused on the rapid tracking of smartphone users in hazardous scenarios, serving as an inspiration for our approach, which integrates multiple sensor data streams to improve recognition accuracy&#x000a0;[<xref rid="B67-sensors-25-01266" ref-type="bibr">67</xref>]. Additionally, Jamil&#x000a0;et&#x000a0;al. (2024) introduced a hybrid federated learning strategy that optimizes model aggregation across edge devices, thereby enhancing activity recognition&#x000a0;[<xref rid="B68-sensors-25-01266" ref-type="bibr">68</xref>]. The&#x000a0;Swarm Learning Empowered Federated Deep Learning framework has further contributed to optimizing communication efficiency in federated learning, a&#x000a0;key aspect incorporated into our elderly care system&#x000a0;[<xref rid="B69-sensors-25-01266" ref-type="bibr">69</xref>].</p><p>The proposed federated learning-based elderly monitoring system employs smartphones as edge devices as&#x000a0;illustrated in <xref rid="sensors-25-01266-f001" ref-type="fig">Figure 1</xref>. The&#x000a0;system continuously collects real-time data from built-in smartphone sensors for activity recognition, location tracking, altitude estimation, and&#x000a0;contextual awareness. Federated learning ensures that models are trained locally, with&#x000a0;periodic updates aggregated at a central server, enhancing performance without sharing raw&#x000a0;data.</p><p>The process involves sensor-based data collection, local preprocessing, and&#x000a0;activity classification using personalized models. These locally trained models are periodically updated and merged using the Federated Averaging (FedAvg) algorithm to create a global model, which is redistributed to all devices for continuous refinement. This approach supports real-time emergency detection, activity monitoring, and&#x000a0;user tracking while maintaining privacy through decentralized learning, making it a scalable and efficient solution for elderly&#x000a0;care.</p><p>The following sections detail the system architecture, covering sensor-based data collection, local activity recognition, the&#x000a0;federated learning process, global model fusion, and&#x000a0;continuous monitoring strategies. The&#x000a0;system leverages smartphone sensors to collect real-time user data, preprocess them locally, and&#x000a0;classify activities using machine learning models. These models are periodically updated and aggregated via federated learning, ensuring privacy-preserving global model improvement. The&#x000a0;final system provides real-time monitoring, emergency detection, and&#x000a0;adaptive learning, making it suitable for scalable and efficient elderly care&#x000a0;solutions.</p><sec id="sec3dot1-sensors-25-01266"><title>3.1. Edge Device Data&#x000a0;Collection</title><p>Each edge device (smartphone) utilizes inbuilt sensors to collect real-time data for activity recognition, location tracking, altitude detection, and&#x000a0;contextual awareness. These sensors continuously capture motion and environmental data, which is preprocessed locally for various monitoring tasks, including user activity classification, movement tracking, and&#x000a0;altitude estimation. The&#x000a0;collected data undergo feature extraction and signal processing to enhance accuracy before being used in machine learning models. Below, we detail the role of each sensor and the mathematical models employed in processing the&#x000a0;data.</p></sec><sec id="sec3dot2-sensors-25-01266"><title>3.2. Activity&#x000a0;Recognition</title><p>The accelerometer is the primary sensor used for detecting human activities. It measures the rate of change in velocity along three orthogonal axes: <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and&#x000a0;<inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The&#x000a0;overall movement intensity is represented by the acceleration magnitude, computed using the Euclidean norm:<disp-formula id="FD1-sensors-25-01266"><label>(1)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Acc</mml:mi><mml:mi>magnitude</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This metric helps distinguish activities such as walking, sitting, standing, and&#x000a0;running. For&#x000a0;instance, walking exhibits periodic oscillations, whereas sitting or standing results in near-zero&#x000a0;acceleration.</p><p>To enhance activity recognition, accelerometer data are segmented into fixed time windows, and&#x000a0;key statistical features such as mean, variance, and&#x000a0;root mean square (RMS) are extracted. A&#x000a0;machine learning model, represented as <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is trained locally on the device using these features, where <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the sensor readings, and&#x000a0;<inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the model parameters. The&#x000a0;model optimization follows a <italic toggle="yes">cross-entropy loss function</italic>:<disp-formula id="FD2-sensors-25-01266"><label>(2)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>activity</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> is the total number of data samples, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the true activity label, and&#x000a0;<inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:math></inline-formula> is the loss function used for&#x000a0;classification.</p></sec><sec id="sec3dot3-sensors-25-01266"><title>3.3. Location&#x000a0;Tracking</title><p>In GPS-denied environments, a&#x000a0;combination of gyroscope, magnetometer, and&#x000a0;accelerometer sensors is used to estimate the user&#x02019;s movement trajectory. The&#x000a0;gyroscope measures the rotational velocity around the <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and <italic toggle="yes">z</italic> axes, while the magnetometer provides absolute orientation by detecting the Earth&#x02019;s magnetic&#x000a0;field.</p><p>The orientation change is computed by integrating the gyroscope&#x02019;s angular velocity:<disp-formula id="FD3-sensors-25-01266"><label>(3)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the rotational velocities along the three axes. The&#x000a0;magnetometer is used to correct drift in gyroscope&#x000a0;measurements.</p><p>The user&#x02019;s updated position <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is determined as<disp-formula id="FD4-sensors-25-01266"><label>(4)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold">p</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mo>(</mml:mo><mml:mo form="prefix">cos</mml:mo><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo form="prefix">sin</mml:mo><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where we have the following:<list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the previous position;</p></list-item><list-item><p><inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is velocity estimated from accelerometer readings;</p></list-item><list-item><p><inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the time interval;</p></list-item><list-item><p><inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> is the corrected orientation angle.</p></list-item></list></p><p>This sensor fusion approach enables accurate indoor localization, compensating for GPS limitations in confined&#x000a0;environments.</p></sec><sec id="sec3dot4-sensors-25-01266"><title>3.4. Altitude&#x000a0;Detection</title><p>The barometer sensor measures atmospheric pressure, which decreases with increasing altitude. This relationship allows altitude estimation using the barometric formula<disp-formula id="FD5-sensors-25-01266"><label>(5)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>L</mml:mi></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>P</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mfenced><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:mfrac></mml:mstyle></mml:msup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where we have the following:<list list-type="bullet"><list-item><p><italic toggle="yes">h</italic> is the altitude (m);</p></list-item><list-item><p><inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the standard temperature at sea level (K);</p></list-item><list-item><p><italic toggle="yes">L</italic> is the temperature lapse rate (K/m);</p></list-item><list-item><p><italic toggle="yes">P</italic> is the measured atmospheric pressure (Pa);</p></list-item><list-item><p><inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the sea-level standard atmospheric pressure (101,325 Pa);</p></list-item><list-item><p><italic toggle="yes">R</italic> is the specific gas constant for dry air (287.05 J/(kg&#x000b7;K));</p></list-item><list-item><p><italic toggle="yes">g</italic> is the gravitational acceleration (9.80665 m/s<sup>2</sup>).</p></list-item></list></p><p>By continuously monitoring the altitude variations, the&#x000a0;system detects floor-level transitions in multi-story buildings, enabling enhanced indoor localization and movement&#x000a0;tracking.</p></sec><sec id="sec3dot5-sensors-25-01266"><title>3.5. Contextual&#x000a0;Information</title><p>The accelerometer, gyroscope, and&#x000a0;magnetometer provide contextual information about the user&#x02019;s movement patterns and environment. By&#x000a0;analyzing sensor variations, the&#x000a0;system infers whether the user is indoors or outdoors, stationary or moving, and&#x000a0;engaged in specific activities such as walking, running, or&#x000a0;climbing stairs. This information enhances activity recognition&#x000a0;accuracy.</p><p>To extract meaningful features, statistical metrics such as variance, root mean square (RMS), skewness, and&#x000a0;correlation are computed over sliding time&#x000a0;windows.</p><p>The accelerometer measures acceleration along three axes (<inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>), capturing linear movement. The&#x000a0;overall movement magnitude is<disp-formula id="FD6-sensors-25-01266"><label>(6)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Acc</mml:mi><mml:mi>magnitude</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Key statistical features from accelerometer data include the following:<list list-type="bullet"><list-item><p>Variance (<inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>)&#x02014;identifies movement intensity:<disp-formula id="FD7-sensors-25-01266"><label>(7)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Root mean square (RMS)&#x02014;measures the movement magnitude:<disp-formula id="FD8-sensors-25-01266"><label>(8)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>RMS</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Skewness&#x02014;detects asymmetric movement patterns:<disp-formula id="FD9-sensors-25-01266"><label>(9)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Skewness</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfenced><mml:mrow><mml:mn>3</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item></list></p><p>The gyroscope captures angular velocity (<inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>), reflecting rotational movement. The&#x000a0;same statistical features&#x02014;variance, RMS, and&#x000a0;skewness&#x02014;are extracted to measure the intensity, magnitude, and&#x000a0;irregularity of rotational&#x000a0;motion.</p><p>The magnetometer measures the Earth&#x02019;s magnetic field (<inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) to infer user heading and orientation. Relevant features include the following:<list list-type="bullet"><list-item><p>Correlation&#x02014;measures directional consistency:<disp-formula id="FD10-sensors-25-01266"><label>(10)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Corr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Magnetic Field Magnitude&#x02014;captures environmental changes:<disp-formula id="FD11-sensors-25-01266"><label>(11)</label><mml:math id="mm32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>magnitude</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item></list></p><p>The extracted sensor features are combined into a feature vector:<disp-formula id="FD12-sensors-25-01266"><label>(12)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>Var</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>RMS</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>Skewness</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>Var</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>RMS</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>Skewness</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003c9;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>Corr</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>M</mml:mi><mml:mi>magnitude</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This feature vector serves as input to a neural network for activity and environmental classification, enhancing real-time activity recognition and movement&#x000a0;tracking.</p><sec id="sec3dot5dot1-sensors-25-01266"><title>3.5.1. Local Model&#x000a0;Training</title><p>Each edge device (smartphone) trains a machine learning model, such as a neural network or Random Forest, to&#x000a0;classify the user&#x02019;s activities based on sensor data from the accelerometer, gyroscope, magnetometer, and&#x000a0;barometer. Local training enables each device to personalize the model while ensuring data privacy by keeping the raw sensor data&#x000a0;on-device.</p><p>The raw sensor data are processed to extract meaningful features, such as variance, root mean square (RMS), skewness, and&#x000a0;correlation. Let <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> be the feature vector for the <italic toggle="yes">i</italic>-th time window, where <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and <italic toggle="yes">d</italic> is the number of extracted features. These features are used as input for the multi-class activity classification&#x000a0;model.</p><p>For multi-class classification, the&#x000a0;model predicts a probability distribution over <italic toggle="yes">C</italic> activity classes (e.g., walking, running, and sitting). The&#x000a0;training objective minimizes the difference between the predicted and ground truth class labels using the cross-entropy loss:<disp-formula id="FD13-sensors-25-01266"><label>(13)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the true label, <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the model prediction, and&#x000a0;<inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow></mml:math></inline-formula> represents the cross-entropy&#x000a0;function:<disp-formula id="FD14-sensors-25-01266"><label>(14)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a binary indicator for class <italic toggle="yes">c</italic>, and&#x000a0;<inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the predicted probability for that class, computed using the softmax function:<disp-formula id="FD15-sensors-25-01266"><label>(15)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:msup><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the raw model output (logit) for class <italic toggle="yes">c</italic>.</p><p>For neural networks, model parameters <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> are updated using gradient descent:<disp-formula id="FD16-sensors-25-01266"><label>(16)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:msub><mml:mo>&#x02207;</mml:mo><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:math></inline-formula> is the learning rate, and&#x000a0;<inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mo>&#x02207;</mml:mo><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the gradient of the loss function. Random Forest models, instead of using gradient descent, construct multiple decision trees, and&#x000a0;the final classification is determined through majority voting or probability&#x000a0;averaging.</p><p>To prevent overfitting, L2 regularization is applied:<disp-formula id="FD17-sensors-25-01266"><label>(17)</label><mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>local</mml:mi></mml:mrow><mml:mi>reg</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msubsup><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></inline-formula> is the regularization&#x000a0;coefficient.</p><p>The final model classifies each sample by selecting the class with the highest probability:<disp-formula id="FD18-sensors-25-01266"><label>(18)</label><mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo form="prefix">arg</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In summary, each edge device trains a local model using extracted sensor features and optimizes it via cross-entropy loss minimization. Softmax transformation ensures probability-based classification and regularization techniques such as L2 regularization mitigate overfitting. These locally trained models are periodically updated and aggregated in the federated learning process to improve the overall system&#x000a0;performance.</p></sec><sec id="sec3dot5dot2-sensors-25-01266"><title>3.5.2. Preprocessing</title><p>Preprocessing is a crucial step to ensure the reliability of sensor data before they are used for activity classification. The&#x000a0;raw data from the accelerometer, gyroscope, and&#x000a0;magnetometer undergo noise filtering and normalization to maintain consistency across different sensor&#x000a0;readings.</p><p>The preprocessed data are then segmented into sliding windows to capture short-term activity patterns. For&#x000a0;each window, key statistical features are extracted, forming a feature vector that represents the sensor readings. The&#x000a0;extracted features include the following:<list list-type="bullet"><list-item><p>Mean: represents the average activity level within the window.</p></list-item><list-item><p>Standard deviation: measures the variability in movement.</p></list-item><list-item><p>Skewness: indicates the asymmetry in the distribution of values.</p></list-item><list-item><p>Entropy: quantifies the complexity of movement patterns.</p></list-item></list></p><p>These features provide a compact yet informative representation of the sensor data, which are then used as input to the machine learning models for activity classification. The&#x000a0;preprocessing step ensures that the data are structured, free of noise, and&#x000a0;ready for the real-time recognition of user&#x000a0;activities.</p></sec><sec id="sec3dot5dot3-sensors-25-01266"><title>3.5.3. Local Model&#x000a0;Training</title><p>Each edge device trains a machine learning model, such as a Bidirectional Long Short-Term Memory (Bi-LSTM) network, to&#x000a0;classify user activities based on sensor data. Bi-LSTM is well suited for time-series data, as it captures both forward and backward dependencies, making it ideal for activity recognition. The&#x000a0;input to the Bi-LSTM is the feature vector <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, derived from the sliding window approach during preprocessing, which includes features such as mean, standard deviation, skewness, and&#x000a0;entropy.</p><p>Let <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represent the sequence of feature vectors over <italic toggle="yes">T</italic> windows. Each Bi-LSTM unit processes this sequence while maintaining two hidden states for forward and backward passes:<disp-formula id="FD19-sensors-25-01266"><label>(19)</label><mml:math id="mm54" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the forward and backward hidden states at time <italic toggle="yes">t</italic>. The&#x000a0;final hidden state <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is passed through a fully connected layer with a softmax activation function to compute the probability distribution over activity classes:<disp-formula id="FD20-sensors-25-01266"><label>(20)</label><mml:math id="mm58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>softmax</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:math></inline-formula> are the weight matrix and bias for the output&#x000a0;layer.</p><p>The model is trained to minimize the cross-entropy loss:<disp-formula id="FD21-sensors-25-01266"><label>(21)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the true label, and&#x000a0;<inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the predicted probability distribution. The&#x000a0;parameters <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> are updated using gradient-based optimization such as Stochastic Gradient Descent (SGD) or Adam:<disp-formula id="FD22-sensors-25-01266"><label>(22)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:msub><mml:mo>&#x02207;</mml:mo><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:msub><mml:mi>L</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:math></inline-formula> is the learning rate. To&#x000a0;prevent overfitting, dropout regularization, and&#x000a0;L2 weight regularization are applied:<disp-formula id="FD23-sensors-25-01266"><label>(23)</label><mml:math id="mm67" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>local</mml:mi></mml:mrow><mml:mi>reg</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msubsup><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></inline-formula> is the regularization&#x000a0;parameter.</p><p>The locally trained Bi-LSTM model improves over time by leveraging its ability to capture temporal dependencies in sequential data, ensuring the better recognition of activities such as walking, sitting, or&#x000a0;transitioning between states. These local models are periodically updated and aggregated in a federated learning setting to improve overall system&#x000a0;performance.</p><p>Algorithm 1 describes the process of collecting, processing, and&#x000a0;transmitting sensor data from edge devices to a federated server for elderly monitoring. Each edge device gathers data from accelerometer, gyroscope, magnetometer, and&#x000a0;barometer sensors, which are then preprocessed through noise filtering, normalization, and&#x000a0;segmentation into time windows. Extracted features are used for activity recognition, location tracking, and&#x000a0;altitude estimation. A&#x000a0;Bi-LSTM model is trained locally to classify activities based on sensor data, minimizing a local loss&#x000a0;function.</p><p>After training, the&#x000a0;model parameters are compressed and sent to the federated server along with the location and altitude data. The&#x000a0;server aggregates the updates from multiple devices using the Federated Averaging (FedAvg) algorithm to improve the global model, which is then redistributed to the edge devices. This iterative process ensures the continuous refinement of activity recognition while preserving privacy by keeping raw data on local devices. The&#x000a0;server also maintains real-time location and altitude data for emergency detection, optimizing elderly care monitoring through federated&#x000a0;learning.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1</bold>&#x000a0;Federated learning system: edge device to server&#x000a0;communication</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Input</bold>: Sensor data from accelerometer, gyroscope, magnetometer, and&#x000a0;barometer on each edge device.<break/><bold>Output</bold>: Updated model parameters, user location, altitude, and&#x000a0;contextual information sent to the federated server.
<list list-type="simple"><list-item><label>&#x000a0;&#x000a0;1:</label><p><bold>Sensor Data Collection</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;2:</label><p>Collect sensor data: accelerometer <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, gyroscope <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, magnetometer <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and&#x000a0;barometer <italic toggle="yes">P</italic> with timestamp <italic toggle="yes">t</italic>.</p></list-item><list-item><label>&#x000a0;&#x000a0;3:</label><p><bold>Preprocessing</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;4:</label><p>Apply noise filtering, normalization, and&#x000a0;segment data into fixed time windows of size <italic toggle="yes">w</italic>.</p></list-item><list-item><label>&#x000a0;&#x000a0;5:</label><p><bold>Feature Extraction</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;6:</label><p>Compute task-specific features:
<list list-type="bullet"><list-item><p><bold>Activity Recognition</bold>: Compute acceleration magnitude:<disp-formula><mml:math id="mm72" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Acc</mml:mi><mml:mi>magnitude</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p><bold>Location Tracking</bold>: Estimate user location <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">L</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> using sensor fusion.</p></list-item><list-item><p><bold>Altitude Detection</bold>: Compute altitude <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from barometric pressure:<disp-formula><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>T</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>L</mml:mi></mml:mfrac></mml:mstyle><mml:mfenced separators="" open="(" close=")"><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mfrac></mml:mstyle></mml:mfenced><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:mfrac></mml:mstyle></mml:msup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item></list></p></list-item><list-item><label>&#x000a0;&#x000a0;7:</label><p><bold>Local Model Training</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;8:</label><p>Train Bi-LSTM model on edge device using extracted features:<disp-formula><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>local</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;9:</label><p><bold>Transmission to Server</bold></p></list-item><list-item><label>10:</label><p>Compress model parameters <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mi>compressed</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, send <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mi>compressed</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, location <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">L</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, altitude <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and&#x000a0;contextual information to the server.</p></list-item><list-item><label>11:</label><p><bold>Server Aggregation</bold></p></list-item><list-item><label>12:</label><p>Aggregate received model parameters via Federated Averaging (FedAvg):<disp-formula><mml:math id="mm81" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mi>compressed</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Store user location, altitude, and&#x000a0;contextual information for monitoring.</p></list-item><list-item><label>13:</label><p><bold>Global Model Update</bold></p></list-item><list-item><label>14:</label><p>Update the global model and distribute <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> back to edge devices for the next training round.</p></list-item></list></td></tr></tbody></array></p></sec></sec><sec id="sec3dot6-sensors-25-01266"><title>3.6. Federated Learning&#x000a0;Process</title><p>After training the local Bi-LSTM model on each edge device, the&#x000a0;model parameters are shared with a central server, where federated learning is employed to aggregate these local models into a global model. This ensures data privacy by keeping raw sensor data on the devices while only transmitting learned parameters. Each device <italic toggle="yes">k</italic> computes its local model parameters <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, including input-to-hidden weights <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>ih</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, hidden-to-hidden weights <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>hh</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and&#x000a0;bias terms <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:math></inline-formula>, which are then sent to the central server for&#x000a0;aggregation.</p><p>The Federated Averaging (FedAvg) algorithm is used to aggregate models from all participating devices. The&#x000a0;global model <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is obtained by computing a weighted average of local models:<disp-formula id="FD24-sensors-25-01266"><label>(24)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">K</italic> is the number of devices, <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the data samples on device <italic toggle="yes">k</italic>, and&#x000a0;<inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the total number of samples. Each device contributes proportionally based on its data volume. The&#x000a0;aggregated model parameters, including weights and biases, are updated layer-wise using<disp-formula id="FD25-sensors-25-01266"><label>(25)</label><mml:math id="mm91" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">W</mml:mi><mml:mi>global</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:msup><mml:mi mathvariant="bold">W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD26-sensors-25-01266"><label>(26)</label><mml:math id="mm92" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">b</mml:mi><mml:mi>global</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:msup><mml:mi mathvariant="bold">b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The global model is then distributed back to the devices, initializing local models for the next round of training:<disp-formula id="FD27-sensors-25-01266"><label>(27)</label><mml:math id="mm93" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This iterative process continues until the global model converges to an optimal solution. The&#x000a0;training process minimizes a global objective function:<disp-formula id="FD28-sensors-25-01266"><label>(28)</label><mml:math id="mm94" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mi>&#x003b8;</mml:mi></mml:munder><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>local</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>local</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the local loss function for device <italic toggle="yes">k</italic>. The&#x000a0;FedAvg algorithm ensures that the global model effectively integrates knowledge from all devices while maintaining privacy. The&#x000a0;convergence of this process depends on factors such as the number of devices, data distribution, and&#x000a0;the number of training&#x000a0;rounds.</p></sec><sec id="sec3dot7-sensors-25-01266"><title>3.7. Global Model Fusion and&#x000a0;Distribution</title><p>Once the global model <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is updated on the central server through the aggregation of local model parameters from all participating devices, it is distributed back to the edge devices. This enables each device to benefit from the collective knowledge across the network, incorporating diverse data distributions into its local model. The&#x000a0;global model serves as the initialization point for subsequent local training, leading to improved accuracy and adaptation for each edge&#x000a0;device.</p><p>After receiving <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, each edge device <italic toggle="yes">k</italic> initializes its local model for the next training&#x000a0;round:<disp-formula id="FD29-sensors-25-01266"><label>(29)</label><mml:math id="mm98" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the local model parameters for device <italic toggle="yes">k</italic> at iteration <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and&#x000a0;<inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the global model from the previous&#x000a0;iteration.</p><p>By leveraging the globally fused parameters, local models benefit from a broader range of data, particularly from devices with larger or more diverse datasets. This process accelerates convergence, reducing the number of training epochs needed to achieve optimal accuracy. Moreover, local models are further fine-tuned on individual device data <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, ensuring personalization while maintaining&#x000a0;privacy.</p><p>The local training objective for each device in the next iteration is<disp-formula id="FD30-sensors-25-01266"><label>(30)</label><mml:math id="mm103" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo form="prefix">arg</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:munder><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>local</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>local</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> represents the local loss function. The&#x000a0;iterative process of updating and redistributing the global model continues for multiple rounds, progressively refining both the global and local models. As&#x000a0;the global model aggregates updates from all devices, it becomes increasingly robust. Simultaneously, local models improve with each iteration by initializing with more accurate global parameters, leading to higher activity recognition accuracy while ensuring data&#x000a0;privacy.</p></sec><sec id="sec3dot8-sensors-25-01266"><title>3.8. Continuous Monitoring and&#x000a0;Adaptation</title><p>The system provides continuous monitoring of elderly individuals by integrating real-time data collection from edge devices with adaptive model updates. This ensures responsiveness to changes in user behavior, enabling the timely detection of abnormalities and automatic alerts when necessary. The&#x000a0;combination of activity recognition, location tracking, and&#x000a0;model adaptation forms the core of the system&#x02019;s ability to ensure user&#x000a0;safety.</p><sec id="sec3dot8dot1-sensors-25-01266"><title>3.8.1. Real-Time Activity&#x000a0;Recognition</title><p>Edge devices continuously perform activity recognition using the latest global model <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> received from the central server. This synchronization ensures that the local Bi-LSTM model on each device benefits from aggregated knowledge across all users, improving the accuracy and reliability of real-time&#x000a0;predictions.</p><p>For each time window <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> at time <italic toggle="yes">t</italic>, the&#x000a0;feature vector <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></inline-formula> is processed by the Bi-LSTM model on the edge device, initialized with the global parameters:<disp-formula id="FD31-sensors-25-01266"><label>(31)</label><mml:math id="mm108" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>Bi-LSTM</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the predicted activity class at time <italic toggle="yes">t</italic>, and&#x000a0;<inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>Bi</mml:mi><mml:mo>-</mml:mo><mml:mi>LSTM</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the model function using <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>The system continuously recognizes activities such as walking, sitting, and&#x000a0;running. Deviations from expected behavior, such as prolonged inactivity or abnormal movements, are detected by comparing <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with predefined normal behaviors:<disp-formula id="FD32-sensors-25-01266"><label>(32)</label><mml:math id="mm113" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>dist</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>expected</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>If the deviation <inline-formula><mml:math id="mm114" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> exceeds a threshold <inline-formula><mml:math id="mm115" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>threshold</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, an&#x000a0;alert is triggered:<disp-formula id="FD33-sensors-25-01266"><label>(33)</label><mml:math id="mm116" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>threshold</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4.pt"/><mml:mi>trigger</mml:mi><mml:mspace width="4.pt"/><mml:mi>alert</mml:mi><mml:mspace width="4.pt"/><mml:mi>to</mml:mi><mml:mspace width="4.pt"/><mml:mrow><mml:mi>caregivers</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This mechanism ensures real-time notifications for caregivers or administrators in the case of abnormal activity&#x000a0;detection.</p></sec><sec id="sec3dot8dot2-sensors-25-01266"><title>3.8.2. Emergency Alerts and Location&#x000a0;Tracking</title><p>In addition to activity recognition, the&#x000a0;system continuously tracks the user&#x02019;s location and altitude for real-time monitoring and emergency detection. IMU sensors, including accelerometers, gyroscopes, and&#x000a0;magnetometers, capture movement patterns, while the barometer detects altitude changes. The&#x000a0;location at time <italic toggle="yes">t</italic> is computed as<disp-formula id="FD34-sensors-25-01266"><label>(34)</label><mml:math id="mm117" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">L</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Location</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Rapid altitude changes combined with inactivity may indicate emergencies such as falls. If&#x000a0;a sudden drop in altitude (<inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>threshold</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) is detected along with prolonged inactivity (<inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>inactivity</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), an&#x000a0;emergency alert is triggered:<disp-formula id="FD35-sensors-25-01266"><label>(35)</label><mml:math id="mm120" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>if</mml:mi><mml:mspace width="4.pt"/><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>threshold</mml:mi></mml:msub><mml:mspace width="4.pt"/><mml:mi>and</mml:mi><mml:mspace width="4.pt"/><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>inactivity</mml:mi><mml:mo>,</mml:mo><mml:mspace width="4.pt"/><mml:mi>trigger</mml:mi><mml:mspace width="4.pt"/><mml:mi>emergency</mml:mi><mml:mspace width="4.pt"/><mml:mrow><mml:mi>alert</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Location <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">L</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and altitude data are logged and transmitted to the central server for real-time monitoring, enabling caregivers to track the user&#x02019;s status&#x000a0;remotely.</p></sec><sec id="sec3dot8dot3-sensors-25-01266"><title>3.8.3. Adaptive Model&#x000a0;Updates</title><p>The system adapts dynamically by continuously refining local models as more data are collected. Edge devices periodically retrain their models using the latest global parameters <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the most recent local data, improving real-time monitoring&#x000a0;accuracy.</p><p>At each training round <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, updated global parameters <inline-formula><mml:math id="mm124" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are used to reinitialize the local models:<disp-formula id="FD36-sensors-25-01266"><label>(36)</label><mml:math id="mm125" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This continuous feedback loop between monitoring, emergency detection, and&#x000a0;model adaptation ensures the system remains robust and responsive to behavioral changes, enhancing the reliability of elderly care&#x000a0;monitoring.</p><p>Algorithm 2 describes a federated learning framework for continuous monitoring and activity recognition using edge devices. Each global round begins with the distribution of the global model to all participating edge devices. These devices then locally update their Bi-LSTM models using collected sensor data. The&#x000a0;locally trained models are optionally compressed and transmitted back to the central server, where they are aggregated using the Federated Averaging (FedAvg) algorithm to update the global model. This refined global model is redistributed to edge devices for real-time activity recognition and tracking. Alerts are triggered for anomalies, such as abnormal activity patterns, prolonged inactivity, or&#x000a0;sudden altitude variations indicative of emergencies. This iterative process continuously enhances model accuracy and system responsiveness, ensuring reliable elderly monitoring while preserving user&#x000a0;privacy.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2</bold>&#x000a0;Federated learning system for continuous monitoring and activity&#x000a0;recognition</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><bold>Input</bold>: Data on each edge device <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, learning rate <inline-formula><mml:math id="mm127" overflow="scroll"><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow></mml:math></inline-formula>, number of local epochs <italic toggle="yes">E</italic>, number of global rounds <italic toggle="yes">T</italic>, compression factor <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula>, communication threshold <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>threshold</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula><break/><bold>Output</bold>: Global model <inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula><list list-type="simple"><list-item><label>&#x000a0;&#x000a0;1:</label><p>Initialize global model <inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> with random weights</p></list-item><list-item><label>&#x000a0;&#x000a0;2:</label><p><bold>for</bold> each global round <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <italic toggle="yes">T</italic>&#x000a0;<bold>do</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;3:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>for</bold> each edge device <inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <italic toggle="yes">K</italic>&#x000a0;<bold>do</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;4:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Distribute <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> to device <italic toggle="yes">k</italic></p></list-item><list-item><label>&#x000a0;&#x000a0;5:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Initialize local model: <inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;6:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>for</bold> each local epoch <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <italic toggle="yes">E</italic>&#x000a0;<bold>do</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;7:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Update local model parameters:<disp-formula><mml:math id="mm137" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02190;</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b7;</mml:mi><mml:msub><mml:mo>&#x02207;</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>local</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><label>&#x000a0;&#x000a0;8:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>end for</bold></p></list-item><list-item><label>&#x000a0;&#x000a0;9:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Compress model parameters: <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>compressed</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>10:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Send <inline-formula><mml:math id="mm139" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>compressed</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> to server</p></list-item><list-item><label>11:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>end for</bold></p></list-item><list-item><label>12:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Aggregate models on server:<disp-formula><mml:math id="mm140" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>n</mml:mi></mml:mfrac></mml:mstyle><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>compressed</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><label>13:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>for</bold> each edge device <inline-formula><mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <italic toggle="yes">K</italic>&#x000a0;<bold>do</bold></p></list-item><list-item><label>14:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Perform real-time activity recognition:<disp-formula><mml:math id="mm142" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>Bi-LSTM</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p></list-item><list-item><label>15:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>if</bold>&#x000a0;<inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>dist</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>expected</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>threshold</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>then</bold></p></list-item><list-item><label>16:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Trigger alert to caregivers</p></list-item><list-item><label>17:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>end if</bold></p></list-item><list-item><label>18:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Track location <inline-formula><mml:math id="mm144" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">L</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and detect altitude <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item><list-item><label>19:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>if</bold>&#x000a0;<inline-formula><mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>threshold</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm147" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>inactivity</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>&#x000a0;<bold>then</bold></p></list-item><list-item><label>20:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Trigger emergency alert</p></list-item><list-item><label>21:</label><p>&#x02009;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>end if</bold></p></list-item><list-item><label>22:</label><p>&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>end for</bold></p></list-item><list-item><label>23:</label><p><bold>end for</bold></p></list-item><list-item><label>24:</label><p><bold>return</bold> Global model <inline-formula><mml:math id="mm148" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p></list-item></list></td></tr></tbody></array></p></sec></sec></sec><sec id="sec4-sensors-25-01266"><title>4. Experimental Results and&#x000a0;Discussion</title><sec id="sec4dot1-sensors-25-01266"><title>4.1. Development&#x000a0;Environment</title><p>The development environment of the proposed system is categorized into hardware and software components.</p><sec id="sec4dot1dot1-sensors-25-01266"><title>4.1.1. Hardware Environment</title><p>The hardware setup consists of Android smartphones, specifically Samsung Galaxy Note 20 devices, equipped with inbuilt IMU sensors, including accelerometers, gyroscopes, magnetometers, and barometers, for real-time sensor data acquisition. Experiments were conducted in Building 4 of Jeju National University, Republic of Korea, with 25 participants (9 females, 16 males) across three age groups: 10 participants aged 30&#x02013;35 (4 females, 6 males), 5 participants aged 46&#x02013;50 (2 females, 3 males), and 10 participants aged 55&#x02013;60 (3 females, 7 males). Each participant performed six predefined activities&#x02014;walking, running, sitting, standing, climbing stairs, and descending stairs&#x02014;recorded for 2 min per activity per individual at a sampling rate of 50 Hz, resulting in approximately 900,000 data samples.</p><p>A custom Android application was developed to acquire and manage sensor data using the Android Sensor API. It captured high-frequency IMU and barometer readings in real-time and integrated Pedestrian Dead Reckoning (PDR) for trajectory estimation using accelerometer and gyroscope data, even in GPS-limited environments. A step detection algorithm employed the peak detection of vertical accelerations to identify walking steps and compute step counts. Barometer readings enabled altitude estimation using the barometric formula, facilitating floor transition monitoring. Raw sensor data were stored locally in CSV format and later transferred to a computer system for preprocessing and analysis.</p></sec><sec id="sec4dot1dot2-sensors-25-01266"><title>4.1.2. Software Environment</title><p>The software environment consisted of Python-based tools and libraries. Data preprocessing involved low-pass filtering for noise reduction and signal enhancement, followed by feature extraction, where metrics such as root mean square (RMS), variance, skewness, and correlation are computed for each sensor axis.</p><p>Activity recognition was performed using a Bidirectional Long Short-Term Memory (Bi-LSTM) network, implemented with TensorFlow and Scikit-learn. The Bi-LSTM architecture comprised an input layer, two bidirectional LSTM layers with 64 hidden units each, and a fully connected output layer with 6 nodes corresponding to activity classes. Hidden layers used ReLU activation, while the output layer employed softmax activation for class probability estimation. The model, consisting of approximately 24,000 trainable parameters, was trained using the Adam optimizer with a learning rate of 0.001, minimizing categorical cross-entropy loss.</p><p>The federated learning framework implemented the FedAvg algorithm for decentralized model training while preserving data privacy. Each device trained a local Bi-LSTM model on its own sensor data, minimizing a local loss function. The trained model parameters were then transmitted to a central server for aggregation, where a weighted average of the parameters was computed based on the number of data samples per device. The updated global model was redistributed to all edge devices for further training, iterating until convergence. FedAvg facilitated secure distributed learning without sharing raw data, ensuring privacy while enhancing overall model performance.</p><p>Additional libraries, including NumPy (version 2.2.3), Pandas (version 2.2.2), and Matplotlib (version 3.9.0), were used for data manipulation and visualization, offering insights into model performance metrics and behavior. The integration of hardware, software tools, and federated learning enabled robust data handling, efficient model training, and privacy-preserving aggregation, ensuring the effectiveness of the proposed system.</p></sec></sec><sec id="sec4dot2-sensors-25-01266"><title>4.2. Implementation</title><p>In this study, &#x0201c;accuracy&#x0201d; primarily refers to the system&#x02019;s performance in real-time activity recognition, classifying activities such as walking, running, sitting, and climbing stairs based on sensor data. Local model accuracy represents the classification performance on individual edge devices, while global model accuracy, obtained through Federated Averaging (FedAvg), reflects the improvement achieved by aggregating knowledge across multiple devices. For emergency detection, accuracy measures the system&#x02019;s ability to reliably identify critical events such as sudden altitude drops and prolonged inactivity, with minimal false positives. While the study also considers altitude estimation, location tracking, and emergency detection using metrics such as response time and false positive rate, the accuracy metrics in <xref rid="sensors-25-01266-t002" ref-type="table">Table 2</xref>, <xref rid="sensors-25-01266-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-01266-t004" ref-type="table">Table 4</xref> and <xref rid="sensors-25-01266-f002" ref-type="fig">Figure 2</xref>, <xref rid="sensors-25-01266-f003" ref-type="fig">Figure 3</xref> and <xref rid="sensors-25-01266-f004" ref-type="fig">Figure 4</xref> primarily focus on real-time activity recognition and emergency detection performance.</p><p><xref rid="sensors-25-01266-t002" ref-type="table">Table 2</xref> presents a comparison of local Bi-LSTM models trained on individual edge devices versus the global model obtained through FedAvg. The local models exhibit varying accuracies due to differences in local data distributions, with an average accuracy of 83.9%. Following FedAvg, the global model achieves an improved accuracy of 91.7%, demonstrating the effectiveness of federated learning in enhancing real-time activity recognition. This result underscores the advantage of knowledge aggregation across multiple edge devices while maintaining data privacy, leading to a more generalized and accurate activity recognition model.</p><p><xref rid="sensors-25-01266-t003" ref-type="table">Table 3</xref> tracks the global model&#x02019;s accuracy improvement over 50 global rounds. The accuracy increases from 70.4% in the first round to 91.7% by the 50th round, with rapid early improvements and gradual convergence later. This illustrates the iterative power of federated learning, where continual aggregation of local models enhances global model performance, ensuring scalability and effective monitoring in real-time systems.</p><p><xref rid="sensors-25-01266-f002" ref-type="fig">Figure 2</xref> illustrates the varied training accuracy for each device over 50 iterations, comparing local (dashed lines) and global (solid lines) model performance before and after FedAvg. The local models, trained independently on edge devices, exhibit greater variability due to limited and biased local data. In contrast, the global models, after FedAvg, show more stability and consistently higher accuracy across all devices. This reflects the benefit of federated learning, where the global model aggregates knowledge from multiple devices, improving generalization and mitigating the impact of data heterogeneity, resulting in faster convergence and better overall performance.</p><p><xref rid="sensors-25-01266-f003" ref-type="fig">Figure 3</xref> shows the varied testing accuracy before and after FedAvg for each device over 50 iterations. The local models (dashed lines) exhibit lower and more fluctuating accuracy due to limited and biased local data, starting between 75% and 80%. In contrast, the global models (solid lines) achieve higher, more stable accuracy, reaching around 90% by the 50th iteration. This improvement is attributed to FedAvg, where aggregated knowledge from multiple devices enables better generalization during testing. The stepwise progression of the global models reflects the benefits of federated learning in stabilizing and improving performance across all devices.</p><p><xref rid="sensors-25-01266-t004" ref-type="table">Table 4</xref> presents the precision, recall, and F1-score for real-time activity recognition using local Bi-LSTM models. The system performs well across all activities, achieving an average F1-score of 91.6%. The model demonstrates reliable classification for common activities like walking and running, with minor variations between activity classes, reflecting strong overall performance in real-time monitoring.</p><p><xref rid="sensors-25-01266-t005" ref-type="table">Table 5</xref> evaluates the system&#x02019;s performance in detecting emergencies, including sudden altitude drops and prolonged inactivity. The system achieves an average accuracy of 95.6%, a response time of 1.3 s, and a low false positive rate of 2.8%. By continuously monitoring sensor data and comparing them to predefined thresholds for altitude changes and inactivity, the system ensures timely emergency detection. Calibration during testing minimizes false positives while maintaining high sensitivity. The system&#x02019;s integration of activity recognition with emergency thresholds further enhances reliability, triggering alerts for critical patterns such as combined altitude drops and inactivity.</p><p>The horizontal bar chart in <xref rid="sensors-25-01266-f004" ref-type="fig">Figure 4</xref> visualizes the emergency detection performance across three critical scenarios: Sudden Altitude Drop, Prolonged Inactivity, and Combined Altitude Drop and Inactivity. It compares accuracy, response time, and false positive rate for each scenario. The system achieves consistently high accuracy, ranging from 94.3% to 96.7%, while maintaining a quick response time between 1.2 and 1.5 s. Additionally, the false positive rate remains low (between 2.5% and 3.1%), demonstrating the system&#x02019;s reliability in minimizing false alarms. These results highlight the effectiveness of the proposed federated learning-based system in real-time emergency detection for elderly care. The system detects emergencies with high accuracy and swift response, ensuring the safety and well-being of elderly individuals.</p></sec><sec id="sec4dot3-sensors-25-01266"><title>4.3. Model Compression and Communication&#x000a0;Efficiency</title><p>Model compression is crucial in federated learning to reduce communication overhead between edge devices and the central server. By applying quantization, model parameters are compressed to fewer bits (e.g., 8-bit or 16-bit precision), significantly lowering communication costs. Without compression, the communication cost is given by<disp-formula id="FD37-sensors-25-01266"><label>(37)</label><mml:math id="mm149" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>total</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>params</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mn>32</mml:mn><mml:mspace width="4pt"/><mml:mi>bits</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">K</italic> is the number of devices, and <inline-formula><mml:math id="mm150" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>params</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of parameters. With <italic toggle="yes">b</italic>-bit compression, the cost reduces to<disp-formula id="FD38-sensors-25-01266"><label>(38)</label><mml:math id="mm151" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>compressed</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>params</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi>b</mml:mi><mml:mspace width="4pt"/><mml:mi>bits</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>
yielding a reduction factor of<disp-formula id="FD39-sensors-25-01266"><label>(39)</label><mml:math id="mm152" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>reduction</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mi>b</mml:mi><mml:mn>32</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For instance, 8-bit compression reduces communication costs by 75%. The trade-off between compression and model performance is quantified by the quantization loss<disp-formula id="FD40-sensors-25-01266"><label>(40)</label><mml:math id="mm153" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>compressed</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>global</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm154" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mi>global</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>compressed</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the compressed global model. In applications like elderly care monitoring, compression enhances scalability, reduces network congestion, and maintains near-original model accuracy, ensuring real-time monitoring without network overload.</p><p><xref rid="sensors-25-01266-t006" ref-type="table">Table 6</xref> highlights the impact of model compression on communication costs and global model performance. As compression increases (e.g., 16-bit and 8-bit), the amount of data transferred is significantly reduced, with an 8-bit compression achieving a 75% reduction. The global model accuracy drops slightly from 91.8% (no compression) to 91.2% (8-bit), demonstrating that compression effectively reduces the communication overhead while maintaining strong model performance.</p><p>The visualization in <xref rid="sensors-25-01266-f005" ref-type="fig">Figure 5</xref> illustrates the trade-off between communication cost and global model accuracy in the federated learning system. As compression increases from no compression (32-bit) to 2-bit, the data transferred reduce by over 90%. However, the global model accuracy remains relatively stable, demonstrating that compression optimizes bandwidth usage while preserving high accuracy.</p></sec><sec id="sec4dot4-sensors-25-01266"><title>4.4. Convergence and Communication&#x000a0;Strategies</title><p><xref rid="sensors-25-01266-t007" ref-type="table">Table 7</xref> presents the impact of different communication strategies on the number of global rounds required for model convergence and the final global model accuracy. The model converges fastest with full 32-bit updates in 35 rounds, achieving 91.8% accuracy. With moderate compression (16-bit or 8-bit), the number of rounds increases slightly to 40 and 45, with minimal accuracy loss. However, with more aggressive compression (4-bit or 2-bit), convergence slows, requiring up to 60 rounds, and accuracy declines to 89.7%. These results emphasize the trade-off between reducing communication overhead and maintaining performance.</p><p>The visualization in <xref rid="sensors-25-01266-f006" ref-type="fig">Figure 6</xref> illustrates the trade-off between communication strategies and model convergence. The blue bars represent the number of global rounds needed for convergence, while the green line shows the final accuracy for each strategy. As compression increases from 32-bit to 2-bit, convergence slows from 35 to 60 rounds, but accuracy remains stable, emphasizing the balance between communication efficiency and model performance.</p></sec><sec id="sec4dot5-sensors-25-01266"><title>4.5. Scalability and Network Load&#x000a0;Analysis</title><p><xref rid="sensors-25-01266-t008" ref-type="table">Table 8</xref> evaluates the system&#x02019;s scalability by analyzing the impact of varying numbers of edge devices on communication overhead and model performance. As the number of devices increases from 10 to 200, the total amount of data transferred grows significantly. However, the global model accuracy remains stable, decreasing slightly from 91.8% to 90.8%. Network bandwidth usage scales linearly, demonstrating the system&#x02019;s ability to maintain high accuracy under increasing load.</p><p>The visualization in <xref rid="sensors-25-01266-f007" ref-type="fig">Figure 7</xref> shows that as the number of devices increases, the data transferred and bandwidth usage grow, yet the global model accuracy remains stable, demonstrating efficient scalability.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01266"><title>5. Conclusions and Future&#x000a0;Direction</title><p>This study proposes a federated learning-based system to enhance elderly care by enabling real-time activity recognition, emergency detection, and continuous monitoring of location, altitude, and contextual information. Leveraging Android smartphones equipped with inbuilt IMU sensors&#x02014;accelerometers, gyroscopes, magnetometers, and barometers&#x02014;the system collects real-time sensor data to classify six predefined activities: walking, running, sitting, standing, climbing stairs, and descending stairs. The system employs Bidirectional Long Short-Term Memory (Bi-LSTM) networks for activity recognition, achieving an accuracy of 91.8%. Federated learning, implemented using the FedAvg algorithm, enables privacy-preserving decentralized model training by aggregating local updates on a central server while keeping raw data on edge devices. The framework incorporates model compression techniques, reducing the communication overhead by 75% while maintaining a minimal performance loss of 0.6%. Scalability tests demonstrate the system&#x02019;s ability to handle up to 200 edge devices while maintaining robust performance and efficient bandwidth utilization.</p><p>The system&#x02019;s effectiveness was validated through experiments involving 25 participants, including 9 females and 16 males across three age groups, performing activities in a controlled environment. While the proposed framework demonstrates significant advancements in scalability, privacy, and real-time monitoring, certain challenges remain, such as potential variations in sensor placement, environmental factors affecting altitude estimation, and the energy consumption of federated learning on edge devices. Future work aims to address these limitations by integrating multimodal sensor data, developing energy-efficient model optimization techniques, and expanding real-world deployments to diverse environments for enhanced generalizability.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Data curation, R.A.A.; Formal analysis, F.J.; Funding acquisition, R.A.A.; Investigation, F.J.; Methodology, R.A.A.; Software, F.J.; Supervision, R.A.A.; Validation, R.A.A.; Visualization, F.J.; Writing&#x02014;original draft, R.A.A.; Writing&#x02014;review and editing, R.A.A. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01266"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ometov</surname><given-names>A.</given-names></name>
<name><surname>Shubina</surname><given-names>V.</given-names></name>
<name><surname>Klus</surname><given-names>L.</given-names></name>
<name><surname>Skibi&#x00144;ska</surname><given-names>J.</given-names></name>
<name><surname>Saafi</surname><given-names>S.</given-names></name>
<name><surname>Pascacio</surname><given-names>P.</given-names></name>
<name><surname>Flueratoru</surname><given-names>L.</given-names></name>
<name><surname>Gaibor</surname><given-names>D.Q.</given-names></name>
<name><surname>Chukhno</surname><given-names>N.</given-names></name>
<name><surname>Chukhno</surname><given-names>O.</given-names></name>
<etal/>
</person-group><article-title>A Survey on Wearable Technology: History, State-of-the-Art and Current Challenges</article-title><source>Comput. Netw.</source><year>2021</year><volume>193</volume><fpage>108074</fpage><pub-id pub-id-type="doi">10.1016/j.comnet.2021.108074</pub-id></element-citation></ref><ref id="B2-sensors-25-01266"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Subedi</surname><given-names>S.</given-names></name>
<name><surname>Pyun</surname><given-names>J.Y.</given-names></name>
</person-group><article-title>A Survey of Smartphone-Based Indoor Positioning System Using RF-Based Wireless Technologies</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>7230</elocation-id><pub-id pub-id-type="doi">10.3390/s20247230</pub-id><pub-id pub-id-type="pmid">33348701</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01266"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>D.</given-names></name>
<name><surname>Zhu</surname><given-names>T.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Xiong</surname><given-names>Z.</given-names></name>
</person-group><article-title>LFRSNet: A Robust Light Field Semantic Segmentation Network Combining Contextual and Geometric Features</article-title><source>Front. Environ. Sci.</source><year>2022</year><volume>10</volume><elocation-id>1443</elocation-id><pub-id pub-id-type="doi">10.3389/fenvs.2022.996513</pub-id></element-citation></ref><ref id="B4-sensors-25-01266"><label>4.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Hofmann-Wellenhof</surname><given-names>B.</given-names></name>
<name><surname>Lichtenegger</surname><given-names>H.</given-names></name>
<name><surname>Collins</surname><given-names>J.</given-names></name>
</person-group><source>Global Positioning System: Theory and Practice</source><publisher-name>Springer Science &#x00026; Business Media</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2012</year></element-citation></ref><ref id="B5-sensors-25-01266"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>G.</given-names></name>
<name><surname>Liao</surname><given-names>D.</given-names></name>
<name><surname>Zhao</surname><given-names>D.</given-names></name>
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Yu</surname><given-names>H.</given-names></name>
</person-group><article-title>Live Migration for Multiple Correlated Virtual Machines in Cloud-Based Data Centers</article-title><source>IEEE Trans. Serv. Comput.</source><year>2018</year><volume>11</volume><fpage>279</fpage><lpage>291</lpage><pub-id pub-id-type="doi">10.1109/TSC.2015.2477825</pub-id></element-citation></ref><ref id="B6-sensors-25-01266"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Catania</surname><given-names>P.</given-names></name>
<name><surname>Comparetti</surname><given-names>A.</given-names></name>
<name><surname>Febo</surname><given-names>P.</given-names></name>
<name><surname>Morello</surname><given-names>G.</given-names></name>
<name><surname>Orlando</surname><given-names>S.</given-names></name>
<name><surname>Roma</surname><given-names>E.</given-names></name>
<name><surname>Vallone</surname><given-names>M.</given-names></name>
</person-group><article-title>Positioning Accuracy Comparison of GNSS Receivers Used for Mapping and Guidance of Agricultural Machines</article-title><source>Agronomy</source><year>2020</year><volume>10</volume><elocation-id>924</elocation-id><pub-id pub-id-type="doi">10.3390/agronomy10070924</pub-id></element-citation></ref><ref id="B7-sensors-25-01266"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Roberts</surname><given-names>C.M.</given-names></name>
</person-group><article-title>Radio frequency identification (RFID)</article-title><source>Comput. Secur.</source><year>2006</year><volume>25</volume><fpage>18</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/j.cose.2005.12.003</pub-id></element-citation></ref><ref id="B8-sensors-25-01266"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rong</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Ding</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Gao</surname><given-names>J.</given-names></name>
</person-group><article-title>Du-Bus: A Realtime Bus Waiting Time Estimation System Based On Multi-Source Data</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2022</year><volume>23</volume><fpage>24524</fpage><lpage>24539</lpage><pub-id pub-id-type="doi">10.1109/TITS.2022.3210170</pub-id></element-citation></ref><ref id="B9-sensors-25-01266"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Yan</surname><given-names>Y.</given-names></name>
<name><surname>Shi</surname><given-names>X.</given-names></name>
</person-group><article-title>A dynamic localization network for regional navigation under global navigation satellite system denial environments</article-title><source>Int. J. Distrib. Sens. Netw.</source><year>2019</year><volume>15</volume><fpage>155014771983442</fpage><pub-id pub-id-type="doi">10.1177/1550147719834427</pub-id></element-citation></ref><ref id="B10-sensors-25-01266"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ijaz</surname><given-names>F.</given-names></name>
<name><surname>Yang</surname><given-names>H.K.</given-names></name>
<name><surname>Ahmad</surname><given-names>A.W.</given-names></name>
<name><surname>Lee</surname><given-names>C.</given-names></name>
</person-group><article-title>Indoor positioning: A review of indoor ultrasonic positioning systems</article-title><source>Proceedings of the 2013 15th International Conference on Advanced Communications Technology (ICACT)</source><conf-loc>Pyeongchang, Republic of Korea</conf-loc><conf-date>27&#x02013;30 January 2013</conf-date><fpage>1146</fpage><lpage>1150</lpage></element-citation></ref><ref id="B11-sensors-25-01266"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fard</surname><given-names>H.K.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Son</surname><given-names>K.K.</given-names></name>
</person-group><article-title>Indoor positioning of mobile devices with agile iBeacon deployment</article-title><source>Proceedings of the 2015 IEEE 28th Canadian Conference on Electrical and Computer Engineering (CCECE)</source><conf-loc>Halifax, NS, Canada</conf-loc><conf-date>3&#x02013;6 May 2015</conf-date><fpage>275</fpage><lpage>279</lpage><pub-id pub-id-type="doi">10.1109/CCECE.2015.7129199</pub-id></element-citation></ref><ref id="B12-sensors-25-01266"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zuo</surname><given-names>C.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Yan</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
</person-group><article-title>GUGEN: Global User Graph Enhanced Network for Next POI Recommendation</article-title><source>IEEE Trans. Mob. Comput.</source><year>2024</year><volume>23</volume><fpage>14975</fpage><lpage>14986</lpage><pub-id pub-id-type="doi">10.1109/TMC.2024.3455107</pub-id></element-citation></ref><ref id="B13-sensors-25-01266"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alarifi</surname><given-names>A.</given-names></name>
<name><surname>Al-Salman</surname><given-names>A.</given-names></name>
<name><surname>Alsaleh</surname><given-names>M.</given-names></name>
<name><surname>Alnafessah</surname><given-names>A.</given-names></name>
<name><surname>Al-Hadhrami</surname><given-names>S.</given-names></name>
<name><surname>Al-Ammar</surname><given-names>M.A.</given-names></name>
<name><surname>Al-Khalifa</surname><given-names>H.S.</given-names></name>
</person-group><article-title>Ultra Wideband Indoor Positioning Technologies: Analysis and Recent Advances</article-title><source>Sensors</source><year>2016</year><volume>16</volume><elocation-id>707</elocation-id><pub-id pub-id-type="doi">10.3390/s16050707</pub-id><pub-id pub-id-type="pmid">27196906</pub-id>
</element-citation></ref><ref id="B14-sensors-25-01266"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>P.</given-names></name>
<name><surname>Song</surname><given-names>W.</given-names></name>
<name><surname>Qi</surname><given-names>H.</given-names></name>
<name><surname>Zhou</surname><given-names>C.</given-names></name>
<name><surname>Li</surname><given-names>F.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Q.</given-names></name>
</person-group><article-title>Server-Initiated Federated Unlearning to Eliminate Impacts of Low-Quality Data</article-title><source>IEEE Trans. Serv. Comput.</source><year>2024</year><volume>17</volume><fpage>1196</fpage><lpage>1211</lpage><pub-id pub-id-type="doi">10.1109/TSC.2024.3355188</pub-id></element-citation></ref><ref id="B15-sensors-25-01266"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>G.</given-names></name>
<name><surname>Kong</surname><given-names>D.</given-names></name>
<name><surname>Zhang</surname><given-names>K.</given-names></name>
<name><surname>Xu</surname><given-names>S.</given-names></name>
<name><surname>Cao</surname><given-names>Y.</given-names></name>
<name><surname>Mao</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
</person-group><article-title>A Model Value Transfer Incentive Mechanism for Federated Learning With Smart Contracts in AIoT</article-title><source>IEEE Internet Things J.</source><year>2024</year><volume>12</volume><fpage>2530</fpage><lpage>2544</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2024.3468443</pub-id></element-citation></ref><ref id="B16-sensors-25-01266"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Xia</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>T.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Cui</surname><given-names>P.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
</person-group><article-title>GRASS: Learning Spatial-Temporal Properties From Chainlike Cascade Data for Microscopic Diffusion Prediction</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2024</year><volume>35</volume><fpage>16313</fpage><lpage>16327</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2023.3293689</pub-id></element-citation></ref><ref id="B17-sensors-25-01266"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>L.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>N.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Multiscale Spatio-Temporal Feature Fusion Based Non-Intrusive Appliance Load Monitoring for Multiple Industrial Industries</article-title><source>Appl. Soft Comput.</source><year>2024</year><volume>167</volume><fpage>112445</fpage><pub-id pub-id-type="doi">10.1016/j.asoc.2024.112445</pub-id></element-citation></ref><ref id="B18-sensors-25-01266"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>L.</given-names></name>
<name><surname>Ma</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>C.</given-names></name>
<name><surname>Xu</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>N.</given-names></name>
</person-group><article-title>Imbalanced Industrial Load Identification Based on Optimized CatBoost with Entropy Features</article-title><source>J. Electr. Eng. Technol.</source><year>2024</year><volume>19</volume><fpage>4817</fpage><lpage>4832</lpage><pub-id pub-id-type="doi">10.1007/s42835-024-01933-5</pub-id></element-citation></ref><ref id="B19-sensors-25-01266"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jian Yin</surname><given-names>L.</given-names></name>
</person-group><article-title>A New Distance Vector-Hop Localization Algorithm Based on Half-Measure Weighted Centroid</article-title><source>Mob. Inf. Syst.</source><year>2019</year><volume>2019</volume><fpage>e9892512</fpage><pub-id pub-id-type="doi">10.1155/2019/9892512</pub-id></element-citation></ref><ref id="B20-sensors-25-01266"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shi</surname><given-names>Y.</given-names></name>
<name><surname>Shi</surname><given-names>W.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Xiao</surname><given-names>X.</given-names></name>
</person-group><article-title>An RSSI Classification and Tracing Algorithm to Improve Trilateration-Based Positioning</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>4244</elocation-id><pub-id pub-id-type="doi">10.3390/s20154244</pub-id><pub-id pub-id-type="pmid">32751485</pub-id>
</element-citation></ref><ref id="B21-sensors-25-01266"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qiu</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Qin</surname><given-names>K.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Hu</surname><given-names>H.</given-names></name>
</person-group><article-title>Inertial/magnetic sensors based pedestrian dead reckoning by means of multi-sensor fusion</article-title><source>Inf. Fusion</source><year>2018</year><volume>39</volume><fpage>108</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1016/j.inffus.2017.04.006</pub-id></element-citation></ref><ref id="B22-sensors-25-01266"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Townsend</surname><given-names>K.</given-names></name>
<name><surname>Cuf&#x000ed;</surname><given-names>C.</given-names></name>
<name><surname>Akiba</surname></name>
<name><surname>Davidson</surname><given-names>R.</given-names></name>
</person-group><source>Getting Started with Bluetooth Low Energy: Tools and Techniques for Low-Power Networking</source><publisher-name>O&#x02019;Reilly Media, Inc.</publisher-name><publisher-loc>Sebastopol, CA, USA</publisher-loc><year>2014</year></element-citation></ref><ref id="B23-sensors-25-01266"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zuhdiyanto</surname><given-names>D.R.O.</given-names></name>
<name><surname>Asriningtias</surname><given-names>Y.</given-names></name>
</person-group><article-title>Real-Time Location Monitoring and Routine Reminders Based on Internet of Things Integrated with Mobile for Dementia Disorder</article-title><source>J. RESTI (Rekayasa Sist. Teknol. Inf.)</source><year>2025</year><volume>9</volume><fpage>77</fpage><lpage>84</lpage></element-citation></ref><ref id="B24-sensors-25-01266"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gkiolnta</surname><given-names>E.</given-names></name>
<name><surname>Roy</surname><given-names>D.</given-names></name>
<name><surname>Fragulis</surname><given-names>G.F.</given-names></name>
</person-group><article-title>Challenges and Ethical Considerations in Implementing Assistive Technologies in Healthcare</article-title><source>Technologies</source><year>2025</year><volume>13</volume><elocation-id>48</elocation-id><pub-id pub-id-type="doi">10.3390/technologies13020048</pub-id></element-citation></ref><ref id="B25-sensors-25-01266"><label>25.</label><element-citation publication-type="other"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Zheng</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Hao</surname><given-names>A.</given-names></name>
<name><surname>Hou</surname><given-names>X.</given-names></name>
</person-group><article-title>TalkingStyle: Personalized Speech-Driven 3D Facial Animation with Style Preservation</article-title><source>IEEE Trans. Vis. Comput. Graph.</source><year>2024</year><comment>
<italic toggle="yes">early access</italic>
</comment><pub-id pub-id-type="doi">10.1109/TVCG.2024.3409568</pub-id></element-citation></ref><ref id="B26-sensors-25-01266"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>T.</given-names></name>
<name><surname>Hui</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Hui</surname><given-names>P.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group><article-title>Mobile User Traffic Generation Via Multi-Scale Hierarchical GAN</article-title><source>ACM Trans. Knowl. Discov. Data</source><year>2024</year><volume>18</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1145/3664655</pub-id></element-citation></ref><ref id="B27-sensors-25-01266"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Wei</surname><given-names>E.</given-names></name>
<name><surname>Berry</surname><given-names>R.</given-names></name>
<name><surname>Huang</surname><given-names>J.</given-names></name>
</person-group><article-title>Age-Dependent Differential Privacy</article-title><source>IEEE Trans. Inf. Theory</source><year>2024</year><volume>70</volume><fpage>1300</fpage><lpage>1319</lpage><pub-id pub-id-type="doi">10.1109/TIT.2023.3340147</pub-id></element-citation></ref><ref id="B28-sensors-25-01266"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Bai</surname><given-names>L.</given-names></name>
<name><surname>Fang</surname><given-names>Z.</given-names></name>
<name><surname>Han</surname><given-names>R.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Choi</surname><given-names>J.</given-names></name>
</person-group><article-title>Age of Information Based URLLC Transmission for UAVs on Pylon Turn</article-title><source>IEEE Trans. Veh. Technol.</source><year>2024</year><volume>73</volume><fpage>8797</fpage><lpage>8809</lpage><pub-id pub-id-type="doi">10.1109/TVT.2024.3358844</pub-id></element-citation></ref><ref id="B29-sensors-25-01266"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bai</surname><given-names>L.</given-names></name>
<name><surname>Han</surname><given-names>P.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
</person-group><article-title>Throughput Maximization for Multipath Secure Transmission in Wireless Ad-Hoc Networks</article-title><source>IEEE Trans. Commun.</source><year>2024</year><volume>72</volume><fpage>6810</fpage><lpage>6821</lpage><pub-id pub-id-type="doi">10.1109/TCOMM.2024.3409539</pub-id></element-citation></ref><ref id="B30-sensors-25-01266"><label>30.</label><element-citation publication-type="other"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Hou</surname><given-names>D.</given-names></name>
<name><surname>Xiong</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group><article-title>EALLR: Energy-Aware Low-Latency Routing Data Driven Model in Mobile Edge Computing</article-title><source>IEEE Trans. Consum. Electron.</source><year>2024</year><comment>
<italic toggle="yes">early access</italic>
</comment><pub-id pub-id-type="doi">10.1109/TCE.2024.3507158</pub-id></element-citation></ref><ref id="B31-sensors-25-01266"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Jiang</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Hao</surname><given-names>A.</given-names></name>
<name><surname>Hou</surname><given-names>X.</given-names></name>
<name><surname>Qin</surname><given-names>H.</given-names></name>
</person-group><article-title>Expressive 3D Facial Animation Generation Based on Local-to-Global Latent Diffusion</article-title><source>IEEE Trans. Vis. Comput. Graph.</source><year>2024</year><volume>30</volume><fpage>7397</fpage><lpage>7407</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2024.3456213</pub-id><pub-id pub-id-type="pmid">39255115</pub-id>
</element-citation></ref><ref id="B32-sensors-25-01266"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>C.</given-names></name>
</person-group><article-title>Research on Emotion Recognition-Based Smart Assistant System: Emotional Intelligence and Personalized Services</article-title><source>J. Syst. Manag. Sci.</source><year>2023</year><volume>13</volume><fpage>227</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.33168/JSMS.2023.0515</pub-id></element-citation></ref><ref id="B33-sensors-25-01266"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Xie</surname><given-names>K.</given-names></name>
<name><surname>Wu</surname><given-names>T.</given-names></name>
<name><surname>Ma</surname><given-names>C.</given-names></name>
<name><surname>Ma</surname><given-names>T.</given-names></name>
</person-group><article-title>Distributed Neural Tensor Completion for Network Monitoring Data Recovery</article-title><source>Inf. Sci.</source><year>2024</year><volume>662</volume><fpage>120259</fpage><pub-id pub-id-type="doi">10.1016/j.ins.2024.120259</pub-id></element-citation></ref><ref id="B34-sensors-25-01266"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Marques</surname><given-names>J.P.P.</given-names></name>
<name><surname>Cunha</surname><given-names>D.C.</given-names></name>
<name><surname>Harada</surname><given-names>L.M.</given-names></name>
<name><surname>Silva</surname><given-names>L.N.</given-names></name>
<name><surname>Silva</surname><given-names>I.D.</given-names></name>
</person-group><article-title>A cost-effective trilateration-based radio localization algorithm using machine learning and sequential least-square programming optimization</article-title><source>Comput. Commun.</source><year>2021</year><volume>177</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.comcom.2021.06.005</pub-id></element-citation></ref><ref id="B35-sensors-25-01266"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Deng</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Qi</surname><given-names>H.</given-names></name>
</person-group><article-title>Toward Smart Multizone HVAC Control by Combining Context-Aware System and Deep Reinforcement Learning</article-title><source>IEEE Internet Things J.</source><year>2022</year><volume>9</volume><fpage>21010</fpage><lpage>21024</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2022.3175728</pub-id></element-citation></ref><ref id="B36-sensors-25-01266"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>Q.</given-names></name>
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Sun</surname><given-names>R.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Weng</surname><given-names>D.</given-names></name>
</person-group><article-title>RANSAC-based Instantaneous Real-Time Kinematic Positioning with GNSS Triple-Frequency Signals in Urban Areas</article-title><source>J. Geod.</source><year>2024</year><volume>98</volume><fpage>24</fpage><pub-id pub-id-type="doi">10.1007/s00190-024-01833-6</pub-id></element-citation></ref><ref id="B37-sensors-25-01266"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>T.</given-names></name>
<name><surname>Long</surname><given-names>Q.</given-names></name>
<name><surname>Chai</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Jiang</surname><given-names>F.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group><article-title>Generative AI Empowered Network Digital Twins: Architecture, Technologies, and Applications</article-title><source>ACM Comput. Surv.</source><year>2025</year><fpage>accepted</fpage><pub-id pub-id-type="doi">10.1145/3711682</pub-id></element-citation></ref><ref id="B38-sensors-25-01266"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gu</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Lu</surname><given-names>P.</given-names></name>
<name><surname>Lan</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
</person-group><article-title>SiMaLSTM-SNP: Novel Semantic Relatedness Learning Model Preserving Both Siamese Networks and Membrane Computing</article-title><source>J. Supercomput.</source><year>2024</year><volume>80</volume><fpage>3382</fpage><lpage>3411</lpage><pub-id pub-id-type="doi">10.1007/s11227-023-05592-7</pub-id></element-citation></ref><ref id="B39-sensors-25-01266"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ding</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Lu</surname><given-names>P.</given-names></name>
<name><surname>Yang</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
</person-group><article-title>DialogueINAB: An Interaction Neural Network Based on Attitudes and Behaviors of Interlocutors for Dialogue Emotion Recognition</article-title><source>J. Supercomput.</source><year>2023</year><volume>79</volume><fpage>20481</fpage><lpage>20514</lpage><pub-id pub-id-type="doi">10.1007/s11227-023-05439-1</pub-id></element-citation></ref><ref id="B40-sensors-25-01266"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamil</surname><given-names>F.</given-names></name>
<name><surname>Kim</surname><given-names>D.</given-names></name>
</person-group><article-title>Enhanced Kalman filter algorithm using fuzzy inference for improving position estimation in indoor navigation</article-title><source>J. Intell. Fuzzy Syst.</source><year>2021</year><volume>40</volume><fpage>8991</fpage><lpage>9005</lpage><pub-id pub-id-type="doi">10.3233/JIFS-201352</pub-id></element-citation></ref><ref id="B41-sensors-25-01266"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lan</surname><given-names>K.C.</given-names></name>
<name><surname>Shih</surname><given-names>W.Y.</given-names></name>
</person-group><article-title>On Calibrating the Sensor Errors of a PDR-Based Indoor Localization System</article-title><source>Sensors</source><year>2013</year><volume>13</volume><fpage>4781</fpage><lpage>4810</lpage><pub-id pub-id-type="doi">10.3390/s130404781</pub-id><pub-id pub-id-type="pmid">23575036</pub-id>
</element-citation></ref><ref id="B42-sensors-25-01266"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bao</surname><given-names>H.</given-names></name>
<name><surname>Wong</surname><given-names>W.C.</given-names></name>
</person-group><article-title>A Novel Map-Based Dead-Reckoning Algorithm for Indoor Localization</article-title><source>J. Sens. Actuator Netw.</source><year>2014</year><volume>3</volume><fpage>44</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.3390/jsan3010044</pub-id></element-citation></ref><ref id="B43-sensors-25-01266"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mikov</surname><given-names>A.</given-names></name>
<name><surname>Moschevikin</surname><given-names>A.</given-names></name>
<name><surname>Fedorov</surname><given-names>A.</given-names></name>
<name><surname>Sikora</surname><given-names>A.</given-names></name>
</person-group><article-title>A localization system using inertial measurement units from wireless commercial hand-held devices</article-title><source>Proceedings of the International Conference on Indoor Positioning and Indoor Navigation</source><conf-loc>Montbeliard, France</conf-loc><conf-date>28&#x02013;31 October 2013</conf-date><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1109/IPIN.2013.6817924</pub-id></element-citation></ref><ref id="B44-sensors-25-01266"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tian</surname><given-names>Q.</given-names></name>
<name><surname>Salcic</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>K.I.K.</given-names></name>
<name><surname>Pan</surname><given-names>Y.</given-names></name>
</person-group><article-title>An enhanced pedestrian dead reckoning approach for pedestrian tracking using smartphones</article-title><source>Proceedings of the 2015 IEEE Tenth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)</source><conf-loc>Singapore</conf-loc><conf-date>7&#x02013;9 April 2015</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/ISSNIP.2015.7106923</pub-id></element-citation></ref><ref id="B45-sensors-25-01266"><label>45.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>R.</given-names></name>
<name><surname>Xia</surname><given-names>W.</given-names></name>
<name><surname>Jia</surname><given-names>Z.</given-names></name>
<name><surname>Shen</surname><given-names>L.</given-names></name>
</person-group><article-title>The indoor localization method based on the integration of RSSI and inertial sensor</article-title><source>Proceedings of the 2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE)</source><conf-loc>Tokyo, Japan</conf-loc><conf-date>7&#x02013;10 October 2014</conf-date><fpage>332</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1109/GCCE.2014.7031256</pub-id></element-citation></ref><ref id="B46-sensors-25-01266"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jimenez Ruiz</surname><given-names>A.R.</given-names></name>
<name><surname>Seco Granja</surname><given-names>F.</given-names></name>
<name><surname>Prieto Honorato</surname><given-names>J.C.</given-names></name>
<name><surname>Guevara Rosas</surname><given-names>J.I.</given-names></name>
</person-group><article-title>Accurate Pedestrian Indoor Navigation by Tightly Coupling Foot-Mounted IMU and RFID Measurements</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2012</year><volume>61</volume><fpage>178</fpage><lpage>189</lpage><pub-id pub-id-type="doi">10.1109/TIM.2011.2159317</pub-id></element-citation></ref><ref id="B47-sensors-25-01266"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chirakkal</surname><given-names>V.V.</given-names></name>
<name><surname>Park</surname><given-names>M.</given-names></name>
<name><surname>Han</surname><given-names>D.S.</given-names></name>
</person-group><article-title>Exploring Smartphone-Based Indoor Navigation: A QR Code Assistance-Based Approach</article-title><source>IEIE Trans. Smart Process. Comput.</source><year>2015</year><volume>4</volume><fpage>173</fpage><lpage>182</lpage><pub-id pub-id-type="doi">10.5573/IEIESPC.2015.4.3.173</pub-id></element-citation></ref><ref id="B48-sensors-25-01266"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Meliones</surname><given-names>A.</given-names></name>
<name><surname>Sampson</surname><given-names>D.</given-names></name>
</person-group><article-title>Blind MuseumTourer: A System for Self-Guided Tours in Museums and Blind Indoor Navigation</article-title><source>Technologies</source><year>2018</year><volume>6</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3390/technologies6010004</pub-id></element-citation></ref><ref id="B49-sensors-25-01266"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
<name><surname>Zheng</surname><given-names>X.</given-names></name>
<name><surname>Chen</surname><given-names>R.</given-names></name>
<name><surname>Xiong</surname><given-names>H.</given-names></name>
<name><surname>Guo</surname><given-names>S.</given-names></name>
</person-group><article-title>Image-Based Localization Aided Indoor Pedestrian Trajectory Estimation Using Smartphones</article-title><source>Sensors</source><year>2018</year><volume>18</volume><elocation-id>258</elocation-id><pub-id pub-id-type="doi">10.3390/s18010258</pub-id><pub-id pub-id-type="pmid">29342123</pub-id>
</element-citation></ref><ref id="B50-sensors-25-01266"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Al-Madani</surname><given-names>B.</given-names></name>
<name><surname>Orujov</surname><given-names>F.</given-names></name>
<name><surname>Maskeliunas</surname><given-names>R.</given-names></name>
<name><surname>Damasevicius</surname><given-names>R.</given-names></name>
<name><surname>Ven&#x0010d;kauskas</surname><given-names>A.</given-names></name>
</person-group><article-title>Fuzzy Logic Type-2 Based Wireless Indoor Localization System for Navigation of Visually Impaired People in Buildings</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>2114</elocation-id><pub-id pub-id-type="doi">10.3390/s19092114</pub-id><pub-id pub-id-type="pmid">31067769</pub-id>
</element-citation></ref><ref id="B51-sensors-25-01266"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Ahmad</surname><given-names>N.S.</given-names></name>
</person-group><article-title>A Comprehensive Review on Sensor Fusion Techniques for Localization of a Dynamic Target in GPS-Denied Environments</article-title><source>IEEE Access</source><year>2024</year><volume>13</volume><fpage>2252</fpage><lpage>2285</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3519874</pub-id></element-citation></ref><ref id="B52-sensors-25-01266"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Skog</surname><given-names>I.</given-names></name>
<name><surname>Handel</surname><given-names>P.</given-names></name>
<name><surname>Nilsson</surname><given-names>J.O.</given-names></name>
<name><surname>Rantakokko</surname><given-names>J.</given-names></name>
</person-group><article-title>Zero-Velocity Detection&#x02014;An Algorithm Evaluation</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2010</year><volume>57</volume><fpage>2657</fpage><lpage>2666</lpage><pub-id pub-id-type="doi">10.1109/TBME.2010.2060723</pub-id></element-citation></ref><ref id="B53-sensors-25-01266"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nascimento</surname><given-names>L.M.S.d.</given-names></name>
<name><surname>Bonfati</surname><given-names>L.V.</given-names></name>
<name><surname>Freitas</surname><given-names>M.L.B.</given-names></name>
<name><surname>Mendes Junior</surname><given-names>J.J.A.</given-names></name>
<name><surname>Siqueira</surname><given-names>H.V.</given-names></name>
<name><surname>Stevan</surname><given-names>S.L.</given-names><suffix>Jr.</suffix></name>
</person-group><article-title>Sensors and systems for physical rehabilitation and health monitoring&#x02014;A review</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>4063</elocation-id><pub-id pub-id-type="doi">10.3390/s20154063</pub-id><pub-id pub-id-type="pmid">32707749</pub-id>
</element-citation></ref><ref id="B54-sensors-25-01266"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Trabelsi</surname><given-names>D.</given-names></name>
<name><surname>Mohammed</surname><given-names>S.</given-names></name>
<name><surname>Chamroukhi</surname><given-names>F.</given-names></name>
<name><surname>Oukhellou</surname><given-names>L.</given-names></name>
<name><surname>Amirat</surname><given-names>Y.</given-names></name>
</person-group><article-title>An Unsupervised Approach for Automatic Activity Recognition Based on Hidden Markov Model Regression</article-title><source>IEEE Trans. Autom. Sci. Eng.</source><year>2013</year><volume>10</volume><fpage>829</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1109/TASE.2013.2256349</pub-id></element-citation></ref><ref id="B55-sensors-25-01266"><label>55.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Seitz</surname><given-names>J.</given-names></name>
<name><surname>Vaupel</surname><given-names>T.</given-names></name>
<name><surname>Meyer</surname><given-names>S.</given-names></name>
<name><surname>Boronat</surname><given-names>J.G.</given-names></name>
<name><surname>Thielecke</surname><given-names>J.</given-names></name>
</person-group><article-title>A Hidden Markov Model for pedestrian navigation</article-title><source>Proceedings of the Navigation and Communication 2010 7th Workshop on Positioning</source><conf-loc>Dresden, Germany</conf-loc><conf-date>11&#x02013;12 March 2010</conf-date><fpage>120</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1109/WPNC.2010.5650501</pub-id></element-citation></ref><ref id="B56-sensors-25-01266"><label>56.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ryu</surname><given-names>S.J.</given-names></name>
<name><surname>Kim</surname><given-names>J.H.</given-names></name>
</person-group><article-title>Classification of long-term motions using a two-layered hidden Markov model in a wearable sensor system</article-title><source>Proceedings of the 2011 IEEE International Conference on Robotics and Biomimetics</source><conf-loc>Phuket, Thailand</conf-loc><conf-date>7&#x02013;11 December 2011</conf-date><fpage>2975</fpage><lpage>2980</lpage><pub-id pub-id-type="doi">10.1109/ROBIO.2011.6181758</pub-id></element-citation></ref><ref id="B57-sensors-25-01266"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ahmad</surname><given-names>N.</given-names></name>
<name><surname>Han</surname><given-names>L.</given-names></name>
<name><surname>Iqbal</surname><given-names>K.</given-names></name>
<name><surname>Ahmad</surname><given-names>R.</given-names></name>
<name><surname>Abid</surname><given-names>M.A.</given-names></name>
<name><surname>Iqbal</surname><given-names>N.</given-names></name>
</person-group><article-title>SARM: Salah activities recognition model based on smartphone</article-title><source>Electronics</source><year>2019</year><volume>8</volume><elocation-id>881</elocation-id><pub-id pub-id-type="doi">10.3390/electronics8080881</pub-id></element-citation></ref><ref id="B58-sensors-25-01266"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ahmed</surname><given-names>S.</given-names></name>
<name><surname>Irfan</surname><given-names>S.</given-names></name>
<name><surname>Kiran</surname><given-names>N.</given-names></name>
<name><surname>Masood</surname><given-names>N.</given-names></name>
<name><surname>Anjum</surname><given-names>N.</given-names></name>
<name><surname>Ramzan</surname><given-names>N.</given-names></name>
</person-group><article-title>Remote health monitoring systems for elderly people: A survey</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>7095</elocation-id><pub-id pub-id-type="doi">10.3390/s23167095</pub-id><pub-id pub-id-type="pmid">37631632</pub-id>
</element-citation></ref><ref id="B59-sensors-25-01266"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aouedi</surname><given-names>O.</given-names></name>
<name><surname>Sacco</surname><given-names>A.</given-names></name>
<name><surname>Khan</surname><given-names>L.U.</given-names></name>
<name><surname>Nguyen</surname><given-names>D.C.</given-names></name>
<name><surname>Guizani</surname><given-names>M.</given-names></name>
</person-group><article-title>Federated Learning for Human Activity Recognition: Overview, Advances, and Challenges</article-title><source>IEEE Open J. Commun. Soc.</source><year>2024</year><volume>5</volume><fpage>7341</fpage><lpage>7367</lpage><pub-id pub-id-type="doi">10.1109/OJCOMS.2024.3484228</pub-id></element-citation></ref><ref id="B60-sensors-25-01266"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Horta-Vel&#x000e1;zquez</surname><given-names>A.</given-names></name>
<name><surname>Ramos-Ortiz</surname><given-names>G.</given-names></name>
<name><surname>Morales-Narv&#x000e1;ez</surname><given-names>E.</given-names></name>
</person-group><article-title>The optimal color space enables advantageous smartphone-based colorimetric sensing</article-title><source>Biosens. Bioelectron.</source><year>2025</year><volume>273</volume><elocation-id>117089</elocation-id><pub-id pub-id-type="doi">10.1016/j.bios.2024.117089</pub-id><pub-id pub-id-type="pmid">39818181</pub-id>
</element-citation></ref><ref id="B61-sensors-25-01266"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ngoupayou Limbepe</surname><given-names>Z.</given-names></name>
<name><surname>Gai</surname><given-names>K.</given-names></name>
<name><surname>Yu</surname><given-names>J.</given-names></name>
</person-group><article-title>Blockchain-Based Privacy-Enhancing Federated Learning in Smart Healthcare: A Survey</article-title><source>Blockchains</source><year>2025</year><volume>3</volume><elocation-id>1</elocation-id><pub-id pub-id-type="doi">10.3390/blockchains3010001</pub-id></element-citation></ref><ref id="B62-sensors-25-01266"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamil</surname><given-names>F.</given-names></name>
<name><surname>Kim</surname><given-names>D.H.</given-names></name>
</person-group><article-title>Improving Accuracy of the Alpha&#x02013;Beta Filter Algorithm Using an ANN-Based Learning Mechanism in Indoor Navigation System</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>3946</elocation-id><pub-id pub-id-type="doi">10.3390/s19183946</pub-id><pub-id pub-id-type="pmid">31547395</pub-id>
</element-citation></ref><ref id="B63-sensors-25-01266"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamil</surname><given-names>F.</given-names></name>
<name><surname>Iqbal</surname><given-names>N.</given-names></name>
<name><surname>Ahmad</surname><given-names>S.</given-names></name>
<name><surname>Kim</surname><given-names>D.H.</given-names></name>
</person-group><article-title>Toward Accurate Position Estimation Using Learning to Prediction Algorithm in Indoor Navigation</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>4410</elocation-id><pub-id pub-id-type="doi">10.3390/s20164410</pub-id><pub-id pub-id-type="pmid">32784667</pub-id>
</element-citation></ref><ref id="B64-sensors-25-01266"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamil</surname><given-names>H.</given-names></name>
<name><surname>Qayyum</surname><given-names>F.</given-names></name>
<name><surname>Jamil</surname><given-names>F.</given-names></name>
<name><surname>Kim</surname><given-names>D.H.</given-names></name>
</person-group><article-title>Enhanced pdr-ble compensation mechanism based on hmm and awcla for improving indoor localization</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>6972</elocation-id><pub-id pub-id-type="doi">10.3390/s21216972</pub-id><pub-id pub-id-type="pmid">34770279</pub-id>
</element-citation></ref><ref id="B65-sensors-25-01266"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamil</surname><given-names>H.</given-names></name>
<name><surname>Qayyum</surname><given-names>F.</given-names></name>
<name><surname>Iqbal</surname><given-names>N.</given-names></name>
<name><surname>Jamil</surname><given-names>F.</given-names></name>
<name><surname>Kim</surname><given-names>D.H.</given-names></name>
</person-group><article-title>Optimal ensemble scheme for human activity recognition and floor detection based on AutoML and weighted soft voting using smartphone sensors</article-title><source>IEEE Sens. J.</source><year>2022</year><volume>23</volume><fpage>2878</fpage><lpage>2890</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3228120</pub-id></element-citation></ref><ref id="B66-sensors-25-01266"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamil</surname><given-names>H.</given-names></name>
<name><surname>Kim</surname><given-names>D.H.</given-names></name>
</person-group><article-title>Optimal fusion-based localization method for tracking of smartphone user in tall complex buildings</article-title><source>CAAI Trans. Intell. Technol.</source><year>2023</year><volume>8</volume><fpage>1104</fpage><lpage>1123</lpage><pub-id pub-id-type="doi">10.1049/cit2.12262</pub-id></element-citation></ref><ref id="B67-sensors-25-01266"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamil</surname><given-names>H.</given-names></name>
<name><surname>Jian</surname><given-names>Y.</given-names></name>
</person-group><article-title>An Evolutionary Enhance Particle Filter Based Fusion Localization Scheme For Fast Tracking Of Smartphone Users In Tall Complex Buildings For Hazardous Situations</article-title><source>IEEE Sens. J.</source><year>2024</year><volume>24</volume><fpage>6799</fpage><lpage>6812</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2024.3352599</pub-id></element-citation></ref><ref id="B68-sensors-25-01266"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamil</surname><given-names>H.</given-names></name>
<name><surname>Khan</surname><given-names>M.A.</given-names></name>
<name><surname>Jamil</surname><given-names>F.</given-names></name>
</person-group><article-title>A novel hybrid strategy based on Swarm and Heterogeneous Federated Learning using model credibility awareness for activity recognition in cross-silo multistorey building</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>138</volume><fpage>109126</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.109126</pub-id></element-citation></ref><ref id="B69-sensors-25-01266"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamil</surname><given-names>H.</given-names></name>
<name><surname>Jian</surname><given-names>Y.</given-names></name>
<name><surname>Jamil</surname><given-names>F.</given-names></name>
<name><surname>Ahmad</surname><given-names>S.</given-names></name>
</person-group><article-title>Swarm Learning Empowered Federated Deep Learning for Seamless Smartphone-Based Activity Recognition</article-title><source>IEEE Trans. Consum. Electron.</source><year>2024</year><volume>70</volume><fpage>6919</fpage><lpage>6935</lpage><pub-id pub-id-type="doi">10.1109/TCE.2024.3479078</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01266-f001"><label>Figure 1</label><caption><p>Federated learning-based elderly monitoring system architecture.</p></caption><graphic xlink:href="sensors-25-01266-g001" position="float"/></fig><fig position="float" id="sensors-25-01266-f002"><label>Figure 2</label><caption><p>Varied training accuracy before and after FedAvg for each device (50 iterations).</p></caption><graphic xlink:href="sensors-25-01266-g002" position="float"/></fig><fig position="float" id="sensors-25-01266-f003"><label>Figure 3</label><caption><p>Varied testing accuracy before and after FedAvg for each device (50 iterations).</p></caption><graphic xlink:href="sensors-25-01266-g003" position="float"/></fig><fig position="float" id="sensors-25-01266-f004"><label>Figure 4</label><caption><p>Emergency detection performance.</p></caption><graphic xlink:href="sensors-25-01266-g004" position="float"/></fig><fig position="float" id="sensors-25-01266-f005"><label>Figure 5</label><caption><p>Impact of model compression on communication cost and global model performance.</p></caption><graphic xlink:href="sensors-25-01266-g005" position="float"/></fig><fig position="float" id="sensors-25-01266-f006"><label>Figure 6</label><caption><p>Convergence and model updates with different communication strategies.</p></caption><graphic xlink:href="sensors-25-01266-g006" position="float"/></fig><fig position="float" id="sensors-25-01266-f007"><label>Figure 7</label><caption><p>Scalability and network load analysis.</p></caption><graphic xlink:href="sensors-25-01266-g007" position="float"/></fig><table-wrap position="float" id="sensors-25-01266-t001"><object-id pub-id-type="pii">sensors-25-01266-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparative analysis of existing positioning&#x000a0;techniques.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Sensors</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Technique</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Environment</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Max Distance</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Error (m)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Achieved Accuracy</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gyro, Acc&#x000a0;[<xref rid="B41-sensors-25-01266" ref-type="bibr">41</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zero velocity update, map&#x000a0;matching</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensor on waist</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.683 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.26%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mag, Acc&#x000a0;[<xref rid="B42-sensors-25-01266" ref-type="bibr">42</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PDR, map matching</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensor in pocket</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">104 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(0.55&#x02013;0.93) m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ave LE (0.55&#x02013;0.93 m)</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc, Gyro&#x000a0;[<xref rid="B43-sensors-25-01266" ref-type="bibr">43</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Quaternion complementary&#x000a0;filter</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smartphone in trousers/jacket/hand</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">270 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.529 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Above 98%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMU&#x000a0;[<xref rid="B62-sensors-25-01266" ref-type="bibr">62</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning-based prediction</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NGIMU sensor on body</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0223c;50 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.102 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Above 98.7%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMU&#x000a0;[<xref rid="B63-sensors-25-01266" ref-type="bibr">63</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ANN and KF prediction</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Noisy sensor measurements</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0223c;50 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.009 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Above 99%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc, Gyro&#x000a0;[<xref rid="B44-sensors-25-01266" ref-type="bibr">44</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Model classification</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smartphone in hand/pocket while walking</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">168.55 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.31 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ave LE, 1.35 m</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc, Gyro, Wi-Fi&#x000a0;[<xref rid="B45-sensors-25-01266" ref-type="bibr">45</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zigbee RSSI fusion with EKF and PDR</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Zigbee and IMU sensor on&#x000a0;waist</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max LE, 4 m</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc, Gyro, Mag, RFI&#x000a0;[<xref rid="B46-sensors-25-01266" ref-type="bibr">46</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RFID RSSI fusion with EKF and PDR</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMU on foot, RFID tags in&#x000a0;rooms</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1000 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.721 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ave LE, 98.73%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acc, Gyro&#x000a0;[<xref rid="B47-sensors-25-01266" ref-type="bibr">47</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Assistive QR code with&#x000a0;PDR</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">QR codes along path, smartphone in hand</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Above 99%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMU, BLE beacon&#x000a0;[<xref rid="B48-sensors-25-01266" ref-type="bibr">48</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BLE beacon, inertial dead&#x000a0;reckoning</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Indoor environment</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Above 97.47%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMU, Camera&#x000a0;[<xref rid="B49-sensors-25-01266" ref-type="bibr">49</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PDR, camera-based&#x000a0;tracking</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Meeting room</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.56 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">BLE beacon&#x000a0;[<xref rid="B50-sensors-25-01266" ref-type="bibr">50</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fuzzy logic, BLE fingerprinting</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Indoor environment</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.43 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Proposed Model</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Federated Learning with IMU sensors</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smartphone-based monitoring</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dynamic</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0003c;0.1&#x000a0;m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Above 99%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01266-t002"><object-id pub-id-type="pii">sensors-25-01266-t002_Table 2</object-id><label>Table 2</label><caption><p>Model accuracy evaluation (local Bi-LSTM vs. global model with FedAvg).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Edge Device</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Local Model Accuracy (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Global Model Accuracy After FedAvg (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Device 1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Device 2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Device 3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Device 4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Device 5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.7</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01266-t003"><object-id pub-id-type="pii">sensors-25-01266-t003_Table 3</object-id><label>Table 3</label><caption><p>Accuracy improvements after each global round.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Global Round</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Global Model Accuracy (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70.4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">78.9</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.7</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01266-t004"><object-id pub-id-type="pii">sensors-25-01266-t004_Table 4</object-id><label>Table 4</label><caption><p>Real-time activity recognition performance (local Bi-LSTM models).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Activity Class</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1-Score (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Walking</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sitting</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.1</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Running</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Standing</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Walking Upstairs</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.7</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Walking Downstairs</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.6</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01266-t005"><object-id pub-id-type="pii">sensors-25-01266-t005_Table 5</object-id><label>Table 5</label><caption><p>Emergency detection performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Scenario</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Response Time (s)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">False Positive <break/>
Rate (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sudden Altitude Drop</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prolonged Inactivity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Combined Altitude Drop &#x00026; Inactivity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Average</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.8</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01266-t006"><object-id pub-id-type="pii">sensors-25-01266-t006_Table 6</object-id><label>Table 6</label><caption><p>Impact of model compression on communication cost and global model performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Compression Type</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total Data Transferred (MB)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Global Model Accuracy (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Reduction in Data Transferred (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">No Compression (32-bit)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1200</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16-bit Compression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">600</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8-bit Compression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">300</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4-bit Compression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">150</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">87.5</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2-bit Compression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.7</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.7</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01266-t007"><object-id pub-id-type="pii">sensors-25-01266-t007_Table 7</object-id><label>Table 7</label><caption><p>Convergence and model updates with different communication strategies.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Communication Strategy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Global Rounds to Convergence</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Final Global Model Accuracy (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Full Update (32-bit)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16-bit Compression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.6</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8-bit Compression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.2</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4-bit Compression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2-bit Compression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89.7</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01266-t008"><object-id pub-id-type="pii">sensors-25-01266-t008_Table 8</object-id><label>Table 8</label><caption><p>Scalability and network load analysis.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number of <break/>
Edge Devices</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total Data Transferred (MB)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Global Model Accuracy (%)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Network Bandwidth <break/>
Usage (%)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10 Devices</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">150</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">30</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20 Devices</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">300</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50 Devices</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">750</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.4</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100 Devices</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1500</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">200 Devices</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100</td></tr></tbody></table></table-wrap></floats-group></article>