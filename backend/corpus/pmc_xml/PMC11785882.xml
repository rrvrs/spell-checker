<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Asian Bioeth Rev</journal-id><journal-id journal-id-type="iso-abbrev">Asian Bioeth Rev</journal-id><journal-title-group><journal-title>Asian Bioethics Review</journal-title></journal-title-group><issn pub-type="ppub">1793-8759</issn><issn pub-type="epub">1793-9453</issn><publisher><publisher-name>Springer Nature Singapore</publisher-name><publisher-loc>Singapore</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11785882</article-id><article-id pub-id-type="publisher-id">315</article-id><article-id pub-id-type="doi">10.1007/s41649-024-00315-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Paper</subject></subj-group></article-categories><title-group><article-title>The Permissibility of Biased AI in a Biased World: An Ethical Analysis of AI for Screening and Referrals for Diabetic Retinopathy in Singapore</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8506-0659</contrib-id><name><surname>Muyskens</surname><given-names>Kathryn</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ballantyne</surname><given-names>Angela</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Savulescu</surname><given-names>Julian</given-names></name><address><email>julian.savulescu@philosophy.ox.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Nasir</surname><given-names>Harisan Unais</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Muralidharan</surname><given-names>Anantharaman</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01tgyzw49</institution-id><institution-id institution-id-type="GRID">grid.4280.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2180 6431</institution-id><institution>Centre for Biomedical Ethics, </institution><institution>Yong Loo Lin School of Medicine, </institution></institution-wrap>National University of Singapore, Singapore </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01jmxt844</institution-id><institution-id institution-id-type="GRID">grid.29980.3a</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 7830</institution-id><institution>Department of Primary Health Care and General Practice, </institution><institution>University of Otago, </institution></institution-wrap>Dunedin, New Zealand </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/052gg0110</institution-id><institution-id institution-id-type="GRID">grid.4991.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8948</institution-id><institution>University of Oxford, </institution></institution-wrap>Oxford, UK </aff></contrib-group><pub-date pub-type="epub"><day>31</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>31</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="collection"><month>1</month><year>2025</year></pub-date><volume>17</volume><issue>1</issue><fpage>167</fpage><lpage>185</lpage><history><date date-type="received"><day>19</day><month>12</month><year>2023</year></date><date date-type="rev-recd"><day>1</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>28</day><month>7</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>
<bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">A significant and important ethical tension in resource allocation and public health ethics is between utility and equity. We explore this tension between utility and equity in the context of health AI through an examination of a diagnostic AI screening tool for diabetic retinopathy developed by a team of researchers at Duke-NUS in Singapore. While this tool was found to be effective, it was not equally effective across every ethnic group in Singapore, being less effective for the minority Malay population than for the Chinese majority. We discuss the problematic normative nature of bias in health AI and explore the ways in which bias can interact with various forms of social inequalities. From there, we examine the specifics of the diabetic retinopathy case and weigh up specific trade-offs between utility and equity. Ultimately, we conclude that it is ethically permissible to prioritise utility over equity where certain criteria hold. Given that any medical AI is more likely than not to have lingering bias due to bias in the training data that may reflect other social inequalities, we argue that it is permissible to implement an AI tool with residual bias where: (1) its introduction reduces the influence of biases (even if overall inequality is worsened), and/or (2) where the utility gained is significant enough and shared across groups (even if unevenly).</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>AI ethics</kwd><kwd>Bias</kwd><kwd>Data ethics</kwd><kwd>Healthcare ethics</kwd><kwd>Public health ethics</kwd><kwd>Equity</kwd><kwd>Utility</kwd></kwd-group><funding-group><award-group><funding-source><institution>Singapore Ministry of Health&#x02019;s National Medical Research Council under its Science, Health, and Policy Relevant Ethics, Singapore (SHAPES) Programme</institution></funding-source><award-id>MOH-000951</award-id><principal-award-recipient><name><surname>Savulescu</surname><given-names>Julian</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>NUS Start-up Grant </institution></funding-source><award-id>NUHSRO/2022/035/Startup/05</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; National University of Singapore and Springer Nature Singapore Pte Ltd. 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">A significant and important ethical tension in resource allocation and public health ethics is between utility and equity. Ideally, we want to introduce and fund health interventions that improve population health and decrease health inequities. However, sometimes these two values are in conflict. When it proves impossible to maximise both utility and equity, we must decide which to prioritise. Furthermore, we must determine when it is ethically acceptable to deploy AI-based tools in a clinical setting that may contribute to health disparities between groups. Thus, there is a need for better guidance on how to pick between imperfect options. In this paper, we will explore a case where the application of a particular form of health AI presents us with one such choice, namely the question of whether implementing a tool that increases wellbeing at the cost of worsening a pre-existing inequity is ethically permissible. We explore this tension between utility and equity in the context of health AI through an examination of a case study of a diagnostic tool for the screening of diabetic retinopathy developed by a team of researchers at Duke-NUS in Singapore. While this tool was found to be effective, it was not equally effective in its detection of diabetic retinopathy across every ethnic group in Singapore, being less effective for the minority Malay population than for the Chinese majority.</p><p id="Par3">In our exploration of the case, we will discuss the problematic normative nature of bias in health AI &#x02014; a phenomenon that has generated apprehension and diverse strategies to minimise algorithmic bias, but which remains under-theorised from a philosophical or bioethics perspective. There is general agreement that algorithmic bias is a significant concern with respect to AI in health and that such bias should be investigated and minimised where possible. While there are a number of recent articles covering other aspects of AI in clinical application, from AI in support for ethical decision-making (see Biller-Adorno et al.&#x000a0;<xref ref-type="bibr" rid="CR6">2022</xref>) to epistemological challenges in using AI for diagnostics (see Kempt and Nagel <xref ref-type="bibr" rid="CR19">2022</xref>), there has been limited work analysing exactly when bias amounts to an ethical barrier to deploying AI in the clinical context. Specifically, we wish to ask, when does algorithmic bias represent a substantial enough injury to justice that the unfairness would outweigh the potential utility benefits offered by the given AI program, rendering its use ethically unacceptable? The framing of this question admits that some degree of bias or inequality may be ethically tolerable. We postulate that algorithmic bias in AI may be ethically acceptable for two reasons, because (a) systemic bias is already present in the health system in question and (b) the utility gain offered by the AI may ethically outweigh the equity considerations.</p><p id="Par4">For our analysis, we take utility to mean maximising the aggregate health gain for the population. Broadly speaking, utility could be achieved via decreasing morbidity or mortality, improving quality of life, reducing health resource waste, improving triage, improving clinical pathways or processes, or achieving the same health outcome but with reduced costs. According to the World Health Organization (WHO), equity refers to the absence of unfair, avoidable, and remediable differences between groups of people, whether those groups are defined socially, economically, demographically, or geographically or by other dimensions of inequality (e.g. sex, gender, ethnicity, disability, or sexual orientation) (WHO <xref ref-type="bibr" rid="CR50">n.d.</xref>).</p><p id="Par5">Utility and equity are both important goals of the health system. This has been recognised and articulated in international declarations, such as the Organization for Economic Cooperation and Development (OECD) government policy and the WHO&#x02019;s public health ethics frameworks and statements. That said, the best ways to achieve and appropriately balance considerations of utility and equity in relation to specific clinical interventions &#x02014; such as triage scores, resource allocation protocols, clinical decisions aids, or the use of AI apps &#x02014; is complicated and heavily debated. In part, this is because any given clinical tool is introduced to a real-world health system and social context, which may feature historical injustices, current disparities in care, and unique social and economic power dynamics.</p><p id="Par6">To explore these tensions between utility and equity, we consider a specific case of an AI screening tool for detecting diabetic retinopathy in Singapore. To make our case, we also provide an analysis of several types of bias in AI, and how this interacts with the background injustice present within whatever social context such a tool might be deployed. We explore the various ways in which utility and equity interact in this scenario and attempt to extrapolate insights that can inform ethical decision-making in analogous cases in the future. We hope that our analysis will be illustrative and informative, but we also acknowledge its highly contextual nature, and that different trade-offs may be justifiable within different social contexts and healthcare systems.</p></sec><sec id="Sec2"><title>The Case</title><p id="Par7">Diabetes affects a growing number of people worldwide. The International Diabetes Federation (IDF) reports that 10.5% of the adult population (between 20 and 70&#x000a0;years old) has diabetes, estimating that half of those are unaware that they are living with the condition (International Diabetes Federation <xref ref-type="bibr" rid="CR15">2021</xref>), and up to 9.5% of the population in Singapore as of 2019 are affected by the disease (Diabetes&#x000a0;Singapore <xref ref-type="bibr" rid="CR12">n.d.</xref>). According to the WHO&#x000a0;(<xref ref-type="bibr" rid="CR49">2023</xref>), &#x0201c;In 2019, diabetes was the direct cause of 1.5 million deaths and 48% of all deaths due to diabetes occurred before the age of 70&#x000a0;years&#x0201d;. It is a chronic and progressive illness that requires careful management in the form of medication, and/or diet and exercise in order to stave off complications. Unmanaged diabetes can lead to the development of many other comorbidities, including effects on the heart, kidney, and other vital organs. Diabetes can also affect the eyes, causing damage to the blood vessels in the retina (diabetic retinopathy) or to the optic nerve (glaucoma) (Mayo Clinic Staff <xref ref-type="bibr" rid="CR30">2023</xref>). Early detection and treatment are vital to prevent further complications of these conditions. Diabetic patients also suffer financial burdens, from increased insurance premiums, the cost of medication and treatment, and the risk of job loss in the event that their condition worsens (Lim <xref ref-type="bibr" rid="CR23">2022</xref>). For a bit of context, while all Singaporeans and permanent residents receive some insurance coverage for diabetes-related hospitalisation, outpatient treatments, and medication, they are still required to pay at least 15% out of pocket. Moreover, this state insurance is paid for from compulsory medical savings accounts, for those who are able to afford it, but covered by the state for those who are not. Many Singaporeans may also purchase private insurance to improve coverage.</p><p id="Par8">As a chronic and progressive condition, diabetes does not look the same in every patient. Some patients will have much more severe symptoms, and this becomes more likely when the disease is poorly controlled (e.g. where the patient adopts less management strategies, like frequent exercise, careful nutrition, or regular check-ups). To minimise these costs, it is important to screen for and detect the condition and its comorbidities early. To that end, a team from Duke-NUS led by Ting and colleagues (<xref ref-type="bibr" rid="CR47">2017</xref>) has developed a deep learning system to detect the progression of diabetic retinopathy and glaucoma (hereafter referred to as DLSDR for a deep learning system for diabetic retinopathy). Retinal images in the DLSDR training set were labelled by professional graders, and disagreements were settled by retinal specialists. Where the progression of a disease achieves a certain level, the algorithm tags it as referable and radiologists may use this result to refer the patient to a specialist. While the accuracy of detection by the algorithm was, overall, comparable to human experts, the algorithm exhibited significantly lower sensitivity (97.1%) in detecting referable diabetic retinopathy for Malays than for non-Malays<xref ref-type="fn" rid="Fn1">1</xref> (99.3% for Indians and 100% for Chinese<xref ref-type="fn" rid="Fn2">2</xref>), but significantly higher specificity. The terms &#x0201c;sensitivity&#x0201d; and &#x0201c;specificity&#x0201d; here relate to the accuracy of the tool. A tool&#x02019;s sensitivity shows how many positive cases are detected out of the total pool of positive cases, expressed as a percentage score. The specificity score refers to how often a tool accurately identifies a patient as having a negative result &#x02014; e.g. the tool detects no diabetic retinopathy where there indeed is no diabetic retinopathy. In diagnostic terms, a low sensitivity score would mean a high chance of a false negative, and a low specificity score a high chance of a false positive.</p><p id="Par9">Compared to unaided clinical judgment (99%), the DLSDR system was lower in specificity across all ethnic groups: 82% for Malay; 73% for Indian, and 76% for Chinese patients.<xref ref-type="fn" rid="Fn3">3</xref> DLSDR therefore had the highest specificity for Malays (Ting et al.&#x000a0;<xref ref-type="bibr" rid="CR47">2017</xref>, 2217&#x02013;2220). DLSDR had the lowest sensitivity for Malay patients: 97% for Malays, 99.3% for Indians, and 100% for Chinese. Malay patients were the most likely to be underdiagnosed. Used as Ting et al. (<xref ref-type="bibr" rid="CR47">2017</xref>) designed it, the DLSDR performs the initial grading of the retinal images, a curated sample (with a 10% false negative rate) is then passed on to human graders for review. Since DLSDR and the human graders have roughly equal sensitivity, but human graders have higher specificity, this process ought to streamline workflow and improve the overall accuracy and efficiency of human graders.</p></sec><sec id="Sec3"><title>Social Context</title><p id="Par10">Singapore is a multi-ethnic, multicultural and multilingual society, and has been since its founding as a city in 1819. As of 2020, according to the Singapore Department of Statistics, Singapore&#x02019;s ethnic makeup was nearly three-quarters Chinese (74.3%), 13.5% Malay, 9% Indian, and 3.2% other. Historically, the Malays have the longest history in the region. Yet, like much of Southeast Asia, Singapore has been a hub for trade and migration of people for centuries. While the British colonial period (roughly 1819&#x02013;1963) saw waves of both Chinese and Indian migration, the Chinese population has connections to Singapore that date back at least 700&#x000a0;years, and the long history of Indian influence in the region can be seen through the number of loan words within the Malay language that persists today (Chong and Bak&#x000a0;<xref ref-type="bibr" rid="CR10">2019</xref>; Low <xref ref-type="bibr" rid="CR27">2004</xref>).</p><p id="Par11">In Singapore, Malay patients are currently less likely to be referred to specialists for diabetic retinopathy than non-Malays. Malays account for at least 20% (according to Singhealth <xref ref-type="bibr" rid="CR43">2017</xref>) of diabetic patients in Singapore and are therefore at higher risk for developing diabetic retinopathy than non-Malays, especially given that they also experience poorer disease control (due at least in part to the lower engagement in health-seeking behaviours among Malays), which makes it more likely that they will experience this and other complications of unmanaged diabetes. In an unbiased system, we would expect that rates of referable diabetic retinopathy to roughly track the proportion of diabetics who are Malay (&#x0003e;&#x02009;20%) weighted by the average degree of disease control. Despite this, they are still referred to specialists less often than other groups. Only 7.3% of those referred to specialist care are Malay (Sia et al.&#x000a0;<xref ref-type="bibr" rid="CR42">2020</xref>).<xref ref-type="fn" rid="Fn4">4</xref> This would seem to indicate some background bias or other prior unjust disparities present in the social environment in which the DLSDR system would be deployed. We must ask, what explains this disparity? How we account for the unexpectedly low rates of referral (7.3%) in the clinical setting prior to the DLSDR system&#x02019;s use will influence the permissibility of introducing an AI tool that performs differently for different ethnic groups.</p><p id="Par12">In theory, the background bias in referrals could derive from multiple sources: including personal (conscious or unconscious) bias on the part of physicians or graders; disparities in patient participation in screening; distrust of the medical establishment; workplace discrimination; poverty; religious fatalism; lack of education; cultural norms or personal choice for example (see Kaur-Gill <xref ref-type="bibr" rid="CR18">2022</xref>; Zainal et al.&#x000a0;<xref ref-type="bibr" rid="CR52">2021</xref>; Mohammad <xref ref-type="bibr" rid="CR35">2021</xref>). A study by Huang et al. (<xref ref-type="bibr" rid="CR14">2009</xref>) found that Malays in Singapore were more likely to be unaware that they had diabetes or diabetic retinopathy. This tendency itself could be also attributable to the lower consumption rate of preventative care among Malays when compared with non-Malays in Singapore (Ministry of Health <xref ref-type="bibr" rid="CR31">2020</xref>). This lack of uptake of preventive care may itself indicate a dearth of trust among the Malay population, or a lack of opportunity to seek such care due to other socio-economic constraints, like working in the gig economy (Mohammad <xref ref-type="bibr" rid="CR35">2021</xref>).</p><p id="Par13">This economic reality places an additional barrier to participating in health screening, since they are unlikely to have the luxury of paid leave. Since Malays, on average live closer to the poverty line (see Sulaiman <xref ref-type="bibr" rid="CR44">2022</xref>), the urgency of that loss of income would be more severe, again feeding into a reticence to take time off of work for medical screening. It has been shown that Malays experience workplace discrimination, as the ethnic Chinese majority in Singapore has been found, on average, to be inclined to judge Malay candidates as being less competent than non-Malay candidates despite being similarly credentialled (Chew et al. <xref ref-type="bibr" rid="CR8">2019</xref>).<xref ref-type="fn" rid="Fn5">5</xref> As a result, Malays might reasonably perceive that frequent work absences would feed into the negative assumptions about them, and hence put their job at risk. This, again, could easily translate into a reticence to take time off from work for medical check ups.</p><p id="Par14">The potential for prejudice on the part of clinicians, whether conscious or not, is also important to consider. This would naturally feed into another barrier to participation in screening in the form of distrust towards the medical profession (Goh et al.&#x000a0;<xref ref-type="bibr" rid="CR13">2022</xref>). This has been found, in other countries, to be causally linked to prior injustices (Best et al.&#x000a0;<xref ref-type="bibr" rid="CR5">2021</xref>; Wiencek <xref ref-type="bibr" rid="CR51">2023</xref>). A recent report by the Community Action Network (Singapore) (<xref ref-type="bibr" rid="CR11">n.d.</xref>)&#x000a0;to the UN Committee on the Elimination of Racial Discrimination on Singapore&#x02019;s Compliance with the International Convention on the Elimination of All Forms of Racial Discrimination in Singapore has observed that ethnic minorities (Malays and Indians) experience a higher incidence of chronic illnesses as well as higher mortality rates compared to ethnic Chinese, &#x0201c;indicating systemic inequalities in healthcare &#x02026; that occur along ethnic lines, suggesting that there is indeed racial discrimination in the right to public health&#x0201d;. The lower referral rate could also be spurred on by conscious or unconscious racial bias on the part of clinicians (Wong <xref ref-type="bibr" rid="CR48">2021</xref>). It is possible that family physicians might be less willing to send Malay patients for screening or less willing to refer them to specialists despite the grader positively identifying their scans as displaying referable diabetic retinopathy. Indeed, it has been found that the most commonly cited reason that patients do not engage with the health system is prior bad experiences, including experience of discrimination, judgemental or dismissive treatment, and lack of therapeutic rapport or respect (Taber et al.&#x000a0;<xref ref-type="bibr" rid="CR46">2015</xref>).</p><p id="Par15">Language barriers could also be an issue, as it has been observed in other contexts that language barriers can play a large role in a patient&#x02019;s likelihood of accessing screening and treatment (Zheng et al.&#x000a0;<xref ref-type="bibr" rid="CR53">2012</xref>). In Singapore, there are four national languages (English, Malay, Mandarin, and Tamil), and while the modern healthcare system is conducted predominantly in English, many of the older generations of Singapore are likely to be more comfortable in another language. Again, the Community Action Network (<xref ref-type="bibr" rid="CR11">n.d.</xref>)&#x000a0;report describes, &#x0201c;Despite being two of the four official languages in Singapore, there is a lack of readily available Malay and Tamil language interpreters in healthcare settings&#x0201d;. Interestingly, in these reports of health discrimination, the group most commonly reported as experiencing discrimination within the Singapore health system is those of Indian ethnicity. If health provider discrimination were the sole cause of the disparity in diabetic referrals, we would expect to see a similar gap for the Indian community as observed for Malay patients.</p><p id="Par16">A study by Lim et al. (<xref ref-type="bibr" rid="CR24">2013</xref>) found that in Singapore&#x02019;s rapid modernization between 1965 and 2009, life expectancy increased across all groups. Additionally, though there was a &#x0201c;convergence in life expectancy between Indians and Chinese, the (substantial) gap between Malays and the other two ethnic groups has remained (Lim et al.&#x000a0;<xref ref-type="bibr" rid="CR24">2013</xref>). This data, together, would suggest that while discrimination is very likely a component of the explanation for the disparity in diabetic retinopathy referrals experienced by Malays, it is also likely that other factors beyond systemic discrimination in the health system are at play.</p><p id="Par17">Other factors, including some specific to the Malay-Muslim community in Singapore, play a role in shaping both the disease risk, level of control, and likelihood of referral. On one hand, Malay-Muslim people may be motivated to maintain their health as an important part of the religion (part of &#x0201c;good stewardship&#x0201d; of the body, given to them by God) (Zainal et al.&#x000a0;<xref ref-type="bibr" rid="CR52">2021</xref>). On the other hand, religiously-inspired fatalism, whereby someone may be unwilling to seek medical attention because they regard whether one lives or dies as up to God&#x02019;s (Allah&#x02019;s) will, is also common (Goh et al.&#x000a0;<xref ref-type="bibr" rid="CR13">2022</xref>). Additionally, as Zainal et al. (<xref ref-type="bibr" rid="CR52">2021</xref>) describe, Islamic norms of modesty (which emphasize the need for women to cover all parts of the body, except for hands and face) as well as traditional gender roles can constrain Malay-Muslim women&#x02019;s ability to engage in physical exercise. This is a practical constraint on their ability to engage in healthy behaviours, as well as a possible barrier to seeking health care and physical exams more broadly.</p><p id="Par18">Even the prevalence and distribution of diseases themselves are not immune to social pressures. As we can see with a condition like diabetes, the risk factors for developing the condition and the ease of managing it are both subject to social conditions. The freedom to take the time to exercise as well as the affordability and availability of healthy food options are not evenly distributed, and these things tend to be harder to acquire for those who are socially disadvantaged (for whatever reason). For Malays in Singapore, then, the fact that they experience both a higher incidence of diabetes and poorer disease control is likely linked.</p><p id="Par19">All of these factors likely intersect to contribute to present health disparities experienced by Malay patients in Singapore. Though an exact diagnosis of the origin of the inequalities experienced by Malays in Singapore is beyond the scope of this paper, what we have presented here gives us reason to believe that at least some non-negligible proportion is insidious in nature. Given that at least some portion of the existing disparities are rooted in inequality and bias, the potential widening of a health disparity is concerning.</p><p id="Par20">Implementing the DLSDR into this social context would likely result in more referrals to specialists across groups when compared with unaided clinical judgment due to the boost in efficiency that the system provides. However, it could also widen the existing disparity between Malays and non-Malays. This would be because while the sensitivity in detecting diabetic retinopathy improves for all groups, the sensitivity improves for non-Malays more than Malays. Meanwhile, most if not all of the causes of the disparity remain untouched by the implementation of the DLSDR.</p></sec><sec id="Sec4"><title>Algorithmic <italic>Bias</italic> in an Imperfect World</title><p id="Par21">Bias comes in numerous forms, algorithmic, social, and statistical. Algorithmic bias occurs when the prediction or outputs of a model would, if implemented under prevailing circumstances, unfairly and unjustifiably benefit or disadvantage certain individuals or groups (Kordzadeh and Ghasemaghaei <xref ref-type="bibr" rid="CR21">2022</xref>).<xref ref-type="fn" rid="Fn6">6</xref> There are a variety of different sources of algorithmic bias; we do not explore these in depth because it has been well canvassed elsewhere (see O&#x02019;Neil <xref ref-type="bibr" rid="CR37">2017</xref>; Mittelstadt et al.&#x000a0;<xref ref-type="bibr" rid="CR33">2016</xref>; Mittelstadt et al.&#x000a0;<xref ref-type="bibr" rid="CR34">2023</xref>; and Marjanovic et al.&#x000a0;<xref ref-type="bibr" rid="CR29">2022</xref>). Bias can influence an AI model at various stages, from the way in which the problem is framed, the process of curating and preparing the data, bias being present in the data itself, through to inappropriate deployment in a new context where it has not been validated on local data, or failure to consider the social context in which the model is deployed (Leslie&#x000a0;et al. <xref ref-type="bibr" rid="CR22">2021</xref>). Training data may be statistically biased because it is unrepresentative of the target patient population. For example, studies into tissue-based functional genomics (investigating the impact of genomic diversity on human biological function including health and disease) currently rely on samples from donors who are predominantly of European descent (ranging from 82 to 95%) (Long et al.&#x000a0;<xref ref-type="bibr" rid="CR26">2022</xref>). Research populations often fail to reflect patient populations in terms of age, gender, race, socioeconomic status, comorbidities, and disease severity (Mosenifar <xref ref-type="bibr" rid="CR36">2007</xref>). Real-world data captured by electronic health records is more representative than research data but still fails to account for the unmet health needs of patients who cannot or do not (for various reasons) access health care when sick. To minimise statistical bias, AI algorithms should be trained with large datasets of diverse and balanced information (Agostina et al.&#x000a0;<xref ref-type="bibr" rid="CR1">2020</xref>).</p><p id="Par22">Alternatively, bias in data used to train the AI program may be due to systemic social bias such as differential access to the underlying social determinants of health (e.g. experiences of poverty, lack of education and living conditions); differential access to healthcare (e.g. distance to hospital or insurance status); or patterns of discrimination (such as racism and sexism). Hence, even when the data used to train the model may be representative and accurate, it may still capture and reflect objectionable aspects of the real world such as stigma, discrimination, or oppression (Mitchell et al.&#x000a0;<xref ref-type="bibr" rid="CR32">2021</xref>). For example, an unconscious racial bias that leads to the over-policing of people of colour in the USA can generate statistical bias in arrest data if people of colour are more likely to be arrested and if arrests are considered a proxy measure of actual crime (Lum and Isaac <xref ref-type="bibr" rid="CR28">2016</xref>). If we were, hypothetically, able to find a perfect measure of the crime rate this would be free from statistical bias but still be coloured by social bias. This is because individuals&#x02019; participation in crime is in part socially influenced and reflects unequal social structures, and societal bias arises even in how crime is defined (Raphling <xref ref-type="bibr" rid="CR40">2018</xref>). There is no neutral health data set that does not include these biases and therefore the impact of social bias on AI programs is impossible to avoid.</p><p id="Par23">Thus, it seems to be the case that no AI can be made completely unbiased. Likewise, it is also true that no social context in which a given AI might be employed is completely unbiased. Thus, we will begin our arguments from a position we think is most reasonable: It is unclear what proportion of the inequalities have blameworthy roots, but it is a near certainty that some undefined, but non-negligible proportion is.</p><p id="Par24">We have stipulated that both utility and equity are important considerations for any healthcare policy. Bias exists not just in the algorithms that we create, but in the environments where such tools would be deployed as well. It seems at present that we are unable to completely remove bias or inequality from either. This forces us to consider the following questions: When might it be permissible to use a biased AI in an already biased world? And is it ever permissible to worsen an existing disparity for the sake of utility?</p></sec><sec id="Sec5"><title>The Impact of <italic>Bias</italic> on Utility and Equity</title><p id="Par25">Correcting for fairness within the AI model is notoriously difficult and requires differentiating and balancing bias, variance, and noise (Petersen et al.&#x000a0;<xref ref-type="bibr" rid="CR39">2023</xref>). While many data scientists are working to improve statistical metrics for fairness, we approach this case study from the perspective that most AI training data in health is biased (both socially and statistically) to some extent, and even if app developers have an ethical obligation to reduce bias where possible, we must expect that some residual bias will remain. The purpose of this paper is to consider the ethical trade-offs between equity and utility in the context of a specific case study. The bottom-up, case-based approach we take here provides a useful complement to more abstract discussion of algorithmic fairness and shows how consideration of bias might play out in a clinical context. In our view, judgments about an acceptable degree of bias and how this should be weighed against health utility must account for normative, political, and social aspects of the specific context, health condition, and AI tool. This requires consideration of the social, economic, and historical context and current barriers and enablers to healthcare access.</p><p id="Par26">When thinking about AI in the context of health we might find that utility and equity intersect in the following ways, wherein the AI model could:<list list-type="alpha-lower"><list-item><p id="Par27">Be less biased than current clinical care models, resulting in decreased health disparities between groups</p></list-item><list-item><p id="Par28">Be as biased as current clinical care models, but offering improved aggregate population health utility</p></list-item><list-item><p id="Par29">Be more biased than current clinical care models, resulting in increased health disparities between groups, but offering improved aggregate population health utility <italic>and</italic> improves the health of the disadvantaged group</p></list-item></list></p><p id="Par30">In order to make the comparison with current clinical models viable, we might think of an AI as being more biased if it would create new or exacerbate existing unjust disparities in society (Panch et al.&#x000a0;<xref ref-type="bibr" rid="CR38">2019</xref>). If an AI is more biased than current clinical care models, then there is a strong, if not wholly decisive, reason to not implement the AI. The DLSDR case that we present in this paper, seems exceptional in that the AI is more biased than current clinical models and yet there is no strong reason against implementing it.</p><p id="Par31">In part, this is due to the nature of the particular forms of bias at play in this case. As mentioned, it seems reasonable to conclude that at least some of the existing background disparity in Malay referrals was rooted in blameworthy forms of bias. It is also true that although the DLSDR system was slightly more biased than the status quo, nevertheless the quality of that bias is likely to be less objectionable.</p><p id="Par32">We know that DLSDR is a biased system, in that it is more likely to underdiagnose Malays than it is to underdiagnose other groups, while at the same time, it is also more biased than the status quo. The question of the ethical permissibility of using such a biased system depends on its interactions with the present social context, which we also know to be characterised by bias and inequality. How these different layers of bias interact is determined at least in part by the quality of the background bias (as discussed in section II). We postulate that it is permissible to implement a biased AI tool where (1) its introduction reduces the influence of biases, and/or (2) where the utility gained is significant enough and shared across groups. We know that algorithmic bias in this case is likely to contain some social bias, though how much is unknown. In what follows, we will explore some possible sources of the background inequality experienced by Malays in this case, and how this may shape our assessment of the permissibility of the AI tool.</p></sec><sec id="Sec6"><title>The Question of Permissibility</title><p id="Par33">How does the diagnosis of the origin of the disparities between Malays and other races in terms of diabetic screening and referrals affect the permissibility of the DLSDR system in this case?</p><p id="Par34">If grader bias is the origin of most of the disparity, employing the AI will almost certainly reduce this disparity, since it would remove such biased individuals from the process. Grader bias occurs if the trained graders are more likely to underdiagnose Malay patients than non-Malay patients. To see why the disparity would be reduced, let us consider the hypothetical situation where the disparity in referrals is explained entirely by grader bias. This means that Malay patients attend screening at roughly the same rates as their non-Malay counterparts. In such a situation, the percentage of attendees who are Malay should reflect the percentage of diabetics who are Malay or exceed it. Of those who attend the screening, the percentage of Malay patients with referable diabetic retinopathy who are actually referred would be about 33%, and the percentage of non-Malay patients with referable diabetic retinopathy would have to be above 90% (Ting et al.&#x000a0;<xref ref-type="bibr" rid="CR47">2017</xref>; Sia et al.&#x000a0;<xref ref-type="bibr" rid="CR42">2020</xref>). That is the only way a mere 7% of those who are referred could be Malay if the percentage who attend screening is about 20%. By contrast, given the same rate of screening attendance, with DLSDR, the percentage of Malay patients with referable diabetic retinopathy who are referred would be 97%, while for non-Malays, it would be at 100% or very close. Of course, these numbers only describe a scenario wherein diabetic Malays seek screening at the same rate as diabetic non-Malays. Moreover, this situation may be very unlikely.</p><p id="Par35">While some have argued that humans are often more prone to bias than algorithms (Kleinberg et al.&#x000a0;<xref ref-type="bibr" rid="CR20">2019</xref>; Kahneman et al.&#x000a0;<xref ref-type="bibr" rid="CR17">2021</xref>; Sunstein <xref ref-type="bibr" rid="CR45">2021</xref>) due to cognitive bias, it is unclear which cognitive bias is operating on graders such that they are somehow more likely to under-diagnose anonymised retinal scans from Malays than from non-Malays. To be sure, there are interethnic differences in the fundus pigmentation of eyes with Indian and African populations having darker pigmentation than Chinese and White populations (Cheung et al.&#x000a0;<xref ref-type="bibr" rid="CR7">2007</xref>; Rochtchina et al.&#x000a0;<xref ref-type="bibr" rid="CR41">2008</xref>; Ting et al.&#x000a0;<xref ref-type="bibr" rid="CR47">2017</xref>). However, while differences in fundus pigmentation have been hypothesised to affect the measurement of retinal vessel calibre, there is no evidence to suggest that it affects the measurement and detection of retinopathy. Given the other possible factors outside of grader bias that prevent Malays from seeking screening, the real-world difference will likely be less dramatic or likely even biased in the other direction. Nevertheless, if grader bias is indeed a cause of the disparity, implementing DLSDR would result in a potentially significant reduction in that disparity and would make the AI preferable to unaided decision-making in this case.<xref ref-type="fn" rid="Fn7">7</xref> By taking the biased graders out of the loop and replacing them with a biased AI screening tool, both Malays and non-Malays stand to benefit.</p><p id="Par36">If all, or almost all the disparity is caused by other factors which reduce rates of participation in screening (like religious fatalism, socio-economic disparities, suspicion of the medical system, or biased family physicians being less likely to send Malays for screening) or otherwise contribute to reduced rates of referral (like a biased physician being less likely to refer Malay patients to specialists despite positive diagnosis by graders), then the disparity will be likely to increase when DLSDR is implemented. This is because these causes of the disparity are not removed when the AI is introduced. Meanwhile, the AI introduces a new source of inequality. To see why, consider the case where the differential referral rate is due entirely to lower rates of participation in screening. If the AI is implemented, we might expect the proportion of Malays referred for diabetic retinopathy to drop even lower to 7.1%<xref ref-type="fn" rid="Fn8">8</xref> (Sia et al.&#x000a0;<xref ref-type="bibr" rid="CR42">2020</xref>). While this drop of 0.2% might not seem like much, it does count as a widening of the disparity (see Appendix for a walkthrough of the math).</p><p id="Par37">In this way, it can be seen that a diagnosis of the origin of the background inequalities and the nature of the existing bias in the social context has implications for the acceptability of implementing a biased AI system. Given the details of the case, it seems that implementing DLSDR is likely to improve the referral rates for Malays if the original disparity was explained by mostly unjust forms of bias such as clinical bias in referral rates. This is true even if the DLSDR system itself is not entirely free of bias. Conversely, the more neutral the origin of the existing disparity is &#x02014; for example, religious fatalism as a cultural barrier to screening &#x02014; the less likely the DLSDR system is to improve upon the disparity &#x02014; and in fact, may widen the gap because the AI toll will increase detection rates and referrals for specialist care for other ethnicities but not for Malay patients.</p><p id="Par38">Let us turn our attention back to the central tension in this case: tradeoffs between utility and equity. The mere fact that an AI intervention widens pre-existing disparities may not be enough to show that using it is impermissible. Before we can make a ruling one way or another, we must explore the utility that stands to be gained.</p><p id="Par39">As mentioned in the case study, the DLSDR system is comparable in accuracy compared to human experts. Implementing it as it was designed should have the effect of making screening for diabetic retinopathy cheaper and quicker, which has the potential to translate into serving more people and catching and treating more cases. For the average Singaporean who might otherwise be affected, having the condition caught early can mean avoiding blindness, job loss, and other consequences. To suggest that we withhold what would be a significant benefit for the majority of the population because it did not manage to do so without worsening a disparity is to weigh equity higher than utility. Doing so in this case seems counterintuitive for a number of reasons.</p><p id="Par40">One significant consideration in favour of implementing DLSDR is that despite the gap between Malays and non-Malays increasing, in absolute terms, Malays will still be better off on average with the DLSDR tool than without it. After all, while a human grader&#x02019;s sensitivity can go as low as 90%, the AI tool&#x02019;s sensitivity for Malays is 97% (Ting et al.&#x000a0;<xref ref-type="bibr" rid="CR47">2017</xref>; Sia et al.&#x000a0;<xref ref-type="bibr" rid="CR42">2020</xref>). Of the patients who attend screening and have diabetic retinopathy, 7% more will be referred to the specialist when using the AI than without the AI. As a result, more Malay patients will have access to specialist care, and thereby be better enabled to manage their condition. Though this gain comes only by means of a trade (sacrificing some degree of equity), it may translate into real benefits in wellbeing, as well as wider social benefits, through the prevention of job loss and disability. So, with a full accounting of the social context, the trade seems a fair one to make, even if it is not perfect. Ideally, we would like to eliminate disparities and raise everyone&#x02019;s standard of wellbeing equally. In the absence of that option, however, this seems to be a worthwhile outcome.</p><p id="Par41">Moreover, the difference in sensitivity, although statistically significant, is small: 97% vs 99 or 100%. While we should aim at equity by bringing up the sensitivity for Malays, we should not delay the roll out of such AI which stands to benefit all significantly. The amount of inequity matters, and in this case it is small.</p><p id="Par42">That said, the exact degree of utility created depends on the manner in which the technology is implemented in the clinical context. As discussed, it is likely that at least some of the disparity in referrals is rooted in differences in participation in screening across groups. The DLSDR system does not by itself reduce any burden on the patients being screened, but if it were to be rolled out in a manner that did so, then the disparity would naturally shrink. Thus, we would recommend that DLSDR be implemented in partnership with efforts to maximise screening,<xref ref-type="fn" rid="Fn9">9</xref> for example by ensuring that screening is delivered in a way that maximises accessibility and cultural acceptability for Malay patients. As the absolute number of Malay diabetics getting screened increases, more would benefit, despite the algorithm&#x02019;s internal bias.</p><p id="Par43">In this analysis, it can be seen that implementing an imperfect system like DLSDR has the potential to meaningfully improve the referral and treatment of diabetic retinopathy for the Malay population, even though, in a worse-case scenario it could widen the gap in referral rates thereby increasing a health disparity between ethnic groups. Implementing the tool has the potential to mitigate differential referral rates based on conscious or unconscious racial bias on the part of clinicians, and also increase the absolute number of Malays that are diagnosed and referred. In a situation like the DLSDR case study, as with many, there is no perfect outcome. Rather, we are presented with a choice between two types of trade-off. In the case that we implement the AI app, we worsen some inequity in exchange for a meaningful amount of utility, and that utility is shared among all ethnic groups in Singapore (even if unevenly). In the case that we do not implement the DLSDR, then we maintain the current level of inequity and miss out on the utility that could have been gained. This opportunity cost seems tough to justify. Thus, implementing the AI here is the preferable option.</p><p id="Par44">The DLSDR case study is an example of an AI system with relatively low bias, and we have argued that because of the particulars of this case, it is permissible to implement the system regardless of the fact that a pre-existing disparity is worsened. Additionally, due to the particulars of this scenario, though the disparity was increased, the Malay population still benefits in the absolute sense.</p><p id="Par45">In a hypothetical alternative scenario &#x02014; wherein Malays do not enjoy any of the benefits of this system and the utility gained is only enjoyed by the majority group &#x02014;, there would be less weight on the scale in favour of permitting the algorithm&#x02019;s use. While we contend that it is permissible to widen the inequality in the DLSDR case, at least part of the reasoning is rooted in the fact that all boats rise. Thus, if there was no benefit to be found for the Malay population (in either the relative or absolute sense) then the use of such an algorithm would be less justifiable. However, if additional measures were taken (like some kind of compensation or offset being given to the disadvantaged group), it could still be permissible to employ the biased system, if the gain in utility is considered important enough.</p><p id="Par46">Before we move on to the conclusion, it is worth entertaining one final concern. One possible remaining objection to employing a biased system like this (even when the disparity is only slightly worsened, and even when utility is gained for all groups) could still be raised on the grounds that its use expresses some kind of disrespect for members of the disadvantaged group. If employing a biased algorithm that worsens a pre-existing unjust disparity is considered disrespectful or disparaging, then we have at least one more reason not to condone its use (see Lippert-Rasmussen (<xref ref-type="bibr" rid="CR25">2023</xref>) for more on this). While we agree in the abstract that this is a valid concern, in the case of DLSDR, it is difficult to see how its use would be disrespectful to the Malay population, given that it is still likely to increase the total number of diabetic retinopathy patients being diagnosed. Additionally, it would seem to matter what the origin of the lower accuracy in the minority group is rooted before one can claim that an algorithm&#x02019;s use is disrespectful. Many medical devices fail to be equally accurate across all possible phenotypes (for example, pulse oximeters are often less accurate on darker skin). If the bias of the DLSDR system was rooted in a similar origin, it is hard to see how this would be related to disrespect. Further, if one does take the use of such a device to be disrespectful, then many common medical devices will be just as guilty (e.g. the pulse oximeter).</p></sec><sec id="Sec7"><title>Conclusion</title><p id="Par47">When faced with the need to choose between competing goods like utility and equity, top-down reasoning and generalizations are not always sufficient. In an ideal world, of course, we would never need to trade between the two. However, we do not live in an ideal world, and in practice, trade-offs are often unavoidable. The nature and extent of inequality and utility matter in weighing these two values. In this paper, we have shown that a bottom-up, case-based approach can illuminate important nuances in the question of how to navigate some of the ethical dilemmas presented by the use of biased AI in healthcare. The case of the AI screening tool for diabetic retinopathy represents one instance where the gains in utility justify the clinical deployment of the tool, despite the potential equity implications. Through our exploration of this case, we hope to have offered some useful lessons for when an AI ought to be implemented even when doing so may create or widen disparities, even ones that are admittedly unjust. Our analysis of the ethical trade-offs involved in this case is context-specific &#x02014; grounded in the nature of the social, cultural, and historical contacts within Singapore. The lesson from the current case seems to be that pursuing utility at the expense of equity is at least sometimes permissible, as long as that gain in utility is enjoyed by all groups involved. We should not let the permissibility of implementing the AI tool in this case prevent us from trying to develop less biased versions of the AI going forward. Nevertheless, until the less biased version is available, it is appropriate to implement the imperfect one. Waiting for perfection would mean a loss in well-being born by all members of society. Those who would wait for perfection may indeed be waiting for a long time.</p></sec></body><back><app-group><app id="App1"><sec id="Sec8"><title>Appendix</title><p id="Par49">We are assuming here that the disparity in referrals is explained entirely by differences in rates at which Malays and non-Malays attend screening for diabetic retinopathy. For simplicity, we will assume that a patient is referred if and only if the trained grader detects referable diabetic retinopathy. To see why the disparity in referral rates would worsen, consider a hypothetical where there are currently 1000 patients with diabetic retinopathy being referred to the ophthalmologist for said condition.</p><p id="Par50">At current rates of referral,</p><p id="Par51">1000&#x02009;&#x000d7;&#x02009;7.3%&#x02009;=&#x02009;73 of these patients would be Malay and</p><p id="Par52">1000&#x02009;&#x000d7;&#x02009;92.7%&#x02009;=&#x02009;927 would be non-Malay.</p><p id="Par53">At current unaided sensitivity rates of about 90% (Ting et al.&#x000a0;<xref ref-type="bibr" rid="CR47">2017</xref>; Sia et al.&#x000a0;<xref ref-type="bibr" rid="CR42">2020</xref>), the number of Malay patients who had diabetic retinopathy and attended screening would be</p><p id="Par54">100&#x02009;&#x000d7;&#x02009;73/90 &#x02248; 81</p><p id="Par55">Likewise, the number of non-Malay patients who had diabetic retinopathy and attended screening would be</p><p id="Par56">100&#x02009;&#x000d7;&#x02009;927/90&#x02009;=&#x02009;1030</p><p id="Par57">Under the algorithm, the sensitivity for detecting diabetic retinopathy among Malays would be 97%. As such, the number of Malays who are detected and referred would be</p><p id="Par58">0.97 &#x000d7; 81&#x02009;=&#x02009;78.6 &#x02248; 79</p><p id="Par59">Likewise, given that the algorithm has 100% sensitivity for non-Malays, the number of non-Malays who would be referred is 1030.</p><p id="Par60">With the AI, the percentage of referrals who would be Malay is:</p><p id="Par61">100%&#x02009;&#x000d7;&#x02009;79/(1030&#x02009;+&#x02009;79)&#x02009;=&#x02009;7.1%</p><p id="Par62">There would thus be a 0.2% drop in the Malay share of referrals even if more Malays were referred to specialists for diabetic retinopathy.</p><p id="Par63">We might try to weaken the assumption that a patient is referred to if and only if their scan is graded as displaying referable diabetic retinopathy. Suppose, instead, that a patient was not always referred to a specialist when the grader detected referable diabetic retinopathy. This might be because the question as to whether a patient should be referred is not settled by the scan result. Suppose only a fraction, q, of non-Malay patients who are graded as having referrable DR are in fact referred. Suppose further that as a result of physician bias an even lower fraction, r of Malay patients who are graded as having referrable DR are referred to a specialist.</p><p id="Par64">Consider, again, a hypothetical where there are currently 1,000 patients with diabetic retinopathy being referred to the ophthalmologist for said condition.</p><p id="Par65">At current rates of referral,</p><p id="Par66">1000&#x02009;&#x000d7;&#x02009;7.3%&#x02009;=&#x02009;73 of these patients would be Malay and</p><p id="Par67">1000&#x02009;&#x000d7;&#x02009;92.7%&#x02009;=&#x02009;927 would be non-Malay.</p><p id="Par68">At current unaided sensitivity rates of about 90% (Ting et al.&#x000a0;<xref ref-type="bibr" rid="CR47">2017</xref>; Sia et al.&#x000a0;<xref ref-type="bibr" rid="CR42">2020</xref>), the number of Malay patients who had diabetic retinopathy and attended screening would be</p><p id="Par69">100&#x02009;&#x000d7;&#x02009;73/90r &#x02248; 81/r</p><p id="Par70">Likewise, the number of non-Malay patients who had referable diabetic retinopathy and attended screening would be</p><p id="Par71">100&#x02009;&#x000d7;&#x02009;927/90q&#x02009;=&#x02009;1030/q</p><p id="Par72">Under the algorithm, the sensitivity for detecting diabetic retinopathy among Malays would be 97%. As such, the number of Malays who are detected would be</p><p id="Par73">0.97 &#x000d7; 81/r&#x02009;=&#x02009;78.6/r &#x02248; 79/r</p><p id="Par74">Since the physician is still in the loop and has to refer the patient to the specialist, the number of Malays who would be referred is</p><p id="Par75">79/r&#x02009;&#x000d7;&#x02009;r&#x02009;=&#x02009;79</p><p id="Par76">Likewise, given that the algorithm has 100% sensitivity for non-Malays, the number of non-Malays who would be detected is 1030/q</p><p id="Par77">The number of non-Malays who would be referred is</p><p id="Par78">1030/q&#x02009;&#x000d7;&#x02009;q&#x02009;=&#x02009;1030</p><p id="Par79">With the AI, the percentage of referrals who would be Malay is:</p><p id="Par80">100%&#x02009;&#x000d7;&#x02009;79/(1030&#x02009;+&#x02009;79)&#x02009;=&#x02009;7.1%</p><p id="Par81">There would thus still be a 0.2% drop in the Malay share of referrals even if the disparity is partly contributed to by biased physicians being more reluctant to refer Malay patients despite the patient&#x02019;s scans being graded as presenting with referrable DR.</p></sec></app></app-group><fn-group><fn id="Fn1"><label>1</label><p id="Par82"> Using the results provided by Ting et al. (<xref ref-type="bibr" rid="CR47">2017</xref>), we reconstructed the number of true positives, false negatives, false positives, and true negatives for each ethnic group. We then conducted a Fisher&#x02019;s exact test using these numbers. Comparing sensitivity of Malays vs. Indians and then vs Chinese, the <italic>p</italic>-value was at 0.0021 and 0.001 respectively, which is well below 5% significance. By comparison, when comparing the difference in sensitivity between Indian and Chinese, the <italic>p</italic>-value was 0.3497 which indicates there is no significant difference between those groups.</p></fn><fn id="Fn2"><label>2</label><p id="Par83"> These numbers reflect the scores when the DLSDR system is set to maximise sensitivity. This setting can be adjusted, but it was determined by the developers that greater sensitivity was preferable to aid workflow of the human graders (DLSDR is intended as a first line screening to narrow the pool of scans for human graders to examine). It is important to note that though this score can be adjusted, perfect parity across groups was not achievable.</p></fn><fn id="Fn3"><label>3</label><p id="Par84"> Figures taken from Table&#x000a0;5 in Ting et al. (<xref ref-type="bibr" rid="CR47">2017</xref>), page 2220. See also Fig.&#x000a0;2, Graph A in Ting et al. (<xref ref-type="bibr" rid="CR47">2017</xref>), wherein it can be seen that for those under 60, the specificity can be boosted to 95% (if the graph is accurate). This would be a less significant drop in specificity.</p></fn><fn id="Fn4"><label>4</label><p id="Par85"> For comparison, in Singapore, the proportion of diabetics who are Chinese is 66.2%, while their referral rate for diabetic retinopathy is 78.6% (higher than would be expected), and for Indians, who represent 13.6% of diabetics in Singapore, their referral rate is 11.7% (Sia et al.&#x000a0;<xref ref-type="bibr" rid="CR42">2020</xref>). Indians are the least likely to be underdiagnosed, and Malays the most likely to be.</p></fn><fn id="Fn5"><label>5</label><p id="Par86"> see Baker (<xref ref-type="bibr" rid="CR3">2019</xref>) and Chia (<xref ref-type="bibr" rid="CR9">2019</xref>) for other examples of discrimination faced by Malays</p></fn><fn id="Fn6"><label>6</label><p id="Par87"> See also Panch et al. (<xref ref-type="bibr" rid="CR38">2019</xref>); Awan (<xref ref-type="bibr" rid="CR2">2023</xref>); Barocas and Selbst (<xref ref-type="bibr" rid="CR4">2016</xref>) for other similarly normatively loaded accounts of bias. Non-normative accounts of bias like Johnson&#x02019;s (<xref ref-type="bibr" rid="CR16">2021</xref>) are not in accord with ordinary usage and are thus not able to ground normative inferences.</p></fn><fn id="Fn7"><label>7</label><p id="Par88"> Even if we attributed the disparity in referrals to bias in the referring physician, using the algorithm would not reduce the disparity. Referring physician bias can be conceived of as occurring when physicians are more likely to discount a positive diagnosis by the grader/AI for Malays than for non-Malays. However, replacing the grader with an AI does not eliminate any bias by the physician (see the Appendix for the detailed argument).</p></fn><fn id="Fn8"><label>8</label><p id="Par89"> This is assuming that using the AI does not result in a disproportionately higher rate of participation in screening for Malay patients. After all, if adoption of AI did increase Malay participation in screening significantly more than it did for non-Malay patients, the gap in referral rates could easily narrow. However, there is little reason to think that this would be the case. The AI at best replaces the grader who is downstream from all the factors which reduce participation in screening.</p></fn><fn id="Fn9"><label>9</label><p id="Par90"> This is no small task, however, since screening is still likely to eat up half of a patient&#x02019;s work day at the least, given that the medication required to dilate the pupils so that images of the retina can be taken impairs vision for four hours after the retina is imaged (and may also cause headaches).</p></fn><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>We would like to acknowledge Associate Professor Daniel Ting of Duke-NUS for his insights and work, which served as the case study for the ethical analysis.</p></ack><notes notes-type="funding-information"><title>Funding</title><p>This work was supported by the Singapore Ministry of Health&#x02019;s National Medical Research Council under its Science, Health, and Policy Relevant Ethics, Singapore (SHAPES) Programme (grant number MOH-000951) and by the National University of Singapore under its NUS Start-up Grant (grant number NUHSRO/2022/035/Startup/05).</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Conflict of Interest</title><p id="Par48">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Agostina</surname><given-names>Larrazabal</given-names></name><name><surname>Nieto</surname><given-names>Nicol&#x000e1;s</given-names></name><name><surname>Peterson</surname><given-names>Victoria</given-names></name></person-group><article-title>Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis</article-title><source>PNAS</source><year>2020</year><volume>117</volume><issue>23</issue><fpage>12592</fpage><lpage>12594</lpage><pub-id pub-id-type="doi">10.1073/pnas.1919012117</pub-id><pub-id pub-id-type="pmid">32457147</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Agostina, Larrazabal, Nicol&#x000e1;s Nieto, and Victoria Peterson. 2020. Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis. <italic>PNAS</italic> 117 (23): 12592&#x02013;12594. 10.1073/pnas.1919012117.<pub-id pub-id-type="pmid">32457147</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><mixed-citation publication-type="other">Awan, Abid Ali. 2023. &#x02018;What is algorithmic bias?&#x02019; <italic>Datacamp</italic>, 17 July 2023. <ext-link ext-link-type="uri" xlink:href="https://www.datacamp.com/blog/what-is-algorithmic-bias">https://www.datacamp.com/blog/what-is-algorithmic-bias</ext-link>.</mixed-citation></ref><ref id="CR3"><mixed-citation publication-type="other">Baker, J.A. 2019. Survey finds rise in perception of work-related discrimination among Malays, Indians in Singapore. <italic>Channel News Asia</italic>. <ext-link ext-link-type="uri" xlink:href="https://www.channelnewsasia.com/singapore/survey-finds-rise-perception-work-related-discrimination-among-malays-indians-singapore-1320146">https://www.channelnewsasia.com/singapore/survey-finds-rise-perception-work-related-discrimination-among-malays-indians-singapore-1320146</ext-link>.&#x000a0;Accessed 23 Dec 2023.</mixed-citation></ref><ref id="CR4"><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Barocas</surname><given-names>Solon</given-names></name><name><surname>Selbst</surname><given-names>Andrew D</given-names></name></person-group><article-title>Big data&#x02019;s disparate impact</article-title><source>California Law Review</source><year>2016</year><volume>104</volume><issue>3</issue><fpage>671</fpage><lpage>732</lpage><pub-id pub-id-type="doi">10.15779/Z38BG31</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Barocas, Solon, and Andrew D. Selbst. 2016. Big data&#x02019;s disparate impact. <italic>California Law Review</italic> 104 (3): 671&#x02013;732. 10.15779/Z38BG31.</mixed-citation></citation-alternatives></ref><ref id="CR5"><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Best</surname><given-names>AL</given-names></name><name><surname>Fletcher</surname><given-names>FE</given-names></name><name><surname>Kadono</surname><given-names>M</given-names></name><name><surname>Warren</surname><given-names>RC</given-names></name></person-group><article-title>Institutional distrust among African Americans and building trustworthiness in the COVID-19 response: Implications for ethical public health practice</article-title><source>Journal of Health Care for the Poor and Underserved</source><year>2021</year><volume>32</volume><issue>1</issue><fpage>90</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1353/hpu.2021.0010</pub-id><pub-id pub-id-type="pmid">33678683</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Best, A.L., F.E. Fletcher, M. Kadono, and R.C. Warren. 2021. Institutional distrust among African Americans and building trustworthiness in the COVID-19 response: Implications for ethical public health practice. <italic>Journal of Health Care for the Poor and Underserved</italic> 32 (1): 90&#x02013;98. 10.1353/hpu.2021.0010.<pub-id pub-id-type="pmid">33678683</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Biller-Adorno</surname><given-names>Nikola</given-names></name><name><surname>Ferrario</surname><given-names>Andrea</given-names></name><name><surname>Joebges</surname><given-names>Susan</given-names></name><name><surname>Krones</surname><given-names>Tanja</given-names></name><name><surname>Massini</surname><given-names>Federico</given-names></name><name><surname>Barth</surname><given-names>Phyllis</given-names></name><name><surname>Arampatzis</surname><given-names>Georgios</given-names></name><name><surname>Krauthammer</surname><given-names>Michael</given-names></name></person-group><article-title>AI support for ethical decision making around resuscitation: Proceed with caution</article-title><source>Journal of Medical Ethics</source><year>2022</year><volume>48</volume><issue>3</issue><fpage>175</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1136/medethics-2020-106786</pub-id><pub-id pub-id-type="pmid">33687916</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Biller-Adorno, Nikola, Andrea Ferrario, Susan Joebges, Tanja Krones, Federico Massini, Phyllis Barth, Georgios Arampatzis, and Michael Krauthammer. 2022. AI support for ethical decision making around resuscitation: Proceed with caution. <italic>Journal of Medical Ethics</italic> 48 (3): 175&#x02013;183.<pub-id pub-id-type="pmid">33687916</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname><given-names>N</given-names></name><name><surname>Islam</surname><given-names>FMA</given-names></name><name><surname>Saw</surname><given-names>SM</given-names></name><name><surname>Shankar</surname><given-names>A</given-names></name><name><surname>de Haseth</surname><given-names>K</given-names></name><name><surname>Mitchell</surname><given-names>P</given-names></name><name><surname>Wong</surname><given-names>TY</given-names></name></person-group><article-title>Distribution and associations of retinal vascular caliber with ethnicity, gender, and birth parameters in young children</article-title><source>Investigative Opthalmology and Visual Science</source><year>2007</year><volume>48</volume><issue>3</issue><fpage>1018</fpage><lpage>1024</lpage><pub-id pub-id-type="doi">10.1167/iovs.06-0978</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Cheung, N., F.M.A. Islam, S.M. Saw, A. Shankar, K. de Haseth, P. Mitchell, and T.Y. Wong. 2007. Distribution and associations of retinal vascular caliber with ethnicity, gender, and birth parameters in young children. <italic>Investigative Opthalmology and Visual Science</italic> 48 (3): 1018&#x02013;1024. 10.1167/iovs.06-0978.</mixed-citation></citation-alternatives></ref><ref id="CR8"><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Chew</surname><given-names>PKH</given-names></name><name><surname>Young</surname><given-names>JL</given-names></name><name><surname>Tan</surname><given-names>GPK</given-names></name></person-group><article-title>Racism and the Pinkerton syndrome in Singapore: Effects of race on hiring decisions</article-title><source>Journal of Pacific Rim Psychology</source><year>2019</year><volume>13</volume><fpage>e16</fpage><pub-id pub-id-type="doi">10.1017/prp.2019.9</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Chew, P.K.H., J.L. Young, and G.P.K. Tan. 2019. Racism and the Pinkerton syndrome in Singapore: Effects of race on hiring decisions. <italic>Journal of Pacific Rim Psychology</italic> 13: e16. 10.1017/prp.2019.9.</mixed-citation></citation-alternatives></ref><ref id="CR9"><mixed-citation publication-type="other">Chia, Rachel G. 2019. Half of all Malays, Indians in Singapore feel discriminated against when applying for jobs: IPS survey. <italic>Business Insider</italic>, 31 July 2019. <ext-link ext-link-type="uri" xlink:href="https://lkyspp.nus.edu.sg/docs/default-source/ips/businessinsider_half-of-all-malays-indians-in-singapore-feel-discriminated-against-when-applying-for-jobs-ips-survey_310719.pdf">https://lkyspp.nus.edu.sg/docs/default-source/ips/businessinsider_half-of-all-malays-indians-in-singapore-feel-discriminated-against-when-applying-for-jobs-ips-survey_310719.pdf</ext-link>. Accessed 5 Oct 2024.</mixed-citation></ref><ref id="CR10"><mixed-citation publication-type="other">Chong, Guan Kwa, and Bak Lim Kua, eds. 2019. <italic>A general history of the Chinese in Singapore</italic>. Singapore: World Scientific. 10.1142/11195.</mixed-citation></ref><ref id="CR11"><mixed-citation publication-type="other">Community Action Network (Singapore). n.d. Health and Racial Discrimination. <ext-link ext-link-type="uri" xlink:href="https://tbinternet.ohchr.org/_layouts/15/TreatyBodyExternal/DownloadDraft.aspx?key=ICEnwWR8rbeJM8O1ALabP3EpMVzXUsy1JQWquKqoRYVcPwI6C5yhh4LWTicblXC38ZGsmA5SQWqqIqcpXX7W8w==">https://tbinternet.ohchr.org/_layouts/15/TreatyBodyExternal/DownloadDraft.aspx?key=ICEnwWR8rbeJM8O1ALabP3EpMVzXUsy1JQWquKqoRYVcPwI6C5yhh4LWTicblXC38ZGsmA5SQWqqIqcpXX7W8w==</ext-link>.&#x000a0;Accessed 5 Oct 2024.</mixed-citation></ref><ref id="CR12"><mixed-citation publication-type="other">Diabetes Singapore. n.d. About Diabetes.&#x000a0;<ext-link ext-link-type="uri" xlink:href="https://www.diabetes.org.sg/about-diabetes/">https://www.diabetes.org.sg/about-diabetes/</ext-link>. Accessed 30 Nov 2023.
</mixed-citation></ref><ref id="CR13"><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Goh</surname><given-names>SA</given-names></name><name><surname>Lee</surname><given-names>JK</given-names></name><name><surname>She</surname><given-names>WY</given-names></name><name><surname>Ho</surname><given-names>EQY</given-names></name><name><surname>Hartman</surname><given-names>M</given-names></name><name><surname>Chou</surname><given-names>C</given-names></name><name><surname>Wong</surname><given-names>ML</given-names></name></person-group><article-title>Multi-level determinants of breast cancer screening among Malay-Muslim women in Singapore: A sequential mixed-methods study</article-title><source>BMC Women&#x02019;s Health</source><year>2022</year><volume>22</volume><fpage>383</fpage><pub-id pub-id-type="doi">10.1186/s12905-022-01972-y</pub-id><pub-id pub-id-type="pmid">36123600</pub-id>
</element-citation><mixed-citation id="mc-CR13" publication-type="journal">Goh, S.A., J.K. Lee, W.Y. She, E.Q.Y. Ho, M. Hartman, C. Chou, and M.L. Wong. 2022. Multi-level determinants of breast cancer screening among Malay-Muslim women in Singapore: A sequential mixed-methods study. <italic>BMC Women&#x02019;s Health</italic> 22: 383. 10.1186/s12905-022-01972-y.<pub-id pub-id-type="pmid">36123600</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR14"><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>OS</given-names></name><name><surname>Tay</surname><given-names>WT</given-names></name><name><surname>Tai</surname><given-names>ES</given-names></name><name><surname>Wang</surname><given-names>JJ</given-names></name><name><surname>Saw</surname><given-names>SM</given-names></name><name><surname>Jeganathan</surname><given-names>VS</given-names></name><name><surname>Sandar</surname><given-names>M</given-names></name><name><surname>Wong</surname><given-names>TY</given-names></name></person-group><article-title>Lack of awareness amongst community patients with diabetes and diabetic retinopathy: The Singapore Malay eye study</article-title><source>Annals of the Academy of Medicine, Singapore</source><year>2009</year><volume>38</volume><issue>12</issue><fpage>1048</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.47102/annals-acadmedsg.V38N12p1048</pub-id><pub-id pub-id-type="pmid">20052439</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Huang, O.S., W.T. Tay, E.S. Tai, J.J. Wang, S.M. Saw, V.S. Jeganathan, M. Sandar, and T.Y. Wong. 2009. Lack of awareness amongst community patients with diabetes and diabetic retinopathy: The Singapore Malay eye study. <italic>Annals of the Academy of Medicine, Singapore</italic> 38 (12): 1048&#x02013;55.<pub-id pub-id-type="pmid">20052439</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><mixed-citation publication-type="other">International Diabetes Federation. 2021. Facts &#x00026; Figures. <ext-link ext-link-type="uri" xlink:href="https://idf.org/about-diabetes/diabetes-facts-figures/">https://idf.org/about-diabetes/diabetes-facts-figures/</ext-link>. Accessed 5 Oct 2024. </mixed-citation></ref><ref id="CR16"><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>Gabbrielle M</given-names></name></person-group><article-title>Algorithmic bias: On the implicit biases of social technology</article-title><source>Synthese</source><year>2021</year><volume>198</volume><issue>10</issue><fpage>9941</fpage><lpage>9961</lpage><pub-id pub-id-type="doi">10.1007/s11229-020-02696-y</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Johnson, Gabbrielle M. 2021. Algorithmic bias: On the implicit biases of social technology. <italic>Synthese</italic> 198 (10): 9941&#x02013;9961. 10.1007/s11229-020-02696-y.</mixed-citation></citation-alternatives></ref><ref id="CR17"><citation-alternatives><element-citation id="ec-CR17" publication-type="book"><person-group person-group-type="author"><name><surname>Kahneman</surname><given-names>Daniel</given-names></name><name><surname>Sibony</surname><given-names>Olivier</given-names></name><name><surname>Sunstein</surname><given-names>Cass R</given-names></name></person-group><source>Noise: A flaw in human judgment</source><year>2021</year><publisher-loc>New York</publisher-loc><publisher-name>Little, Brown Spark</publisher-name></element-citation><mixed-citation id="mc-CR17" publication-type="book">Kahneman, Daniel, Olivier Sibony, and Cass R. Sunstein. 2021. <italic>Noise: A flaw in human judgment</italic>. New York: Little, Brown Spark.</mixed-citation></citation-alternatives></ref><ref id="CR18"><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Kaur-Gill</surname><given-names>Satveer</given-names></name></person-group><article-title>The meanings of heart health among low-income Malay women in Singapore: Narratives of food insecurity, caregiving stressors, and shame</article-title><source>Journal of Applied Communication Research</source><year>2022</year><volume>50</volume><issue>2</issue><fpage>111</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1080/00909882.2022.2033298</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Kaur-Gill, Satveer. 2022. The meanings of heart health among low-income Malay women in Singapore: Narratives of food insecurity, caregiving stressors, and shame. <italic>Journal of Applied Communication Research</italic> 50 (2): 111&#x02013;128. 10.1080/00909882.2022.2033298.</mixed-citation></citation-alternatives></ref><ref id="CR19"><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Kempt</surname><given-names>Hendrik</given-names></name><name><surname>Nagel</surname><given-names>Saskia K</given-names></name></person-group><article-title>Responsibility, second opinions and peer disagreement: Ethical and epistemological challenges of using AI in clinical diagnostic contexts</article-title><source>Journal of Medical Ethics</source><year>2022</year><volume>48</volume><issue>4</issue><fpage>222</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1136/medethics-2021-107440</pub-id><pub-id pub-id-type="pmid">34907006</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Kempt, Hendrik, and Saskia K. Nagel. 2022. Responsibility, second opinions and peer disagreement: Ethical and epistemological challenges of using AI in clinical diagnostic contexts. <italic>Journal of Medical Ethics</italic> 48 (4): 222&#x02013;229. 10.1136/medethics-2021-107440.<pub-id pub-id-type="pmid">34907006</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Kleinberg</surname><given-names>Jon</given-names></name><name><surname>Ludwig</surname><given-names>Jens</given-names></name><name><surname>Mullainathan</surname><given-names>Sendhil</given-names></name><name><surname>Sunstein</surname><given-names>Cass R</given-names></name></person-group><article-title>Discrimination in the age of algorithms</article-title><source>Journal of Legal Analysis</source><year>2019</year><volume>10</volume><fpage>113</fpage><lpage>174</lpage><pub-id pub-id-type="doi">10.1093/jla/laz001</pub-id></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Kleinberg, Jon, Jens Ludwig, Sendhil Mullainathan, and Cass R. Sunstein. 2019. Discrimination in the age of algorithms. <italic>Journal of Legal Analysis</italic> 10: 113&#x02013;174. 10.1093/jla/laz001.</mixed-citation></citation-alternatives></ref><ref id="CR21"><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Kordzadeh</surname><given-names>N</given-names></name><name><surname>Ghasemaghaei</surname><given-names>M</given-names></name></person-group><article-title>Algorithmic bias: Review, synthesis, and future research directions</article-title><source>European Journal of Information Systems</source><year>2022</year><volume>31</volume><fpage>388</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1080/0960085X.2021.1927212]</pub-id></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Kordzadeh, N., and M. Ghasemaghaei. 2022. Algorithmic bias: Review, synthesis, and future research directions. <italic>European Journal of Information Systems</italic> 31: 388&#x02013;409. 10.1080/0960085X.2021.1927212].</mixed-citation></citation-alternatives></ref><ref id="CR22"><mixed-citation publication-type="other">Leslie, David, Anjali Mazumder, Aidan Peppin, Maria K. Wolters, and Alexa Hagerty. 2021. Does &#x0201c;AI&#x0201d; stand for augmenting inequality in the era of healthcare? <italic>BMJ</italic> 372: n304. 10.1136/bmj.n304.</mixed-citation></ref><ref id="CR23"><mixed-citation publication-type="other">Lim, Vanessa. 2022. What is the cost of diabetes? A look at the economic impact of the disease on patients. <italic>Channel News Asia</italic>, 13 November 2022. <ext-link ext-link-type="uri" xlink:href="https://www.channelnewsasia.com/singapore/diabetes-cost-patients-medication-treatment-checkups-insurance-3060436">https://www.channelnewsasia.com/singapore/diabetes-cost-patients-medication-treatment-checkups-insurance-3060436</ext-link>.&#x000a0;Accessed 6 Oct 2024.</mixed-citation></ref><ref id="CR24"><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>RBT</given-names></name><name><surname>Zheng</surname><given-names>H</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name><name><surname>Cook</surname><given-names>AR</given-names></name><name><surname>Chia</surname><given-names>KS</given-names></name><name><surname>Lim</surname><given-names>WY</given-names></name></person-group><article-title>Ethnic and gender specific life expectancies of the Singapore population, 1965 to 2009&#x02013;Converging, or diverging?</article-title><source>BMC Public Health</source><year>2013</year><volume>13</volume><fpage>1012</fpage><pub-id pub-id-type="doi">10.1186/1471-2458-13-1012</pub-id><pub-id pub-id-type="pmid">24160733</pub-id>
</element-citation><mixed-citation id="mc-CR24" publication-type="journal">Lim, Raymond Boon Tar, Huili Zheng, Qian Yang, Alex Richard Cook, Kee Seng Chia, and Wei Yen Lim. 2013. Ethnic and gender specific life expectancies of the Singapore population, 1965 to 2009&#x02013;Converging, or diverging? <italic>BMC Public Health</italic> 13: 1012. 10.1186/1471-2458-13-1012.<pub-id pub-id-type="pmid">24160733</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR25"><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Lippert-Rasmussen</surname><given-names>Kasper</given-names></name></person-group><article-title>Using (un)fair algorithms in an unjust world</article-title><source>Res Publica</source><year>2023</year><volume>29</volume><fpage>283</fpage><lpage>302</lpage><pub-id pub-id-type="doi">10.1007/s11158-022-09558-z</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Lippert-Rasmussen, Kasper. 2023. Using (un)fair algorithms in an unjust world. <italic>Res Publica</italic> 29: 283&#x02013;302. 10.1007/s11158-022-09558-z.</mixed-citation></citation-alternatives></ref><ref id="CR26"><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>E</given-names></name><name><surname>Garc&#x000ed;a-Closas</surname><given-names>M</given-names></name><name><surname>Chanock</surname><given-names>SJ</given-names></name><etal/></person-group><article-title>The case for increasing diversity in tissue-based functional genomics datasets to understand human disease susceptibility</article-title><source>Nature Communications</source><year>2022</year><volume>13</volume><fpage>2907</fpage><pub-id pub-id-type="doi">10.1038/s41467-022-30650-8</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Long, E., M. Garc&#x000ed;a-Closas, S.J. Chanock, et al. 2022. The case for increasing diversity in tissue-based functional genomics datasets to understand human disease susceptibility. <italic>Nature Communications</italic> 13: 2907. 10.1038/s41467-022-30650-8.</mixed-citation></citation-alternatives></ref><ref id="CR27"><mixed-citation publication-type="other">Low, Cheryl-Ann Mei Gek. 2004. Singapore from the 14th to 19th century. In <italic>Early Singapore 1300s&#x02013;1819: Evidence in maps, text and artefacts,</italic> edited by John N. Miksic and Cheryl-Ann Mei Gek Low, 15. Singapore: Singapore History Museum.</mixed-citation></ref><ref id="CR28"><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Lum</surname><given-names>Kristian</given-names></name><name><surname>Isaac</surname><given-names>William</given-names></name></person-group><article-title>To predict and serve?</article-title><source>Significance</source><year>2016</year><volume>13</volume><issue>5</issue><fpage>14</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1111/j.1740-9713.2016.00960.x</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Lum, Kristian, and William Isaac. 2016. To predict and serve? <italic>Significance</italic> 13 (5): 14&#x02013;19. 10.1111/j.1740-9713.2016.00960.x.</mixed-citation></citation-alternatives></ref><ref id="CR29"><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Marjanovic</surname><given-names>Olivera</given-names></name><name><surname>Cecez-Kecmanovic</surname><given-names>Dubravka</given-names></name><name><surname>Vidgen</surname><given-names>Richard</given-names></name></person-group><article-title>Theorising algorithmic justice</article-title><source>European Journal of Information Systems</source><year>2022</year><volume>31</volume><issue>3</issue><fpage>269</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1080/0960085X.2021.1934130</pub-id></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Marjanovic, Olivera, Dubravka Cecez-Kecmanovic, and Richard Vidgen. 2022. Theorising algorithmic justice. <italic>European Journal of Information Systems</italic> 31 (3): 269&#x02013;287. 10.1080/0960085X.2021.1934130.</mixed-citation></citation-alternatives></ref><ref id="CR30"><mixed-citation publication-type="other">Mayo Clinic Staff. 2023. Diabetic retinopathy. <italic>Mayo Clinic</italic>, 21 February 2023.&#x000a0;<ext-link ext-link-type="uri" xlink:href="https://www.mayoclinic.org/diseases-conditions/diabetic-retinopathy/symptoms-causes/syc-20371611">https://www.mayoclinic.org/diseases-conditions/diabetic-retinopathy/symptoms-causes/syc-20371611</ext-link>.&#x000a0;Accessed 5 Oct 2024. </mixed-citation></ref><ref id="CR31"><mixed-citation publication-type="other">Ministry of Health. 2020. National Population Health Survey Report 2020: Household Interview andHealth Examination.&#x000a0;<ext-link ext-link-type="uri" xlink:href="https://www.moh.gov.sg/docs/librariesprovider5/default-document-library/nphs-2020-survey-report.pdf">https://www.moh.gov.sg/docs/librariesprovider5/default-document-library/nphs-2020-survey-report.pdf</ext-link>.&#x000a0;Accessed 6 Oct 2024. </mixed-citation></ref><ref id="CR32"><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Mitchell</surname><given-names>Shira</given-names></name><name><surname>Potash</surname><given-names>Eric</given-names></name><name><surname>Barocas</surname><given-names>Solon</given-names></name><name><surname>D&#x02019;Amour</surname><given-names>Alexander</given-names></name><name><surname>Lum</surname><given-names>Kristian</given-names></name></person-group><article-title>Algorithmic fairness: Choices, assumptions, and definitions</article-title><source>Annual Review of Statistics and Its Application</source><year>2021</year><volume>8</volume><issue>1</issue><fpage>141</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1146/annurev-statistics-042720-125902</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Mitchell, Shira, Eric Potash, Solon Barocas, Alexander D&#x02019;Amour, and Kristian Lum. 2021. Algorithmic fairness: Choices, assumptions, and definitions. <italic>Annual Review of Statistics and Its Application</italic> 8 (1): 141&#x02013;163. 10.1146/annurev-statistics-042720-125902.</mixed-citation></citation-alternatives></ref><ref id="CR33"><mixed-citation publication-type="other">Mittelstadt, Brent D., Patrick Allo, Mariarosaria Taddeo, Sandra Wachter, and Luciano Floridi. 2016. The ethics of algorithms: Mapping the debate. <italic>Big Data &#x00026; Society</italic> 3 (2): 2053951716679679. 10.1177/2053951716679679.</mixed-citation></ref><ref id="CR34"><mixed-citation publication-type="other">Mittelstadt, Brent D., Sandra Wachter, and Chris Russell. 2023 The unfairness of fair machine learning: Levelling down and strict egalitarianism by default. <italic>ArXiv</italic> 2302.02404. 10.48550/arXiv.2302.02404. 
</mixed-citation></ref><ref id="CR35"><mixed-citation publication-type="other">Mohammad, Nabilah. 2021. <italic>Food delivery workers: Riding the waves of uncertainty</italic>. Singapore: Centre for Research on Islamic and Malay Affairs (RIMA). <ext-link ext-link-type="uri" xlink:href="https://rima.sg/wp-content/uploads/2021/12/Food-Delivery-Workers-Riding-the-Waves-of-Uncertainty.pdf">https://rima.sg/wp-content/uploads/2021/12/Food-Delivery-Workers-Riding-the-Waves-of-Uncertainty.pdf</ext-link>.&#x000a0;Accessed 6 Oct 2024.</mixed-citation></ref><ref id="CR36"><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name><surname>Mosenifar</surname><given-names>Z</given-names></name></person-group><article-title>Population issues in clinical trials</article-title><source>Proceedings of the American Thoracic Society</source><year>2007</year><volume>4</volume><issue>2</issue><fpage>185</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1513/pats.200701-009GC</pub-id><pub-id pub-id-type="pmid">17494729</pub-id>
</element-citation><mixed-citation id="mc-CR36" publication-type="journal">Mosenifar, Z. 2007. Population issues in clinical trials. <italic>Proceedings of the American Thoracic Society</italic> 4 (2): 185&#x02013;7. 10.1513/pats.200701-009GC.<pub-id pub-id-type="pmid">17494729</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR37"><mixed-citation publication-type="other">O&#x02019;Neil, Cathy. 2017. <italic>Weapons of math destruction: How big data increases inequality and threatens democracy</italic>. New York, NY: Penguin Press.</mixed-citation></ref><ref id="CR38"><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Panch</surname><given-names>T</given-names></name><name><surname>Mattie</surname><given-names>H</given-names></name><name><surname>Atun</surname><given-names>R</given-names></name></person-group><article-title>Artificial intelligence and algorithmic bias: Implications for health systems</article-title><source>Journal of Global Health</source><year>2019</year><volume>9</volume><issue>2</issue><fpage>020318</fpage><pub-id pub-id-type="doi">10.7189/jogh.09.020318</pub-id><pub-id pub-id-type="pmid">31788229</pub-id>
</element-citation><mixed-citation id="mc-CR38" publication-type="journal">Panch, T., H. Mattie, and R. Atun. 2019. Artificial intelligence and algorithmic bias: Implications for health systems. <italic>Journal of Global Health</italic> 9 (2): 020318. 10.7189/jogh.09.020318.<pub-id pub-id-type="pmid">31788229</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR39"><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>Eike</given-names></name><name><surname>Holm</surname><given-names>Sune</given-names></name><name><surname>Ganz</surname><given-names>Melanie</given-names></name><name><surname>Feragen</surname><given-names>Aasa</given-names></name></person-group><article-title>The path toward equal performance in medical machine learning</article-title><source>Patterns</source><year>2023</year><volume>4</volume><issue>7</issue><fpage>100790</fpage><pub-id pub-id-type="doi">10.1016/j.patter.2023.100790</pub-id><pub-id pub-id-type="pmid">37521051</pub-id>
</element-citation><mixed-citation id="mc-CR39" publication-type="journal">Petersen, Eike, Sune Holm, Melanie Ganz, and Aasa Feragen. 2023. The path toward equal performance in medical machine learning. <italic>Patterns</italic> 4 (7): 100790. 10.1016/j.patter.2023.100790.<pub-id pub-id-type="pmid">37521051</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR40"><mixed-citation publication-type="other">Raphling, John. 2018. Criminalizing homelessness violates basic human rights. <italic>Human Rights Watch</italic>, 5 July 2018. <ext-link ext-link-type="uri" xlink:href="https://www.hrw.org/news/2018/07/05/criminalizing-homelessness-violates-basic-human-rights">https://www.hrw.org/news/2018/07/05/criminalizing-homelessness-violates-basic-human-rights</ext-link>.&#x000a0;Accessed 6 Oct 2024.</mixed-citation></ref><ref id="CR41"><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Rochtchina</surname><given-names>E</given-names></name><name><surname>Wang</surname><given-names>JJ</given-names></name><name><surname>Taylor</surname><given-names>B</given-names></name><name><surname>Wong</surname><given-names>TY</given-names></name><name><surname>Mitchell</surname><given-names>P</given-names></name></person-group><article-title>Ethnic variability in retinal vessel caliber: A potential source of measurement error from ocular pigmentation?&#x02013;The Sydney Childhood Eye Study</article-title><source>Investigative Opthalmology &#x00026; Visual Science</source><year>2008</year><volume>49</volume><issue>4</issue><fpage>1362</fpage><lpage>1366</lpage><pub-id pub-id-type="doi">10.1167/iovs.07-0150</pub-id></element-citation><mixed-citation id="mc-CR41" publication-type="journal">Rochtchina, E., J.J. Wang, B. Taylor, T.Y. Wong, and P. Mitchell. 2008. Ethnic variability in retinal vessel caliber: A potential source of measurement error from ocular pigmentation?&#x02013;The Sydney Childhood Eye Study. <italic>Investigative Opthalmology &#x00026; Visual Science</italic> 49 (4): 1362&#x02013;1366. 10.1167/iovs.07-0150.</mixed-citation></citation-alternatives></ref><ref id="CR42"><mixed-citation publication-type="other">Sia, J.T., A.T.L. Gan, B.L.P. Soh, E. Fenwick, J. Quah, T. Sahil, Y. Tao, N.C. Tan, C. Sabanayagam, E.L. Lamoureux, and R.E.K. Man. 2020. Rates and predictors of nonadherence to postophthalmic screening tertiary referrals in patients with type 2 diabetes. <italic>Translational Vision Science and Technology</italic> 9(6): 15. 10.1167/tvst.9.6.15. 
</mixed-citation></ref><ref id="CR43"><mixed-citation publication-type="other">Singhealth. 2017. Kidney Failure Rates Soar Among Malays Here. <ext-link ext-link-type="uri" xlink:href="https://www.singhealth.com.sg/news/others/kidney-failure-rates-soar-among-malays">https://www.singhealth.com.sg/news/others/kidney-failure-rates-soar-among-malays</ext-link>. 
</mixed-citation></ref><ref id="CR44"><mixed-citation publication-type="other">Sulaiman, Yusef. 2022. 2020 Census &#x02013; A community perspective. <italic>The Karyawan</italic>, 14 January 2022. <ext-link ext-link-type="uri" xlink:href="https://karyawan.sg/2020-census-a-community-perspective/">https://karyawan.sg/2020-census-a-community-perspective/</ext-link>.&#x000a0;Accessed 6 Oct 2024.</mixed-citation></ref><ref id="CR45"><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Sunstein</surname><given-names>Cass R</given-names></name></person-group><article-title>Governing by algorithm? No noise and (potentially) less bias</article-title><source>Duke Law Journal</source><year>2021</year><volume>71</volume><issue>6</issue><fpage>1175</fpage><lpage>1205</lpage><pub-id pub-id-type="doi">10.2139/ssrn.3925240</pub-id></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Sunstein, Cass R. 2021. Governing by algorithm? No noise and (potentially) less bias. <italic>Duke Law Journal</italic> 71 (6): 1175&#x02013;1205. 10.2139/ssrn.3925240.</mixed-citation></citation-alternatives></ref><ref id="CR46"><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Taber</surname><given-names>Jennifer M</given-names></name><name><surname>Leyva</surname><given-names>Bryan</given-names></name><name><surname>Persoskie</surname><given-names>Alexander</given-names></name></person-group><article-title>Why do people avoid medical care? A qualitative study using national data</article-title><source>Journal of General Internal Medicine</source><year>2015</year><volume>30</volume><issue>3</issue><fpage>290</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1007/s11606-014-3089-1</pub-id><pub-id pub-id-type="pmid">25387439</pub-id>
</element-citation><mixed-citation id="mc-CR46" publication-type="journal">Taber, Jennifer M., Bryan Leyva, and Alexander Persoskie. 2015. Why do people avoid medical care? A qualitative study using national data. <italic>Journal of General Internal Medicine</italic> 30 (3): 290&#x02013;297. 10.1007/s11606-014-3089-1.<pub-id pub-id-type="pmid">25387439</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR47"><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Ting</surname><given-names>DSW</given-names></name><name><surname>Cheung</surname><given-names>CYL</given-names></name><name><surname>Lim</surname><given-names>G</given-names></name><name><surname>Tan</surname><given-names>GSW</given-names></name><name><surname>Quang</surname><given-names>ND</given-names></name><name><surname>Gan</surname><given-names>A</given-names></name><name><surname>Hamzah</surname><given-names>H</given-names></name><name><surname>Garcia-Franco</surname><given-names>R</given-names></name><name><surname>Yeo</surname><given-names>IYS</given-names></name><name><surname>Lee</surname><given-names>SY</given-names></name><name><surname>Wong</surname><given-names>EYM</given-names></name><name><surname>Sabanayagam</surname><given-names>C</given-names></name><name><surname>Baskaran</surname><given-names>M</given-names></name><name><surname>Ibrahim</surname><given-names>F</given-names></name><name><surname>Tan</surname><given-names>NC</given-names></name><name><surname>Finkelstein</surname><given-names>EA</given-names></name><name><surname>Lamoreux</surname><given-names>EL</given-names></name><name><surname>Wong</surname><given-names>IY</given-names></name><name><surname>Bressler</surname><given-names>NM</given-names></name><name><surname>Sivaprasad</surname><given-names>S</given-names></name><name><surname>Varma</surname><given-names>R</given-names></name><name><surname>Jonas</surname><given-names>JB</given-names></name><name><surname>He</surname><given-names>MG</given-names></name><name><surname>Cheng</surname><given-names>CY</given-names></name><name><surname>Cheung</surname><given-names>GCM</given-names></name><name><surname>Aung</surname><given-names>T</given-names></name><name><surname>Hsu</surname><given-names>W</given-names></name><name><surname>Lee</surname><given-names>ML</given-names></name><name><surname>Wong</surname><given-names>TY</given-names></name></person-group><article-title>Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes</article-title><source>Journal of the American Medical Association</source><year>2017</year><volume>318</volume><issue>22</issue><fpage>2211</fpage><lpage>2223</lpage><pub-id pub-id-type="doi">10.1001/jama.2017.18152</pub-id><pub-id pub-id-type="pmid">29234807</pub-id>
</element-citation><mixed-citation id="mc-CR47" publication-type="journal">Ting, D.S.W., C.Y.L. Cheung, G. Lim, G.S.W. Tan, N.D. Quang, A. Gan, H. Hamzah, R. Garcia-Franco, I.Y.S. Yeo, S.Y. Lee, E.Y.M. Wong, C. Sabanayagam, M. Baskaran, F. Ibrahim, N.C. Tan, E.A. Finkelstein, E.L. Lamoreux, I.Y. Wong, N.M. Bressler, S. Sivaprasad, R. Varma, J.B. Jonas, M.G. He, C.Y. Cheng, G.C.M. Cheung, T. Aung, W. Hsu, M.L. Lee, and T.Y. Wong. 2017. Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. <italic>Journal of the American Medical Association</italic> 318 (22): 2211&#x02013;2223. 10.1001/jama.2017.18152.<pub-id pub-id-type="pmid">29234807</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR48"><mixed-citation publication-type="other">Wong, Pei Ting. 2021. New workgroup to improve health of Malays and Indians, close &#x02018;significant disparities&#x02019; in outcomes across ethnicities: MOH. <italic>Today</italic>, 5 March 2021. <ext-link ext-link-type="uri" xlink:href="https://www.todayonline.com/singapore/new-workgroup-improve-health-malays-and-indians-close-significant-disparities-outcomes">https://www.todayonline.com/singapore/new-workgroup-improve-health-malays-and-indians-close-significant-disparities-outcomes</ext-link>. 
</mixed-citation></ref><ref id="CR49"><mixed-citation publication-type="other">WHO. 2023. Diabetes. <italic>World Health Organization</italic>, 5 April 2023. <ext-link ext-link-type="uri" xlink:href="https://www.who.int/news-room/fact-sheets/detail/diabetes">https://www.who.int/news-room/fact-sheets/detail/diabetes</ext-link>.&#x000a0;Accessed 5 Oct 2024.</mixed-citation></ref><ref id="CR50"><mixed-citation publication-type="other">WHO. n.d. Health Equity. <ext-link ext-link-type="uri" xlink:href="https://www.who.int/health-topics/health-equity#tab=tab_1">https://www.who.int/health-topics/health-equity#tab=tab_1</ext-link>.&#x000a0;Accessed 6 Oct 2024.</mixed-citation></ref><ref id="CR51"><mixed-citation publication-type="other">Wiencek, Erica. 2023. The need for providing culturally sensitive health care: Be an advocate. <italic>Journal of Diagnostic Medical Sonography</italic> 40(2): 125&#x02013;126. 10.1177/87564793231187418. 
</mixed-citation></ref><ref id="CR52"><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name><surname>Zainal</surname><given-names>H</given-names></name><name><surname>Masud</surname><given-names>DM</given-names></name><name><surname>Mohamed</surname><given-names>Nasir K</given-names></name></person-group><article-title>Singaporean Malay-Muslim Women&#x02019;s lifestyle habits and attitudes towards health</article-title><source>Contemporary Islam</source><year>2021</year><volume>15</volume><issue>3</issue><fpage>287</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1007/s11562-021-00472-4</pub-id></element-citation><mixed-citation id="mc-CR52" publication-type="journal">Zainal, H., D.M. Masud, and Nasir K. Mohamed. 2021. Singaporean Malay-Muslim Women&#x02019;s lifestyle habits and attitudes towards health. <italic>Contemporary Islam</italic> 15 (3): 287&#x02013;305. 10.1007/s11562-021-00472-4.</mixed-citation></citation-alternatives></ref><ref id="CR53"><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Lamoureux</surname><given-names>EL</given-names></name><name><surname>Chiang</surname><given-names>PCP</given-names></name><etal/></person-group><article-title>Language barrier and its relationship to diabetes and diabetic retinopathy</article-title><source>BMC Public Health</source><year>2012</year><volume>12</volume><fpage>781</fpage><pub-id pub-id-type="doi">10.1186/1471-2458-12-781</pub-id><pub-id pub-id-type="pmid">22974298</pub-id>
</element-citation><mixed-citation id="mc-CR53" publication-type="journal">Zheng, Y., E.L. Lamoureux, P.C.P. Chiang, et al. 2012. Language barrier and its relationship to diabetes and diabetic retinopathy. <italic>BMC Public Health</italic> 12: 781. 10.1186/1471-2458-12-781.<pub-id pub-id-type="pmid">22974298</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>