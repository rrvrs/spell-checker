<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="editorial"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1662-4548</issn><issn pub-type="epub">1662-453X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC12098266</article-id><article-id pub-id-type="doi">10.3389/fnins.2025.1615276</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Editorial</subject></subj-group></subj-group></article-categories><title-group><article-title>Editorial: Advances in computer vision: from deep learning models to practical applications</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhu</surname><given-names>Hancheng</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/1899365/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>Yao</surname><given-names>Rui</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/2115051/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Tang</surname><given-names>Lu</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/1780992/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>School of Computer Science and Technology, China University of Mining and Technology</institution>, <addr-line>Xuzhou</addr-line>, <country>China</country></aff><aff id="aff2"><sup>2</sup><institution>Xuzhou Medical University</institution>, <addr-line>Xuzhou</addr-line>, <country>China</country></aff><author-notes><fn fn-type="edited-by"><p>Edited and reviewed by: Yi Bao, Stevens Institute of Technology, United States</p></fn><corresp id="c001">*Correspondence: Lu Tang <email>xztanglu@xzhmu.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>09</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>19</volume><elocation-id>1615276</elocation-id><history><date date-type="received"><day>21</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>25</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025 Zhu, Yao and Tang.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Zhu, Yao and Tang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><related-article related-article-type="commentary-article" id="RA1" xlink:href="https://www.frontiersin.org/research-topics/62105/advances-in-computer-vision-from-deep-learning-models-to-practical-applications" ext-link-type="uri">Editorial on the Research Topic <article-title>Advances in computer vision: from deep learning models to practical applications</article-title></related-article><kwd-group><kwd>computer vision</kwd><kwd>deep learning models</kwd><kwd>practical applications</kwd><kwd>lightweight network</kwd><kwd>medical image analysis and processing system</kwd></kwd-group><counts><fig-count count="0"/><table-count count="0"/><equation-count count="0"/><ref-count count="0"/><page-count count="3"/><word-count count="1460"/></counts><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Visual Neuroscience</meta-value></custom-meta></custom-meta-group></article-meta></front><body><p>Computer vision has emerged as one of the most transformative areas of artificial intelligence, with deep learning models driving unprecedented advancements in both theoretical understanding and practical applications. Over the past decade, the rapid development of deep learning techniques has enabled machines to perform tasks such as image recognition, object detection, and video analysis with remarkable accuracy and efficiency. However, as the field continues to evolve, there is a growing need to bridge the gap between theoretical models and real-world applications to ensure that these technologies are powerful but also practical, efficient, and scalable. This Research Topic, &#x0201c;<italic>Advances in computer vision: from deep learning models to practical applications</italic>,&#x0201d; is dedicated to exploring the latest innovations in computer vision that are addressing these challenges and pushing the boundaries of what is achievable.</p><p>The articles in this Research Topic represent a diverse range of research directions and applications, reflecting the interdisciplinary nature of computer vision. From efficient single-image super-resolution techniques to lightweight network architectures for traffic sign recognition, and from medical image processing to action recognition in autonomous systems, the contributions highlight the versatility, and potential of computer vision technologies. Below, we provide a brief overview of the accepted articles, emphasizing their key contributions and practical implications.</p><sec id="s1"><title>Efficient and lightweight deep learning models for real-world applications</title><p>One of the central themes of this Research Topic is the development of efficient and lightweight deep learning models that can operate effectively in resource-constrained environments. <ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1431033" ext-link-type="uri">An et al.</ext-link> presented a lightweight network architecture based on an enhanced LeNet-5 model for traffic sign recognition. By optimizing the network structure and reducing the number of parameters, they achieved state-of-the-art performance on standard benchmarks, making their solution suitable for deployment in real-world autonomous driving systems.</p><p><ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1502499" ext-link-type="uri">Qu and Ke</ext-link> proposed an asymmetric large kernel distillation network for single image super-resolution, which leveraged asymmetric kernels to achieve high computational efficiency while maintaining superior performance in image restoration. Their approach demonstrated the importance of balancing model complexity and practical applicability, particularly in scenarios where computational resources are limited.</p><p><ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1471089" ext-link-type="uri">Xie et al.</ext-link> proposed an extremely lightweight pathological myopia instance segmentation method (SMLS-YOLO) that combined attention mechanisms with efficient network design to achieve real-time performance. Their approach was particularly valuable for applications in ophthalmology, where rapid and accurate segmentation is critical for diagnosing and monitoring conditions such as pathological myopia. The integration of attention mechanisms into lightweight models highlighted the importance of optimizing both computational efficiency and accuracy to ensure that these technologies can be deployed in real-world settings.</p></sec><sec id="s2"><title>Deep learning in medical image processing</title><p>Medical image processing is another area where deep learning has shown tremendous potential. <ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1441791" ext-link-type="uri">Li L. et al.</ext-link> explored the use of Swin Transformer-based automatic delineation of the hippocampus in MRI scans for hippocampus-sparing whole-brain radiotherapy. Their work showcased the effectiveness of transformer-based architectures in medical image segmentation, providing a more accurate and automated approach to treatment planning.</p><p><ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1415679" ext-link-type="uri">Tian and Zhang</ext-link> presented a GAN-guided nuance perceptual attention network (G2NPAN) for multimodal medical fusion image quality assessment. Their work combined generative adversarial networks (GANs) with attention mechanisms to evaluate the quality of fused medical images, ensuring that the outputs were both visually appealing and diagnostically useful. This approach highlighted the importance of integrating advanced deep learning techniques with practical applications in healthcare, where image quality and interpretability are critical.</p><p><ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1448294" ext-link-type="uri">Zhou et al.</ext-link> focused on lymph node segmentation in lung cancer diagnosis, introducing a dual-stream feature-fusion attention U-Net (DFA-UNet). By incorporating attention mechanisms into the U-Net architecture, they achieved improved segmentation accuracy and computational efficiency, which are essential for clinical applications where time and resource constraints are significant.</p><p><ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1370024" ext-link-type="uri">Gu et al.</ext-link> proposed a motion-sensitive network for action recognition in autonomous systems, leveraging insights from motion perception to improve decision-making in real-time control scenarios. Their work underscored the importance of designing models that can efficiently process dynamic visual inputs, which have direct applications in robotics and autonomous vehicles.</p></sec><sec id="s3"><title>Practical applications and beyond</title><p>The practical applications of computer vision are vast and varied, ranging from culture to education to security systems. <ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1362567" ext-link-type="uri">Ramesh et al.</ext-link> presented a hybrid manifold smoothing and label propagation technique for handwritten Kannada character recognition, demonstrating how advanced deep learning methods can be adapted for tasks involving handwritten text. Their work had implications for document analysis, OCR systems, and cultural heritage preservation, where handwritten text recognition remains a challenging yet important task.</p><p><ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1482735" ext-link-type="uri">Li X. et al.</ext-link> proposed a parameter-dense three-dimensional convolution residual network for classroom teaching applications. By incorporating dense 3D convolutions, they demonstrated improved performance in handling complex, multidimensional data.</p><p>The work by <ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1449527" ext-link-type="uri">Wang</ext-link> on attention-enhanced computation in multimedia affective computing explored how attention mechanisms can be used to evaluate and analyze visual perception, particularly in the context of affective computing. By integrating attention-based models, Wang demonstrated how visual perception can be quantified and optimized for applications such as emotion recognition and human-computer interaction.</p><p><ext-link xlink:href="https://doi.org/10.3389/fnins.2024.1362286" ext-link-type="uri">Ma et al.</ext-link> proposed a study on face anti-spoofing based on pseudo-negative feature generation, addressing the critical issue of ensuring security and reliability in facial recognition systems. By generating pseudo-negative features to enhance robustness against spoofing attacks, their work contributed to the development of more secure and trustworthy biometric systems. This is particularly relevant in practical applications where facial recognition is increasingly used for authentication and access control.</p><p>This Research Topic, &#x0201c;<italic>Advances in computer vision: from deep learning models to practical applications</italic>,&#x0201d; reflects the current state of the field and its trajectory toward solving real-world problems. Collectively, the articles demonstrate how deep learning models can be optimized for efficiency, scalability, and practicality while maintaining high performance in diverse applications. From lightweight architectures for traffic sign recognition and super-resolution imaging to advanced attention mechanisms for medical image segmentation and action recognition, these contributions highlight the interdisciplinary nature of computer vision and its potential to revolutionize diverse domains.</p><p>This Research Topic of articles inspires further research and collaboration between researchers, practitioners, and industry. The integration of deep learning with practical applications not only enhances the functionality of computer vision systems but also ensures their relevance and usability in addressing real-world challenges. As the field continues to grow, we anticipate even more exciting developments that will bridge the gap between theoretical models and practical deployment, paving the way for smarter, more efficient, and more reliable vision technologies.</p><p>Finally, we would like to extend our gratitude to all the authors for their insightful contributions and to the reviewers for their valuable feedback. This Research Topic will serve as a valuable resource for researchers and practitioners alike, fostering innovation and advancing the field of computer vision toward practical and impactful applications.</p></sec></body><back><sec sec-type="author-contributions" id="s4"><title>Author contributions</title><p>HZ: Writing &#x02013; original draft. RY: Writing &#x02013; review &#x00026; editing. LT: Writing &#x02013; review &#x00026; editing.</p></sec><sec sec-type="COI-statement" id="conf1"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="disclaimer" id="s5"><title>Publisher's note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec></back></article>