<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40394076</article-id><article-id pub-id-type="pmc">PMC12092738</article-id>
<article-id pub-id-type="publisher-id">2111</article-id><article-id pub-id-type="doi">10.1038/s41598-025-02111-x</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Real-time driver drowsiness detection using transformer architectures: a novel deep learning approach</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hassan</surname><given-names>Osama F.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Ibrahim</surname><given-names>Ahmed F.</given-names></name><address><email>ahmed.farouk@ksiu.edu.eg</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Gomaa</surname><given-names>Ahmed</given-names></name><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Makhlouf</surname><given-names>M. A.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hafiz</surname><given-names>B.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02m82p074</institution-id><institution-id institution-id-type="GRID">grid.33003.33</institution-id><institution-id institution-id-type="ISNI">0000 0000 9889 5690</institution-id><institution>Information Systems Department, Faculty of Computers and Informatics, </institution><institution>Suez Canal University, </institution></institution-wrap>Ismailia, 41522 Egypt </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04gj69425</institution-id><institution>Artificial Intelligence Department, Faculty of Computer Science and Engineering, </institution><institution>King Salman International University (KSIU), </institution></institution-wrap>South Sinai, 46511 Egypt </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02x66tk73</institution-id><institution-id institution-id-type="GRID">grid.440864.a</institution-id><institution-id institution-id-type="ISNI">0000 0004 5373 6441</institution-id><institution>Computer Science and Engineering Department, </institution><institution>Egypt Japan University of Science and Technology (E-JUST), </institution></institution-wrap>Alexandria, 21934 Egypt </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01cb2rv04</institution-id><institution-id institution-id-type="GRID">grid.459886.e</institution-id><institution-id institution-id-type="ISNI">0000 0000 9905 739X</institution-id><institution>Computer Science and Engineering Department, </institution><institution>National Research Institute of Astronomy and Geophysics (NRIAG), </institution></institution-wrap>Helwan, 11731 Egypt </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><elocation-id>17493</elocation-id><history><date date-type="received"><day>6</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>12</day><month>5</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Driver drowsiness is a leading cause of road accidents, resulting in significant societal, economic, and emotional losses. This paper introduces a novel and robust deep learning-based framework for real-time driver drowsiness detection, leveraging state-of-the-art transformer architectures and transfer learning models to achieve unprecedented accuracy and reliability. The proposed methodology addresses key challenges in drowsiness detection by integrating advanced data preprocessing techniques, including image normalization, augmentation, and region-of-interest selection using Haar Cascade classifiers. We employ the MRL Eye Dataset to classify eye states into &#x0201c;Open-Eyes&#x0201d; and &#x0201c;Close-Eyes,&#x0201d; evaluating a range of models, including Vision Transformer (ViT), Swin Transformer, and fine-tuned transfer learning models such as VGG19, DenseNet169, ResNet50V2, InceptionResNetV2, InceptionV3, and MobileNet. The ViT and Swin Transformer models achieved groundbreaking accuracy rates of 99.15% and 99.03%, respectively, outperforming all other models in precision, recall, and F1-score. To ensure the generalization and robustness of the proposed models, we also evaluate their performance on the NTHU-DDD and CEW datasets, which provide diverse real-world scenarios and challenging conditions. This represents a significant advancement over existing methods, demonstrating the effectiveness of transformer-based architectures in capturing complex spatial dependencies and extracting relevant features for drowsiness detection. The proposed system also incorporates a real-time drowsiness scoring mechanism, which triggers alarms when prolonged eye closure is detected, ensuring timely intervention to prevent accidents. A key novelty of this work lies in the integration of Class Activation Mapping (CAM) for enhanced model interpretability, allowing the system to focus on critical eye regions and improve decision-making transparency. The system was rigorously tested under varying lighting conditions and scenarios involving glasses, showcasing its robustness and adaptability for real-world deployment. By combining cutting-edge deep learning techniques with real-time processing capabilities, this research offers a contactless, reliable, and efficient solution for driver drowsiness detection, significantly contributing to improved road safety and accident prevention. The proposed framework sets a new benchmark in drowsiness detection, highlighting its potential for widespread adoption in advanced driver assistance systems.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computational science</kwd><kwd>Computer science</kwd></kwd-group><funding-group><award-group><funding-source><institution>King Salman International University</institution></funding-source></award-group><open-access><p>Open access funding provided by The Science, Technology &#x00026; Innovation Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank (EKB).</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">In the modern era, based on our ways of living, constant tiredness is more common than it has ever been. Today, an individual sleeps 20 percent less than what was normal a hundred years ago<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. This remains a primary cause for the decline of performance across various tasks in daily life. Subsequently, this leads to people being less focused when engaging in simple activities. This does not change when it comes to driving. A driver who is exhausted suffers a great deal when it comes to performance, and this decline is even more pronounced in the absence of proper sleep. A person who does not want to suffer the negative consequences of being excessively tired simply needs to get sufficient sleep. For a healthy human adult, the average number of hours of recommended sleep is around 7 hours, but currently, 1 in 3 adults suffer from deprivation of this amount of sleep<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. This is caused by the nature of today&#x02019;s lifestyle. Most of these consequences stem from being excessively drowsy, and hence road accidents have seen an upward spike recently<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. The aftermath of these accidents incurs a diverse range of societal, economic, and emotional losses.</p><p id="Par3">From 2000 to 2016, the case fatality rate and human losses due to road accidents in China increased by 19.0% and 63.7%, respectively<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. A 2002 survey in Ontario, Canada, showed that more than 58% of the 750 respondents confessed to driving while fatigued, with 14.5% of them dozing off at the wheel<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. The economic costs linked to drowsy driving was estimated at $12.4 billion in a single year<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Recognition of these facts is the reason why it is recommended to put in place strategies that will guarantee traffic safety. Certain businesses such as Mercedes-Benz, Tesla, and others provide a variety of driver assistance systems<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. For instance, automatic brakes, cruise control, lane change warning, assisted steering, and many others. This has greatly assisted in preventing road accidents and saving innocent lives. Nevertheless, the potential of a driver feeling drowsy or fatigued without any standard means of assessment poses a threat to safety on the road. Fatigue, often characterized by diminished alertness and slower reaction times, plays a critical role in the onset of drowsy driving and has been linked to increased accident risk. The brain activity patterns offer valuable physiological indicators of fatigue, reinforcing the importance of accurate detection in preventing accidents<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>.</p><p id="Par4">For the successful averting of road accidents, detection of drowsiness is a fundamental need. So we were looking to figure out how to come up with a program that could guess when the driver was going to feel drowsy and notify him beforehand to avert a serious incident on the road. In working toward this goal, we need to keep track of the driver&#x02019;s face and search for clues that may suggest that he is dozing off. To solve this problem, we applied deep learning methods, which are explained in the following sections.</p><p id="Par5">To effectively detect and prevent drowsy driving, different approaches have been devised targeting and measuring driver performance, behavior, and physiological state. These approaches can typically be classified into biological, image or video-based, vehicle-based, and hybrid measurements. Each type captures signs of fatigue physiologically and behaviorally, as well as through vehicle dynamics, using internal and external measures. Different associations of each measurement type is provided in Table <xref rid="Tab1" ref-type="table">1</xref> and their definitions, representative examples, advantages, and limitations summarized. This type of classification illustrates the broad multi-dimensional driver drowsiness detection systems and methods currently available, just as much as it explains the ease with which systems can be developed or integrated within existing frameworks designed for future research and application.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Overview of driver drowsiness detection measurement types.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Measurement type</th><th align="left">Definition</th><th align="left">Examples</th><th align="left">Advantages</th><th align="left">Limitations</th></tr></thead><tbody><tr><td align="left">Biological measurements</td><td align="left">Use physiological signals to assess driver&#x02019;s internal state and fatigue levels</td><td align="left">Brain Signal, Respiratory Signal, Heart Signal, Skin Signal, eye signal</td><td align="left">High accuracy in detecting true drowsiness; real-time monitoring of body functions</td><td align="left">Intrusive; requires wearable sensors; may cause discomfort or distraction</td></tr><tr><td align="left">Image-/video-based measurements</td><td align="left">Analyze driver&#x02019;s facial features and head posture via camera input</td><td align="left">Eye, Mouth, Head, Hybrid (Eye, Mouth, Head)</td><td align="left">Non-intrusive; cost-effective; compatible with deep learning and computer vision methods</td><td align="left">Sensitive to lighting, occlusion, and sunglasses; high computational cost</td></tr><tr><td align="left">Vehicle-based measurements</td><td align="left">Derive behavioral data from vehicle dynamics and driver interaction</td><td align="left">Steering Wheel, Lane Deviation</td><td align="left">Non-intrusive; utilizes existing vehicle systems; no need for driver contact</td><td align="left">Indirect measurement; affected by road conditions and driving style</td></tr><tr><td align="left">Hybrid measurements</td><td align="left">Combine two or more types to increase detection accuracy</td><td align="left">Vehicle &#x00026; Image, Biological &#x00026; Image, Vehicle &#x00026; Biological, Vehicle, Image &#x00026; biological</td><td align="left">Robust and reliable; complements strengths of individual methods</td><td align="left">Complex system integration; increased cost and processing requirements</td></tr></tbody></table></table-wrap></p><p id="Par6">Recent advances in driver drowsiness detection have shifted from heuristic-based methods (e.g., PERCLOS, eye aspect ratio) to deep learning models, particularly CNNs like VGG16 and ResNet, which achieve 92&#x02013;97% accuracy on benchmark datasets<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. However, these models struggle with long-range spatial dependencies in facial features (e.g., subtle eyelid movements during micro-sleeps) due to their localized receptive fields. Transformers, with their self-attention mechanisms, address this by capturing global context-a critical advantage for drowsiness detection where holistic facial cues (e.g., brow furrowing, slow blinks) are as informative as local eye states. Despite their success in NLP and image classification, transformer-based drowsiness detection remains underexplored, with only preliminary studies like<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> reporting ViT adaptations for this task. Our work bridges this gap by rigorously evaluating ViT and Swin Transformer architectures against state-of-the-art CNNs, demonstrating that transformers outperform CNNs not only in accuracy (99.15% vs. 98.7% for VGG19) but also in robustness to lighting variations. Unlike prior CNN-based systems<sup><xref ref-type="bibr" rid="CR13">13</xref>&#x02013;<xref ref-type="bibr" rid="CR18">18</xref></sup>, our framework integrates Class Activation Mapping (CAM) to visually justify predictions-a critical step for real-world deployment where false alarms risk user distrust. Transformers excel in drowsiness detection due to their ability to model relationships between distant facial regions (e.g., correlating yawning with prolonged eye closure) via self-attention. This contrasts with CNNs, which require deeper architectures to achieve similar context awareness, often at the cost of computational efficiency. Our Swin Transformer variant further optimizes this by hierarchically processing windows of attention, reducing latency for real-time use.</p><p id="Par7">This paper aims to develop a robust eye states classification of Open-Eyes and Close-Eyes based on a deep learning framework using the MRL dataset and deploy the best-performing model for real feedback accurate eye states. The proposed methodology incorporates a set sequence of steps starting with data preparation which involves adjusting the image size, normalization, and augmentation. Several deep learning practices are used which include transformer-based models such as Vision Transformer (ViT) and Swin Transformer, and for transfer learning, models such as VGG19, Attention VGG19, DenseNet169, ResNet50V2, InceptionResNetV2, InceptionV3, and MobileNet. From a set of specific criteria such as accuracy, precision, recall and F1-score, the best-performing model is selected after training and evaluation. The determined model is then fitted into a system that allows feeding real-time processed video frames that apply Haar Cascade classifiers to locate the face and the eyes of the driver, and subsequently, observe the position of the driver&#x02019;s eyes in order to detect whether he or she is drowsy. An alarm is issued to the driver if a significant period of time is spent with the eyes closed. By leveraging state-of-the-art deep learning models and real-time processing techniques, this study provides a reliable and effective approach for driver drowsiness detection, contributing to improved road safety and accident prevention.</p><p id="Par8">The key contributions of this work are as follows:<list list-type="bullet"><list-item><p id="Par9">Extensive model assessment: We performed model assessments utilizing various types of deep learning models which include transformer techniques and transfer learning algorithms for identifying Open-Eyes and Close-Eyes images from the MRL dataset.</p></list-item><list-item><p id="Par10">Optimized drowsiness detection pipeline: The drowsiness detection system has been enhanced whereby it now involves a sequence of activities such as data preparation, evaluation of the performance of the model, and visualization using Class Activation Mapping (CAM) for interpretability.</p></list-item><list-item><p id="Par11">Real-time implementation: A real-world setting where the optimal proposed model trained is used, the system uses a face and eye tracker that has an active score keeper to monitor the environment and sound alarms when people are drowsy.</p></list-item><list-item><p id="Par12">Enhanced road safety: It&#x02019;s a contactless technology that can be deployed in-car driver assisting tools so that fatigue or drowsy driving can be decreased and accidents reduced.</p></list-item></list>The rest of the paper is structured as follows: In &#x0201c;Literature review&#x0201d; section, present a critical appraisal of related works. In &#x0201c;Proposed methodology&#x0201d; section, we discuss the methodology proposed. &#x0201c;Experiments and results&#x0201d; section presents the experimental results. &#x0201c;Comparative study&#x0201d; section presents the comprehensive study. In &#x0201c;Discussion&#x0201d; section, the discussion is presented. &#x0201c;Conclusion and future directions&#x0201d; section: concludes this paper.</p></sec><sec id="Sec2"><title>Literature review</title><p id="Par13">Some researchers have worked on real-time eye state recognition systems that can be used in embedded devices with limited resources. In this respect, the authors in<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> discuss the application of eye tracking data as an unobtrusive measure of driver behavior for detecting drowsiness. Tracking of eye movements was carried out for 53 participants in a driving simulation task, while channels of multichannel electroencephalogram (EEG) were obtained as baselines. Vigilance states are phase classified using a random forest (RF), and a binary classifier based on a nonlinear support vector machine (SVM). The extraction of features was done for various lengths of eye movement tracking epochs, and the assessment of each classifier was done for each epoch length. The RF classifier provided high accuracy irrespective of the epoch lengths ranging from 88.37% to 91.18%. The SVM classifier remained inferior with an accuracy of 77.12% to 82.62% at all epoch lengths.</p><p id="Par14">DriCare is a system introduced by authors in<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> which utilizes video images to detect the fatigue of drivers without the necessity to connect any devices to them. It uses yawning, blinking and the duration of eye closure as cues. The authors present a new tactic targeting the problems that previous algorithms exhibited, face tracking for increasing the tracking precision. Moreover, he introduces a new approach for detecting facial regions corresponding to 68 major facial characteristics. These regions are then utilized to evaluate the state of the driver. DriCare is able to warn a driver with a system message if there are features detected in the mouth or eyes that could indicate fatigue. DriCare gets a value of 92% directly according to published experimental results.</p><p id="Par15">The authors in<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> detailed the creation of an inexpensive portable apparatus that would evaluate the drowsiness level of a driver. This device is composed of an IR illuminator and a camera that sends the image to a Raspberry Pi 3 Model B. This configuration allows for a system that is less sensitive to changes in environmental light and vision blockage. The processing model takes upper face and oral movements into account (PERCLOS, ECD, and AOT) and establishes three levels of drowsiness based on fuzzy problems (Low-Normal State, Medium-Drowsy State, and High-Severe Drowsiness State). Two different methods of face feature recognition were tested: a facial cascade classifier with haar like filter features and a linear support vector machine SVM with HOG features.</p><p id="Par16">A new suggestion regarding advanced facial thermal imaging technology able to analyze drivers&#x02019; respiration without any physical interaction is discussed in this paper<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. During a car simulator experiment with thirty participants, respiration signals were gathered when participants were encouraged to gradually get sleepier. Time ratios regarding emblems of inspiration to expiration and mean values along with rate standard deviations in respiration cycled were averaged from the signals. We applied the Svm and KNN classifiers for detecting when an individual was drowsy or asleep. The proposed method was utilized alongside the Observer Rating of Drowsiness approach to grade the sleepiness levels of individuals. The k-fold and confusion matrix cross validation approaches were evaluated to understand better the classifiers and predictions regarding napping. These techniques gave an accuracy of up to 90% in detecting drowsiness, 92% in capturing the soundness, 85% in identifying the more specific explains, and 91% in well calculated measurement fulfilling the requirements of this study.</p><p id="Par17">In<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> authors provide a novel approach to the issue of driver drowsiness while behind the steering wheel using a system that is able to accurately identify drowsiness. In this system, Convolutional Neural Networks (CNN) are utilized in real time to ensure a higher degree accuracy while detecting the tendency of a driver to fall asleep. To classify the status of the eye, three networks were proposed, one of which is a Fully Designed Neural Network (FD-NN), and the other two are TL-VGG, which is solely Transfer Learning with VGG16 and VGG19 and has additional layers fused with them.</p><p id="Par18">The authors in<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> put forward the proposal of creating a system of real-time monitoring such as a webcam that detects drowsiness without pores physically interfacing the eyes such as an electrooculogram while ML and computer vision, which are more cost-effective, were employed. Automated alertness monitoring is essential for reducing the chances of human error, and the methodology being proposed employs drowsiness rules based on blink patterns sourced from neuroscience literature, this approach in neuro-physics, The Model exhibited tolerable behavior by not issuing alarms when participants were awake, and significantly flared up by issuing an average of 16.1 alarms for drowsy subjects, thus achieving an impressive accuracy of 94.44%.</p><p id="Par19">The authors have developed a set of learning techniques that help in recognizing that a particular driver is drowsy and which are discussed in<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. The deep learning models integrated into the system include but are not limited to: VGG-FaceNet, FlowImageNet, ResNet and AlexNet, where their primary function is analyzing video feed of the driver in order to ascertain if he is exhibiting the necessary signs. The entire system relies on a range of nodal features, head movements, gestures, behavior and facial expressions. For instance, AlexNet was integrated specifically for focusing fitting on sight ranges like indoor as well as outdoor. VGG-FaceNet transmits gender or ethnicity-related features while FlowImageNet interprets behavioral patterns and headturns, and ResNet discriminates hand motions. The enhanced hand motions recognition is a crucial aspect of the total system performance. These structures divide the recognized features into four groups: active normal state, yawning with blinking of eyes, clenching counterpart teeth, and nodding. With a real-time accuracy of 85%, the system is effective in detecting drowsiness in real time.</p><p id="Par20">The authors in<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> suggest a vision-based solution that applies an ensemble of two lightweight CNNs, each of them being responsible for a video stream and trained with eye patches. To mitigate the problem of overfitting when fine-tuning,&#x0201d; the aforementioned approach employs transfer learning. The system combines both; CNNs and fine-tune them in tandem to enhance the accuracy. The results of the experiments suggest that the proposed system, which the authors refer to as DCNNE, has a higher recognition success rate. For example, its accuracy on the ZJU dataset was 97.99%, which was more than the previous high of 97.20%. The model also performed well on the CEW, MRL and other databases. Moreover, the fine-tuned CNNs were implemented on two separate embedded devices assuring real-world application within time requirements.</p><p id="Par21">According to<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, a vehicle that protects itself from sleep-driving has been developed by the authors of the paper. The primary aim of their initiative is to protect individuals on the road from sleep-related accidents. Capturing these snapshots of driver&#x02019;s face took no more than sixty seconds. Combined communication, behavior and recurrent networks were the first step towards reducing false rates. An additional fuzzy logic-derived approach allowed for manipulating visual data into up-to-date particulars about the driver.</p><p id="Par22">The authors in<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> develop an approach to image processing based detection of drowsiness recognition and traffic signs using the convolutional neural network, CNN. Accurate and efficient detection of road signs is essential in improving safety in road driving. Traffic signs contain essential data about traffic regulations, the state of the road, and intended travel for passengers, motorists, and pedestrians alike. To ignore or to circumvent traffic signs for any basis constitutes a high hazard for all road users and this project is a means to assist motorists to drive more responsibly.</p><p id="Par23">The approach adopted in<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> which involves a blend of deep learning techniques and CNN accompanied LSTM networks is novel as it predicts a driver&#x02019;s drowsiness. CNN specializes in feature representation, and LSTM specializes in feature sequence construction which together, amplify the forecasting performance of the model. The model was given the NTHUDDD driver drowsiness dataset for testing and its performance was then registered against a range of old and modern systems. The evidence showed that the suggested model gave better performance with training accuracy reaching up to 98.30% and validation accuracy reaching to 97.31%.</p><p id="Par24">In<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, the authors proposed a model to estimate level of fatigue with eye movement analysis by a convolutional neural network. They also applied the CNN and VGG16 models for detecting and classifying the facial sleepiness poses into four states: open, closed, yawning and not yawning. Evaluation was carried out on a dataset of 2900 images of eye conditions due to boxer driver drowsiness, with gender, age, and brightness range included. For the scores displaying results, the levels of accuracy were also elevated with further assessments, the CNN model in this case received a 97 rating for the accuracy section 99 precision, recall and F-Score values, and 99 for the other sets. Notably, the VGG16 model achieved the converse 74% accuracy.</p><p id="Par25">In<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, the authors outlined a concept of the eye state classification based on convolutional neural networks, or CNN master adaptive mapping, and presented to it three simple CNN models, VGG16, VGG19 and new 4D models, for testing and sources. The MRL Eye dataset was utilized to train the 4D model, which was built specifically for eye condition assessment of sleepiness in the more advanced, quite darkened room. The 4D model also surpassed the pivot VGG16 and VGG19 models, having higher impressive state-predicting accuracy than the VGG16 and VGG19 models. This analysis presents an all-encompassing system of a driving model which predicts the state of the driver&#x02019;s eyes in order to check if the person is feeling drowsy and sends alerts to improve safety on the roads.</p><p id="Par26">In<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, the authors suggested a non-invasive device for real-time monitoring of driver drowsiness that employs visual data obtained from car dashboard videos. The system uses facial landmarks and face mesh detectors to locate mouth aspect ratio, eye aspect ratio, and head pose, among other key regions. These features are used in three classifiers: random forest, sequential neural network, and linear support vector machine. The evaluation on the datasets from National Tsing Hua University showed that the system is capable of detecting drowsy drivers and sending appropriate alerts in time, with an accuracy of nearly 99%.</p><p id="Par27"><sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, the authors have integrated deep learning technology in identifying driver fatigue with the aim of averting accidents. In their study they have detailed in their model training and testing Seven types of transfer learning based deep learning techniques which include VGG19, ResNet50V2, MobileNetV2, Xception, InceptionV3, DenseNet169, InceptionResNetV2 deep networks. Such models were analyzed and compared using the NTHU-DDD2 data set. Depicted as the most successful model reinforcing the deep learning models, VGG19 was noted to be very efficient. With the accuracy of 96.51%, precision 98.14%, recall of 95.36%, and a F1 score of 96.73%.</p><p id="Par28">Authors in<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> Study of a non-invasive drowsiness detection system operating in real time, which employs the use of convolutional neural networks, was thorough, detailed and demanding. Employing the technology One C and state-of-the-art video monitoring devices in the car cabin, the system analyses facial images emphasizing ascertaining whether the individual yawns and whether their eyes blink. A large training data set was used in the development of the system which encompassed images captured in an environment with different lighting conditions as well as taken from various angles of the face. Additionally, it makes use of Haar cascade classifiers for face region detection and more efficient methods of image analysis for determination of fatigue. The results of the experiment proved that the performance in the test reached 96.54% which evidenced how efficient behavioral indicators like yawns and eye state could improve image analysis process performance.</p><p id="Par29">The authors in<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> performed a detailed study on various drowsiness detection techniques with special emphasis on convolutional neural networks (CNN) along with transfer learning. In addition to their theoretical research, they also developed a relatively easy-to-use mobile application that utilizes these technologies. Many datasets were utilized to properly evaluate the developed model, establishing its efficacy. The drowsiness detection system as proposed was accurately between 90% - 99.86% for both multi and two-class detection systems.</p><p id="Par30">The authors in<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> proposed a powerful data-driven framework to capture drunk drivers&#x02019; behavior using t-distributed stochastic neighbor embedding (t-SNE) in conjunction with the Isolation Forest (iF) algorithm, where the authors autonomously confirmed the driver&#x02019;s drunkenness level. As a feature extracting algorithm, the t-SNE model was chosen due to its property of maintaining local and global structures of feature space while performing dimensionality reduction on a nonlinear dataset. After feature extraction, the iF method unsupervised and tree-based anomaly detection model was applied, which has previously been trained to recognize only normal driving data, validating its use in practical scenarios. This study made use of publicly accessible sensor data from gas and temperature sensors and a digital camera, claiming an AUC of almost 95% and therefore high detection accuracy. The suggested approach also surpassed other techniques and combinations of dimensionality reduction and anomaly detection that included PCA, IPCA, ICA, kPCA, MDS with iForest, EE, and LOF, proving its strength and reliability in detecting alcohol-impaired driving behavior.</p><p id="Par31">The authors in<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> also integrated Independent Component Analysis (ICA), Kantorovitch Distance (KD), and the double Exponentially Weighted Moving Average (DEWMA) to propose a semi-supervised anomaly detection technique to classify and detect drunk driving behavior. In this approach modeling framework, ICA served the purpose of feature extraction for non-Gaussian multivariate data, while KD calculated the distance between normal and abnormal event intervals. DEWMA worked on the KD statistic to ensure sensitive change detection without a priori assumptions about the data distribution. To improve sensitivity, a nonparametric threshold was applied. This approach is particularly advantageous in real-world contexts, as labeled instances of drunk driving are hard to come by, since the method functions without labeled data. The study also XGBoost and used SHAP values to explain and defend feature importance in the detection process and interpretation. Evaluation done on a public dataset obtained from gas and temperature sensors and a digital camera showcased the model&#x02019;s robustness, where it achieved an F1-score of 98%, significantly surpassing the performance of PCA and ICA based methods.</p><p id="Par32">The prior research presented here has made remarkable advances in the drowsiness detection field by implementing real-time classification of the eye state. While most studies, such as those focused on CNN-based architectures or ensembles like the Dual CNN Ensemble (DCNNE), tend to accomplish a fair degree of accuracy, they struggle with generalizability and high computational costs. Moreover, SVM and KNN certainly have their merits; however, when stacked against deep learning models, they lack accuracy. Many models do not perform well across diverse datasets, or they are over-reliant on specific conditions, such as controlled lighting, for the environment in which they were trained.</p><p id="Par33">The studies cited above serve as the baseline from which I propose my work leveraging the Vision Transformer (ViT) and Swin Transformer due to the self-attention-based feature extraction that aids those models in surpassing performance expectations. Those models have the ability to attend to precise features, unlike CNNs which tend to overlook important details in complicated real world scenarios. Moreover, the results that these transformer models offered, in my case high accuracy, especially using the ViT and Swin Transformer, shift the criticisms that were placed on the previous approaches based on CNN structures in terms of accuracy, robustness, and adaptability to varying conditions.</p><p id="Par34">As shown in Table <xref rid="Tab2" ref-type="table">2</xref>, previous studies have made substantial progress in driver drowsiness detection using CNN-based models and multimodal techniques. These approaches often demonstrate high accuracy when evaluated on constrained datasets; however, they tend to struggle with generalization in real-world conditions that involve variations in lighting, occlusion, head pose, and diverse facial features. In contrast, transformer-based models - although more computationally intensive - have recently shown promise in capturing long-range spatial dependencies and improving robustness. Despite their potential, few studies have applied transformer architectures specifically to driver drowsiness detection or incorporated interpretability mechanisms such as Class Activation Mapping (CAM) to enhance decision transparency. Table <xref rid="Tab2" ref-type="table">2</xref> provides a comparative summary of these related studies, outlining their datasets, techniques, accuracy, strengths, and weaknesses.<table-wrap id="Tab2"><label>Table 2</label><caption><p>The summary of the analyzed related works.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Ref</th><th align="left">Year</th><th align="left">Dataset</th><th align="left">Classification model</th><th align="left">Accuracy</th><th align="left">Strengths and Weaknesses</th></tr></thead><tbody><tr><td align="left"><sup><xref ref-type="bibr" rid="CR19">19</xref></sup></td><td align="left">2019</td><td align="left">Self-prepared dataset</td><td align="left">RF and non-linear SVM</td><td align="left">RF: 88.37% to 91.18%</td><td align="left">Strengths: Good performance with varying epoch lengths. Weaknesses: SVM accuracy is consistently lower</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR20">20</xref></sup></td><td align="left">2019</td><td align="left">CelebA, YawDD</td><td align="left">Multiple CNN-kernelized correlation filters method</td><td align="left">92%</td><td align="left">Strengths: High accuracy in a variety of conditions, robust to environmental variations. Weaknesses: Limited to CNN models, lacks broader evaluation across other architectures</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR21">21</xref></sup></td><td align="left">2020</td><td align="left">300-W dataset</td><td align="left">Mamdani fuzzy inference system</td><td align="left">95.5%</td><td align="left">Strengths: Incorporates fuzzy logic for drowsiness detection, useful for real-time applications. Weaknesses: May struggle with fine-tuning or handling complex image data without enhancement</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR22">22</xref></sup></td><td align="left">2020</td><td align="left">Self-prepared thermal</td><td align="left">SVM and KNN</td><td align="left">SVM: 90%</td><td align="left">Strengths: Non-invasive method, useful for thermal monitoring in different lighting conditions. Weaknesses: Thermal imaging may require high-end equipment, and KNN struggles in complex environments.</td></tr><tr><td align="left"/><td align="left"/><td align="left">image dataset</td><td align="left"/><td align="left">KNN:83%</td><td align="left"/></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR23">23</xref></sup></td><td align="left">2020</td><td align="left">Self-prepared ZJU dataset</td><td align="left">FD-NN, TL-VGG16, and TL-VGG19</td><td align="left">FD-NN: 98.15%, TL-VGG16: 95.45%, TL-VGG19: 95%</td><td align="left">Strengths: Impressive performance for fatigue detection, able to capture fine-grain eye movement features. Weaknesses: Limited dataset, reliance on predefined classifiers</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR24">24</xref></sup></td><td align="left">2020</td><td align="left">Self-prepared dataset (DROZY database)</td><td align="left">Multilayer perceptron, RF, and SVM</td><td align="left">SVM: 94.9%</td><td align="left">Strengths: Good feature extraction using basic neural networks for fatigue detection. Weaknesses: Lack of deep learning-based approaches, might miss subtle facial cues.</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR10">10</xref></sup></td><td align="left">2021</td><td align="left">NTHU-DDD video dataset</td><td align="left">Deep-CNN-based ensemble</td><td align="left">85%</td><td align="left">Strengths: Ensemble learning improves detection, high accuracy in diverse environments. Weaknesses: Lower overall performance compared to transformer-based models</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR25">25</xref></sup></td><td align="left">2022</td><td align="left">CEW, ZJU, MRL</td><td align="left">Dual CNN Ensemble (DCNNE)</td><td align="left">CEW: 97.56%, ZJU: 97.99%, MRL: 98.98%</td><td align="left">Strengths: High performance across multiple datasets with ensemble methods. Weaknesses: May not generalize well to datasets outside of the tested range</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR26">26</xref></sup></td><td align="left">2022</td><td align="left">UTA-RLDD dataset</td><td align="left">RNN and CNN</td><td align="left">60%</td><td align="left">Strengths: Low computational cost, suitable for mobile applications. Weaknesses: Low accuracy, particularly for complex real-time applications</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR27">27</xref></sup></td><td align="left">2022</td><td align="left">Self-prepared dataset for traffic signs</td><td align="left">CNN</td><td align="left">98.53%</td><td align="left">Strengths: Strong accuracy in traffic-related scenarios, applicable to many monitoring systems. Weaknesses: Lacks focus on drowsiness detection, limited to specific contexts</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR13">13</xref></sup></td><td align="left">2022</td><td align="left">NTHU-DDD</td><td align="left">CNN + LSTM</td><td align="left">97.3%</td><td align="left">Strengths: Efficient for sequential data analysis, effective in dynamic environments. Weaknesses: Struggles with long-duration analysis or sustained predictions in real-time</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR32">32</xref></sup></td><td align="left">2022</td><td align="left">Public dataset using gas sensor, temperature sensor, and digital camera</td><td align="left">t-SNE for feature extraction + Isolation Forest (iF) for anomaly detection</td><td align="left">95%</td><td align="left">Strengths: Effective detection using only normal (non-drunk) data, Handles nonlinear, high-dimensional data well, Unsupervised approach suitable for real-time detection. Weaknesses: t-SNE is computationally intensive and not ideal for real-time processing, Model performance may vary with different sensor quality or configurations</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR11">11</xref></sup></td><td align="left">2023</td><td align="left">Drowsiness dataset</td><td align="left">CNN and VGG16</td><td align="left">CNN: 97%, VGG16: 94%</td><td align="left">Strengths: High performance in detecting drowsiness from real-time data. Weaknesses: Limited to VGG16 architecture, potential underperformance in new data types</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR28">28</xref></sup></td><td align="left">2023</td><td align="left">MRL</td><td align="left">VGG16, VGG19, and 4D</td><td align="left">VGG16: 95.93%, VGG19: 95.03%, 4D: 97.53%</td><td align="left">Strengths: Strong results across multiple configurations, robust for real-time driver drowsiness detection. Weaknesses: Dependence on specific VGG-based models may limit flexibility in dynamic environments</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR29">29</xref></sup></td><td align="left">2023</td><td align="left">NTHUDDD dataset</td><td align="left">RF, SVM, and sequential NN</td><td align="left">RF: 99%, SVM: 80%, 4D: 96%</td><td align="left">Strengths: RF offers excellent performance, especially for simple fatigue detection scenarios. Weaknesses: SVM underperformed significantly, less robust across diverse environmental conditions</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR33">33</xref></sup></td><td align="left">2024</td><td align="left">Public dataset using gas sensor, temperature sensor, and digital camera</td><td align="left">ICA for feature extraction + Kantorovitch Distance (KD) + DEWMA for anomaly detection; XGBoost for SHAP analysis</td><td align="left">F1-score = 98%</td><td align="left">Strengths: Does not require labeled data (semi-supervised) High sensitivity using DEWMA with nonparametric threshold, SHAP adds explainability to the model, Effective on non-Gaussian multivariate data. Weaknesses: Complexity due to integration of multiple techniques, DEWMA and KD may require careful tuning for different datasets, Potentially computationally intensive for real-time systems</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR30">30</xref></sup></td><td align="left">2024</td><td align="left">NTHU-DDD</td><td align="left">VGG19</td><td align="left">96.51%</td><td align="left">Strengths: Efficient in various lighting and environmental conditions. Weaknesses: Performance variation across datasets, still limited by fixed network architectures</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR31">31</xref></sup></td><td align="left">2024</td><td align="left">YawDD, MRL</td><td align="left">VGG16 and CNN</td><td align="left">VGG16: 95.85%, CNN: 96.54%</td><td align="left">Strengths: High performance in real-time detection with various feature extraction methods. Weaknesses: Not ideal for low-complexity devices, might need more robust processing power</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR18">18</xref></sup></td><td align="left">2024</td><td align="left">MRL</td><td align="left">CNN, InceptionV3, and MobileNetV2</td><td align="left">CNN: 96%, MobileNetV2: 97%, InceptionV3: 98%</td><td align="left">Strengths: Excellent performance with quick response times, particularly in driver monitoring. Weaknesses: InceptionV3 and MobileNetV2 still face computational trade-offs</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec3"><title>Proposed methodology</title><p id="Par35">This paper introduces a comprehensive and systematic methodology for real-time driver drowsiness detection, leveraging state-of-the-art deep learning techniques to classify eye states into &#x0201c;Open-Eyes&#x0201d; and &#x0201c;Close-Eyes.&#x0201d; The proposed framework is designed to address the limitations of existing approaches by integrating advanced data preprocessing, transformer-based architectures, and transfer learning models. The methodology begins with data preparation, including image resizing, normalization, and augmentation, to enhance the generalization capabilities of the models. The MRL, NTHU-DDD, CEW datasets are utilized, with an 80-20 split for training and testing, ensuring a robust evaluation of the models. A diverse set of deep learning architectures, including Vision Transformer (ViT), Swin Transformer, and fine-tuned transfer learning models such as VGG19, DenseNet169, ResNet50V2, InceptionResNetV2, InceptionV3, and MobileNet, are trained and evaluated based on key performance metrics such as accuracy, precision, recall, and F1-score. The best-performing model is then deployed in a real-time system that utilizes Haar Cascade classifiers for face and eye detection, coupled with a drowsiness scoring mechanism to trigger alarms when prolonged eye closure is detected. This end-to-end approach not only ensures high accuracy but also provides a practical and scalable solution for improving road safety.The summary diagram of how the driver drowsiness detection model works is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref> which outlines the sequence of steps followed in this research.<fig id="Fig1"><label>Fig. 1</label><caption><p>The workflow architecture.</p></caption><graphic xlink:href="41598_2025_2111_Fig1_HTML" id="MO1"/></fig></p><p id="Par36">For the purpose of evaluating drowsiness, videos are processed by detecting the frames in real time and then classifying them in real time, starting from driver&#x02019;s face to both the left and right eye using Haar Cascade classifiers. The captured eye regions of the driver are preprocessed to conform to the set standards of the model including scaling as well as normalization, once this step has been completed, the model with optimally set parameters derived from the previous workflow is deployed to identify whether the eyes were open or closed. Every drowsiness score starts from zero, a score increases by 1 unit in case closed eyes are detected. The score for drowsiness will be triggered in the case that the score has remained constant for 15 frames, meaning the driver will alert once a frame reaches a certain threshold. If the predictions generated indicate the eyes of the driver are open, then the score is decreased by one. The real-time detection process is visually represented in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Fig. 2</label><caption><p>Real-time detection architecture.</p></caption><graphic xlink:href="41598_2025_2111_Fig2_HTML" id="MO2"/></fig></p><sec id="Sec4"><title>Dataset description</title><p id="Par37">With emerging technologies, selecting appropriate datasets in the field of driver drowsiness detection is crucial, as they are deemed fit for robust model calibration as well as generalizability. Every dataset comes with its own set of challenges affecting the system&#x02019;s detection accuracy and performance. These hurdles include variations in lighting, facial occlusions from glasses or other objects, head movements, and subtle but impactful slow blinking and yawning, as well as other sleepy behaviors. Understanding these dataset-specific issues is essential for model development, preprocessing strategies, and evaluation. The key challenges associated with the NTHU-DDD, CEW, and MRL datasets used in this study are summarized in Table <xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Key challenges in drowsiness detection datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Key challenges</th></tr></thead><tbody><tr><td align="left" rowspan="4">MRL</td><td align="left">Poor lighting conditions in night mode</td></tr><tr><td align="left">Presence of eyeglasses</td></tr><tr><td align="left">Close-up eye images only (no full face)</td></tr><tr><td align="left">Grayscale images only</td></tr><tr><td align="left" rowspan="6">NTHU-DDD</td><td align="left">Varying facial angles and head pose</td></tr><tr><td align="left">Different lighting conditions (day/night)</td></tr><tr><td align="left">Slow blink with nodding</td></tr><tr><td align="left">Yawning</td></tr><tr><td align="left">Sleepy combinations</td></tr><tr><td align="left">Eye occlusion due to glasses</td></tr><tr><td align="left" rowspan="4">CEW</td><td align="left">Varying head orientation</td></tr><tr><td align="left">Wide diversity in facial expressions</td></tr><tr><td align="left">Presence of eyeglasses and sunglasses</td></tr><tr><td align="left">Occlusions and real-world variability (e.g., hair, hand)</td></tr></tbody></table></table-wrap></p><p id="Par38">For the experiment, the MRL Eye Dataset<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> is used. This dataset has been popular for other research as well notably for drowsy driver detection tasks that focus on eye-state recognition. The MRL dataset contains 84,898 samples in total, all of which can be classified into two main categories, these being Open-Eyes and Close-Eyes. In the aforementioned categories there are 42,952 images for Open-Eyes and 41,946 images for Close-Eyes; Hence, the two categories are distributed almost equally. The images included in this dataset span different resolutions, light conditions, and even different orientation of the eye: so it is a very hard dataset for construction of effective classification models. All these factors suit our research - the dataset&#x02019;s size allows for deep learning models that are able to detect drowsiness in real-life cases.</p><p id="Par39">NTHU-DDD dataset is a comprehensive and diverse collection of driver videos recorded in real vehicles under daytime and nighttime environments. It has annotated behaviors such as yawning, slow blinking, and nodding, along with sleepy states&#x02019; combination, recorded at various facial angles and head movements. This dataset helped us significantly in our work by enabling us to test a complete range of sleepy behavior and account for performance in operating conditions that are representative of real-world scenarios. Within the dataset as a whole, there are 66521 640 <inline-formula id="IEq1"><alternatives><tex-math id="d33e933">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq1.gif"/></alternatives></inline-formula> 480-resolution grayscale images of each of the two classes, i.e., drowsy and not-drowsy. There are 36030 sleepy images in the dataset altogether and 30491 images altogether which are not sleepy.</p><p id="Par40">Closed Eyes in the Wild (CEW) dataset contains a collection of facial images with closed and open eyes that were collected in unconstrained scenarios. It presents huge variation in facial orientation, expression, and occlusions such as glasses and hair. CEW was employed in our research as a cross-validation reference to measure model generalization to in-the-wild data, simulating real-world deployment scenarios where changing visual features should be correctly understood. The CEW dataset comprises 27200 images of closed and open eyes. In contrast to the MRL dataset, the CEW dataset comprises full-face images, posing additional challenges in the guise of occlusions, different lighting, head pose variation, and motion blur. These conditions make the dataset very representative of real-world driving situations, where environmental conditions are changing and unpredictable.</p><p id="Par41">Interpreting Figs. <xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4</xref> represent the data distribution, and sample images of the obtained images from the dataset. The dataset has a total of two purposes hence two classifications; one for training, and one for testing. In simpler terms, out of the total dataset 80% was allocated for training and 20% reserved for testing.<fig id="Fig3"><label>Fig. 3</label><caption><p>Dataset distribution across MRL, NTHU-DDD, and CEW.</p></caption><graphic xlink:href="41598_2025_2111_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>Sample images.</p></caption><graphic xlink:href="41598_2025_2111_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec5"><title>Data preprocessing</title><p id="Par42">This part introduces the need of data preprocessing. This is one of the core processes in both machine learning and deep learning as it minimizes the chances of over fitting and generalizes the model. Any kind of working on the input data to make it ready for further action is termed as &#x0201c;data preprocessing.&#x0201d;. The concept of preprocessing in data makes a lot of applications such as machine learning, deep learning, data mining, and computer vision more friendly and effective. For the purpose of this experiment, we carry out data normalization and scaling. It is necessary to ensure that all images are of the same size in order to allow for the same dimensional input throughout the model. In our experimental of the proposed model, all the images were scaled to dimensions of <inline-formula id="IEq2"><alternatives><tex-math id="d33e967">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$224 \times 224$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq2.gif"/></alternatives></inline-formula>. The model learning process is improved by reducing the influence of external pixel values. To attain that, each pixel is normalized into the range of [0,1] by dividing each pixel value by 255.</p><p id="Par43">In an effort to prevent overfitting and make the model more generalizable, data augmentation methods are utilized on the MRL dataset throughout the preprocessing phase. Many transformations are done on the training images to keep their main characteristic intact while being changed. The ShiftScaleRotate tools in this case moves random images off center and out of proportion, the model then grows stronger to these changes in the position of the eye. HorizontalFlip is often paired with images to give a reflection to the rest of them to strengthen and prepare the model for different angles. Images RandomBrightnessContrast changes the brightness and contrast of images allowing the model to work under different lighting scenarios. Some more tools are rotation_range, width_shift_range, height_shift_range that slightly rotate and move images vertically and horizontally in order to simulate real life movement of the head and the position of the eyes. As a result of editing with these augmentation tools the model became stronger at identifying Open-Eyes and Close-Eyes states giving better real-time drowsiness detection.</p></sec><sec id="Sec6"><title>Region of interest selection</title><p id="Par44">This study uses Haar cascade classifiers to detect, locate and extract the face and eye regions for drowsiness detection preprocessing. Haar cascade is a technique based on machine learning, where essential features are gained from a portrait, or integral image, and categorized utilizing AdaBoost<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. The algorithm scans an image over different intervals, and identifies elements depending on pre-instructed features. The classifier scans parts of an image over different intervals, employing predefined features. In this case, face or eyes, it seeks special features such as the bridge of the nose is lighter than the eyes, the eyes are darker than the cheeks and the forehead is lighter than the eyes. Because of the classification algorithm, face is extracted and from the extracted face box, eye regions are extracted. This step is performed to reduce probabilistic and computational cost of false positives.</p><p id="Par45">The shape of a human eye can be described by a set of points. The relative position of these points defines how open or closed the eye is. So as to measure how open an eye is, it&#x02019;s essential to know which 2D landmarks to use around the eyes<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. There are some important points p1 to p6 are the 2D facial landmarks of the eye as shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. The eye&#x02019;s state, open or close, is detected by calculating the Eye Aspect Ratio (EAR) as shown in Eq. (<xref rid="Equ1" ref-type="disp-formula">1</xref>). The vertical distances between the eye landmarks are summed up and placed in the numerator, whereas those placed horizontally are in the denominator. A lower EAR value refers to a closed eye, while a higher value refers to an open eye. An EAR score less than 0.25 would define the eye as closed whereas an EAR that scores above will define the eye as open.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e995">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} EAR = \frac{\Vert P_2 - P_6 \Vert + \Vert P_3 - P_5 \Vert }{2 \times \Vert P_1 - P_4 \Vert } \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2111_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><fig id="Fig5"><label>Fig. 5</label><caption><p>Eye extraction.</p></caption><graphic xlink:href="41598_2025_2111_Fig5_HTML" id="MO5"/></fig></p><p id="Par46">After detecting the face, the Haar cascade classifier is trained to detect the eye region, after which a deep learning classifier is used to classify the extracted eye images. The model predicts the unlocked or locked status of the eye, which adds to the overall drowsiness score. If the score from both eyes is under a certain value for a set time, then an alarm is sounded. This combination of Haar cascade classifier for Region of Interest (ROI) selection and deep learning for classification proved to be effective and accurate for real-time drowsiness detection.</p></sec><sec id="Sec7"><title>Ethical statement</title><p id="Par47">The human face visible in Fig. <xref rid="Fig27" ref-type="fig">27</xref> is that of one of the authors. The author has provided informed consent for the use of their image in this manuscript. All methods and procedures involving human subjects were carried out in accordance with relevant guidelines and regulations. No institutional or licensing committee approval was required for the use of the author&#x02019;s image, as it was voluntarily provided for research purposes.</p></sec><sec id="Sec8"><title>Transformer architectures</title><p id="Par48">The advancement of transformers in deep learning particularly natural language processing and image processing models cannot be ignored. They were first introduced in the research paper &#x02018;Attention Is All You Need&#x02019;<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> and in this paper, the architecture has replaced the fully connected recurrent and convolution structures by self attention which allows models to be able to tackle full sequences or full images effortlessly.</p><p id="Par49">With those properties, transformers are suitable in image processing tasks considering the fact that they can be used to model complex dependencies between far apart elements in space. Not like CNNs that are built upon local areas, vision transformers in a similar fashion to traditional ones, self attention is spread all over the parts of the images which make them outperform other network models on numerous tasks such as image identification and locating objects within an image.</p><p id="Par50">Meanwhile, the Swin Transformer Adds ons to ViT by proposing a window shifting approach alongside a multi level bidirectional design to enable quick processing and enhance the efficacy of working with large images. Vision transformer&#x02019;s issues are addressed in the Swin Transformer by using a sliding window approach and also resulted in maintaining a productive result in vision activities while also lessening computation cost.</p><sec id="Sec9"><title>ViT transformer</title><p id="Par51">ViT is among the first transformer-based models used to perform image classification tasks<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. While CNNs rely on convolutional layers to hierarchically analyze the features of images, ViTs process images by dividing them into fixed-size patches which get linearly embedded and treated as a sequence through the use of a Transformer encoder<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>.</p><p id="Par52">ViT has a number of primary elements: The Classification Head is the last part to process the feature mapping to enable an image determined to be Open-Eyes or Close-Eyes when combined into a fully connected layer. The Transformer decoder, which comprises a number of self-attention layers and feed forward networks that function alongside the patch embeddings. ViT also has a classification kernel, or Layer Merge. Embedded images have positional information pasted on them, typically 16x16 patches. The self and cross attention modules together ensure that the spatial data learned is used. ViT has proven useful on large scale datasets especially for image classification tasks<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. In this case, ViT is slightly altered and instead of determining whether a driver is awake or not, it determines whether a drivers eyes are open or closed or in other words whether a driver is.</p><p id="Par53">The MRL dataset fine-tuning starts with resizing images adjusted to 224<inline-formula id="IEq3"><alternatives><tex-math id="d33e1049">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq1.gif"/></alternatives></inline-formula>224 pixels. Also, horizontal flipping is used at random to promote better training. The first step of training starts with a weight trained model google/vit-base-patch16-224 which is adjusted for a two state eye model and an AdamW optimizer is applied with a rate of 5e-5 with a CrossEntropyLoss applied. The model is equipped with an early stopping validator with a patience of five. For the computing conditions, the model doesn&#x02019;t start depending on a bias since, the masks aren&#x02019;t utilized, achieving balanced multi orientated images, however the model utilizes a default value of 32 across the line up of 30 and splitting testing and training. Performance was judged by metrics of loss and accuracy during the two sets of training and testing that were held in a single session. The process is stopped as soon as one of the early stopping parameters is reached to ensure that the model doesn&#x02019;t overfit and handles testing well.</p><p id="Par54">The proposed Vision Transformer (ViT) model that has been submitted for drowsiness detection works by splitting input images into smaller patches followed by a linear projection accompanied by the addition of positional embeddings. These linear projections ensure that spatial information is preserved. The patches that have been transformed are then run through a number of transformer encoder layers. Here, global dependencies are captured by mechanisms of multi-head self-attention along with normalisation layers which improve the model&#x02019;s robustness. The model also includes a Head, MLP, and Multi-Layer Perceptron for classification into Open-Eyes and close-Eyes states. More so, Class Activation Mapping (CAM) is implemented for enhanced visibility/easier understanding of the decision made by focus modeling and at the same time identifying clearly the areas that were of interest in the decision. The block diagram of this structure is presented in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. It visually depicts the different stages in the process of extracting and classifying features.<fig id="Fig6"><label>Fig. 6</label><caption><p>The ViT transformer architecture.</p></caption><graphic xlink:href="41598_2025_2111_Fig6_HTML" id="MO6"/></fig></p></sec><sec id="Sec10"><title>Swin transformer</title><p id="Par55">The Swin Transformer architecture involves multiple components. To begin with, the image is sliced into square blocks with no overlaps and then a Patch Embedding Layer is used to map the image blocks into higher dimensional vectors. The structure of the Swin Transformer is hierarchical in nature and it enables local and global dependencies to be captured via the use of shifting windows of self-attention<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The self-attention blocks are restricted within each window and once again the windows are moved throughout the layers of the self attention model so that the model can learn cross-window interactions. In addition, patch merging layers are employed in order to down sample the feature map thereby permitting the model to learn deeper scales features. At last, a classifier head is utilized to make the final decision and classifies the grayscale image as Open-Eyes or Close-Eyes. The Swin Transformer does well in image classification problems dealing with high-resolution images<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> and is tuned in this work to drowsiness detection of a person&#x02019;s eyes whether open or closed.</p><p id="Par56">We take an image with the label of Open-Eyes or Close-Eyes and send it through our model trained specifically from the Hugging Face SwinForImageClassification swin-tiny-patch4-window7-224 instance. To achieve a binary classification, the model undergoes further adjustments. The image encoder is swapped with a custom one that has a dropout layer and a linear projection layer which are used to tackle overfitting. Razoring images with random flips and alterations to contrast and brightness up prepares the dataset for the model enlargement allowing the model to deal with a larger variety of classes. The AdamW optimizer is combined with a learning rate and a cross-entropy loss function to train the model, streamlining the fine-tuning procedure. Other than that, early stopping is used while monitoring the validation loss to ensure the model trains efficiently without overfitting. Moving weight averages (SWA) are applied to boost generalization and improve overall performance. After the model is fine-tuned to our needs, we conduct further evaluations using accuracy, loss metrics and drowsiness detection optimizations allowing the model to distinguish between Open-Eyes and Close-Eyes efficiently.</p><p id="Par57">To ensure maximum effectiveness, a set of hyperparameters are established in the Swin Transformer model during the fine tuning process. The model is set to a batch size of 32, this helps in effective image data processing during the training and evaluation phases. Different learning rates are applied to the backbone and classifier parts of the model: 1e-5 is defined in the former and 1e-3 is defined in the latter. As a result in the issued setting, the parameters learn the last classification layers faster than 1e-5. The training is set to span across 30 epochs and has an early stopping mechanism which halts the training process after 5 epochs without an improvement in validation loss. This is done to mitigate the risk of overfitting. A weight decay of 1e-4 is set in the loss function to facilitate the parameters optimization process using AdamW optimizer. Label Smoothing Cross Entropy is the loss function applied with a smoothing factor of 0.1, this reduces both overfitting and increases generalization. The block diagram of the Swin transformer is presented in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. It visually depicts the different stages in the process of extracting and classifying features.<fig id="Fig7"><label>Fig. 7</label><caption><p>The swin transformer architecture.</p></caption><graphic xlink:href="41598_2025_2111_Fig7_HTML" id="MO7"/></fig></p></sec></sec><sec id="Sec11"><title>CNN models architectures (fine-tuning transfer learning models)</title><sec id="Sec12"><title>VGG19</title><p id="Par58">The visual geometry group at Oxford University introduced the VGG architecture that is extended into what is now known as VGG19<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. It was developed in the year 2014 and consists of Nineteen layers, out of these layers&#x02014;out of which 16 are convolutional and 3 are fully connected, along with that it contains strategically placed max-pooling layers which serve to reduce the spatial dimension while retaining important features. Just like the VGG16 model, the VGG19 model also uses 2 <inline-formula id="IEq4"><alternatives><tex-math id="d33e1103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq1.gif"/></alternatives></inline-formula> 2 max-pooling kernels and 3 <inline-formula id="IEq5"><alternatives><tex-math id="d33e1109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq1.gif"/></alternatives></inline-formula> 3 convolutional filters, they enhance the efficiency of the entire design of the system. With the addition of the layers, VGG19 has enhanced capacity to perform deep feature extraction, hence allowing it to perform complex image classification with high accuracy.</p><p id="Par59">Since the VGG19 is hierarchical in nature, it is able to recognize intricate patterns with ease<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. The very basic and simple features such as edges and textures are concentrated in the shallow layers however more higher level features, like facial movements or jaw movements, are contained within the deeper sides of the layer. The fine granularity that this allows is useful in scenarios where driver drowsiness detection is needed and xeyelid closure and prolonged blinking are informative. Althought the architecture cost for the VGG19 is higher than that of the VGG16, the accuracy and robustness in the detection of fatigued drivers is better in VGG19.</p><p id="Par60">The VGG19 model has a keen aptitude when it comes to telling distinguishing features of a face that&#x02019;s associated with drowsiness, especially when it is used with well trained domain centered datasets like MRL. Sitting down for this further improves its performance at being able to tell the difference between open and closed eyes which works well with letting it being used in systems that are aimed for monitoring the state of a driver in real time. With fatigue based patterns being effectively determined along with the high-level accuracy that the model provides, it makes it great for use in various situations while a person is behind the wheel.</p></sec><sec id="Sec13"><title>VGG19 + attention</title><p id="Par61">In this subsection, we describe the process of fine-tuning a pre-trained VGG19 model with an attention mechanism to classify the open-eye and close-eye states within the MRL dataset. The aim is to make use of the feature extraction of VGG19 but at the same time, disentangle with attention block over relevant image features.</p><p id="Par62">The model of the fine-tuned VGG19 is composed of the already trained VGG19 model but without the fully connected layers on top, preloaded with weights obtained from ImageNet. We then freeze the first 15 layers to keep the pre-trained feature extraction and allow the rest of the deeper layers to be trained on the dataset. A multi-layer perceptron attention mechanism is used to enhance the feature maps by first applying global average pooling, dense layers and a sigmoid activation function to the input feature maps. Global Average Pooling eliminates unnecessary spatial information but retains the major residual components. Fully connected layers of 512, and 256 neurons were then added with ReLU activation and with dropout for regularization. The last layer is a softmax classifier that predicts two output classes: Open-Eyes and Close-Eyes.</p><p id="Par63">Now, let&#x02019;s analyze the channel attention mechanism that is able to weight feature maps given by VGG19. It first applies Global Average Pooling which converts the feature maps to a single channel. Then, the vector is processed with fully connected layers that are aimed at preserving some essential spatial relationships. A sigmoid function is the last step, it generates attention weights that are then multiplied with feature maps over individual elements so that important areas can be highlighted<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>.</p><p id="Par64">To enhance the performance of the model while testing, data augmentation is applied to the training images, which include a degree of random rotations, width and height shifts, as well as, horizontal flipping. Both the training and test data is obtained from folders and scaled to 224 <inline-formula id="IEq6"><alternatives><tex-math id="d33e1137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq1.gif"/></alternatives></inline-formula> 224. Since this is a multi-class classification problem, the categorical class mode is used.</p><p id="Par65">In order to increase performance, a few measures have been taken. The training is stopped after a predetermined number of epochs without improvement of the validation loss. This is done to avoid overfitting. In addition, there is a model checkpoint that saves the model with the best validation accuracy in order to restore the appropriate weights later. If the validation loss does not improve, the learning rate is lowered then, which helps in achieving convergence.</p><p id="Par66">After the model has been trained, it goes through an evaluation stage where it is provided with a test dataset. In this stage, a few key metrics, such as the accuracy of the model and the loss are noted and recorded. The particular model that has the lowest loss is saved and used for classifying eye states of the drivers in real time to assess their drowsiness in future applications.</p></sec><sec id="Sec14"><title>DenseNet169</title><p id="Par67">DenseNet169 is a deep convolutional network that is about 169 layers deep and was developed to eliminate the vanishing gradient problem<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. The goal was to greatly increase the effectiveness of learning new features while decreasing usage of parameters. It is efficient when it comes to leveraging previous knowledge, and while doing so, facilitates the propagation of gradients. Each layer was designed to pull information from all layers at once after itself, in doing so, it was able to enhance the redundancy of parameters.</p><p id="Par68">DenseNet169 perceives intricate multi-level features that are essential for recognizing driver drowsiness, specifically for drivers who have some signs of closed eyes and slow blinking, as they are slight eye movement indicators, this proves useful to wide driver fatigue detection<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>.</p><p id="Par69">Constantly adjusting the DenseNet169 on the MRL dataset increases the effectiveness with which the model can tell supporting it with the capability of classifying the model accurately. Dense connectivity architecture broadly assists with the capability of generalization ensuring vast and immense performance of the model in real life driver monitoring situations.</p></sec><sec id="Sec15"><title>ResNet50V2</title><p id="Par70">ResNet50V2 is a Microsoft Research based deep convolution neural network with residual learning which is an updated version of ResNet50<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. This network consist of fifty layers which include identity and convolutional residual blocks which enhance the propagation of gradients leading to resolving the problem of vanishing gradients. Because of this architecture, networks can be deeper while maintaining stability during training, making it efficient in tasks that require complex image classification.</p><p id="Par71">Driver drowsiness detection gets a huge boost from ResNet50V2&#x02019;s deep hierarchical feature extraction capabilities<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. The initial layers focus on basic image properties while the high level ones zoom eye closing and minute facial expressions. And as the model relies on residual connections, that ensures there will be greater efficiency with avoiding overfitting during fine tuning the model to the MRL data set.</p><p id="Par72">The alter as described above allows ResNet50V2 to perform better with MRL dataset as it can now effectively discriminate Open vs Closed eyes and thus classify drowsiness states. The model does not suffer from any generalization issues suggesting it&#x02019;s viable for real world deployment where the use case is driver supervision in real time, ensuring the detection is accurate regardless of the light or surrounding.</p></sec><sec id="Sec16"><title>InceptionResNetV2</title><p id="Par73">InceptionResNetV2 combines Inception and ResNet into a single framework as it adds inception modules with residual connections to boost accuracy and loss to gain efficiency<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. Because of this hybrid design, there is less need to worry about the complications that arise when training a model when the networks are very deep since there is a high degree of accuracy achieved.</p><p id="Par74">Because of its features, the model is appropriate for driver drowsiness detection. Its inception blocks are residual, and they can identify complex and abstract facial features which are relevant such as the closure of one&#x02019;s eyes and other signs of fatigue<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>.</p><p id="Par75">By applying InceptionResNetV2 on a particular dataset called MRL, we can improve the model&#x02019;s ability to effectively distinguish between closed and opened eyes. A real time system that monitors a driver to determine if they are drowsy can be possible while ensuring safety on the roads against accidents that are caused by fatigue, owing to InceptionResNetV2, which is able to extract relevant features and learn efficiently.</p></sec><sec id="Sec17"><title>InceptionV3</title><p id="Par76">InceptionV3 is another great product that Google developed. It is an AI algorithm that makes use of deep learning and specializes in Amalgamation neural networking processes for efficient computational resource and cost reduction of an enhanced deep learning model. Two other of its components are classifier aids and de-factored filtering<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>.</p><p id="Par77">Inception V3 also allows monitoring of drowsy drivers as it is able to capture fine and coarse details which will construct proper facial features. When coupled with inception modules, it becomes easier to precisely identify eye closure and other signs relating to fatigue which improves the necessary cue in the monitoring process to control the drowsiness of the driver.</p><p id="Par78">To a large extent, the reliability of the driver is enhanced such that InceptionV3 has been fine-tuned on the MRL dataset greatly improving its monitoring tools and detecting drowsiness for drivers regardless of their status. Being safe, the software continues to prove extremely efficient in real-time applications.</p></sec><sec id="Sec18"><title>MobileNet</title><p id="Par79">MobileNetV2 is a light-weight Convolutional Neural Network specifically targeted for mobile and embedded systems. It was developed by Google and employs the two techniques of depthwise separable convolutions and the inverted residual blocks<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. These techniques decrease the complexity of computations without losing a lot of accuracy. Owing to its efficient architecture, MobileNetV2 is well-suited for use in environments with limited resources and which require real-time inferences.</p><p id="Par80">The capability of MobileNetV2 to hierarchically extract features, makes this architecture appropriate for detection of driver drowsiness. This model is competent in determining facial indicators like eye closure and extended blinking of the eyes; which is not computationally intensive at all. This efficacy particularly helps in embedded applications using real-time analytics, where every millisecond counts.</p><p id="Par81">MobileNetV2&#x02019;s eye classification is fine-tuned on the MRL dataset which allows for the quick determination of either open or closed eyes and thus drowsiness is detected with low latency. Furthermore, this architecture is easily implemented on in-vehicle driver monitoring systems and thus ensures that timely warnings about fatigue related issues are issued. The problem with this model is that it gives low accuracy.</p></sec></sec></sec><sec id="Sec19"><title>Experiments and results</title><sec id="Sec20"><title>Experimental setup</title><p id="Par82">The ViT transformer model and eight different models (Swin Transformer, VGG19 with Attention, VGG19, DenseNet169, ResNet50V2, InceptionResNetV2, InceptionV3, and MobileNet), all of those are implemented in Python using the Kaggle platform. The execution of the code happened against the following system configuration, as represented in Table <xref rid="Tab4" ref-type="table">4</xref><table-wrap id="Tab4"><label>Table 4</label><caption><p>System configuration details.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Configuration</th><th align="left"/></tr></thead><tbody><tr><td align="left">IDE</td><td align="left">Kaggle</td></tr><tr><td align="left">Programming Language</td><td align="left">Python</td></tr><tr><td align="left">Libraries</td><td align="left">Tensorflow, Keras, Torch, Pandas, Matplotlib, scikit-learn</td></tr><tr><td align="left">GPU</td><td align="left">NVIDIA Tesla P100 with 16 GB VRAM</td></tr><tr><td align="left">CPU</td><td align="left">Intel Xeon CPU (2.3 GHz, 46 MB cache)</td></tr><tr><td align="left">RAM</td><td align="left">16 GB of system memory</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec21"><title>Training phase</title><p id="Par83">The images belonging to the MRL dataset are subdivided into two sets for the purposes of the training phase. The categorization of the dataset into two sets is found to be 80% training and 20% testing. Hence, the total number of images is 84,898, with 67,917 images used for training and 16,981 images used for testing. Similarly, the NTHU-DDD dataset is also divided using an 80%-20% train-test split, comprising a total of 66,521 images, of which 53,216 images are allocated for training and 13,305 images for testing. In addition, the CEW dataset follows the same train-test ratio, with a total of 27,200 images, including 21,760 training images and 5,440 testing images. This distribution is summarized in Table <xref rid="Tab5" ref-type="table">5</xref>.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Number of images in each dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Categories</th><th align="left">MRL dataset</th><th align="left">NTHU-DDD dataset</th><th align="left">CEW dataset</th></tr></thead><tbody><tr><td align="left">Total</td><td align="left">84,898</td><td align="left">66,521</td><td align="left">27,200</td></tr><tr><td align="left">Train</td><td align="left">67,917</td><td align="left">53,216</td><td align="left">21,760</td></tr><tr><td align="left">Test</td><td align="left">16,981</td><td align="left">13,305</td><td align="left">5440</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec22"><title>Performance evaluation methods</title><p id="Par84">The model performance was evaluated based on some common metrics like confusion matrix, accuracy, recall, precision, and F1 score.</p><sec id="Sec23"><title>Confusion matrix</title><p id="Par85">The confusion matrix comprises a table of correct and wrong predictions made by the model. It is derived from four parameters: True Positive, True Negative, False Positive, and False Negative. The definitions of these parameters are as follows:<list list-type="bullet"><list-item><p id="Par86">(TP): correct prediction of class1 when the actual label is class1.</p></list-item><list-item><p id="Par87">(TN): correct prediction of class2 when the actual label is class2.</p></list-item><list-item><p id="Par88">(FP): prediction class1 when the actual label is class2.</p></list-item><list-item><p id="Par89">(FN): prediction class2 when the actual label is class1.</p></list-item></list>These four values from confusion matrix are utilized to calculate accuracy, precision, recall, and F1-score. These metrics provide a comprehensive evaluation of the model&#x02019;s performance in classifying &#x0201c;Open-Eyes&#x0201d; and &#x0201c;Close-Eyes&#x0201d;.</p></sec><sec id="Sec24"><title>Accuracy</title><p id="Par90">It&#x02019;s the number of instances that the classifier had predicted correctly divided by the total number of instances. Accuracy is nothing but the overall correct representation of the model. It is calculated using:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e1352">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2111_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec25"><title>Precision</title><p id="Par91">Precision is the ratio of correctly predicted positive instances to the predicted positive instances. This is very useful when the cost of false positives is critical. Precision is calculated as follows:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e1362">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Precision} = \frac{TP}{TP + FP} \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2111_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec26"><title>Recall</title><p id="Par92">Recall, also known as a sensitivity or truth positive rate, measures the ratio of actual positives that a model is able to capture through its prediction. Recall would be a scenario where we want to minimize false negatives. It is calculated using:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e1372">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Recall} = \frac{TP}{TP + FN} \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2111_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec27"><title>F1Score</title><p id="Par93">This means that the F1 score takes into account both precision and recall, and is defined as the harmonic mean of precision and recall, thereby generating a single metric that captures the degree of trade-off between these two: Its definition is:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e1382">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {F1-score} = \frac{2 * \text {Precision} * \text {Recall}}{\text {Precision} + \text {Recall}} \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_2111_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p></sec></sec><sec id="Sec28"><title>Hyperparameters</title><p id="Par94">The selection of hyperparameters in this study was based on a combination of empirical testing and values commonly recommended in prior research. Initial values for learning rate, batch size, and number of epochs were adopted from similar deep learning applications in driver monitoring and visual classification tasks (e.g.,<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>). These values were then fine-tuned using manual experimentation on a validation split of the training data. For each model, early stopping was applied with a patience of 5 to prevent overfitting. While we did not employ exhaustive grid or random search due to computational constraints, iterative testing allowed us to achieve reliable convergence and performance across datasets.</p><p id="Par95">Hyperparameter tuning is essential when it comes to deep architectural structures to achieve the optimum performance. Several of the hyperparameters used included batch size, learning rate, epochs, optimizer chosen, loss function used, early stopping on patience, and all of these were used during the training to improve the classification accuracy on the MRL data set.<list list-type="bullet"><list-item><p id="Par96">Batch size: The batch size is the number of training samples to work through before the internal model parameters are updated. A bigger batch size produces slower stable gradient updates but necessitates more memory; a smaller batch size results in more gradient updates and, whilst the training is noisier, convergence is quicker. After experimentation, a size of 32 batches was found to be ideal as it is a more cost-effective than advocating for model performance.</p></list-item><list-item><p id="Par97">Learning rate: The learning rate determines the size of each update step for weights in gradient descent. A model could take too long to converge if the learning rate is set too low, while a model could overshoot the optimal setting with a higher learning rate. For the purposes of this research, a learning rate of 0.0001 was used since this enabled reliable yet quick convergence during training.</p></list-item><list-item><p id="Par98">Number of epochs: The number of epochs limits the number of times the complete model will work with the training data. An increased number of epochs can cause the model to undergo overfitting, while a reduced number of epochs can induce underfitting. In conjunction with early stopping, 30 epochs were used at maximum in order to allow the model to end training early, should there be no observable performance improvement.</p></list-item><list-item><p id="Par99">Optimizer: The optimizer chosen does affect the rate of convergence as well as the extent to which convergence is achieved. For this paper, We have used the Adam Optimizer able to use the advantages of learning rates that self-adapt.</p></list-item><list-item><p id="Par100">Loss function: A binary cross-entropy loss function was utilized in training the model since this is suitable for binary class problems. Such functionality calculates the difference between the predicted values and the values represented by the given labels thereby helping the model decrease training error.</p></list-item><list-item><p id="Par101">Early stopping with patience: The method of early stopping was used with patience of 5 epochs, with an aim to reduce overfitting and increase performance on unseen datasets. It indicates when there is no enhancement in the validation loss for a span of 5 epochs, the training cycle needs to be terminated. The use of early stopping slightly resolved expending computational resources by not allowing training beyond its optimal point and was efficient in aiding in conflict of overfitting by securing an optimal stopping point.</p></list-item></list>Table <xref rid="Tab6" ref-type="table">6</xref> depicts the values for some hyperparameters in the experiment. Adjustment of these hyperparameters led to improved generalization on unseen data with no compromise on computational costs. The values chosen ensured that there was no fitting of the model on the data and there was likewise no overfitting to create a robust classification model that could successfully detect open and closed eyes in the MRL dataset classification model.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Hyperparameter settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Fine-tuning Transfer learning models</th><th align="left">ViT transformer</th><th align="left">Swin transformer</th></tr></thead><tbody><tr><td align="left">Batch size</td><td align="left">32</td><td align="left">32</td><td align="left">32</td></tr><tr><td align="left">Learning rate</td><td align="left">1e-4</td><td align="left">5e-5</td><td align="left">Backbone: 1e-5 classifier: 1e-3</td></tr><tr><td align="left">Epoch</td><td align="left">30</td><td align="left">30</td><td align="left">30</td></tr><tr><td align="left">Early stopping</td><td align="left">Patience = 5</td><td align="left">Patience = 5</td><td align="left">Patience = 5</td></tr><tr><td align="left">Optimizer</td><td align="left">Adam</td><td align="left">AdamW</td><td align="left">AdamW (decay:1e-4)</td></tr><tr><td align="left">Loss</td><td align="left">Binary CrossEntropy</td><td align="left">CrossEntropyLoss</td><td align="left">Label smoothing cross entropy (0.1)</td></tr></tbody></table></table-wrap></p><p id="Par102">Table <xref rid="Tab7" ref-type="table">7</xref> presents the size and estimation of total parameters, trainable parameters, and others for each of the nine models. The complexity of each model can be seen in the table.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Total parameters comparison of the models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Sr. no.</th><th align="left">Model name</th><th align="left">Image size</th><th align="left">Trainable parameters before fine-tuning</th><th align="left">Trainable parameters after fine-tuning</th><th align="left">Model size (MB)</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">ViT transformer</td><td align="left">224 * 224</td><td align="left">85,800,194</td><td align="left">85,800,194</td><td align="left">327.30</td></tr><tr><td align="left">2</td><td align="left">Swin transformer</td><td align="left">224 * 224</td><td align="left">27,914,108</td><td align="left">27,914,108</td><td align="left">106.48</td></tr><tr><td align="left">3</td><td align="left">VGG + attention</td><td align="left">224 * 224</td><td align="left">12,226,850</td><td align="left">12,226,850</td><td align="left">78.02</td></tr><tr><td align="left">4</td><td align="left">VGG19</td><td align="left">224 * 224</td><td align="left">2,099,201</td><td align="left">143,668,241</td><td align="left">548.05</td></tr><tr><td align="left">5</td><td align="left">DenseNet169</td><td align="left">224 * 224</td><td align="left">1,705,985</td><td align="left">14,190,465</td><td align="left">54.74</td></tr><tr><td align="left">6</td><td align="left">ResNet50V2</td><td align="left">224 * 224</td><td align="left">2,099,201</td><td align="left">25,618,561</td><td align="left">97.90</td></tr><tr><td align="left">7</td><td align="left">InceptionResNetV2</td><td align="left">224 * 224</td><td align="left">1,574,913</td><td align="left">55,851,105</td><td align="left">213.29</td></tr><tr><td align="left">8</td><td align="left">InceptionV3</td><td align="left">224 * 224</td><td align="left">2,099,201</td><td align="left">23,867,553</td><td align="left">91.18</td></tr><tr><td align="left">9</td><td align="left">MobileNet</td><td align="left">224 * 224</td><td align="left">1025</td><td align="left">3,208,001</td><td align="left">12.32</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec29"><title>Accuracy evalution</title><p id="Par103">This research compares several sophisticated deep learning approaches, including Tensorflow models such as ViT Transformer, Swin Transformer, VGG19 with Attention, VGG19, DenseNet169, ResNet50V2, InceptionResNetV2, InceptionV3, and MobileNet against multiple images containing &#x0201c;Close-eyes&#x0201d; as well as &#x0201c;Open-eyes.&#x0201d; All of the models were subjected to early stopping with a patience value of 5 for a maximum of 30 epochs kept for training. The models&#x02019; performance was evaluated in terms of accuracy, precision, recall, and F1-score. The results for the MRL dataset are listed in Table <xref rid="Tab8" ref-type="table">8</xref>, the NTHU-DDD dataset in Table <xref rid="Tab9" ref-type="table">9</xref>, and the CEW dataset in Table <xref rid="Tab10" ref-type="table">10</xref>. The MRL dataset served as the primary dataset for training and evaluating all models, while the NTHU-DDD and CEW datasets were employed to validate the generalizability and robustness of the proposed models under more diverse, real-world conditions. Notably, the Transformer-based models-specifically ViT and Swin-consistently outperformed all other models across the datasets, demonstrating superior accuracy compared to both traditional machine learning and transfer learning-based CNN architectures. To ensure the robustness and statistical reliability of the results, each experiment was conducted over five independent runs, with accuracy reported as a mean &#x000b1; standard deviation. It was observed that classical machine learning models performed significantly weaker, especially when applied to complex and varied real-world image conditions. These findings further validate the advantage of Transformer architectures in capturing intricate spatial dependencies and enhancing model generalization in real-time driver drowsiness detection tasks.<table-wrap id="Tab8"><label>Table 8</label><caption><p>Performance evaluation and comparison of various models on the <bold>MRL</bold> dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Precision (in %)</th><th align="left">Recall (in %)</th><th align="left">F1 score (in %)</th><th align="left">Accuracy (in %)</th></tr></thead><tbody><tr><td align="left">ViT Transformer</td><td align="left">99</td><td align="left">99</td><td align="left">99</td><td align="left">99.15 <inline-formula id="IEq7"><alternatives><tex-math id="d33e1700">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.12$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq7.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">Swin Transformer</td><td align="left">99</td><td align="left">99</td><td align="left">99</td><td align="left">99.03 <inline-formula id="IEq8"><alternatives><tex-math id="d33e1718">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.13$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq8.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">VGG19 + Attention</td><td align="left">99</td><td align="left">99</td><td align="left">99</td><td align="left">98.85 <inline-formula id="IEq9"><alternatives><tex-math id="d33e1736">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.11$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq9.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">VGG19</td><td align="left">98.5</td><td align="left">98.5</td><td align="left">99</td><td align="left">98.7 <inline-formula id="IEq10"><alternatives><tex-math id="d33e1754">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.15$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq10.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">DenseNet169</td><td align="left">98.5</td><td align="left">98.5</td><td align="left">99</td><td align="left">98.65 <inline-formula id="IEq11"><alternatives><tex-math id="d33e1772">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.14$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq11.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">ResNet50V2</td><td align="left">98.5</td><td align="left">98.5</td><td align="left">99</td><td align="left">98.5 <inline-formula id="IEq12"><alternatives><tex-math id="d33e1790">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.12$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq7.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">InceptionResNetV2</td><td align="left">98.5</td><td align="left">98.5</td><td align="left">98.5</td><td align="left">98.5 <inline-formula id="IEq13"><alternatives><tex-math id="d33e1808">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.13$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq8.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">InceptionV3</td><td align="left">98.5</td><td align="left">98.5</td><td align="left">98.5</td><td align="left">98.5 <inline-formula id="IEq14"><alternatives><tex-math id="d33e1826">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.14$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq11.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">MobileNet</td><td align="left">98</td><td align="left">98</td><td align="left">98</td><td align="left">97.99 <inline-formula id="IEq15"><alternatives><tex-math id="d33e1844">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.11$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq9.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">Random Forest</td><td align="left">98</td><td align="left">98</td><td align="left">98</td><td align="left">97.93 <inline-formula id="IEq16"><alternatives><tex-math id="d33e1862">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.11$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq9.gif"/></alternatives></inline-formula>)</td></tr></tbody></table></table-wrap><table-wrap id="Tab9"><label>Table 9</label><caption><p>Performance evaluation and comparison of various models on the <bold>NTHU-DDD</bold> dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Precision (in %)</th><th align="left">Recall (in %)</th><th align="left">F1 Score (in %)</th><th align="left">Accuracy (in %)</th></tr></thead><tbody><tr><td align="left">ViT Transformer</td><td align="left">99</td><td align="left">100</td><td align="left">100</td><td align="left">99.52 <inline-formula id="IEq17"><alternatives><tex-math id="d33e1908">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.09$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq17.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">Swin Transformer</td><td align="left">99</td><td align="left">99</td><td align="left">99</td><td align="left">98.76 <inline-formula id="IEq18"><alternatives><tex-math id="d33e1926">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.11$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq9.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">VGG19 + Attention</td><td align="left">99.44</td><td align="left">98.52</td><td align="left">98.98</td><td align="left">99.07 <inline-formula id="IEq19"><alternatives><tex-math id="d33e1944">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.12$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq7.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">VGG19</td><td align="left">98</td><td align="left">98</td><td align="left">98</td><td align="left">98.66 <inline-formula id="IEq20"><alternatives><tex-math id="d33e1962">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.13$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq8.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">DenseNet169</td><td align="left">98.52</td><td align="left">98.44</td><td align="left">98.48</td><td align="left">98.60 <inline-formula id="IEq21"><alternatives><tex-math id="d33e1980">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.12$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq7.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">ResNet50V2</td><td align="left">97.41</td><td align="left">99.48</td><td align="left">98.43</td><td align="left">98.55 <inline-formula id="IEq22"><alternatives><tex-math id="d33e1998">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.13$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq8.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">InceptionResNetV2</td><td align="left">98.74</td><td align="left">94.70</td><td align="left">96.68</td><td align="left">97.02 <inline-formula id="IEq23"><alternatives><tex-math id="d33e2016">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.14$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq11.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">InceptionV3</td><td align="left">98.71</td><td align="left">97.74</td><td align="left">98.22</td><td align="left">98.38 <inline-formula id="IEq24"><alternatives><tex-math id="d33e2034">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.13$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq8.gif"/></alternatives></inline-formula>)</td></tr><tr><td align="left">MobileNet</td><td align="left">90.20</td><td align="left">89.52</td><td align="left">89.86</td><td align="left">91.15 <inline-formula id="IEq25"><alternatives><tex-math id="d33e2052">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.12$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq7.gif"/></alternatives></inline-formula>)</td></tr></tbody></table></table-wrap><table-wrap id="Tab10"><label>Table 10</label><caption><p>Performance evaluation and comparison of various models on the <bold>CEW</bold> dataset<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Precision (in %)</th><th align="left">Recall (in %)</th><th align="left">F1 score (in %)</th><th align="left">Accuracy (in %)</th></tr></thead><tbody><tr><td align="left">Swin Transformer</td><td align="left">100</td><td align="left">100</td><td align="left">100</td><td align="left">100 <inline-formula id="IEq26"><alternatives><tex-math id="d33e2102">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.0 )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq26.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">ViT Transformer</td><td align="left">97</td><td align="left">95</td><td align="left">96</td><td align="left">97.25 <inline-formula id="IEq27"><alternatives><tex-math id="d33e2119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.12 )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq27.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">VGG19 + Attention</td><td align="left">97</td><td align="left">97</td><td align="left">97</td><td align="left">98.17 <inline-formula id="IEq28"><alternatives><tex-math id="d33e2136">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.11 )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq28.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">VGG19</td><td align="left">96</td><td align="left">93</td><td align="left">95</td><td align="left">96.3 <inline-formula id="IEq29"><alternatives><tex-math id="d33e2153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.12 )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq27.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">DenseNet169</td><td align="left">95</td><td align="left">95</td><td align="left">95</td><td align="left">96.3 <inline-formula id="IEq30"><alternatives><tex-math id="d33e2170">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.14 )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq30.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">InceptionResNetV2</td><td align="left">89</td><td align="left">94</td><td align="left">91</td><td align="left">93.6 <inline-formula id="IEq31"><alternatives><tex-math id="d33e2187">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.12 )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq27.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">MobileNet</td><td align="left">91</td><td align="left">90</td><td align="left">91</td><td align="left">93.6 <inline-formula id="IEq32"><alternatives><tex-math id="d33e2204">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.13 )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq32.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">InceptionV3</td><td align="left">88</td><td align="left">89</td><td align="left">88</td><td align="left">91.7 <inline-formula id="IEq33"><alternatives><tex-math id="d33e2221">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.13 )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq32.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">ResNet50V2</td><td align="left">85</td><td align="left">86</td><td align="left">86</td><td align="left">89.9 <inline-formula id="IEq34"><alternatives><tex-math id="d33e2238">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\pm 0.12 )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_2111_Article_IEq27.gif"/></alternatives></inline-formula></td></tr></tbody></table></table-wrap></p><sec id="Sec30"><title>ViT transformer</title><p id="Par104">The ViT Transformer performed remarkably well, earning the highest scores in every metric of the analysis. It achieved an accuracy rate of 99.15% and also impressive precision and recall of 99%. Moreover, the F1 score of the model is 99% which is a strong indication of its good balance between recall and precision. The accuracy plot in Fig. <xref rid="Fig8" ref-type="fig">8</xref>a illustrates a well-defined accuracy curve that indicates good learning stability, while Fig. <xref rid="Fig8" ref-type="fig">8</xref>b shows a low loss convergence trend demonstrating strong training dynamics. Furthermore, the confusion matrix which is presented in Fig. <xref rid="Fig9" ref-type="fig">9</xref> contains a few misclassification errors which support the high accuracy and reliability of the model.<fig id="Fig8"><label>Fig. 8</label><caption><p>ViT transformer accuracy and loss graphs on testing MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig8_HTML" id="MO8"/></fig><fig id="Fig9"><label>Fig. 9</label><caption><p>ViT transformer confusion matrix for MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig9_HTML" id="MO9"/></fig></p></sec><sec id="Sec31"><title>Swin transformer</title><p id="Par105">The Swin Transformer model performed impressively closely to the ViT model amassing an impressive accuracy of 99.03%. It managed to maintain good performance across the board with all three metrics scoring 99% each for precision, recall and F1 score. The model performance improved steadily as shown in Fig. <xref rid="Fig10" ref-type="fig">10</xref>a . The loss curve in Fig. <xref rid="Fig10" ref-type="fig">10</xref>b also provides evidence of optimization of the model. Examining Fig. <xref rid="Fig11" ref-type="fig">11</xref>, the confusion matrix also illustrates the discriminative capacity of the model showing only a handful of misclassification.<fig id="Fig10"><label>Fig. 10</label><caption><p>Swin transformer accuracy and loss graphs on testing MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig10_HTML" id="MO10"/></fig><fig id="Fig11"><label>Fig. 11</label><caption><p>Swin transformer confusion matrix for MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig11_HTML" id="MO11"/></fig></p></sec><sec id="Sec32"><title>VGG19 + attention</title><p id="Par106">VGG19 with the Attention mechanism achieved slightly lower results than the ViT Transformer and Swin Transformer with an accuracy of 98.85%. The model had 99% precision and recall resulting in the F1 score of 99%. The accuracy progress presented in Fig. <xref rid="Fig12" ref-type="fig">12</xref>a shows good learning stability, which was also complemented by the loss curve in Fig. <xref rid="Fig12" ref-type="fig">12</xref>b pointing to strong convergence. The confusion matrix in Fig. <xref rid="Fig13" ref-type="fig">13</xref> further illustrates the strong performance of the model in the classification of different categories.<fig id="Fig12"><label>Fig. 12</label><caption><p>VGG19 with attention accuracy and loss graphs on testing MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig12_HTML" id="MO12"/></fig><fig id="Fig13"><label>Fig. 13</label><caption><p>VGG19 with attention confusion matrix for MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig13_HTML" id="MO13"/></fig></p></sec><sec id="Sec33"><title>VGG19</title><p id="Par107">The measurements gathered indicated that VGG19 model produced an accuracy of 98.7%, recall of 98.5%, F1 score of 98.5% and a precision of 98.5%. As it can be seen in Fig. <xref rid="Fig14" ref-type="fig">14</xref>a , the accuracy curve indicates that graph learning remained smooth for multiple epochs. From Fig. <xref rid="Fig14" ref-type="fig">14</xref>b we see a loss curve where the model appeared to converge, though more gradually as compared to the other models. Lastly, the confusion matrix is represented in Fig. <xref rid="Fig15" ref-type="fig">15</xref>.<fig id="Fig14"><label>Fig. 14</label><caption><p>VGG19 accuracy and loss graphs on testing MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig14_HTML" id="MO14"/></fig><fig id="Fig15"><label>Fig. 15</label><caption><p>VGG19 confusion matrix for MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig15_HTML" id="MO15"/></fig></p><p id="Par108">.</p></sec><sec id="Sec34"><title>DenseNet169</title><p id="Par109">DenseNet169 model, on the other hand, achieved an accuracy of 98.65% with other parameters-precision, recall, and F1 score of 98.5%. In Fig. <xref rid="Fig16" ref-type="fig">16</xref>a the accuracy curve maintains a proper equilibrium suggesting step learning while the loss curve as seen in Fig. <xref rid="Fig16" ref-type="fig">16</xref>b displays strong convergence with very low overfitting. Referring to Fig. <xref rid="Fig17" ref-type="fig">17</xref>, the confusion matrix depicts a modest misclassification rate compared to the best models but overall maintains a high degree of perceived reliability.<fig id="Fig16"><label>Fig. 16</label><caption><p>DenseNet169 accuracy and loss graphs on testing MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig16_HTML" id="MO16"/></fig><fig id="Fig17"><label>Fig. 17</label><caption><p>DenseNet169 confusion matrix for MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig17_HTML" id="MO17"/></fig></p><p id="Par110">.</p></sec><sec id="Sec35"><title>ResNet50V2</title><p id="Par111">The ResNet50V2 model performed exceptionally well, achieving an overall accuracy of 98.5% along with F1 score of 99%, recall of 98.5% and precision of 98.5%. The accuracy curve as depicted by Fig. <xref rid="Fig18" ref-type="fig">18</xref>a managed to grow exponentially which leads to the model&#x02019;s improvement across various epochs. Furthermore, Fig. <xref rid="Fig18" ref-type="fig">18</xref>b showcases that the ResNet50V2 had a low loss curve while being trained. Similarly, the figure addressed above features the confusion matrix which shows that the ResNet50V2 performed just well enough as the DenseNet169 by avoiding misclassification as shown in Fig. <xref rid="Fig19" ref-type="fig">19</xref>.<fig id="Fig18"><label>Fig. 18</label><caption><p>ResNet50V2 accuracy and loss graphs on testing MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig18_HTML" id="MO18"/></fig><fig id="Fig19"><label>Fig. 19</label><caption><p>ResNet50V2 confusion matrix for MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig19_HTML" id="MO19"/></fig></p><p id="Par112">.</p></sec><sec id="Sec36"><title>InceptionResNetV2</title><p id="Par113">In revolutionizing models for analyzing images, the InceptionResNetV2 model managed to score equally well to the ResNet50V2 and DenseNet169 by showcasing 98.5% for all evaluation metrics. The accuracy curve as illustrated in Fig. <xref rid="Fig20" ref-type="fig">20</xref>a started at a significantly low point, however with the help of training the curve managed to steadily progress upwards. The loss curve in Fig. <xref rid="Fig20" ref-type="fig">20</xref>b shows smooth convergence. Moreover, the confusion matrix in Fig. <xref rid="Fig21" ref-type="fig">21</xref> reflects results that are similar to ResNet50V2 and DenseNet169 models which means it is also suitable for image classification.<fig id="Fig20"><label>Fig. 20</label><caption><p>InceptionResNetV2 accuracy and loss graphs on testing MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig20_HTML" id="MO20"/></fig><fig id="Fig21"><label>Fig. 21</label><caption><p>InceptionResNetV2 confusion matrix for MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig21_HTML" id="MO21"/></fig></p><p id="Par114">.</p></sec><sec id="Sec37"><title>InceptionV3</title><p id="Par115">With accuracy, precision, recall and F1 score all being 98.5%, the InceptionV3 model achieved an accuracy of 98.5%. Figure <xref rid="Fig22" ref-type="fig">22</xref>a indicates a constant training period according to the accuracy graph, whilst the loss graph in Fig. <xref rid="Fig22" ref-type="fig">22</xref>b reinforces that there was no excessive overfitting. As seen in the confusion matrix displayed in Fig. <xref rid="Fig23" ref-type="fig">23</xref>, the model achieved results nearly identical to InceptionResNetV2 model, despite having a comparatively greater misclassification rate than the best of the best.<fig id="Fig22"><label>Fig. 22</label><caption><p>InceptionV3 accuracy and loss graphs on testing MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig22_HTML" id="MO22"/></fig><fig id="Fig23"><label>Fig. 23</label><caption><p>InceptionV3 confusion matrix for MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig23_HTML" id="MO23"/></fig></p><p id="Par116">.</p></sec><sec id="Sec38"><title>MobileNet</title><p id="Par117">The MobileNet model was the least performing model out of the set of defined models, with 97.99% of pattern accuracy. However, it had a precision and recall of 98% with the F1 score also being equal to 98%. Figure <xref rid="Fig24" ref-type="fig">24</xref>a illustrates that the accuracy graph remains the same for each epoch which indicates consistency in performance, but the loss graph in Fig. <xref rid="Fig24" ref-type="fig">24</xref>b suggests that the convergence was effective but slower relative to the rest of the models. Emphasizing a higher error rate than the other models, the confusion matrix found in Fig. <xref rid="Fig25" ref-type="fig">25</xref> suggests that mobile net was the least effective model within this context.<fig id="Fig24"><label>Fig. 24</label><caption><p>MobileNet accuracy and loss graphs on testing MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig24_HTML" id="MO24"/></fig><fig id="Fig25"><label>Fig. 25</label><caption><p>MobileNet confusion matrix for MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig25_HTML" id="MO25"/></fig></p><p id="Par118">.</p></sec></sec><sec id="Sec39"><title>Model learning visualization</title><p id="Par119">The visualization of model learning is achieved through the use of a class activation map (CAM). CAM works by localising the different features in an image that assist in the final classification of a certain output and therefore is a significant tool for ensuring that a deep learning model is interpretable<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>. CAM makes images of lower quality that limit the model in certain features during predictions, this explains how to model predicts the states of open or close eyes and works well for drowsiness detection since it prevents the model from focusing on the surrounding areas of the eye and instead enabling them to focus on the eye itself.</p><p id="Par120">In this research, CAM is utilized for classifying the Open-Eyes and Close-Eyes and visualizing their attention maps. The two examples in Fig. <xref rid="Fig26" ref-type="fig">26</xref> demonstrate these results, in which (a) shows the original Open-Eye image together with its attention map, depicting eye-area activations, thus proving that the model can appropriately predict open eyes. Likewise, (b) shows the original Close-Eye image and its attention map with activations mostly found on the area of the closed eyelids showing that the model can accurately ascertain the state of the eye as closed With these visualizations, the model ensures reliability in practical settings such as real time driver drowsiness detection, as stated above, while still improving model interpretable by showing that the model in fact focuses on the most salient features necessary for the classification.<fig id="Fig26"><label>Fig. 26</label><caption><p>Activation map for open-eyes and close-eyes for MRL dataset.</p></caption><graphic xlink:href="41598_2025_2111_Fig26_HTML" id="MO26"/></fig></p></sec><sec id="Sec40"><title>Real-time deployment of drowsiness detection</title><p id="Par121">The OpenCV Library was used for detecting facial landmarks for each video frame sent by a standard webcam during the actual model deployment. To detect the right and left eye, the system first detects the face and then detects the eyes using the Haar cascade classifier. Once the model was trained, the eye states were continuously checked to see which counted frames had eyes that were closed. For this scenario, a drowsiness score was computed, and an alarm was set off when this score was more than 15. The system has been used under varying conditions, such as normal lighting without glasses, with glasses, and in low-light conditions. Fig. <xref rid="Fig27" ref-type="fig">27</xref> shows the results of detection, where (a) indicates the first group of images was captured without glasses, (b) shows the second group of images was captured during average illumination with glasses, and (c) displays the last group of images captured with low brightness. All images include multiple states: open eyes, closed right eye, closed left eye, and fully closed eyes, with an alert activation when drowsiness is detected. The proposed system functions in real-time and does not require any advanced specialized hardware other than a standard webcam, which makes it easier to implement on desktop computers, mobile devices, and other platforms.<fig id="Fig27"><label>Fig. 27</label><caption><p>Real-time detection.</p></caption><graphic xlink:href="41598_2025_2111_Fig27_HTML" id="MO27"/></fig></p><p id="Par122">In addition to accuracy metrics, we performed a comprehensive evaluation of model complexity to assess the efficiency and practical applicability of the proposed framework. As shown in Table <xref rid="Tab11" ref-type="table">11</xref>, we compared the number of trainable parameters and average inference time per image across all models. Transformer-based models such as ViT and Swin Transformer achieved the highest accuracy, significantly outperforming all other models in classification performance. However, these models are relatively heavier and require further optimization to improve inference time for real-time applications. On the other hand, transfer learning models such as MobileNet and VGG19 demonstrated excellent real-time performance due to their lightweight architecture and faster inference time but showed relatively lower accuracy compared to transformer models. This trade-off highlights a key insight: while transformers are optimal for accuracy-critical scenarios, transfer learning models remain a practical solution where real-time constraints are stringent. Furthermore, through optimization techniques such as pruning or quantization, transformer models can be adapted for deployment on edge devices. These findings underscore the need for balancing accuracy and efficiency to ensure scalable and robust real-time driver drowsiness detection.<table-wrap id="Tab11"><label>Table 11</label><caption><p>Model performance and complexity comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Accuracy (%)</th><th align="left">Parameters</th><th align="left">Average inference time (ms/frame)</th><th align="left">Average FPS</th></tr></thead><tbody><tr><td align="left">ViT transformer</td><td align="left">99.15</td><td align="left">85,800,194</td><td align="left"><bold>1021.83</bold></td><td align="left">0.98</td></tr><tr><td align="left">Swin transformer</td><td align="left">99.03</td><td align="left">27,914,108</td><td align="left"><bold>472.8</bold></td><td align="left">2.12</td></tr><tr><td align="left">InceptionV3</td><td align="left">98.5</td><td align="left">23,867,553</td><td align="left"><bold>136.33</bold></td><td align="left">7.35</td></tr><tr><td align="left">InceptionResNetV2</td><td align="left">98.5</td><td align="left">55,851,105</td><td align="left"><bold>122.8</bold></td><td align="left">8.14</td></tr><tr><td align="left">MobileNet</td><td align="left">98</td><td align="left">3,208,001</td><td align="left"><bold>55</bold></td><td align="left">18.18</td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec41"><title>Comparative study</title><p id="Par123">Eye state recognition has faced obstacles in previous studies. More seriously some constraints in resources appear to be difficult not only to improve but also to optimize accuracy with timing. Regarding the work presented in<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> which employs a dual CNN ensemble else, there exist a few other approaches which highlight a significant increase in speed over previous architecture. Their system has achieved unprecedented accuracy of 97.99% on the ZJU dataset which surpassed the previous achievement by 0.79% at 97.20%. Their method based on transfer learning and fine tuning circumvents the issue of overfitting when dealing with small data sets and unlike other models there is no emphasis in parameters or lower recognition rate. Unlike previous solutions such as those raising the problem of gaining speed at the sacrifice of accuracy, or others optimizing for computational efficiency, this system has been able to work efficiently with embedded systems while maintaining high accuracy for multiple datasets concurrently.</p><p id="Par124">In<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, authors proposed the use of deep mastering adaptation mapping of convolutional neural networks&#x02019; practices. To this vision, have been included three CNN models, namely VGG16, VGG19, and new models of 4D. MRL Eye dataset was used to develop teaching sets for the 4D model, which was previously designed only for the assessment of driver drowsiness in more advanced quite dark conditions. The 4D model also had a better predictive performance than both the VGG16 and VGG19 models. This information describes a practical integrated system of a driving simulator comprising an intelligent system that estimates the driver visual state in order to detect drowsiness and provide an appropriate warning for road safety.</p><p id="Par125">In this article<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, the authors conducted an in-depth study with practical applications from their findings of a CNN based real-time, non-invasive drowsiness detection system that analyzes a driver by utilizing video footage obtained from a camera that is installed within the vehicle to assess the driver&#x02019;s fatigue. In the system, yawning, eye states and various facial expressions are classified in order to detect signs of fatigue. The model is based off of a dependable architecture that has been trained with a wide range of datasets containing images taken under different lighting conditions and angles. It also uses Haar cascade classifiers for facial detection alongside fatigue detection technology. Furthermore, the study observes a 96.54% testing accuracy, confirming that CNN models improve safety on roads by decreasing the amount of collisions caused by drowsy drivers&#x02019; inactivity.</p><p id="Par126">Trying to improve over other drowsiness detection methods, the authors in<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> embarked on a quest that involved drowsiness detection deployment through a mobile application which makes use of an amalgamation of transfer learning and CNN. As for their theoretical research, the effectiveness of their mobile app was thoroughly evaluated to establish its practicality which involved gathering numerous datasets. The mobile application operates under the hypothesis that the drowsiness detection system is expected to be between 96 and 98% with the MRL dataset.</p><p id="Par127">The transformer-based model proposed in this paper surpasses all existing methods in the Open-Eyes Vs Close-Eyes classification problem. As opposed to the CNN-based algorithms, the Use of the transformer structures improves the process of feature extraction by paying attention to only relevant features and not unnecessary ones. Testing results with dataset MRL validate our proposed model as noted from the remarkable 99.15% testing accuracy attained using a ViT transformer or 99.03% when using Swin Transformer and all 99 percent of precision, recall and F1 score in respect to most other models tested. The proposed model outperformed all other seven models tested with all the fine-tuned hyperparameters in accuracy as well as robustness. While a number of these models including VGG19, ResNet50V2, MobileNetV2, and Inception V3, DenseNet169, InceptionResNetV2 yielded high scores for accuracy none were able to exceed the efficiency provided by a Transformer based model. This increase in performance reiterates through evidence the strengths associated with self attention mechanisms targeting the distinct eye states feature proving the superiority of the Transformer architecture especially in real time scenarios involving drowsiness detection. The comparison of the results with the previous related works results is displayed in Table <xref rid="Tab12" ref-type="table">12</xref>.<table-wrap id="Tab12"><label>Table 12</label><caption><p>Comparison of results of previous literature results on MRL dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Ref</th><th align="left">Year</th><th align="left"/><th align="left">Model</th><th align="left">Accuracy (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left"><sup><xref ref-type="bibr" rid="CR25">25</xref></sup></td><td align="left">2022</td><td align="left"/><td align="left">Dual CNN Ensemble (DCNNE)</td><td align="left">98.98</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR28">28</xref></sup></td><td align="left">2023</td><td align="left"/><td align="left">VGG16</td><td align="left">95.93</td><td align="left">93.15</td><td align="left">93.87</td><td align="left">&#x02013;</td></tr><tr><td align="left"/><td align="left"/><td align="left"/><td align="left">VGG19</td><td align="left">95.03</td><td align="left">94.82</td><td align="left">95.47</td><td align="left">&#x02013;</td></tr><tr><td align="left"/><td align="left"/><td align="left"/><td align="left">4D</td><td align="left">97.53</td><td align="left">97.35</td><td align="left">97.06</td><td align="left">&#x02013;</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR31">31</xref></sup></td><td align="left">2024</td><td align="left"/><td align="left">VGG16</td><td align="left">95.85</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">- -</td></tr><tr><td align="left"/><td align="left"/><td align="left"/><td align="left">CNN</td><td align="left">96.54</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">- -</td></tr><tr><td align="left"><sup><xref ref-type="bibr" rid="CR18">18</xref></sup></td><td align="left">2024</td><td align="left"/><td align="left">CNN</td><td align="left">96</td><td align="left">around 94:98</td><td align="left">around 94:98</td><td align="left">around 94:98</td></tr><tr><td align="left"/><td align="left"/><td align="left"/><td align="left">MobileNetV2</td><td align="left">97</td><td align="left">99.42</td><td align="left">92.32</td><td align="left">95.61</td></tr><tr><td align="left"/><td align="left"/><td align="left"/><td align="left">InceptionV3</td><td align="left">98</td><td align="left">Around 97:98</td><td align="left">Around 97:98</td><td align="left">Around 97:98</td></tr><tr><td align="left"/><td align="left"/><td align="left"/><td align="left">Proposed swin transformer</td><td align="left">99.03</td><td align="left">99</td><td align="left">99</td><td align="left">99</td></tr><tr><td align="left"/><td align="left"/><td align="left"/><td align="left">Proposed ViT transformer</td><td align="left">99.15</td><td align="left">99</td><td align="left">99</td><td align="left">99</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec42"><title>Discussion</title><p id="Par128">The current study demonstrates the strong potential of transformer-based models, particularly ViT and Swin Transformer, in detecting driver drowsiness with high accuracy and generalizability. The ViT model achieved 99.15% accuracy on the MRL dataset and retained robust performance on unconstrained datasets such as CEW and NTHU-DDD. These results show a marked improvement over traditional CNN-based models (e.g., ResNet50V2 and InceptionV3), particularly in handling occlusions, lighting variability, and facial expressions. The transformer models&#x02019; self-attention mechanisms were instrumental in capturing nuanced spatial features like partial eye closure and facial angle variability. While transformer architectures are computationally more demanding, their superior performance in safety-critical applications such as real-time drowsiness detection justifies the computational trade-off. The results suggest that the proposed system can be effectively implemented in real-time driver assistance platforms.</p><p id="Par129">One notable limitation of this study is the reliance on the MRL Eye Dataset as the primary source for training the deep learning models. While the MRL dataset offers a balanced and well-structured set of close-up eye images, its constrained environment - including grayscale images, close-up framing, and limited variability in lighting and facial characteristics - may restrict the generalizability of the trained models to real-world conditions. Despite these limitations, MRL was chosen due to its large volume of well-labeled eye state images, which provide a reliable foundation for training and benchmarking classification models. To mitigate the risk of overfitting to this controlled dataset and to assess the robustness of our models, we extended our evaluation to include two additional datasets: NTHU-DDD, which provides annotated video sequences under diverse lighting, pose, and behavioral conditions; and CEW, which includes unconstrained facial images with variations in head orientation, facial expressions, and occlusions such as glasses. These supplementary evaluations helped test the adaptability of the proposed models beyond the conditions present in MRL and confirmed their competitive performance in more diverse, real-world scenarios. Nonetheless, future work will aim to further enhance generalizability through training on larger, real-world datasets and integrating techniques such as domain adaptation and field-deployable validation. We also plan to combine these datasets in real-time applications to further improve robustness and practical deployment.</p><p id="Par130">While the ViT model achieved a high classification accuracy of 99.15% on the MRL dataset, and similarly high results were obtained on NTHU-DDD and CEW, these metrics do not guarantee equivalent performance in real-world driving environments. Real-world conditions introduce unpredictable variables such as poor or rapidly changing lighting, head movements, facial occlusions due to sunglasses or hair, and differences in driver facial features and skin tones. Although the CEW and NTHU-DDD datasets were included specifically to simulate some of these conditions - such as varied facial angles, occlusions, and lighting - it remains a limitation that the system has not yet been field-tested in uncontrolled environments. Future work will involve deploying the system in real vehicles under various real-world conditions to assess its robustness and make necessary model adjustments. Additionally, incorporating adaptive learning or domain adaptation techniques may further help mitigate performance drops caused by environmental variability.</p><p id="Par131">Another limitation of the current study lies in the granularity of the labels within the MRL dataset. Since it only includes static images categorized into binary eye states - Open and Closed - the models trained on this dataset are inherently constrained in their ability to detect more subtle or gradual signs of drowsiness, such as heavy blinking, partial eye closure, or slow eyelid drooping. These nuances often precede full eye closure and are critical for early intervention. To partially address this issue, we incorporated the NTHU-DDD dataset in our evaluation, which includes annotated behavioral cues like slow blinking, yawning, and nodding across video frames. However, fully capturing these transitional or temporal features requires future work involving sequential modeling techniques (e.g., RNNs or LSTM-based approaches) and datasets with continuous drowsiness annotations. Expanding to such models would enable the system to respond more sensitively to early drowsiness indicators, improving real-world applicability and safety impact.</p><p id="Par132">While transfer learning and pre-trained models such as ViT, Swin Transformer, and VGG19 enabled significant performance improvements in this study, these models come with inherent limitations. Since they are initially trained on large-scale general datasets (e.g., ImageNet), their learned features may not fully capture the specific nuances of driver drowsiness behavior, particularly under unique environmental or cultural contexts. Moreover, highly complex pre-trained models can sometimes overfit to training data when fine-tuned on smaller or less diverse datasets, potentially reducing adaptability in real-world deployment. To mitigate this, we employed data augmentation and cross-dataset validation using CEW and NTHU-DDD. However, future research could explore domain-specific model pretraining or self-supervised approaches tailored to driver monitoring systems, improving robustness and generalization across unseen environments.</p><p id="Par133">Compared to prior models that rely primarily on CNN architectures, such as VGG19 or Inception-based networks, the proposed transformer-based models - particularly ViT and Swin Transformer - demonstrated significantly higher robustness across datasets with real-world variability (e.g., CEW and NTHU-DDD). The ViT model achieved 99.15% accuracy on the MRL dataset and retained strong performance on more unconstrained data from CEW (97.25%), where models like ResNet50V2 and InceptionV3 showed lower generalization. This suggests that the self-attention mechanism in transformers allows for better spatial feature representation, especially when dealing with subtle visual cues like partial eye closure or facial occlusion. While transformer models are generally more computationally intensive than traditional CNNs, their improved accuracy in complex conditions justifies the trade-off for real-time safety-critical applications. Moreover, unlike models tailored to a single dataset or environment, our approach generalized well across multiple benchmark datasets, highlighting its adaptability and robustness in diverse scenarios. Future comparative studies will aim to evaluate these models in field conditions and compare not only accuracy but also latency and resource efficiency for embedded deployment.</p><p id="Par134">To address concerns about generalizability and real-world applicability, we expanded our evaluation to include two challenging and diverse datasets: NTHU-DDD and CEW. These datasets present complex driving-related conditions such as varied head poses, facial expressions, occlusions (e.g., sunglasses), and lighting conditions, making them suitable for assessing the robustness of our proposed framework. The high performance of our model on these datasets supports its adaptability to more realistic settings. Nonetheless, we recognize that deeper behavioral analysis - such as incorporating multimodal inputs like head pose estimation, facial emotion recognition, or yawning detection - would strengthen the system&#x02019;s ability to model driver fatigue more comprehensively. In future work, we plan to expand the model to integrate these cues, enabling a richer representation of driver state and supporting more accurate, context-aware interventions for road safety.</p><p id="Par135">While transformer-based models demonstrated superior accuracy, they were more computationally intensive compared to transfer learning models. In contrast, transfer learning models offered faster inference but slightly lower accuracy. This trade-off highlights the importance of selecting the appropriate model based on the deployment environment-prioritizing accuracy for high-performance systems and speed for real-time, resource-constrained applications.</p></sec><sec id="Sec43"><title>Conclusion and future directions</title><p id="Par136">This work presents a highly effective deep learning-based framework for real-time driver drowsiness detection, achieving superior results through the application of state-of-the-art transformer architectures and transfer learning models. The proposed system, which classifies eye states into &#x0201c;Open-Eyes&#x0201d; and &#x0201c;Close-Eyes&#x0201d; using the MRL Eye Dataset, demonstrated exceptional performance, with the Vision Transformer (ViT) and Swin Transformer models achieving remarkable accuracies of 99.15% and 99.03%, respectively. These results significantly outperform existing methods, highlighting the superiority of transformer-based models in capturing complex spatial dependencies and extracting relevant features for drowsiness detection. The integration of Haar Cascade classifiers for real-time face and eye detection, coupled with a drowsiness scoring mechanism, ensures timely and accurate alerts, enhancing road safety by mitigating the risks associated with fatigue-related accidents. The proposed framework was rigorously tested under various lighting conditions, including normal, low-light, and scenarios involving glasses, demonstrating its robustness and adaptability for real-world deployment. The use of Class Activation Mapping (CAM) further enhanced model interpretability, ensuring that the system focuses on critical eye regions for accurate classification. Compared to existing methods, the proposed framework offers a significant improvement in accuracy, precision, recall, and F1-score, making it a reliable and efficient solution for real-time drowsiness detection. Future work will focus on optimizing the system for deployment on resource-constrained embedded devices, exploring multi-modal approaches that integrate additional physiological signals, and extending the dataset to include more diverse driving scenarios. By addressing these challenges, the proposed framework has the potential to significantly contribute to the development of advanced driver assistance systems, ultimately reducing the incidence of drowsiness-related accidents and improving overall road safety. While our real-time tests included glasses and low-light scenarios through 3 different datasets, broader dataset diversity is needed for universal deployment. Future work will integrate multi-modal inputs (e.g., infrared cameras for low-light) and federated learning to adapt to unseen environments. The superior results achieved in this work underscore the effectiveness of the proposed approach and its potential for real-world application. Current drowsiness detection systems, including ours, primarily react to observable symptoms (e.g., eye closure). However, proactive prevention requires identifying pre-fatigue cues (e.g., increased blink duration, gaze instability) before severe drowsiness occurs. While our transformer models achieve 99.15% accuracy in classifying eye states, future iterations must integrate temporal dynamics and multi-modal signals (e.g., steering patterns, head nods) to predict drowsiness onset.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization, M.A.M.; methodology, O.F. and B.H.; software, A.F.I. , A.G., and O.F.; validation, A.F.I. , A.G., and O.F.; formal analysis, A.F.I., A.G., and O.F.; investigation, B.H. , A.G., and M.A.M.; resources, B.H. and M.A.M.; data curation, A.F.I. and A.G.; writing-original draft preparation, A.F.I., O.F., and A.G.; writing-review and editing, M.A.M., O.F., A.G. and B.H. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>Open access funding provided by The Science, Technology &#x00026; Innovation Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank (EKB).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The paper has a datasets available in the Kaggle repository MRL Dataset: <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/imadeddinedjerarda/mrl-eye-dataset">https://www.kaggle.com/datasets/imadeddinedjerarda/mrl-eye-dataset</ext-link> NTHU-DDD Dataset: <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/banudeep/nthuddd2">https://www.kaggle.com/datasets/banudeep/nthuddd2</ext-link> CEW Dataset: <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/ahamedfarouk/cew-dataset">https://www.kaggle.com/datasets/ahamedfarouk/cew-dataset</ext-link></p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par141">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Quddus, A., Zandi, A. S., Prest, L. &#x00026; Comeau, F. J. Using long short term memory and convolutional neural networks for driver drowsiness detection. <italic>Accident Anal.</italic><italic>Prevent.</italic><bold>156</bold>, 106107 (2021).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">CDC Newsroom. CDC Newsroom: Sleep Deprivation and Public Health (2016).</mixed-citation></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>El-Nabi</surname><given-names>SA</given-names></name><etal/></person-group><article-title>Machine learning and deep learning techniques for driver fatigue and drowsiness detection: A review</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>83</volume><fpage>9441</fpage><lpage>9477</lpage><pub-id pub-id-type="doi">10.1007/s11042-023-15054-0</pub-id></element-citation><mixed-citation id="mc-CR3" publication-type="journal">El-Nabi, S. A. et al. Machine learning and deep learning techniques for driver fatigue and drowsiness detection: A review. <italic>Multimed. Tools Appl.</italic><bold>83</bold>, 9441&#x02013;9477 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>D</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Ma</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Cong</surname><given-names>H</given-names></name></person-group><article-title>Road traffic accident severity analysis: A census-based study in China</article-title><source>J. Saf. Res.</source><year>2019</year><volume>70</volume><fpage>135</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.jsr.2019.06.002</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Wang, D., Liu, Q., Ma, L., Zhang, Y. &#x00026; Cong, H. Road traffic accident severity analysis: A census-based study in China. <italic>J. Saf. Res.</italic><bold>70</bold>, 135&#x02013;147 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Smolensky</surname><given-names>MH</given-names></name><name><surname>Di Milia</surname><given-names>L</given-names></name><name><surname>Ohayon</surname><given-names>MM</given-names></name><name><surname>Philip</surname><given-names>P</given-names></name></person-group><article-title>Sleep disorders, medical conditions, and road accident risk</article-title><source>Accident Anal. Prevent.</source><year>2011</year><volume>43</volume><fpage>533</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1016/j.aap.2009.12.004</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Smolensky, M. H., Di Milia, L., Ohayon, M. M. &#x00026; Philip, P. Sleep disorders, medical conditions, and road accident risk. <italic>Accident Anal. Prevent.</italic><bold>43</bold>, 533&#x02013;548 (2011).</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Chan</surname><given-names>AH</given-names></name></person-group><article-title>Sleepiness and the risk of road accidents for professional drivers: A systematic review and meta-analysis of retrospective studies</article-title><source>Saf. Sci.</source><year>2014</year><volume>70</volume><fpage>180</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/j.ssci.2014.05.022</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Zhang, T. &#x00026; Chan, A. H. Sleepiness and the risk of road accidents for professional drivers: A systematic review and meta-analysis of retrospective studies. <italic>Saf. Sci.</italic><bold>70</bold>, 180&#x02013;188 (2014).</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Jabbar, R. et al. Driver drowsiness detection model using convolutional neural networks techniques for android application. In <italic>2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)</italic>. 237&#x02013;242 (IEEE, 2020).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Abd El-Nabi, S., El-Rabaie, E.-S.&#x000a0;M., Emam, A. &#x00026; Ramadan, K.&#x000a0;F. An efficient approach of drowsiness detection based on resnet50 and xception architectures. In <italic>2023 3rd International Conference on Electronic Engineering (ICEEM)</italic>. 1&#x02013;6 (IEEE, 2023).</mixed-citation></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Alghanim</surname><given-names>M</given-names></name><etal/></person-group><article-title>A hybrid deep neural network approach to recognize driving fatigue based on EEG signals</article-title><source>Int. J. Intell. Syst.</source><year>2024</year><volume>2024</volume><fpage>9898333</fpage><pub-id pub-id-type="doi">10.1155/2024/9898333</pub-id></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Alghanim, M. et al. A hybrid deep neural network approach to recognize driving fatigue based on EEG signals. <italic>Int. J. Intell. Syst.</italic><bold>2024</bold>, 9898333 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Dua, M., Shakshi, Singla, R., Raj, S. &#x00026; Jangra, A. Deep CNN models-based ensemble approach to driver drowsiness detection. <italic>Neural Comput. Appl.</italic><bold>33</bold>, 3155&#x02013;3168 (2021).</mixed-citation></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmed</surname><given-names>MIB</given-names></name><etal/></person-group><article-title>A deep-learning approach to driver drowsiness detection</article-title><source>Safety</source><year>2023</year><volume>9</volume><fpage>65</fpage><pub-id pub-id-type="doi">10.3390/safety9030065</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Ahmed, M. I. B. et al. A deep-learning approach to driver drowsiness detection. <italic>Safety</italic><bold>9</bold>, 65 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Azmi, M. M. B.&#x000a0;M. &#x00026; Zaman, F. H.&#x000a0;K. Driver drowsiness detection using vision transformer. In <italic>2024 IEEE 14th Symposium on Computer Applications</italic> &#x00026; <italic>Industrial Electronics (ISCAIE)</italic>. 329&#x02013;336 (IEEE, 2024).</mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Gomaa</surname><given-names>MW</given-names></name><name><surname>Mahmoud</surname><given-names>RO</given-names></name><name><surname>Sarhan</surname><given-names>AM</given-names></name></person-group><article-title>A CNN-LSTM-based deep learning approach for driver drowsiness prediction</article-title><source>J. Eng. Res.</source><year>2022</year><volume>6</volume><fpage>59</fpage><lpage>70</lpage></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Gomaa, M. W., Mahmoud, R. O. &#x00026; Sarhan, A. M. A CNN-LSTM-based deep learning approach for driver drowsiness prediction. <italic>J. Eng. Res.</italic><bold>6</bold>, 59&#x02013;70 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Gomaa, A. Advanced domain adaptation technique for object detection leveraging semi-automated dataset construction and enhanced yolov8. In <italic>2024 6th Novel Intelligent and Leading Emerging Sciences Conference (NILES)</italic>. 211&#x02013;214 (IEEE, 2024).</mixed-citation></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Gomaa</surname><given-names>A</given-names></name><name><surname>Abdalrazik</surname><given-names>A</given-names></name></person-group><article-title>Novel deep learning domain adaptation approach for object detection using semi-self building dataset and modified yolov4</article-title><source>World Electr. Veh. J.</source><year>2024</year><volume>15</volume><fpage>255</fpage><pub-id pub-id-type="doi">10.3390/wevj15060255</pub-id></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Gomaa, A. &#x00026; Abdalrazik, A. Novel deep learning domain adaptation approach for object detection using semi-self building dataset and modified yolov4. <italic>World Electr. Veh. J.</italic><bold>15</bold>, 255 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Gomaa, A. &#x00026; Saad, O.&#x000a0;M. Residual channel-attention (RCA) network for remote sensing image scene classification. <italic>Multimed. Tools Appl.</italic> 1&#x02013;25 (2025).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Salem, M., Gomaa, A. &#x00026; Tsurusaki, N. Detection of earthquake-induced building damages using remote sensing data and deep learning: A case study of Mashiki town, Japan. In <italic>IGARSS 2023-2023 IEEE International Geoscience and Remote Sensing Symposium</italic>. 2350&#x02013;2353 (IEEE, 2023).</mixed-citation></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Salem</surname><given-names>D</given-names></name><name><surname>Waleed</surname><given-names>M</given-names></name></person-group><article-title>Drowsiness detection in real-time via convolutional neural networks and transfer learning</article-title><source>J. Eng. Appl. Sci.</source><year>2024</year><volume>71</volume><fpage>122</fpage><pub-id pub-id-type="doi">10.1186/s44147-024-00457-z</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Salem, D. &#x00026; Waleed, M. Drowsiness detection in real-time via convolutional neural networks and transfer learning. <italic>J. Eng. Appl. Sci.</italic><bold>71</bold>, 122 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Zandi</surname><given-names>AS</given-names></name><name><surname>Quddus</surname><given-names>A</given-names></name><name><surname>Prest</surname><given-names>L</given-names></name><name><surname>Comeau</surname><given-names>FJ</given-names></name></person-group><article-title>Non-intrusive detection of drowsy driving based on eye tracking data</article-title><source>Transport. Res. Rec.</source><year>2019</year><volume>2673</volume><fpage>247</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.1177/0361198119847985</pub-id></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Zandi, A. S., Quddus, A., Prest, L. &#x00026; Comeau, F. J. Non-intrusive detection of drowsy driving based on eye tracking data. <italic>Transport. Res. Rec.</italic><bold>2673</bold>, 247&#x02013;257 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>W</given-names></name><name><surname>Wu</surname><given-names>R</given-names></name></person-group><article-title>Real-time driver-drowsiness detection system using facial features</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>118727</fpage><lpage>118738</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2936663</pub-id></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Deng, W. &#x00026; Wu, R. Real-time driver-drowsiness detection system using facial features. <italic>IEEE Access</italic><bold>7</bold>, 118727&#x02013;118738 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Celecia</surname><given-names>A</given-names></name><name><surname>Figueiredo</surname><given-names>K</given-names></name><name><surname>Vellasco</surname><given-names>M</given-names></name><name><surname>Gonz&#x000e1;lez</surname><given-names>R</given-names></name></person-group><article-title>A portable fuzzy driver drowsiness estimation system</article-title><source>Sensors</source><year>2020</year><volume>20</volume><fpage>4093</fpage><pub-id pub-id-type="doi">10.3390/s20154093</pub-id><pub-id pub-id-type="pmid">32717787</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Celecia, A., Figueiredo, K., Vellasco, M. &#x00026; Gonz&#x000e1;lez, R. A portable fuzzy driver drowsiness estimation system. <italic>Sensors</italic><bold>20</bold>, 4093 (2020).<pub-id pub-id-type="pmid">32717787</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Kiashari</surname><given-names>SEH</given-names></name><name><surname>Nahvi</surname><given-names>A</given-names></name><name><surname>Bakhoda</surname><given-names>H</given-names></name><name><surname>Homayounfard</surname><given-names>A</given-names></name><name><surname>Tashakori</surname><given-names>M</given-names></name></person-group><article-title>Evaluation of driver drowsiness using respiration analysis by thermal imaging on a driving simulator</article-title><source>Multimed. Tools Appl.</source><year>2020</year><volume>79</volume><fpage>17793</fpage><lpage>17815</lpage><pub-id pub-id-type="doi">10.1007/s11042-020-08696-x</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Kiashari, S. E. H., Nahvi, A., Bakhoda, H., Homayounfard, A. &#x00026; Tashakori, M. Evaluation of driver drowsiness using respiration analysis by thermal imaging on a driving simulator. <italic>Multimed. Tools Appl.</italic><bold>79</bold>, 17793&#x02013;17815 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Hashemi</surname><given-names>M</given-names></name><name><surname>Mirrashid</surname><given-names>A</given-names></name><name><surname>Beheshti Shirazi</surname><given-names>A</given-names></name></person-group><article-title>Driver safety development: Real-time driver drowsiness detection system based on convolutional neural network</article-title><source>SN Comput. Sci.</source><year>2020</year><volume>1</volume><fpage>289</fpage><pub-id pub-id-type="doi">10.1007/s42979-020-00306-9</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Hashemi, M., Mirrashid, A. &#x00026; Beheshti Shirazi, A. Driver safety development: Real-time driver drowsiness detection system based on convolutional neural network. <italic>SN Comput. Sci.</italic><bold>1</bold>, 289 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Maior, C. B.&#x000a0;S., das Chagas Moura, M.&#x000a0;J., Santana, J. M.&#x000a0;M. &#x00026; Lins, I.&#x000a0;D. Real-time classification for autonomous drowsiness detection using eye aspect ratio. <italic>Exp. Syst. Appl.</italic><bold>158</bold>, 113505 (2020).</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Saurav</surname><given-names>S</given-names></name><name><surname>Gidde</surname><given-names>P</given-names></name><name><surname>Saini</surname><given-names>R</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name></person-group><article-title>Real-time eye state recognition using dual convolutional neural network ensemble</article-title><source>J. Real-Time Image Process.</source><year>2022</year><volume>19</volume><fpage>607</fpage><lpage>622</lpage><pub-id pub-id-type="doi">10.1007/s11554-022-01211-5</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Saurav, S., Gidde, P., Saini, R. &#x00026; Singh, S. Real-time eye state recognition using dual convolutional neural network ensemble. <italic>J. Real-Time Image Process.</italic><bold>19</bold>, 607&#x02013;622 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Mag&#x000e1;n</surname><given-names>E</given-names></name><name><surname>Sesmero</surname><given-names>MP</given-names></name><name><surname>Alonso-Weber</surname><given-names>JM</given-names></name><name><surname>Sanchis</surname><given-names>A</given-names></name></person-group><article-title>Driver drowsiness detection by applying deep learning techniques to sequences of images</article-title><source>Appl. Sci.</source><year>2022</year><volume>12</volume><fpage>1145</fpage><pub-id pub-id-type="doi">10.3390/app12031145</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Mag&#x000e1;n, E., Sesmero, M. P., Alonso-Weber, J. M. &#x00026; Sanchis, A. Driver drowsiness detection by applying deep learning techniques to sequences of images. <italic>Appl. Sci.</italic><bold>12</bold>, 1145 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Pandey, R., Bhasin, P., Popli, S., Sharma, M. &#x00026; Sharma, N. Driver drowsiness detection and traffic sign recognition system. In <italic>Emerging Technologies in Data Mining and Information Security: Proceedings of IEMIS 2022</italic>. Vol.&#x000a0;1. 25&#x02013;40 (Springer, 2022).</mixed-citation></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Jahan</surname><given-names>I</given-names></name><etal/></person-group><article-title>4D: A real-time driver drowsiness detector using deep learning</article-title><source>Electronics</source><year>2023</year><volume>12</volume><fpage>235</fpage><pub-id pub-id-type="doi">10.3390/electronics12010235</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Jahan, I. et al. 4D: A real-time driver drowsiness detector using deep learning. <italic>Electronics</italic><bold>12</bold>, 235 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Albadawi</surname><given-names>Y</given-names></name><name><surname>AlRedhaei</surname><given-names>A</given-names></name><name><surname>Takruri</surname><given-names>M</given-names></name></person-group><article-title>Real-time machine learning-based driver drowsiness detection using visual features</article-title><source>J. Imaging</source><year>2023</year><volume>9</volume><fpage>91</fpage><pub-id pub-id-type="doi">10.3390/jimaging9050091</pub-id><pub-id pub-id-type="pmid">37233309</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Albadawi, Y., AlRedhaei, A. &#x00026; Takruri, M. Real-time machine learning-based driver drowsiness detection using visual features. <italic>J. Imaging</italic><bold>9</bold>, 91 (2023).<pub-id pub-id-type="pmid">37233309</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Mate</surname><given-names>P</given-names></name><name><surname>Apte</surname><given-names>N</given-names></name><name><surname>Parate</surname><given-names>M</given-names></name><name><surname>Sharma</surname><given-names>S</given-names></name></person-group><article-title>Detection of driver drowsiness using transfer learning techniques</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>83</volume><fpage>35553</fpage><lpage>35582</lpage><pub-id pub-id-type="doi">10.1007/s11042-023-16952-z</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Mate, P., Apte, N., Parate, M. &#x00026; Sharma, S. Detection of driver drowsiness using transfer learning techniques. <italic>Multimed. Tools Appl.</italic><bold>83</bold>, 35553&#x02013;35582 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Makhmudov</surname><given-names>F</given-names></name><name><surname>Turimov</surname><given-names>D</given-names></name><name><surname>Xamidov</surname><given-names>M</given-names></name><name><surname>Nazarov</surname><given-names>F</given-names></name><name><surname>Cho</surname><given-names>Y-I</given-names></name></person-group><article-title>Real-time fatigue detection algorithms using machine learning for yawning and eye state</article-title><source>Sensors</source><year>2024</year><volume>24</volume><fpage>7810</fpage><pub-id pub-id-type="doi">10.3390/s24237810</pub-id><pub-id pub-id-type="pmid">39686347</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">Makhmudov, F., Turimov, D., Xamidov, M., Nazarov, F. &#x00026; Cho, Y.-I. Real-time fatigue detection algorithms using machine learning for yawning and eye state. <italic>Sensors</italic><bold>24</bold>, 7810 (2024).<pub-id pub-id-type="pmid">39686347</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Dairi</surname><given-names>A</given-names></name><name><surname>Harrou</surname><given-names>F</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name></person-group><article-title>Efficient driver drunk detection by sensors: A manifold learning-based anomaly detector</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>119001</fpage><lpage>119012</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3221145</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Dairi, A., Harrou, F. &#x00026; Sun, Y. Efficient driver drunk detection by sensors: A manifold learning-based anomaly detector. <italic>IEEE Access</italic><bold>10</bold>, 119001&#x02013;119012 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Harrou</surname><given-names>F</given-names></name><name><surname>Kini</surname><given-names>KR</given-names></name><name><surname>Madakyaru</surname><given-names>M</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name></person-group><article-title>A semi-supervised anomaly detection strategy for drunk driving detection: A feasibility study</article-title><source>Front. Sens.</source><year>2024</year><volume>5</volume><fpage>1375034</fpage><pub-id pub-id-type="doi">10.3389/fsens.2024.1375034</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Harrou, F., Kini, K. R., Madakyaru, M. &#x00026; Sun, Y. A semi-supervised anomaly detection strategy for drunk driving detection: A feasibility study. <italic>Front. Sens.</italic><bold>5</bold>, 1375034 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Viola, P. &#x00026; Jones, M. Rapid object detection using a boosted cascade of simple features. In <italic>Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001</italic>. Vol.&#x000a0;1. I&#x02013;I (IEEE, 2001).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Bajaj, S. et al. A real-time driver drowsiness detection using opencv, dlib. In <italic>ICT Analysis and Applications: Proceedings of ICT4SD 2022</italic>. 639&#x02013;649 (Springer, 2022).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Vaswani, A. Attention is all you need. In <italic>Advances in Neural Information Processing Systems</italic> (2017).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Dosovitskiy, A. An image is worth 16 x 16 words: Transformers for image recognition at scale. <italic>arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2010.11929">arXiv:2010.11929</ext-link> (2020).</mixed-citation></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>H</given-names></name><name><surname>Xiao</surname><given-names>Z</given-names></name><name><surname>Ji</surname><given-names>P</given-names></name></person-group><article-title>Real-time fatigue driving detection system based on multi-module fusion</article-title><source>Comput. Graph.</source><year>2022</year><volume>108</volume><fpage>22</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1016/j.cag.2022.09.001</pub-id></element-citation><mixed-citation id="mc-CR38" publication-type="journal">Jia, H., Xiao, Z. &#x00026; Ji, P. Real-time fatigue driving detection system based on multi-module fusion. <italic>Comput. Graph.</italic><bold>108</bold>, 22&#x02013;33 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In <italic>Proceedings of the IEEE/CVF International Conference on Computer Vision</italic>. 10012&#x02013;10022 (2021).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Song, C., Song, Q. &#x00026; Cao, F. Driver distraction detection based on improved swin transformer. In <italic>2024 43rd Chinese Control Conference (CCC)</italic>. 8032&#x02013;8037 (IEEE, 2024).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Simonyan, K. &#x00026; Zisserman, A. Very deep convolutional networks for large-scale image recognition. <italic>arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.1556">arXiv:1409.1556</ext-link> (2014).</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Bansal, M., Kumar, M., Sachdeva, M. &#x00026; Mittal, A. Transfer learning for image classification using vgg19: Caltech-101 image data set. <italic>J. Ambient Intell. Hum. Comput.</italic> 1&#x02013;12 (2023).</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Chen, L. et al. SCA-CNN: Spatial and channel-wise attention in convolutional networks for image captioning. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>. 5659&#x02013;5667 (2017).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Huang, G., Liu, Z., Van Der Maaten, L. &#x00026; Weinberger, K.&#x000a0;Q. Densely connected convolutional networks. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>. 4700&#x02013;4708 (2017).</mixed-citation></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Ranjan</surname><given-names>A</given-names></name><name><surname>Sharma</surname><given-names>S</given-names></name><name><surname>Mate</surname><given-names>P</given-names></name><name><surname>Verma</surname><given-names>A</given-names></name></person-group><article-title>An efficient deep learning technique for driver drowsiness detection</article-title><source>SN Comput. Sci.</source><year>2024</year><volume>5</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1007/s42979-024-03316-z</pub-id></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Ranjan, A., Sharma, S., Mate, P. &#x00026; Verma, A. An efficient deep learning technique for driver drowsiness detection. <italic>SN Comput. Sci.</italic><bold>5</bold>, 1&#x02013;18 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &#x00026; Sun, J. Deep residual learning for image recognition. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>. 770&#x02013;778 (2016).</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Yadav, A.&#x000a0;K., Sharma, A. et al. Real time drowsiness detection system based on resnet-50. In <italic>2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS)</italic>. 1&#x02013;7 (IEEE, 2022).</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Szegedy, C., Ioffe, S., Vanhoucke, V. &#x00026; Alemi, A. Inception-v4, inception-resnet and the impact of residual connections on learning. In <italic>Proceedings of the AAAI Conference on Artificial Intelligence</italic>. Vol.&#x000a0;31 (2017).</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. &#x00026; Wojna, Z. Rethinking the inception architecture for computer vision. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>. 2818&#x02013;2826 (2016).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Howard, A.&#x000a0;G. Mobilenets: Efficient convolutional neural networks for mobile vision applications. <italic>arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1704.04861">arXiv:1704.04861</ext-link> (2017).</mixed-citation></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Yao</surname><given-names>W</given-names></name><name><surname>Fu</surname><given-names>R</given-names></name></person-group><article-title>Real driving environment EEG-based detection of driving fatigue using the wavelet scattering network</article-title><source>J. Neurosci. Methods</source><year>2023</year><volume>400</volume><fpage>109983</fpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2023.109983</pub-id><pub-id pub-id-type="pmid">37838152</pub-id>
</element-citation><mixed-citation id="mc-CR51" publication-type="journal">Wang, F., Chen, D., Yao, W. &#x00026; Fu, R. Real driving environment EEG-based detection of driving fatigue using the wavelet scattering network. <italic>J. Neurosci. Methods</italic><bold>400</bold>, 109983 (2023).<pub-id pub-id-type="pmid">37838152</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name><surname>Hossain</surname><given-names>MU</given-names></name><etal/></person-group><article-title>Automatic driver distraction detection using deep convolutional neural networks</article-title><source>Intel. Syst. Appl.</source><year>2022</year><volume>14</volume><fpage>200075</fpage></element-citation><mixed-citation id="mc-CR52" publication-type="journal">Hossain, M. U. et al. Automatic driver distraction detection using deep convolutional neural networks. <italic>Intel. Syst. Appl.</italic><bold>14</bold>, 200075 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Abd El-Nabi, S. et al. Driver drowsiness detection using swin transformer and diffusion models for robust image denoising. <italic>IEEE Access</italic> (2025).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Minhas, A.&#x000a0;A., Jabbar, S., Farhan, M. &#x00026; Najam ul Islam, M. A smart analysis of driver fatigue and drowsiness detection using convolutional neural networks. <italic>Multimed. Tools Appl.</italic><bold>81</bold>, 26969&#x02013;26986 (2022).</mixed-citation></ref></ref-list></back></article>