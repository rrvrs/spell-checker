<!--

File produced by pipelineRunner package (for JATS 2 SCJATS with pipeline SCJATS)
At: 2025-05-22T15:36:18.445Z

Version        : 1.16.1
Last update    : 2024-08-27
Modified by    : dunnm

-->
<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Soc Cogn Affect Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Soc Cogn Affect Neurosci</journal-id><journal-id journal-id-type="publisher-id">scan</journal-id><journal-title-group><journal-title>Social Cognitive and Affective Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1749-5016</issn><issn pub-type="epub">1749-5024</issn><publisher><publisher-name>Oxford University Press</publisher-name><publisher-loc>UK</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39417256</article-id><article-id pub-id-type="pmc">PMC12036660</article-id>
<article-id pub-id-type="doi">10.1093/scan/nsae071</article-id><article-id pub-id-type="publisher-id">nsae071</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Research &#x02013; Neuroscience</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI01880</subject></subj-group></article-categories><title-group><article-title>A beautiful face is good when we&#x02019;re judged by others, a moral character is better</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1212-5244</contrib-id><name><surname>Baum</surname><given-names>Julia</given-names></name><!--julia.baum@hu-berlin.de--><aff>
<institution content-type="department">Faculty of Life Sciences, Department of Psychology, Humboldt-Universit&#x000e4;t zu Berlin</institution>, Berlin 10099, <country country="DE">Germany</country></aff><aff>
<institution content-type="department">Humboldt-Universit&#x000e4;t zu Berlin, Faculty of Philosophy, Berlin School of Mind and Brain</institution>, Berlin 10099, <country country="DE">Germany</country></aff><xref rid="COR0001" ref-type="corresp"/></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8438-1570</contrib-id><name><surname>Abdel Rahman</surname><given-names>Rasha</given-names></name><aff>
<institution content-type="department">Faculty of Life Sciences, Department of Psychology, Humboldt-Universit&#x000e4;t zu Berlin</institution>, Berlin 10099, <country country="DE">Germany</country></aff><aff>
<institution content-type="department">Humboldt-Universit&#x000e4;t zu Berlin, Faculty of Philosophy, Berlin School of Mind and Brain</institution>, Berlin 10099, <country country="DE">Germany</country></aff></contrib></contrib-group><author-notes><corresp id="COR0001">*Corresponding author. Humboldt-Universit&#x000e4;t zu Berlin, Institut f&#x000fc;r Psychologie, Unter den Linden 6, Berlin 10099, Germany. E-mail: <email xlink:href="julia.baum@hu-berlin.de">julia.baum@hu-berlin.de</email></corresp></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub" iso-8601-date="2024-10-17"><day>17</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>17</day><month>10</month><year>2024</year></pub-date><volume>20</volume><issue>1</issue><elocation-id>nsae071</elocation-id><history><date date-type="received"><day>11</day><month>8</month><year>2023</year></date><date date-type="rev-recd"><day>19</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>05</day><month>4</month><year>2025</year></date><date date-type="editorial-decision"><day>23</day><month>9</month><year>2024</year></date><date date-type="corrected-typeset"><day>28</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024. Published by Oxford University Press.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="nsae071.pdf"/><abstract><title>Abstract</title><p>Moral beauty, reflected in one&#x02019;s actions, and facial beauty both affect how we are judged. Here, we investigated how moral and facial beauty interact to affect social judgments and emotional responses, employing event-related brain potentials (ERPs). All participants (female) associated positive, neutral, or negative verbal information with faces scoring high or low on attractiveness and performed ratings of the faces as manipulation checks. In a separate test phase, the faces were presented again, and participants made valenced social judgments of the persons. Results show a dominance of moral beauty in valenced social judgments as well as ERPs related to reflexive and evaluative emotional responses (early posterior negativity and late positive potential), whereas facial attractiveness mattered little. In contrast, facial attractiveness affected visual processing (N170). Similarly, relatively shallow impressions of attractiveness and likability that require no knowledge about the person were influenced by both facial attractiveness and social-emotional information. This pattern of dominant effects of social-emotional information regardless of attractiveness shows that when it comes to our emotional responses and social judgments, moral beauty is what matters most, even in the face of physical beauty.</p></abstract><kwd-group><kwd>face processing</kwd><kwd>attractiveness</kwd><kwd>halo of beauty</kwd><kwd>social-emotional person knowledge</kwd><kwd>social judgments</kwd><kwd>event-related potentials</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Studienstiftung des Deutschen Volkes</institution><institution-id institution-id-type="DOI">10.13039/501100004350</institution-id></institution-wrap>
</funding-source><award-id>Professorin Ruebsamen-Schaeff Stipend</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Deutsche Forschungsgemeinschaft</institution><institution-id institution-id-type="DOI">10.13039/501100001659</institution-id></institution-wrap>
</funding-source><award-id>AB 277-6</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Humboldt-Research Track</institution></institution-wrap>
</funding-source></award-group></funding-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Studienstiftung des Deutschen Volkes</institution><institution-id institution-id-type="DOI">10.13039/501100004350</institution-id></institution-wrap>
</funding-source><award-id>Professorin Ruebsamen-Schaeff Stipend</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Deutsche Forschungsgemeinschaft</institution><institution-id institution-id-type="DOI">10.13039/501100001659</institution-id></institution-wrap>
</funding-source><award-id>AB 277-6</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Humboldt-Research Track</institution></institution-wrap>
</funding-source></award-group></funding-group><counts><page-count count="14"/></counts></article-meta></front><body><p>Facial beauty and moral beauty both matter for our social impressions. Previous research has focused on either source of information in isolation, but little is known about how these different sources of visual and nonvisual information interact. This study investigated the combined effects of moral beauty and facial beauty on social judgments and emotional responses. Using event-related brain potentials (ERPs) to identify the underlying neurocognitive processes, we tested temporally resolved effects on face processing from visual processing to reflexive emotional brain responses mid-latency, and later, more evaluative responses.</p><p>Moral beauty relates to a person&#x02019;s character and social behavior and can be derived from verbally transmitted, social-emotional information. It changes the affective value of people via mechanisms of evaluative learning (<xref rid="R31" ref-type="bibr">Goodwin 2015</xref>, <xref rid="R94" ref-type="bibr">Xu et&#x000a0;al. 2016</xref>, <xref rid="R27" ref-type="bibr">Ferrari et&#x000a0;al. 2020</xref>). Social-emotional information has been shown to influence the perception and evaluation of others (<xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R87" ref-type="bibr">Tsukiura and Cabeza 2011b</xref>, <xref rid="R78" ref-type="bibr">Suess et&#x000a0;al. 2013</xref>, <xref rid="R15" ref-type="bibr">Cui et&#x000a0;al. 2019</xref>, <xref rid="R47" ref-type="bibr">Maier et&#x000a0;al. 2022</xref>), ranging from affecting the chances of when a face is consciously perceived (<xref rid="R3" ref-type="bibr">Anderson et&#x000a0;al. 2011</xref>, <xref rid="R51" ref-type="bibr">Mo et&#x000a0;al. 2016</xref>, <xref rid="R58" ref-type="bibr">Rabovsky et&#x000a0;al. 2016</xref>, <xref rid="R24" ref-type="bibr">Eiserbeck and Abdel Rahman 2020</xref>, <xref rid="R25" ref-type="bibr">Eiserbeck et al. 2024</xref>), over indirect evaluations of facial features (<xref rid="R33" ref-type="bibr">Hassin and Trope 2000</xref>), trustworthiness (<xref rid="R49" ref-type="bibr">Mattarozzi et&#x000a0;al. 2019</xref>, <xref rid="R24" ref-type="bibr">Eiserbeck and Abdel Rahman 2020</xref>), attractiveness (<xref rid="R53" ref-type="bibr">Nisbett and Wilson 1977</xref>, <xref rid="R35" ref-type="bibr">He et al. 2024</xref>), and likability, to explicit social judgments of the person and their social character (<xref rid="R13" ref-type="bibr">Bliss-Moreau et&#x000a0;al. 2008</xref>, <xref rid="R83" ref-type="bibr">Todorov and Olson 2008</xref>, <xref rid="R78" ref-type="bibr">Suess et&#x000a0;al. 2013</xref>, <xref rid="R6" ref-type="bibr">Baum and Abdel Rahman 2020</xref>, <xref rid="R7" ref-type="bibr">2021</xref>, <xref rid="R9" ref-type="bibr">Baum et&#x000a0;al. 2020</xref>). Social-emotional information is spontaneously processed in face perception as reflected in neural regions associated with social cognition and emotion, including emotional responses in the anterior insula known to process disgust-related emotional pictures (<xref rid="R81" ref-type="bibr">Todorov et&#x000a0;al. 2007</xref>).</p><p>Facial beauty, on the other hand, is directly visible in a person&#x02019;s physical appearance. We attribute more positive traits like trustworthiness, friendliness, and social competence to attractive individuals (the halo effect and beautiful-is-good effect), whereas individuals scoring low in attractiveness are seen in a less positive light and face even negative prejudices (<xref rid="R19" ref-type="bibr">Dion et&#x000a0;al. 1972</xref>, <xref rid="R22" ref-type="bibr">Eagly et&#x000a0;al. 1991</xref>, <xref rid="R41" ref-type="bibr">Langlois et&#x000a0;al. 2000</xref>, <xref rid="R95" ref-type="bibr">Zebrowitz and Montepare 2008</xref>, <xref rid="R54" ref-type="bibr">Oh et&#x000a0;al. 2023</xref>). Despite its low diagnostic value we feel confident in the validity of our evaluations based on physical appearance (<xref rid="R33" ref-type="bibr">Hassin and Trope 2000</xref>, <xref rid="R56" ref-type="bibr">Olivola et&#x000a0;al. 2014</xref>). Neurocognitive evidence suggests that the neural mechanisms for judging facial beauty and moral beauty (e.g. from depictions of scenes containing moral actions) are highly similar, suggesting that biases from attractiveness are emotionally and socially relevant (Tsukiura and Cabeza <xref rid="R87" ref-type="bibr">2011b</xref>, <xref rid="R89" ref-type="bibr">Wang et&#x000a0;al. 2014</xref>, <xref rid="R51" ref-type="bibr">Mo et&#x000a0;al. 2016</xref>, <xref rid="R15" ref-type="bibr">Cui et&#x000a0;al. 2019</xref>). Impressions of high versus low attractiveness have been found in differential effects related to approach and avoidance in the medial orbitofrontal cortex and insular cortex, respectively (<xref rid="R86" ref-type="bibr">Tsukiura and Cabeza 2011a</xref>, <xref rid="R87" ref-type="bibr">2011b</xref>).</p><p>We investigate the interplay of moral and facial beauty in well-described ERP components from earlier to later face processing. Visual perception is reflected in the P1 and N170 components, with the P1 reflecting low-level processing like perceived contrast [100&#x02009;ms after face presentation; <xref rid="R34" ref-type="bibr">Haynes et&#x000a0;al. (2003</xref>)], and the N170 reflecting higher-level configural encoding [170&#x02009;ms, occipito-temporal regions; <xref rid="R23" ref-type="bibr">Eimer (2011</xref>)]. Both have been found to be sensitive to facial expression analysis and emotion processing, although with less stable emotion effects compared to later ERPs (<xref rid="R50" ref-type="bibr">Meeren et&#x000a0;al. 2005</xref>, <xref rid="R79" ref-type="bibr">Thierry et&#x000a0;al. 2007</xref>, <xref rid="R18" ref-type="bibr">Dering et&#x000a0;al. 2011</xref>, <xref rid="R37" ref-type="bibr">Hinojosa et&#x000a0;al. 2015</xref>, <xref rid="R70" ref-type="bibr">Schindler and Bublatzky 2020</xref>, <xref rid="R69" ref-type="bibr">Schindler et&#x000a0;al. 2023</xref>). Mid-latency, we test effects on fast and reflexive emotional processing in the early posterior negativity [early posterior negativity (EPN), 200&#x02013;350&#x02009;ms, occipito-temporal regions]. Late in the processing, we test effects on slower, higher-level evaluation of emotional meaning in the late positive potential [late positive potential (LPP), 400&#x02013;600&#x02009;ms, centro-parietal regions]. Original findings related to emotional picture processing, i.e. pleasant or unpleasant visual scenes or objects, as well as friendly or threatening faces in comparison to relatively neutral stimuli found that emotionally arousing content elicited modulations starting from about 100 to 150&#x02009;ms onwards with modulations of the EPN and LPP. Findings suggest that affective meaning is processed independent of physical picture features, and even when stimuli are presented very briefly (<xref rid="R39" ref-type="bibr">Jungh&#x000f6;fer et&#x000a0;al. 2001</xref>, <xref rid="R73" ref-type="bibr">Schupp et&#x000a0;al. 2004</xref>). Both EPN and LPP were found to be primarily driven by the arousal dimension of emotional content such that effects of affective arousal can be similarly found for positive and negative valenced stimuli (e.g. <xref rid="R21" ref-type="bibr">Dolcos and Cabeza 2002</xref>, <xref rid="R74" ref-type="bibr">Schupp et&#x000a0;al. 2003</xref>, <xref rid="R75" ref-type="bibr">2006</xref>). While EPN effects seem to occur relatively reflexively, LPP modulations are more sensitive to the meaning or relevance of emotional stimuli, depending on the current task and motivation (<xref rid="R72" ref-type="bibr">Schupp et&#x000a0;al. 2000</xref>, <xref rid="R64" ref-type="bibr">Schacht and Sommer 2009</xref>, <xref rid="R60" ref-type="bibr">Rellecke et&#x000a0;al. 2011</xref>, <xref rid="R61" ref-type="bibr">2012</xref>, <xref rid="R12" ref-type="bibr">Blechert et&#x000a0;al. 2012</xref>).</p><p>Knowing about the moral beauty of a person affects ERPs during face processing. Reported effects by associated social-emotional information start in ERP components related to visual processing (N170), which may reflect attention effects or altered visual perception (<xref rid="R46" ref-type="bibr">Luo et&#x000a0;al. 2016</xref>, <xref rid="R30" ref-type="bibr">Gim&#x000e9;nez-Fern&#x000e1;ndez et&#x000a0;al. 2020</xref>, <xref rid="R7" ref-type="bibr">Baum and Abdel Rahman 2021</xref>, <xref rid="R68" ref-type="bibr">Schindler et&#x000a0;al. 2021</xref>). EPN effects show that social-emotional information affects reflexive emotional processing (<xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R40" ref-type="bibr">Kissler and Strehlow 2017</xref>, <xref rid="R6" ref-type="bibr">Baum and Abdel Rahman 2020</xref>, <xref rid="R7" ref-type="bibr">2021</xref>), possibly without attention to the information (<xref rid="R78" ref-type="bibr">Suess et&#x000a0;al. 2013</xref>, <xref rid="R94" ref-type="bibr">Xu et&#x000a0;al. 2016</xref>, <xref rid="R25" ref-type="bibr">Eiserbeck et al. 2024</xref>), yet at this stage effects may depend on the saliency of the stimulus and the available processing capacities (<xref rid="R68" ref-type="bibr">Schindler et&#x000a0;al. 2021</xref>). LPP modulations show effects on more controlled, elaborate person evaluation (<xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R94" ref-type="bibr">Xu et&#x000a0;al. 2016</xref>, <xref rid="R40" ref-type="bibr">Kissler and Strehlow 2017</xref>, <xref rid="R6" ref-type="bibr">Baum and Abdel Rahman 2020</xref>, <xref rid="R7" ref-type="bibr">2021</xref>, <xref rid="R9" ref-type="bibr">Baum et&#x000a0;al. 2020</xref>). EPN and LPP effects seem primarily driven by arousal. Although few studies even included positive social-emotional information, earlier ERP effects have more often been found for negative information, whereas effects in the LPP have been found for positive and negative stimuli.</p><p>Facial beauty has likewise been shown to affect ERPs during face processing. Modulations by different levels of perceived attractiveness start with the P1, P2, and N170 but have been reported in different directions (<xref rid="R64" ref-type="bibr">Schacht and Sommer 2009</xref>, <xref rid="R48" ref-type="bibr">Marzi and Viggiano 2010</xref>, <xref rid="R96" ref-type="bibr">Zhang and Deng 2012</xref>, <xref rid="R85" ref-type="bibr">Trujillo et&#x000a0;al. 2014</xref>, <xref rid="R32" ref-type="bibr">Hahn et&#x000a0;al. 2016</xref>). Since these components are sensitive to low-level differences in visual stimuli, it is possible that the inconsistent findings are due to specific perceptual stimulus features. EPN modulations have been found with enhanced amplitudes for attractive compared to less attractive faces (<xref rid="R91" ref-type="bibr">Werheid et&#x000a0;al. 2007</xref>, <xref rid="R48" ref-type="bibr">Marzi and Viggiano 2010</xref>, <xref rid="R93" ref-type="bibr">Wiese et&#x000a0;al. 2014</xref>, <xref rid="R80" ref-type="bibr">Thiruchselvam et&#x000a0;al. 2016</xref>). The LPP was shown to be modulated by attractiveness, albeit in different directions. While some findings show that both high and low attractive faces elicit LPP effects in comparison to more neutral or intermediate faces, others show enhanced amplitudes for attractive compared to less attractive faces (<xref rid="R91" ref-type="bibr">Werheid et&#x000a0;al. 2007</xref>, <xref rid="R65" ref-type="bibr">Schacht et&#x000a0;al. 2008</xref>, <xref rid="R48" ref-type="bibr">Marzi and Viggiano 2010</xref>, <xref rid="R62" ref-type="bibr">Revers et&#x000a0;al. 2023</xref>).</p><p>Here, we examined how behavioral and ERP effects of social-emotional information are influenced by facial attractiveness in a social judgment task (<xref rid="F1" ref-type="fig">Fig.&#x000a0;1</xref>). If effects are modulated, we expect modulations by the high or low end of attractiveness, possibly related to (in)congruency with information. We included intermediate attractive faces only as fillers and always paired with neutral information (see <xref rid="s8" ref-type="sec">supplementary material</xref>, SI-page 14&#x02009;f for additional analyses including fillers). This furthermore balanced the instances of extremes of attractiveness and information in the context of the experiment. Additionally, participants rated their impressions of attractiveness and spontaneous likeability as manipulation checks both before and after the person-related information was given. These ratings allow us to investigate effects when social-emotional information is not or only indirectly relevant.</p><fig position="float" id="F1" fig-type="figure"><label>Figure&#x000a0;1.</label><caption><p>Schematic overview of the study phases and experimental manipulations: In Phase 1 likability and attractiveness of the faces was rated before and after praticipants were exposed to social-emotional information about the faces; in Phase 2 the faces were presented without the associated social-emotional information and the EEG of the participants was acquired while they judged the persons as positive, neutral, or negative, taking all available information into account (AI generated example faces are shown; Academic Dataset by Generated Photos <ext-link xlink:href="https://generated.photos/datasets" ext-link-type="uri">https://generated.photos/datasets</ext-link>).</p></caption><graphic xlink:href="nsae071f1" position="float"/></fig><p>We expected effects of positive and negative information on valenced social judgments and modulations of the EPN and LPP, reflecting key processes involved in social-emotional face and person evaluation, and where previous research has found effects of moral and facial beauty but has not studied their interaction. Associated emotional information may already influence the N170 (see <xref rid="R68" ref-type="bibr">Schindler et&#x000a0;al. 2021</xref>), yet visual processing (P1, N170) may be most influenced by attractiveness; however, this might be partly due to low-level differences in the facial stimuli. Crucially, our investigation focuses on whether, how, and when facial attractiveness modulates behavioral and neurocognitive effects of associated social-emotional information.</p><sec id="s2"><title>Method</title><sec id="s2-s1"><title>Participants</title><p>The sample size was planned according to the requirements of the counterbalancing and based on power analyses, see SI-page 1 in <xref rid="s8" ref-type="sec">supplementary material</xref>. The dataset consists of 24 female participants (mean age&#x02009;=&#x02009;23.46 (SD&#x02009;=&#x02009;4.78), 23 right-handed). We invited females only to avoid differences in the perception of attractiveness in male and female faces (see e.g. <xref rid="R86" ref-type="bibr">Tsukiura and Cabeza 2011a</xref>, <xref rid="R87" ref-type="bibr">2011b</xref>). Five participants had to be replaced with new participants (reasons: one mistake in the counterbalancing, one could not remember the gist of the person information, two did not press buttons, and one had no vision on one eye). Participants were (de) briefed about the procedures and signed informed consent. The study was approved by the local ethics committee.</p></sec></sec><sec id="s3"><title>Materials</title><p>Thirty-six unfamiliar faces with neutral facial expressions were selected from a database previously used to study effects of attractiveness (<xref rid="R65" ref-type="bibr">Schacht et&#x000a0;al. 2008</xref>). Based on independent pre-ratings (see SI-page 1 in <xref rid="s8" ref-type="sec">supplementary material</xref>), we selected 12 high and 12 low attractiveness target faces, and 12 filler faces of medium attractiveness, with half male and half female faces, respectively. Photographs were grayscale, adjusted in luminance for similar brightness and placed on a light blue background (2.7&#x02009;&#x000d7;&#x02009;3.5&#x02009;cm on a 19&#x0201d;, 60&#x02009;Hz, 1280&#x02009;&#x000d7;&#x02009;1024 monitor, viewing distance 70&#x02009;cm; equating to a visual image size of app. 2.5&#x000b0;) to assimilate low-level features.</p><p>Faces were associated with social-emotional information that was either positive, negative, or neutral in valence (for pre-ratings see SI-page 2 in <xref rid="s8" ref-type="sec">supplementary material</xref>). The information referred to the person&#x02019;s social and moral character, e.g. positive: &#x0201c;He saved a family of refugees from drowning&#x0201d;, negative: &#x0201c;He mixed date-rape drugs in his date&#x02019;s drink&#x0201d; (or was neutral: &#x0201c;He showed the new collection to the customers&#x0201d;). All 36 sentences were recorded by the same male speaker.</p><p>Taken together, each attractiveness (high, low) by information (positive, negative, neutral) condition consisted of four target face stimuli. By counterbalancing the assignment of information to target faces between participants, we ensured that across participants each face was associated equally often with each of the three information levels. Please note that the assigned information to each face was consistent within a participant. The twelve filler faces were always paired with neutral information.</p><sec id="s3-s1"><title>Procedure</title><p>The within-subject design consisted of two phases, a well-established procedure in studying face perception and social-emotional information (<xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>, <xref rid="R6" ref-type="bibr">Baum and Abdel Rahman 2020</xref>, <xref rid="R24" ref-type="bibr">Eiserbeck and Abdel Rahman 2020</xref>, <xref rid="R78" ref-type="bibr">Suess et&#x000a0;al. 2013</xref>). In Phase 1, the experiment started with a likability and an attractiveness rating of all 36 presented faces as manipulation checks, and the order of the ratings was counterbalanced between participants [7-point scales adopted from the Self-Assessment Manikin; (<xref rid="R14" ref-type="bibr">Bradley and Lang 1994</xref>), from very likable to very unlikable, and very high to very low]. For the exposure to social-emotional information, participants saw each face and were presented with auditory information about them. Presented were blocks of nine faces that included all experimental conditions and three fillers. In each trial, the face was presented for 6 s and each face was repeated five times in total (but not directly repeating). To ensure participants paid attention, they occasionally answered simple yes-or-no questions referring to the information interspersed between blocks (e.g. Is the behavior of this person common?). After a 15-min break, Phase 1 concluded with the same likability and attractiveness ratings of all faces, serving as before vs. after information exposure manipulation checks. In total, Phase 1 had 324 trials per participant.</p><p>In Phase 2, the faces were shown in isolation and participants performed the social judgment task as the main task and their EEG was recorded. They judged the persons referring to their social characteristics based on all available information (&#x0201c;Based on all information, how positive, neutral, or negative is this person to you?&#x0201d;, 3-point scale). We chose this explicit judgment to compare the results with other studies using a similar social judgment task, but did not manipulate the facial appearance (e.g. Baum et&#x000a0;al. 2018; <xref rid="R6" ref-type="bibr">Baum and Abdel Rahman 2020</xref>, <xref rid="R7" ref-type="bibr">2021</xref>, <xref rid="R68" ref-type="bibr">Schindler et&#x000a0;al. 2021</xref>). Each trial started with a 500-ms fixation cross, and faces were presented until button press or for 2 s, followed by a 500-ms interstimulus interval. The social judgment task was repeated 20 times in blocks of all 36 faces separated by self-paced breaks (resulting in 80 trials per participant per condition). Participants were told that repetitions are necessary for EEG measurements. In total, Phase 2 had 792 trials per participant.</p><p>The direction of answer scales was counterbalanced between participants, i.e. half of the participants had scales and buttons range from positive (left) to negative (right), and the other half vice versa. After the experiment, participants were asked to write down the gist of the information associated with each face to check for sufficient acquisition of the information.</p></sec><sec id="s3-s2"><title>EEG data recording and preprocessing</title><p>The EEG was recorded from 62 Ag/AgCl scalp electrodes as specified by the extended 10&#x02013;20 system, referenced to the left mastoid with FCz as the ground electrode (see SI-page 3 for setup in <xref rid="s8" ref-type="sec">supplementary material</xref>). The impedance was kept below 5k&#x003a9;. EEG data were recorded at a sampling rate of 5&#x02009;kHz and down-sampled to 500&#x02009;Hz using a low cutoff of 0.016&#x02009;Hz and a high cutoff of 1000&#x02009;Hz. Horizontal and vertical electrooculograms were obtained with peripheral electrodes at the left and right canthi of both eyes, and above and below the left eye. A short calibration procedure traced individual eye movements after the experiment, which were later used to correct for eye movement artefacts.</p><p>Offline, the continuous EEG was transformed to average reference (see SI-page 3 for all electrodes in <xref rid="s8" ref-type="sec">supplementary material</xref>), and high-pass filtered at 0.1&#x02009;Hz and low-pass filtered at 30&#x02009;Hz pass-band edge [zero-phase FIR-filter with transition band width of 0.1&#x02009;Hz and cutoff frequency (&#x02212;6&#x02009;dB): 0.05, 30.05&#x02009;Hz, EEGlab-toolbox version 13_5_4b; <xref rid="R17" ref-type="bibr">Delorme and Makeig (2004)</xref>]. Using BESA (<xref rid="R11" ref-type="bibr">Berg and Scherg 1991</xref>), we removed artefacts due to eye movements by applying a spatiotemporal dipole modeling procedure for each participant individually. Trials with remaining artefacts were rejected, i.e. trials with amplitudes over &#x000b1;200&#x02009;&#x000b5;V, changing more than 50&#x02009;&#x000b5;V between samples or more than 200&#x02009;&#x000b5;V within single epochs, or containing baseline drifts. Error- and artefact-free EEG data were segmented into epochs of 1 s, starting 200&#x02009;ms prior to stimulus onset, with a 200-ms pre-stimulus baseline. For EEG analysis, an average of 78 trials per participant remained (range: 70-80) in each condition. Overall, 97% of trials were kept (high attractive: positive 1869, negative 1877, neutral 1844, low attractive: positive 1870, negative 1876, neutral 1860). Trials where no judgment was given or latencies below 150&#x02009;ms were excluded (in the social judgment task that were 323 excluding fillers, leaving a total of 11&#x02009;196 trials).</p></sec><sec id="s3-s3"><title>Data analysis</title><p>ERP analyses focused on the EPN component (electrode sites PO7, PO8, PO9, PO10, TP9, TP10, 200&#x02013;350&#x02009;ms after face stimulus onset), and the LPP component (electrode sites Pz, CPz, POz, P3, P4, 400&#x02013;600&#x02009;ms), according to previous findings of effects of social-emotional information (<xref rid="R1" ref-type="bibr">Abdel Rahman 2011</xref>; Baum et&#x000a0;al. 2018, <xref rid="R6" ref-type="bibr">Baum and Abdel Rahman 2020</xref>, <xref rid="R7" ref-type="bibr">2021</xref>), emotion effects (<xref rid="R74" ref-type="bibr">Schupp et&#x000a0;al. 2003</xref>), and attractiveness effects (<xref rid="R91" ref-type="bibr">Werheid et&#x000a0;al. 2007</xref>, <xref rid="R65" ref-type="bibr">Schacht et&#x000a0;al. 2008</xref>, <xref rid="R48" ref-type="bibr">Marzi and Viggiano 2010</xref>, <xref rid="R93" ref-type="bibr">Wiese et&#x000a0;al. 2014</xref>, <xref rid="R80" ref-type="bibr">Thiruchselvam et&#x000a0;al. 2016</xref>). Additionally, we analyzed effects during early visual face processing in the P1 (PO3, PO4, O1, O2, 80&#x02013;120&#x02009;ms) and the N170 (P7, P8, PO7, PO8, 130&#x02013;200&#x02009;ms), also based on previous research (<xref rid="R2" ref-type="bibr">Abdel Rahman and Sommer 2012</xref>, <xref rid="R37" ref-type="bibr">Hinojosa et&#x000a0;al. 2015</xref>, <xref rid="R6" ref-type="bibr">Baum and Abdel Rahman 2020</xref>, <xref rid="R7" ref-type="bibr">2021</xref>, <xref rid="R68" ref-type="bibr">Schindler et&#x000a0;al. 2021</xref>). Amplitudes were averaged over the respective regions of interest and time windows on single-trial level.</p><p>We performed mixed-effects regression models on single-trial data of ERPs and behavioral measures in R (<xref rid="R5" ref-type="bibr">Bates et&#x000a0;al. 2015</xref>, <xref rid="R28" ref-type="bibr">Fr&#x000f6;mer et&#x000a0;al. 2018</xref>). We calculated the effects of positive information (positive vs. neutral) and negative information (negative vs. neutral) each in interaction with attractiveness in separate 2&#x02009;&#x000d7;&#x02009;2 models. We analyzed the effects of positive and negative information separately because they have resulted in different outcomes in our previous studies (e.g. <xref rid="R7" ref-type="bibr">Baum and Abdel Rahman 2021</xref>). We specified each model with the fixed effects for the experimental factors &#x0201c;information&#x0201d; (positive or negative vs. neutral; with neutral as the reference level) and &#x0201c;attractiveness&#x0201d; (high vs. low; with low as the reference level) and their interaction. For the manipulation checks (likability rating and attractiveness rating), we nested the factors information and attractiveness with the factor levels of &#x0201c;phase&#x0201d; (before, after). All factors were modeled as repeated contrasts that compare the means of factor levels to the respective reference level (<xref rid="R66" ref-type="bibr">Schad et&#x000a0;al. 2020</xref>). For continuous dependent variables (ERPs, RTs), we used linear mixed models (LMMs; <italic toggle="yes">lme4</italic> v.1.1-26; RTs were reciprocally transformed to &#x02212;1000/RT; amplitudes were normally distributed), and for the social judgments and ratings, we used cumulative link mixed models (CLMMs; <italic toggle="yes">ordinal</italic> v.2022.11-16). We fitted models with by-subject random intercept and slopes, and by-face random intercepts. If necessary, random-slopes correlation parameters were set to zero and slopes explaining zero variance were omitted to achieve convergence and avoid overparameterization (<xref rid="R5" ref-type="bibr">Bates et&#x000a0;al. 2015</xref>). We test the significance of fixed effects coefficients (<italic toggle="yes">P</italic>-value&#x02009;&#x0003c;&#x02009;.05) via the <italic toggle="yes">summary</italic> command (with Satterthwaite statistics for LMMs and Wald statistics for CLMMs). We report unstandardized fixed effects coefficients as effect sizes (<italic toggle="yes">b</italic> with standard error SE). Separate comparisons were performed using <italic toggle="yes">emmeans</italic> (v.1.4.6, <xref rid="R42" ref-type="bibr">Lenth 2020</xref>) with false discovery rate-adjusted <italic toggle="yes">P</italic>-values (<xref rid="R10" ref-type="bibr">Benjamini and Hochberg 1995</xref>). Data and code are available online (<ext-link xlink:href="https://osf.io/e529z/" ext-link-type="uri">https://osf.io/e529z/</ext-link>).</p></sec></sec><sec id="s4"><title>Results</title><sec id="s4-s1"><title>Social judgment task (Phase 2): social judgments and brain responses as a function of social-emotional information and attractiveness</title><sec id="s4-s1-s1"><title>Behavioral results</title><p>Faces were presented and participants judged the persons based on all available information on a valence scale (<xref rid="F2" ref-type="fig">Fig.&#x000a0;2a</xref>). Faces associated with negative information were judged as more negative persons compared to neutral information, regardless of attractiveness (<xref rid="T1" ref-type="table">Table&#x000a0;1</xref>). Faces associated with positive information were judged as more positive persons compared to faces associated with neutral information, also regardless of attractiveness (<xref rid="T1" ref-type="table">Table&#x000a0;1</xref>).</p><fig position="float" id="F2" fig-type="figure"><label>Figure&#x000a0;2.</label><caption><p>Social Judgment Task (Phase 2): (a) Social judgments were dominated by the associated social-emotional information, and attractiveness did not modulate judgments; (b) latencies of social judgments were affected by the congruency of social-emotional information and attractiveness, with faster judgments of high attractive faces associated with positive information and faster judgments of low attractive faces associated with negative information (dots represent individual participant means, error bars show 95% confidence intervals).</p></caption><graphic xlink:href="nsae071f2" position="float"/></fig><table-wrap position="float" id="T1"><label>Table&#x000a0;1.</label><caption><p>Mixed model summary statistics and separate comparisons show effects of positive information and negative information each in interaction with attractiveness on social judgments</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th valign="bottom" colspan="5" align="center" rowspan="1">Social judgments</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Coefficient</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">z</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High vs. low attractiveness</td><td align="left" rowspan="1" colspan="1">0.41</td><td align="left" rowspan="1" colspan="1">0.36</td><td align="left" rowspan="1" colspan="1">1.11</td><td align="left" rowspan="1" colspan="1">.27</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral information</td><td align="left" rowspan="1" colspan="1">&#x02212;8.15</td><td align="left" rowspan="1" colspan="1">0.44</td><td align="left" rowspan="1" colspan="1">&#x02212;18.61</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral information</td><td align="left" rowspan="1" colspan="1">6.76</td><td align="left" rowspan="1" colspan="1">0.39</td><td align="left" rowspan="1" colspan="1">17.46</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">&#x02212;0.29</td><td align="left" rowspan="1" colspan="1">0.49</td><td align="left" rowspan="1" colspan="1">&#x02212;0.59</td><td align="left" rowspan="1" colspan="1">.56</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">&#x02212;0.68</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">&#x02212;0.98</td><td align="left" rowspan="1" colspan="1">.33</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Separate comparisons</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">z</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: negative vs. neutral info</td><td align="left" rowspan="1" colspan="1">&#x02212;8.29</td><td align="left" rowspan="1" colspan="1">0.53</td><td align="left" rowspan="1" colspan="1">&#x02212;15.43</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: positive vs. neutral info</td><td align="left" rowspan="1" colspan="1">6.42</td><td align="left" rowspan="1" colspan="1">0.45</td><td align="left" rowspan="1" colspan="1">14.16</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: negative vs. positive info</td><td align="left" rowspan="1" colspan="1">&#x02212;14.71</td><td align="left" rowspan="1" colspan="1">0.53</td><td align="left" rowspan="1" colspan="1">&#x02212;27.58</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: negative vs. neutral info</td><td align="left" rowspan="1" colspan="1">&#x02212;8.00</td><td align="left" rowspan="1" colspan="1">0.46</td><td align="left" rowspan="1" colspan="1">&#x02212;17.24</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: positive vs. neutral info</td><td align="left" rowspan="1" colspan="1">7.10</td><td align="left" rowspan="1" colspan="1">0.58</td><td align="left" rowspan="1" colspan="1">12.25</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: negative vs. positive info</td><td align="left" rowspan="1" colspan="1">&#x02212;15.11</td><td align="left" rowspan="1" colspan="1">0.83</td><td align="left" rowspan="1" colspan="1">&#x02212;18.13</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative information: high vs. low Att</td><td align="left" rowspan="1" colspan="1">0.44</td><td align="left" rowspan="1" colspan="1">0.51</td><td align="left" rowspan="1" colspan="1">0.87</td><td align="left" rowspan="1" colspan="1">.43</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive information: high vs. low Att</td><td align="left" rowspan="1" colspan="1">0.05</td><td align="left" rowspan="1" colspan="1">0.66</td><td align="left" rowspan="1" colspan="1">0.07</td><td align="left" rowspan="1" colspan="1">.94</td></tr><tr><td align="left" rowspan="1" colspan="1">Neutral information: high vs. low att</td><td align="left" rowspan="1" colspan="1">0.73</td><td align="left" rowspan="1" colspan="1">0.41</td><td align="left" rowspan="1" colspan="1">1.77</td><td align="left" rowspan="1" colspan="1">.098</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Formula of converging model</italic>
</td><td colspan="4" align="left" rowspan="1">Judgments&#x02009;&#x0223c;&#x02009;information&#x02009;&#x000d7;&#x02009;attractiveness + (information&#x02009;&#x000d7;&#x02009;attractiveness| subject) + (1 | face)</td></tr></tbody></table><table-wrap-foot><fn id="T0001-fn1"><p>
<italic toggle="yes">Note.&#x02009;</italic>&#x000d7;&#x02009;stands for interaction. Double bars in random effects terms set correlation parameters to zero. Separate comparisons&#x02019; <italic toggle="yes">P</italic>-values were FDR corrected for nine tests. <italic toggle="yes">P</italic>-values &#x0003c; .05 are printed in bold.</p></fn></table-wrap-foot></table-wrap><p>For the latencies of social judgments, the effects of information and attractiveness interacted such that congruent conditions lead to faster judgments (<xref rid="F2" ref-type="fig">Fig.&#x000a0;2b</xref>). Faces associated with negative information were judged faster when they were low in attractiveness, and faces associated with positive information were judged faster when they were high in attractiveness (<xref rid="T2" ref-type="table">Table&#x000a0;2</xref>).</p><table-wrap position="float" id="T2"><label>Table&#x000a0;2.</label><caption><p>Linear mixed model summary statistics and separate comparisons show effects of positive information and negative information each in interaction with attractiveness on latencies of social judgments</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th valign="bottom" colspan="5" align="center" rowspan="1">Latencies of social judgments [-1000 / latency (ms)]</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Coefficient</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">t</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Intercept (grand mean)</td><td align="left" rowspan="1" colspan="1">&#x02212;1.44</td><td align="left" rowspan="1" colspan="1">0.05</td><td align="left" rowspan="1" colspan="1">&#x02212;28.50</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High vs. low attractiveness</td><td align="left" rowspan="1" colspan="1">0.011</td><td align="left" rowspan="1" colspan="1">0.02</td><td align="left" rowspan="1" colspan="1">0.49</td><td align="left" rowspan="1" colspan="1">.63</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral information</td><td align="left" rowspan="1" colspan="1">0.008</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">0.27</td><td align="left" rowspan="1" colspan="1">.79</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral information</td><td align="left" rowspan="1" colspan="1">0.056</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">1.74</td><td align="left" rowspan="1" colspan="1">.095</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.056</td><td align="left" rowspan="1" colspan="1">0.02</td><td align="left" rowspan="1" colspan="1">2.28</td><td align="left" rowspan="1" colspan="1">
<bold>.032</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">&#x02212;0.11</td><td align="left" rowspan="1" colspan="1">0.02</td><td align="left" rowspan="1" colspan="1">&#x02212;5.57</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Separate comparisons</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">t</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: negative vs. neutral info</td><td align="left" rowspan="1" colspan="1">0.035</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">1.15</td><td align="left" rowspan="1" colspan="1">.39</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: positive vs. neutral info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.00</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">&#x02212;0.00</td><td align="left" rowspan="1" colspan="1">.99</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: negative vs. positive info</td><td align="left" rowspan="1" colspan="1">0.034</td><td align="left" rowspan="1" colspan="1">0.04</td><td align="left" rowspan="1" colspan="1">0.80</td><td align="left" rowspan="1" colspan="1">.548</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: negative vs. neutral info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.020</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">&#x02212;0.66</td><td align="left" rowspan="1" colspan="1">.58</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: positive vs. neutral info</td><td align="left" rowspan="1" colspan="1">0.11</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">3.32</td><td align="left" rowspan="1" colspan="1">
<bold>.010</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: negative vs. positive info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.13</td><td align="left" rowspan="1" colspan="1">0.04</td><td align="left" rowspan="1" colspan="1">&#x02212;2.98</td><td align="left" rowspan="1" colspan="1">
<bold>.010</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative information: high vs. low att</td><td align="left" rowspan="1" colspan="1">0.085</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">3.08</td><td align="left" rowspan="1" colspan="1">
<bold>.010</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive information: high vs. low att</td><td align="left" rowspan="1" colspan="1">&#x02212;0.082</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">&#x02212;3.08</td><td align="left" rowspan="1" colspan="1">
<bold>.010</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Neutral information: high vs. low att</td><td align="left" rowspan="1" colspan="1">0.030</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">1.17</td><td align="left" rowspan="1" colspan="1">.39</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Formula of converging model</italic>
</td><td colspan="4" align="left" rowspan="1">Latencies&#x02009;&#x0223c;&#x02009;information&#x02009;&#x000d7;&#x02009;attractiveness + (information&#x02009;&#x000d7;&#x02009;attractiveness|| subject) + (1 | face)</td></tr></tbody></table><table-wrap-foot><fn id="T0002-fn1"><p>
<italic toggle="yes">Note.&#x02009;</italic>&#x000d7;&#x02009;stands for interaction. Double bars in random effects terms set correlation parameters to zero. Separate comparisons&#x02019; <italic toggle="yes">P</italic>-values were FDR corrected for nine tests. <italic toggle="yes">P</italic>-values &#x0003c; .05 are printed in bold.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s4-s1-s2"><title>Event-related brain potentials</title><p>
<bold>P1 and N170</bold>. For effects on visual face perception, we investigated the P1 and the N170. No modulations were found in the P1 (see SI-page 3 in <xref rid="s8" ref-type="sec">supplementary material</xref>). The N170 was enhanced for low attractive faces compared to high attractive faces (<xref rid="T3" ref-type="table">Table&#x000a0;3</xref>, <xref rid="F3" ref-type="fig">Fig.&#x000a0;3</xref>). Trends for interaction effects of negative and positive information with attractiveness showed, if anything, enhanced N170 amplitudes for low vs. high attractive faces in the negative and positive information condition, but not in the neutral condition.</p><fig position="float" id="F3" fig-type="figure"><label>Figure&#x000a0;3.</label><caption><p>N170 (130&#x02013;200&#x02009;ms) modulations show attractiveness effects for positive and negative social-emotional information during the Social Judgment Task in Phase 2 (grand average ERPs are shown at the respective region of interest shown as black dots in the topographies, scalp topographies show the effects as differences between conditions in the time windows shaded in gray).</p></caption><graphic xlink:href="nsae071f3" position="float"/></fig><table-wrap position="float" id="T3"><label>Table&#x000a0;3.</label><caption><p>Linear mixed model summary statistics and separate comparisons show effects of positive information and negative information each in interaction with attractiveness on the N170</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th valign="bottom" colspan="5" align="center" rowspan="1">N170</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Coefficient</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">t</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Intercept (grand mean)</td><td align="left" rowspan="1" colspan="1">&#x02212;1.49</td><td align="left" rowspan="1" colspan="1">0.61</td><td align="left" rowspan="1" colspan="1">&#x02212;2.45</td><td align="left" rowspan="1" colspan="1">
<bold>.021</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High vs. low attractiveness</td><td align="left" rowspan="1" colspan="1">0.49</td><td align="left" rowspan="1" colspan="1">0.17</td><td align="left" rowspan="1" colspan="1">2.82</td><td align="left" rowspan="1" colspan="1">
<bold>.009</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral information</td><td align="left" rowspan="1" colspan="1">&#x02212;0.11</td><td align="left" rowspan="1" colspan="1">0.11</td><td align="left" rowspan="1" colspan="1">&#x02212;1.00</td><td align="left" rowspan="1" colspan="1">.33</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral information</td><td align="left" rowspan="1" colspan="1">&#x02212;0.08</td><td align="left" rowspan="1" colspan="1">0.13</td><td align="left" rowspan="1" colspan="1">&#x02212;0.60</td><td align="left" rowspan="1" colspan="1">.55</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.37</td><td align="left" rowspan="1" colspan="1">0.22</td><td align="left" rowspan="1" colspan="1">1.69</td><td align="left" rowspan="1" colspan="1">.098</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.43</td><td align="left" rowspan="1" colspan="1">0.22</td><td align="left" rowspan="1" colspan="1">1.95</td><td align="left" rowspan="1" colspan="1">.051</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Separate comparisons</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">SE</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">t</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">p</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: negative vs. neutral info</td><td align="left" rowspan="1" colspan="1">0.08</td><td align="left" rowspan="1" colspan="1">0.16</td><td align="left" rowspan="1" colspan="1">0.52</td><td align="left" rowspan="1" colspan="1">.78</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: positive vs. neutral info</td><td align="left" rowspan="1" colspan="1">0.14</td><td align="left" rowspan="1" colspan="1">0.17</td><td align="left" rowspan="1" colspan="1">0.81</td><td align="left" rowspan="1" colspan="1">.63</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: negative vs. positive info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.06</td><td align="left" rowspan="1" colspan="1">0.17</td><td align="left" rowspan="1" colspan="1">&#x02212;0.34</td><td align="left" rowspan="1" colspan="1">.83</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: negative vs. Neutral info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.29</td><td align="left" rowspan="1" colspan="1">0.16</td><td align="left" rowspan="1" colspan="1">&#x02212;1.88</td><td align="left" rowspan="1" colspan="1">.18</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: positive vs. neutral info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.29</td><td align="left" rowspan="1" colspan="1">0.17</td><td align="left" rowspan="1" colspan="1">&#x02212;1.73</td><td align="left" rowspan="1" colspan="1">.20</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: negative vs. positive info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.00</td><td align="left" rowspan="1" colspan="1">0.17</td><td align="left" rowspan="1" colspan="1">&#x02212;0.01</td><td align="left" rowspan="1" colspan="1">.99</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative information: high vs. low att</td><td align="left" rowspan="1" colspan="1">0.60</td><td align="left" rowspan="1" colspan="1">0.22</td><td align="left" rowspan="1" colspan="1">2.77</td><td align="left" rowspan="1" colspan="1">
<bold>.036</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive information: high vs. low att</td><td align="left" rowspan="1" colspan="1">0.65</td><td align="left" rowspan="1" colspan="1">0.22</td><td align="left" rowspan="1" colspan="1">3.03</td><td align="left" rowspan="1" colspan="1">
<bold>.033</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Neutral information: high vs. low att</td><td align="left" rowspan="1" colspan="1">0.22</td><td align="left" rowspan="1" colspan="1">0.22</td><td align="left" rowspan="1" colspan="1">1.04</td><td align="left" rowspan="1" colspan="1">.55</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Formula of converging model</italic>
</td><td colspan="4" align="left" rowspan="1">N170&#x02009;&#x0223c;&#x02009;information&#x02009;&#x000d7;&#x02009;attractiveness + (pos-neu&#x02009;+&#x02009;high-low&#x02009;+&#x02009;neg-neu-high-low|| subject) + (1 | face)</td></tr></tbody></table><table-wrap-foot><fn id="T0003-fn1"><p>
<italic toggle="yes">Note.&#x02009;</italic>&#x000d7;&#x02009;stands for interaction. Double bars in random effects terms set correlation parameters to zero. Separate comparisons&#x02019; <italic toggle="yes">P</italic>-values were FDR corrected for nine tests. <italic toggle="yes">P</italic>-values &#x0003c; .05 are printed in bold.</p></fn></table-wrap-foot></table-wrap><p>
<bold>EPN</bold>. For fast and reflexive emotional processing we investigated the EPN component (<xref rid="F4" ref-type="fig">Fig.&#x000a0;4a and b</xref>). The EPN was enhanced for faces associated with negative information compared to neutral and for faces associated with positive information compared to neutral (<xref rid="T4" ref-type="table">Table&#x000a0;4</xref>). Attractiveness did not modulate EPN effects (<xref rid="T4" ref-type="table">Table&#x000a0;4</xref>).</p><fig position="float" id="F4" fig-type="figure"><label>Figure&#x000a0;4.</label><caption><p>EPN (200&#x02013;350 ms) and LPP (400&#x02013;600&#x02009;ms) modulations show dominant effects of positive and negative social-emotional information that were largely unaffected by attractiveness during the Social Judgment Task in Phase 2 (grand average ERPs are shown at the respective region of interest shown as black dots in the topographies, scalp topographies show the effects as differences between conditions in the time windows shaded in gray).</p></caption><graphic xlink:href="nsae071f4" position="float"/></fig><table-wrap position="float" id="T4"><label>Table&#x000a0;4.</label><caption><p>Linear mixed model summary statistics and separate comparisons show effects of positive information and negative information each in interaction with attractiveness on the EPN</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th valign="bottom" colspan="5" align="center" rowspan="1">EPN</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Coefficient</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">t</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Intercept (grand mean)</td><td align="left" rowspan="1" colspan="1">&#x02212;0.11</td><td align="left" rowspan="1" colspan="1">0.55</td><td align="left" rowspan="1" colspan="1">&#x02212;0.20</td><td align="left" rowspan="1" colspan="1">.84</td></tr><tr><td align="left" rowspan="1" colspan="1">High vs. low attractiveness</td><td align="left" rowspan="1" colspan="1">&#x02212;0.06</td><td align="left" rowspan="1" colspan="1">0.36</td><td align="left" rowspan="1" colspan="1">&#x02212;0.17</td><td align="left" rowspan="1" colspan="1">.87</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral information</td><td align="left" rowspan="1" colspan="1">&#x02212;0.88</td><td align="left" rowspan="1" colspan="1">0.12</td><td align="left" rowspan="1" colspan="1">&#x02212;7.45</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral Information</td><td align="left" rowspan="1" colspan="1">&#x02212;0.64</td><td align="left" rowspan="1" colspan="1">0.15</td><td align="left" rowspan="1" colspan="1">&#x02212;4.30</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.05</td><td align="left" rowspan="1" colspan="1">0.29</td><td align="left" rowspan="1" colspan="1">0.19</td><td align="left" rowspan="1" colspan="1">.85</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.21</td><td align="left" rowspan="1" colspan="1">0.22</td><td align="left" rowspan="1" colspan="1">0.99</td><td align="left" rowspan="1" colspan="1">.33</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Separate comparisons</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">t</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: negative vs. neutral info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.85</td><td align="left" rowspan="1" colspan="1">0.19</td><td align="left" rowspan="1" colspan="1">&#x02212;4.53</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: positive vs. neutral info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.53</td><td align="left" rowspan="1" colspan="1">0.18</td><td align="left" rowspan="1" colspan="1">&#x02212;2.88</td><td align="left" rowspan="1" colspan="1">
<bold>.012</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: negative vs. positive info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.32</td><td align="left" rowspan="1" colspan="1">0.21</td><td align="left" rowspan="1" colspan="1">&#x02212;1.48</td><td align="left" rowspan="1" colspan="1">.26</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: negative vs. neutral info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.90</td><td align="left" rowspan="1" colspan="1">0.19</td><td align="left" rowspan="1" colspan="1">&#x02212;4.83</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: positive vs. neutral Info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.74</td><td align="left" rowspan="1" colspan="1">0.18</td><td align="left" rowspan="1" colspan="1">&#x02212;4.05</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: negative vs. positive info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.16</td><td align="left" rowspan="1" colspan="1">0.22</td><td align="left" rowspan="1" colspan="1">&#x02212;0.75</td><td align="left" rowspan="1" colspan="1">.68</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative information: high vs. low att</td><td align="left" rowspan="1" colspan="1">&#x02212;0.10</td><td align="left" rowspan="1" colspan="1">0.41</td><td align="left" rowspan="1" colspan="1">&#x02212;0.24</td><td align="left" rowspan="1" colspan="1">.88</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive information: high vs. low att</td><td align="left" rowspan="1" colspan="1">0.06</td><td align="left" rowspan="1" colspan="1">0.39</td><td align="left" rowspan="1" colspan="1">0.16</td><td align="left" rowspan="1" colspan="1">.88</td></tr><tr><td align="left" rowspan="1" colspan="1">Neutral information: high vs. low att</td><td align="left" rowspan="1" colspan="1">&#x02212;0.15</td><td align="left" rowspan="1" colspan="1">0.39</td><td align="left" rowspan="1" colspan="1">&#x02212;0.39</td><td align="left" rowspan="1" colspan="1">.88</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Formula of converging model</italic>
</td><td colspan="4" align="left" rowspan="1">EPN&#x02009;&#x0223c;&#x02009;information&#x02009;&#x000d7;&#x02009;attractiveness + (information&#x02009;&#x000d7;&#x02009;attractiveness|| subject) + (1 | face)</td></tr></tbody></table><table-wrap-foot><fn id="T0004-fn1"><p>
<italic toggle="yes">Note.&#x02009;</italic>&#x000d7;&#x02009;stands for interaction. Double bars in random effects terms set correlation parameters to zero. Separate comparisons&#x02019; <italic toggle="yes">P</italic>-values were FDR corrected for nine tests. <italic toggle="yes">P</italic>-values &#x0003c; .05 are printed in bold.</p></fn></table-wrap-foot></table-wrap><p>
<bold>LPP</bold>. For slower and more elaborate evaluative processing, we investigated the LPP component (<xref rid="F4" ref-type="fig">Fig.&#x000a0;4c and d</xref>). The LPP was enhanced for faces associated with negative compared to neutral information and for faces associated with positive compared to neutral information (<xref rid="T5" ref-type="table">Table&#x000a0;5</xref>). Attractiveness did not modulate LPP effects of positive information. Attractiveness interacted with the effect of negative information such that LPP amplitudes for negative associated information were most pronounced for low attractive faces, suggesting an effect of congruency (<xref rid="T5" ref-type="table">Table&#x000a0;5</xref>).</p><table-wrap position="float" id="T5"><label>Table&#x000a0;5.</label><caption><p>Linear mixed model summary statistics and separate comparisons show effects of positive information and negative information each in interaction with attractiveness on the LPP</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead><tr><th valign="bottom" colspan="5" align="center" rowspan="1">LPP</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Coefficient</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">t</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Intercept (grand mean)</td><td align="left" rowspan="1" colspan="1">5.96</td><td align="left" rowspan="1" colspan="1">0.58</td><td align="left" rowspan="1" colspan="1">10.26</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High vs. low attractiveness</td><td align="left" rowspan="1" colspan="1">0.06</td><td align="left" rowspan="1" colspan="1">0.18</td><td align="left" rowspan="1" colspan="1">0.32</td><td align="left" rowspan="1" colspan="1">.75</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral information</td><td align="left" rowspan="1" colspan="1">2.04</td><td align="left" rowspan="1" colspan="1">0.21</td><td align="left" rowspan="1" colspan="1">9.91</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral information</td><td align="left" rowspan="1" colspan="1">1.74</td><td align="left" rowspan="1" colspan="1">0.16</td><td align="left" rowspan="1" colspan="1">10.74</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">&#x02212;0.60</td><td align="left" rowspan="1" colspan="1">0.26</td><td align="left" rowspan="1" colspan="1">&#x02212;2.31</td><td align="left" rowspan="1" colspan="1">
<bold>.027</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.16</td><td align="left" rowspan="1" colspan="1">0.23</td><td align="left" rowspan="1" colspan="1">0.69</td><td align="left" rowspan="1" colspan="1">.49</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Separate comparisons</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">t</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: negative vs. Neutral info</td><td align="left" rowspan="1" colspan="1">1.74</td><td align="left" rowspan="1" colspan="1">0.24</td><td align="left" rowspan="1" colspan="1">7.15</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: positive vs. Neutral info</td><td align="left" rowspan="1" colspan="1">1.82</td><td align="left" rowspan="1" colspan="1">0.20</td><td align="left" rowspan="1" colspan="1">9.10</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">High attractiveness: negative vs. Positive info</td><td align="left" rowspan="1" colspan="1">&#x02212;0.08</td><td align="left" rowspan="1" colspan="1">0.27</td><td align="left" rowspan="1" colspan="1">&#x02212;0.29</td><td align="left" rowspan="1" colspan="1">.77</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: negative vs. Neutral info</td><td align="left" rowspan="1" colspan="1">2.34</td><td align="left" rowspan="1" colspan="1">0.24</td><td align="left" rowspan="1" colspan="1">9.62</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: positive vs. neutral info</td><td align="left" rowspan="1" colspan="1">1.66</td><td align="left" rowspan="1" colspan="1">0.20</td><td align="left" rowspan="1" colspan="1">8.30</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Low attractiveness: negative vs. positive info</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.27</td><td align="left" rowspan="1" colspan="1">2.52</td><td align="left" rowspan="1" colspan="1">
<bold>.030</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative information: high vs. low att</td><td align="left" rowspan="1" colspan="1">&#x02212;0.39</td><td align="left" rowspan="1" colspan="1">0.24</td><td align="left" rowspan="1" colspan="1">&#x02212;1.64</td><td align="left" rowspan="1" colspan="1">.16</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive information: high vs. low att</td><td align="left" rowspan="1" colspan="1">0.37</td><td align="left" rowspan="1" colspan="1">0.23</td><td align="left" rowspan="1" colspan="1">1.58</td><td align="left" rowspan="1" colspan="1">.15</td></tr><tr><td align="left" rowspan="1" colspan="1">Neutral information: high vs. low att</td><td align="left" rowspan="1" colspan="1">0.20</td><td align="left" rowspan="1" colspan="1">0.23</td><td align="left" rowspan="1" colspan="1">0.88</td><td align="left" rowspan="1" colspan="1">.43</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Formula of converging model</italic>
</td><td colspan="4" align="left" rowspan="1">LPP&#x02009;&#x0223c;&#x02009;information&#x02009;&#x000d7;&#x02009;attractiveness + (information&#x02009;&#x000d7;&#x02009;attractiveness|| subject) + (1 | face)</td></tr></tbody></table><table-wrap-foot><fn id="T0005-fn1"><p>
<italic toggle="yes">Note.&#x02009;</italic>&#x000d7;&#x02009;stands for interaction. Double bars in random effects terms set correlation parameters to zero. Separate comparisons&#x02019; <italic toggle="yes">P</italic>-values were FDR corrected for nine tests. <italic toggle="yes">P</italic>-values &#x0003c; .05 are printed in bold.</p></fn></table-wrap-foot></table-wrap></sec></sec><sec id="s4-s2"><title>Manipulation checks (Phase 1)</title><sec id="s4-s2-s1"><title>Likability ratings before and after information exposure</title><p>Before information exposure, we replicated effects from the literature showing that faces high in attractiveness were more likable than faces low in attractiveness (<xref rid="F5" ref-type="fig">Fig.&#x000a0;5</xref>, <xref rid="T6" ref-type="table">Table&#x000a0;6</xref>). The interaction of attractiveness with information before information was given was unexpected, because the assignment of faces to information conditions was counterbalanced and thereby controlled (see SI-page 4&#x02009;f for additional analyses in <xref rid="s8" ref-type="sec">supplementary material</xref> showing that this pertains the neutral condition).</p><fig position="float" id="F5" fig-type="figure"><label>Figure&#x000a0;5.</label><caption><p>Likability (a) and attractiveness (b) ratings of the faces in Phase 1 were determinded by attractiveness before social-emotional information was acquired, whereas, after exposure to social-emotional information ratings were influenced by both attractiveness and social-emotional information.</p></caption><graphic xlink:href="nsae071f5" position="float"/></fig><table-wrap position="float" id="T6"><label>Table&#x000a0;6.</label><caption><p>Mixed model summary statistics show effects of positive knowledge and negative knowledge each in interaction with attractiveness on likability ratings before and after knowledge acquisition</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><tbody><tr><th valign="bottom" colspan="5" align="center" rowspan="1">Likability rating</th></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Coefficient</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">z</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Phase (before vs. after)</td><td align="left" rowspan="1" colspan="1">0.30</td><td align="left" rowspan="1" colspan="1">0.19</td><td align="left" rowspan="1" colspan="1">1.61</td><td align="left" rowspan="1" colspan="1">.11</td></tr><tr><td colspan="5" align="left" rowspan="1">Before information exposure</td></tr><tr><td align="left" rowspan="1" colspan="1">High vs. low attractiveness</td><td align="left" rowspan="1" colspan="1">2.92</td><td align="left" rowspan="1" colspan="1">0.43</td><td align="left" rowspan="1" colspan="1">6.86</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral information</td><td align="left" rowspan="1" colspan="1">&#x02212;0.25</td><td align="left" rowspan="1" colspan="1">0.21</td><td align="left" rowspan="1" colspan="1">&#x02212;1.18</td><td align="left" rowspan="1" colspan="1">.24</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral information</td><td align="left" rowspan="1" colspan="1">&#x02212;0.07</td><td align="left" rowspan="1" colspan="1">0.23</td><td align="left" rowspan="1" colspan="1">&#x02212;0.28</td><td align="left" rowspan="1" colspan="1">.78</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.80</td><td align="left" rowspan="1" colspan="1">0.44</td><td align="left" rowspan="1" colspan="1">1.80</td><td align="left" rowspan="1" colspan="1">.07</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.94</td><td align="left" rowspan="1" colspan="1">0.46</td><td align="left" rowspan="1" colspan="1">2.07</td><td align="left" rowspan="1" colspan="1">
<bold>.039</bold>
</td></tr><tr><td colspan="5" align="left" rowspan="1">After information exposure</td></tr><tr><td align="left" rowspan="1" colspan="1">High vs. low attractiveness</td><td align="left" rowspan="1" colspan="1">2.29</td><td align="left" rowspan="1" colspan="1">0.48</td><td align="left" rowspan="1" colspan="1">4.79</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral information</td><td align="left" rowspan="1" colspan="1">&#x02212;4.97</td><td align="left" rowspan="1" colspan="1">0.45</td><td align="left" rowspan="1" colspan="1">&#x02212;10.97</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral information</td><td align="left" rowspan="1" colspan="1">2.62</td><td align="left" rowspan="1" colspan="1">0.59</td><td align="left" rowspan="1" colspan="1">4.42</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">&#x02212;0.65</td><td align="left" rowspan="1" colspan="1">0.57</td><td align="left" rowspan="1" colspan="1">&#x02212;1.15</td><td align="left" rowspan="1" colspan="1">.25</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.09</td><td align="left" rowspan="1" colspan="1">0.56</td><td align="left" rowspan="1" colspan="1">0.17</td><td align="left" rowspan="1" colspan="1">.87</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Formula of converging model</italic>
</td><td colspan="4" align="left" rowspan="1">Rating&#x02009;&#x0223c;&#x02009;phase/ information&#x02009;&#x000d7;&#x02009;attractiveness + (1&#x02009;+&#x02009;phase/ information&#x02009;&#x000d7;&#x02009;attractiveness| subject) + (1 | face)</td></tr></tbody></table><table-wrap-foot><fn id="T0006-fn1"><p>
<italic toggle="yes">Note.&#x02009;</italic>&#x000d7;&#x02009;stands for interaction. The / in the model formula denotes nesting. <italic toggle="yes">P</italic>-values &#x0003c; .05 are printed in bold.</p></fn></table-wrap-foot></table-wrap><p>After information exposure, likability ratings showed main effects of information and attractiveness, and no interactions (<xref rid="F5" ref-type="fig">Fig.&#x000a0;5</xref>, <xref rid="T6" ref-type="table">Table&#x000a0;6</xref>). Faces associated with positive information and faces high in attractiveness were rated more likable than those with neutral information and low attractiveness, respectively. Faces associated with negative information and faces low in attractiveness were rated less likable than those with neutral information and high attractiveness, respectively.</p></sec><sec id="s4-s2-s2"><title>Attractiveness ratings before and after information exposure</title><p>We manipulated the attractiveness of faces based on pre-ratings as high or low and validated our manipulation with the current sample.</p><p>Before information exposure, results confirmed that faces were clearly perceived as differentially attractive (<xref rid="F5" ref-type="fig">Fig.&#x000a0;5</xref>, <xref rid="T7" ref-type="table">Table&#x000a0;7</xref>). Unexpectedly, we found interaction effects of attractiveness and information before participants were exposed to information. As was the case for the likability rating, these effects were driven by the neutral information condition (see SI-page 4&#x02009;f in <xref rid="s8" ref-type="sec">supplementary material</xref>).</p><table-wrap position="float" id="T7"><label>Table&#x000a0;7.</label><caption><p>Mixed model summary statistics and follow-up tests show effects of positive information and negative information each in interaction with attractiveness on attractiveness ratings before and after information exposure</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><tbody><tr><th valign="bottom" colspan="5" align="center" rowspan="1">Attractiveness rating</th></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Coefficient</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">b</italic>
</td><td align="left" rowspan="1" colspan="1">SE</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">z</italic>
</td><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">P</italic>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Phase (before vs. after)</td><td align="left" rowspan="1" colspan="1">0.03</td><td align="left" rowspan="1" colspan="1">0.12</td><td align="left" rowspan="1" colspan="1">0.26</td><td align="left" rowspan="1" colspan="1">.80</td></tr><tr><td colspan="5" align="left" rowspan="1">Before information exposure</td></tr><tr><td align="left" rowspan="1" colspan="1">High vs. low attractiveness</td><td align="left" rowspan="1" colspan="1">6.18</td><td align="left" rowspan="1" colspan="1">0.57</td><td align="left" rowspan="1" colspan="1">10.76</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral information</td><td align="left" rowspan="1" colspan="1">&#x02212;0.25</td><td align="left" rowspan="1" colspan="1">0.20</td><td align="left" rowspan="1" colspan="1">&#x02212;1.29</td><td align="left" rowspan="1" colspan="1">.20</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral information</td><td align="left" rowspan="1" colspan="1">&#x02212;0.12</td><td align="left" rowspan="1" colspan="1">0.21</td><td align="left" rowspan="1" colspan="1">&#x02212;0.55</td><td align="left" rowspan="1" colspan="1">.58</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">1.28</td><td align="left" rowspan="1" colspan="1">0.41</td><td align="left" rowspan="1" colspan="1">3.14</td><td align="left" rowspan="1" colspan="1">
<bold>.002</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">1.40</td><td align="left" rowspan="1" colspan="1">0.43</td><td align="left" rowspan="1" colspan="1">3.25</td><td align="left" rowspan="1" colspan="1">
<bold>.001</bold>
</td></tr><tr><td colspan="5" align="left" rowspan="1">After information exposure</td></tr><tr><td align="left" rowspan="1" colspan="1">High vs. low attractiveness</td><td align="left" rowspan="1" colspan="1">5.38</td><td align="left" rowspan="1" colspan="1">0.64</td><td align="left" rowspan="1" colspan="1">8.36</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral information</td><td align="left" rowspan="1" colspan="1">&#x02212;2.15</td><td align="left" rowspan="1" colspan="1">0.33</td><td align="left" rowspan="1" colspan="1">&#x02212;6.61</td><td align="left" rowspan="1" colspan="1">
<bold>&#x0003c;.001</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral information</td><td align="left" rowspan="1" colspan="1">0.78</td><td align="left" rowspan="1" colspan="1">0.31</td><td align="left" rowspan="1" colspan="1">2.48</td><td align="left" rowspan="1" colspan="1">
<bold>.013</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Negative vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.56</td><td align="left" rowspan="1" colspan="1">0.45</td><td align="left" rowspan="1" colspan="1">1.25</td><td align="left" rowspan="1" colspan="1">.21</td></tr><tr><td align="left" rowspan="1" colspan="1">Positive vs. neutral info&#x02009;&#x000d7;&#x02009;high vs. low att</td><td align="left" rowspan="1" colspan="1">0.84</td><td align="left" rowspan="1" colspan="1">0.42</td><td align="left" rowspan="1" colspan="1">2.01</td><td align="left" rowspan="1" colspan="1">
<bold>.04</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Formula of converging model</italic>
</td><td colspan="4" align="left" rowspan="1">Rating&#x02009;&#x0223c;&#x02009;phase/ information&#x02009;&#x000d7;&#x02009;attractiveness + (1&#x02009;+&#x02009;phase/ information&#x02009;&#x000d7;&#x02009;attractiveness| subject) + (1 | face)</td></tr></tbody></table><table-wrap-foot><fn id="T0007-fn1"><p>
<italic toggle="yes">Note.&#x02009;</italic>&#x000d7;&#x02009;stands for interaction. The / in the model formula denotes nesting. <italic toggle="yes">P</italic>-values &#x0003c; .05 are printed in bold.</p></fn></table-wrap-foot></table-wrap><p>After information exposure, social-emotional information also affected how attractive participants found the faces (<xref rid="F5" ref-type="fig">Fig.&#x000a0;5</xref>, <xref rid="T7" ref-type="table">Table&#x000a0;7</xref>). Apart from the main effect of attractiveness, faces associated with positive information were rated as more attractive, and faces associated with negative information were rated as less attractive compared to neutral information, respectively. An interaction of the effect of positive information and attractiveness suggests that the effect of information was more pronounced for the congruent, high attractive faces.</p></sec></sec></sec><sec id="s5"><title>Discussion</title><p>How do facial and moral beauty jointly shape how we judge others? We investigated effects of facial attractiveness and social-emotional person-related information and their interplay on ERPs and social judgments. These global social evaluations should reflect a natural tendency of forming social impressions from visual appearance or semantic information (<xref rid="R81" ref-type="bibr">Todorov et&#x000a0;al. 2007</xref>, <xref rid="R13" ref-type="bibr">Bliss-Moreau et&#x000a0;al. 2008</xref>, <xref rid="R31" ref-type="bibr">Goodwin 2015</xref>, <xref rid="R88" ref-type="bibr">Uhlmann et&#x000a0;al. 2015</xref>, <xref rid="R7" ref-type="bibr">Baum and Abdel Rahman 2021</xref>). Results show a dominance of moral beauty in valenced social judgments as well as ERPs related to reflexive and evaluative emotional responses (EPN, LPP), whereas facial attractiveness mattered little. In contrast, visual processing (N170) and impressions of attractiveness and likability, that can be formed based on visual information without knowledge about the person, were influenced by both facial attractiveness and social-emotional information.</p><p>We found general halo effects of facial and moral beauty in relatively shallow judgments of spontaneous likability and attractiveness. They can be considered shallow or superficial because they can be based on visual impression with no need for additional person information and faces are supposedly processed more as percepts rather than concepts or persons (<xref rid="R71" ref-type="bibr">Schnuerch et&#x000a0;al. 2015</xref>, <xref rid="R77" ref-type="bibr">Schwartz and Yovel 2016</xref>). Consistent with the literature on halo effects, we show that high vs. low attractiveness determined likeability impressions when no other information was available (e.g. <xref rid="R19" ref-type="bibr">Dion et&#x000a0;al. 1972</xref>, <xref rid="R22" ref-type="bibr">Eagly et&#x000a0;al. 1991</xref>, <xref rid="R41" ref-type="bibr">Langlois et&#x000a0;al. 2000</xref>, <xref rid="R95" ref-type="bibr">Zebrowitz and Montepare 2008</xref>), and that social-emotional information changed the likability and the attractiveness of faces (<xref rid="R53" ref-type="bibr">Nisbett and Wilson 1977</xref>, <xref rid="R35" ref-type="bibr">He et al. 2024</xref>). The pattern of results suggests that facial and moral beauty can affect judgments that are not directly related to facial or moral beauty. However, such halo effects may be most pronounced for those relatively shallow judgments.</p><p>Importantly, the ratings of attractiveness and likability before and after exposure to social-emotional information show that both factors were successfully manipulated, confirming that faces were clearly perceived as either high or low in attractiveness and that person-related information was perceived either positive, negative, or neutral.</p><p>In contrast to the relatively shallow judgments of attractiveness and likeability, we seem less likely to generalize vision-based impressions for explicit social judgments, and more likely to base these judgments on person-related information, rather than facial beauty. Social judgments employed here can be considered less shallow because we asked participants to form person judgments taking all available information into account&#x02014;that is judgments for which moral character should be most relevant (<xref rid="R31" ref-type="bibr">Goodwin 2015</xref>). Results show that only the latencies of social judgments, but not the actual outcomes of the social judgments, were affected by facial attractiveness. Participants hesitated with their judgments when the facial attractiveness was incongruent with the social-emotional information. This suggests that facial beauty was processed during social judgments, even though interestingly, the valence of the judgments was influenced only be the social-emotional information and not by attractiveness.</p><p>We investigated the effects on visual face perception in the P1 and N170. The P1 has been associated with generators in the ventral extrastriate cortex (V4) of the fusiform gyrus (<xref rid="R20" ref-type="bibr">Di Russo et&#x000a0;al. 2002</xref>), while the N170 with the fusiform face area and superior temporal sulcus (<xref rid="R36" ref-type="bibr">Herrmann et&#x000a0;al. 2005</xref>, <xref rid="R16" ref-type="bibr">Deffke et&#x000a0;al. 2007</xref>, <xref rid="R52" ref-type="bibr">Nguyen and Cunnington 2014</xref>, <xref rid="R29" ref-type="bibr">Gao et&#x000a0;al. 2019</xref>, <xref rid="R69" ref-type="bibr">Schindler et&#x000a0;al. 2023</xref>). While the P1 was not modulated, we found enhanced N170 amplitudes for faces of low compared to high attractiveness. This suggests an early and automatic processing of facial attractiveness, presumably related to a modulation of activity in face-selective brain regions. If anything, trends furthermore suggested that this effect was pronounced for positive and negative, but not neutral, associated information. Effects in the N170 suggest enhanced visual processing of faces scoring low in facial beauty. This replicates earlier reports of attractiveness modulations in the N170 (<xref rid="R32" ref-type="bibr">Hahn et&#x000a0;al. 2016</xref>, <xref rid="R85" ref-type="bibr">Trujillo et&#x000a0;al. 2014</xref>; but see <xref rid="R62" ref-type="bibr">Revers et&#x000a0;al. 2023</xref> for mixed findings for the direction of the effect). It has been suggested that enhanced N170 amplitudes for lower attractiveness could be related to less fluent configural processing compared to higher or average attractiveness (<xref rid="R85" ref-type="bibr">Trujillo et&#x000a0;al. 2014</xref>). However, in contrast to social-emotional information where we fully counterbalanced the assignment to faces, the attractiveness manipulation entailed purely physical differences in the faces that may have contributed to the N170 modulation, even though luminance was adjusted.</p><p>We investigated effects on reflexive emotion processing in the EPN. The EPN has been related to increased extrastriate cortex activation by emotional stimuli content, possibly due to feedback from the amygdala to sensory pathways (<xref rid="R57" ref-type="bibr">Pourtois et&#x000a0;al. 2013</xref>, <xref rid="R67" ref-type="bibr">Schettino et&#x000a0;al. 2016</xref>). Our findings show EPN effects of positive and negative social-emotional information independent of attractiveness, suggesting that moral beauty is most relevant to fast and reflexive emotional processing. Although attractiveness as vision-based information could be directly accessed, it did not affect reflexive emotional brain responses during social judgments (<xref rid="R91" ref-type="bibr">Werheid et&#x000a0;al. 2007</xref>, <xref rid="R65" ref-type="bibr">Schacht et&#x000a0;al. 2008</xref>). Instead, the EPN results indicate that the nonvisual, social-emotional information mattered most during EPN-generating sensory processes starting at about 200&#x02009;ms, a time of the processing when visual perception is not yet fully completed and that is largely independent of the current goal or motivation (<xref rid="R64" ref-type="bibr">Schacht and Sommer 2009</xref>, <xref rid="R60" ref-type="bibr">Rellecke et&#x000a0;al. 2011</xref>, <xref rid="R61" ref-type="bibr">2012</xref>).</p><p>Finally, we investigated effects on more elaborate and post-perceptual evaluations in the LPP. Broader brain networks underlie the generation of such later ERPs, including widespread cortical, corticolimbic, and subcortical activations associated with visual and emotional processing (<xref rid="R76" ref-type="bibr">Schupp et&#x000a0;al. 2007</xref>, <xref rid="R45" ref-type="bibr">Liu et&#x000a0;al. 2012</xref>, <xref rid="R63" ref-type="bibr">Sabatinelli et&#x000a0;al. 2013</xref>, <xref rid="R84" ref-type="bibr">Trautmann-Lengsfeld et&#x000a0;al. 2013</xref>). LPP results showed dominant effects of social-emotional information during social judgments, but also suggest that facial attractiveness was processed. Specifically, for faces scoring low in attractiveness, the LPP effect of negative information was slightly more pronounced compared to positive information. This may indicate a compounded effect of negative social-emotional information and low attractiveness during this sustained evaluation of the person. Evidence for similar compounded effects in the LPP was reported for peer opinions of facial beauty and vision-based attractiveness during facial attractiveness judgments (<xref rid="R80" ref-type="bibr">Thiruchselvam et&#x000a0;al. 2016</xref>). Furthermore, more pronounced LPP amplitudes were related to faster judgments (see SI-page 6 in <xref rid="s8" ref-type="sec">supplementary material</xref> for an additional model with LPP amplitudes as predictor for latencies of social judgments). Consistent with the effects found for latencies and LPP, this relation may indicate that the fast social judgments for congruent attractiveness and social-emotional information were processed more elaborately and efficiently on a neurocognitive level (<xref rid="R72" ref-type="bibr">Schupp et&#x000a0;al. 2000</xref>).</p><p>One may ask whether results were different when the subjective attractiveness rating of each subject for each face was considered. Additional analyses replacing the predefined attractiveness factor with the subjective attractiveness ratings of the participants show, if anything, that social judgments were modulated by subjective attractiveness impressions when the information was positive or neutral (see SI-page 7ff in <xref rid="s8" ref-type="sec">supplementary material</xref>). However, social judgments remained unaffected by either attractiveness scores&#x02014;consensus or subjective&#x02014;when the information was negative. Thus, if attractiveness affected social judgments, it was only as a subjective impression and only when person-related information was positive or neutral. This is in line with related findings showing that effects of positive, but not effects of negative emotional information may be modulated by facial appearance or other potentially modulating factors (<xref rid="R7" ref-type="bibr">Baum and Abdel Rahman 2021</xref>, <xref rid="R26" ref-type="bibr">Eiserbeck et&#x000a0;al. 2023</xref>). For ERPs, analyses including subjective attractiveness remained to show the main pattern of results with dominant effects of social-emotional information (SI-page 9ff in <xref rid="s8" ref-type="sec">supplementary material</xref>).</p><p>Overall, the current findings show largely similar effects of negative and positive social-emotional information. Nevertheless, the effect of negative may be stronger than the effect of positive social-emotional information possibly related to their intensity and differences in underlying processes which should be further investigated in future studies (please note that pre-ratings of our materials showed that positive and negative information was equally arousing, SI-page 2 in <xref rid="s8" ref-type="sec">supplementary material</xref>). For instance, the effects of positive information may be more susceptible to modulating factors (<xref rid="R38" ref-type="bibr">Isen et&#x000a0;al. 1992</xref>, <xref rid="R4" ref-type="bibr">Ashby et&#x000a0;al. 1999</xref>, <xref rid="R7" ref-type="bibr">Baum and Abdel Rahman 2021</xref>, <xref rid="R26" ref-type="bibr">Eiserbeck et&#x000a0;al. 2023</xref>). Furthermore, effects of negative emotions may be less modulated due to a prioritization of potential threat (<xref rid="R55" ref-type="bibr">&#x000d6;hman and Mineka 2001</xref>).</p><p>One possible mechanism that can help explain the dominance of moral beauty is emotional arousal. As discussed before, modulations of the EPN and LPP are primarily driven by the arousal dimension of emotions (e.g. <xref rid="R74" ref-type="bibr">Schupp et&#x000a0;al. 2003</xref>). High and low attractive faces may be similarly arousing (<xref rid="R44" ref-type="bibr">Liu and Chen 2012</xref>). In a <italic toggle="yes">post hoc</italic> rating of the faces used in our study, we also show that the high and low attractiveness conditions differ in valence but not in arousal (see SI-page 2 in <xref rid="s8" ref-type="sec">supplementary material</xref>). Moreover, the arousal does not differ between the high/low faces and the medium attractive filler faces, with all conditions showing relatively low arousal ratings on average (M<sub>all</sub>&#x02009;=&#x02009;2.48, SD<sub>all</sub>&#x02009;=&#x02009;0.97 on a 7-point scale, SI-page 2 in <xref rid="s8" ref-type="sec">supplementary material</xref>). Therefore, emotional responses may prioritize the more arousing social-emotional information as seen in modulations of the EPN and LPP. This dominance is in line with findings employing a similar paradigm with social-emotional information and the same social judgment task (<xref rid="R6" ref-type="bibr">Baum and Abdel Rahman 2020</xref>, <xref rid="R7" ref-type="bibr">2021</xref>, <xref rid="R9" ref-type="bibr">Baum et&#x000a0;al. 2020</xref>; <xref rid="R8" ref-type="bibr">Baum et al. 2024</xref>). Thus, our behavioral and neurocognitive findings imply that social judgments and underlying processes involved in social-emotional face and person evaluation rely on the most emotionally arousing information, which is moral beauty, rather than facial beauty.</p><p>We cannot rule out the possibility that smaller effects of attractiveness would be reflected in social judgments behaviorally if the scale was more fine-grained. Yet, we also find dominant effects of social-emotional information in ERPs that are more implicit, especially the EPN, which should be minimally influenced by the specific answer scale (e.g. <xref rid="R60" ref-type="bibr">Rellecke et&#x000a0;al. 2011</xref>). As discussed above, the effects on social judgments seem to be strongly driven by arousal which can help explain the lack of effects of attractiveness on judgments. Importantly, our finding that social-emotional information shows dominant effects is consistent with the literature where appearance-based effects were reduced or absent in face of social-emotional or moral information independent of how fine-grained the scale was (see <xref rid="R51" ref-type="bibr">Mo et&#x000a0;al. 2016</xref>, <xref rid="R24" ref-type="bibr">Eiserbeck and Abdel Rahman 2020</xref>, <xref rid="R25" ref-type="bibr">Eiserbeck et al. 2024</xref>).</p><sec id="s5-s1"><title>Limitations and future directions</title><p>Despite carful counterbalancing of the assignment of faces to information, we found unexpected interaction effects of attractiveness and social-emotional information in the ratings before any information was given. We show that this was driven by the neutral information condition and, critically, that there were no effects related to positive or negative information before participants were even exposed to it. Thus, these unexpected modulations seem unproblematic to our key manipulations of positive and negative social-emotional information since these were unaffected.</p><p>Our sample consisted of female participants only to avoid confounds due to the different perceptions of attractiveness related to the gender of the participants [see also Tsukiura and Cabeza (<xref rid="R86" ref-type="bibr">2011a</xref>, <xref rid="R87" ref-type="bibr">2011b</xref>)]. For this study, we used faces for which the consensus impressions were similar in pre-ratings and the experiment. However, subjective differences in the perception of attractiveness are evident which may depend on e.g. gender and cultures and should be investigated in future studies [see <xref rid="R82" ref-type="bibr">Todorov et&#x000a0;al. (2015</xref>), <xref rid="R59" ref-type="bibr">Rafiee and Schacht (2023</xref>)]. Furthermore, other impressions from facial appearance and their interplay with moral beauty during social judgments should be studied, for example facial trustworthiness (<xref rid="R43" ref-type="bibr">Lischke et&#x000a0;al. 2018</xref>, <xref rid="R90" ref-type="bibr">Wendt et&#x000a0;al. 2019</xref>, <xref rid="R92" ref-type="bibr">Weymar et&#x000a0;al. 2019</xref>, <xref rid="R24" ref-type="bibr">Eiserbeck and Abdel Rahman 2020</xref>, <xref rid="R54" ref-type="bibr">Oh et&#x000a0;al. 2023</xref>).</p></sec></sec><sec id="s6"><title>Conclusion</title><p>Taken together, this study shows that moral beauty, although not physically visible, matters more to our emotional responses and social evaluations of others than visually derived facial beauty. While we do appreciate and process the facial beauty of others and rely on it for shallow judgments, for deeper social judgments we count more on people&#x02019;s moral beauty than on their facial beauty.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>nsae071_Supp</label><media xlink:href="nsae071_supp.zip"/></supplementary-material></sec></body><back><ack id="ack1"><title>Acknowledgements</title><p>We thank Guido Kiecker for technical support. We thank Asne Senberg, Leolo Vogt, Luisa Balzus, and Cornelius Braun for help in stimulus preparation and /or data acquisition. Special thanks to Bernhard Glocksin and Sabrina Rosetto for inspiring conversations about the wonders of moral beauty.</p></ack><sec id="s7"><title>Author contributions</title><p>J.B. and R.A.R designed research and wrote the paper; J.B. performed research and analyzed data.</p></sec><sec id="s8"><title>Supplementary data</title><p>
<xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> is available at <italic toggle="yes">SCAN</italic> online.</p></sec><sec sec-type="COI-statement" id="s9"><title>Conflict of interest</title><p>None declared.</p></sec><sec id="s10"><title>Funding</title><p>This work was supported by a PhD scholarship of Humboldt-Research Track and a Professorin Ruebsamen-Schaeff Stipend by Studienstiftung des deutschen Volkes to Julia Baum and a German Research Foundation (AB 277-6 to R.A.R.).</p></sec><sec sec-type="data-availability" id="s11"><title>Data availability</title><p>Data and code are available online (<ext-link xlink:href="https://osf.io/e529z/" ext-link-type="uri">https://osf.io/e529z/</ext-link>).</p></sec><ref-list id="ref1"><title>References</title><ref id="R1"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name>
</person-group>. <article-title>Facing good and evil: early brain signatures of affective biographical knowledge in face recognition</article-title>. <source><italic toggle="yes">Emotion</italic></source> &#x000a0;<year>2011</year>;<volume>11</volume>:<fpage>1397</fpage>&#x02013;<lpage>405</lpage>. doi: <pub-id pub-id-type="doi">10.1037/a0024717</pub-id><pub-id pub-id-type="pmid">21859200</pub-id>
</mixed-citation></ref><ref id="R2"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Sommer</surname> &#x000a0;<given-names>W</given-names></string-name></person-group>. <article-title>Knowledge scale effects in face recognition: an electrophysiological investigation</article-title>. <source><italic toggle="yes">Cogn Affect Behav Neurosci</italic></source> &#x000a0;<year>2012</year>;<volume>12</volume>:<fpage>161</fpage>&#x02013;<lpage>174</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13415-011-0063-9</pub-id><pub-id pub-id-type="pmid">21979895</pub-id>
</mixed-citation></ref><ref id="R3"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Anderson</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Siegel</surname> &#x000a0;<given-names>EH</given-names></string-name>, <string-name><surname>Bliss-Moreau</surname> &#x000a0;<given-names>E</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The visual impact of gossip</article-title>. <source><italic toggle="yes">Science</italic></source> &#x000a0;<year>2011</year>;<volume>332</volume>:<fpage>1446</fpage>&#x02013;<lpage>48</lpage>. doi: <pub-id pub-id-type="doi">10.1126/science.1201574</pub-id><pub-id pub-id-type="pmid">21596956</pub-id>
</mixed-citation></ref><ref id="R4"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ashby</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Isen</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Turken</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>A neuropsychological theory of positive affect and its influence on cognition</article-title>. <source><italic toggle="yes">Psychol Rev</italic></source> &#x000a0;<year>1999</year>;<volume>106</volume>:<fpage>529</fpage>&#x02013;<lpage>50</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0033-295X.106.3.529</pub-id><pub-id pub-id-type="pmid">10467897</pub-id>
</mixed-citation></ref><ref id="R5"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bates</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>M&#x000e4;chler</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Bolker</surname> &#x000a0;<given-names>BM</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Fitting linear mixed-effects models using lme4</article-title>. <source><italic toggle="yes">J Stat Softw</italic></source> &#x000a0;<year>2015</year>;<volume>67</volume>:<fpage>1</fpage>&#x02013;<lpage>48</lpage>. doi: <pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></mixed-citation></ref><ref id="R6"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Baum</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Emotional news affects social judgments independent of perceived media credibility</article-title>. <source><italic toggle="yes">Soc Cognit Affective Neurosci</italic></source> &#x000a0;<year>2020</year>;<volume>16</volume>:<fpage>280</fpage>&#x02013;<lpage>91</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsaa164</pub-id></mixed-citation></ref><ref id="R7"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Baum</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Negative news dominates fast and slow brain responses and social judgments even after source credibility evaluation</article-title>. <source><italic toggle="yes">NeuroImage</italic></source> &#x000a0;<year>2021</year>;<volume>244</volume>:<page-range>118572</page-range>. doi: <pub-id pub-id-type="doi">10.1016/J.NEUROIMAGE.2021.118572</pub-id></mixed-citation></ref><ref id="R8"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Baum</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Fr&#x000f6;mer</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Emotional content reduces the cognitive effort invested in processing the credibility of social (mis)information</article-title>. <source><italic toggle="yes">Emotion</italic></source> &#x000a0;<year>2024</year>;<volume>24</volume>:<fpage>1468</fpage>&#x02013;<lpage>80</lpage>. doi: <pub-id pub-id-type="doi">10.1037/emo0001355</pub-id><pub-id pub-id-type="pmid">38512199</pub-id>
</mixed-citation></ref><ref id="R9"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Baum</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Rabovsky</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Rose</surname> &#x000a0;<given-names>SB</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Clear judgments based on unclear evidence: person evaluation is strongly influenced by untrustworthy gossip</article-title>. <source><italic toggle="yes">Emotion</italic></source> &#x000a0;<year>2020</year>;<volume>20</volume>:<fpage>248</fpage>&#x02013;<lpage>60</lpage>. doi: <pub-id pub-id-type="doi">10.1037/emo0000545</pub-id><pub-id pub-id-type="pmid">30589302</pub-id>
</mixed-citation></ref><ref id="R10"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Benjamini</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Hochberg</surname> &#x000a0;<given-names>Y</given-names></string-name></person-group>. <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source><italic toggle="yes">J R Stat Soc Ser B</italic></source> &#x000a0;<year>1995</year>;<volume>57</volume>:<fpage>289</fpage>&#x02013;<lpage>300</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></mixed-citation></ref><ref id="R11"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Berg</surname> &#x000a0;<given-names>P</given-names></string-name>, <string-name><surname>Scherg</surname> &#x000a0;<given-names>M</given-names></string-name></person-group>. <article-title>Dipole models of eye movements and blinks</article-title>. <source><italic toggle="yes">Electroencephalography and Clinical Neurophysiology</italic></source> &#x000a0;<year>1991</year>;<volume>79</volume>:<fpage>36</fpage>&#x02013;<lpage>44</lpage>. doi: <pub-id pub-id-type="doi">10.1016/0013-4694(91)90154-V</pub-id><pub-id pub-id-type="pmid">1713550</pub-id>
</mixed-citation></ref><ref id="R12"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Blechert</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Sheppes</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Tella</surname> &#x000a0;<given-names>C</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>See what you think: reappraisal modulates behavioral and neural responses to social stimuli</article-title>. <source><italic toggle="yes">Psychol Sci</italic></source> &#x000a0;<year>2012</year>;<volume>23</volume>:<fpage>346</fpage>&#x02013;<lpage>53</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797612438559</pub-id><pub-id pub-id-type="pmid">22431908</pub-id>
</mixed-citation></ref><ref id="R13"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bliss-Moreau</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Barrett</surname> &#x000a0;<given-names>LF</given-names></string-name>, <string-name><surname>Wright</surname> &#x000a0;<given-names>CI</given-names></string-name></person-group>. <article-title>Individual differences in learning the affective value of others under minimal conditions</article-title>. <source><italic toggle="yes">Emotion</italic></source> &#x000a0;<year>2008</year>;<volume>8</volume>:<fpage>479</fpage>&#x02013;<lpage>93</lpage>. doi: <pub-id pub-id-type="doi">10.1037/1528-3542.8.4.479</pub-id><pub-id pub-id-type="pmid">18729580</pub-id>
</mixed-citation></ref><ref id="R14"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bradley</surname> &#x000a0;<given-names>MM</given-names></string-name>, <string-name><surname>Lang</surname> &#x000a0;<given-names>PJ</given-names></string-name></person-group>. <article-title>Measuring emotion: the self-assessment manikin and the semantic differential</article-title>. <source><italic toggle="yes">J Behav Ther Exp Psychiatry</italic></source> &#x000a0;<year>1994</year>;<volume>25</volume>:<fpage>49</fpage>&#x02013;<lpage>59</lpage>. doi: <pub-id pub-id-type="doi">10.1016/0005-7916(94)90063-9</pub-id><pub-id pub-id-type="pmid">7962581</pub-id>
</mixed-citation></ref><ref id="R15"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Cui</surname> &#x000a0;<given-names>X</given-names></string-name>, <string-name><surname>Cheng</surname> &#x000a0;<given-names>Q</given-names></string-name>, <string-name><surname>Lin</surname> &#x000a0;<given-names>W</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Author correction: different influences of facial attractiveness on judgments of moral beauty and moral goodness</article-title>. <source><italic toggle="yes">Sci Rep</italic></source> &#x000a0;<year>2019</year>;<volume>9</volume>:<fpage>212</fpage>&#x02013;<lpage>49</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-019-53743-9</pub-id><pub-id pub-id-type="pmid">30659214</pub-id>
</mixed-citation></ref><ref id="R16"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Deffke</surname> &#x000a0;<given-names>I</given-names></string-name>, <string-name><surname>Sander</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Heidenreich</surname> &#x000a0;<given-names>J</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>MEG/EEG sources of the 170-ms response to faces are co-localized in the fusiform gyrus</article-title>. <source><italic toggle="yes">NeuroImage</italic></source> &#x000a0;<year>2007</year>;<volume>35</volume>:<fpage>1495</fpage>&#x02013;<lpage>501</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.01.034</pub-id><pub-id pub-id-type="pmid">17363282</pub-id>
</mixed-citation></ref><ref id="R17"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Delorme</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Makeig</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source>J Neurosci Methods</source> &#x000a0;<year>2004</year>;<volume>134</volume>:<fpage>9</fpage>&#x02013;<lpage>21</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id>
</mixed-citation></ref><ref id="R18"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dering</surname> &#x000a0;<given-names>B</given-names></string-name>, <string-name><surname>Martin</surname> &#x000a0;<given-names>CD</given-names></string-name>, <string-name><surname>Moro</surname> &#x000a0;<given-names>S</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Face-sensitive processes one hundred milliseconds after picture onset</article-title>. <source><italic toggle="yes">Front Human Neurosci</italic></source> &#x000a0;<year>2011</year>;<volume>5</volume>:<page-range>93</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fnhum.2011.00093</pub-id></mixed-citation></ref><ref id="R19"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dion</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Berscheid</surname> &#x000a0;<given-names>E</given-names></string-name>, <string-name><surname>Walster</surname> &#x000a0;<given-names>E</given-names></string-name></person-group>. <article-title>What is beautiful is good</article-title>. <source><italic toggle="yes">J Pers Soc Psychol</italic></source> &#x000a0;<year>1972</year>;<volume>24</volume>:<fpage>285</fpage>&#x02013;<lpage>90</lpage>. doi: <pub-id pub-id-type="doi">10.1037/h0033731</pub-id><pub-id pub-id-type="pmid">4655540</pub-id>
</mixed-citation></ref><ref id="R20"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Di Russo</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Mart&#x000ed;nez</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Sereno</surname> &#x000a0;<given-names>MI</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Cortical sources of the early components of the visual evoked potential</article-title>. <source><italic toggle="yes">Human Brain Mapp</italic></source> &#x000a0;<year>2002</year>;<volume>15</volume>:<fpage>95</fpage>&#x02013;<lpage>111</lpage>. doi: <pub-id pub-id-type="doi">10.1002/hbm.10010</pub-id></mixed-citation></ref><ref id="R21"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dolcos</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Cabeza</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Event-related potentials of emotional memory: encoding pleasant, unpleasant, and neutral pictures</article-title>. <source><italic toggle="yes">Cognit Affect Behav Neurosci</italic></source> &#x000a0;<year>2002</year>;<volume>2</volume>:<fpage>252</fpage>&#x02013;<lpage>63</lpage>. doi: <pub-id pub-id-type="doi">10.3758/CABN.2.3.252</pub-id><pub-id pub-id-type="pmid">12775189</pub-id>
</mixed-citation></ref><ref id="R22"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Eagly</surname> &#x000a0;<given-names>AH</given-names></string-name>, <string-name><surname>Ashmore</surname> &#x000a0;<given-names>RD</given-names></string-name>, <string-name><surname>Makhijani</surname> &#x000a0;<given-names>MG</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>What is beautiful is good, but &#x02026;: a meta-analytic review of research on the physical attractiveness stereotype</article-title>. <source><italic toggle="yes">Psychol Bull</italic></source> &#x000a0;<year>1991</year>;<volume>110</volume>:<fpage>109</fpage>&#x02013;<lpage>28</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0033-2909.110.1.109</pub-id></mixed-citation></ref><ref id="R23"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Eimer</surname> &#x000a0;<given-names>M</given-names></string-name>
</person-group>. <article-title>The face-sensitivity of the N170 component</article-title>. <source><italic toggle="yes">Front Human Neurosci</italic></source> &#x000a0;<year>2011</year>;<volume>5</volume>:<page-range>119</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fnhum.2011.00119</pub-id></mixed-citation></ref><ref id="R24"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Eiserbeck</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Visual consciousness of faces in the attentional blink: knowledge-based effects of trustworthiness dominate over appearance-based impressions</article-title>. <source><italic toggle="yes">Conscious Cogn</italic></source> &#x000a0;<year>2020</year>;<volume>83</volume>:<page-range>102977</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.concog.2020.102977</pub-id></mixed-citation></ref><ref id="R25"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Eiserbeck</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Enge</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Rabovsky</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et al</etal>. <article-title>Distrust before first sight? examining knowledge- and appearance-based effects of trustworthiness on the visual consciousness of faces</article-title>. <source>Conscious Cogn</source> &#x000a0;<year>2024</year>;<volume>117</volume>:<fpage>103629</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.concog.2023.103629</pub-id><pub-id pub-id-type="pmid">38150782</pub-id>
</mixed-citation></ref><ref id="R26"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Eiserbeck</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Maier</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Baum</surname> &#x000a0;<given-names>J</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Deepfake smiles matter less&#x02014;the psychological and neural impact of presumed AI-generated faces</article-title>. <source><italic toggle="yes">Sci Rep</italic></source> &#x000a0;<year>2023</year>;<volume>13</volume>:<page-range>16111</page-range>. Article 1. doi: <pub-id pub-id-type="doi">10.1038/s41598-023-42802-x</pub-id></mixed-citation></ref><ref id="R27"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ferrari</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Oh</surname> &#x000a0;<given-names>DW</given-names></string-name>, <string-name><surname>Labbree</surname> &#x000a0;<given-names>BP</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Learning the affective value of people: more than affect-based mechanisms</article-title>. <source><italic toggle="yes">Acta Psychol</italic></source> &#x000a0;<year>2020</year>;<volume>203</volume>:<page-range>103011</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.actpsy.2020.103011</pub-id></mixed-citation></ref><ref id="R28"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fr&#x000f6;mer</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Maier</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Group-level EEG-processing pipeline for flexible single trial-based analyses including linear mixed models</article-title>. <source><italic toggle="yes">Front Neurosci</italic></source> &#x000a0;<year>2018</year>;<volume>12</volume>:<page-range>970</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2018.00048</pub-id></mixed-citation></ref><ref id="R29"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gao</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Conte</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Richards</surname> &#x000a0;<given-names>JE</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The neural sources of N170: understanding timing of activation in face-selective areas</article-title>. <source><italic toggle="yes">Psychophysiology</italic></source> &#x000a0;<year>2019</year>;<volume>56</volume>:<page-range>e13336</page-range>. doi: <pub-id pub-id-type="doi">10.1111/psyp.13336</pub-id></mixed-citation></ref><ref id="R30"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gim&#x000e9;nez-Fern&#x000e1;ndez</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Kessel</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Fern&#x000e1;ndez-Folgueiras</surname> &#x000a0;<given-names>U</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Prejudice drives exogenous attention to outgroups</article-title>. <source><italic toggle="yes">Soc Cognit Affective Neurosci</italic></source> &#x000a0;<year>2020</year>;<volume>15</volume>:<fpage>615</fpage>&#x02013;<lpage>24</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsaa087</pub-id></mixed-citation></ref><ref id="R31"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Goodwin</surname> &#x000a0;<given-names>GP</given-names></string-name>
</person-group>. <article-title>Moral character in person perception</article-title>. <source><italic toggle="yes">Curr Dir Psychol Sci</italic></source> &#x000a0;<year>2015</year>;<volume>24</volume>:<fpage>38</fpage>&#x02013;<lpage>44</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0963721414550709</pub-id></mixed-citation></ref><ref id="R32"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hahn</surname> &#x000a0;<given-names>AC</given-names></string-name>, <string-name><surname>Symons</surname> &#x000a0;<given-names>LA</given-names></string-name>, <string-name><surname>Kredel</surname> &#x000a0;<given-names>T</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Early and late event-related potentials are modulated by infant and adult faces of high and low attractiveness</article-title>. <source><italic toggle="yes">Soc Neurosci</italic></source> &#x000a0;<year>2016</year>;<volume>11</volume>:<fpage>207</fpage>&#x02013;<lpage>20</lpage>. doi: <pub-id pub-id-type="doi">10.1080/17470919.2015.1059361</pub-id><pub-id pub-id-type="pmid">26160142</pub-id>
</mixed-citation></ref><ref id="R33"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hassin</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Trope</surname> &#x000a0;<given-names>Y</given-names></string-name></person-group>. <article-title>Facing faces: studies on the cognitive aspects of physiognomy</article-title>. <source><italic toggle="yes">J Pers Soc Psychol</italic></source> &#x000a0;<year>2000</year>;<volume>78</volume>:<fpage>837</fpage>&#x02013;<lpage>52</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0022-3514.78.5.837</pub-id><pub-id pub-id-type="pmid">10821193</pub-id>
</mixed-citation></ref><ref id="R34"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Haynes</surname> &#x000a0;<given-names>JD</given-names></string-name>, <string-name><surname>Roth</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Stadler</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Neuromagnetic correlates of perceived contrast in primary visual cortex</article-title>. <source><italic toggle="yes">J Neurophysiol</italic></source> &#x000a0;<year>2003</year>;<volume>89</volume>:<fpage>2655</fpage>&#x02013;<lpage>66</lpage>. doi: <pub-id pub-id-type="doi">10.1152/jn.00820.2002</pub-id><pub-id pub-id-type="pmid">12612045</pub-id>
</mixed-citation></ref><ref id="R35"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>He</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Workman</surname> &#x000a0;<given-names>CI</given-names></string-name>, <string-name><surname>He</surname> &#x000a0;<given-names>X</given-names></string-name></person-group> &#x000a0;<etal>et al</etal>. <article-title>What is good is beautiful (and what isn&#x02019;t, isn&#x02019;t): how moral character affects perceived facial attractiveness</article-title>. <source>Psychol Aesthet Creat Arts</source> &#x000a0;<year>2024</year>;<volume>18</volume>:<fpage>633</fpage>&#x02013;<lpage>641</lpage>. doi: <pub-id pub-id-type="doi">10.1037/aca0000454</pub-id></mixed-citation></ref><ref id="R36"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Herrmann</surname> &#x000a0;<given-names>MJ</given-names></string-name>, <string-name><surname>Ehlis</surname> &#x000a0;<given-names>A-C</given-names></string-name>, <string-name><surname>Muehlberger</surname> &#x000a0;<given-names>A</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Source localization of early stages of face processing</article-title>. <source><italic toggle="yes">Brain Topogr</italic></source> &#x000a0;<year>2005</year>;<volume>18</volume>:<fpage>77</fpage>&#x02013;<lpage>85</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s10548-005-0277-7</pub-id><pub-id pub-id-type="pmid">16341576</pub-id>
</mixed-citation></ref><ref id="R37"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hinojosa</surname> &#x000a0;<given-names>JA</given-names></string-name>, <string-name><surname>Mercado</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Carreti&#x000e9;</surname> &#x000a0;<given-names>L</given-names></string-name></person-group>. <article-title>N170 sensitivity to facial expression: a meta-analysis</article-title>. <source><italic toggle="yes">Neurosci Biobehav Rev</italic></source> &#x000a0;<year>2015</year>;<volume>55</volume>:<fpage>498</fpage>&#x02013;<lpage>509</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.06.002</pub-id><pub-id pub-id-type="pmid">26067902</pub-id>
</mixed-citation></ref><ref id="R38"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Isen</surname> &#x000a0;<given-names>AM</given-names></string-name>, <string-name><surname>Niedenthal</surname> &#x000a0;<given-names>PM</given-names></string-name>, <string-name><surname>Cantor</surname> &#x000a0;<given-names>N</given-names></string-name></person-group>. <article-title>An influence of positive affect on social categorization</article-title>. <source><italic toggle="yes">Motiv Emot</italic></source> &#x000a0;<year>1992</year>;<volume>16</volume>:<fpage>65</fpage>&#x02013;<lpage>78</lpage>. doi: <pub-id pub-id-type="doi">10.1007/BF00996487</pub-id></mixed-citation></ref><ref id="R39"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Jungh&#x000f6;fer</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Bradley</surname> &#x000a0;<given-names>MM</given-names></string-name>, <string-name><surname>Elbert</surname> &#x000a0;<given-names>TR</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Fleeting images: a new look at early emotion discrimination</article-title>. <source><italic toggle="yes">Psychophysiology</italic></source> &#x000a0;<year>2001</year>;<volume>38</volume>:<fpage>175</fpage>&#x02013;<lpage>78</lpage>. doi: <pub-id pub-id-type="doi">10.1017/S0048577201000762</pub-id><pub-id pub-id-type="pmid">11347862</pub-id>
</mixed-citation></ref><ref id="R40"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kissler</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Strehlow</surname> &#x000a0;<given-names>J</given-names></string-name></person-group>. <article-title>Something always sticks? How emotional language modulates neural processes involved in face encoding and recognition memory</article-title>. <source><italic toggle="yes">Pozn Stud Contemp Linguist</italic></source> &#x000a0;<year>2017</year>;<volume>53</volume>:<fpage>63</fpage>&#x02013;<lpage>93</lpage>. doi: <pub-id pub-id-type="doi">10.1515/psicl-2017-0004</pub-id></mixed-citation></ref><ref id="R41"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Langlois</surname> &#x000a0;<given-names>JH</given-names></string-name>, <string-name><surname>Kalakanis</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Rubenstein</surname> &#x000a0;<given-names>AJ</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Maxims or myths of beauty? A meta-analytic and theoretical review</article-title>. <source><italic toggle="yes">Psychol Bull</italic></source> &#x000a0;<year>2000</year>;<volume>126</volume>:<fpage>390</fpage>&#x02013;<lpage>414</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0033-2909.126.3.390</pub-id><pub-id pub-id-type="pmid">10825783</pub-id>
</mixed-citation></ref><ref id="R42"><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Lenth</surname> &#x000a0;<given-names>R.</given-names></string-name>
</person-group> &#x000a0;<article-title>Emmeans: estimated marginal means, aka least-squares means. R package version 1.4.6</article-title>. <year>2020</year>. <ext-link xlink:href="https://CRAN.R-project.org/package=emmeans" ext-link-type="uri">https://CRAN.R-project.org/package=emmeans</ext-link></mixed-citation></ref><ref id="R43"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lischke</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Junge</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Hamm</surname> &#x000a0;<given-names>AO</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Enhanced processing of untrustworthiness in natural faces with neutral expressions</article-title>. <source><italic toggle="yes">Emotion</italic></source> &#x000a0;<year>2018</year>;<volume>18</volume>:<fpage>181</fpage>&#x02013;<lpage>89</lpage>. doi: <pub-id pub-id-type="doi">10.1037/emo0000318</pub-id><pub-id pub-id-type="pmid">28447825</pub-id>
</mixed-citation></ref><ref id="R44"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Liu</surname> &#x000a0;<given-names>CH</given-names></string-name>, <string-name><surname>Chen</surname> &#x000a0;<given-names>W</given-names></string-name></person-group>. <article-title>Beauty is better pursued: effects of attractiveness in multiple-face tracking</article-title>. <source><italic toggle="yes">Q J Exp Psychol</italic></source> &#x000a0;<year>2012</year>;<volume>65</volume>:<fpage>553</fpage>&#x02013;<lpage>564</lpage>. doi: <pub-id pub-id-type="doi">10.1080/17470218.2011.624186</pub-id></mixed-citation></ref><ref id="R45"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Liu</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Huang</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>McGinnis-Deweese</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Neural substrate of the late positive potential in emotional processing</article-title>. <source><italic toggle="yes">J Neurosci</italic></source> &#x000a0;<year>2012</year>;<volume>32</volume>:<fpage>14563</fpage>&#x02013;<lpage>72</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3109-12.2012</pub-id><pub-id pub-id-type="pmid">23077042</pub-id>
</mixed-citation></ref><ref id="R46"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Luo</surname> &#x000a0;<given-names>QL</given-names></string-name>, <string-name><surname>Wang</surname> &#x000a0;<given-names>HL</given-names></string-name>, <string-name><surname>Dzhelyova</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Effect of affective personality information on face processing: evidence from ERPs</article-title>. <source><italic toggle="yes">Front Psychol</italic></source> &#x000a0;<year>2016</year>;<volume>7</volume>:<page-range>1397</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2016.00810</pub-id></mixed-citation></ref><ref id="R47"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Maier</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Blume</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Bideau</surname> &#x000a0;<given-names>P</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Knowledge-augmented face perception: prospects for the bayesian brain-framework to align AI and human vision</article-title>. <source><italic toggle="yes">Conscious Cogn</italic></source> &#x000a0;<year>2022</year>;<volume>101</volume>:<page-range>103301</page-range>. doi: <pub-id pub-id-type="doi">10.1016/J.CONCOG.2022.103301</pub-id></mixed-citation></ref><ref id="R48"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Marzi</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Viggiano</surname> &#x000a0;<given-names>MP</given-names></string-name></person-group>. <article-title>When memory meets beauty: insights from event-related potentials</article-title>. <source><italic toggle="yes">Biol Psychol</italic></source> &#x000a0;<year>2010</year>;<volume>84</volume>:<fpage>192</fpage>&#x02013;<lpage>205</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.biopsycho.2010.01.013</pub-id><pub-id pub-id-type="pmid">20109520</pub-id>
</mixed-citation></ref><ref id="R49"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mattarozzi</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Colonnello</surname> &#x000a0;<given-names>V</given-names></string-name>, <string-name><surname>Thayer</surname> &#x000a0;<given-names>JF</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Trusting your heart: long-term memory for bad and good people is influenced by resting vagal tone</article-title>. <source><italic toggle="yes">Conscious Cogn</italic></source> &#x000a0;<year>2019</year>;<volume>75</volume>:<page-range>102810</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.concog.2019.102810</pub-id></mixed-citation></ref><ref id="R50"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Meeren</surname> &#x000a0;<given-names>HKM</given-names></string-name>, <string-name><surname>van Heijnsbergen</surname> &#x000a0;<given-names>CCRJ</given-names></string-name>, <string-name><surname>de Gelder</surname> &#x000a0;<given-names>B</given-names></string-name></person-group>. <article-title>Rapid perceptual integration of facial expression and emotional body language</article-title>. <source><italic toggle="yes">Proc Natl Acad Sci</italic></source> &#x000a0;<year>2005</year>;<volume>102</volume>:<fpage>16518</fpage>&#x02013;<lpage>23</lpage>. doi: <pub-id pub-id-type="doi">10.1073/pnas.0507650102</pub-id><pub-id pub-id-type="pmid">16260734</pub-id>
</mixed-citation></ref><ref id="R51"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mo</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Xia</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Qin</surname> &#x000a0;<given-names>K</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Natural tendency towards beauty in humans: evidence from binocular rivalry</article-title>. <source><italic toggle="yes">PLoS One</italic></source> &#x000a0;<year>2016</year>;<volume>11</volume>:<page-range>e0150147</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0150147</pub-id></mixed-citation></ref><ref id="R52"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Nguyen</surname> &#x000a0;<given-names>VT</given-names></string-name>, <string-name><surname>Cunnington</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>The superior temporal sulcus and the N170 during face processing: single trial analysis of concurrent EEG&#x02013;fMRI</article-title>. <source><italic toggle="yes">NeuroImage</italic></source> &#x000a0;<year>2014</year>;<volume>86</volume>:<fpage>492</fpage>&#x02013;<lpage>502</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.047</pub-id><pub-id pub-id-type="pmid">24185024</pub-id>
</mixed-citation></ref><ref id="R53"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Nisbett</surname> &#x000a0;<given-names>RE</given-names></string-name>, <string-name><surname>Wilson</surname> &#x000a0;<given-names>TD</given-names></string-name></person-group>. <article-title>The halo effect: evidence for unconscious alteration of judgments</article-title>. <source><italic toggle="yes">J Pers Soc Psychol</italic></source> &#x000a0;<year>1977</year>;<volume>35</volume>:<fpage>250</fpage>&#x02013;<lpage>56</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0022-3514.35.4.250</pub-id></mixed-citation></ref><ref id="R54"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Oh</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Wedel</surname> &#x000a0;<given-names>N</given-names></string-name>, <string-name><surname>Labbree</surname> &#x000a0;<given-names>B</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Trustworthiness judgments without the halo effect: a data-driven computational modeling approach</article-title>. <source><italic toggle="yes">Perception</italic></source> &#x000a0;<year>2023</year>;<volume>52</volume>:<fpage>590</fpage>&#x02013;<lpage>607</lpage>. doi: <pub-id pub-id-type="doi">10.1177/03010066231178489</pub-id><pub-id pub-id-type="pmid">37321648</pub-id>
</mixed-citation></ref><ref id="R55"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>&#x000d6;hman</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Mineka</surname> &#x000a0;<given-names>S</given-names></string-name></person-group>. <article-title>Fears, phobias, and preparedness: toward an evolved module of fear and fear learning</article-title>. <source><italic toggle="yes">Psychol Rev</italic></source> &#x000a0;<year>2001</year>;<volume>108</volume>:<fpage>483</fpage>&#x02013;<lpage>522</lpage>. doi: <pub-id pub-id-type="doi">10.1037/0033-295X.108.3.483</pub-id><pub-id pub-id-type="pmid">11488376</pub-id>
</mixed-citation></ref><ref id="R56"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Olivola</surname> &#x000a0;<given-names>CY</given-names></string-name>, <string-name><surname>Funk</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Todorov</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Social attributions from faces bias human choices</article-title>. <source><italic toggle="yes">Trends Cogn Sci</italic></source> &#x000a0;<year>2014</year>;<volume>18</volume>:<fpage>566</fpage>&#x02013;<lpage>70</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tics.2014.09.007</pub-id><pub-id pub-id-type="pmid">25344029</pub-id>
</mixed-citation></ref><ref id="R57"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pourtois</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Schettino</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Vuilleumier</surname> &#x000a0;<given-names>P</given-names></string-name></person-group>. <article-title>Brain mechanisms for emotional influences on perception and attention: what is magic and what is not</article-title>. <source><italic toggle="yes">Biol Psychol</italic></source> &#x000a0;<year>2013</year>;<volume>92</volume>:<fpage>492</fpage>&#x02013;<lpage>512</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.biopsycho.2012.02.007</pub-id><pub-id pub-id-type="pmid">22373657</pub-id>
</mixed-citation></ref><ref id="R58"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Rabovsky</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Stein</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Access to awareness for faces during continuous flash suppression is not modulated by affective knowledge</article-title>. <source><italic toggle="yes">PLoS One</italic></source> &#x000a0;<year>2016</year>;<volume>11</volume>:<page-range>e0150931</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0150931</pub-id></mixed-citation></ref><ref id="R59"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Rafiee</surname> &#x000a0;<given-names>Y</given-names></string-name>, <string-name><surname>Schacht</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Sex differences in emotion recognition: investigating the moderating effects of stimulus features</article-title>. <source><italic toggle="yes">Cogn Emot</italic></source> &#x000a0;<year>2023</year>;<volume>37</volume>:<fpage>863</fpage>&#x02013;<lpage>73</lpage>. doi: <pub-id pub-id-type="doi">10.1080/02699931.2023.2222579</pub-id><pub-id pub-id-type="pmid">37310161</pub-id>
</mixed-citation></ref><ref id="R60"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Rellecke</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Palazova</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Sommer</surname> &#x000a0;<given-names>W</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>On the automaticity of emotion processing in words and faces: event-related brain potentials evidence from a superficial task</article-title>. <source><italic toggle="yes">Brain Cogn</italic></source> &#x000a0;<year>2011</year>;<volume>77</volume>:<fpage>23</fpage>&#x02013;<lpage>32</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.bandc.2011.07.001</pub-id><pub-id pub-id-type="pmid">21794970</pub-id>
</mixed-citation></ref><ref id="R61"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Rellecke</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Sommer</surname> &#x000a0;<given-names>W</given-names></string-name>, <string-name><surname>Schacht</surname> &#x000a0;<given-names>A</given-names></string-name></person-group>. <article-title>Does processing of emotional facial expressions depend on intention? Time-resolved evidence from event-related brain potentials</article-title>. <source><italic toggle="yes">Biol Psychol</italic></source> &#x000a0;<year>2012</year>;<volume>90</volume>:<fpage>23</fpage>&#x02013;<lpage>32</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.biopsycho.2012.02.002</pub-id><pub-id pub-id-type="pmid">22361274</pub-id>
</mixed-citation></ref><ref id="R62"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Revers</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>Van Deun</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Vroomen</surname> &#x000a0;<given-names>J</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Neural responses to facial attractiveness: event-related potentials differentiate between salience and valence effects</article-title>. <source><italic toggle="yes">Biol Psychol</italic></source> &#x000a0;<year>2023</year>;<volume>179</volume>:<page-range>108549</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.biopsycho.2023.108549</pub-id></mixed-citation></ref><ref id="R63"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sabatinelli</surname> &#x000a0;<given-names>D</given-names></string-name>, <string-name><surname>Keil</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Frank</surname> &#x000a0;<given-names>DW</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Emotional perception: correspondence of early and late event-related potentials with cortical and subcortical functional MRI</article-title>. <source><italic toggle="yes">Biol Psychol</italic></source> &#x000a0;<year>2013</year>;<volume>92</volume>:<fpage>513</fpage>&#x02013;<lpage>19</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.biopsycho.2012.04.005</pub-id><pub-id pub-id-type="pmid">22560889</pub-id>
</mixed-citation></ref><ref id="R64"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schacht</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Sommer</surname> &#x000a0;<given-names>W</given-names></string-name></person-group>. <article-title>Emotions in word and face processing: early and late cortical responses</article-title>. <source><italic toggle="yes">Brain Cogn</italic></source> &#x000a0;<year>2009</year>;<volume>69</volume>:<fpage>538</fpage>&#x02013;<lpage>50</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.bandc.2008.11.005</pub-id><pub-id pub-id-type="pmid">19097677</pub-id>
</mixed-citation></ref><ref id="R65"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schacht</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Werheid</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Sommer</surname> &#x000a0;<given-names>W</given-names></string-name></person-group>. <article-title>The appraisal of facial beauty is rapid but not mandatory</article-title>. <source><italic toggle="yes">Cognit Affective Behav Neurosci</italic></source> &#x000a0;<year>2008</year>;<volume>8</volume>:<fpage>132</fpage>&#x02013;<lpage>42</lpage>. doi: <pub-id pub-id-type="doi">10.3758/CABN.8.2.132</pub-id></mixed-citation></ref><ref id="R66"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schad</surname> &#x000a0;<given-names>DJ</given-names></string-name>, <string-name><surname>Vasishth</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Hohenstein</surname> &#x000a0;<given-names>S</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>How to capitalize on a priori contrasts in linear (mixed) models: a tutorial</article-title>. <source><italic toggle="yes">J Memory Lang</italic></source> &#x000a0;<year>2020</year>;<volume>110</volume>:<page-range>104038</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.jml.2019.104038</pub-id></mixed-citation></ref><ref id="R67"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schettino</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Keil</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Porcu</surname> &#x000a0;<given-names>E</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Shedding light on emotional perception: interaction of brightness and semantic content in extrastriate visual cortex</article-title>. <source><italic toggle="yes">NeuroImage</italic></source> &#x000a0;<year>2016</year>;<volume>133</volume>:<fpage>341</fpage>&#x02013;<lpage>53</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.03.020</pub-id><pub-id pub-id-type="pmid">26994832</pub-id>
</mixed-citation></ref><ref id="R68"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schindler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Bruchmann</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Krasowski</surname> &#x000a0;<given-names>C</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Charged with a crime: the neuronal signature of processing negatively evaluated faces under different attentional conditions</article-title>. <source><italic toggle="yes">Psychol Sci</italic></source> &#x000a0;<year>2021</year>;<volume>32</volume>:<fpage>1311</fpage>&#x02013;<lpage>24</lpage>. doi: <pub-id pub-id-type="doi">10.1177/0956797621996667</pub-id><pub-id pub-id-type="pmid">34296955</pub-id>
</mixed-citation></ref><ref id="R69"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schindler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Bruchmann</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Straube</surname> &#x000a0;<given-names>T</given-names></string-name></person-group>. <article-title>Beyond facial expressions: a systematic review on effects of emotional relevance of faces on the N170</article-title>. <source><italic toggle="yes">Neurosci Biobehav Rev</italic></source> &#x000a0;<year>2023</year>;<volume>153</volume>:<page-range>105399</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.neubiorev.2023.105399</pub-id></mixed-citation></ref><ref id="R70"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schindler</surname> &#x000a0;<given-names>S</given-names></string-name>, <string-name><surname>Bublatzky</surname> &#x000a0;<given-names>F</given-names></string-name></person-group>. <article-title>Attention and emotion: an integrative review of emotional face processing as a function of attention</article-title>. <source><italic toggle="yes">Cortex</italic></source> &#x000a0;<year>2020</year>;<volume>130</volume>:<fpage>362</fpage>&#x02013;<lpage>86</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cortex.2020.06.010</pub-id><pub-id pub-id-type="pmid">32745728</pub-id>
</mixed-citation></ref><ref id="R71"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schnuerch</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Koppehele-Gossel</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Gibbons</surname> &#x000a0;<given-names>H</given-names></string-name></person-group>. <article-title>Weak encoding of faces predicts socially influenced judgments of facial attractiveness</article-title>. <source><italic toggle="yes">Soc Neurosci</italic></source> &#x000a0;<year>2015</year>;<volume>10</volume>:<fpage>624</fpage>&#x02013;<lpage>34</lpage>. doi: <pub-id pub-id-type="doi">10.1080/17470919.2015.1017113</pub-id><pub-id pub-id-type="pmid">25719443</pub-id>
</mixed-citation></ref><ref id="R72"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schupp</surname> &#x000a0;<given-names>HT</given-names></string-name>, <string-name><surname>Cuthbert</surname> &#x000a0;<given-names>BN</given-names></string-name>, <string-name><surname>Bradley</surname> &#x000a0;<given-names>MM</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Affective picture processing: the late positive potential is modulated by motivational relevance</article-title>. <source><italic toggle="yes">Psychophysiology</italic></source> &#x000a0;<year>2000</year>;<volume>37</volume>:<fpage>257</fpage>&#x02013;<lpage>61</lpage>. doi: <pub-id pub-id-type="doi">10.1017/S0048577200001530</pub-id><pub-id pub-id-type="pmid">10731776</pub-id>
</mixed-citation></ref><ref id="R73"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schupp</surname> &#x000a0;<given-names>HT</given-names></string-name>, <string-name><surname>Jungh&#x000f6;fer</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>&#x000d6;hman</surname> &#x000a0;<given-names>A</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The facilitated processing of threatening faces: an ERP analysis</article-title>. <source><italic toggle="yes">Emotion</italic></source> &#x000a0;<year>2004</year>;<volume>4</volume>:<fpage>189</fpage>&#x02013;<lpage>200</lpage>. doi: <pub-id pub-id-type="doi">10.1037/1528-3542.4.2.189</pub-id><pub-id pub-id-type="pmid">15222855</pub-id>
</mixed-citation></ref><ref id="R74"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schupp</surname> &#x000a0;<given-names>HT</given-names></string-name>, <string-name><surname>Jungh&#x000f6;fer</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Weike</surname> &#x000a0;<given-names>AI</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Emotional facilitation of sensory processing in the visual cortex</article-title>. <source><italic toggle="yes">Psychol Sci</italic></source> &#x000a0;<year>2003</year>;<volume>14</volume>:<fpage>7</fpage>&#x02013;<lpage>13</lpage>. doi: <pub-id pub-id-type="doi">10.1111/1467-9280.01411</pub-id><pub-id pub-id-type="pmid">12564747</pub-id>
</mixed-citation></ref><ref id="R75"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schupp</surname> &#x000a0;<given-names>HT</given-names></string-name>, <string-name><surname>Stockburger</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Codispoti</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Stimulus novelty and emotion perception: the near absence of habituation in the visual cortex</article-title>. <source><italic toggle="yes">NeuroReport</italic></source> &#x000a0;<year>2006</year>;<volume>17</volume>:<page-range>365</page-range>. doi: <pub-id pub-id-type="doi">10.1097/01.wnr.0000203355.88061.c6</pub-id></mixed-citation></ref><ref id="R76"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schupp</surname> &#x000a0;<given-names>HT</given-names></string-name>, <string-name><surname>Stockburger</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Codispoti</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Selective visual attention to emotion</article-title>. <source><italic toggle="yes">J Neurosci</italic></source> &#x000a0;<year>2007</year>;<volume>27</volume>:<fpage>1082</fpage>&#x02013;<lpage>89</lpage>. doi: <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3223-06.2007</pub-id><pub-id pub-id-type="pmid">17267562</pub-id>
</mixed-citation></ref><ref id="R77"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schwartz</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Yovel</surname> &#x000a0;<given-names>G</given-names></string-name></person-group>. <article-title>The roles of perceptual and conceptual information in face recognition</article-title>. <source><italic toggle="yes">J Exp Psychol Gen</italic></source> &#x000a0;<year>2016</year>;<volume>145</volume>:<fpage>1493</fpage>&#x02013;<lpage>511</lpage>. doi: <pub-id pub-id-type="doi">10.1037/xge0000220</pub-id><pub-id pub-id-type="pmid">27690515</pub-id>
</mixed-citation></ref><ref id="R78"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Suess</surname> &#x000a0;<given-names>F</given-names></string-name>, <string-name><surname>Rabovsky</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Abdel Rahman</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Perceiving emotions in neutral faces: expression processing is biased by affective person knowledge</article-title>. <source><italic toggle="yes">Soc Cognit Affective Neurosci</italic></source> &#x000a0;<year>2013</year>;<volume>10</volume>:<fpage>531</fpage>&#x02013;<lpage>36</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsu088</pub-id></mixed-citation></ref><ref id="R79"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Thierry</surname> &#x000a0;<given-names>G</given-names></string-name>, <string-name><surname>Martin</surname> &#x000a0;<given-names>CD</given-names></string-name>, <string-name><surname>Downing</surname> &#x000a0;<given-names>P</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Controlling for interstimulus perceptual variance abolishes N170 face selectivity</article-title>. <source><italic toggle="yes">Nat Neurosci</italic></source> &#x000a0;<year>2007</year>;<volume>10</volume>:<fpage>505</fpage>&#x02013;<lpage>11</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nn1864</pub-id><pub-id pub-id-type="pmid">17334361</pub-id>
</mixed-citation></ref><ref id="R80"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Thiruchselvam</surname> &#x000a0;<given-names>R</given-names></string-name>, <string-name><surname>Harper</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Homer</surname> &#x000a0;<given-names>AL</given-names></string-name></person-group>. <article-title>Beauty is in the belief of the beholder: cognitive influences on the neural response to facial attractiveness</article-title>. <source><italic toggle="yes">Soc Cognit Affective Neurosci</italic></source> &#x000a0;<year>2016</year>;<volume>11</volume>:<fpage>1999</fpage>&#x02013;<lpage>2008</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsw115</pub-id></mixed-citation></ref><ref id="R81"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Todorov</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Gobbini</surname> &#x000a0;<given-names>MI</given-names></string-name>, <string-name><surname>Evans</surname> &#x000a0;<given-names>KK</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Spontaneous retrieval of affective person knowledge in face perception</article-title>. <source><italic toggle="yes">Neuropsychologia</italic></source> &#x000a0;<year>2007</year>;<volume>45</volume>:<fpage>163</fpage>&#x02013;<lpage>73</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.04.018</pub-id><pub-id pub-id-type="pmid">16759672</pub-id>
</mixed-citation></ref><ref id="R82"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Todorov</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Olivola</surname> &#x000a0;<given-names>CY</given-names></string-name>, <string-name><surname>Dotsch</surname> &#x000a0;<given-names>R</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Social attributions from faces: determinants, consequences, accuracy, and functional significance</article-title>. <source><italic toggle="yes">Annu Rev Psychol</italic></source> &#x000a0;<year>2015</year>;<volume>66</volume>:<fpage>519</fpage>&#x02013;<lpage>45</lpage>. doi: <pub-id pub-id-type="doi">10.1146/annurev-psych-113011-143831</pub-id><pub-id pub-id-type="pmid">25196277</pub-id>
</mixed-citation></ref><ref id="R83"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Todorov</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Olson</surname> &#x000a0;<given-names>IR</given-names></string-name></person-group>. <article-title>Robust learning of affective trait associations with faces when the hippocampus is damaged, but not when the amygdala and temporal pole are damaged</article-title>. <source><italic toggle="yes">Soc Cognit Affective Neurosci</italic></source> &#x000a0;<year>2008</year>;<volume>3</volume>:<fpage>195</fpage>&#x02013;<lpage>203</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsn013</pub-id></mixed-citation></ref><ref id="R84"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Trautmann-Lengsfeld</surname> &#x000a0;<given-names>SA</given-names></string-name>, <string-name><surname>Dom&#x000ed;nguez-Borr&#x000e0;s</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Escera</surname> &#x000a0;<given-names>C</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>The perception of dynamic and static facial expressions of happiness and disgust investigated by ERPs and fMRI constrained source analysis</article-title>. <source><italic toggle="yes">PLOS One</italic></source> &#x000a0;<year>2013</year>;<volume>8</volume>:<page-range>e66997</page-range>. doi: <pub-id pub-id-type="doi">10.1371/journal.pone.0066997</pub-id></mixed-citation></ref><ref id="R85"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Trujillo</surname> &#x000a0;<given-names>LT</given-names></string-name>, <string-name><surname>Jankowitsch</surname> &#x000a0;<given-names>JM</given-names></string-name>, <string-name><surname>Langlois</surname> &#x000a0;<given-names>JH</given-names></string-name></person-group>. <article-title>Beauty is in the ease of the beholding: a neurophysiological test of the averageness theory of facial attractiveness</article-title>. <source><italic toggle="yes">Cognit Affect Behav Neurosci</italic></source> &#x000a0;<year>2014</year>;<volume>14</volume>:<fpage>1061</fpage>&#x02013;<lpage>76</lpage>. doi: <pub-id pub-id-type="doi">10.3758/s13415-013-0230-2</pub-id><pub-id pub-id-type="pmid">24326966</pub-id>
</mixed-citation></ref><ref id="R86"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Tsukiura</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Cabeza</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Remembering beauty: roles of orbitofrontal and hippocampal regions in successful memory encoding of attractive faces</article-title>. <source><italic toggle="yes">NeuroImage</italic></source> &#x000a0;<year>2011a</year>;<volume>54</volume>:<fpage>653</fpage>&#x02013;<lpage>60</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.046</pub-id><pub-id pub-id-type="pmid">20659568</pub-id>
</mixed-citation></ref><ref id="R87"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Tsukiura</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Cabeza</surname> &#x000a0;<given-names>R</given-names></string-name></person-group>. <article-title>Shared brain activity for aesthetic and moral judgments: implications for the beauty-is-good stereotype</article-title>. <source><italic toggle="yes">Soc Cognit Affective Neurosci</italic></source> &#x000a0;<year>2011b</year>;<volume>6</volume>:<fpage>138</fpage>&#x02013;<lpage>48</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsq025</pub-id></mixed-citation></ref><ref id="R88"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Uhlmann</surname> &#x000a0;<given-names>EL</given-names></string-name>, <string-name><surname>Pizarro</surname> &#x000a0;<given-names>DA</given-names></string-name>, <string-name><surname>Diermeier</surname> &#x000a0;<given-names>D</given-names></string-name></person-group>. <article-title>A person-centered approach to moral judgment</article-title>. <source><italic toggle="yes">Perspect Psychol Sci</italic></source> &#x000a0;<year>2015</year>;<volume>10</volume>:<fpage>72</fpage>&#x02013;<lpage>81</lpage>. doi: <pub-id pub-id-type="doi">10.1177/1745691614556679</pub-id><pub-id pub-id-type="pmid">25910382</pub-id>
</mixed-citation></ref><ref id="R89"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname> &#x000a0;<given-names>T</given-names></string-name>, <string-name><surname>Mo</surname> &#x000a0;<given-names>L</given-names></string-name>, <string-name><surname>Mo</surname> &#x000a0;<given-names>C</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Is moral beauty different from facial beauty? Evidence from an fMRI study</article-title>. <source><italic toggle="yes">Soc Cognit Affective Neurosci</italic></source> &#x000a0;<year>2014</year>;<volume>10</volume>:<fpage>814</fpage>&#x02013;<lpage>23</lpage>. doi: <pub-id pub-id-type="doi">10.1093/scan/nsu123</pub-id></mixed-citation></ref><ref id="R90"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wendt</surname> &#x000a0;<given-names>J</given-names></string-name>, <string-name><surname>Weymar</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Junge</surname> &#x000a0;<given-names>M</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Heartfelt memories: cardiac vagal tone correlates with increased memory for untrustworthy faces</article-title>. <source><italic toggle="yes">Emotion</italic></source> &#x000a0;<year>2019</year>;<volume>19</volume>:<fpage>178</fpage>&#x02013;<lpage>82</lpage>. doi: <pub-id pub-id-type="doi">10.1037/emo0000396</pub-id><pub-id pub-id-type="pmid">29553757</pub-id>
</mixed-citation></ref><ref id="R91"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Werheid</surname> &#x000a0;<given-names>K</given-names></string-name>, <string-name><surname>Schacht</surname> &#x000a0;<given-names>A</given-names></string-name>, <string-name><surname>Sommer</surname> &#x000a0;<given-names>W</given-names></string-name></person-group>. <article-title>Facial attractiveness modulates early and late event-related brain potentials</article-title>. <source><italic toggle="yes">Biol Psychol</italic></source> &#x000a0;<year>2007</year>;<volume>76</volume>:<fpage>100</fpage>&#x02013;<lpage>08</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.biopsycho.2007.06.008</pub-id><pub-id pub-id-type="pmid">17681418</pub-id>
</mixed-citation></ref><ref id="R92"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Weymar</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Ventura-Bort</surname> &#x000a0;<given-names>C</given-names></string-name>, <string-name><surname>Wendt</surname> &#x000a0;<given-names>J</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal> &#x000a0;<article-title>Behavioral and neural evidence of enhanced long-term memory for untrustworthy faces</article-title>. <source><italic toggle="yes">Sci Rep</italic></source> &#x000a0;<year>2019</year>;<volume>9</volume>:<page-range>19217</page-range>. doi: <pub-id pub-id-type="doi">10.1038/s41598-019-55705-7</pub-id></mixed-citation></ref><ref id="R93"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wiese</surname> &#x000a0;<given-names>H</given-names></string-name>, <string-name><surname>Altmann</surname> &#x000a0;<given-names>CS</given-names></string-name>, <string-name><surname>Schweinberger</surname> &#x000a0;<given-names>SR</given-names></string-name></person-group>. <article-title>Effects of attractiveness on face memory separated from distinctiveness: evidence from event-related brain potentials</article-title>. <source><italic toggle="yes">Neuropsychologia</italic></source> &#x000a0;<year>2014</year>;<volume>56</volume>:<fpage>26</fpage>&#x02013;<lpage>36</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2013.12.023</pub-id><pub-id pub-id-type="pmid">24406982</pub-id>
</mixed-citation></ref><ref id="R94"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Xu</surname> &#x000a0;<given-names>M</given-names></string-name>, <string-name><surname>Li</surname> &#x000a0;<given-names>Z</given-names></string-name>, <string-name><surname>Diao</surname> &#x000a0;<given-names>L</given-names></string-name></person-group> &#x000a0;<etal>et&#x000a0;al.</etal>` <article-title>Contextual valence and sociality jointly influence the early and later stages of neutral face processing</article-title>. <source><italic toggle="yes">Front Psychol</italic></source> &#x000a0;<year>2016</year>;<volume>7</volume>:<page-range>1446</page-range>. doi: <pub-id pub-id-type="doi">10.3389/fpsyg.2016.01258</pub-id></mixed-citation></ref><ref id="R95"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zebrowitz</surname> &#x000a0;<given-names>LA</given-names></string-name>, <string-name><surname>Montepare</surname> &#x000a0;<given-names>JM</given-names></string-name></person-group>. <article-title>Social psychological face perception: why appearance matters</article-title>. <source><italic toggle="yes">Soc Personal Psychol Compass</italic></source> &#x000a0;<year>2008</year>;<volume>2</volume>:<fpage>1497</fpage>&#x02013;<lpage>517</lpage>. doi: <pub-id pub-id-type="doi">10.1111/j.1751-9004.2008.00109.x</pub-id><pub-id pub-id-type="pmid">20107613</pub-id>
</mixed-citation></ref><ref id="R96"><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zhang</surname> &#x000a0;<given-names>Z</given-names></string-name>, <string-name><surname>Deng</surname> &#x000a0;<given-names>Z</given-names></string-name></person-group>. <article-title>Gender, facial attractiveness, and early and late event-related potential components</article-title>. <source><italic toggle="yes">J Integr Neurosci</italic></source> &#x000a0;<year>2012</year>;<volume>11</volume>:<fpage>477</fpage>&#x02013;<lpage>87</lpage>. doi: <pub-id pub-id-type="doi">10.1142/S0219635212500306</pub-id><pub-id pub-id-type="pmid">23351053</pub-id>
</mixed-citation></ref></ref-list></back></article>