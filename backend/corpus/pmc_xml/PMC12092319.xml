<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Health Inf Sci Syst</journal-id><journal-id journal-id-type="iso-abbrev">Health Inf Sci Syst</journal-id><journal-title-group><journal-title>Health Information Science and Systems</journal-title></journal-title-group><issn pub-type="epub">2047-2501</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40400660</article-id><article-id pub-id-type="pmc">PMC12092319</article-id><article-id pub-id-type="publisher-id">352</article-id><article-id pub-id-type="doi">10.1007/s13755-025-00352-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Cerebral ischemia detection using deep learning techniques</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4089-9538</contrib-id><name><surname>Pastor-Vargas</surname><given-names>Rafael</given-names></name><address><email>rpastor@scc.uned.es</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ant&#x000f3;n-Mun&#x000e1;rriz</surname><given-names>Cristina</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Haut</surname><given-names>Juan M.</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Robles-G&#x000f3;mez</surname><given-names>Antonio</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Paoletti</surname><given-names>Mercedes E.</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ben&#x000ed;tez-Andrades</surname><given-names>Jos&#x000e9; Alberto</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02msb5n36</institution-id><institution-id institution-id-type="GRID">grid.10702.34</institution-id><institution-id institution-id-type="ISNI">0000 0001 2308 8920</institution-id><institution>Communications and Control Systems, </institution><institution>Computer Engineering Science Faculty, UNED, </institution></institution-wrap>Calle Juan del Rosal 16, 28040 Madrid, Spain </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03phm3r45</institution-id><institution-id institution-id-type="GRID">grid.411730.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2191 685X</institution-id><institution>Radiology area, </institution><institution>Hospital Universitario de Navarra, </institution></institution-wrap>Navarra, Spain </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0174shg90</institution-id><institution-id institution-id-type="GRID">grid.8393.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 1941 2521</institution-id><institution>School of Technology, </institution><institution>University of Extremadura, </institution></institution-wrap>C&#x000e1;ceres, Extremadura Spain </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02tzt0b78</institution-id><institution-id institution-id-type="GRID">grid.4807.b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2187 3167</institution-id><institution>ALBA Research Group, Department of Electric, Systems and Automatics Engineering, </institution><institution>University of Leon, </institution></institution-wrap>Campus of Vegazana s/n, 24071 Le&#x000f3;n, Castilla y Le&#x000f3;n Spain </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2025</year></pub-date><volume>13</volume><issue>1</issue><elocation-id>36</elocation-id><history><date date-type="received"><day>12</day><month>1</month><year>2024</year></date><date date-type="accepted"><day>29</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Cerebrovascular accident (CVA), commonly known as stroke, stands as a significant contributor to contemporary mortality and morbidity rates, often leading to lasting disabilities. Early identification is crucial in mitigating its impact and reducing mortality. Non-contrast computed tomography (NCCT) remains the primary diagnostic tool in stroke emergencies due to its speed, accessibility, and cost-effectiveness. NCCT enables the exclusion of hemorrhage and directs attention to ischemic causes resulting from arterial flow obstruction. Quantification of NCCT findings employs the Alberta Stroke Program Early Computed Tomography Score (ASPECTS), which evaluates affected brain structures. This study seeks to identify early alterations in NCCT density in patients with stroke symptoms using a binary classifier distinguishing NCCT scans with and without stroke. To achieve this, various well-known deep learning architectures, namely VGG3D, ResNet3D, and DenseNet3D, validated in the ImageNet challenges, are implemented with 3D images covering the entire brain volume. The training results of these networks are presented, wherein diverse parameters are examined for optimal performance. The DenseNet3D network emerges as the most effective model, attaining a training set accuracy of 98% and a test set accuracy of 95%. The aim is to alert medical professionals to potential stroke cases in their early stages based on NCCT findings displaying altered density patterns.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Cerebral ischemia</kwd><kwd>Computed tomography</kwd><kwd>Deep learning</kwd><kwd>Transfer learning</kwd><kwd>Ictus dataset</kwd></kwd-group><funding-group><award-group><funding-source><institution>Universidad Nacional de Educacion Distancia (UNED)</institution></funding-source></award-group><open-access><p>Open Access funding provided thanks to the CRUE-CSIC agreement with Springer Nature.</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Switzerland AG 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">An obstruction of a cerebral artery by a thrombus or embolus can lead to a lack of blood flow in the irrigated territory of the vessel. Consequently, it results in cellular ischemia, leading to cell death if vascular obstruction persists [<xref ref-type="bibr" rid="CR1">1</xref>]. The ischemia can be reverted in the initial hours if it is detected on time. If not detected in time, this obstruction can produce infarction or neuronal death. Typically, the anterior circulation is the most frequent location of ischemic stroke in the circulation. In particular, internal carotid, anterior and middle cerebral arteries (MCA) are located in this area, being the obstruction of the MCA the more frequent location for the embolus/thrombus. To avoid the irreversible problems discussed above, once the initial symptoms are detected, it is vital to treat intravenous thrombosis within 4&#x02013;6&#x000a0;h. After this time, hemorrhagic complications can occur, leading to neuronal death [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>].</p><p id="Par3">To assess early signs of middle cerebral artery (MCA) involvement, the Alberta Stroke Program Early Computed Tomography Score (ASPECTS) is utilized. This scoring system ranges from 0 to 10, based on the presence of hypodensity in ten brain regions: the lenticular nucleus, insula, caudate nucleus, internal capsule, and six cortical areas [<xref ref-type="bibr" rid="CR4">4</xref>]. A score of 10 on the ASPECTS scale indicates no signs of early ischemia. A score below 7 indicates a worse prognosis and an increased risk of hemorrhage as a complication of thrombolytic therapy. Our dataset includes NCCTs with hypodensity in one or more territories, as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Fig. 1</label><caption><p>ASPECTS areas: Insula (I), Lenticular (L), Caudate (C), Capsule Internal (CI) and cortical areas: M1, M2, M3, M4, M5 and M6</p></caption><graphic xlink:href="13755_2025_352_Fig1_HTML" id="MO1"/></fig></p><p id="Par4">The diagnosis of hypodensities is a challenge that relies heavily on the expertise of the radiologist/physician in using and receiving hypodensities from an NCCT. Hypodensities are very faint, making it difficult to observe the areas where they appear accurately. Since these changes in the hypodensities are not very noticeable, it is necessary to use more advanced techniques such as deep neural networks or deep learning. These networks are characterized by looking for visual patterns in the images they analyze, which are an excellent aid for radiologists/physicians if appropriately used. The neural networks need to be trained to trust in their decisions, and once the correct metrics are established, they can be of great value in detecting the presence of an infraction.</p><p id="Par5">Medical imaging is one of the most widely used areas of neural networks, particularly Convolutional Neural Networks (CNNs). These networks can extract information from the image, such as non-linear features not visible to the human eye. The use of these CNNs and their application in the diagnosis of diseases is booming, especially among researchers in the field of medical informatics. The fields of application are very broad, given the number of existing mechanisms for obtaining images (or equivalent representations) associated with the diagnostic problem.</p><p id="Par6">For the particular case of stroke, 3D images are used. 3D neural networks are an area of development that is being used in contrast to neural networks that use two-dimensional images. The ability to discover in three dimensions makes it possible to obtain patterns that traditional 2D networks cannot learn. However, 3D networks require much more processing power, so it is necessary to use parallel processing technologies and more concerted use of graphics processing units or GPUs. The use of these 3D models has provided specific tools for diagnosis by segmenting and classifying medical images of the specific domain [<xref ref-type="bibr" rid="CR2">2</xref>].</p><p id="Par7">Other artificial intelligence algorithms, such as supervised machine learning algorithms, are already being used. Applications such as e-ASPECTS use these algorithms that are more limited than neural networks. e-ASPECTS uses the random forest algorithm to compare healthy hemispheres with those affected [<xref ref-type="bibr" rid="CR5">5</xref>&#x02013;<xref ref-type="bibr" rid="CR7">7</xref>]. The segmentation of the images must be done manually, and in this case, it must be performed by an experienced radiologist to have acceptable results. Given the limitations of random forests, this work chooses to use neural networks since CNNs are being used in the area of medical imaging for stroke diagnosis using Computed Tomography (CT) as in [<xref ref-type="bibr" rid="CR8">8</xref>], [<xref ref-type="bibr" rid="CR9">9</xref>] and [<xref ref-type="bibr" rid="CR10">10</xref>]. An extensive review of deep learning algorithms for CT images can be found in [<xref ref-type="bibr" rid="CR11">11</xref>]. Other works focus on the application of CCNs using MRI (Magnetic Resonance Imaging), like the one presented in [<xref ref-type="bibr" rid="CR12">12</xref>].</p><p id="Par8">For this paper, the research will not use CT or MRI but rather NCCT images. There are plenty of works on CNNs using CT/MRI images but fewer using NCCT images. Still, given the good results of CNNs with CT images, the research hypothesis is to obtain similar results with NCCT images. This hypothesis can be corroborated by experimental results, primarily through the application of deep learning transfer techniques.</p><p id="Par9">The rest of the paper is organized as follows. Section&#x000a0;<xref rid="Sec2" ref-type="sec">2</xref> presents the importance of the use of convolutional neural networks in the area of medical imaging. This section focuses on using these tools to assist physicians/radiologists in their decisions. Section&#x000a0;<xref rid="Sec6" ref-type="sec">3</xref> indicates how the data set used for network training has been constructed. This data set does not exist in recent academic literature or the data repositories analyzed. For this reason, the construction procedure is shown for reproduction if necessary. Section&#x000a0;<xref rid="Sec11" ref-type="sec">4</xref> details the experiments developed and the data obtained for the defined metrics in order to be used to compare the different results. Section&#x000a0;<xref rid="Sec17" ref-type="sec">5</xref> discusses the results and the factors used to decide on the neural network for the decision assistance process. Finally, in Sect.&#x000a0;<xref rid="Sec18" ref-type="sec">6</xref>, the conclusions obtained and future work are shown.</p></sec><sec id="Sec2"><title>Deep learning and medical images</title><p id="Par10">Since its inception in 1943 by McCulloch and Pitt [<xref ref-type="bibr" rid="CR13">13</xref>], the field of Artificial Neural Networks (ANN) has witnessed a significant transformation, particularly notable in recent years. The seminal work by McCulloch and Pitt introduced a mathematical framework for ANNs, emphasizing the binary nature of neuronal activity and their representation through a threshold function. Neural networks, particularly through the utilization of non-linear activation functions in hidden layers, offer a robust mechanism for modeling complex, non-linear relationships inherent in the training data, a task that is challenging for traditional algorithms. Among the various architectures developed, CNNs, cited in [<xref ref-type="bibr" rid="CR14">14</xref>], have gained prominence in tasks involving image data, such as in the field of medical imaging. The rapid advancements in deep learning, aligned with breakthroughs in biological and medical imaging technologies, have led to an unprecedented ability to generate, collect, and analyze voluminous datasets of medical images, thereby enhancing the capability of CNNs in feature detection and pattern analysis.</p><sec id="Sec3"><title>CCN models</title><p id="Par11">Numerous established applications in the fields of medicine and biology have successfully utilized CNNs for tasks such as image classification and segmentation, similar to the approach we have taken. These networks are instrumental across a diverse array of challenges including, but not limited to, cancer detection, continual disease monitoring, the generation of customized treatment plans, and accurate disease diagnosis. Furthermore, CNNs have demonstrated versatility in handling a variety of data sources including but not limited to X-rays, computed tomography (CT) scans, magnetic resonance imaging (MRI), retinography, pathological anatomy slides, and even sequences from the human genome, as cited in [<xref ref-type="bibr" rid="CR15">15</xref>].</p><p id="Par12">The subsequent examples represent merely a fraction of the applications employing CNNs in medical imaging, showcasing the breadth of ongoing research in this expansive field. These applications illustrate the potential of CNNs to classify a multitude of diseases, employ various imaging modalities, and utilize a diverse range of convolutional network architectures. In the realms of lung nodule classification, detection, and segmentation using CT scans, various deep learning approaches have been employed. These include 2D and 3D CNNs, various architectural designs, and often incorporate autoencoder structures. Such techniques have yielded diagnostic accuracies ranging from 84&#x02013;95%, as reported in several studies [<xref ref-type="bibr" rid="CR16">16</xref>&#x02013;<xref ref-type="bibr" rid="CR21">21</xref>]. Additionally, the MICCAI-BRATS (Brain Tumor Segmentation Challenge) annually conducts competitions for brain tumor segmentation, with outcomes and winners detailed on its website [<xref ref-type="bibr" rid="CR22">22</xref>]. These challenges predominantly use MRI data and have led to significant developments, including a 3D DenseNet neural network aimed at determining the IDH genotype of gliomas, with an accuracy of 84.6% [<xref ref-type="bibr" rid="CR23">23</xref>].</p><p id="Par13">2D and 3D CNNs have been instrumental in classifying Alzheimer&#x02019;s disease using MR imaging, as evidenced by various studies [<xref ref-type="bibr" rid="CR24">24</xref>&#x02013;<xref ref-type="bibr" rid="CR26">26</xref>].</p><p id="Par14">ISLES (Ischemic Stroke Lesion Segmentation) initiative regularly offers challenges aimed at the segmentation of ischemic brain lesions, predominantly utilizing MR imaging as noted in [<xref ref-type="bibr" rid="CR27">27</xref>], with the latest challenges accessible on their website [<xref ref-type="bibr" rid="CR28">28</xref>]. Current research on machine learning for diagnosing acute cerebral ischemic lesions in NCCT primarily focuses on techniques analyzing brain hemisphere symmetry [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>], segmentation with generative networks in contrast-enhanced CT [<xref ref-type="bibr" rid="CR31">31</xref>], and to a lesser extent, employs 2D [<xref ref-type="bibr" rid="CR32">32</xref>] and 3D [<xref ref-type="bibr" rid="CR2">2</xref>] CNNs. Regarding stroke analysis, several studies have implemented various artificial intelligence algorithms for the classification, segmentation, and diagnosis of strokes in NCCT and AngiographyCT images [<xref ref-type="bibr" rid="CR5">5</xref>&#x02013;<xref ref-type="bibr" rid="CR7">7</xref>]. Mokli et al. [<xref ref-type="bibr" rid="CR33">33</xref>] have conducted a review of the market&#x02019;s available applications that utilize automatic and semi-automatic algorithms for image analysis in diagnosing acute cerebral infarction. Nevertheless, detailed information about the technical aspects of these algorithms, as well as specifics on the training and validation data, is often scant in the general descriptions provided on the applications&#x02019; websites [<xref ref-type="bibr" rid="CR34">34</xref>].</p></sec><sec id="Sec4"><title>Goodness metrics</title><p id="Par15">The metrics used to evaluate the performance of the networks were the <italic>accuracy</italic> and the <italic>confusion matrix</italic>. These are standard measures to assess the performance of any classifier, so they can be used to compare other alternatives and validate the efficiency of the solutions presented.</p><p id="Par16">The accuracy definition is quite simple:<disp-formula id="Equ1"><alternatives><tex-math id="d33e398">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$accuracy = \frac{true preditions}{total samples}$$\end{document}</tex-math><mml:math id="d33e403" display="block"><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">truepreditions</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">totalsamples</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="13755_2025_352_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par17">However, this metric alone does not allow for a more complete algorithm evaluation. A better way to evaluate the performance of a classifier is to use the confusion matrix. This matrix allows other concepts to be assessed, as will be seen below. To calculate the confusion matrix, a set of predictions is made with the network in question, with the test/evaluation set. Each row of the confusion matrix represents the actual class (positive/negative), and each column represents the class generated by the model (positive/negative). Some metrics from the confusion matrix can be used:<list list-type="bullet"><list-item><p id="Par18">True positives: the model predicts a value as true, and it really is true.</p></list-item><list-item><p id="Par19">True negatives: the model predicts a value as false, and it really is false.</p></list-item><list-item><p id="Par20">False positives: the model predicts a value as true, and it really is false.</p></list-item><list-item><p id="Par21">False negatives: the model predicts a value as false, and it really is true.</p></list-item></list>From these definitions, two additional metrics can be considered: precision and sensitivity.<disp-formula id="Equ2"><alternatives><tex-math id="d33e438">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$precision = \frac{true \ positives}{true \ positives\ + \ false \ positives}$$\end{document}</tex-math><mml:math id="d33e443" display="block"><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="4pt"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="4pt"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="4pt"/><mml:mo>+</mml:mo><mml:mspace width="4pt"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="4pt"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="13755_2025_352_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par22">The sensitivity or true positive rate (recall) is the proportion of positive examples that are correctly detected by the classifier.<disp-formula id="Equ3"><alternatives><tex-math id="d33e510">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$recall = \frac{true \ positives}{true \ positives\ + \ false \ negatives}$$\end{document}</tex-math><mml:math id="d33e515" display="block"><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="4pt"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="4pt"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="4pt"/><mml:mo>+</mml:mo><mml:mspace width="4pt"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="4pt"/><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="13755_2025_352_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par23">Depending on the problem being considered, it can be of interest to minimizing false positive cases, in which case precision could be used. On the other hand, sensitivity is the adequate metric if interest is focused on reducing false negative cases. In this specific case of stroke detection, it is very important to minimize false negatives which correspond to an incorrect stroke diagnosis. In other words, special attention to sensitivity must be considered. Unfortunately, improving on both metrics simultaneously is impossible, as increasing sensitivity reduces accuracy and vice-versa. Two other metrics from the confusion matrix can be deduced: <italic>specificity</italic> and the so-called <italic>F1-score</italic>, i.e., the weighted harmonic mean of accuracy and sensitivity.<disp-formula id="Equ4"><alternatives><tex-math id="d33e585">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$specificity = \frac{true \ negatives}{true \ negatives\ + \ false \ positives}$$\end{document}</tex-math><mml:math id="d33e590" display="block"><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="4pt"/><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="4pt"/><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="4pt"/><mml:mo>+</mml:mo><mml:mspace width="4pt"/><mml:mi>f</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="4pt"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="13755_2025_352_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><alternatives><tex-math id="d33e658">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F1-score = 2 * \frac{precision * recall}{precision + recall}$$\end{document}</tex-math><mml:math id="d33e663" display="block"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mrow/><mml:mo>&#x02217;</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow/><mml:mo>&#x02217;</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="13755_2025_352_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par24">F1-score can be used as a valid reference metric as it weights the rest of the metrics. In this case, an excellent performance is considered when the value is 1 (or close to it) and a very poor performance when it is close to 0.</p></sec><sec id="Sec5"><title>Existing solutions</title><p id="Par25">To validate the work proposal, it is also necessary not only to analyze the performance of the developed models but also the solutions that are presented in order to validate the results obtained. These solutions are presented here.</p><p id="Par26">Presently, there are five leading software platforms that incorporate machine learning algorithms for stroke detection, including Brainomix e-ASPECTS (Oxford, UK), Olea Medical (La Ciotat, France), Siemens Frontier (Erlangen, Germany), iSchemaView RAPID (California, USA), and Viz.ai (California, USA) [<xref ref-type="bibr" rid="CR35">35</xref>].</p><p id="Par27">Viz.ai specifically utilizes a convolutional neural network for the analysis of head and neck CT angiography to detect occlusions in major vessels of the anterior circulation [<xref ref-type="bibr" rid="CR36">36</xref>].</p><p id="Par28">Both e-ASPECTS and RAPID ASPECTS platforms are clinically certified for stroke diagnosis and segmentation in NCCT, with e-ASPECTS boasting the most validation studies and comparability with expert radiologists&#x02019; performance [<xref ref-type="bibr" rid="CR33">33</xref>]. As medical software, these platforms fall under the category of medical devices and are subject to validation and certification by regulatory bodies such as the US FDA (Food and Drug Administration) and the EU&#x02019;s MDR (Medical Device Regulation).</p><p id="Par29">Frontier ASPECTS is not yet certified, as documented [<xref ref-type="bibr" rid="CR37">37</xref>]. Comparative studies reveal that while e-ASPECTS demonstrates high agreement with expert evaluations, Frontier&#x02019;s performance aligns moderately [<xref ref-type="bibr" rid="CR38">38</xref>].</p><p id="Par30">E-ASPECTS and RAPID ASPECTS both endeavor to quantitatively evaluate focal ischemic damage utilizing the ASPECTS scoring system. They process complete cranial CT scans in DICOM format and use heat maps to demarcate affected areas. The distinction lies in their respective machine learning methodologies: Brainomix employs random forest learning for classification and segmentation, whereas RAPID ASPECTS involves skull and cerebrospinal fluid removal, uses atlases to identify the 20 ASPECTS score regions, and implements random forest learning for classification and segmentation.</p><p id="Par31">These solutions require a previous segmentation in all cases to diagnose and classify ischemic stroke. This classification must be carried out in a single computer algorithm capable of being classified based only on NCCT images.</p><p id="Par32">NCCT scans often suffer from low contrast and noise, making detection challenging. As in the case of previously presented solutions, some works focus on the segmentation of NCCT images, as presented in [<xref ref-type="bibr" rid="CR39">39</xref>] and [<xref ref-type="bibr" rid="CR40">40</xref>], with results similar to those achieved by neuroradiologists [<xref ref-type="bibr" rid="CR41">41</xref>]. In fact, there are recent developments focused on using a combination of Transformers with CCNs [<xref ref-type="bibr" rid="CR42">42</xref>]. Again, it focuses on automatic image segmentation but not image detection. A similar work is presented in [<xref ref-type="bibr" rid="CR43">43</xref>] using a combination of NCCT and CTA (CT Angiography) images) to train a multi-scale 3D CNN. The work shows that if NCCT imaging alone is used, the accuracy achieved is about 53% (0.53&#x000b1;0.09), which is a very far result from what is expected (near 100%). Only by combining NCCT and CTA images accuracies of 90% (0.90&#x000b1;0.04) are achieved. [<xref ref-type="bibr" rid="CR44">44</xref>] uses a combination of NCCT images with Computer Tomography Perfusion (CTP) images. A pair of NCCT-CTP images is used for each case studied. The proposed ensemble model leverages three deep Convolutional Neural Networks (CNNs) to handle three end-to-end feature maps and handcrafted features defined by distinct contra-lateral properties. The results obtained are promising, with 91.16% accuracy, but define three CCN networks&#x02019; complex structure. In the above works, a combination of different types of images is used, but none exclusively use NCCT images.</p><p id="Par33">This research provides an initial requirement to use only NCCT images with signs of early ischemia that can be analyzed with neural networks, as segmentation techniques do with the ASPECTS score [<xref ref-type="bibr" rid="CR45">45</xref>].</p><p id="Par34">Under this assumption, other works use only NCCT images. In [<xref ref-type="bibr" rid="CR46">46</xref>] a two-stage CCN model is trained, which effectively identifies and locates invisible ischemic strokes in non-contrast CT images. The model achieved up to 91.89% identification accuracy for ischemic stroke on NCCT images. In [<xref ref-type="bibr" rid="CR47">47</xref>] another CCN is used which effectively detects and segments a specific type of thrombus (intra-arterial thrombi) using non-contrast computed tomography scans. The CCN has similar sensitivity and specificity to expert neuroradiologists (the paper compares CNN results with two radiologists&#x02019; decisions), achieving 0.86 sensitivity and 0.65 specificity in thrombus detection. CCN accuracy is not calculated, but it can be computed, resulting in a value of 85%, considering a prevalence size of 0.99 (very high value).</p><p id="Par35">A more general ischemic stroke detector is presented in [<xref ref-type="bibr" rid="CR48">48</xref>] The CNN model achieved a success rate of over 90% in distinguishing ischemic stroke cases from healthy controls in medical images. In all the papers presented, the accuracy is below 92%. This fact can define another research question: pre-trained networks can achieve better accuracy in detecting ischemic stroke by directly classifying NCCT images without previous segmentation.</p><p id="Par36">In the case of the use of pre-trained architectures, architectures such as YOLO have been tested [<xref ref-type="bibr" rid="CR49">49</xref>]. YOLO algorithm is used in object identification tasks. The AC-YOLOv5 algorithm, combining adaptive local region contrast enhancement and YOLOv5, effectively detects ischemic stroke in NCCt images with high accuracy and recall rates. Algorithm metrics have a high accuracy (91.7%) and recall (88.6%) rate. Again, accuracy metrics is under 92%.</p><p id="Par37">Within the specific works using pre-trained networks, only this one has been found [<xref ref-type="bibr" rid="CR50">50</xref>]. This research paper proposed a U-Net model based on the VGG-16 backbone, effectively detecting and segmenting infarct core areas in NCCT scans of ischemic stroke patients. Similarity indices such as the Dice coefficient and IoU are used as metrics and explicitly used in segmentation rather than classification. The U-Net model achieves an impressive Intersection over Union (IoU) score of 0.76 and a Dice coefficient of 0.79.</p><p id="Par38">After analyzing the research in the area of ischemic stroke detection using NCCT images, it can be concluded that the use of pre-trained networks is not widespread and that the accuracies obtained by CNN networks do not exceed 92% in the case of non-pre-trained architectures. Therefore, this paper aims to show that it is possible to answer the research question formulated above: pre-trained networks can achieve better accuracy in detecting ischemic stroke by directly classifying NCCT images without previous segmentation.</p><p id="Par39">An additional objective is to propose an Open CNN model that aids radiologists in decision-making, given the proprietary nature or non-utilization of neural network techniques in these applications&#x02019; algorithms. The attainment of this aim necessitates a specific Dataset, details of which will be delineated in the following section.</p></sec></sec><sec id="Sec6"><title>Ictus (stroke) dataset</title><p id="Par40">AI algorithms require high-quality, often pre-labeled data (with and without stroke, i.e., with and without hypodensities in the NCCT) to be effective. A critical preliminary step is to determine if there are specific datasets available for this research. A search for potential datasets in the area of stroke screening was conducted, and two possible sources were identified. The website [<xref ref-type="bibr" rid="CR51">51</xref>] served as the primary source. This site hosts a vast collection of challenges in the realm of medical imaging. Two potential datasets were discovered. The first is related to the segmentation of cerebral infarction in CT angiography and magnetic resonance imaging, initiated by ISLES [<xref ref-type="bibr" rid="CR28">28</xref>] in 2018. The second involves intracranial hemorrhage in non-contrast CT images from Kaggle [<xref ref-type="bibr" rid="CR52">52</xref>], introduced in 2019. These datasets were unsuitable for evaluating early ischemia signs for several reasons. Some were CT scans with contrast (CT angiography), while others used different techniques, such as MRI. Additionally, some of the cases involved hemorrhagic strokes rather than ischemic strokes, which are the primary focus of this research. Consequently, it was decided to develop a specific dataset. To do so, understanding the structure and format of CT-acquired images is necessary.</p><sec id="Sec7"><title>Computed tomography (CT) and hounsfiel units (HU)</title><p id="Par41">In order to train neural networks, a dataset containing 3D images is required. These images are derived from cranial CT scans and involve the conversion of radiation intensity measurements from these scans (which can vary depending on the measuring equipment) into &#x0201c;Hounsfield&#x0201d; units or attenuation coefficient numbers. These units facilitate the creation of a density scale that is applicable to any image obtained with any equipment. To develop this scale, the attenuation produced by water under standard conditions of temperature and pressure on a ray beam was used as a reference, assigning it a value of zero (0 HU). The attenuation value of air under the same conditions is defined as &#x02212;1000 HU, extending up to +1000 HU for dense and attenuated tissues.</p><p id="Par42">Therefore, the Hounsfield scale has 2000 units with different shades, while the human eye cannot distinguish more than 40 and therefore cannot visualize much of the information. Only a partial scale of the HU values obtained in a CT study, a window, is represented to avoid this limitation. Using windows makes it possible to extract and display only part of the information obtained, which is interesting in each anatomical region, to visualize the organ or tissue to be studied.</p><p id="Par43">These windows are defined by two parameters: their center or window level (WL), which corresponds to the middle grey of the visual scale and is assigned according to the density of the tissue to be evaluated, and their width (WW), which varies the range of greys on the visual scale. These parameters are different depending on the anatomical structures to be studied. Wide windows give less contrast. Narrow windows are used to study structures with close densities. The alteration in density that occurs in the early phase of infarction is usually very faint, which is why the radiologist improves visualization by reducing the window width to better distinguish diseased tissue from healthy tissue. For example, the center of the window is approximately &#x02212;200UH (Hounsfield unit) and the window&#x02019;s width is +1500. This window is called the pulmonary window. Other windows are the mediastinal window (WL=+50, WW=+350), the abdominal window (WL=+35, WW=+300), the bone window (WL=+300, WW=+1500) or the cerebral window, which is the one we use, which has WL=+40 and WW=+80, which allows us to differentiate the cerebral structures well thanks to the high contrast it provides. Thus, the anatomical structures will be represented by shades of grey, corresponding to their radiological density, measured in Hounsfield units. This enables the creation of both 3D images and 2D images (axial slices of the 3D image). The structure of the 3D image and the associated 2D images is illustrated in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Fig. 2</label><caption><p>CT-image characteristics</p></caption><graphic xlink:href="13755_2025_352_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec8"><title>Dataset structure</title><p id="Par44">Acquiring a dataset of labeled cranial CT scans, both with and without stroke, has been a time-consuming and complex endeavor, necessitating several progressive steps. Image scans from various diagnostic modalities, such as tomography, ultrasound, magnetic resonance, and retinography, are typically in DICOM format [<xref ref-type="bibr" rid="CR53">53</xref>]. This standard format for clinical image exchange and display includes not just the image but also personal details of the patient, their medical record number, and information about the equipment and location where the image was captured. In line with data protection laws, authorization was obtained from the leaders of the digital imaging department at the hospital for storing the CT studies in DICOM format. Once this consent was secured, the appropriate software was provided to facilitate the acquisition of CT analyzes in an anonymized manner.</p><sec id="Sec9"><title>Data collection</title><p id="Par45">The non-contrast CT dataset in DICOM format comprises images from various hospitalized stroke patients between June 2015 and September 2020, during which the ictus code was activated. Studies indicating hemorrhage or other conditions not classifiable as ischemic stroke were excluded. These studies are categorized by two radiologists as normal (without hypodensities) or showing signs of ischemia (with hypodensity in one or more territories identified by the ASPECTS). The age of the patients and the degree of extension of the hypodensity are not considered, so if these parameters were taken into account, there could be differences in the results. The data selection, therefore, is sufficiently general to avoid possible biases, except for the patient&#x02019;s sex (another possible feature that is not included). Of course, biases due to possible human error in the labeling of studies must always be taken into account.</p><p id="Par46">The data were divided into two groups: group 0, with stroke, and group 1, without stroke. All patients in group 0 exhibited some level of ischemic involvement in the MCA territory due to direct MCA obstruction by thrombus or embolus or secondary to carotid artery obstruction and showed hypodensity in CT according to ASPECTS criteria. The dataset records each patient&#x02019;s identification, whether they belong to group 0 or 1, the affected regions, ASPECTS score (as shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>), and the affected cerebral hemisphere. Patients are clinically monitored, and complementary studies with cranial MRI are performed to confirm the accurate categorization of the tests in each cluster. Finally, the images are acquired in an anonymized DICOM format and securely stored in the cloud using Google Cloud Storage. The images are captured at a thickness of 3&#x000a0;mm using 64-slice multidetector CT equipment. Some older, isolated studies are captured using a helical CT with 5&#x000a0;mm slices.<fig id="Fig3"><label>Fig. 3</label><caption><p>View of ASPECTS punctuation&#x02019;s areas, detailing an ischemia in M2 and L areas</p></caption><graphic xlink:href="13755_2025_352_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>Comparison between original and skull deleted CTs</p></caption><graphic xlink:href="13755_2025_352_Fig4_HTML" id="MO4"/></fig></p></sec><sec id="Sec10"><title>Processing data</title><p id="Par47">The raw dataset is stored in DICOM format. Each CT study contains several axial slices of around 100, covering the whole brain volume with slices of 3&#x000a0;mm thickness. Data processing involves transforming this original data into matrices ready for the deep learning algorithm (python numpy format). Initially, the pixel values from the 2D image set contained in the DICOM format are read. As it was mentioned before, the numerical values in each pixel reflect the amount of energy absorbed by the tissues that the X-ray beam passes through. These values, together with the calibration values of the equipment (slope and intercept values), can be obtained from the DICOM file using the Pydicom library. This functionality allows the numerical values of each pixel to be transformed into Houndsfield units. Once we have obtained the 2D matrix of pixels of all the brain slices in HU, images are stacked to build the 3D matrix needed for the Deep Learning algorithms.</p><p id="Par48">Two additional processing transformations have been used to improve the amount and quality of data. First, the bone was removed, and then data augmentation was applied. Data augmentation using rotations and translations of the original images has been performed considering the difficulty of obtaining many labeled medical images. Rotation (10 degrees to the right and 10 degrees to the left) and translation (10 pixels along the x-axis and 10 pixels along the y-axis) operations were performed to augment the data.</p><p id="Par49">It was decided to remove the outline of the cranial bone to improve the quality of the images and eliminate elements not necessary for the detection of ischemia. It can be seen in the images that the intensity used by the skull (of a white tone) can interfere with the pattern recognition in the images, so it is necessary to try to reduce or eliminate this surface. There are different brain extraction algorithms such as <italic>FSL-BET</italic>, <italic>3DSkullStrip</italic>, <italic>BrainSuite BSE</italic>, <italic>BEsST</italic> or <italic>ROBEX</italic>.</p><p id="Par50">For cranial bone removal, there is software for image segmentation such as <italic>itk</italic> or <italic>3DSlicer</italic> based on some of the algorithms described and others such as region growing, mask creation and other techniques. [<xref ref-type="bibr" rid="CR54">54</xref>]. The <italic>3DSlicer</italic> software was used for the removal of the skull bone because of its free availability and ease of use. 3DSlicer provides the <italic>SwissSkullStrip</italic> algorithm [<xref ref-type="bibr" rid="CR55">55</xref>], which removes cranial bone structures and eyes, and which we chose for two reasons: the resulting image maintains the DICOM format and does not modify the densities in the brain parenchyma image. The ResNet and DenseNet networks trained without bone removal failed to exceed an accuracy of 70&#x02013;75% at best, respectively, rising to 75&#x02013;80% after removal. Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> shows an example of applying the deletion of the cranial bone.</p><p id="Par51">Once the raw data is processed, a dataset of 264 stroke cases and 264 non-stroke cases has been created. Each of them is a 3D image, which can be divided into 2D axial slices. Each image has a sufficient diagnostic resolution (256 <inline-formula id="IEq1"><alternatives><tex-math id="d33e914">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="d33e919"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="13755_2025_352_Article_IEq1.gif"/></alternatives></inline-formula> 256 <inline-formula id="IEq2"><alternatives><tex-math id="d33e923">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="d33e928"><mml:mo>&#x000d7;</mml:mo></mml:math><inline-graphic xlink:href="13755_2025_352_Article_IEq1.gif"/></alternatives></inline-formula> 80 pixels).</p><p id="Par52">Finally, since the number of images is not too large, classical data augmentation techniques (rotation, translation, among others) were implemented [<xref ref-type="bibr" rid="CR56">56</xref>]. These operations resulted in a dataset with 1584 3D images covering the brain volume, 50% labeled with and 50% without stroke.</p></sec></sec></sec><sec id="Sec11"><title>Experimentation</title><p id="Par53">The general methodology of the study follows a trial-and-error strategy based initially on parameters and hyperparameters from previous studies that have obtained good results in 2D image recognition. We then modify several key hyperparameters in our networks, one by one, selecting those that give us the best accuracy on the test data during training and discarding those with inferior results. First, we test the performance enhancements of our networks by increasing the batch size. Since these are networks with many parameters, they require significant processing power, and we have tested batch sizes up to the maximum that our resources allow. The literature suggests better results using mini-batches with sizes between 2 and 32 [<xref ref-type="bibr" rid="CR57">57</xref>]. Subsequently, we assess the loss function and optimizers that work best with our data are evaluated, and finally, tests are carried out with different learning rates, both with and without reduction over time [<xref ref-type="bibr" rid="CR58">58</xref>].</p><p id="Par54">Different tests have been carried out to develop the experimentation, starting from simple architectures and gradually modifying these neural networks along with different parameters and hyper-parameters. A set of 3D deep convolutional neural networks based on well-known architectures with proven effectiveness in classifying 2D images from the Image-Net contest were implemented. The TensorFlow library was used for implementation, and the networks selected for their performance were VGG 3D, ResNet 3D, DenseNet 3D, and NasNet 3D. From now on, these architectures will be referred to by name only, omitting &#x0201c;3D&#x0201d; from the name but keeping in mind that they are networks for 3D image data.</p><sec id="Sec12"><title>Non-pre-trained neural networks</title><p id="Par55">The four networks mentioned above were tested as an initial step to verify that the implementation worked correctly with our data. The network architectures were used without loading the pre-trained networks&#x02019; hyperparameters (weights and bias) to evaluate their effectiveness and applicability in the stroke classification domain. All initial tests with the implemented network architectures yielded disappointing results. There was insufficient memory availability for VGG and NasNet due to their high number of hyperparameters. This limitation is critical for possibly incorporating the inference model into the radiologist&#x02019;s decision support software. Thus, the DenseNet and ResNet networks were selected. However, issues such as overfitting and vanishing gradients in ResNet and DenseNet do not render them entirely useless as stroke classifiers based on untrained values. The initial parameters with which the first networks were trained are shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Training parameters for considered basic neural networks</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Architecture</th><th align="left">Optimizer</th><th align="left">Learning rate (LR)</th><th align="left">Initialization</th><th align="left">Activation</th><th align="left">Epoch</th><th align="left">Batch</th></tr></thead><tbody><tr><td align="left">ResNet20</td><td align="left">Adam</td><td align="left">0.001</td><td align="left">He-normal</td><td align="left">ReLU</td><td align="left">25</td><td align="left">1</td></tr><tr><td align="left">DenseNet121</td><td align="left">Adam</td><td align="left">0.001</td><td align="left">Glorot</td><td align="left">ReLU</td><td align="left">25</td><td align="left">2</td></tr></tbody></table></table-wrap></p><p id="Par56">An adaptive learning rate was used to enhance better performance, as this technique improves an increase in classification performance while reducing training time [<xref ref-type="bibr" rid="CR59">59</xref>]. The initially selected loss function for ResNet was categorical <italic>cross-entropy</italic>, while binary <italic>cross-entropy</italic> was used for DenseNet. The accuracy obtained by these networks was relatively low and is shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Accuracy values for RestNet and DenseNet</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Architecture</th><th align="left">Training accuracy</th><th align="left">Validation accuracy</th></tr></thead><tbody><tr><td align="left">ResNet</td><td align="left">80%</td><td align="left">66%</td></tr><tr><td align="left">DenseNet</td><td align="left">60%</td><td align="left">55%</td></tr></tbody></table></table-wrap></p><p id="Par57">The results shown some improvement in the DenseNet network by changing the Adam optimizer to SGD, with a learning rate of 0.01, achieving training data accuracy of 0.79 and the test data of 0.6. However, neither network successfully converged to an acceptable solution in the test data.</p><p id="Par58">Therefore, it was expected that results from non-pre-trained neural networks would lack statistical significance in terms of accuracy during training and validation/testing phases. Indeed, training with various proposed Convolutional Neural Network (CNN) structures produced inconsistent results, with accuracies falling below 60%.</p><p id="Par59">As a result, the decision was made to utilize transfer learning techniques [<xref ref-type="bibr" rid="CR60">60</xref>], specifically leveraging pre-trained networks known for their effectiveness in other domains of medical image detection, and particularly those utilizing 3D structures. The pre-trained CNNs chosen for this purpose were ResNet, DenseNet, and VGG. Interestingly, VGG, which had been previously disregarded, demonstrated significantly improved performance when implemented as a pre-trained network.</p></sec><sec id="Sec13"><title>Transfer learning techniques</title><p id="Par60">Several versions were trained with different parameters across the three architectures, DenseNet 3D, ResNet 3D, and VGG 3D, using brain-extracted and data-augmented images, with 792 labeled with stroke and 792 without stroke.</p><p id="Par61">To adapt the pre-trained 2D models from ImageNet to 3D medical imaging data, we extended the convolutional and pooling layers to three dimensions while maintaining the overall architectural structure. The pre-trained weights for ImageNet were used as initialization, specifically for the corresponding layers that could be seamlessly extended to 3D. Layers without a direct mapping from 2D to 3D were randomly initialized. Fine-tuning was then performed on the medical imaging dataset to optimize the weights for the specific task. This approach takes advantage of the feature extraction capabilities learned from natural images while allowing adaptation to volumetric brain data.</p><p id="Par62">Regarding the dataset split, the data were randomly divided into two sets: 80% for training and 20% for testing. While we ensured an equal number of stroke and non-stroke cases in each set, we did not explicitly enforce stratification based on stroke subtypes.</p><sec id="Sec14"><title>DenseNet 3D</title><p id="Par63">The results are shown in the Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> and in the Figs.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>,&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>&#x000a0;and&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>. The figures display accuracy graphs for each epoch alongside the confusion matrices for the three versions that outperform their corresponding non-pre-trained network.<table-wrap id="Tab3"><label>Table 3</label><caption><p>DenseNet 3D results, batch size: 1&#x02013;2</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Version</th><th align="left" rowspan="2">Opt.</th><th align="left" rowspan="2">LR</th><th align="left" rowspan="2">Loss</th><th align="left" colspan="2">Accuracy</th><th align="left" colspan="4">Confusion matrix</th><th align="left" rowspan="2">Batch</th></tr><tr><th align="left">Train (%)</th><th align="left">Test (%)</th><th align="left">Sens.</th><th align="left">Spec.</th><th align="left">Prec.</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left">1.0</td><td align="left">SGD</td><td align="left">0.001</td><td align="left">Bin-ce</td><td align="left">100</td><td align="left">40</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">1</td></tr><tr><td align="left">1.1</td><td align="left">Adam</td><td align="left">0.001</td><td align="left">Bin-ce</td><td align="left">100</td><td align="left">50</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">1</td></tr><tr><td align="left">1.1.1</td><td align="left">Adam</td><td align="left">0.001</td><td align="left">Bin-ce</td><td align="left">98</td><td align="left">78</td><td align="left">80%</td><td align="left">75%</td><td align="left">77%</td><td align="left">0.78</td><td align="left">2</td></tr><tr><td align="left">1.2</td><td align="left">SGD</td><td align="left">0.001</td><td align="left">Cat-ce</td><td align="left">100</td><td align="left">36</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">1</td></tr><tr><td align="left">1.2.1</td><td align="left">SGD</td><td align="left">0.001</td><td align="left">Cat-ce</td><td align="left">100</td><td align="left">94</td><td align="left">95%</td><td align="left">93%</td><td align="left">93%</td><td align="left">0.93</td><td align="left">2</td></tr><tr><td align="left">1.3</td><td align="left">Adam</td><td align="left">0.001</td><td align="left">Cat-ce</td><td align="left">99</td><td align="left">82</td><td align="left">88%</td><td align="left">75%</td><td align="left">79%</td><td align="left">0.82</td><td align="left">2</td></tr><tr><td align="left">1.4</td><td align="left">SGD*</td><td align="left">0.001</td><td align="left">Cat-ce</td><td align="left">52</td><td align="left">48</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">&#x02013;</td><td align="left">2</td></tr></tbody></table></table-wrap></p><p id="Par64">
<fig id="Fig5"><label>Fig. 5</label><caption><p>DenseNet results, version 1.1.1</p></caption><graphic xlink:href="13755_2025_352_Fig5_HTML" id="MO5"/></fig>
<fig id="Fig6"><label>Fig. 6</label><caption><p>DenseNet results, version 1.2.1</p></caption><graphic xlink:href="13755_2025_352_Fig6_HTML" id="MO6"/></fig>
<fig id="Fig7"><label>Fig. 7</label><caption><p>DenseNet results, version 1.3</p></caption><graphic xlink:href="13755_2025_352_Fig7_HTML" id="MO7"/></fig>
</p><p id="Par65">From Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>, it is evident that with a batch size of 1, either with SGD or Adam optimizers; and loss function <italic>binary</italic> or <italic>categorical cross-entropy</italic>, the training results are suboptimal, exhibiting severe overfitting and test accuracy not exceeding 0.5.</p><p id="Par66">The network shows significant improvement when the batch size to 2 (versions 1.1.1, 1.2.1 and 1.3). It is observed that the best results are obtained with a network with a SGD optimizer, using <italic>momentum</italic> of 0.9 and accelerated Nesterov gradient and loss function <italic>categorical cross-entropy</italic>. These parameters correspond to version 1.2.1 of the experiments with the DenseNet architecture.</p><p id="Par67">A new version, 1.4, was developed with a DenseNet network with <inline-formula id="IEq3"><alternatives><tex-math id="d33e1351">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$lecun\_{normal}$$\end{document}</tex-math><mml:math id="d33e1356"><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>_</mml:mi><mml:mrow><mml:mi mathvariant="italic">normal</mml:mi></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13755_2025_352_Article_IEq3.gif"/></alternatives></inline-formula> initialization, AlphaDropout and the SELU activation function. According to some authors [<xref ref-type="bibr" rid="CR61">61</xref>], DenseNet is expected to perform better with these features. However, in this case, ir yielded poor results, failing to achieve high accuracy even on training data.</p><p id="Par68">From the previous results, two key parameters were identified as critical for improving network performance: batch size (which should be increased) and learning rate (which should be reduced). Increasing the batch size demands higher computational power; therefore, cloud instances (Google Cloud Platform) with greater capacity were employed, with an additional cost. This decision enabled us to increase the batch size to 15 for DenseNet and 10 for ResNet. For these experiments, SGD was retained for the DenseNet and the Adam optimizer for ResNet, as they had demonstrated superior performance. The categorical <italic>cross-entropy</italic> loss function was also kept. Taking advantage of this computational upgrade, we reintroduced the VGG architecture into experiments, as it could new be reused. However, due to its high memory consumption, training was only feasible with a batch size of 2.</p><p id="Par69">Another crucial parameter for optimizing neural network performance is the Learning Rate (LR). This parameter controls the magnitude of weight updates during training by determining the step size in the gradient computation for minimizing the loss function. If the learning rate is too high, the model may overshoot the minimum, preventing convergence or even causing divergence. In such cases, training is fast but unstable. Conversely, if the learning rate is too low, training becomes excessively slow and resource-intensive, making it necessary to find an optimal value or use adaptive mechanisms. Ultimately, there is no universally optimal learning rate, as its effectiveness depends on the specific dataset used for training.</p><p id="Par70">In Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>, several DenseNet networks were trained with a batch size of 15 (the maximum that our computational capacity allows), using the SGD optimizer. All versions, expect 2.0, incorporated an accelerated <italic>Nesterov</italic> gradient and employed the categorical <italic>cross-entropy</italic> loss function. The experiments analyzed the effects of learning rate changes at epochs 80 and 160.<table-wrap id="Tab4"><label>Table 4</label><caption><p>DenseNet 3D with batch size of 15 SGD optimizer and <italic>categorical cross-entropy</italic> loss function</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Vers.</th><th align="left" rowspan="2">Epochs</th><th align="left" rowspan="2">LR</th><th align="left" colspan="2">Accuracy</th><th align="left" colspan="4">Confusion matrix</th></tr><tr><th align="left">Train (%)</th><th align="left">Test (%)</th><th align="left">Sens. (%)</th><th align="left">Spec. (%)</th><th align="left">Prec. (%)</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left">2.0</td><td align="left">80</td><td align="left">0.01</td><td align="left">95</td><td align="left">80</td><td align="left">64</td><td align="left">96</td><td align="left">95</td><td align="left">0.76</td></tr><tr><td align="left">2.1</td><td align="left">80</td><td align="left">0.001</td><td align="left">97</td><td align="left">87</td><td align="left">77</td><td align="left">98</td><td align="left">96</td><td align="left">0.85</td></tr><tr><td align="left">2.2</td><td align="left">160</td><td align="left">0.001/0.0001 on epoch 80</td><td align="left">98</td><td align="left">95</td><td align="left">95</td><td align="left">96</td><td align="left">96</td><td align="left">0.95</td></tr><tr><td align="left">2.3</td><td align="left">160</td><td align="left">0.001/0.0001 on epoch 30</td><td align="left">96</td><td align="left">91</td><td align="left">90</td><td align="left">92</td><td align="left">92</td><td align="left">0.90</td></tr><tr><td align="left">2.4</td><td align="left">160</td><td align="left">0.001/0.0001 on epoch 20</td><td align="left">97</td><td align="left">92</td><td align="left">93</td><td align="left">91</td><td align="left">91</td><td align="left">0.91</td></tr></tbody></table></table-wrap></p><p id="Par71">Version 2.2, which is shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>, achieves the best performance, demonstrating the highest sensitivity. It is important to highlight that this metric is particularly relevant, as the network must minimize false negatives to be reliable in the decision-making process. For clinical applicability, it is crucial that the model correctly identifies stroke patients and minimizes wrong classifications as normal cases.<fig id="Fig8"><label>Fig. 8</label><caption><p>DenseNet 3D results, version 2.2</p></caption><graphic xlink:href="13755_2025_352_Fig8_HTML" id="MO8"/></fig></p></sec><sec id="Sec15"><title>ResNet 3D</title><p id="Par72">Following the same procedure as in the DenseNet case, the experiments were performed with batch sizes of 1 and 2, incorporating the Adam optimizer. The same data was used, but the number of epochs was increased to 25. This type of neural network architecture requires significantly more time per epoch to process all samples compared to DenseNet. The results are shown in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>. Accuracy graphs and the confusion matrix are presented in Figs.&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref>&#x000a0;and&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>.<table-wrap id="Tab5"><label>Table 5</label><caption><p>ResNet 3D results. Batch size 1 and 2</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Version</th><th align="left" rowspan="2">Opt.</th><th align="left" rowspan="2">LR</th><th align="left" rowspan="2">Loss</th><th align="left" colspan="2">Accuracy</th><th align="left" colspan="4">Confusion matrix</th><th align="left" rowspan="2">Batch</th></tr><tr><th align="left">Train (%)</th><th align="left">Test (%)</th><th align="left">Sens. (%)</th><th align="left">Spec. (%)</th><th align="left">Prec. (%)</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left">1.0</td><td align="left">Adam</td><td align="left">0.001</td><td align="left">Cat-ce</td><td align="left">95</td><td align="left">68</td><td align="left">69</td><td align="left">68</td><td align="left">69</td><td align="left">0.68</td><td align="left">1</td></tr><tr><td align="left">1.1</td><td align="left">Adam</td><td align="left">0.001</td><td align="left">Cat-ce</td><td align="left">87</td><td align="left">71</td><td align="left">68</td><td align="left">74</td><td align="left">73</td><td align="left">0.68</td><td align="left">2</td></tr></tbody></table></table-wrap></p><p id="Par73">
<fig id="Fig9"><label>Fig. 9</label><caption><p>ResNet 3D results, version 1.0</p></caption><graphic xlink:href="13755_2025_352_Fig9_HTML" id="MO9"/></fig>
<fig id="Fig10"><label>Fig. 10</label><caption><p>ResNet 3D results, version 1.1</p></caption><graphic xlink:href="13755_2025_352_Fig10_HTML" id="MO10"/></fig>
</p><p id="Par74">As with DenseNet, the following experiments focused on modifying the essential parameters, particularly the batch size. Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref> summarizes the performance obtained with the ResNet versions with a batch size of 10 for 40 epochs with Adam and SGD optimizers. Training this network time-consuming, so only two two representative test versions were evaluated. Version 2.0, which employs the Adam optimizer, achieves higher accuracy in both training and test data, with reduced overfitting compared to version 2.1, which uses the SGD optimizer. However, the latter demonstrates higher sensitivity.<table-wrap id="Tab6"><label>Table 6</label><caption><p>ResNet 3D results. Optimizers: Adam and SGD. Loss function: <italic>categorical cross-entropy</italic></p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Version</th><th align="left" rowspan="2">Epochs</th><th align="left" rowspan="2">LR</th><th align="left" colspan="2">Accuracy</th><th align="left" colspan="4">Confusion matrix</th></tr><tr><th align="left">Train (%)</th><th align="left">Test (%)</th><th align="left">Sens. (%)</th><th align="left">Spec. (%)</th><th align="left">Prec. (%)</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left">2.0</td><td align="left">40/Adam</td><td align="left">0.001</td><td align="left">95</td><td align="left">79</td><td align="left">75</td><td align="left">83</td><td align="left">82</td><td align="left">0.78</td></tr><tr><td align="left">2.1</td><td align="left">40/SGD</td><td align="left">0.001</td><td align="left">100</td><td align="left">76</td><td align="left">77</td><td align="left">77</td><td align="left">78</td><td align="left">0.77</td></tr></tbody></table></table-wrap></p><p id="Par75">ResNet also shows an improved performance with the increased batch size. The graphs and confusion matrices for both versions are shown in Figs.&#x000a0;<xref rid="Fig11" ref-type="fig">11</xref>&#x000a0;and&#x000a0;<xref rid="Fig12" ref-type="fig">12</xref>.<fig id="Fig11"><label>Fig. 11</label><caption><p>ResNet 3D results, version 2.0</p></caption><graphic xlink:href="13755_2025_352_Fig11_HTML" id="MO11"/></fig><fig id="Fig12"><label>Fig. 12</label><caption><p>ResNet 3D results, version 2.1</p></caption><graphic xlink:href="13755_2025_352_Fig12_HTML" id="MO12"/></fig></p></sec><sec id="Sec16"><title>VGG 3D</title><p id="Par76">In the case of the VGG 3D architecture, experiments have only been conducted with a small batch size computational limitation. It is important to note that the number of hyperparameters in VGG 3D is significantly higher than in DenseNet or ResNet, leading substantial RAM requirements for training and inference (179.1 million hyperparameters) [<xref ref-type="bibr" rid="CR62">62</xref>].</p><p id="Par77">Table&#x000a0;<xref rid="Tab7" ref-type="table">7</xref> shows the training results of the VGG network over 40 epochs, using the SGD optimizer and batch size of 2; the maximum that the computational capacity allows. The results indicated significant overfitting, though performance could likely improve with a larger batch size. Version 3.0 uses a constant LR of 0.001 throughout the 40 epochs, while version 3.1 starts with a LR of 0.001 until epoch 20, then decreases to 0.0001 until epoch 40, resulting in a slight improvement in test accuracy. Figures&#x000a0;<xref rid="Fig13" ref-type="fig">13</xref>&#x000a0;and&#x000a0;<xref rid="Fig14" ref-type="fig">14</xref> show the corresponding accuracy plots and confusion matrices. Both versions outperform the ResNet network, but fall short compared to the DenseNet network.<table-wrap id="Tab7"><label>Table 7</label><caption><p>VGG 3D results. SGD and Adam optimizers and <italic>categorical cross-entropy</italic> loss function</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Version</th><th align="left" rowspan="2">Epochs</th><th align="left" rowspan="2">LR</th><th align="left" colspan="2">Accuracy</th><th align="left" colspan="4">Confusion matrix</th></tr><tr><th align="left">Train (%)</th><th align="left">Test (%)</th><th align="left">Sens. (%)</th><th align="left">Spec. (%)</th><th align="left">Prec. (%)</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left">3.0</td><td align="left">40/SGD</td><td align="left">0.001</td><td align="left">100</td><td align="left">87</td><td align="left">85</td><td align="left">90</td><td align="left">90</td><td align="left">0.86</td></tr><tr><td align="left">3.1</td><td align="left">40/SGD</td><td align="left">0.001</td><td align="left">100</td><td align="left">89</td><td align="left">85</td><td align="left">94</td><td align="left">94</td><td align="left">0.88</td></tr></tbody></table></table-wrap></p><p id="Par78">
<fig id="Fig13"><label>Fig. 13</label><caption><p>VGG 3D results: version 3.0</p></caption><graphic xlink:href="13755_2025_352_Fig13_HTML" id="MO13"/></fig>
<fig id="Fig14"><label>Fig. 14</label><caption><p>VGG 3D results, version 3.1</p></caption><graphic xlink:href="13755_2025_352_Fig14_HTML" id="MO14"/></fig>
</p></sec></sec></sec><sec id="Sec17"><title>Results and discussion</title><p id="Par79">Tables of results presented in the previous section allow us to identify the best version of the trained network for each neural network architecture used. For DenseNet 3D, an accuracy of 98% is obtained for the training data, while 95% for the accuracy with the test/evaluation data. A remarkable fact is that there is no network overfitting; therefore, the model could generalize the unseen cases well (images). It is also important to note that the sensitivity metric is high (95%) and allows the number of false negatives to be small. If the ResNet 3D architecture data is analyzed, the accuracy is at its best at 95% for the training data but rather worse for the evaluation/test data, which is 79%. Accuracy is generally worse than that obtained with the DenseNet 3D network.</p><p id="Par80">The VGG 3D architecture produces excellent results for the training set, but there is a significant decrease in the case of the test/evaluation set. The training accuracy is 100%, and given the 89% accuracy of the best option (version 3.1), it can be indicated that there is over-fitting. This situation is more than likely due to the amount of image data available and could be verified by increasing this number. Given the work involved in labeling and generating the dataset, it is feasible for future work but not for this research. The existence of overfitting makes the model non-generalized and, therefore, not the most suitable model to be used in this particular case.</p><p id="Par81">Regarding the convergence of the three models with the best metrics for DenseNet 3D, ResNet 3D and VGG 3D, it can be observed that DenseNet 3D has a better behavior as it converges faster. Regarding the accuracy metrics, VGG 3D has better results in the training set (100%) but has overfitting. In addition, the data for the sensitivity metrics are worse, as well as the accuracy in the test set.</p><p id="Par82">A compromise between the different factors/metrics must be considered in determining the best solution. These are as follows:<list list-type="bullet"><list-item><p id="Par83">Accuracy of the neural network regarding the test/assessment set. This issue is critical as the model needs to be able to generalize to unseen stroke cases. This generalization is not feasible if the network accuracy metric for the training/test is 100% and there is overfitting.</p></list-item><list-item><p id="Par84">Sensibility of the neural network. It has been mentioned before that it is crucial not to diagnose stroke cases as if they were normal. This situation can lead to deaths and is a case that should be avoided at all costs (false positive).</p></list-item><list-item><p id="Par85">Neural Network complexity (number of hyperparameters). In order to implement the neural network inference as a stroke classifier, it is necessary to run this inference on a light computational system (a computer/personal computer without high performance). The fewer hyperparameters in the network, the lower the computational cost. In the VGG 3D case, the number of hyperparameters is high, while in DenseNet 3D or ResNet 3D is low (in comparison with VGG 3D).</p></list-item></list>Table&#x000a0;<xref rid="Tab8" ref-type="table">8</xref> summarizes these factors for the best results on each neural network architecture. Given these factors or metrics, the best balance between them is achieved by the DenseNet 3D network. Therefore, the model chosen is DenseNet 2.2, with the parameters shown in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>.<table-wrap id="Tab8"><label>Table 8</label><caption><p>Factors considered for neural network 3D architecture selection</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Factor/metric</th><th align="left">Accuracy (%)</th><th align="left">Sensibility (%)</th><th align="left">Complexity</th></tr></thead><tbody><tr><td align="left">DenseNet</td><td align="left">95</td><td align="left">95</td><td align="left">Low</td></tr><tr><td align="left">ResNet</td><td align="left">100</td><td align="left">77</td><td align="left">Low</td></tr><tr><td align="left">VGG</td><td align="left">100</td><td align="left">85</td><td align="left">High</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec18"><title>Conclusions and future works</title><p id="Par86">This research exposes a trained binary classification of NCCT studies in cases with and without stroke as a main feature. No datasets are available for this particular case, so a new dataset has been built using real patient data. This dataset includes 3D images that train several CCN architectures from scratch, and later pre-trained networks. The research focuses on transfer learning techniques because experimentation with the more known 3D deep learning architectures has poor results in terms of metrics. Using pre-trained neural networks produces the best results, adapting the pre-trained hyperparameters to the particular case. Different parameters were tested to select the best neural network 3D architecture. For each network and parameter version, metrics were saved to compare these versions. DenseNet 3D network has excellent results regarding the factors chosen for the architecture being used as a possible assistant for decision-making. In addition to the metrics, the network complexity factor has been used to evaluate the architectures. This factor is crucial when implementing reference models to assist radiologists/physicians in detecting stroke. In clinical practice, the shorter the time from stroke to diagnosis and treatment, the better the prognosis. The availability of an artificial intelligence application that, during the NCCT scan, can rule out areas of hypodensity would assist the radiologist in deciding whether to extend the study and the neurologist in establishing treatment quickly. A significant research result is the building of the ictus dataset. This new dataset has been generated specifically for stroke detection, and there is no other known dataset, as the authors know. The number of cranial CT studies is sufficient to apply transfer learning techniques to perform the classification task. In addition, the DICOM format, a standard used worldwide for storing and communicating medical images, has made the construction procedure replicable and extensible to add new case studies and validate the neural network.</p><p id="Par87">In future research, we aim to expand our dataset by incorporating a broader range of NCCT scans from diverse patient populations to enhance model robustness and reduce potential biases. Additionally, we plan to explore alternative deep learning architectures, particularly Vision Transformers (ViTs) and hybrid CNN-Transformer models, to evaluate their performance compared to traditional 3D CNNs. Another key area of improvement is the clinical validation of the model, where we intend to test our classifier with data from different hospitals and conduct comparative studies with radiologists to assess its real-world applicability. To facilitate its practical deployment, we will develop a graphical interface for easier interaction with medical professionals and work on optimizing the model for execution on standard clinical hardware. These improvements aim to make our AI-based stroke detection system more reliable, interpretable, and suitable for integration into existing diagnostic workflows.</p></sec></body><back><fn-group><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>Rafael Pastor-Vargas, Juan M. Haut, Antonio Robles-G&#x000f3;mez, Mercedes E. Paoletti, Jos&#x000e9; Alberto Ben&#x000ed;tez-Andrades have contributed equally to this work.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The authors thank the University Hospital of Navarra for their support in developing the work. The building of the dataset was only possible with the hospital&#x02019;s help and anonymous patient data. Also, we thank the Ministry of Economy, Science and Digital Agenda of the Regional Government of Extremadura and the European Regional Development Fund (ERDF) of the European Union. They supported us with the grant reference GR21040. This contribution is also supported within the framework of the CiberCSI UNED research group, as well as the CiberGID UNED innovation group.&#x000a0;The authors would like to thank UNED for funding open access publishing.</p></ack><notes notes-type="funding-information"><title>Funding</title><p>Open Access funding provided thanks to the CRUE-CSIC agreement with Springer Nature.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The images used in this research have been provided by the Hospital of Navarra and anonymized for research use. They are not publicly available, but can be obtained from the corresponding author, Rafael Pastor Vargas (rpastor@scc.uned.es), with the prior consent of the Hospital of Navarra.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Conflict of interest</title><p id="Par88">All authors declare that there is no conflict of interest.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Ant&#x000f3;n-Mun&#x000e1;rriz C, Pastor-Vargas R, Haut JM, et&#x000a0;al (2023) Detection of cerebral ischaemia using transfer learning techniques. In: 2023 IEEE 36th international symposium on computer-based medical systems (CBMS), pp 589&#x02013;59. 10.1109/CBMS58004.2023.00284</mixed-citation></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Oman</surname><given-names>O</given-names></name><name><surname>Makela</surname><given-names>T</given-names></name><name><surname>Salli</surname><given-names>E</given-names></name><etal/></person-group><article-title>3d convolutional neural networks applied to ct angiography in the detection of acute ischemic stroke</article-title><source>Eur Radiol Exp</source><year>2019</year><volume>3</volume><issue>1</issue><fpage>8</fpage><pub-id pub-id-type="doi">10.1186/s41747-019-0085-6</pub-id><pub-id pub-id-type="pmid">30758694</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Oman O, Makela T, Salli E, et al. 3d convolutional neural networks applied to ct angiography in the detection of acute ischemic stroke. Eur Radiol Exp. 2019;3(1):8.<pub-id pub-id-type="pmid">30758694</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Ramos</surname><given-names>MM</given-names></name><name><surname>Giadas</surname><given-names>TC</given-names></name></person-group><article-title>Evaluaci&#x000f3;n vascular en el c&#x000f3;digo ictus: papel de la angio-tomograf&#x000ed;a computarizada</article-title><source>Radiologia</source><year>2015</year><volume>57</volume><issue>2</issue><fpage>156</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/j.rx.2013.11.006</pub-id><pub-id pub-id-type="pmid">25060835</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Ramos MM, Giadas TC. Evaluaci&#x000f3;n vascular en el c&#x000f3;digo ictus: papel de la angio-tomograf&#x000ed;a computarizada. Radiologia. 2015;57(2):156&#x02013;66.<pub-id pub-id-type="pmid">25060835</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>PA</given-names></name><name><surname>Demchuk</surname><given-names>AM</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><etal/></person-group><article-title>Validity and reliability of a quantitative computed tomography score in predicting outcome of hyperacute stroke before thrombolytic therapy</article-title><source>Lancet</source><year>2000</year><volume>355</volume><issue>9216</issue><fpage>1670</fpage><lpage>1674</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(00)02237-6</pub-id><pub-id pub-id-type="pmid">10905241</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Barber PA, Demchuk AM, Zhang J, et al. Validity and reliability of a quantitative computed tomography score in predicting outcome of hyperacute stroke before thrombolytic therapy. Lancet. 2000;355(9216):1670&#x02013;4.<pub-id pub-id-type="pmid">10905241</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Lisowska A, O&#x02019;Neil A, Dilys V, et&#x000a0;al (2017) Context-aware convolutional neural networks for stroke sign detection in non-contrast ct scans. In: Annual conference on medical image understanding and analysis, Springer, pp 494&#x02013;505</mixed-citation></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>A</given-names></name><name><surname>Gordillo</surname><given-names>N</given-names></name><name><surname>Montseny</surname><given-names>E</given-names></name><etal/></person-group><article-title>Automated detection of parenchymal changes of ischemic stroke in non-contrast computer tomography: a fuzzy approach</article-title><source>Biomed Signal Process Control</source><year>2018</year><volume>45</volume><fpage>117</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1016/j.bspc.2018.05.037</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Davis A, Gordillo N, Montseny E, et al. Automated detection of parenchymal changes of ischemic stroke in non-contrast computer tomography: a fuzzy approach. Biomed Signal Process Control. 2018;45:117&#x02013;27.</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Stoel</surname><given-names>BC</given-names></name><name><surname>Marquering</surname><given-names>HA</given-names></name><name><surname>Staring</surname><given-names>M</given-names></name><etal/></person-group><article-title>Automated brain computed tomographic densitometry of early ischemic changes in acute stroke</article-title><source>J Med Imaging</source><year>2015</year><volume>2</volume><issue>1</issue><fpage>014004</fpage><pub-id pub-id-type="doi">10.1117/1.JMI.2.1.014004</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Stoel BC, Marquering HA, Staring M, et al. Automated brain computed tomographic densitometry of early ischemic changes in acute stroke. J Med Imaging. 2015;2(1): 014004.</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Chin CL, Lin BJ, Wu GR, et&#x000a0;al (2017) An automated early ischemic stroke detection system using cnn deep learning algorithm. In: 2017 IEEE 8th international conference on awareness science and technology (iCAST), pp 368&#x02013;37. 10.1109/ICAwST.2017.8256481</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Reddy MK, Kovuri K, Avanija J, et&#x000a0;al (2022) Brain stroke prediction using deep learning: a cnn approach. In: 2022 4th International conference on inventive research in computing applications (ICIRCA), pp 775&#x02013;7810.1109/ICIRCA54612.2022.9985596</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Pereira DR, Filho PPR, de&#x000a0;Rosa GH, et&#x000a0;al (2018) Stroke lesion detection using convolutional neural networks. In: 2018 International joint conference on neural networks (IJCNN), 10.1109/IJCNN.2018.8489199</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Tan K, Marvell YA, Gunawan AAS (2023) Early ischemic stroke detection using deep learning: a systematic literature review. In: 2023 International seminar on application for technology of information and communication (iSemantic) pp 7&#x02013;1. 10.1109/iSemantic59612.2023.10295339</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Gaidhani BR, R.Rajamenakshi R, Sonavane S (2019) Brain stroke detection using convolutional neural network and deep learning models. In: 2019 2nd International conference on intelligent communication and computational techniques (ICCT), pp 242&#x02013;24. 10.1109/ICCT46177.2019.8969052</mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>McCulloch</surname><given-names>WS</given-names></name><name><surname>Pitts</surname><given-names>W</given-names></name></person-group><article-title>A logical calculus of the ideas immanent in nervous activity</article-title><source>Bull Math Biophys</source><year>1943</year><volume>5</volume><issue>4</issue><fpage>115</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1007/BF02478259</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">McCulloch WS, Pitts W. A logical calculus of the ideas immanent in nervous activity. Bull Math Biophys. 1943;5(4):115&#x02013;33.</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Gradient-based learning applied to document recognition</article-title><source>Proc IEEE</source><year>1998</year><volume>86</volume><issue>11</issue><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition. Proc IEEE. 1998;86(11):2278&#x02013;324.</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Razzak MI, Naz S, Zaib A (2018) Deep learning for medical image processing: overview, challenges and the future. In: Classification in BioApps. Springer, pp 323&#x02013;350</mixed-citation></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>L</given-names></name><name><surname>Qi</surname><given-names>S</given-names></name><etal/></person-group><article-title>Agile convolutional neural network for pulmonary nodule classification using ct images</article-title><source>Int J Comput Assist Radiol Surg</source><year>2018</year><volume>13</volume><issue>4</issue><fpage>585</fpage><lpage>595</lpage><pub-id pub-id-type="doi">10.1007/s11548-017-1696-0</pub-id><pub-id pub-id-type="pmid">29473129</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Zhao X, Liu L, Qi S, et al. Agile convolutional neural network for pulmonary nodule classification using ct images. Int J Comput Assist Radiol Surg. 2018;13(4):585&#x02013;95.<pub-id pub-id-type="pmid">29473129</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Cao</surname><given-names>P</given-names></name><name><surname>Zhao</surname><given-names>D</given-names></name><etal/></person-group><article-title>Pulmonary nodule classification with deep convolutional neural networks on computed tomography images</article-title><source>Comput Math Methods Med</source><year>2016</year><volume>2016</volume><fpage>6215085</fpage><pub-id pub-id-type="doi">10.1155/2016/6215085</pub-id><pub-id pub-id-type="pmid">28070212</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Li W, Cao P, Zhao D, et al. Pulmonary nodule classification with deep convolutional neural networks on computed tomography images. Comput Math Methods Med. 2016;2016:6215085.<pub-id pub-id-type="pmid">28070212</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Alakwaa</surname><given-names>W</given-names></name><name><surname>Nassef</surname><given-names>M</given-names></name><name><surname>Badr</surname><given-names>A</given-names></name></person-group><article-title>Lung cancer detection and classification with 3d convolutional neural network (3d-cnn)</article-title><source>Lung Cancer</source><year>2017</year><volume>8</volume><issue>8</issue><fpage>409</fpage></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Alakwaa W, Nassef M, Badr A. Lung cancer detection and classification with 3d convolutional neural network (3d-cnn). Lung Cancer. 2017;8(8):409.</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Q</given-names></name><name><surname>Zhao</surname><given-names>L</given-names></name><name><surname>Luo</surname><given-names>X</given-names></name><etal/></person-group><article-title>Using deep learning for classification of lung nodules on computed tomography images</article-title><source>J Healthc Eng</source><year>2017</year><volume>2017</volume><fpage>8314740</fpage><pub-id pub-id-type="doi">10.1155/2017/8314740</pub-id><pub-id pub-id-type="pmid">29065651</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Song Q, Zhao L, Luo X, et al. Using deep learning for classification of lung nodules on computed tomography images. J Healthc Eng. 2017;2017:8314740.<pub-id pub-id-type="pmid">29065651</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>L</given-names></name><name><surname>Jiang</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Automated pulmonary nodule detection in ct images using 3d deep squeeze-and-excitation networks</article-title><source>Int J Comput Assist Radiol Surg</source><year>2019</year><volume>14</volume><issue>11</issue><fpage>1969</fpage><lpage>1979</lpage><pub-id pub-id-type="doi">10.1007/s11548-019-01979-1</pub-id><pub-id pub-id-type="pmid">31028657</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Gong L, Jiang S, Yang Z, et al. Automated pulmonary nodule detection in ct images using 3d deep squeeze-and-excitation networks. Int J Comput Assist Radiol Surg. 2019;14(11):1969&#x02013;79.<pub-id pub-id-type="pmid">31028657</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>F</given-names></name><name><surname>Liang</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Evaluate the malignancy of pulmonary nodules using the 3-d deep leaky noisy-or network</article-title><source>IEEE Trans Neural Netw Learn Syst</source><year>2019</year><volume>30</volume><issue>11</issue><fpage>3484</fpage><lpage>3495</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2019.2892409</pub-id><pub-id pub-id-type="pmid">30794190</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Liao F, Liang M, Li Z, et al. Evaluate the malignancy of pulmonary nodules using the 3-d deep leaky noisy-or network. IEEE Trans Neural Netw Learn Syst. 2019;30(11):3484&#x02013;95.<pub-id pub-id-type="pmid">30794190</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">(RSNA) NARS (2021) Brats challenge 2021, rsna-asnr-miccai. <ext-link ext-link-type="uri" xlink:href="http://braintumorsegmentation.org/">http://braintumorsegmentation.org/</ext-link>, accessed on: 2025/05/07 12:03:21</mixed-citation></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Liang</surname><given-names>D</given-names></name><etal/></person-group><article-title>Multimodal 3d densenet for idh genotype prediction in gliomas</article-title><source>Genes</source><year>2018</year><volume>9</volume><issue>8</issue><fpage>382</fpage><pub-id pub-id-type="doi">10.3390/genes9080382</pub-id><pub-id pub-id-type="pmid">30061525</pub-id>
</element-citation><mixed-citation id="mc-CR23" publication-type="journal">Liang S, Zhang R, Liang D, et al. Multimodal 3d densenet for idh genotype prediction in gliomas. Genes. 2018;9(8):382.<pub-id pub-id-type="pmid">30061525</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Yang C, Rangarajan A, Ranka S (2018) Visual explanations from deep 3d convolutional neural networks for Alzheimer&#x02019;s disease classification. In: AMIA annual symposium proceedings, American medical informatics association, p 1571</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>XW</given-names></name><name><surname>Hui</surname><given-names>R</given-names></name><name><surname>Tian</surname><given-names>Z</given-names></name></person-group><article-title>Classification of ct brain images based on deep learning networks</article-title><source>Comput Methods Programs Biomed</source><year>2017</year><volume>138</volume><fpage>49</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2016.10.007</pub-id><pub-id pub-id-type="pmid">27886714</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Gao XW, Hui R, Tian Z. Classification of ct brain images based on deep learning networks. Comput Methods Programs Biomed. 2017;138:49&#x02013;56.<pub-id pub-id-type="pmid">27886714</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Khvostikov A, Aderghal K, Benois-Pineau J, et&#x000a0;al (2018) 3d cnn-based classification using smri and md-dti images for alzheimer disease studies. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1801.05968">arXiv:1801.05968</ext-link></mixed-citation></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Praveen</surname><given-names>G</given-names></name><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Sundaram</surname><given-names>P</given-names></name><etal/></person-group><article-title>Ischemic stroke lesion segmentation using stacked sparse autoencoder</article-title><source>Comput Biol Med</source><year>2018</year><volume>99</volume><fpage>38</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2018.05.027</pub-id><pub-id pub-id-type="pmid">29883752</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Praveen G, Agrawal A, Sundaram P, et al. Ischemic stroke lesion segmentation using stacked sparse autoencoder. Comput Biol Med. 2018;99:38&#x02013;52.<pub-id pub-id-type="pmid">29883752</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Computing MI, Conference CAII (2022) Ischemic stroke lesion segmentation (isles) 2022. <ext-link ext-link-type="uri" xlink:href="http://www.isles-challenge.org/">http://www.isles-challenge.org/</ext-link>, accessed on: 2025/05/07 12:03:21</mixed-citation></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Lo</surname><given-names>CM</given-names></name><name><surname>Hung</surname><given-names>PH</given-names></name><name><surname>Hsieh</surname><given-names>KLC</given-names></name></person-group><article-title>Computer-aided detection of hyperacute stroke based on relative radiomic patterns in computed tomography</article-title><source>Appl Sci</source><year>2019</year><volume>9</volume><issue>8</issue><fpage>1668</fpage><pub-id pub-id-type="doi">10.3390/app9081668</pub-id></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Lo CM, Hung PH, Hsieh KLC. Computer-aided detection of hyperacute stroke based on relative radiomic patterns in computed tomography. Appl Sci. 2019;9(8):1668.</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>G</given-names></name><name><surname>Lin</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><etal/></person-group><article-title>Early identification of ischemic stroke in noncontrast computed tomography</article-title><source>Biomed Signal Process Control</source><year>2019</year><volume>52</volume><fpage>41</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1016/j.bspc.2019.03.008</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Wu G, Lin J, Chen X, et al. Early identification of ischemic stroke in noncontrast computed tomography. Biomed Signal Process Control. 2019;52:41&#x02013;52.</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Song T (2019) Generative model-based ischemic stroke lesion segmentation. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1906.02392">arXiv:1906.02392</ext-link></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Takahashi N, Shinohara Y, Kinoshita T, et&#x000a0;al (2019) Computerized identification of early ischemic changes in acute stroke in noncontrast ct using deep learning. In: Medical imaging 2019: computer-aided diagnosis, international society for optics and photonics, p 109503A</mixed-citation></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Mokli</surname><given-names>Y</given-names></name><name><surname>Pfaff</surname><given-names>J</given-names></name><name><surname>Dos Santos</surname><given-names>DP</given-names></name><etal/></person-group><article-title>Computer-aided imaging analysis in acute ischemic stroke-background and clinical applications</article-title><source>Neurol Res Pract</source><year>2019</year><volume>1</volume><issue>1</issue><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1186/s42466-019-0028-y</pub-id><pub-id pub-id-type="pmid">33324867</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">Mokli Y, Pfaff J, Dos Santos DP, et al. Computer-aided imaging analysis in acute ischemic stroke-background and clinical applications. Neurol Res Pract. 2019;1(1):1&#x02013;13.<pub-id pub-id-type="pmid">33324867</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Olthof</surname><given-names>AW</given-names></name><name><surname>van Ooijen</surname><given-names>PM</given-names></name><name><surname>Mehrizi</surname><given-names>MHR</given-names></name></person-group><article-title>Promises of artificial intelligence in neuroradiology: a systematic technographic review</article-title><source>Neuroradiology</source><year>2020</year><volume>62</volume><issue>10</issue><fpage>1265</fpage><lpage>1278</lpage><pub-id pub-id-type="doi">10.1007/s00234-020-02424-w</pub-id><pub-id pub-id-type="pmid">32318774</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">Olthof AW, van Ooijen PM, Mehrizi MHR. Promises of artificial intelligence in neuroradiology: a systematic technographic review. Neuroradiology. 2020;62(10):1265&#x02013;78.<pub-id pub-id-type="pmid">32318774</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Haifa T, Jihene B, Lamia S, et&#x000a0;al (2020) e-aspects for early detection and diagnosis of ischemic stroke. In: 2020 5th International conference on advanced technologies for signal and image processing (ATSIP), IEEE, pp 1&#x02013;5</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Zeiler MD, Fergus R (2014) Visualizing and understanding convolutional networks. In: European conference on computer vision, Springer, pp 818&#x02013;833</mixed-citation></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>Nowinski</surname><given-names>WL</given-names></name><name><surname>Walecki</surname><given-names>J</given-names></name><name><surname>P&#x000f3;&#x00142;torak-Szymczak</surname><given-names>G</given-names></name><etal/></person-group><article-title>Ischemic infarct detection, localization, and segmentation in noncontrast ct human brain scans: review of automated methods</article-title><source>PeerJ</source><year>2020</year><volume>8</volume><fpage>e10444</fpage><pub-id pub-id-type="doi">10.7717/peerj.10444</pub-id><pub-id pub-id-type="pmid">33391867</pub-id>
</element-citation><mixed-citation id="mc-CR37" publication-type="journal">Nowinski WL, Walecki J, P&#x000f3;&#x00142;torak-Szymczak G, et al. Ischemic infarct detection, localization, and segmentation in noncontrast ct human brain scans: review of automated methods. PeerJ. 2020;8: e10444.<pub-id pub-id-type="pmid">33391867</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Goebel</surname><given-names>J</given-names></name><name><surname>Stenzel</surname><given-names>E</given-names></name><name><surname>Guberina</surname><given-names>N</given-names></name><etal/></person-group><article-title>Automated aspect rating: comparison between the frontier aspect score software and the brainomix software</article-title><source>Neuroradiology</source><year>2018</year><volume>60</volume><issue>12</issue><fpage>1267</fpage><lpage>1272</lpage><pub-id pub-id-type="doi">10.1007/s00234-018-2098-x</pub-id><pub-id pub-id-type="pmid">30219935</pub-id>
</element-citation><mixed-citation id="mc-CR38" publication-type="journal">Goebel J, Stenzel E, Guberina N, et al. Automated aspect rating: comparison between the frontier aspect score software and the brainomix software. Neuroradiology. 2018;60(12):1267&#x02013;72.<pub-id pub-id-type="pmid">30219935</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Tuladhar</surname><given-names>A</given-names></name><name><surname>Schimert</surname><given-names>S</given-names></name><name><surname>Rajashekar</surname><given-names>D</given-names></name><etal/></person-group><article-title>Automatic segmentation of stroke lesions in non-contrast computed tomography datasets with convolutional neural networks</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>94871</fpage><lpage>9487</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2995632</pub-id></element-citation><mixed-citation id="mc-CR39" publication-type="journal">Tuladhar A, Schimert S, Rajashekar D, et al. Automatic segmentation of stroke lesions in non-contrast computed tomography datasets with convolutional neural networks. IEEE Access. 2020;8:94871&#x02013;9487. 10.1109/ACCESS.2020.2995632.</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Luo C, Zhang J, Chen X, et&#x000a0;al (2021) Ucatr: Based on cnn and transformer encoding and cross-attention decoding for lesion segmentation of acute ischemic stroke in non-contrast computed tomography images. In; 2021 43rd Annual international conference of the IEEE engineering in medicine &#x00026; biology society (EMBC) pp 3565&#x02013;356. 10.1109/EMBC46164.2021.9630336</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Ostmeier S, Heit J, Axelrod B, et&#x000a0;al (2022) Non-inferiority of deep learning model to segment acute stroke on non-contrast ct compared to neuroradiologists. ArXiv abs/2211.153410.48550/arXiv.2211.15341</mixed-citation></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Kuang</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><etal/></person-group><article-title>Hybrid cnn-transformer network with circular feature interaction for acute ischemic stroke lesion segmentation on non-contrast ct scans</article-title><source>IEEE Trans Med Imaging</source><year>2024</year><volume>43</volume><fpage>2303</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1109/TMI.2024.3362879</pub-id><pub-id pub-id-type="pmid">38319756</pub-id>
</element-citation><mixed-citation id="mc-CR42" publication-type="journal">Kuang H, Wang Y, Liu J, et al. Hybrid cnn-transformer network with circular feature interaction for acute ischemic stroke lesion segmentation on non-contrast ct scans. IEEE Trans Med Imaging. 2024;43:2303&#x02013;231. 10.1109/TMI.2024.3362879.<pub-id pub-id-type="pmid">38319756</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Shi</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>M</given-names></name><etal/></person-group><article-title>Deep learning-based identification of acute ischemic core and deficit from non-contrast ct and cta</article-title><source>J Cerebral Blood Flow Metab</source><year>2021</year><volume>41</volume><fpage>3028</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1177/0271678X211023660</pub-id></element-citation><mixed-citation id="mc-CR43" publication-type="journal">Wang C, Shi Z, Yang M, et al. Deep learning-based identification of acute ischemic core and deficit from non-contrast ct and cta. J Cerebral Blood Flow Metab. 2021;41:3028&#x02013;303. 10.1177/0271678X211023660.</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name><surname>Kaothanthong</surname><given-names>N</given-names></name><name><surname>Atsavasirilert</surname><given-names>K</given-names></name><name><surname>Sarampakhul</surname><given-names>S</given-names></name><etal/></person-group><article-title>Artificial intelligence for localization of the acute ischemic stroke by non-contrast computed tomography</article-title><source>PLoS ONE</source><year>2022</year><pub-id pub-id-type="doi">10.1371/journal.pone.0277573</pub-id></element-citation><mixed-citation id="mc-CR44" publication-type="journal">Kaothanthong N, Atsavasirilert K, Sarampakhul S, et al. Artificial intelligence for localization of the acute ischemic stroke by non-contrast computed tomography. PLoS ONE. 2022. 10.1371/journal.pone.0277573.</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Wei</surname><given-names>SR</given-names></name><etal/></person-group><article-title>Improving the diagnosis of acute ischemic stroke on non-contrast ct using deep learning: a multicenter study</article-title><source>Insights Imaging</source><year>2022</year><pub-id pub-id-type="doi">10.1186/s13244-022-01331-3</pub-id></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Chen W, Wu J, Wei SR, Wu, et al. Improving the diagnosis of acute ischemic stroke on non-contrast ct using deep learning: a multicenter study. Insights Imaging. 2022. 10.1186/s13244-022-01331-3.</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>G</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Lin</surname><given-names>J</given-names></name><etal/></person-group><article-title>Identification of invisible ischemic stroke in non-contrast ct based on novel two-stage convolutional neural network model</article-title><source>Med Phys</source><year>2020</year><pub-id pub-id-type="doi">10.1002/mp.14691</pub-id></element-citation><mixed-citation id="mc-CR46" publication-type="journal">Wu G, Chen X, Lin J, et al. Identification of invisible ischemic stroke in non-contrast ct based on novel two-stage convolutional neural network model. Med Phys. 2020. 10.1002/mp.14691.</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Tolhuisen</surname><given-names>M</given-names></name><name><surname>Ponomareva</surname><given-names>E</given-names></name><name><surname>Boers</surname><given-names>A</given-names></name><etal/></person-group><article-title>A convolutional neural network for anterior intra-arterial thrombus detection and segmentation on non-contrast computed tomography of patients with acute ischemic stroke</article-title><source>Appl Sci</source><year>2020</year><pub-id pub-id-type="doi">10.3390/app10144861</pub-id></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Tolhuisen M, Ponomareva E, Boers A, et al. A convolutional neural network for anterior intra-arterial thrombus detection and segmentation on non-contrast computed tomography of patients with acute ischemic stroke. Appl Sci. 2020. 10.3390/app10144861.</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name><surname>Yadav</surname><given-names>A</given-names></name><name><surname>Mehta</surname><given-names>H</given-names></name><name><surname>Shah</surname><given-names>M</given-names></name></person-group><article-title>From pixels to predictions: leveraging cnns for timely ischemic stroke detection</article-title><source>Int J Sci Res Sci, Eng Technol</source><year>2024</year><pub-id pub-id-type="doi">10.32628/ijsrset2411411</pub-id></element-citation><mixed-citation id="mc-CR48" publication-type="journal">Yadav A, Mehta H, Shah M. From pixels to predictions: leveraging cnns for timely ischemic stroke detection. Int J Sci Res Sci, Eng Technol. 2024. 10.32628/ijsrset2411411.</mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Cui</surname><given-names>H</given-names></name><name><surname>Hu</surname><given-names>A</given-names></name><etal/></person-group><article-title>An improved detection algorithm for ischemic stroke ncct based on yolov5</article-title><source>Diagnostics</source><year>2022</year><pub-id pub-id-type="doi">10.3390/diagnostics12112591</pub-id></element-citation><mixed-citation id="mc-CR49" publication-type="journal">Zhang L, Cui H, Hu A, et al. An improved detection algorithm for ischemic stroke ncct based on yolov5. Diagnostics. 2022. 10.3390/diagnostics12112591.</mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Akhil U, Menon V, Nambiar V, et&#x000a0;al (2023) Automated segmentation of infarct core in non-contrast ct scans of ischemic stroke patients. In: 2023 7th International conference on computer applications in electrical engineering-recent advances (CERA) pp 1&#x02013;6. 10.1109/CERA59325.2023.10455170</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Challenge G (2023) Medical image analysis challenges. <ext-link ext-link-type="uri" xlink:href="https://grand-challenge.org/challenges/">https://grand-challenge.org/challenges/</ext-link>, accessed on: 2025/05/07 12:03:21</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Kaggle (2023) Rsna intracranial hemorrhage detection. <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/data">https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/data</ext-link>, accessed on: 2025/05/07 12:03:21</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Nema (2019) Current edition digital imaging and communications in medicine. url: https://wwwdicomstandardorg/current/l Last access on 9th of May, 2023</mixed-citation></ref><ref id="CR54"><label>54.</label><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name><surname>Fatima</surname><given-names>A</given-names></name><name><surname>Shahid</surname><given-names>AR</given-names></name><name><surname>Raza</surname><given-names>B</given-names></name><etal/></person-group><article-title>State-of-the-art traditional to the machine-and deep-learning-based skull stripping techniques, models, and algorithms</article-title><source>J Digit Imaging</source><year>2020</year><volume>33</volume><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1007/s10278-020-00367-5</pub-id><pub-id pub-id-type="pmid">32076923</pub-id>
</element-citation><mixed-citation id="mc-CR54" publication-type="journal">Fatima A, Shahid AR, Raza B, et al. State-of-the-art traditional to the machine-and deep-learning-based skull stripping techniques, models, and algorithms. J Digit Imaging. 2020;33:1&#x02013;22.<pub-id pub-id-type="pmid">32076923</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">nacionales de Salud&#x000a0;y desarrolladores I (2020) 3dslicer, free and open software. url:https://wwwslicerorg/ Last access on 9th of May, 2023</mixed-citation></ref><ref id="CR56"><label>56.</label><citation-alternatives><element-citation id="ec-CR56" publication-type="journal"><person-group person-group-type="author"><name><surname>Shorten</surname><given-names>C</given-names></name><name><surname>Khoshgoftaar</surname><given-names>T</given-names></name></person-group><article-title>A survey on image data augmentation for deep learning</article-title><source>J Big Data</source><year>2019</year><volume>6</volume><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1186/s40537-019-0197-0</pub-id></element-citation><mixed-citation id="mc-CR56" publication-type="journal">Shorten C, Khoshgoftaar T. A survey on image data augmentation for deep learning. J Big Data. 2019;6:1&#x02013;4. 10.1186/s40537-019-0197-0.</mixed-citation></citation-alternatives></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Masters D, Luschi C (2018) Revisiting small batch training for deep neural networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1804.07612">arXiv:1804.07612</ext-link></mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Roth K, Milbich T, Sinha S, et&#x000a0;al (2020) Revisiting training strategies and generalization performance in deep metric learning. In: International conference on machine learning, PMLR, pp 8242&#x02013;8252</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Unipython (2020) Aumento del rendimiento con tasas de aprendizaje. url: https://unipythoncom/aumento-del-rendimiento-con-tasas-de-aprendizaje/ Last access on 9th of May, 2023</mixed-citation></ref><ref id="CR60"><label>60.</label><citation-alternatives><element-citation id="ec-CR60" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><etal/></person-group><article-title>Transfer learning with deep recurrent neural networks for remaining useful life estimation</article-title><source>Appl Sci</source><year>2018</year><volume>8</volume><fpage>2416</fpage><pub-id pub-id-type="doi">10.3390/app8122416</pub-id></element-citation><mixed-citation id="mc-CR60" publication-type="journal">Zhang A, Wang H, Li S, et al. Transfer learning with deep recurrent neural networks for remaining useful life estimation. Appl Sci. 2018;8:2416.</mixed-citation></citation-alternatives></ref><ref id="CR61"><label>61.</label><citation-alternatives><element-citation id="ec-CR61" publication-type="journal"><person-group person-group-type="author"><name><surname>Klambauer</surname><given-names>G</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Mayr</surname><given-names>A</given-names></name><etal/></person-group><article-title>Self-normalizing neural networks</article-title><source>Adv Neural Inform Process Syst</source><year>2017</year><volume>30</volume><fpage>971</fpage><lpage>980</lpage></element-citation><mixed-citation id="mc-CR61" publication-type="journal">Klambauer G, Unterthiner T, Mayr A, et al. Self-normalizing neural networks. Adv Neural Inform Process Syst. 2017;30:971&#x02013;80.</mixed-citation></citation-alternatives></ref><ref id="CR62"><label>62.</label><citation-alternatives><element-citation id="ec-CR62" publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>MC</given-names></name><name><surname>Prasad</surname><given-names>DK</given-names></name><name><surname>Lee</surname><given-names>YT</given-names></name><etal/></person-group><article-title>Semi-cnn architecture for effective spatio-temporal learning in action recognition</article-title><source>Appl Sci</source><year>2020</year><pub-id pub-id-type="doi">10.3390/app10020557</pub-id></element-citation><mixed-citation id="mc-CR62" publication-type="journal">Leong MC, Prasad DK, Lee YT, et al. Semi-cnn architecture for effective spatio-temporal learning in action recognition. Appl Sci. 2020. 10.3390/app10020557.</mixed-citation></citation-alternatives></ref></ref-list></back></article>