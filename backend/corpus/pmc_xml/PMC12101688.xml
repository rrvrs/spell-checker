<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408321</article-id><article-id pub-id-type="pmc">PMC12101688</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0322607</article-id><article-id pub-id-type="publisher-id">PONE-D-24-39865</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Mental Health and Psychiatry</subject><subj-group><subject>Dementia</subject><subj-group><subject>Alzheimer's Disease</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Neurology</subject><subj-group><subject>Dementia</subject><subj-group><subject>Alzheimer's Disease</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Medical Conditions</subject><subj-group><subject>Neurodegenerative Diseases</subject><subj-group><subject>Alzheimer's Disease</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Neurology</subject><subj-group><subject>Neurodegenerative Diseases</subject><subj-group><subject>Alzheimer's Disease</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Diagnostic Medicine</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Magnetic Resonance Imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Magnetic Resonance Imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Radiology and Imaging</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Magnetic Resonance Imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Neuroimaging</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Neuroscience</subject><subj-group><subject>Cognitive Neurology</subject><subj-group><subject>Cognitive Impairment</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Neuroscience</subject><subj-group><subject>Cognitive Neurology</subject><subj-group><subject>Cognitive Impairment</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Neurology</subject><subj-group><subject>Cognitive Neurology</subject><subj-group><subject>Cognitive Impairment</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Gene Expression</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Hippocampus</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Hippocampus</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Diagnostic Medicine</subject></subj-group></subj-group></article-categories><title-group><article-title>Leveraging transformers and explainable AI for Alzheimer&#x02019;s disease interpretability</article-title><alt-title alt-title-type="running-head">Use of transformer and XAI for Alzheimer&#x02019;s disease detection</alt-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Anzum</surname><given-names>Humaira</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0001-0831-136X</contrib-id><name><surname>Sammo</surname><given-names>Nabil Sadd</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x02013; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="cor001" ref-type="corresp"/></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1408-9133</contrib-id><name><surname>Akhter</surname><given-names>Shamim</given-names></name><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>AISIP Lab, Ahsanullah University of Science and Technology, Dhaka, Bangladesh</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Bangladesh University of Engineering and Technology, Dhaka, Bangladesh</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Uysal</surname><given-names>Fatih</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Kafkas University, T&#x000dc;RKIYE</addr-line>
</aff><author-notes><corresp id="cor001">* E-mail: <email>nabilsadd.araf.999@gmail.com</email></corresp><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>There are no competing interests for biasing in this work.</p></fn></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0322607</elocation-id><history><date date-type="received"><day>12</day><month>9</month><year>2024</year></date><date date-type="accepted"><day>25</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Anzum et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Anzum et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0322607.pdf"/><abstract><p>Alzheimer&#x02019;s disease (AD) is a progressive brain ailment that causes memory loss, cognitive decline, and behavioral changes. It is quite concerning that one in nine adults over the age of 65 have AD. Currently there is almost no cure for AD except very few experimental treatments. However, early detection offers chances to take part in clinical trials or other investigations looking at potential new and effective Alzheimer&#x02019;s treatments. To detect Alzheimer&#x02019;s disease, brain scans such as computed tomography (CT), magnetic resonance imaging (MRI), or positron emission tomography (PET) can be performed. Many researches have been undertaken to use computer vision on MRI images, and their accuracy ranges from 80&#x02013;90%, new computer vision algorithms and cutting-edge transformers have the potential to improve this performance.We utilize advanced transformers and computer vision algorithms to enhance diagnostic accuracy, achieving an impressive 99% accuracy in categorizing Alzheimer&#x02019;s disease stages through translating RNA text data and brain MRI images in near-real-time. We integrate the Local Interpretable Model-agnostic Explanations (LIME) explainable AI (XAI) technique to ensure the transformers&#x02019; acceptance, reliability, and human interpretability. LIME helps identify crucial features in RNA sequences or specific areas in MRI images essential for diagnosing AD.</p></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="8"/><table-count count="7"/><page-count count="21"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data are within the manuscript and its Supporting Information files.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data are within the manuscript and its Supporting Information files.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>1 Introduction</title><p>Alzheimer&#x02019;s disease (AD) typically begins with mild memory loss and cognitive decline, representing a progressive and degenerative brain disorder. The absence of preventive medications utilizing modern medical technologies raises concerns about the escalating number of AD patients in the coming decades, imposing significant strain on patients, caregivers, and healthcare systems [<xref rid="pone.0322607.ref001" ref-type="bibr">1</xref>]. Despite the widespread prevalence and severity of the condition, effective therapy for Alzheimer&#x02019;s disease remains elusive. Current diagnostic tests necessitate a comprehensive understanding of the patient&#x02019;s medical history but often fail to provide conclusive results during the patient&#x02019;s lifetime [<xref rid="pone.0322607.ref002" ref-type="bibr">2</xref>].</p><p>Early diagnosis of AD is crucial for timely intervention. However, the diagnosis of AD is often delayed, with a substantial time gap between the onset of symptoms and confirmation. Individuals in the early stages are frequently identified with mild cognitive impairment (MCI), a precursor to Alzheimer&#x02019;s disease [<xref rid="pone.0322607.ref003" ref-type="bibr">3</xref>]. Various assessments, including cognitive tests, blood tests, neurological evaluations [<xref rid="pone.0322607.ref004" ref-type="bibr">4</xref>], and brain imaging techniques such as computed tomography (CT) [<xref rid="pone.0322607.ref005" ref-type="bibr">5</xref>], magnetic resonance imaging (MRI) [<xref rid="pone.0322607.ref006" ref-type="bibr">6</xref>], and positron emission tomography (PET) [<xref rid="pone.0322607.ref007" ref-type="bibr">7</xref>], are employed for detection. In traditional research, manually extracting regions of interest such as the hippocampus and amygdala has been the norm for identifying AD characteristics. Recognizing AD early is also essential for enrolling patients in clinical trials or investigative studies.</p><p>Machine Learning (ML) offers a promising avenue for improving diagnostic accuracy in Alzheimer&#x02019;s disease. The well-established presence of neurological changes in AD-related MRI scans justifies evaluating the effectiveness of ML approaches. Many ML-based methods for AD diagnosis rely on conventional techniques like support vector machines (SVM), logistic regression (LR), linear program boosting methods (LPBM), and support vector machine-recursive feature elimination (SVM-RFE) to analyze patterns and predict AD progression. In addition, ML models can integrate gene expression data to enhance AD detection capabilities. However, traditional ML approaches often require domain-specific expertise to identify valuable features, limiting their accessibility.</p><p>Deep Learning (DL), a subset of ML, overcomes these limitations by enabling incremental feature learning. DL models are highly effective at recognizing complex patterns and processing large datasets. The traditional recurrent neural networks (RNNs) suffer from the vanishing gradient problem [<xref rid="pone.0322607.ref008" ref-type="bibr">8</xref>], which limits their ability to capture long-range dependencies in sequential data. A notable advancement in DL is the development of the Transformer architecture, which employs attention mechanisms to capture relationships between elements in data. Initially designed for Natural Language Processing (NLP), the Transformer&#x02019;s architecture has been adapted for image analysis, resulting in the Visual Transformer (ViT). ViT is better than conventional CNNs because they capture global dependencies in images through self-attention mechanisms. CNNs particularly focus on local patterns while ViTs can capture intricate patterns in an image due to their ability to their ability to extract long range dependencies across the entire image, rather than just local features like CNNs. Also, ViTs offer more flexibility and scalability, especially with large datasets. This model utilizes parallel processing to enhance speed, capacity, and accuracy in image-based applications.</p><p>Despite these advancements, the application of transformer-based models in AD diagnosis remains relatively underexplored. Notably, Convolutional Neural Networks (CNNs) and SpinalNet have demonstrated considerable accuracy in determining different stages of AD [<xref rid="pone.0322607.ref009" ref-type="bibr">9</xref>]. However, there is a gap in incorporating explainable artificial intelligence (XAI) techniques into transformer-based models.</p><p>This limitation highlights the need for further investigation into the integration of XAI tools to enhance interpretability and reliability in AD diagnostics. XAI can provide detailed explanations for model predictions, aiding clinicians in understanding the reasoning behind diagnostic outcomes. This is particularly important in a critical field like AD detection, where transparency and trust are essential for ensuring clinical adoption and improving patient outcomes</p><p>In this study, we aim to utilize transformer models for Alzheimer&#x02019;s Disease (AD) detection and evaluate their effectiveness in translating RNA text data sequences and brain MRI images. The key contributions of this work include:</p><list list-type="simple"><list-item><p>(a) Implementation and evaluation of attention-based transformers with encoder-decoder architecture on AD datasets, specifically RNA sequences and MRI images.</p></list-item><list-item><p>(b) Formation of a distinctive architecture by combining vision and text transformers, designed for processing both text sequences and MRI image data.</p></list-item><list-item><p>(c) Integration of an explainable artificial intelligence (XAI) method, LIME, with the transformer. This inclusion aims to interpret the crucial features in sequence and image datasets that are essential for identifying an individual with AD.</p></list-item></list><p>The paper&#x02019;s structure is outlined as follows: Section <xref rid="sec001" ref-type="sec">1</xref> presents the introduction, Section <xref rid="sec002" ref-type="sec">2</xref> summarize previous works. In Section <xref rid="sec003" ref-type="sec">3</xref>, we present the datasets, highlighting the interconnection between input attributes. Section <xref rid="sec006" ref-type="sec">4</xref> details the methodology and architecture of the proposed framework. The experimental approach, results, and analysis are elucidated in Section <xref rid="sec015" ref-type="sec">5</xref>. Section <xref rid="sec024" ref-type="sec">6</xref> provides the discussion, and Section <xref rid="sec025" ref-type="sec">7</xref> concludes the study, while Section <xref rid="sec026" ref-type="sec">8</xref> briefly discusses future works in this field.</p></sec><sec id="sec002"><title>2 Literature review</title><p>Various earlier studies in the field have explored the prediction of Alzheimer&#x02019;s disease (AD) using diverse techniques, including machine learning (ML) and computer vision. In one study [<xref rid="pone.0322607.ref010" ref-type="bibr">10</xref>], RNA-Seq methodology demonstrated greater effectiveness than microarray analysis in assessing gene expression profiles. Machine learning models applied to Differentially Expressed Genes (DEGs) identified 740 DEGs (361 upregulated and 379 downregulated) in AD patients with varying lifespans. The Robust Rank Aggregation (RRA) technique facilitated meta-analysis of DEGs across multiple microarray platforms.</p><p>In another study [<xref rid="pone.0322607.ref011" ref-type="bibr">11</xref>], researchers utilized three large blood gene expression datasets (ANM1, ANM2, and ADNI) to identify genes associated with AD and classify patients using ML techniques. The study employed two procedures: extracting DEGs and feature engineering to select informative genes from the training set. Five classification techniques (logistic regression, L1-regularized logistic regression, Support Vector Machines [SVM], Random Forest [RF], and Deep Neural Networks [DNN]) were used to develop a prediction model. Both internal and external validation showed promising results, suggesting that blood-based biomarkers could advance AD diagnostics and treatments. However, challenges such as data heterogeneity, sample size variability, and RNA quality need to be addressed.</p><p>Hind Alamro <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref012" ref-type="bibr">12</xref>] combined three brain tissue-based AD GEO datasets, resulting in 189 AD samples and 256 non-AD samples. They identified 924 DEGs and used RF, SVM, DNN, and Convolutional Neural Networks (CNNs) for prediction. Using genes selected through LASSO and Ridge algorithms, their models achieved an Area Under the Curve (AUC) of 97.9% on independent test datasets, demonstrating the efficacy of these feature selection and classification methods.</p><p>Bhatkoti <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref013" ref-type="bibr">13</xref>] proposed a hybrid multi-class deep learning (DL) framework for early AD diagnosis. Their enhanced k-Sparse Autoencoder (KSA) algorithm identified degraded brain regions using MRI, cerebrospinal fluid (CSF), and positron emission tomography (PET) images from the ADNI dataset. The modified KSA achieved an accuracy of 83.143% compared to 71.327% with traditional methods. However, the need for manual tuning in determining sparsity levels (k) remains a limitation.</p><p>Tran <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref014" ref-type="bibr">14</xref>] introduced a computational strategy combining CNN and Gaussian Mixture Model (GMM) for brain tissue segmentation, followed by a hybrid classification model utilizing Extreme Gradient Boosting (XGBoost) and SVM. Their approach yielded high classification accuracies (0.88 and 0.80) on two datasets, with segmentation Dice coefficients of 0.96. The authors suggested further segmentation of specific brain tissues to improve precision, particularly for datasets with complex anatomical changes due to aging.</p><p>Cheng <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref015" ref-type="bibr">15</xref>] proposed the use of 3D-CNNs to extract features from MRI brain scans, achieving an accuracy of 87.15% and an AUC of 92.26% on the ADNI dataset. This automated method effectively identified key features for AD classification. Similarly, Isik <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref016" ref-type="bibr">16</xref>] achieved approximately 80% accuracy using CNNs for sMR brain images from the OASIS and MIRIAD datasets. However, they noted challenges in distinguishing mild cognitive impairment from AD.</p><p>Wang <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref017" ref-type="bibr">17</xref>] developed a multimodal deep learning framework for Alzheimer&#x02019;s disease dementia assessment, integrating data from neuroimaging, genetic markers, and cognitive tests. Their approach combines CNNs and recurrent neural networks (RNNs) to capture both spatial and temporal patterns in the data. The study shows that multimodal data fusion significantly improves diagnostic accuracy compared to single-modality approaches. The authors reported an accuracy of 92.1% for Alzheimer&#x02019;s disease (AD) diagnosis using their multimodal deep learning framework, which integrated neuroimaging, genetic markers, and cognitive tests. This multi-modal approach was quite a new addition to the field of AD diagnosis at the time of publication but new state-of-the-art algorithms like models can be used for further accuracy improvements.</p><p>Another study conducted by Liu M <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref018" ref-type="bibr">18</xref>] proposed a deep learning system for the differential diagnosis of Alzheimer&#x02019;s disease (AD) and mild cognitive impairment (MCI) using structural MRI. Their model leverages 3D convolutional neural networks (CNNs) to extract features from brain scans, achieving high accuracy in distinguishing between AD, MCI, and healthy controls. The study highlights the potential of deep learning in automating AD diagnosis and emphasizes the importance of structural MRI as a key biomarker for early detection. reported an accuracy of 88.6% for differentiating Alzheimer&#x02019;s disease (AD) from healthy controls and 76.5% for distinguishing mild cognitive impairment (MCI) from healthy controls using their deep learning system based on structural MRI. The accuracy still remains relatively low due to the use of CNN based architectures and the usability of moderately accurate model can cause serious discrepancy in the field of AD detection.</p><p>In contrast, G. Kwon <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref019" ref-type="bibr">19</xref>] achieved 96.12% accuracy using a CNN pipeline inspired by ResNet and ConvMixer, which reduced computational complexity while effectively classifying AD stages. Other studies, such as that by Tufail <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref020" ref-type="bibr">20</xref>], demonstrated the superiority of transfer learning over traditional CNNs, achieving 77.23% accuracy in binary AD classification with InceptionV3 and Xception architectures. Thamaraiselvi <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref021" ref-type="bibr">21</xref>] utilized DenseNet 169 for AD identification, achieving a validation accuracy of 82.23%, highlighting its efficacy in transfer learning-based approaches.</p><p>Sarwar Kamal <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref009" ref-type="bibr">9</xref>] explored SpinalNet and CNN for AD stage classification, achieving accuracies of 89.6% and 96.6%, respectively. They further analyzed gene expression data using SVM, k-Nearest Neighbors (KNN), and XGBoost, with SVM reaching an accuracy of 82.4%. The study employed the LIME explainable AI library to elucidate the role of genes in classification, enhancing interpretability.</p><p>Recent work by [<xref rid="pone.0322607.ref022" ref-type="bibr">22</xref>] applied transformer-based models, including Swin Transformer, Vision Transformer (ViT), and Bidirectional Encoder Representation from Image Transformers (BEiT), to classify Alzheimer&#x02019;s and Parkinson&#x02019;s diseases using brain imaging data. The study utilized a balanced dataset of 450 brain images, achieving classification accuracy exceeding 80%, with ViT demonstrating the highest performance (94.4% accuracy, 94.7% precision). While the results highlight the efficacy of transformer architectures in disease detection, the study has notable shortcomings. The dataset size (450 images) is relatively small, which may limit the generalizability of the findings. Also, the accuracy is not satisfactory in terms of AD disease diagnosis.</p><p>Transformer models, initially designed for natural language processing, have recently gained attention for their ability to handle long sequences through attention mechanisms and parallel processing. Despite their success in various domains, their application in AD diagnosis remains underexplored. This study addresses this gap by applying transformer-based models to both MRI and mRNA sequencing data, leveraging their capacity to identify novel biomarkers and complex patterns. Unlike conventional DL models like CNNs and Recurrent Neural Networks (RNNs), transformers overcome limitations such as vanishing gradients and sequential processing inefficiencies. Incorporating the LIME explanation technique enhances the interpretability of transformer models, bridging the gap between performance and clinical usability. An overview of the studies reviewed in this work, including their algorithms, data types, and accuracies, is summarized in <xref rid="pone.0322607.t001" ref-type="table">Table 1</xref>, while their originality, strengths, and limitations are presented in <xref rid="pone.0322607.t002" ref-type="table">Table 2</xref>.</p><table-wrap position="float" id="pone.0322607.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.t001</object-id><label>Table 1</label><caption><title>Algorithms, data type, and accuracy.</title></caption><alternatives><graphic xlink:href="pone.0322607.t001" id="pone.0322607.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Author</th><th align="left" rowspan="1" colspan="1">Algorithm</th><th align="left" rowspan="1" colspan="1">Data type</th><th align="left" rowspan="1" colspan="1">Accuracies</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Lee <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref011" ref-type="bibr">11</xref>]</td><td align="left" rowspan="1" colspan="1">L1-Regularized (LR), SVM, RF, DNN</td><td align="left" rowspan="1" colspan="1">Gene Expression</td><td align="left" rowspan="1" colspan="1">87.40% (ANMI1), 80.40% (ANM2), 65.70% (ADNI)</td></tr><tr><td align="left" rowspan="1" colspan="1">Cheng <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref015" ref-type="bibr">15</xref>]</td><td align="left" rowspan="1" colspan="1">3D-CNN</td><td align="left" rowspan="1" colspan="1">MRI Images</td><td align="left" rowspan="1" colspan="1">87.12% (ADNI)</td></tr><tr><td align="left" rowspan="1" colspan="1">Ahsan Bil Tufail <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref020" ref-type="bibr">20</xref>]</td><td align="left" rowspan="1" colspan="1">2D-CNN</td><td align="left" rowspan="1" colspan="1">MRI Images</td><td align="left" rowspan="1" colspan="1">77.23% (OASIS)</td></tr><tr><td align="left" rowspan="1" colspan="1">Kwon <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref019" ref-type="bibr">19</xref>]</td><td align="left" rowspan="1" colspan="1">CNN</td><td align="left" rowspan="1" colspan="1">MRI Images</td><td align="left" rowspan="1" colspan="1">96.12% (ADNI)</td></tr><tr><td align="left" rowspan="1" colspan="1">Isik <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref016" ref-type="bibr">16</xref>]</td><td align="left" rowspan="1" colspan="1">CNN</td><td align="left" rowspan="1" colspan="1">MRI Images</td><td align="left" rowspan="1" colspan="1">80% (OASIS, MIRIAD)</td></tr><tr><td align="left" rowspan="1" colspan="1">Thamaraiselvi <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref021" ref-type="bibr">21</xref>]</td><td align="left" rowspan="1" colspan="1">DenseNet</td><td align="left" rowspan="1" colspan="1">MRI Images</td><td align="left" rowspan="1" colspan="1">82.23% (OASIS)</td></tr><tr><td align="left" rowspan="1" colspan="1">Md. Sarwar Kamal <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref009" ref-type="bibr">9</xref>]</td><td align="left" rowspan="1" colspan="1">SpinalNet, CNN, SVC</td><td align="left" rowspan="1" colspan="1">MRI Images, Gene Expression</td><td align="left" rowspan="1" colspan="1">89.60% (MRI), 96.60% (MRI), 82.40% (Gene)</td></tr><tr><td align="left" rowspan="1" colspan="1">Hind Alamro <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref012" ref-type="bibr">12</xref>]</td><td align="left" rowspan="1" colspan="1">RF, SVM, DNN, CNN</td><td align="left" rowspan="1" colspan="1">Gene Expression</td><td align="left" rowspan="1" colspan="1">97.9% AUC</td></tr><tr><td align="left" rowspan="1" colspan="1">Bhatkoti <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref013" ref-type="bibr">13</xref>]</td><td align="left" rowspan="1" colspan="1">Modified KSA</td><td align="left" rowspan="1" colspan="1">MRI, CSF, PET</td><td align="left" rowspan="1" colspan="1">83.143%</td></tr><tr><td align="left" rowspan="1" colspan="1">Tran <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref014" ref-type="bibr">14</xref>]</td><td align="left" rowspan="1" colspan="1">CNN, GMM, XGBoost, SVM</td><td align="left" rowspan="1" colspan="1">MRI Images</td><td align="left" rowspan="1" colspan="1">0.88, 0.80 (Dice coefficients)</td></tr><tr><td align="left" rowspan="1" colspan="1">Wang <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref017" ref-type="bibr">17</xref>]</td><td align="left" rowspan="1" colspan="1">CNN, RNN</td><td align="left" rowspan="1" colspan="1">Multimodal (MRI, Genetic, Cognitive)</td><td align="left" rowspan="1" colspan="1">92.1%</td></tr><tr><td align="left" rowspan="1" colspan="1">Liu <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref018" ref-type="bibr">18</xref>]</td><td align="left" rowspan="1" colspan="1">3D-CNN</td><td align="left" rowspan="1" colspan="1">MRI Images</td><td align="left" rowspan="1" colspan="1">88.6% (AD), 76.5% (MCI)</td></tr></tbody></table></alternatives></table-wrap><table-wrap position="float" id="pone.0322607.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.t002</object-id><label>Table 2</label><caption><title>Originality, plus aspects, and minus aspects.</title></caption><alternatives><graphic xlink:href="pone.0322607.t002" id="pone.0322607.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Author</th><th align="left" rowspan="1" colspan="1">Originality</th><th align="left" rowspan="1" colspan="1">Plus Aspects</th><th align="left" rowspan="1" colspan="1">Minus Aspects</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">T. Lee <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref011" ref-type="bibr">11</xref>]</td><td align="left" rowspan="1" colspan="1">Blood-based biomarkers for AD diagnosis</td><td align="left" rowspan="1" colspan="1">High interpretability, external validation</td><td align="left" rowspan="1" colspan="1">Limited by data heterogeneity and small sample sizes</td></tr><tr><td align="left" rowspan="1" colspan="1">D. Cheng <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref015" ref-type="bibr">15</xref>]</td><td align="left" rowspan="1" colspan="1">Automated feature extraction from 3D MRI</td><td align="left" rowspan="1" colspan="1">High accuracy, robust feature extraction</td><td align="left" rowspan="1" colspan="1">Limited to single modality (MRI)</td></tr><tr><td align="left" rowspan="1" colspan="1">Ahsan Bil Tufail <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref020" ref-type="bibr">20</xref>]</td><td align="left" rowspan="1" colspan="1">Transfer learning with InceptionV3 and Xception</td><td align="left" rowspan="1" colspan="1">Reduced computational complexity</td><td align="left" rowspan="1" colspan="1">Lower accuracy compared to state-of-the-art</td></tr><tr><td align="left" rowspan="1" colspan="1">G. Kwon <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref019" ref-type="bibr">19</xref>]</td><td align="left" rowspan="1" colspan="1">ResNet and ConvMixer-inspired pipeline</td><td align="left" rowspan="1" colspan="1">High accuracy, reduced computational complexity</td><td align="left" rowspan="1" colspan="1">Limited interpretability of model decisions</td></tr><tr><td align="left" rowspan="1" colspan="1">Z. Isik <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref016" ref-type="bibr">16</xref>]</td><td align="left" rowspan="1" colspan="1">CNN for sMRI classification</td><td align="left" rowspan="1" colspan="1">Effective for sMRI data</td><td align="left" rowspan="1" colspan="1">Difficulty distinguishing MCI from AD</td></tr><tr><td align="left" rowspan="1" colspan="1">D. Thamaraiselvi <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref021" ref-type="bibr">21</xref>]</td><td align="left" rowspan="1" colspan="1">Transfer learning with DenseNet 169</td><td align="left" rowspan="1" colspan="1">High efficacy in transfer learning</td><td align="left" rowspan="1" colspan="1">Moderate accuracy, limited to MRI data</td></tr><tr><td align="left" rowspan="1" colspan="1">Md. Sarwar Kamal <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref009" ref-type="bibr">9</xref>]</td><td align="left" rowspan="1" colspan="1">SpinalNet for AD classification, LIME for interpretability</td><td align="left" rowspan="1" colspan="1">High accuracy, explainable AI integration</td><td align="left" rowspan="1" colspan="1">Limited by dataset size and complexity</td></tr><tr><td align="left" rowspan="1" colspan="1">Hind Alamro <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref012" ref-type="bibr">12</xref>]</td><td align="left" rowspan="1" colspan="1">Integration of multiple GEO datasets</td><td align="left" rowspan="1" colspan="1">High AUC, robust feature selection</td><td align="left" rowspan="1" colspan="1">Limited to gene expression data</td></tr><tr><td align="left" rowspan="1" colspan="1">Bhatkoti <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref013" ref-type="bibr">13</xref>]</td><td align="left" rowspan="1" colspan="1">Hybrid multi-class DL framework</td><td align="left" rowspan="1" colspan="1">Improved accuracy over traditional methods</td><td align="left" rowspan="1" colspan="1">Requires manual tuning of sparsity levels</td></tr><tr><td align="left" rowspan="1" colspan="1">Tran <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref014" ref-type="bibr">14</xref>]</td><td align="left" rowspan="1" colspan="1">Hybrid segmentation and classification model</td><td align="left" rowspan="1" colspan="1">High Dice coefficients, effective segmentation</td><td align="left" rowspan="1" colspan="1">Complex pipeline, limited to specific datasets</td></tr><tr><td align="left" rowspan="1" colspan="1">Wang <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref017" ref-type="bibr">17</xref>]</td><td align="left" rowspan="1" colspan="1">Multimodal data fusion for AD diagnosis</td><td align="left" rowspan="1" colspan="1">High accuracy, leverages multiple data sources</td><td align="left" rowspan="1" colspan="1">Computationally intensive, requires full multimodal data</td></tr><tr><td align="left" rowspan="1" colspan="1">Liu <italic toggle="yes">et al</italic>. [<xref rid="pone.0322607.ref018" ref-type="bibr">18</xref>]</td><td align="left" rowspan="1" colspan="1">Deep learning for AD and MCI differentiation</td><td align="left" rowspan="1" colspan="1">High accuracy for AD, robust feature extraction</td><td align="left" rowspan="1" colspan="1">Lower accuracy for MCI, limited generalizability</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec003"><title>3 Dataset description</title><p>This section provides an overview of the datasets utilized in our study, encompassing MRI images and mRNA sequencing data in tabular CSV format.</p><sec id="sec004"><title>3.1 MRI image dataset (Alzheimer&#x02019;s dataset)</title><p>We utilized the Alzheimer&#x02019;s dataset from Kaggle [<xref rid="pone.0322607.ref024" ref-type="bibr">24</xref>], which includes gray MRI scans of the brain from individuals in various stages of Alzheimer&#x02019;s disease. The dataset encompasses four classes: MildDemented, ModerateDemented, NonDemented, and VeryMildDemented. It is organized into two folders: Train and Test, comprising a total of 5121 training images across the four classes and 1379 test images. Table ?? illustrates the distribution of images in the dataset. This dataset is valuable for predicting Alzheimer&#x02019;s disease stages using computer vision algorithms. Given the limited size of the test set, we allocated 10% of the training dataset for validation to ensure a robust evaluation of the model&#x02019;s performance. This approach was necessary because the dataset did not provide a separate validation set, and splitting the training set further would have risked overfitting due to the imbalanced class distribution. Using a small portion of the test set for validation allowed us to tune hyper-parameters and monitor model performance during training without significantly compromising the final evaluation on the remaining test data.</p><table-wrap position="float" id="pone.0322607.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.t003</object-id><label>Table 3</label><caption><title>Dataset split for Alzheimer&#x02019;s disease classes.</title></caption><alternatives><graphic xlink:href="pone.0322607.t003" id="pone.0322607.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Type</th><th align="left" rowspan="1" colspan="1">Moderate</th><th align="left" rowspan="1" colspan="1">Mild</th><th align="left" rowspan="1" colspan="1">Non</th><th align="left" rowspan="1" colspan="1">Very Mild</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Train</td><td align="left" rowspan="1" colspan="1">645</td><td align="left" rowspan="1" colspan="1">137</td><td align="left" rowspan="1" colspan="1">2304</td><td align="left" rowspan="1" colspan="1">1613</td></tr><tr><td align="left" rowspan="1" colspan="1">Validation</td><td align="left" rowspan="1" colspan="1">72</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">256</td><td align="left" rowspan="1" colspan="1">179</td></tr><tr><td align="left" rowspan="1" colspan="1">Test</td><td align="left" rowspan="1" colspan="1">179</td><td align="left" rowspan="1" colspan="1">112</td><td align="left" rowspan="1" colspan="1">640</td><td align="left" rowspan="1" colspan="1">448</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec005"><title>3.2 NCBI&#x02019;s RNA-sequencing datasets</title><p>In this dataset, a comprehensive collection of 191,890 nuclei associated with Alzheimer&#x02019;s disease (AD) has been incorporated at the single-nucleus level, providing multi-omic information. The dataset captures significant cellular heterogeneity by concurrently assessing chromatin accessibility and gene expression in the same biological samples. Leveraging single-nucleus ATAC-sequencing and RNA-sequencing, this dataset serves as a multi-omics exploration of Alzheimer&#x02019;s Disease in human brain tissue, accessible through the National Center for Biotechnology Information (NCBI) database under Accession Number: 174367 [<xref rid="pone.0322607.ref025" ref-type="bibr">25</xref>].</p><p>The assay for transpose-accessible chromatin with sequencing (ATAC-Seq) is a prevalent technique employed for evaluating chromatin accessibility throughout the genome. This method involves sequencing open chromatin regions, allowing the determination of how chromatin packaging and other factors influence gene expression. ATAC-sequencing is instrumental in detecting open chromatin regions and regulatory elements in the genome, facilitating the identification of locations potentially responsible for unregulated gene expression associated with Alzheimer&#x02019;s disease. The dataset integrates ATAC and RNA sequencing data, complemented by crucial factors such as age and gender, forming a binary classification dataset. This comprehensive dataset enables objective comparisons of gene expression levels between AD patients and healthy individuals, shedding light on the molecular underpinnings of the disease.</p></sec></sec><sec sec-type="materials|methods" id="sec006"><title>4 Methodology</title><p>In our methodology, we employ a unified network of vision and text transformers. To classify MRI images, we utilize a vision transformer algorithm, while a text transformer is employed for determining the presence of Alzheimer&#x02019;s Disease (AD) using mRNA sequence data. The vision transformer algorithm was initially introduced by Alexey Dosovitskiy <italic toggle="yes">et al</italic>. in [<xref rid="pone.0322607.ref026" ref-type="bibr">26</xref>]. These individual models are then assembled to form a cohesive transformer model, applicable for both MRI images and mRNA sequencing data. To enhance interpretability, we incorporate the LIME algorithm, providing explanations for these transformer models. This combined approach allows for a comprehensive analysis of both imaging and textual data in the context of AD diagnosis.</p><sec id="sec007"><title>4.1 Proposed transformer model architecture</title><p>In our study, we deal with two types of input data&#x02014;images and sequential data. For processing MRI images, we utilize a patch division layer that divides the images into fixed-size, non-overlapping patches, treating each patch as a meaningful unit. On the other hand, when working with mRNA sequencing data, numerical data are converted into word sequences, allowing us to represent the information as sequences of words. This dual approach, involving patch embedding for images and word sequence conversion for sequential data, enables our model to effectively handle both data types, contributing to a comprehensive analysis for Alzheimer&#x02019;s Disease diagnosis. <xref rid="pone.0322607.g001" ref-type="fig">Fig. 1</xref> depitcs the unified network of Vision and text transformers.</p><fig position="float" id="pone.0322607.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.g001</object-id><label>Fig 1</label><caption><title>The unified network of vision and text transformers.</title></caption><graphic xlink:href="pone.0322607.g001" position="float"/></fig><sec id="sec008"><title>4.1.1 Patch embedding (for MRI images).</title><p>In order to accommodate 2D images, we transform the image <inline-formula id="pone.0322607.e001"><alternatives><graphic xlink:href="pone.0322607.e001.jpg" id="pone.0322607.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>W</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> into a sequence of flattened 2D patches <inline-formula id="pone.0322607.e002"><alternatives><graphic xlink:href="pone.0322607.e002.jpg" id="pone.0322607.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>&#x000b7;</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pone.0322607.e003"><alternatives><graphic xlink:href="pone.0322607.e003.jpg" id="pone.0322607.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mn>640</mml:mn><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mn>640</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> represents the original image resolution, <italic toggle="yes">C</italic>&#x02009;=&#x02009;3 is the number of channels, <inline-formula id="pone.0322607.e004"><alternatives><graphic xlink:href="pone.0322607.e004.jpg" id="pone.0322607.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the resolution of each image patch, and <inline-formula id="pone.0322607.e005"><alternatives><graphic xlink:href="pone.0322607.e005.jpg" id="pone.0322607.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>400</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> is the resulting number of patches. This <italic toggle="yes">N</italic> value also serves as the effective input sequence length for the transformer.Output class xclass is also prepared as a learnable class embedding to the sequence of embedded patches. Throughout all its layers, the transformer maintains a constant latent vector size denoted as D. Consequently, we flatten the patches and project them onto D dimensions using a trainable linear projection, as expressed by <xref rid="pone.0322607.e006" ref-type="disp-formula">Eq. 1</xref> [<xref rid="pone.0322607.ref020" ref-type="bibr">20</xref>]. Position embeddings are added to the output of this projection to retain positional information.</p><disp-formula id="pone.0322607.e006"><alternatives><graphic xlink:href="pone.0322607.e006.jpg" id="pone.0322607.e006g" position="anchor"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mtext>class</mml:mtext></mml:mrow></mml:msub><mml:mi>;</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mi>E</mml:mi><mml:mi>;</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mi>E</mml:mi><mml:mi>;</mml:mi><mml:mi>&#x02026;</mml:mi><mml:mi>;</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mi>E</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>pos</mml:mtext></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mi>&#x02026;</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><p>where <inline-formula id="pone.0322607.e007"><alternatives><graphic xlink:href="pone.0322607.e007.jpg" id="pone.0322607.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>E</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>&#x000b7;</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>&#x000d7;</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0322607.e008"><alternatives><graphic xlink:href="pone.0322607.e008.jpg" id="pone.0322607.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>pos</mml:mtext></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211d;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>&#x000d7;</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p></sec><sec id="sec009"><title>4.1.2 Token embedding (mRNA sequencing datasets).</title><p>The standard text transformer receives as input a 1D mRNA sequence of tokens, where these tokens are words or sub-words in combination with different numbers and characters. Each token is associated with an embedding vector (512 size), and an embedding layer is utilized to link each token in the input sequence to its respective embedding vector. This approach enables the model to handle continuous vector representations of discrete tokens, enhancing its ability to process and comprehend the information embedded in the input sequences more effectively. The mode_max_length attribute is set to 512, which indicates that the maximum length of the input sequence that the text transformer model can handle is 512 tokens.</p></sec><sec id="sec010"><title>4.1.3 Positional encoding.</title><p>In the process of patch embedding, positional encoding values are computed based on the positions of patches within the image grid. Each position in the grid corresponds to a unique set of positional encoding values. For each patch embedding, these positional encoding values are element-wise added. This addition imbues the patch embeddings with information regarding their spatial positions relative to other patches in the image. Contrastingly, in the context of token embeddings, each token is assigned a distinct position. These positions signify the order of the tokens within the sequence. Positively, sinusoidal functions are employed to generate positional encodings. These functions generate a series of continuous values that smoothly change in all directions. The positional encoding values for each token embedding are then added element-wise. In both cases, the resulting sequence of embedding vectors serves as input to the encoder layer.</p></sec><sec id="sec011"><title>4.1.4 Transformer encoder layer.</title><p>A multi-headed attention (MHA) mechanism, a 2-layer MLP, layer normalization, and residual connections are all included in the encoder component. Around each sub-layer, residual connections and layer normalization are used to improve information flow. The dimension of each sub-layer and embedding layer output is <inline-formula id="pone.0322607.e009"><alternatives><graphic xlink:href="pone.0322607.e009.jpg" id="pone.0322607.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mtext>model</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. We have implemented Z-Score Normalization in the hidden layer, it involves dividing each activation value by the standard deviation after subtracting the mean value from each activation. This results in a new set of activation functions with a mean of 0 and a standard deviation of 1. Notably, the normalization is conducted independently for each channel in the output tensor.</p><p>The following <xref rid="pone.0322607.e010" ref-type="disp-formula">Eq 2</xref> corresponds to the multi-head attention (MHA) step.</p><disp-formula id="pone.0322607.e010"><alternatives><graphic xlink:href="pone.0322607.e010.jpg" id="pone.0322607.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>MHA</mml:mtext><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mtext>AT</mml:mtext><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mtext>LN</mml:mtext><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>&#x02026;</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label></disp-formula><disp-formula id="pone.0322607.e011"><alternatives><graphic xlink:href="pone.0322607.e011.jpg" id="pone.0322607.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>AT</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>softmax</mml:mtext><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>q</mml:mi><mml:msup><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(3)</label></disp-formula><disp-formula id="pone.0322607.e012"><alternatives><graphic xlink:href="pone.0322607.e012.jpg" id="pone.0322607.e012g" position="anchor"/><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>MLP</mml:mtext><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mtext>LN</mml:mtext><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msubsup><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>l</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>&#x02026;</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(4)</label></disp-formula><disp-formula id="pone.0322607.e013"><alternatives><graphic xlink:href="pone.0322607.e013.jpg" id="pone.0322607.e013g" position="anchor"/><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mtext>LN</mml:mtext><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msubsup><mml:mi>z</mml:mi><mml:mi>L</mml:mi><mml:mn>0</mml:mn></mml:msubsup><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(5)</label></disp-formula><p>The Attention (AT) function operates within the Multi-Head Attention (MHA) function, involving three vectors: the query (<italic toggle="yes">q</italic>), key (<italic toggle="yes">k</italic>), and value (<inline-formula id="pone.0322607.e014"><alternatives><graphic xlink:href="pone.0322607.e014.jpg" id="pone.0322607.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>). The key vector is employed to compute the attention score with the query vector, involving the transpose of the key. To normalize this computation, it is divided by the square root of <italic toggle="yes">d</italic><sub><italic toggle="yes">k</italic></sub>, representing the dimension of the keys. Subsequently, the softmax function is applied to the result, resulting in scaled dot product attention. These dot products are employed across six attention layers, known as heads, as detailed in <xref rid="pone.0322607.e011" ref-type="disp-formula">Eq. 3</xref>. <xref rid="pone.0322607.e012" ref-type="disp-formula">Eq. 4</xref> corresponds to the step involving a multilayer perceptron (MLP). A 2-layer MLP is utilized for pre-training, and the network&#x02019;s final output is a vector with dimensions (1, <inline-formula id="pone.0322607.e015"><alternatives><graphic xlink:href="pone.0322607.e015.jpg" id="pone.0322607.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>cls</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>), containing probabilities for each of the <inline-formula id="pone.0322607.e016"><alternatives><graphic xlink:href="pone.0322607.e016.jpg" id="pone.0322607.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>cls</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> classes.</p></sec><sec id="sec012"><title>4.1.5 Classification head.</title><p>The classification head only uses the final representation of the patches and tokens. In the context of MRI image prediction, the softmax activation function is used due to the multi-class classification nature of the problem. For token embeddings or sequential RNA data, the activation function is sigmoid, given the binary classification nature of the problem. <xref rid="pone.0322607.e013" ref-type="disp-formula">Eq. 5</xref> corresponds to the output step found in each of the L-stacked transformers.</p></sec></sec><sec id="sec013"><title>4.2 Model application and hyperparameter tuning</title><p>Our data is loaded from the training directory using the Data_loaders function. All images are resized to <inline-formula id="pone.0322607.e017"><alternatives><graphic xlink:href="pone.0322607.e017.jpg" id="pone.0322607.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mn>256</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mn>256</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> pixels to ensure uniformity. Subsequently, the images were center-cropped to obtain a size of <inline-formula id="pone.0322607.e018"><alternatives><graphic xlink:href="pone.0322607.e018.jpg" id="pone.0322607.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mn>224</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mn>224</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, as required by the vision transformer algorithm. The images are then converted to Python arrays using the to.tensor function and the array values are normalized. We utilize the timm library [<xref rid="pone.0322607.ref027" ref-type="bibr">27</xref>] to load a pre-trained ViT model (&#x0201c;vit_base_patch16_224&#x0201d;). To customize the model, we replace its head (topmost classification layer) with a bespoke head comprising fully connected layers. The topmost classification layer consists of a fully connected layer with some input features and 512 output features. ReLU serves as the activation function, and dropout regularization with a dropout probability of 0.3 is applied to prevent overfitting. The ReLU activation introduced non-linearity to the model.</p><p>After configuring the topmost classification head, our transformer model is created. We set a batch size of 100 for the model, using LabelSmoothingCrossEntropy as the loss function in each iteration. This loss function, tailores for classification tasks, is a modification of conventional cross-entropy and addresses overconfidence and overfitting issues during training [<xref rid="pone.0322607.ref028" ref-type="bibr">28</xref>]. The number of epochs is set to 30, and the Adam optimizer function is employed as the optimizer. Adam optimizes model parameters efficiently and adaptively, extending the stochastic gradient descent (SGD) method. Finally, we train our model using the model.fit method in Python. The training is conducted on a Google Colab GPU, resulting in a test average accuracy of 98.6%. <xref rid="pone.0322607.t004" ref-type="table">Table 4</xref> depicts the key training parameters that were used tp train the model.</p><table-wrap position="float" id="pone.0322607.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.t004</object-id><label>Table 4</label><caption><title>Summary of key training parameters.</title></caption><alternatives><graphic xlink:href="pone.0322607.t004" id="pone.0322607.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Parameter</th><th align="left" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Image size (after preprocessing)</td><td align="left" rowspan="1" colspan="1"><inline-formula id="pone.0322607.e019"><alternatives><graphic xlink:href="pone.0322607.e019" id="pone.0322607.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mn>224</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mn>224</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> pixels</td></tr><tr><td align="left" rowspan="1" colspan="1">Batch Size</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">Loss function</td><td align="left" rowspan="1" colspan="1">LabelSmoothingCrossEntropy</td></tr><tr><td align="left" rowspan="1" colspan="1">Number of epochs</td><td align="left" rowspan="1" colspan="1">30</td></tr><tr><td align="left" rowspan="1" colspan="1">Optimizer</td><td align="left" rowspan="1" colspan="1">Adam</td></tr><tr><td align="left" rowspan="1" colspan="1">Dropout probability</td><td align="left" rowspan="1" colspan="1">0.3</td></tr><tr><td align="left" rowspan="1" colspan="1">Activation function</td><td align="left" rowspan="1" colspan="1">ReLU</td></tr><tr><td align="left" rowspan="1" colspan="1">Pretrained model</td><td align="left" rowspan="1" colspan="1">ViT (&#x0201c;vit_base_patch16_224&#x0201d;)</td></tr><tr><td align="left" rowspan="1" colspan="1">Fully connected layer output features</td><td align="left" rowspan="1" colspan="1">512</td></tr><tr><td align="left" rowspan="1" colspan="1">Training platform</td><td align="left" rowspan="1" colspan="1">Google Colab GPU</td></tr><tr><td align="left" rowspan="1" colspan="1">Test accuracy</td><td align="left" rowspan="1" colspan="1">98.6%</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec014"><title>4.3 LIME algorithm</title><p>Understanding how to articulate a predictive model and ensure precise predictions is essential. Explainable AI (XAI) is gaining traction due to its straightforward and easily understandable processes. Among the popular XAI techniques for interpreting predictive models is Local Interpretable Model-agnostic Explanations (LIME) [<xref rid="pone.0322607.ref029" ref-type="bibr">29</xref>]. In this context, consider X as the feature space, and x as a specific instance of a feature in the data. LIME serves the purpose of describing a prediction model. The two integral components of LIME include the black-box model (p) and the explanation (f). LIME locally elucidates the procedure by employing an interpretable function.</p><disp-formula id="pone.0322607.e020"><alternatives><graphic xlink:href="pone.0322607.e020.jpg" id="pone.0322607.e020g" position="anchor"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:msub><mml:mo>min</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">[</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>&#x003c9;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(6)</label></disp-formula><p>In this context, the loss function <inline-formula id="pone.0322607.e021"><alternatives><graphic xlink:href="pone.0322607.e021.jpg" id="pone.0322607.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is composed of three elements: <italic toggle="yes">p</italic>, representing the black-box model; <inline-formula id="pone.0322607.e022"><alternatives><graphic xlink:href="pone.0322607.e022.jpg" id="pone.0322607.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, denoting the interpretable feature explained by LIME. The term <inline-formula id="pone.0322607.e023"><alternatives><graphic xlink:href="pone.0322607.e023.jpg" id="pone.0322607.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003a9;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> accounts for the penalty associated with the complexity of the model <italic toggle="yes">f</italic>, while <inline-formula id="pone.0322607.e024"><alternatives><graphic xlink:href="pone.0322607.e024.jpg" id="pone.0322607.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> represents the similarity measure between data points <italic toggle="yes">x</italic>. The function <italic toggle="yes">f</italic> serves as the explainer. Through the utilization of anomaly data to address <xref rid="pone.0322607.e020" ref-type="disp-formula">Eqn. 6</xref>, LIME is capable of pinpointing the features.</p><p>To elucidate the image classifications conducted by a machine learning model, we devised a Python function named show_img_exp that leverages the LIME (Local Interpretable Model-Agnostic Explanations) module. This function takes two parameters: model, representing the machine learning model to be explained, and infile, denoting the file path to the input image.</p><p>Within the function, a LimeImageExplainer object is instantiated, facilitating explanations for image classifications. The input image undergoes preprocessing through an unspecified img_prep function, and the LIME explainer is employed to generate an explanation. This explanation accentuates areas of the image primarily influencing the top predicted label in the model. To visually emphasize these significant areas, the mark_boundaries function from the scikit-image package is applied, overlaying borders on the image. This process aims to highlight regions critical to the model&#x02019;s top prediction. Finally, the plt.show() function is employed to display the resulting image with highlighted boundaries, enabling a clear understanding of how the model arrived at its prediction for the input image.</p></sec></sec><sec id="sec015"><title>5 Results and analysis</title><p>We have obtained promising outcomes in our algorithms after completing their implementation on the datasets.</p><sec id="sec016"><title>5.1 Results of transformer for classifying MRI images</title><p><xref rid="pone.0322607.t006" ref-type="table">Table 6</xref> outlines the outcomes obtained by employing a vision transformer across various classes in the testing dataset. The NonDemented class attains the highest accuracy at 99.7%, while the MildDemented class registers the lowest accuracy at 97.31%. The vision transformer accurately classifies 98.6% of cases, with only a few instances of mis-prediction. The accuracy of the MildDemented class is less due to the presence of fewer training images in the dataset. The model performs a little less when there is a small number of training data. Other classes achieve appreciable results. Consequently, the overall accuracy of the model stands at 98.6% for all testing images, with a 95% confidence interval (CI) of [97.41%, 99.36%], indicating a high degree of reliability in classification performance. We derive the CI using 5-fold cross-validation, where the dataset was split into five subsets, training and testing the model iteratively to ensure stable performance across folds. The narrow CI range confirms robustness, as it reflects consistent accuracy with minimal variation across these folds. The CI provides a lower and upper range because it accounts for statistical uncertainty in the sample data, estimating the plausible bounds of the true accuracy. The lower bound (97.41%) represents the minimum expected performance with 95% confidence, while the upper bound (99.36%) suggests the potential peak accuracy under optimal conditions. The corresponding loss and accuracy curves are depicted in <xref rid="pone.0322607.g003" ref-type="fig">Figs 3</xref> and <xref rid="pone.0322607.g002" ref-type="fig">2</xref>. The accuracy curves for both training and testing exhibit an initial surge to 80% within the initial 5 epochs, eventually surpassing 95% after 15 epochs. Both curves demonstrate similar trends in loss calculations, reaching saturation after 15 epochs at the same point. Also, the ROC curve is displayed in <xref rid="pone.0322607.g004" ref-type="fig">Fig 4</xref>. The Cohen Kappa coefficient value we obtain is 0.97, indicating almost perfect agreement between the true and predicted labels. Similarly, the Matthews Correlation Coefficient (MCC) is 0.98, reflecting a strong correlation and robust performance of the model across all classes.</p><fig position="float" id="pone.0322607.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.g002</object-id><label>Fig 2</label><caption><title>Accuracy vs epoch curve of vision transformer.</title></caption><graphic xlink:href="pone.0322607.g002" position="float"/></fig><fig position="float" id="pone.0322607.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.g003</object-id><label>Fig 3</label><caption><title>Loss vs epoch curve of vision transformer.</title></caption><graphic xlink:href="pone.0322607.g003" position="float"/></fig><fig position="float" id="pone.0322607.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.g004</object-id><label>Fig 4</label><caption><title>AUC score of all classes of ViT.</title></caption><graphic xlink:href="pone.0322607.g004" position="float"/></fig><p>In <xref rid="pone.0322607.g005" ref-type="fig">Fig 5</xref>, LIME explanations are presented for all four classes of test images. The LIME explanation highlights the specific regions of the images that contributed to the prediction of these classes. Red patches in the explanation images represent areas that positively influenced the model&#x02019;s accurate prediction. Positive influence implies that these regions were instrumental in the model correctly identifying the class or label. In essence, the presence of red zones indicates that the characteristics or patterns in those areas align with the expected class. In <xref rid="pone.0322607.g006" ref-type="fig">Fig 6</xref>, overlays of the explanations are displayed. The yellow lines point out distinct pixels that played significant roles in predicting the class of these particular images.</p><fig position="float" id="pone.0322607.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.g005</object-id><label>Fig 5</label><caption><title>LIME Explanation of (a) MildDemented, (b) ModerateDemented, (c) NonDemented, and (d) VeryMildDemented Class images.</title></caption><graphic xlink:href="pone.0322607.g005" position="float"/></fig><fig position="float" id="pone.0322607.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.g006</object-id><label>Fig 6</label><caption><title>Overlay of (a) MildDemented, (b) ModerateDemented, (c) NonDemented, and (d) VeryMildDemented class images.</title></caption><graphic xlink:href="pone.0322607.g006" position="float"/></fig></sec><sec id="sec017"><title>5.2 Comparative analysis of vision transformer and other state-of-the-art computer vision algorithms on MRI image classification</title><p>In our MRI image classification study, we apply four off-the-shelf computer vision algorithms which are- the vision transformer (ViT), MobileNetV2, Swin Transformer and Skip-connected Convolutional Autoencoder (SCAE). Among these algorithms, ViT and MobileNetv2, both exhibit excellent performance with only a marginal 1% accuracy difference. We select MobileNetV2 for its capability to be easily ported into mobile devices aligns well with our plans of building an embedded device for detecting Alzheimer&#x02019;s Disease (AD). MobileNetV2 is known for its efficiency and suitability for deployment on resource-constrained platforms, making it a practical choice for mobile applications. This decision reflects a strategic consideration for the practicality of deploying your AD detection model in real-world scenarios, especially on devices with limited computational resources. The motivation behind selecting Swin Transformer is its hierarchical feature extraction and shift-window mechanism. This mechanism particularly adept at capturing fine-grained spatial information in images, which makes it highly effective for medical imaging tasks, as reflected by its strong performance, achieving an accuracy of 91.67% on the test set.The hierarchical design of Swin Transformer makes it suitable for large-scale image classification tasks. Additionally, we apply SCAE for its lightweight architecture and computational efficiency which gives us an accuracy of 88.31%. Additionally, EfficientNet-B0 and ResNet-50 are included for benchmarking, achieving accuracies of 91.57% and 86.59%, respectively, further validating the robustness of our approach. These results collectively highlight the trade-offs between accuracy, computational efficiency, and deployment feasibility. Based on our findings, it can be conclusively stated that the ViT proves to be a highly effective and efficient algorithm for the classification of MRI images related to Alzheimer&#x02019;s Disease. <xref rid="pone.0322607.t005" ref-type="table">Table 5</xref> displays the Accuracy, F1-score, Cohen&#x02019;s Kappa and Matthew&#x02019;s correlation coefficient of each model on the dataset. Based on our findings, it can be conclusively stated that the Vision Transformer proves to be a highly effective and efficient algorithm for the classification of MRI images related to Alzheimer&#x02019;s Disease. Also, <xref rid="pone.0322607.g004" ref-type="fig">Fig 4</xref> represents the ROC curve of ViT, where we can see that ViT achieves high Area Under the Curve (AUC) scores across all Alzheimer&#x02019;s Disease classes, reflecting its strong ability to distinguish between positive and negative cases for each category. The AUC values 0.98 for ModerateDemented, 0.96 for MildDemented, 1.00 for NonDemented, and 0.99 for VeryMildDemented indicate near-perfect discrimination, with NonDemented reaching an ideal 1.00, aligning with its top accuracy of 99.70%. These results underscore ViT&#x02019;s robustness in handling class-specific patterns in brain imaging data, even for challenging cases like MildDemented as the number of training images are very low, where accuracy dips slightly and so does the AUC. Also, the Cohen&#x02019;s Kappa and Matthew&#x02019;s correlation coefficients are consistent with these findings.</p><table-wrap position="float" id="pone.0322607.t005"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.t005</object-id><label>Table 5</label><caption><title>Overall test accuracy.</title></caption><alternatives><graphic xlink:href="pone.0322607.t005" id="pone.0322607.t005g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Algorithm</th><th align="left" rowspan="1" colspan="1">Accuracy</th><th align="left" rowspan="1" colspan="1">F1- score</th><th align="left" rowspan="1" colspan="1">AUC Score</th><th align="left" rowspan="1" colspan="1">Cohen&#x02019;s Kappa</th><th align="left" rowspan="1" colspan="1">Matthew&#x02019;s Coefficient</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">ViT</td><td align="left" rowspan="1" colspan="1">98.60%</td><td align="left" rowspan="1" colspan="1">98.47%</td><td align="left" rowspan="1" colspan="1">0.992</td><td align="left" rowspan="1" colspan="1">0.97</td><td align="left" rowspan="1" colspan="1">0.98</td></tr><tr><td align="left" rowspan="1" colspan="1">MobileNetV2</td><td align="left" rowspan="1" colspan="1">97.61%</td><td align="left" rowspan="1" colspan="1">97.52%</td><td align="left" rowspan="1" colspan="1">0.969</td><td align="left" rowspan="1" colspan="1">0.95</td><td align="left" rowspan="1" colspan="1">0.96</td></tr><tr><td align="left" rowspan="1" colspan="1">Swin Transformer</td><td align="left" rowspan="1" colspan="1">91.67%</td><td align="left" rowspan="1" colspan="1">91.93%</td><td align="left" rowspan="1" colspan="1">0.907</td><td align="left" rowspan="1" colspan="1">0.84</td><td align="left" rowspan="1" colspan="1">0.85</td></tr><tr><td align="left" rowspan="1" colspan="1">SCAE</td><td align="left" rowspan="1" colspan="1">88.31%</td><td align="left" rowspan="1" colspan="1">88.83%</td><td align="left" rowspan="1" colspan="1">0.893</td><td align="left" rowspan="1" colspan="1">0.78</td><td align="left" rowspan="1" colspan="1">0.80</td></tr><tr><td align="left" rowspan="1" colspan="1">EfficientNet-B0</td><td align="left" rowspan="1" colspan="1">91.57%</td><td align="left" rowspan="1" colspan="1">92.01%</td><td align="left" rowspan="1" colspan="1">0.92</td><td align="left" rowspan="1" colspan="1">0.83</td><td align="left" rowspan="1" colspan="1">0.84</td></tr><tr><td align="left" rowspan="1" colspan="1">ResNet-50</td><td align="left" rowspan="1" colspan="1">86.59%</td><td align="left" rowspan="1" colspan="1">86.87%</td><td align="left" rowspan="1" colspan="1">0.878</td><td align="left" rowspan="1" colspan="1">0.77</td><td align="left" rowspan="1" colspan="1">0.79</td></tr></tbody></table></alternatives></table-wrap><table-wrap position="float" id="pone.0322607.t006"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.t006</object-id><label>Table 6</label><caption><title>Test accuracy for the classes.</title></caption><alternatives><graphic xlink:href="pone.0322607.t006" id="pone.0322607.t006g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Classes</th><th align="left" rowspan="1" colspan="1">Test Accuracy</th><th align="left" rowspan="1" colspan="1">AUC</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">ModerateDemented</td><td align="left" rowspan="1" colspan="1">99.20%</td><td align="left" rowspan="1" colspan="1">0.98</td></tr><tr><td align="left" rowspan="1" colspan="1">MildDemented</td><td align="left" rowspan="1" colspan="1">97.31%</td><td align="left" rowspan="1" colspan="1">0.96</td></tr><tr><td align="left" rowspan="1" colspan="1">NonDemented</td><td align="left" rowspan="1" colspan="1">99.70%</td><td align="left" rowspan="1" colspan="1">1.00</td></tr><tr><td align="left" rowspan="1" colspan="1">VeryMildDemented</td><td align="left" rowspan="1" colspan="1">98.19%</td><td align="left" rowspan="1" colspan="1">0.99</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec018"><title>5.3 Results of transformer for classifying RNA-sequencing</title><p>The text transformer demonstrates a 98.9% accuracy on the test RNA-Sequencing Datasets, with an 80% training and 20% testing split of the entire RNA datasets. Following this, model explanations are provided using LIME. The LIME explanation reveals the feature importance values contributing to the prediction of the probability of Alzheimer&#x02019;s Disease (AD). Notably, the Age attribute emerges as a highly significant factor in AD presence, while Sex shows no discernible impact. RIN, cluster, Tangle.Stage, Cell.Type, and PMI exhibit varying degrees of significance in influencing AD prediction. Additionally, a Deep Neural Network (DNN) was employed, achieving a 93.23% accuracy. Figs <xref rid="pone.0322607.g007" ref-type="fig">7</xref> and <xref rid="pone.0322607.g008" ref-type="fig">8</xref> present LIME explanations for further insights.</p><fig position="float" id="pone.0322607.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.g007</object-id><label>Fig 7</label><caption><title>LIME explanation of feature value range on prediction.</title></caption><graphic xlink:href="pone.0322607.g007" position="float"/></fig><fig position="float" id="pone.0322607.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.g008</object-id><label>Fig 8</label><caption><title>LIME feature impacts on prediction.</title></caption><graphic xlink:href="pone.0322607.g008" position="float"/></fig></sec><sec id="sec019"><title>5.4 Combined results of transformer models</title><p>Impressively, by employing ensemble vision and text transformers for Alzheimer&#x02019;s Disease (AD) prediction using both MRI images and categorical data, we attain a remarkable accuracy of 98.75%. The model demonstrates high reliability, especially in terms of accuracy. In the next two subsections, the robustness of our model is discussed.</p></sec><sec id="sec020"><title>5.5 Interpretation of the model from a medical perspective</title><p>From <xref rid="pone.0322607.g005" ref-type="fig">Fig 5</xref> we can see that the regions highlighted by LIME are responsible for the respective predictions. In the image (b) the brain is moderately demented. It can be seen that the LIME explanation highlighting the ventricular and cerebellar regions in the MRI scan aligns well with known clinical biomarkers of Alzheimer&#x02019;s Disease (AD). Ventricular enlargement, as observed in the highlighted regions, is a well-established indicator of gray matter atrophy, particularly in the hippocampus and temporal lobes, which are critical for memory processing [<xref rid="pone.0322607.ref030" ref-type="bibr">30</xref>]. This degeneration seen in AD leads to cognitive impairment, and this shrinkage causes the lateral ventricles to expand [<xref rid="pone.0322607.ref031" ref-type="bibr">31</xref>]. The highlighted cerebellar region is also noteworthy, as recent studies suggest that cerebellar atrophy correlates with late-stage AD and cognitive decline [<xref rid="pone.0322607.ref032" ref-type="bibr">32</xref>]. Furthermore, cortical atrophy in the parietal and temporal lobes, also contributing to dementia symptoms, is consistent with the LIME-explained model&#x02019;s focus. The model&#x02019;s emphasis on these regions suggests that it has effectively learned clinically relevant patterns for AD detection. Also, in image (a), which corresponds to a mildly demented patient, the LIME overlay highlights critical brain regions associated with early-stage Alzheimer&#x02019;s Disease (AD). The hippocampus and temporal lobe regions are highlighted in red show significant importance for the model&#x02019;s classification. Clinically, hippocampal atrophy is a well-known early biomarker for AD, leading to short-term memory loss and cognitive decline [<xref rid="pone.0322607.ref030" ref-type="bibr">30</xref>]. Additionally, ventricular enlargement is apparent, likely due to gray matter loss. As a result, cerebrospinal fluid (CSF)-filled spaces will expand [<xref rid="pone.0322607.ref031" ref-type="bibr">31</xref>]. In image (c), corresponding to a non-demented individual, the LIME explanation shows activation in white matter regions and cerebellum. The absence of significant highlights in the hippocampus or temporal lobe suggests a healthy brain structure with no signs of neurodegeneration. The mild highlighting in the cerebellum and cortical areas could be due to normal variations in MRI signal intensity or non-pathological age-related changes [<xref rid="pone.0322607.ref032" ref-type="bibr">32</xref>]. Unlike the demented case, the ventricular spaces appear normal, indicating that the brain volume is well-preserved. Thus it can be concluded based on clinical research evidence that, the LIME explanation supports the validity of the model. For the RNA sequencing data, <xref rid="pone.0322607.g007" ref-type="fig">Fig 7</xref> illustrates a LIME-based decision boundary, showing how different features contribute to predicting Alzheimer&#x02019;s Disease (AD). The Tangle.Stage, Plaque.Stage, and Cell.Type features play a significant role, as neurofibrillary tangles and amyloid plaques are established biomarkers of AD pathology [<xref rid="pone.0322607.ref031" ref-type="bibr">31</xref>]. <xref rid="pone.0322607.g008" ref-type="fig">Fig 8</xref> highlights the feature importance ranking, where Tangle.Stage (3.00) and Plaque.Stage (3.00) have strong contributions, supporting their role in AD progression. Age (90 years) and cluster (16.00) also impact prediction, aligning with research showing that age-related neuronal changes increase AD risk [<xref rid="pone.0322607.ref030" ref-type="bibr">30</xref>]. The model&#x02019;s reliance on these biologically relevant features suggests clinically meaningful predictions, reinforcing the validity of LIME explanations.</p></sec><sec id="sec021"><title>5.6 Additional proof of robustness</title><sec id="sec022"><title>5.6.1 Validation using ADNI dataset</title><p>After training our model using the ADNI dataset, we receive remarkable accuracy of 98.29% in the test dataset with the same hyperparameters and configuration. The accuracies of different classes are depicted in the <xref rid="pone.0322607.t001" ref-type="table">Table 7</xref>.</p><table-wrap position="float" id="pone.0322607.t007"><object-id pub-id-type="doi">10.1371/journal.pone.0322607.t007</object-id><label>Table 7</label><caption><title>Class-wise accuracy of ADNI dataset.</title></caption><alternatives><graphic xlink:href="pone.0322607.t007" id="pone.0322607.t007g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Class</th><th align="left" rowspan="1" colspan="1">Accuracy (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">CN (Cognitively Normal)</td><td align="left" rowspan="1" colspan="1">99.1</td></tr><tr><td align="left" rowspan="1" colspan="1">EMCI (Early Mild Cognitive Impairment)</td><td align="left" rowspan="1" colspan="1">97.3</td></tr><tr><td align="left" rowspan="1" colspan="1">LMCI (Late Mild Cognitive Impairment)</td><td align="left" rowspan="1" colspan="1">98.5</td></tr><tr><td align="left" rowspan="1" colspan="1">AD (Alzheimer&#x02019;s Disease)</td><td align="left" rowspan="1" colspan="1">98.29</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec023"><title>5.6.2 LIME interpretation</title><p>The model&#x02019;s robustness is reinforced through LIME-based interpretability, which highlights clinically significant brain regions associated with Alzheimer&#x02019;s Disease (AD). The ventricular enlargement, hippocampal atrophy, and cerebellar atrophy identified in the LIME explanations align with well-documented biomarkers of AD. These findings confirm that the model is not relying on spurious correlations but instead learning meaningful anatomical features relevant to AD progression. Additionally, the LIME interpretations for non-demented cases show no significant highlighting in the hippocampus or temporal lobe, further validating the model&#x02019;s ability to differentiate between pathological and healthy brain structures. Furthermore, in the RNA sequencing data, the model correctly emphasizes Tangle.Stage and Plaque.Stage, which are well-known indicators of AD pathology. The alignment between model predictions and established clinical knowledge supports the model&#x02019;s reliability and generalizability, demonstrating its robustness in detecting AD across different modalities.</p></sec></sec></sec><sec sec-type="conclusions" id="sec024"><title>6 Discussion</title><p>The findings of this study underscore the transformative potential of advanced deep learning models, such as Vision Transformer (ViT) and MobileNetV2, in enhancing the accuracy of Alzheimer&#x02019;s disease (AD) diagnosis through MRI image analysis. ViT&#x02019;s superior performance, achieving an overall accuracy of 98.6% with a tight 95% confidence interval [97.41%, 99.36%], highlights its ability to capture intricate patterns in brain imaging data that traditional methods might overlook. MobileNetV2, while slightly less accurate, offers a lightweight alternative, making it viable for resource-constrained settings, which broadens the practical scope of AI-driven diagnostics. The integration of Explainable AI (XAI) techniques like LIME further elevates these models by providing interpretable insights, such as highlighting the hippocampus and temporal cortex as key regions in AD progression. This interpretability bridges the gap between complex AI outputs and clinical utility, fostering trust among healthcare professionals who rely on actionable explanations. Extending the analysis to RNA-sequencing data with transformer learning revealed complementary insights, suggesting that multi-modal approaches could refine AD prediction beyond imaging alone. However, the study&#x02019;s reliance on specific datasets raises concerns about generalizability, as demographic imbalances&#x02014;age, gender, or ethnicity&#x02014;may skew model performance across diverse populations. Similarly, variations in MRI equipment and protocols across institutions introduce noise, potentially undermining the models&#x02019; consistency in real-world scenarios. These limitations echo broader challenges in AI healthcare research, where dataset quality often dictates success more than algorithmic sophistication. The high AUC scores across AD stages (e.g., 1.00 for NonDemented, 0.96 for MildDemented) from ViT suggest robust discrimination, yet they also prompt questions about overfitting to the training data&#x02019;s characteristics. From a clinical perspective, aligning AI outputs with known AD biomarkers, like hippocampal atrophy, strengthens diagnostic confidence, but validation against larger, standardized cohorts remains critical. The regulatory landscape adds another layer of complexity, as FDA approval demands rigorous, multi-center trials to ensure safety and efficacy beyond academic benchmarks. Encouragingly, the narrow confidence intervals from 5-fold cross-validation affirm the models&#x02019; stability, yet future work must address scalability to diverse clinical environments. Integrating multi-center datasets with standardized protocols could mitigate biases and enhance deployment feasibility. Ultimately, this study lays a foundation for AI to revolutionize AD diagnostics, but its success hinges on overcoming technical, ethical, and regulatory hurdles in tandem.</p></sec><sec sec-type="conclusions" id="sec025"><title>7 Conclusion</title><p>This study demonstrates the efficacy of employing ensemble vision and text transformer model enhanced by XAI techniques like LIME, in accurately diagnosing AD stages using both MRI and RNA-sequencing data. The high accuracy (98.75%) and interpretability underscore their potential to improve clinical understanding and early detection of AD. Our experiments also highlight the robustness, absence of overfitting and tendency to bias towards different types of data. However, noisy dataset and variations of MRI protocol may highlight our model&#x02019;s limitations that future work must address. Regulatory challenges, including FDA approval, remain significant for real-world adoption. These findings pave the way for advanced AI tools in AD management, pending broader validation and standardization.</p></sec><sec id="sec026"><title>8 Future works</title><p>In the future, the progression of this research could involve the development of an embedded hardware system employing Vision Transformer techniques to enhance the accuracy of MRI image predictions. Additionally, the inclusion of diverse and standardized datasets for model training could lead to the creation of a more generalized and efficient system, improving overall prediction accuracy. Furthermore, utilizing the MobileNetV2 algorithm, there is potential for the creation of a mobile cloud application dedicated to Alzheimer&#x02019;s Disease detection using MRI images.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0322607.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Heber</surname><given-names>LE</given-names></name>, <name><surname>Weuve</surname><given-names>J</given-names></name>, <name><surname>Scherr</surname><given-names>PA</given-names></name>, <name><surname>Evans</surname><given-names>DA</given-names></name>. <article-title>Alzheimer disease in the United States (2010&#x02013;2050)</article-title>. <source>Neurology</source>
<year>2013</year>;<volume>80</volume>(<issue>19</issue>):<fpage>1778</fpage>&#x02013;<lpage>83</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1212/WNL.0b013e31828726f5.</pub-id>
<pub-id pub-id-type="pmid">23390181</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Hurd</surname><given-names>MD</given-names></name>, <name><surname>Martorell</surname><given-names>P</given-names></name>, <name><surname>Delavande</surname><given-names>A</given-names></name>, <name><surname>Mullen</surname><given-names>KJ</given-names></name>, <name><surname>Langa</surname><given-names>KM</given-names></name>. <article-title>Monetary costs of dementia in the United States</article-title>. <source>N Engl J Med</source>
<year>2013</year>;<volume>368</volume>(<issue>14</issue>):<fpage>1326</fpage>&#x02013;<lpage>34</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1056/NEJMsa1204629</pub-id>
<pub-id pub-id-type="pmid">23550670</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Petersen</surname><given-names>R</given-names></name>, <name><surname>Smith</surname><given-names>G</given-names></name>, <name><surname>Waring</surname><given-names>S</given-names></name>, <name><surname>Ivnik</surname><given-names>R</given-names></name>, <name><surname>Tangalos</surname><given-names>E</given-names></name>, <name><surname>Kokmen</surname><given-names>E</given-names></name>. <article-title>Mild cognitive impairment: clinical characterization and outcome</article-title>. <source>Arch Neurol.</source>
<year>1999</year>;<volume>56</volume>(<issue>3</issue>):<fpage>303</fpage>&#x02013;<lpage>8</lpage>.<pub-id pub-id-type="pmid">10190820</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>V&#x000f6;glein</surname><given-names>J</given-names></name>, <name><surname>Franzmeier</surname><given-names>N</given-names></name>, <name><surname>Morris</surname><given-names>JC</given-names></name>, <name><surname>Dieterich</surname><given-names>M</given-names></name>, <name><surname>McDade</surname><given-names>E</given-names></name>, <name><surname>Simons</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Pattern and implications of neurological examination findings in autosomal dominant Alzheimer disease</article-title>. <source>J Alzheimer&#x02019;s Assoc.</source>
<year>2022</year>;<volume>19</volume>(<issue>2</issue>):<fpage>632</fpage>&#x02013;<lpage>45</lpage>.</mixed-citation></ref><ref id="pone.0322607.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>George</surname><given-names>AE</given-names></name>, <name><surname>de Leon</surname><given-names>MJ</given-names></name>, <name><surname>Stylopoulos</surname><given-names>LA</given-names></name>, <name><surname>Miller</surname><given-names>J</given-names></name>, <name><surname>Kluger</surname><given-names>A</given-names></name>, <name><surname>Smith</surname><given-names>G</given-names></name>, <etal>et al</etal>. <article-title>CT diagnostic features of Alzheimer disease: importance of the choroidal/hippocampal fissure complex</article-title>. <source>AJNR Am J Neuroradiol.</source>
<year>1990</year>;<volume>11</volume>(<issue>1</issue>):<fpage>101</fpage>&#x02013;<lpage>7</lpage>.<pub-id pub-id-type="pmid">2105589</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>M</given-names></name>, <name><surname>Moon</surname><given-names>W-J</given-names></name>. <article-title>Structural MR imaging in the diagnosis of Alzheimer&#x02019;s disease and other neurodegenerative dementia: current imaging approach and future perspectives</article-title>. <source>Korean J Radiol</source>
<year>2016</year>;<volume>17</volume>(<issue>6</issue>):<fpage>827</fpage>&#x02013;<lpage>45</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3348/kjr.2016.17.6.827</pub-id>
<pub-id pub-id-type="pmid">27833399</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Coleman</surname><given-names>RE</given-names></name>. <article-title>Positron emission tomography diagnosis of Alzheimer&#x02019;s disease</article-title>. <source>PET Clin.</source>
<year>2007</year>;<volume>2</volume>(<issue>1</issue>):<fpage>25</fpage>&#x02013;<lpage>34</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cpet.2007.09.003</pub-id>
<pub-id pub-id-type="pmid">27157704</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Ochreiter</surname><given-names>S</given-names></name>. <article-title>The vanishing gradient problem during learning recurrent neural nets and problem solutions</article-title>. <source>Int J Uncertain Fuzziness Knowl Based Syst</source>
<year>2016</year>;<volume>6</volume>(<issue>2</issue>):<fpage>107</fpage>&#x02013;<lpage>16</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1142/S0218488598000094</pub-id></mixed-citation></ref><ref id="pone.0322607.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Kamal</surname><given-names>MS</given-names></name>, <name><surname>Northcote</surname><given-names>A</given-names></name>, <name><surname>Chowdhury</surname><given-names>L</given-names></name>, <name><surname>Dey</surname><given-names>N</given-names></name>, <name><surname>Crespo</surname><given-names>RG</given-names></name>. <article-title>Alzheimer&#x02019;s patient analysis using image and gene expression data and explainable-AI to presents associated genes</article-title>. <source>IEEE Trans. Instrum. Meas.</source>
<year>2023</year>;<volume>70</volume>:<fpage>1</fpage>&#x02013;<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TIM.2021.3107056</pub-id></mixed-citation></ref><ref id="pone.0322607.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Xiao</surname><given-names>J</given-names></name>, <name><surname>Xiang</surname><given-names>Y</given-names></name>, <name><surname>Ye</surname><given-names>F</given-names></name>. <article-title>Analysis of gene expression profiles in Alzheimer&#x02019;s disease patients with different lifespan: a bioinformatics study focusing on the disease heterogeneity</article-title>. <source>Front Aging Neurosci</source>. <year>2023</year>;<volume>15</volume>:<fpage>1072184</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fnagi.2023.1072184</pub-id>
<pub-id pub-id-type="pmid">36909942</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>T</given-names></name>, <name><surname>Lee</surname><given-names>H</given-names></name>. <article-title>Prediction of Alzheimer&#x02019;s disease using blood gene expression data</article-title>. <source>Sci Rep</source>
<year>2020</year>;<volume>10</volume>(<issue>1</issue>):<fpage>3485</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-020-60595-1</pub-id>
<pub-id pub-id-type="pmid">32103140</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Alamro</surname><given-names>H</given-names></name>, <name><surname>Thafar</surname><given-names>MA</given-names></name>, <name><surname>Albaradei</surname><given-names>S</given-names></name>, <name><surname>Gojobori</surname><given-names>T</given-names></name>, <name><surname>Essack</surname><given-names>M</given-names></name>, <name><surname>Gao</surname><given-names>X</given-names></name>. <article-title>Exploiting machine learning models to identify novel Alzheimer&#x02019;s disease biomarkers and potential targets</article-title>. <source>Sci Rep</source>. <year>2023</year>;<volume>13</volume>(<issue>1</issue>):<fpage>4979</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-023-30904-5</pub-id>
<pub-id pub-id-type="pmid">36973386</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Bhatkoti</surname><given-names>P</given-names></name>, <name><surname>Paul</surname><given-names>M</given-names></name>. <article-title>Early diagnosis of Alzheimer&#x02019;s disease: a multi-class deep learning framework with modified k-sparse autoencoder classification</article-title>. <part-title>In: 2016 International Conference on Image and Vision Computing New Zealand (IVCNZ), Palmerston North, New Zealand</part-title>, <year>2016</year>, pp. <fpage>1</fpage>&#x02013;<lpage>5</lpage>, <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/IVCNZ.2016.7804459</pub-id></mixed-citation></ref><ref id="pone.0322607.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Tuan</surname><given-names>TA</given-names></name>, <name><surname>Pham</surname><given-names>TB</given-names></name>, <name><surname>Kim</surname><given-names>JY</given-names></name>, <name><surname>Tavares</surname><given-names>JMRS</given-names></name>. <article-title>Alzheimer&#x02019;s diagnosis using deep learning in segmenting and classifying 3D brain MR images</article-title>. <source>Int J Neurosci</source>
<year>2022</year>;<volume>132</volume>(<issue>7</issue>):<fpage>689</fpage>&#x02013;<lpage>98</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/00207454.2020.1835900</pub-id>
<pub-id pub-id-type="pmid">33045895</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>D</given-names></name>, <name><surname>Liu</surname><given-names>M</given-names></name>, <name><surname>Fu</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>. <article-title>Classification of MR brain images by combination of multi-CNNs for AD diagnosis</article-title>. <part-title>In: Ninth International Conference on Digital Image Processing (ICDIP 2017)</part-title>; <year>2017</year>.</mixed-citation></ref><ref id="pone.0322607.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Isik</surname><given-names>Z</given-names></name>, <name><surname>Yigit</surname><given-names>A</given-names></name>. <article-title>Applying deep learning models to structural MRI for stage prediction of Alzheimer&#x02019;s disease</article-title>. <source>Turkish J Electr Eng Comput Sci</source>. <year>2020</year>;<volume>28</volume>(<issue>1</issue>):Article 14. <comment>doi: </comment><pub-id pub-id-type="doi">10.3906/elk-1904-172</pub-id></mixed-citation></ref><ref id="pone.0322607.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Liang</surname><given-names>Y-C</given-names></name>, <name><surname>Lo</surname><given-names>M-H</given-names></name>, <name><surname>Lan</surname><given-names>C-W</given-names></name>, <name><surname>Seo</surname><given-names>H</given-names></name>, <name><surname>Ummenhofer</surname><given-names>CC</given-names></name>, <name><surname>Yeager</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Amplified seasonal cycle in hydroclimate over the Amazon river basin and its plume region</article-title>. <source>Nat Commun</source>
<year>2020</year>;<volume>11</volume>(<issue>1</issue>):<fpage>4390</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41467-020-18187-0</pub-id>
<pub-id pub-id-type="pmid">32873800</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>M</given-names></name>, <name><surname>Li</surname><given-names>F</given-names></name>, <name><surname>Yan</surname><given-names>H</given-names></name>, <name><surname>Wang</surname><given-names>K</given-names></name>, <name><surname>Ma</surname><given-names>Y</given-names></name>, <name><surname>Shen</surname><given-names>L</given-names></name>, <name><surname>A deep learning system for differential diagnosis of Alzheimer&#x02019;s disease and mild cognitive impairment using structural</surname><given-names>MRI</given-names></name>. <article-title>NeuroImage</article-title>. <year>2021</year>;<volume>225</volume>:<fpage>117496</fpage></mixed-citation></ref><ref id="pone.0322607.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Kwon</surname><given-names>G</given-names></name>, <name><surname>Faisal</surname><given-names>FUR</given-names></name>. <article-title>Automated detection of Alzheimer&#x02019;s disease and mild cognitive impairment using whole brain MRI</article-title>. <source>IEEE Access</source>. <year>2022</year>;<volume>10</volume>:<fpage>65055</fpage>&#x02013;<lpage>66</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3180073</pub-id></mixed-citation></ref><ref id="pone.0322607.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Tufail</surname><given-names>AB</given-names></name>, <name><surname>Zhang</surname><given-names>Qn</given-names></name>, <name><surname>Ma</surname><given-names>Yk</given-names></name>. <article-title>Binary classification of Alzheimer&#x02019;s disease using sMRI imaging modality and deep learning</article-title>. <source>J Digit Imaging</source>
<year>2020</year>;<volume>33</volume>(<issue>5</issue>):<fpage>1073</fpage>&#x02013;<lpage>90</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10278-019-00265-5</pub-id>
<pub-id pub-id-type="pmid">32728983</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref021"><label>21</label><mixed-citation publication-type="book"><name><surname>Thamaraiselvi</surname><given-names>D</given-names></name>, <name><surname>Umapathy</surname><given-names>K</given-names></name>, <name><surname>Dharshini</surname><given-names>SS</given-names></name>, <name><surname>Mallika</surname><given-names>RN</given-names></name>, <name><surname>Peesapati</surname><given-names>SN</given-names></name>. <article-title>Deep learning based detection model for Alzheimer&#x02019;s disease</article-title>. <part-title>In: 4th International Conference on Electronics and Sustainable Communication Systems (ICESC), Coimbatore, India</part-title>, <year>2023</year>, pp. <fpage>878</fpage>&#x02014;<lpage>82</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ICESC57686.2023.10192963</pub-id></mixed-citation></ref><ref id="pone.0322607.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>G&#x000fc;ven</surname><given-names>M</given-names></name>. <article-title>Detection of Alzheimer&#x02019;s and Parkinson&#x02019;s diseases using deep learning-based various transformers models</article-title>. <source>In: The 4th International Electronic Conference on Biosensors</source>. <year>2024</year>:<volume>4</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/engproc2024073004</pub-id></mixed-citation></ref><ref id="pone.0322607.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Sherstinsky</surname><given-names>A</given-names></name>. <article-title>Fundamentals of recurrent neural network (RNN) and long short-term memory network (LSTM)</article-title>. <source>Phys D Nonlinear Phenom</source>. <year>2020</year>;<volume>404</volume>:<fpage>132306</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.physd.2019.132306</pub-id></mixed-citation></ref><ref id="pone.0322607.ref024"><label>24</label><mixed-citation publication-type="other">Alzheimer&#x02019;s Dataset - 4 Class of Images. Available from: <ext-link xlink:href="https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images" ext-link-type="uri">https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images</ext-link></mixed-citation></ref><ref id="pone.0322607.ref025"><label>25</label><mixed-citation publication-type="other">Gene Expression Omnibus (GEO) - GSE174367. Available from: <ext-link xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE174367" ext-link-type="uri">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE174367</ext-link></mixed-citation></ref><ref id="pone.0322607.ref026"><label>26</label><mixed-citation publication-type="other">Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, et al. An image is worth 16x16 words: transformers for image recognition at scale. arXiv. preprint. 10.48550/arXiv.2010.11929. 2021.</mixed-citation></ref><ref id="pone.0322607.ref027"><label>27</label><mixed-citation publication-type="other">PyTorch Image Models. timm - PyTorch Image Models. Available from: <ext-link xlink:href="https://pypi.org/project/timm/#models" ext-link-type="uri">https://pypi.org/project/timm/#models</ext-link></mixed-citation></ref><ref id="pone.0322607.ref028"><label>28</label><mixed-citation publication-type="other">Wong W. What is label smoothing? Available from: <ext-link xlink:href="https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06." ext-link-type="uri">https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06.</ext-link></mixed-citation></ref><ref id="pone.0322607.ref029"><label>29</label><mixed-citation publication-type="other">Ribeiro MT, Singh S, Guestrin C. &#x0201c;Why should i trust you?&#x0201d;: Explaining the predictions of any classifier. arXiv. preprint. arXiv:160204938. 2016.</mixed-citation></ref><ref id="pone.0322607.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Frisoni</surname><given-names>GB</given-names></name>, <name><surname>Fox</surname><given-names>NC</given-names></name>, <name><surname>Jack CR</surname><given-names>Jr</given-names></name>, <name><surname>Scheltens</surname><given-names>P</given-names></name>, <name><surname>Thompson</surname><given-names>PM</given-names></name>. <article-title>The clinical use of structural MRI in Alzheimer disease</article-title>. <source>Nat Rev Neurol</source>
<year>2010</year>;<volume>6</volume>(<issue>2</issue>):<fpage>67</fpage>&#x02013;<lpage>77</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nrneurol.2009.215</pub-id>
<pub-id pub-id-type="pmid">20139996</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Jack</surname><given-names>CR</given-names></name>, <name><surname>Knopman</surname><given-names>DS</given-names></name>, <name><surname>Jagust</surname><given-names>WJ</given-names></name>, <name><surname>Petersen</surname><given-names>RC</given-names></name>, <name><surname>Weiner</surname><given-names>MW</given-names></name>, <name><surname>Aisen</surname><given-names>PS</given-names></name>, <etal>et al</etal>. <article-title>Update on hypothetical model of Alzheimer&#x02019;s disease biomarkers</article-title>. <source>Lancet Neurol.</source>
<year>2013</year>;<volume>12</volume>(<issue>2</issue>):<fpage>207</fpage>&#x02013;<lpage>16</lpage>.<pub-id pub-id-type="pmid">23332364</pub-id>
</mixed-citation></ref><ref id="pone.0322607.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Jacobs</surname><given-names>HIL</given-names></name>, <name><surname>Hopkins</surname><given-names>DA</given-names></name>, <name><surname>Mayrhofer</surname><given-names>HC</given-names></name>, <name><surname>Bruner</surname><given-names>E</given-names></name>, <name><surname>van Leeuwen</surname><given-names>FW</given-names></name>, <name><surname>Raaijmakers</surname><given-names>W</given-names></name>, <etal>et al</etal>. <article-title>The cerebellum in Alzheimer&#x02019;s disease: evaluating its role in cognitive decline</article-title>. <source>Brain</source>
<year>2018</year>;<volume>141</volume>(<issue>1</issue>):<fpage>37</fpage>&#x02013;<lpage>47</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/brain/awx194</pub-id>
<pub-id pub-id-type="pmid">29053771</pub-id>
</mixed-citation></ref></ref-list></back><sub-article article-type="author-comment" id="pone.0322607.r001" specific-use="rebutted-decision-letter-unavailable"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0322607.r001</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">12 Sep 2024</named-content>
</p></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0322607.r002" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0322607.r002</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Alzubaidi</surname><given-names>Laith</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Laith Alzubaidi</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Laith Alzubaidi</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0322607" id="rel-obj002" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">25 Nov 2024</named-content>
</p><p><!--<div>-->PONE-D-24-39865<!--</div>--><!--<div>-->Leveraging Transformers and Explainable AI for Alzheimer&#x02019;s Disease Interpretability<!--</div>--><!--<div>-->PLOS ONE</p><p>Dear Dr. Sammo,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by Jan 09 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!--</div>--></p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p><!--<div>-->If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Laith Alzubaidi</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal Requirements:</p><p>When submitting your revision, we need you to address these additional requirements.</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at&#x000a0;</p><p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and&#x000a0;</p><p>
<ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
</p><p>2. Please note that PLOS ONE has spec6ific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, all author-generated code must be made available without restrictions upon publication of the work. Please review our guidelines at <ext-link xlink:href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" ext-link-type="uri">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</ext-link> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p><p>3. We note that your Data Availability Statement is currently as follows: All relevant data are within the manuscript and its Supporting Information files.</p><p>Please confirm at this time whether or not your submission contains all raw data required to replicate the results of your study. Authors must share the &#x0201c;minimal data set&#x0201d; for their submission. PLOS defines the minimal data set to consist of the data required to replicate all study findings reported in the article, as well as related metadata and methods (<ext-link xlink:href="https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition)." ext-link-type="uri">https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition</ext-link>).</p><p>For example, authors should submit the following data:</p><p>- The values behind the means, standard deviations and other measures reported;</p><p>- The values used to build graphs;</p><p>- The points extracted from images for analysis.</p><p>Authors do not need to submit their entire data set if only a portion of the data was used in the reported study.</p><p>If your submission does not contain these data, please either upload them as Supporting Information files or deposit them to a stable, public repository and provide us with the relevant URLs, DOIs, or accession numbers. For a list of recommended repositories, please see <ext-link xlink:href="https://journals.plos.org/plosone/s/recommended-repositories." ext-link-type="uri">https://journals.plos.org/plosone/s/recommended-repositories</ext-link>.</p><p>If there are ethical or legal restrictions on sharing a de-identified data set, please explain them in detail (e.g., data contain potentially sensitive information, data are owned by a third-party organization, etc.) and who has imposed them (e.g., an ethics committee). Please also provide contact information for a data access committee, ethics committee, or other institutional body to which data requests may be sent. If data are owned by a third party, please indicate how others may request data access.</p><p>4. Please include your full ethics statement in the &#x02018;Methods&#x02019; section of your manuscript file. In your statement, please include the full name of the IRB or ethics committee who approved or waived your study, as well as whether or not you obtained informed written or verbal consent. If consent was waived for your study, please include this information in your statement as well.&#x000a0;</p><p>5. Please ensure that you refer to Figure 1 in your text as, if accepted, production will need this reference to link the reader to the figure.</p><p>6. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article&#x02019;s retracted status in the References list and also include a citation and full reference for the retraction notice.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #1:&#x000a0;Partly</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->2. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p>Reviewer #1:&#x000a0;The authors of this manuscript present an interesting approach for the interpretability of Alzheimer&#x02019;s Disease. While the methodology and concept are effective, I have a few suggestions on various sections of the paper that may require improvements to meet the journal's standards.</p><p>1. First, I recommend that the authors consider structuring the introduction into several segments based on relevance. While the introduction begins well, the connections between sentences become weaker as it progresses. Additionally, there are insufficient citations for various claims presented in this section. The overall writing quality of the introduction could be improved. For example, the statement, 'However, there is a gap in incorporating XAI techniques into transformer-based models,' is made but is not followed by an explanation or discussion of the gaps, nor is there any citation provided.</p><p>2. Similar improvements are needed in the literature review. While the content is generally acceptable, the writing style should be enhanced. It would be beneficial to ensure relevance between sentences and to review the word choices throughout. For instance, if you spell out 'Convolutional Neural Network' once, you can simply use the abbreviation 'CNN' in subsequent references.</p><p>3. The results section requires additional validation considering the claims made by the authors. Currently, the authors only performed a comparative analysis with DeepLabV2. I suggest including a few more off-the-shelf network architectures to better validate your results.</p><p>4. Quality of Figure 6 and 7 can be improved.</p><p>5. Check all the section heading for inaccurate writing format.</p><p>Reviewer #2:&#x000a0;Review of Paper PONE-D-24-39865</p><p>Title:</p><p>Leveraging Transformers and Explainable AI for Alzheimer&#x02019;s Disease Interpretability</p><p>Summary:</p><p>This study addresses Alzheimer&#x02019;s disease (AD), a progressive brain condition marked by memory loss, cognitive decline, and behavioral changes, affecting one in nine adults over 65. While computer vision applications in MRI-based AD detection have shown accuracies between 80-90%, this study applies advanced transformer models and computer vision to achieve a 99% accuracy rate in categorizing AD stages. By incorporating RNA data and MRI images in near-real-time, this approach improves diagnostic accuracy. The Local Interpretable Model-agnostic Explanations (LIME) method enhances the model's reliability, highlighting essential RNA features and MRI regions for diagnosing AD, thereby making the model&#x02019;s decisions more interpretable and acceptable for human use.</p><p>Strengths And Weaknesses:</p><p>Strengths</p><p>&#x025cf; It has a thorough explanation of the main idea.</p><p>&#x025cf; Clearly explain the methodology.</p><p>Weaknesses</p><p>Overall, the writing can be significantly improved to address the following concerns.</p><p>&#x025cf; Introduction: You can add more references in the related work to show that you check new papers.</p><p>&#x025cf; Results: The results look good.</p><p>**********</p><p><!--<font color="black">-->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.<!--</div>--></p><supplementary-material id="pone.0322607.s001" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Reviewer Comments.pdf</named-content></p></caption><media xlink:href="pone.0322607.s001.pdf"/></supplementary-material><supplementary-material id="pone.0322607.s002" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">PONE-D-24-39865_reviewer-comments.docx</named-content></p></caption><media xlink:href="pone.0322607.s002.docx"/></supplementary-material></body></sub-article><sub-article article-type="author-comment" id="pone.0322607.r003"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0322607.r003</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0322607" id="rel-obj003" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">31 Jan 2025</named-content>
</p><p>Reviewer 1 Comments and Our Responses</p><p>1. Structuring and Writing Quality of the Introduction</p><p>o Comment: The introduction could be better structured into segments based on</p><p>relevance. Connections between sentences weaken as it progresses, and insufficient</p><p>citations are provided. Additionally, the statement regarding the gap in</p><p>incorporating XAI techniques into transformer-based models lacks explanation or</p><p>citations.</p><p>o Response: We have restructured the introduction into clearly defined segments for</p><p>better readability and logical flow. We have also enhanced the writing quality by</p><p>refining sentence transitions and word choices. For the statement about the gap in</p><p>incorporating XAI techniques into transformer-based models, we have provided a</p><p>brief explanation. We also made an effort to provide the relevant citations.</p><p>2. Improvements in the Literature Review</p><p>o Comment: Similar improvements are needed in the literature review. While the</p><p>content is generally acceptable, the writing style should be enhanced. It would be</p><p>beneficial to ensure relevance between sentences and to review the word choices</p><p>throughout. For instance, if you spell out 'Convolutional Neural Network' once, you</p><p>can simply use the abbreviation 'CNN' in subsequent references..</p><p>o Response: We have carefully reviewed and refined the writing style in the literature</p><p>review to improve readability and maintain relevance between sentences. Word</p><p>choices have been optimized, and abbreviations like "Convolutional Neural</p><p>Network" (CNN) are now consistently used after being spelled out once.</p><p>3. Validation of Results Section</p><p>o Comment: The results section needs further validation beyond the comparative</p><p>analysis with MobileNetV2. Additional off-the-shelf network architectures should</p><p>be included for better validation.</p><p>o Response: We have expanded the results section by including comparative analyses</p><p>with several additional state-of-the-art architectures, such as Swin Transformers</p><p>and Skip-connected Convolutional Autoencoders in addition to previously</p><p>provided Vision Transformers and MobileNetV2. This enhanced validation</p><p>provides stronger evidence supporting our claims and improves the robustness of</p><p>our findings.</p><p>4. Quality of Figures 6 and 7</p><p>o Comment: The quality of Figures 6 and 7 can be improved.</p><p>o Response: We have improved the quality of Figures 6 and 7 by resizing them and</p><p>ensuring visual clarity. Labels, legends, and other graphical elements have been</p><p>adjusted for better comprehensibility.</p><p>5. Inaccurate Writing Format in Section Headings</p><p>o Comment: Check all section headings for formatting inaccuracies.</p><p>o Response: We have thoroughly reviewed and corrected all section headings to</p><p>adhere to the journal's formatting guidelines. Inconsistencies in capitalization,</p><p>numbering, and alignment have been addressed.</p><supplementary-material id="pone.0322607.s003" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response to reviewers.pdf</named-content></p></caption><media xlink:href="pone.0322607.s003.pdf"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0322607.r004" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0322607.r004</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Uysal</surname><given-names>Fatih</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Fatih Uysal</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Fatih Uysal</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0322607" id="rel-obj004" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">11 Mar 2025</named-content>
</p><p><!--<div>-->PONE-D-24-39865R1<!--</div>--><!--<div>-->Leveraging Transformers and Explainable AI for Alzheimer&#x02019;s Disease Interpretability<!--</div>--><!--<div>-->PLOS ONE</p><p>Dear Dr. Sammo,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.<!--</div>--><!--<div>--></p><p>
<bold>Please revise the paper, taking into account both the referee comments and the editor's comments.</bold>
<!--</div>-->
<!--<div>-->
</p><p>Please submit your revised manuscript by Apr 25 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!--</div>--></p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Fatih Uysal, Ph.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>
<bold>Additional Editor Comments:</bold>
</p><p>Please revise the paper, taking into account both the referee comments and the comments I have provided below.</p><p>1. There are serious deficiencies in the evaluation metrics required for the evaluation of the classification results in the study. Please obtain the missing Receiver Operating Characteristic (ROC) Curve, the area under the ROC curve (AUC) scores, Cohen's Kappa and Matthews correlation coefficient (MCC) scores completely.</p><p>2. It is recommended to make an analysis with several state-of-the-art models in order to compare the results in more depth and to make the proposed approach more prominent.</p><p>3. The literature review table needs to be detailed. Here, more new studies related to the current literature should be included and new columns such as "data preprocessing/augmentation, originality, plus and minus aspects" should be added. After this, the difference of this study from the literature and its main contribution to the literature should be expressed in more depth by relating it to the studies in the literature.</p><p>4. Detail the information regarding the dataset along with the justifications in terms of traning, validation, test distribution percentages and amounts, data precoessing/augmentation.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.<!--</font>--></p><p>Reviewer #3:&#x000a0;All comments have been addressed</p><p>Reviewer #4:&#x000a0;(No Response)</p><p>**********</p><p><!--<font color="black">-->2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->3. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p><bold>Reviewer #3:&#x000a0;</bold>The manuscript presents a valuable contribution to Alzheimer&#x02019;s disease detection using advanced transformers and computer vision techniques. The study is well-structured and demonstrates impressive accuracy. To further enhance the work, we recommend citing "Detection of Alzheimer&#x02019;s and Parkinson&#x02019;s Diseases Using Deep Learning-Based Various Transformers Models" (<ext-link xlink:href="https://doi.org/10.3390/engproc2024073004)" ext-link-type="uri">https://doi.org/10.3390/engproc2024073004</ext-link>) to provide additional context and strengthen the research foundation. Overall, we suggest minor revisions and look forward to the updated submission.</p><p><bold>Reviewer #4:&#x000a0;</bold>Your manuscript offers a valuable contribution to AI-driven Alzheimer&#x02019;s disease diagnosis, demonstrating high accuracy and integrating explainability via LIME. The combination of transformers for MRI and RNA sequencing analysis is innovative and well-justified. The literature review is comprehensive, and the methodological framework is clearly presented.</p><p>However, the manuscript could benefit from additional validation on external datasets, a more detailed discussion on clinical applicability, and minor refinements in writing and formatting. These improvements would enhance the paper&#x02019;s clarity and real-world impact :-</p><p>1. Introduction &#x00026; Motivation</p><p>The introduction clearly states the importance of early AD diagnosis and the role of AI in medical imaging, but the research gap could be more explicitly defined to emphasize why transformers are superior to CNNs or traditional machine learning models. Additionally, the claim that "there is no cure for AD" should be softened, as some experimental treatments exist. A brief outline of the paper structure at the end of the introduction would improve readability and guide the reader.</p><p>2. Methodology &#x00026; Model Description</p><p>The methodology section is highly technical and would benefit from a flowchart or diagram illustrating the proposed model architecture. Additionally, key training parameters (batch size, optimizer, number of epochs) should be summarized in a table to improve clarity. The explanation of LIME should be expanded to include how interpretability was validated&#x02014;was expert medical review conducted to confirm whether the AI highlights relevant features?</p><p>3. Results &#x00026; Validation</p><p>The reported accuracy of 99% raises concerns about overfitting, especially since deep learning models typically struggle with medical imaging data generalization. The study should include external validation using an independent dataset (e.g., ADNI, OASIS) or a detailed description to confirm robustness. Additionally, statistical measures such as confidence intervals or standard deviations should be included to ensure the reliability of the reported results. While the comparative analysis with other models is strong, it could be further improved by incorporating benchmarks such as ResNet or EfficientNet.</p><p>4. Figures &#x00026; Tables</p><p>Figures 6 and 7 require more detailed captions explaining the significance of their findings. The LIME heatmaps should include clinical validation, ideally with references to known AD biomarkers. Additionally, the results section would benefit from an ROC-AUC curve to provide a more comprehensive evaluation of classification performance.</p><p>5. Discussion &#x00026; Interpretability</p><p>The discussion should better address dataset biases, such as demographic imbalances and variations in MRI scanning equipment. The interpretability aspect of LIME should be assessed from a clinical perspective&#x02014;does the AI highlight regions relevant to AD diagnosis? Furthermore, potential regulatory challenges (e.g., FDA approval for AI-based diagnostic tools) should be briefly discussed.</p><p>6. Writing &#x00026; Formatting Improvements</p><p>Some sections contain long, complex sentences that could be restructured for clarity. The use of abbreviations should be consistent (e.g., CNN should be abbreviated after its first mention). Additionally, minor grammatical redundancies, such as "AI-based artificial intelligence," should be corrected for conciseness.</p><p>**********</p><p><!--<font color="black">-->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #3:&#x000a0;No</p><p>Reviewer #4:&#x000a0;<bold>Yes:&#x000a0;</bold>Aasim Ayaz Wani</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p><supplementary-material id="pone.0322607.s004" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Reviewer Comment.docx</named-content></p></caption><media xlink:href="pone.0322607.s004.docx"/></supplementary-material></body></sub-article><sub-article article-type="author-comment" id="pone.0322607.r005"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0322607.r005</article-id><title-group><article-title>Author response to Decision Letter 2</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0322607" id="rel-obj005" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">24 Mar 2025</named-content>
</p><p>Responses and Actions to Reviewer Comments</p><p>Dear Editor and Reviewers,</p><p>The authors would like to thank the reviewers for providing insightful comments that helped to make the journal paper better. All review judgments and comments are looked at by the authors. Following the reviewers' recommendations, they also offer remarks, feedback, and necessary changes to the work. The authors express their gratitude to all of the esteemed reviewers for their hard work.</p><p>Best Regards,</p><p>Authors, Manuscript Number: PONE-D-24-39865</p><p>============== Review Comments and Corresponding Actions ======================</p><p>Additional Editor Comments:</p><p>Comment-1:</p><p>There are serious deficiencies in the evaluation metrics required for the evaluation of the classification results in the study. Please obtain the missing Receiver Operating Characteristic (ROC) Curve, the area under the ROC curve (AUC) scores, and Cohen's Kappa and Matthews correlation coefficient (MCC) scores completely.</p><p>Response-1:</p><p>Thank you very much for your comments.</p><p>We add the Cohen's Kappa, Matthews correlation coefficient (MCC) scores, and AUC values of the experimental models in Table 5. AUCs of all classes are generated from the Receiver Operating Characteristic (ROC) curves in Figure 4 using a vision transformer.</p><p>The following lines are included in Section 5.1, line 352:</p><p>&#x0201c;Table 6 outlines the outcomes obtained by employing a vision transformer across various classes in the testing dataset. The NonDemented class attains the highest accuracy at 99.7%, while the MildDemented class registers the lowest accuracy at 97.31%. The vision transformer accurately classifies 98.6% of cases, with only a few instances of mis-prediction. The accuracy of the MildDemented class is less due to the presence of fewer training images in the dataset. The model performs a little less when there is a small number of training data. Other classes achieve appreciable results. Consequently, the overall accuracy of the model stands at 98.6% for all testing images, with a 95% confidence interval (CI) of [97.41%, 99.36%], indicating a high degree of reliability in classification performance. We derive the CI using 5-fold cross-validation, where the dataset was split into five subsets, training and testing the model iteratively to ensure stable performance across folds. The narrow CI range confirms robustness, as it reflects consistent accuracy with minimal variation across these folds. The CI provides a lower and upper range because it accounts for statistical uncertainty in the sample data, estimating the plausible bounds of the true accuracy. The lower bound (97.41%) represents the minimum expected performance with 95% confidence, while the upper bound (99.36%) suggests the potential peak accuracy under optimal conditions. The corresponding loss and accuracy curves are depicted in Fig. 3 and Fig. 2. The accuracy curves for both training and testing exhibit an initial surge to 80% within the initial 5 epochs, eventually surpassing 95% after 15 epochs. Both curves demonstrate similar trends in loss calculations, reaching saturation after 15 epochs at the same point. Also, the ROC curve is displayed in Fig. 4. The Cohen Kappa coefficient value we obtain is 0.97, indicating almost perfect agreement between the true and predicted labels. Similarly, the Matthews Correlation Coefficient (MCC) is 0.98, reflecting a strong correlation and robust performance of the model across all classes. In Fig. 5, LIME explanations are presented for all four classes of test images. The LIME explanation highlights the specific regions of the images that contributed to the prediction of these classes. Red patches in the explanation images represent areas that positively influenced the model&#x02019;s accurate prediction. Positive influence implies that these regions were instrumental in the model correctly identifying the class or label. In essence, the presence of red zones indicates that the characteristics or patterns in those areas align with the expected class. In Fig. 6, overlays of the explanations are displayed. The yellow lines point out distinct pixels that played significant roles in predicting the class of these particular images.&#x0201d;</p><p>Comment-2:</p><p>It is recommended to analyze several state-of-the-art models to compare the results in more depth and to make the proposed approach more prominent.</p><p>Response-2:</p><p>Two new state-of-the-art models, including ResNet 50, and EfficientNet Models, are experimented with, and their results are presented in Table 5.</p><p>The following lines are included in Section 5.2, line 406:</p><p>&#x0201c;Additionally, EfficientNet-B0 and ResNet-50 are included for benchmarking, achieving accuracies of 91.57% and 86.59%, respectively, further validating the robustness of our approach. These results collectively highlight the trade-offs between accuracy, computational efficiency, and deployment feasibility. Based on our findings, it can be conclusively stated that the ViT proves to be a highly effective and efficient algorithm for the classification of MRI images related to Alzheimer&#x02019;s Disease. Table 5 displays the Accuracy, F1-score, Cohen&#x02019;s Kappa, and Matthew&#x02019;s correlation coefficient of each model on the dataset. Based on our findings, it can be conclusively stated that the Vision Transformer proves to be a highly effective and efficient algorithm for the classification of MRI images related to Alzheimer&#x02019;s Disease. Also, Fig. 4 represents the ROC curve of ViT where we can see that ViT achieves high Area Under the Curve (AUC) scores across all Alzheimer&#x02019;s Disease classes, reflecting its strong ability to distinguish between positive and negative cases for each category. The AUC values 0.98 for ModerateDemented, 0.96 for MildDemented, 1.00 for NonDemented, and 0.99 for VeryMildDemented indicate near-perfect discrimination, with NonDemented reaching an ideal 1.00, aligning with its top accuracy of 99.70%. These results underscore ViT&#x02019;s robustness in handling class-specific patterns in brain imaging data, even for challenging cases like MildDemented as the number of training images are very low, where accuracy dips slightly and so does the AUC. Also, the Cohen&#x02019;s Kappa and Matthew&#x02019;s correlation coefficients are consistent with these findings.&#x0201d;</p><p>Comment-3:</p><p>The literature review table needs to be detailed. Here, more new studies related to the current literature should be included, and new columns such as "data preprocessing/augmentation, originality, plus and minus aspects" should be added. After this, the difference of this study from the literature and its main contribution to the literature should be expressed in more depth by relating it to the studies in the literature.</p><p>Response-3:</p><p>Three new related studies are added accordingly, and the literature review table is split into two separate tables to cover all these aspects mentioned (originality, plus and minus aspects).</p><p>In the Reference section, the following three references are added:</p><p>30. Guven M. Detection of Alzheimer&#x02019;s and Parkinson&#x02019;s Diseases Using Deep Learning-Based Various Transformers Models. Eng. Proc. 2024;73(1):4. Presented at the 4th International Electronic Conference on Biosensors, 20&#x02013;22 May 2024. 629 Available online: <ext-link xlink:href="https://sciforum.net/event/IECB2024." ext-link-type="uri">https://sciforum.net/event/IECB2024</ext-link>. <ext-link xlink:href="https://doi.org/10.3390/engproc2024073004." ext-link-type="uri">https://doi.org/10.3390/engproc2024073004</ext-link>.</p><p>31. Liu M, Li F, Yan H, Wang K, Ma Y, Shen L, A deep learning system for differential diagnosis of Alzheimer&#x02019;s disease and mild cognitive impairment using structural MRI. NeuroImage. 2021;225:117496. <ext-link xlink:href="https://doi.org/10.1016/j.neuroimage.2020.117496." ext-link-type="uri">https://doi.org/10.1016/j.neuroimage.2020.117496</ext-link>.</p><p>32. Wang H, Shen Y, Wang S, Xiao T, Deng L, Wang X, Multimodal deep learning for Alzheimer&#x02019;s disease dementia assessment. Nature Communications. 2020;11(1):1&#x02013;9. <ext-link xlink:href="https://doi.org/10.1038/s41467-020-18187-0." ext-link-type="uri">https://doi.org/10.1038/s41467-020-18187-0</ext-link>.</p><p>In addition, the following lines are added in Section 2 Literature Review line 116:</p><p>&#x0201c;Wang et al. [32] developed a multimodal deep learning framework for Alzheimer&#x02019;s Disease dementia assessment, integrating data from neuroimaging, genetic markers, and cognitive tests. Their approach combines CNNs and recurrent neural networks (RNNs) to capture both spatial and temporal patterns in the data. The study shows that multimodal data fusion significantly improves diagnostic accuracy compared to single-modality approaches. The authors reported an accuracy of 92.1% for Alzheimer&#x02019;s Disease (AD) diagnosis using their multimodal deep learning framework, which integrated neuroimaging, genetic markers, and cognitive tests. This multi-modal approach was quite a new addition to the field of AD diagnosis at the time of publication, but new state-of-the-art algorithms like models can be used for further accuracy improvements. Another study conducted by Liu M et al. [31] proposed a deep learning system for the differential diagnosis of Alzheimer&#x02019;s Disease (AD) and mild cognitive impairment (MCI) using structural MRI. Their model leverages 3D convolutional neural networks (CNNs) to extract features from brain scans, achieving high accuracy in distinguishing between AD, MCI, and healthy controls. The study highlights the potential of deep learning in automating AD diagnosis and emphasizes the importance of structural MRI as a key biomarker for early detection. reported an accuracy of 88.6% for differentiating Alzheimer&#x02019;s Disease (AD) from healthy controls and 76.5% for distinguishing mild cognitive impairment (MCI) from healthy controls using their deep learning system based on structural MRI. The accuracy remains relatively low due to the use of CNN-based architectures, and the usability of the moderately accurate model can cause a serious discrepancy in the field of AD detection.&#x0201d;</p><p>The following lines are added in Section 2 Literature Review line 151:</p><p>&#x0201c;Recent work by [30] applied transformer-based models, including Swin Transformer, Vision Transformer (ViT), and Bidirectional Encoder Representation from Image Transformers (BEiT), to classify Alzheimer&#x02019;s and Parkinson&#x02019;s diseases using brain imaging data. The study utilized a balanced dataset of 450 brain images, achieving classification accuracy exceeding 80%, with ViT demonstrating the highest performance (94.4% accuracy, 94.7% precision). While the results highlight the efficacy of transformer architectures in disease detection, the study has notable shortcomings. The dataset size (450 images) is relatively small, which may limit the generalizability of the findings. Also, the accuracy is not satisfactory in terms of AD disease diagnosis.&#x0201d;</p><p>Table 1 is partitioned into two parts, Table 1 and Table 2, to add suggested new columns such as "data preprocessing/augmentation, originality, plus and minus aspects."</p><p>Comment-4:</p><p>Detail the information regarding the dataset along with the justifications in terms of training, validation, test distribution percentages and amounts, and data preprocessing/augmentation.</p><p>Response-4:</p><p>We add detail information on the dataset along with the justifications in terms of training, validation, test distribution percentages, and amounts. The following part is added to the paper in the 3.1 section, page 7:</p><p>&#x0201c;We utilized the Alzheimer&#x02019;s dataset from Kaggle [21], which includes gray MRI scans of the brain from individuals in various stages of Alzheimer&#x02019;s disease. The dataset encompasses four classes: MildDemented, ModerateDemented, NonDemented, and VeryMildDemented. It is organized into two folders: Train and Test, comprising a total of 5121 training images across the four classes and 1379 test images. Table 3 illustrates the distribution of images in the dataset. This dataset is valuable for predicting Alzheimer&#x02019;s disease stages using computer vision algorithms. Given the limited size of the test set, we allocated 10% of the training dataset for validation to ensure a robust evaluation of the model&#x02019;s performance. This approach was necessary because the dataset did not provide a separate validation set, and splitting the training set further would have risked overfitting due to the imbalanced class distribution. Using a small portion of the test set for validation allowed us to tune hyper-parameters and monitor model performance during training without significantly compromising the final evaluation on the remaining test data.&#x0201d;</p><p>Reviewer#3 Comments:</p><p>Comment-1:</p><p>The manuscript presents a valuable contribution to Alzheimer&#x02019;s disease detection using advanced transformers and computer vision techniques. The study is well structured and demonstrates impressive accuracy. To further enhance the work, we recommend citing "Detection of Alzheimer&#x02019;s and Parkinson&#x02019;s Diseases Using Deep Learning-Based Various Transformers Models" (<ext-link xlink:href="https://doi.org/10.3390/engproc2024073004)" ext-link-type="uri">https://doi.org/10.3390/engproc2024073004</ext-link>) to provide additional context and strengthen the research foundation. Overall, we suggest minor revisions and look forward to the updated submission.</p><p>Response-1:</p><p>We appreciate your constructive suggestion. Thank you.</p><p>The paper titled "Detection of Alzheimer&#x02019;s and Parkinson&#x02019;s Diseases Using Deep Learning-Based Various Transformers Models" (<ext-link xlink:href="https://doi.org/10.3390/engproc2024073004)" ext-link-type="uri">https://doi.org/10.3390/engproc2024073004</ext-link>) has been studied rigorously to establish context and strengthen the foundation of this study. Also, it has been added to the reference section.</p><p>In the paper page 20 in the References section, we add the following references:</p><p>30. Guven M. Detection of Alzheimer&#x02019;s and Parkinson&#x02019;s Diseases Using Deep Learning-Based Various Transformers Models. Eng. Proc. 2024;73(1):4. Presented at the 4th International Electronic Conference on Biosensors, 20&#x02013;22 May 2024. Available online: <ext-link xlink:href="https://sciforum.net/event/IECB2024." ext-link-type="uri">https://sciforum.net/event/IECB2024</ext-link>. <ext-link xlink:href="https://doi.org/10.3390/engproc2024073004." ext-link-type="uri">https://doi.org/10.3390/engproc2024073004</ext-link>.</p><p>In the paper in the Literature Review Section, page 4, line 151, we add the following paragraph:</p><p>&#x0201c;Recent work by [30] applied transformer-based models, including Swin Transformer, Vision Transformer (ViT), and Bidirectional Encoder Representation from Image Transformers (BEiT), to classify Alzheimer&#x02019;s and Parkinson&#x02019;s diseases using brain imaging data. The study utilized a balanced dataset of 450 brain images, achieving classification accuracy exceeding 80%, with ViT demonstrating the highest performance (94.4% accuracy, 94.7% precision). While the results highlight the efficacy of transformer architectures in disease detection, the study has notable shortcomings. The dataset size (450 images) is relatively small, which may limit the generalizability of the findings. Also, the accuracy is not satisfactory in terms of AD disease diagnosis.&#x0201d;</p><p>Reviewer#4 Comments:</p><p>Your manuscript offers a valuable contribution to AI-driven Alzheimer&#x02019;s disease diagnosis, demonstrating high accuracy and integrating explainability via LIME. The combination of transformers for MRI and RNA sequencing analysis is innovative and well-justified. The literature review is comprehensive, and the methodological framework is presented. However, the manuscript could benefit from additional validation on external datasets, a more detailed discussion on clinical applicability, and minor refinements in writing and formatting. These improvements would enhance the paper&#x02019;s clarity and real-world impact:</p><p>Comment-1:</p><p>The introduction clearly states the importance of early AD diagnosis and the role of AI in medical imaging, but the research gap could be more explicitly defined to emphasize why transformers are superior to CNNs or traditional machine learning models. Additionally, the claim that "there is no cure for AD" should be softened, as some experimental treatments exist. A brief outline of the paper structure at the end of the introduction would improve readability and guide the reader.</p><p>Response-1:</p><p>Thank you for your valuable suggestions.</p><p>We have revised the introduction to explicitly define the research gap, emphasizing the advantages of transformers over CNNs and traditional machine learning models. The statement "there is no cure for AD" is replaced as &#x0201c;Currently there is almost no cure for AD except very few experimental treatments.&#x0201d;</p><p>The following paragraph is added in the Introduction section, page 2, 32 lines:</p><p>&#x0201c;A notable advancement in DL is the development of the Transformer architecture, which employs attention mechanisms to capture relationships between elements in data. Initially designed for Natural Language Processing (NLP), the Transformer&#x02019;s architecture has been adapted for image analysis,</p><supplementary-material id="pone.0322607.s005" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers.docx</named-content></p></caption><media xlink:href="pone.0322607.s005.docx"/></supplementary-material></body></sub-article><sub-article article-type="editor-report" id="pone.0322607.r006" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0322607.r006</article-id><title-group><article-title>Decision Letter 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Uysal</surname><given-names>Fatih</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Fatih Uysal</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Fatih Uysal</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0322607" id="rel-obj006" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">26 Mar 2025</named-content>
</p><p>Leveraging Transformers and Explainable AI for Alzheimer&#x02019;s Disease Interpretability</p><p>PONE-D-24-39865R2</p><p>Dear Dr. Sammo,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">Editorial Manager&#x000ae;</ext-link>&#x000a0;and clicking the &#x02018;Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p><p>Kind regards,</p><p>Fatih Uysal, Ph.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>After considering the reviewer comments and evaluating the quality of the paper, it has been decided to accept it due to its potential to contribute to the literature and its final form.</p><p>Reviewers' comments:</p></body></sub-article><sub-article article-type="editor-report" id="pone.0322607.r007" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0322607.r007</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Uysal</surname><given-names>Fatih</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Fatih Uysal</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Fatih Uysal</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0322607" id="rel-obj007" related-article-type="reviewed-article"/></front-stub><body><p>PONE-D-24-39865R2</p><p>PLOS ONE</p><p>Dear Dr. Sammo,</p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p><p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p><p>* All references, tables, and figures are properly cited</p><p>* All relevant supporting information is included in the manuscript submission,</p><p>* There are no issues that prevent the paper from being properly typeset</p><p>If revisions are needed, the production department will contact you directly to resolve them. If no revisions are needed, you will receive an email when the publication date has been set. At this time, we do not offer pre-publication proofs to authors during production of the accepted work. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few weeks to review your paper and let you know the next and final steps.</p><p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p><p>If we can help with anything else, please email us at <email>customercare@plos.org</email>.</p><p>Thank you for submitting your work to PLOS ONE and supporting open access.</p><p>Kind regards,</p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Dr. Fatih Uysal</p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>