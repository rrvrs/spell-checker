<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408616</article-id><article-id pub-id-type="pmc">PMC12101840</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0321973</article-id><article-id pub-id-type="publisher-id">PONE-D-24-34732</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Linguistic Morphology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Semantics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Language Acquisition</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Age Groups</subject><subj-group><subject>Children</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Families</subject><subj-group><subject>Children</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Grammar</subject><subj-group><subject>Syntax</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Computational Techniques</subject><subj-group><subject>Split-Decomposition Method</subject><subj-group><subject>Multiple Alignment Calculation</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Quantifying the roles of visual, linguistic, and visual-linguistic complexity in noun and verb acquisition</article-title><alt-title alt-title-type="running-head">Quantifying the roles of visual, linguistic, and visual-linguistic complexity in noun and verb acquisition</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2660-5617</contrib-id><name><surname>Zhou</surname><given-names>Yuchen</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Tarr</surname><given-names>Michael J.</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Yurovsky</surname><given-names>Daniel</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Department of Psychology, Carnegie Mellon University, Pittsburgh, PA, United States</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Neuroscience Institute, Carnegie Mellon University, Pittsburgh, PA, United States</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, United States</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Kehler</surname><given-names>Andrew</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>UCSD: University of California San Diego, UNITED STATES OF AMERICA</addr-line>
</aff><author-notes><corresp id="cor001">* E-mail: <email>zhouyuchen@cmu.edu</email></corresp><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0321973</elocation-id><history><date date-type="received"><day>13</day><month>8</month><year>2024</year></date><date date-type="accepted"><day>13</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Zhou et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Zhou et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0321973.pdf">
</self-uri><abstract><p>Children often learn the meanings of nouns before they grasp the meanings of verbs. This discrepancy could arise from differences in the complexity of visual characteristics for categories that language describes, the inherent structure of language, or how these two sources of information align. To explore this question, we analyze visual and linguistic representations derived from large-scale pre-trained artificial neural networks of common nouns and verbs, focusing on these three hypotheses about early verb learning. Our findings reveal that verb representations are more variable and less distinct within their domain compared to nouns. When only one example per category is available, the alignment between visual and linguistic representations is weaker for verbs than for nouns. However, with multiple examples (mirroring human language development), this alignment improves significantly for verbs, approaching that of nouns. This suggests that the difficulty in learning verbs is not primarily due to mapping visual events to verb meanings, but rather in forming accurate representations of each verb category. Regression analysis indicates that visual variability significantly impacts verb learning, followed by the alignment of visual and linguistic elements and linguistic variability. Our study provides a quantitative and integrative framework to account for the challenges children face in early word learning, opening new avenues for resolving the longstanding debate on why verbs are harder to learn than nouns.</p></abstract><funding-group><award-group id="award001"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap>
</funding-source><award-id>220020506</award-id><principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2660-5617</contrib-id>
<name><surname>Zhou</surname><given-names>Yuchen</given-names></name>
</principal-award-recipient></award-group><award-group id="award002"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap>
</funding-source><award-id>220020506</award-id><principal-award-recipient>
<name><surname>Tarr</surname><given-names>Michael J.</given-names></name>
</principal-award-recipient></award-group><award-group id="award003"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000913</institution-id><institution>James S. McDonnell Foundation</institution></institution-wrap>
</funding-source><award-id>220020506</award-id><principal-award-recipient>
<name><surname>Yurovsky</surname><given-names>Daniel</given-names></name>
</principal-award-recipient></award-group><funding-statement>YZ, MT, and DY received support from the James S. McDonnell Foundation (<ext-link xlink:href="https://www.jsmf.org/" ext-link-type="uri">https://www.jsmf.org/</ext-link>), grant reference number 220020506. The foundation had no involvement in the design of the study, data collection and analysis, decision to publish, or manuscript preparation.</funding-statement></funding-group><counts><fig-count count="4"/><table-count count="0"/><page-count count="17"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All data and code used in this study are openly accessible through the Zenodo repository (DOI: <ext-link xlink:href="https://doi.org/10.5281/zenodo.7738503" ext-link-type="uri">https://doi.org/10.5281/zenodo.7738503)</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All data and code used in this study are openly accessible through the Zenodo repository (DOI: <ext-link xlink:href="https://doi.org/10.5281/zenodo.7738503" ext-link-type="uri">https://doi.org/10.5281/zenodo.7738503)</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Children&#x02019;s early vocabularies are dominated by nouns [<xref rid="pone.0321973.ref001" ref-type="bibr">1</xref>]. Although there is some variability in the size of the effect across cultures and languages, children learn nouns more quickly than verbs and other predicates [<xref rid="pone.0321973.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0321973.ref003" ref-type="bibr">3</xref>]. Efforts to understand the complexities involved in learning verbs have typically concentrated on identifying if the main obstacles arise from the organization of categories in the real world to which language refers, the intrinsic structure of the language, or the alignment between these two elements.</p><p>To know the meaning of a word, learners must learn the perceptual category to which each word refers [<xref rid="pone.0321973.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0321973.ref005" ref-type="bibr">5</xref>]. Consequently, much of the work on early word learning is concerned with the cognitive processes children use to map words to visual categories [<xref rid="pone.0321973.ref006" ref-type="bibr">6</xref>&#x02013;<xref rid="pone.0321973.ref008" ref-type="bibr">8</xref>]. In this context, one explanation for the challenges of verb learning focuses on the relative difficulty of visual category learning: referents labeled by verbs may be more variable or more confusable than those labeled by nouns [<xref rid="pone.0321973.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0321973.ref010" ref-type="bibr">10</xref>]. This difference arises because nouns typically correspond to objects that are naturally consistent and individuated, as proposed by the Natural Partitions Hypothesis [<xref rid="pone.0321973.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0321973.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0321973.ref012" ref-type="bibr">12</xref>]. Objects in the world tend to form discrete, concrete entities that are easier to recognize and categorize. For example, &#x0201c;dogs&#x0201d; differ from each other on a variety of dimensions, such as color, size, and fur texture, but share a common basic shape [<xref rid="pone.0321973.ref013" ref-type="bibr">13</xref>]. In contrast, &#x0201c;walks&#x0201d; differ from each other not only in dimensions such as speed, direction, number of limbs, and gait but also in the identity of the agent doing the walking.</p><p>A second explanation for the challenges of verb learning focuses on the linguistic contexts formed by other words in which a target word appears. Along the lines of Firth&#x02019;s intuition that &#x0201c;you shall know a word by the company it keeps&#x0201d; [<xref rid="pone.0321973.ref014" ref-type="bibr">14</xref>], congenitally blind children can correctly name the colors of common objects and even the similarities among these colors, even though color is a purely visual property [<xref rid="pone.0321973.ref015" ref-type="bibr">15</xref>]. In this context, the difficulty of verb learning may be attributable, at least in part, to the fact that verbs refer to relations among nouns and thus are likely to occur in more diverse linguistic contexts. Supporting this account, children learn verbs that occur in more restricted contexts earlier than verbs that occur in more diverse contexts [<xref rid="pone.0321973.ref016" ref-type="bibr">16</xref>].</p><p>Finally, the difficulty of learning verbs may emerge from the alignment between visual and linguistic sources of information, a phenomenon explained by the Relational Relativity Hypothesis [<xref rid="pone.0321973.ref009" ref-type="bibr">9</xref>]. Under this view, verbs are more difficult to learn than nouns because their linguistic usage is less transparently related to the events with which they occur. For example, English tends to encode the manner of the motion into the verb, but not the direction (e.g., as in &#x0201c;The bottle floated into the cave.&#x0201d;), while Spanish tends to lexicalize the direction of the movement when describing the same event, leaving manner as an optional adjunct (e.g., as in &#x0201c;La botella entr&#x000f3; en la cueva, flotando.&#x0201d;, meaning &#x0201c;The bottle entered the cave floating.&#x0201d;) [<xref rid="pone.0321973.ref009" ref-type="bibr">9</xref>]. Furthermore, the contexts of transitive verbs are fundamentally ambiguous unless the words for their agent and patient are known (e.g., as in &#x0201c;chase&#x0201d; and &#x0201c;flee&#x0201d;) [<xref rid="pone.0321973.ref017" ref-type="bibr">17</xref>]. Accordingly, verbs may require selecting among the available relationships and identifying the syntactic structure of the utterances.</p><p>Tests of these explanations have typically relied on cross-linguistic comparisons [<xref rid="pone.0321973.ref018" ref-type="bibr">18</xref>], studies of atypical learners [<xref rid="pone.0321973.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0321973.ref020" ref-type="bibr">20</xref>], or artificial language learning experiments that try to approximate the real-world learning problems faced by children [<xref rid="pone.0321973.ref021" ref-type="bibr">21</xref>]. Here, we introduce a novel method for quantifying the complexity of both the visual and linguistic contexts in which a word appears, as well as the alignment of both sources of information. By independently assessing these different sources of information for word learning, we can directly assess their explanatory value in addressing the challenges associated with learning verbs. Specifically, we examined how the complexity and alignment of visual and linguistic information impact the learning of common nouns and verbs, selecting these words based on their frequency of occurrence.</p></sec><sec id="sec002"><title>Measuring visual and linguistic complexity</title><p>To better understand how visual information, linguistic information, and visual/linguistic alignment contribute to differences in the learnability of nouns and verbs, it is critical to represent both sources of information in a unified manner. Historically, top-down, hand-designed approaches have predominated in the psychological study of visual meaning structures, where experimenters have explicitly defined and manipulated features such as color, size, and shape [<xref rid="pone.0321973.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0321973.ref023" ref-type="bibr">23</xref>]. In contrast, due to the arbitrary relationship between the form of a word and its meaning [<xref rid="pone.0321973.ref024" ref-type="bibr">24</xref>], a bottom-up approach has been adopted in the study of the structure of individual word meanings, where word meanings are assumed to emerge from the regularity of word statistics. Modern language models build on similar intuitions regarding word co-occurrences, but with more powerful learning algorithms and larger and more diverse training corpora. While these language models such as BERT and recent &#x0201c;Large Language Models&#x0201d; do not explicitly encode predefined meanings [<xref rid="pone.0321973.ref025" ref-type="bibr">25</xref>&#x02013;<xref rid="pone.0321973.ref029" ref-type="bibr">29</xref>], these models produce representations that align with human similarity judgments [<xref rid="pone.0321973.ref030" ref-type="bibr">30</xref>], predict human behavior in semantic fluency tasks [<xref rid="pone.0321973.ref031" ref-type="bibr">31</xref>], predict choices in classical decision-making tasks [<xref rid="pone.0321973.ref032" ref-type="bibr">32</xref>], and capture top-down linguistic features identified by linguists, such as syntax [<xref rid="pone.0321973.ref033" ref-type="bibr">33</xref>]. Therefore, we propose that embeddings derived from these language models serve as a reasonable approximation of meaning and leverage these advances to explore the underlying structure of word meanings.</p><p>In addition to generating word embeddings from the language model [<xref rid="pone.0321973.ref027" ref-type="bibr">27</xref>], we similarly use a pre-trained vision model to generate visual embeddings from pixels that represent the visual structure of the categories to which words refer [<xref rid="pone.0321973.ref034" ref-type="bibr">34</xref>]. Importantly, we intentionally chose a vision model trained via an unsupervised learning method to ensure that the model encodes only visual features, such as color, texture, and edges.</p></sec><sec id="sec003"><title>Learning verbs versus learning nouns</title><p>Given a unified way to represent the linguistic and visual dimensions of word meanings as vectors, we then apply common measures to examine whether the later onset of verb learning arises from one or more of the following factors: 1) identifying stable visual categories; 2) understanding meanings from linguistic contexts; or 3) aligning the visual and linguistic structures corresponding to the same word.</p><p>Intuitively, the meaning of a word may be hard to learn from the contexts in which it appears for two different reasons. First, different instances of a word may carry different meanings. This variability makes it difficult to converge on the central meaning of a word. Second, two different words may appear in similar contexts, making the words confusable. One method for quantifying these two sources of difficulty is to consider both the similarities between multiple instances of the same word and the dissimilarities between different words. For example, as illustrated in <xref rid="pone.0321973.g001" ref-type="fig">Fig 1A</xref>, the distribution of verb exemplars may be more scattered compared to noun exemplars. At the same time, as illustrated in <xref rid="pone.0321973.g001" ref-type="fig">Fig 1B</xref>, verb categories may generally be less similar to one another as compared to noun categories.</p><fig position="float" id="pone.0321973.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0321973.g001</object-id><label>Fig 1</label><caption><title>Potential sources of difficulty for learning nouns and verbs as illustrated by concepts in embedding spaces.</title><p>Large dots represent category centroids, small dots represent exemplars of corresponding categories, and colored ellipses represent 68% confidence (one standard deviation) areas for each category (see Methods for visualization details). (A) Within each category, verb exemplars are more variable as compared to noun exemplars; (B) Verb categories overlap more with one another and are less distinguishable from one another as compared to noun categories; (C) Within the verb system, the structures of linguistic and visual representations are inconsistent across modalities, while in the noun system, the structures of linguistic representations and visual representations are better aligned.</p></caption><graphic xlink:href="pone.0321973.g001" position="float"/></fig><p>We can also measure, as illustrated in <xref rid="pone.0321973.g001" ref-type="fig">Fig 1C</xref>, the alignment between the linguistic representation of a word and the visual representation of the corresponding category. Prior work has demonstrated that representations across these two modalities tend to be highly aligned for concrete nouns, and, furthermore, that the earliest learned words are more highly aligned than later learned words [<xref rid="pone.0321973.ref035" ref-type="bibr">35</xref>]. Here, we explore whether the degree of alignment between the linguistic and visual meanings of words helps distinguish noun learning from verb learning. We hypothesize that nouns will have a visual structure more aligned with their linguistic structure, whereas the visual and linguistic structures of verbs will be less aligned.</p><p>Finally, although each of these measures may independently predict some of the difficulty of word learning, we are interested in the main obstacles that children face when acquiring the meaning of words. Accordingly, we estimate the relative contributions of each of these measures in a single regression analysis to determine how well each factor can predict the age at which a given word is learned.</p></sec><sec sec-type="results" id="sec004"><title>Results</title><p>As a first step, we examine the category structure in each modality using neural network-based representations (see Methods for details) of 210 nouns and 210 verbs from the Visual Genome (VG) dataset [<xref rid="pone.0321973.ref036" ref-type="bibr">36</xref>]. The complete list of words is provided in <xref rid="pone.0321973.s005" ref-type="supplementary-material">S2 Appendix</xref>. We use this word dataset to investigate the difficulty faced by a simulated language learner in forming new categories and differentiating between separable categories. Under this rubric, the category structure is captured by two measures: 1) the degree of dispersal for each individual category (&#x0201c;variability&#x0201d;); and 2) how far categories are from each other (&#x0201c;discriminability&#x0201d;). Accordingly, these structural measures of representational space are quantified by two metrics: the variability of individual categories and the discriminability of different categories (see Methods for mathematical definitions).</p><sec id="sec005"><title>Category structure in the visual modality</title><p>The visual variability of verbs is significantly greater than that of nouns, <inline-formula id="pone.0321973.e001"><alternatives><graphic xlink:href="pone.0321973.e001.jpg" id="pone.0321973.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>418</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>11.71</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.001, and the visual discriminability of verbs is significantly lower than that of nouns, <inline-formula id="pone.0321973.e002"><alternatives><graphic xlink:href="pone.0321973.e002.jpg" id="pone.0321973.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>418</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>9.82</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.001 (<xref rid="pone.0321973.g002" ref-type="fig">Fig 2A</xref>). Consistent with our hypothesis, objects labeled by the same noun are more like one another than events labeled by the same verb, and objects described by different nouns are less confusable with one another than events described by different verbs.</p><fig position="float" id="pone.0321973.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0321973.g002</object-id><label>Fig 2</label><caption><title>Distribution of variability and discriminability of nouns and verbs from the Visual Genome (VG) dataset.</title><p>Dashed lines denote 25th-, 50th-, and 75th-percentile of distributions. (A) Visual modality: Verb categories are more variable and less distinguishable than noun categories. (B) Language modality: Verb categories are equally variable but less distinguishable than noun categories.</p></caption><graphic xlink:href="pone.0321973.g002" position="float"/></fig></sec><sec id="sec006"><title>Category structure in the language modality</title><p>Although the difference in linguistic variability is not statistically significant between nouns and verbs, <inline-formula id="pone.0321973.e003"><alternatives><graphic xlink:href="pone.0321973.e003.jpg" id="pone.0321973.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>418</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1.15</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">p</italic>&#x02009;=&#x02009;0.250, the linguistic discriminability of verbs is reliably lower than the linguistic discriminability of nouns, <inline-formula id="pone.0321973.e004"><alternatives><graphic xlink:href="pone.0321973.e004.jpg" id="pone.0321973.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>418</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>7.63</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.001 (<xref rid="pone.0321973.g002" ref-type="fig">Fig 2B</xref>). These results provide evidence that the linguistic meanings of verbs are less distinguishable compared to those of nouns.</p><p>In sum, the observations from unimodal visual and linguistic analyses offer an explanation for why verbs are harder to learn than nouns: Compared to nouns, it is harder to capture the central meanings of the visual and linguistic categories of verbs and to distinguish between two visual categories of verbs.</p></sec><sec id="sec007"><title>Alignment between visual and language modalities</title><p>Another challenge in learning verbs is that they filter and highlight specific relational information from physical events. This requires learners to determine which physically grounded details are captured within different verb categories. To explore this possibility, we examine the crossmodal alignment between the visual and language modalities of nouns and verbs, asking whether visual categories can be readily mapped onto linguistic categories. We term this measure &#x0201c;Alignment Strength&#x0201d; (see Methods for mathematical definition). To make well-controlled comparisons across systems with different words, we also measure the relative strength of alignment, that is, how well aligned a mapping is compared to alternative permuted mappings in which linguistic and visual representations of different words are randomly matched.</p><p><xref rid="pone.0321973.g003" ref-type="fig">Fig 3A</xref> shows the distribution of alignment strengths of true mappings and permuted mappings in systems with one visual exemplar representing each visual category and one linguistic exemplar representing each linguistic category. The mean alignment strength of true mappings is significantly different from the mean of permuted mappings for both nouns, <inline-formula id="pone.0321973.e005"><alternatives><graphic xlink:href="pone.0321973.e005.jpg" id="pone.0321973.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mn>998</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>30.35</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.001, and verbs, <inline-formula id="pone.0321973.e006"><alternatives><graphic xlink:href="pone.0321973.e006.jpg" id="pone.0321973.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mn>998</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>11.17</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.001, suggesting that true mappings are reliably more aligned than randomly permuted mappings.</p><fig position="float" id="pone.0321973.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0321973.g003</object-id><label>Fig 3</label><caption><title>Alignment between visual and language modalities.</title><p>(A) The distribution of alignment strengths of true mappings (colors) and randomly permuted mappings (grey) with one linguistic and one visual exemplar per category. Nouns are more well-aligned than verbs. (B) The distribution of permuted mappings (grey) in systems with 8 linguistic exemplars and 8 visual exemplars aggregated per category. The alignment strength improves considerably as compared to that in the single-exemplar condition (as in A) for both nouns and verbs. Asterisks represent the mean alignment strengths of the true mappings in both (A) and (B). As visual exemplars (C) or linguistic exemplars (D) aggregate when the other modality has sufficient exemplars to form well-learned prototypes, the relative alignment strength, which measures the percentage of permuted mappings that are less well-aligned than the true mapping, increases and gradually converges to 1.0. The noun system is more well-aligned than the verb system in early aggregation stages, but the verb system becomes almost as well-aligned as the noun system with a sufficient number of learning instances. Error bars represent 95% confidence intervals computed by bootstrapping over 1,000 simulations at each level. (E, F) Relative alignment strength as a function of both the number of linguistic and visual exemplars. Nouns can become highly aligned with only one visual exemplar and sufficient linguistic exemplars per category. In contrast, sufficient visual exemplars and sufficient linguistic exemplars are both necessary for the formation of a well-aligned verb system.</p></caption><graphic xlink:href="pone.0321973.g003" position="float"/></fig><p>The mean alignment strength of true mappings for nouns (<inline-formula id="pone.0321973.e007"><alternatives><graphic xlink:href="pone.0321973.e007.jpg" id="pone.0321973.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>0.027</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) is higher than <inline-formula id="pone.0321973.e008"><alternatives><graphic xlink:href="pone.0321973.e008.jpg" id="pone.0321973.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:mn>75.6</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> of permuted mappings, while the mean alignment strength of true mappings for verbs (<inline-formula id="pone.0321973.e009"><alternatives><graphic xlink:href="pone.0321973.e009.jpg" id="pone.0321973.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mspace width="0.167em"/><mml:mn>0.009</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) is higher than <inline-formula id="pone.0321973.e010"><alternatives><graphic xlink:href="pone.0321973.e010.jpg" id="pone.0321973.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mn>59.8</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> of permuted mappings. Consistent with our hypothesis, as captured by both absolute alignment strength and relative alignment strength, it is more difficult to establish correct mappings for verbs compared to nouns. Thus, identifying the correct mapping between visual categories and linguistic categories may be more challenging in verb learning than in noun learning.</p></sec><sec id="sec008"><title>Improving alignment by exemplar aggregation</title><p>An alternative explanation for the lower alignment strength observed for verbs could be the high variability and low discriminability of verb categories. As such, the specific meaning of a verb may not be well represented by a single exemplar. Consistent with how humans gradually acquire word meanings over multiple exemplars, we aggregate exemplars by taking their arithmetic mean to examine the change in alignment strength as a function of the number of exemplars.</p><p>For visual aggregation, we form well-learned stable linguistic prototypes by aggregating 20 linguistic exemplars per category. We then calculate the relative alignment strength of the system as a function of how many visual exemplars are provided (<xref rid="pone.0321973.g003" ref-type="fig">Fig 3C</xref>). For nouns, the observed true mapping has a higher alignment strength than nearly all of the permuted mappings even with just one visual exemplar per category; additional exemplars produce little improvement in the alignment strength. In contrast, the alignment strength is quite low for verbs with one visual exemplar per category; however, additional exemplars produce a progressive increase in the alignment strength. Indeed, at the point at which eight visual exemplars per category are aggregated, the alignment strength of the verb system approaches that of the noun system.</p><p>Mirroring visual aggregation, with well-learned visual prototypes formed by 20 exemplars per category provided, the alignment strengths of both nouns and verbs improve as linguistic exemplars are aggregated (<xref rid="pone.0321973.g003" ref-type="fig">Fig 3D</xref>). However, in contrast to visual aggregation, for linguistic aggregation, we find that one linguistic exemplar per category cannot produce a good mapping for the noun system. Rather, three or more linguistic exemplars per category are needed to build a well aligned noun mapping. This difference in the number of required exemplars between visual aggregation and linguistic aggregation is consistent with empirical observations from previous studies showing that 3- and 4-month-old infants can discriminate two basic-level visual categories (i.e., cats and horses) after 3 exposures to each stimulus image [<xref rid="pone.0321973.ref037" ref-type="bibr">37</xref>], yet 3- and 4-year-old children cannot understand the meanings of novel words very well after 12 exposures to the words in storybooks [<xref rid="pone.0321973.ref038" ref-type="bibr">38</xref>]. For verbs, similar to our findings for visual aggregation, the alignment strength is quite low with one linguistic exemplar per category; however, additional exemplars produce a progressive increase in the alignment strength.</p><p>Two-dimensional aggregation simulations, where both visual and linguistic exemplars are aggregated, reveal the optimal combination of visual and linguistic exemplars that most efficiently increase alignment strength at different learning stages (<xref rid="pone.0321973.g003" ref-type="fig">Fig 3E</xref> and <xref rid="pone.0321973.g003" ref-type="fig">3F</xref>). For example, suppose at a given point the gradient points 45&#x02009;degrees counterclockwise from the x-axis. In that case, it suggests an optimal 1:1 ratio between visual and linguistic exemplars, meaning the learner needs both visual and linguistic input equally. Conversely, if the gradient points close to 90&#x02009;degrees, it indicates that visual exemplars are more impactful, and adding linguistic exemplars has minimal effect on further increasing alignment strength. We found that verb systems demonstrate a quasi-symmetric pattern (e.g., the alignment strength of the verb system with one visual exemplar and eight linguistic exemplars is close to that of eight visual exemplars and one linguistic exemplar), whereas the noun system displays a strong bias towards linguistic input (e.g., the alignment strength of the noun system with one visual exemplar and eight linguistic exemplars is much better than that of the noun system with eight visual exemplars and one linguistic exemplar). This result is consistent with the <bold>natural partitions hypothesis</bold> [<xref rid="pone.0321973.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0321973.ref011" ref-type="bibr">11</xref>], which posits that concrete nouns are naturally individuated referents and therefore noun learning relies less on learning corresponding visual categories.</p><p>To summarize, aggregating multiple exemplars improves the alignment strength for both nouns and verbs. Importantly, consistent with the success of children learners over time, with sufficient exemplars, the visual and linguistic meanings of verbs can be transparently mapped. This finding contradicts our initial hypothesis, suggesting that there is no inherent global misalignment between visual events and verb usage. The reason is that if such a misalignment existed, it would not be possible to raise the alignment strength to an almost perfect level, no matter how many exemplars are provided. Therefore, we conclude that the difficulty in verb learning largely lies in the challenges of extracting invariant categorical representations of verbs rather than any misalignment between visual events and verb usage. Supporting this conclusion, a replication using a second image dataset yields similar qualitative results (refer to <xref rid="pone.0321973.s001" ref-type="supplementary-material">S1 Appendix</xref>). These findings are illustrated in <xref rid="pone.0321973.s002" ref-type="supplementary-material">S1 Fig</xref>, <xref rid="pone.0321973.s003" ref-type="supplementary-material">S2 Fig</xref>, and <xref rid="pone.0321973.s004" ref-type="supplementary-material">S3 Fig</xref>. The complete list of words is provided in <xref rid="pone.0321973.s006" ref-type="supplementary-material">S3 Appendix</xref>.</p></sec><sec id="sec009"><title>Action-based verb representation</title><p>Nouns typically denote objects that are stable physical entities. In contrast, verbs typically denote actions that unfold dynamically over time and space [<xref rid="pone.0321973.ref039" ref-type="bibr">39</xref>]. Consequently, verb representations derived from static images of actions may miss important aspects of verb learning. As an illustrative example, single images of &#x0201c;stand&#x0201d; and &#x0201c;walk&#x0201d; are visually similar, but the two actions are easily distinguishable from each other in videos. However, it is also possible that a few &#x0201c;key frames&#x0201d; depicting the most diagnostic information may be sufficient to learn the meaning of a verb. Videos may be less efficient than images because they contain irrelevant background objects and actions that may make learning more challenging. To explore these questions, we replicate our analyses using 305 action words from a video dataset, Moments in Time [<xref rid="pone.0321973.ref040" ref-type="bibr">40</xref>], and examine whether the dynamic nature of verbs impacts the alignment of the verb system. The complete list of words is provided in <xref rid="pone.0321973.s007" ref-type="supplementary-material">S4 Appendix</xref>.</p><p>Representations derived from videos of actions (MiT) exhibit a similar alignment pattern to representations derived from static images (VG Verbs): the alignment strength is low at first, but improves incrementally and ultimately hits the ceiling after multiple linguistic/visual exemplars are aggregated (<xref rid="pone.0321973.s003" ref-type="supplementary-material">S2 Fig</xref>). Notably, consistent with the idea that action dynamics are helpful in verb learning, the number of exemplars per category required for good mapping using videos is fewer compared to the number required using static images. This suggests that videos do carry dynamic information that is relevant to verb meaning and therefore support verb learning better than static images.</p></sec><sec id="sec010"><title>Unpacking the contributions of different factors</title><p>Our analyses demonstrate that multiple factors, including variability, discriminability, and alignment strength, reflect important characteristics of word learning. Furthermore, a wide variety of studies have documented that word frequency is a significant predictor of when a word will be learned [<xref rid="pone.0321973.ref041" ref-type="bibr">41</xref>&#x02013;<xref rid="pone.0321973.ref045" ref-type="bibr">45</xref>]. To answer the ultimate question of what drives early word learning, we use a gradient-boosted trees model, &#x0201c;XGBoost&#x0201d; [<xref rid="pone.0321973.ref046" ref-type="bibr">46</xref>], to run a regression analysis that takes into account all of these factors, as well as word frequency and word type to predict the learnability of words. As a proxy for the learnability of words, Age of Acquisition (AoA) is measured using CHILDES [<xref rid="pone.0321973.ref047" ref-type="bibr">47</xref>], a child-directed speech corpus. Following previous literature [<xref rid="pone.0321973.ref042" ref-type="bibr">42</xref>], AoA is defined as the age at which 50% of children in the corpus demonstrate an understanding of a given word.</p><p>For better interpretability of results, we adopt SHAP (SHapley Additive exPlanations) [<xref rid="pone.0321973.ref048" ref-type="bibr">48</xref>] method to visualize the predictions made by the model. <xref rid="pone.0321973.g004" ref-type="fig">Fig 4A</xref> illustrates the contribution of each factor in predicting AoA. Each dot corresponds to one word category, and the position of the dot regarding 0 on the x-axis reflects whether this factor positively or negatively contributes to the value of AoA. Consistent with previous studies, we find that frequency is a reliable predictor of AoA: words with high frequencies typically have low AoAs. Word type is also distinctly associated with the learnability of words: without other information, knowing a word is a verb always increases the expected AoA of the word.</p><fig position="float" id="pone.0321973.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0321973.g004</object-id><label>Fig 4</label><caption><title>Visualization of feature contributions in word learning.</title><p>An XGBoost model was trained to predict the learnability of words as measured by Age of Acquisition (AoA). SHAP (SHapley Additive exPlanations) [<xref rid="pone.0321973.ref048" ref-type="bibr">48</xref>] values were computed to interpret the relative contribution of each feature. (A) Summary of the importance of all features in predicting AoA. Each dot represents one word category. If the SHAP value of a category with regard to a particular feature is positive, then this feature positively impacts the predicted variable in this observation and vice versa. For instance, frequency negatively influences AoA because high word frequency generally corresponds to a low SHAP value (&#x0201c;type&#x0201d; refers to word type with verbs encoded as 1 and nouns encoded as 0). (B) Global feature importance quantified by the mean absolute SHAP values of each feature across all word categories. Features that are positively correlated with the predicted variable (AoA) are colored red, while features that are negatively correlated with AoA are colored blue.</p></caption><graphic xlink:href="pone.0321973.g004" position="float"/></fig><p>Because the contributions of other factors are less obvious visually, we compute the global importance of each factor by calculating the mean of absolute SHAP values of each factor across all words (<xref rid="pone.0321973.g004" ref-type="fig">Fig 4B</xref>). All factors, with the exception of linguistic discriminability, are correlated with AoA as hypothesized: word frequency, alignment, and visual discriminability positively impact AoA, while visual variability, word type, and linguistic variability negatively impact AoA. Interestingly, visual variability is such a strong predictor of AoA that it is comparable to word type in magnitude. In contrast, both visual and linguistic discriminability show only a small contribution in predicting AoA, which may explain the incorrect prediction of our model with respect to the impact of linguistic discriminability.</p><p>Compared to the effects of factors with established benchmarks (word frequency and word type), visual variability appears to be the most important among the additional factors we considered. At the same time, alignment and linguistic variability are also reliably associated with the difficulty of learning words, while neither visual discriminability nor linguistic discriminability predicts when a word will be learned.</p></sec></sec><sec sec-type="conclusions" id="sec011"><title>Discussion</title><p>A long-standing question in the study of language development is why verbs are harder to learn than nouns. In this paper, we examine the relative contribution of three sources of information: (1) the structure of visual categories in the world to which language refers; (2) the structure of language itself; and (3) the interplay between visual and linguistic information. Supporting the natural partitions hypothesis, which states that nouns associated with concrete objects are easy to acquire because they are naturally consistent and individuated referents [<xref rid="pone.0321973.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0321973.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0321973.ref012" ref-type="bibr">12</xref>], we demonstrate that for nouns: (1) robust mappings between nouns and corresponding visual objects can be established with just one encounter with the visual object; (2) robust mappings between nouns and corresponding visual objects can be established with a relatively small number of encounters with linguistic instances for each noun; (3) grasping word meanings, rather than visual object recognition, is the main obstacle to overcome in noun learning. In contrast, verbs associated with dynamic events are held to be harder to acquire because they are more variable. That is, the actions themselves and the identity of the agents performing the actions and the patients receiving the actions vary across different contexts. Supporting this hypothesis, an examination of embedding spaces demonstrates that verbs are more visually variable than nouns. More generally, we find that all three factors discussed above contribute to the difficulty of word learning, but visual variability is the most predictive of the age at which children learn individual words. This finding is consistent with earlier observations that more imaginable concepts (evoking mental images more easily) are acquired more rapidly and earlier in development [<xref rid="pone.0321973.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0321973.ref039" ref-type="bibr">39</xref>].</p><p>Another theoretical point is that verbs are hard to learn because they make a selection of both particular objects in the environment and the relationships between those objects. This is different from nouns, which involve a direct mapping between words and unitary visual objects. Reflecting this difference between verbs and nouns, there is evidence that the selection of particular properties of actions emphasized by verbs varies across languages while nouns are relatively consistent cross-linguistically [<xref rid="pone.0321973.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0321973.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0321973.ref018" ref-type="bibr">18</xref>]. One implication of this theory is that children need to put more effort into identifying the core properties of verb categories as they learn a language (e.g., attending more to the structure of visual examples of verbs and to the larger context of linguistic examples). However, our results suggest that increased attention or other forms of effort to identify core properties of verbs are not necessarily a difficulty to the effective acquisition of verb meanings. We demonstrate that mapping referents with verbs can be resolved by exemplar aggregation, suggesting that the main difficulty in learning verbs lies in the challenge of extracting invariant categorical representations of verbs rather than any inherent global misalignment between visual events and verb usage. In other words, the way by which the English linguistic space is carved up is not arbitrary and is, in fact, structured similarly to how events visually appear to be. To better understand what kinds of relational information are selected to lexicalize events and how that may affect language learning, future studies should pursue cross-linguistic comparisons. It is also possible that alignment strength as a global measurement is not sensitive to local distortions (children may confuse &#x0201c;push&#x0201d; with &#x0201c;pull&#x0201d; but are less likely to confuse &#x0201c;push&#x0201d; with &#x0201c;run&#x0201d;), so more fine-grained analyses would be helpful in addressing this question.</p><p>Our analyses leverage neural network derived representations to simulate how a learner might encounter noun and verb exemplars in real life and extract key information. Although the physical architectures and training objectives of human brains and neural networks clearly differ, both must efficiently capture statistical patterns from sensory inputs to support downstream tasks. As a result, they produce representations that, while potentially differing in form, often encode similar underlying information. Some recent evidence indicates that models performing better on downstream tasks exhibit representations more closely aligned with human neural activity [<xref rid="pone.0321973.ref049" ref-type="bibr">49</xref>, <xref rid="pone.0321973.ref050" ref-type="bibr">50</xref>]. It is also important to note that each neural network, constrained by its architecture, training algorithm, and the dataset used for training, instantiates only one way of representing an image or a word. These underlying assumptions can introduce biases into subsequent analyses. However, models with a greater number of parameters and more diverse training data may help to mitigate these limitations, as they are less likely to be biased towards a specific subset of stimuli and their representations encode more comprehensive and nuanced information.</p><p>Our present work stands in contrast to previous studies on early language acquisition that generally relied on a limited number of case studies or in-lab experiments where stimuli were designed and manipulated manually. Instead, we sample learning exemplars of a relatively large number of words from large corpora and use artificial neural networks that were trained in an unsupervised fashion to allow representations to emerge bottom-up. Most importantly, by encoding visual and linguistic information in the same representational form, our quantitative analyses provide an integrative view, rather than an all-or-nothing mindset, on the difficulties that children face in acquiring nouns and verbs. As such, our approach opens a new door for resolving the long-lasting debate on why verbs are harder to learn than nouns.</p><p>The mechanisms underlying children&#x02019;s rapid word learning are undoubtedly more complex than what we have considered in our analyses. For example, our main focus is on word-level semantic learning and does not take into account other documented factors, including inflectional morphology [<xref rid="pone.0321973.ref051" ref-type="bibr">51</xref>], syntactic constructions [<xref rid="pone.0321973.ref052" ref-type="bibr">52</xref>], and the ease with which a word can be segmented from a continuous speech stream [<xref rid="pone.0321973.ref053" ref-type="bibr">53</xref>]. Moreover, in our approach, children are treated as passive learners in that we only examine language comprehension; factors such as interactions between children and parents/caregivers [<xref rid="pone.0321973.ref054" ref-type="bibr">54</xref>, <xref rid="pone.0321973.ref055" ref-type="bibr">55</xref>] and children&#x02019;s intentions [<xref rid="pone.0321973.ref056" ref-type="bibr">56</xref>, <xref rid="pone.0321973.ref057" ref-type="bibr">57</xref>] are also likely to play a role in guiding language learning. In addition, while our work effectively explains the general trend of why nouns are typically learned earlier than verbs, it does not account for finer-grained effects, such as certain verbs being learned earlier than some nouns or how early-learned nouns and verbs may facilitate the acquisition of other words [<xref rid="pone.0321973.ref058" ref-type="bibr">58</xref>]. However, the primary contribution of our study lies not only in our results, but in presenting a methodological framework for exploring early language acquisition. This framework is flexible and can be readily applied to investigate specific subsets of words. For instance, it could be used to compare early- and late-learned words or to analyze how the first acquired words contribute to the learning of subsequent vocabulary. We believe this approach is an important step forward and has the potential to model a wide range of phenomena in language development and broad concept learning.</p></sec><sec sec-type="materials|methods" id="sec012"><title>Materials and methods</title><sec id="sec013"><title>Datasets and models</title><p>Visual Genome (VG) was used as the dataset to sample visual images of nouns and verbs [<xref rid="pone.0321973.ref036" ref-type="bibr">36</xref>]. Images in VG have dense annotations of objects and relationships between objects in the form of Wordnet Synset [<xref rid="pone.0321973.ref059" ref-type="bibr">59</xref>], which lemmatizes synonymous concepts. We selected the 210 verbs that appear no less than 20 times in the whole dataset to form the verb system. Because nouns are generally more frequent than verbs, we selected the most frequent 210 nouns to ensure the size of the noun system is the same as the size of the verb system. The region of interest (ROI) of a noun is determined by the bounding box of the annotated object, and the ROI of a verb is determined by the union of the bounding boxes of any agents and/or objects that take part in the verb.</p><p>The video dataset we used is Moments in Time (MiT), which includes a collection of one million three-second video clips [<xref rid="pone.0321973.ref040" ref-type="bibr">40</xref>]. Each video clip features only one action and was annotated with one of 305 action words. Considering the length of videos, the visual representation of each video was generated by taking the arithmetic mean of the visual representations of 8 frames uniformly sampled from the video.</p><p>Visual representations were derived from a neural network based on ResNet-50 architecture [<xref rid="pone.0321973.ref060" ref-type="bibr">60</xref>]. The network was pre-trained in an unsupervised fashion by an algorithm called Swapping Assignments between Views on the ImageNet ILSVRC2012 dataset [<xref rid="pone.0321973.ref034" ref-type="bibr">34</xref>]. The unsupervised training paradigm ensures that the model was trained only to encode low-level visual features (e.g., colors, dots, edges).</p><p>For language data, we sampled texts that contain the 210 nouns/verbs from Wikipedia articles. We defined the context of a word to be the 25 words that precede the target word and the 25 that follow it, which sums up to a window of 51 words.</p><p>Bidirectional Encoder Representations from Transformers (BERT) [<xref rid="pone.0321973.ref027" ref-type="bibr">27</xref>] was employed to extract language representations of words because of its ability to produce contextual-sensitive embeddings as the contexts of the target word change. We obtained the pre-trained uncased BERT model from the Transformers library [<xref rid="pone.0321973.ref061" ref-type="bibr">61</xref>]. We used the hidden states of the last layer before the language modeling head as the embedding of the target word.</p></sec><sec id="sec014"><title>Visualization in schematic diagram</title><p>We used representations from real data to illustrate our hypotheses. For each category, 20 exemplars were sampled and t-SNE was applied to project representations to a two-dimensional plane. Data were assumed to follow 2D Gaussian distributions for the estimation of parameters of ellipses. Embeddings in the same modality were plotted on the same scale for visual comparison. Linguistic representations from BERT were taken as an example for demonstration in panel A and panel B.</p></sec><sec id="sec015"><title>Variability and discriminability</title><p>We quantified the structure of embedding spaces by two metrics: the variability of individual categories, which is computed as the average Euclidean distance from exemplars to category centroids, and the discriminability of different categories, which is computed as the average Euclidean distance from all exemplars in each category to category centroids of all other categories. Mathematically, given a system of <italic toggle="yes">N</italic> words, <inline-formula id="pone.0321973.e011"><alternatives><graphic xlink:href="pone.0321973.e011.jpg" id="pone.0321973.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, with <italic toggle="yes">P</italic> linguistic exemplars <inline-formula id="pone.0321973.e012"><alternatives><graphic xlink:href="pone.0321973.e012.jpg" id="pone.0321973.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <italic toggle="yes">Q</italic> visual exemplars <inline-formula id="pone.0321973.e013"><alternatives><graphic xlink:href="pone.0321973.e013.jpg" id="pone.0321973.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for each category, linguistic category centroids <inline-formula id="pone.0321973.e014"><alternatives><graphic xlink:href="pone.0321973.e014.jpg" id="pone.0321973.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and visual category centroids <inline-formula id="pone.0321973.e015"><alternatives><graphic xlink:href="pone.0321973.e015.jpg" id="pone.0321973.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mi>V</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>v</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> are computed as</p><disp-formula id="pone.0321973.e016">
<alternatives><graphic xlink:href="pone.0321973.e016.jpg" id="pone.0321973.e016g" position="anchor"/><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>v</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>Q</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>Linguistic variability and visual variability are then defined as</p><disp-formula id="pone.0321973.e017">
<alternatives><graphic xlink:href="pone.0321973.e017.jpg" id="pone.0321973.e017g" position="anchor"/><mml:math id="M17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover></mml:mstyle><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><disp-formula id="pone.0321973.e018">
<alternatives><graphic xlink:href="pone.0321973.e018.jpg" id="pone.0321973.e018g" position="anchor"/><mml:math id="M18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>Q</mml:mi></mml:munderover></mml:mstyle><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>while linguistic and visual discriminability are defined as</p><disp-formula id="pone.0321973.e019">
<alternatives><graphic xlink:href="pone.0321973.e019.jpg" id="pone.0321973.e019g" position="anchor"/><mml:math id="M19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover></mml:mstyle><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><disp-formula id="pone.0321973.e020">
<alternatives><graphic xlink:href="pone.0321973.e020.jpg" id="pone.0321973.e020g" position="anchor"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>Q</mml:mi></mml:munderover></mml:mstyle><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
</disp-formula><p>where <italic toggle="yes">dist</italic> computes the Euclidean distance between two points in an embedding space.</p></sec><sec id="sec016"><title>Alignment strength</title><p>To estimate the alignment between words&#x02019; linguistic and visual representations, we used a metric termed &#x0201c;Alignment Strength&#x0201d;, which asks whether words&#x02019; linguistic similarities are correlated with their visual similarities. Computationally, we first constructed the similarity matrices of representations within the visual modality, <inline-formula id="pone.0321973.e021"><alternatives><graphic xlink:href="pone.0321973.e021.jpg" id="pone.0321973.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, and language modality, <italic toggle="yes">S</italic><sub><italic toggle="yes">L</italic></sub>, respectively by computing the pairwise cosine similarity of representations. The alignment strength was then computed as Spearman&#x02019;s rank correlation between the upper triangle of <inline-formula id="pone.0321973.e022"><alternatives><graphic xlink:href="pone.0321973.e022.jpg" id="pone.0321973.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <italic toggle="yes">S</italic><sub><italic toggle="yes">L</italic></sub>.</p><p>The relative alignment strength is defined as the percentage of misaligned mappings that have lower alignment strength than the true mapping system. For each true mapping, we randomly sampled 1,000 mappings that were misaligned (e.g., the visual &#x0201c;dog&#x0201d; mapped to language &#x0201c;cat&#x0201d; while the visual &#x0201c;cat&#x0201d; mapped to language &#x0201c;dog&#x0201d;), and then computed the alignment strength of those misaligned systems as a reference for the true mapping.</p><p>We randomly sampled 1,000 systems to estimate the alignment strength of systems with 1 visual exemplar and 1 linguistic exemplar per category (<xref rid="pone.0321973.g003" ref-type="fig">Fig 3A</xref>). Similarly, we randomly sampled 1,000 systems to estimate the alignment strength of systems with 8 visual exemplars and 8 linguistic exemplars per category (<xref rid="pone.0321973.g003" ref-type="fig">Fig 3B</xref>).</p></sec><sec id="sec017"><title>Exemplar aggregation</title><p>Exemplar aggregation was achieved by taking the arithmetic mean of exemplars from the same category. For visual aggregation, we first created stable linguistic prototypes from 20 linguistic exemplars per category (<italic toggle="yes">post-hoc</italic> experiments confirmed that 20 exemplars per category can form well-learned linguistic prototypes). Instead of forming systems by purely random sampling, we incrementally added one visual exemplar to the existing system and compared the alignment strength of the true system with 1,000 permuted mappings to simulate the learning process of humans. We ran 1,000 simulations for each level of number of visual exemplars. The manipulation of linguistic aggregation was identical to visual aggregation, where the roles of visual modality and language modality were exchanged.</p><p>The procedure for two-dimensional aggregation was also similar, where 500 simulations were run for each possible combination of number of visual exemplars and number of linguistic exemplars.</p></sec><sec id="sec018"><title>Regression analysis</title><p>AoAs were measured directly from children&#x02019;s utterances in child-directed speech by vocabulary data retrieved from the Wordbank database [<xref rid="pone.0321973.ref062" ref-type="bibr">62</xref>]. Specifically, AoA is defined as the age at which 50% of children in the corpus demonstrate understanding of a given word, following previous literature [<xref rid="pone.0321973.ref042" ref-type="bibr">42</xref>]. Estimates of the frequency of words that children hear were based on parental/caregivers&#x02019; speech transcripts from the CHILDES database [<xref rid="pone.0321973.ref047" ref-type="bibr">47</xref>].</p><p>Instead of conventional linear models, we chose a type of gradient-boosted tree model called XGBoost [<xref rid="pone.0321973.ref046" ref-type="bibr">46</xref>] for the regression analysis because tree-based models make fewer assumptions about the relationship between input and output and are immune to the issue of multi-collinearity, which is especially important when a large number of predictor variables are involved. The XGBoost model was trained for 10,000 runs with a max depth of 10 and a learning rate of 0.02.</p></sec></sec><sec id="sec019"><title>Acknowledgments</title><p>We thank Leila Wehbe, Brian MacWhinney, Erik Thiessen, and Graham Neubig for their helpful comments.</p></sec><sec id="sec020" sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="pone.0321973.s001" position="float" content-type="local-data"><label>S1 Appendix</label><caption><title>Replication on VRD dataset.</title><p>(PDF)</p></caption><media xlink:href="pone.0321973.s001.pdf"/></supplementary-material><supplementary-material id="pone.0321973.s002" position="float" content-type="local-data"><label>S1 Fig</label><caption><title>Distribution of variability and discriminability of nouns and verbs from the Visual Relationship Detection (VRD) dataset.</title><p>(PDF)</p></caption><media xlink:href="pone.0321973.s002.tif"/></supplementary-material><supplementary-material id="pone.0321973.s003" position="float" content-type="local-data"><label>S2 Fig</label><caption><title>Relative alignment strength as a function of the number of visual/linguistic exemplars per category on the Visual Genome (VG), Visual Relationship Detection (VRD), and Moments in Time (MiT) datasets.</title><p>(PDF)</p></caption><media xlink:href="pone.0321973.s003.tif"/></supplementary-material><supplementary-material id="pone.0321973.s004" position="float" content-type="local-data"><label>S3 Fig</label><caption><title>Relative alignment strength as a function of both the number of linguistic exemplars and the number of visual exemplars on the Visual Relationship Detection (VRD) dataset.</title><p>(PDF)</p></caption><media xlink:href="pone.0321973.s004.tif"/></supplementary-material><supplementary-material id="pone.0321973.s005" position="float" content-type="local-data"><label>S2 Appendix</label><caption><title>Words in the Visual Genome dataset.</title><p>(PDF)</p></caption><media xlink:href="pone.0321973.s005.pdf"/></supplementary-material><supplementary-material id="pone.0321973.s006" position="float" content-type="local-data"><label>S3 Appendix</label><caption><title>Words in the Visual Relationship Detection dataset.</title><p>(PDF)</p></caption><media xlink:href="pone.0321973.s006.pdf"/></supplementary-material><supplementary-material id="pone.0321973.s007" position="float" content-type="local-data"><label>S4 Appendix</label><caption><title>Words in the Moments in Time dataset.</title><p>(PDF)</p></caption><media xlink:href="pone.0321973.s007.pdf"/></supplementary-material></sec></body><back><ref-list><title>References</title><ref id="pone.0321973.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Bates</surname><given-names>E</given-names></name>, <name><surname>Marchman</surname><given-names>V</given-names></name>, <name><surname>Thal</surname><given-names>D</given-names></name>, <name><surname>Fenson</surname><given-names>L</given-names></name>, <name><surname>Dale</surname><given-names>P</given-names></name>, <name><surname>Reznick</surname><given-names>JS</given-names></name>, <etal>et al</etal>. <article-title>Developmental and stylistic variation in the composition of early vocabulary</article-title>. <source>J Child Lang</source>. <year>1994</year>;<volume>21</volume>(<issue>1</issue>):<fpage>85</fpage>&#x02013;<lpage>123</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1017/s0305000900008680</pub-id>
<pub-id pub-id-type="pmid">8006096</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Fenson</surname><given-names>L</given-names></name>, <name><surname>Dale</surname><given-names>PS</given-names></name>, <name><surname>Reznick</surname><given-names>JS</given-names></name>, <name><surname>Bates</surname><given-names>E</given-names></name>, <name><surname>Thal</surname><given-names>DJ</given-names></name>, <name><surname>Pethick</surname><given-names>SJ</given-names></name>, <etal>et al</etal>. <article-title>Variability in early communicative development</article-title>. <source>Monogr Soc Res Child Dev</source>. <year>1994</year>;<volume>59</volume>(<issue>5</issue>):i&#x02013;<lpage>185</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.2307/1166093</pub-id></mixed-citation></ref><ref id="pone.0321973.ref003"><label>3</label><mixed-citation publication-type="book"><name><surname>Frank</surname><given-names>MC</given-names></name>, <name><surname>Braginsky</surname><given-names>M</given-names></name>, <name><surname>Yurovsky</surname><given-names>D</given-names></name>, <name><surname>Marchman</surname><given-names>VA</given-names></name>. <source>Variability and consistency in early language learning: the Wordbank project</source>. <publisher-name>MIT Press</publisher-name>; <year>2021</year>.</mixed-citation></ref><ref id="pone.0321973.ref004"><label>4</label><mixed-citation publication-type="book"><name><surname>Peirce</surname><given-names>CS</given-names></name>. <source>Collected papers of Charles Sanders Peirce</source>. <publisher-name>Harvard University Press</publisher-name>; <year>1974</year>.</mixed-citation></ref><ref id="pone.0321973.ref005"><label>5</label><mixed-citation publication-type="book"><name><surname>Quine</surname><given-names>W</given-names></name>, <name><surname>Van</surname><given-names>O</given-names></name>. <source>Word and object: an inquiry into the linguistic mechanisms of objective reference</source>. <publisher-name>John Wiley</publisher-name>; <year>1960</year>.</mixed-citation></ref><ref id="pone.0321973.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Landau</surname><given-names>B</given-names></name>, <name><surname>Smith</surname><given-names>LB</given-names></name>, <name><surname>Jones</surname><given-names>SS</given-names></name>. <article-title>The importance of shape in early lexical learning</article-title>. <source>Cogn Dev</source>. <year>1988</year>;<volume>3</volume>(<issue>3</issue>):<fpage>299</fpage>&#x02013;<lpage>321</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0885-2014(88)90014-7</pub-id></mixed-citation></ref><ref id="pone.0321973.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Markman</surname><given-names>EM</given-names></name>, <name><surname>Wachtel</surname><given-names>GF</given-names></name>. <article-title>Children&#x02019;s use of mutual exclusivity to constrain the meanings of words</article-title>. <source>Cogn Psychol</source>. <year>1988</year>;<volume>20</volume>(<issue>2</issue>):<fpage>121</fpage>&#x02013;<lpage>57</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0010-0285(88)90017-5</pub-id>
<pub-id pub-id-type="pmid">3365937</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Waxman</surname><given-names>SR</given-names></name>, <name><surname>Markow</surname><given-names>DB</given-names></name>. <article-title>Words as invitations to form categories: evidence from 12-to 13-month-old infants</article-title>. <source>Cognitive Psychol</source>. <year>1995</year>;<volume>29</volume>(<issue>3</issue>):<fpage>257</fpage>&#x02013;<lpage>302</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref009"><label>9</label><mixed-citation publication-type="book"><name><surname>Gentner</surname><given-names>D</given-names></name>. <source>Why nouns are learned before verbs: linguistic relativity versus natural partitioning</source>. <publisher-name>Center for the Study of Reading Technical Report No 257</publisher-name>; <year>1982</year>.</mixed-citation></ref><ref id="pone.0321973.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Golinkoff</surname><given-names>RM</given-names></name>, <name><surname>Hirsh-Pasek</surname><given-names>K</given-names></name>. <article-title>How toddlers begin to learn verbs</article-title>. <source>Trends Cogn Sci</source>. <year>2008</year>;<volume>12</volume>(<issue>10</issue>):<fpage>397</fpage>&#x02013;<lpage>403</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.tics.2008.07.003</pub-id>
<pub-id pub-id-type="pmid">18760656</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Gentner</surname><given-names>D</given-names></name>. <article-title>Some interesting differences between verbs and nouns</article-title>. <source>Cogn Brain Theory</source>. <year>1981</year>;<volume>4</volume>(<issue>2</issue>):<fpage>161</fpage>&#x02013;<lpage>78</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Gentner</surname><given-names>D</given-names></name>, <name><surname>Boroditsky</surname><given-names>L</given-names></name>, <name><surname>Bowerman</surname><given-names>M</given-names></name>, <name><surname>Levinson</surname><given-names>S</given-names></name>. <article-title>Individuation, relativity, and early word</article-title>. <source>Lang Cult Cogn</source>. <year>2001</year>;<volume>3</volume>:<fpage>215</fpage>&#x02013;<lpage>56</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Rosch</surname><given-names>E</given-names></name>, <name><surname>Mervis</surname><given-names>CB</given-names></name>, <name><surname>Gray</surname><given-names>WD</given-names></name>, <name><surname>Johnson</surname><given-names>DM</given-names></name>, <name><surname>Boyes-Braem</surname><given-names>P</given-names></name>. <article-title>Basic objects in natural categories</article-title>. <source>Cogn Psychol</source>. <year>1976</year>;<volume>8</volume>:<fpage>382</fpage>&#x02013;<lpage>439</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref014"><label>14</label><mixed-citation publication-type="book"><name><surname>Firth</surname><given-names>JR</given-names></name>. <source>A synopsis of linguistic theory, 1930&#x02013;1955</source>. <publisher-name>Stud Linguist Anal</publisher-name>; <year>1957</year>.</mixed-citation></ref><ref id="pone.0321973.ref015"><label>15</label><mixed-citation publication-type="book"><name><surname>Landau</surname><given-names>B</given-names></name>, <name><surname>Gleitman</surname><given-names>LR</given-names></name>, <name><surname>Landau</surname><given-names>B</given-names></name>. <source>Language and experience: evidence from the blind child, vol. 8</source>. <publisher-name>Harvard University Press</publisher-name>; <year>2009</year>.</mixed-citation></ref><ref id="pone.0321973.ref016"><label>16</label><mixed-citation publication-type="other">Goldberg AE, Casenhiser DM, Sethuraman N. <article-title>Learning argument structure generalizations</article-title>. <source>Cogn Linguist</source>. <year>2004</year>.</mixed-citation></ref><ref id="pone.0321973.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Gleitman</surname><given-names>L</given-names></name>. <article-title>The structural sources of verb meanings</article-title>. <source>Lang Acquisit</source>. <year>1990</year>;<volume>1</volume>(<issue>1</issue>):<fpage>3</fpage>&#x02013;<lpage>55</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1207/s15327817la0101_2</pub-id></mixed-citation></ref><ref id="pone.0321973.ref018"><label>18</label><mixed-citation publication-type="book"><name><surname>Talmy</surname><given-names>L</given-names></name>. <article-title>Semantics and syntax of motion.</article-title> In: <source>Syntax and semantics volume 4</source>. <publisher-name>Brill</publisher-name>; <year>1975</year>. p. <fpage>181</fpage>&#x02013;<lpage>238</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Snedeker</surname><given-names>J</given-names></name>, <name><surname>Geren</surname><given-names>J</given-names></name>, <name><surname>Shafto</surname><given-names>CL</given-names></name>. <article-title>Starting over: international adoption as a natural experiment in language development</article-title>. <source>Psychol Sci</source>. <year>2007</year>;<volume>18</volume>(<issue>1</issue>):<fpage>79</fpage>&#x02013;<lpage>87</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/j.1467-9280.2007.01852.x</pub-id>
<pub-id pub-id-type="pmid">17362382</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Goldin-Meadow</surname><given-names>S</given-names></name>, <name><surname>Feldman</surname><given-names>H</given-names></name>. <article-title>The development of language-like communication without a language model</article-title>. <source>Science</source>. <year>1977</year>;<volume>197</volume>(<issue>4301</issue>):<fpage>401</fpage>&#x02013;<lpage>3</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.877567</pub-id>
<pub-id pub-id-type="pmid">877567</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Gillette</surname><given-names>J</given-names></name>, <name><surname>Gleitman</surname><given-names>H</given-names></name>, <name><surname>Gleitman</surname><given-names>L</given-names></name>, <name><surname>Lederer</surname><given-names>A</given-names></name>. <article-title>Human simulations of vocabulary learning</article-title>. <source>Cognition</source>. <year>1999</year>;<volume>73</volume>(<issue>2</issue>):<fpage>135</fpage>&#x02013;<lpage>76</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/s0010-0277(99)00036-0</pub-id>
<pub-id pub-id-type="pmid">10580161</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Shepard</surname><given-names>RN</given-names></name>, <name><surname>Hovland</surname><given-names>CI</given-names></name>, <name><surname>Jenkins</surname><given-names>HM</given-names></name>. <article-title>Learning and memorization of classifications.</article-title>
<source>Psychol Monogr: Gen Appl</source>. <year>1961</year>;<volume>75</volume>(<issue>13</issue>):<fpage>1</fpage>&#x02013;<lpage>42</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/h0093825</pub-id></mixed-citation></ref><ref id="pone.0321973.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Ashby</surname><given-names>FG</given-names></name>, <name><surname>Maddox</surname><given-names>WT</given-names></name>. <article-title>Human category learning</article-title>. <source>Annu Rev Psychol</source>. <year>2005</year>;<volume>56</volume>(<issue>1</issue>):<fpage>149</fpage>&#x02013;<lpage>78</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1146/annurev.psych.56.091103.070217</pub-id>
<pub-id pub-id-type="pmid">15709932</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref024"><label>24</label><mixed-citation publication-type="book"><name><surname>De Saussure</surname><given-names>F</given-names></name>. <source>Cours de linguistique g&#x000b4;en&#x000b4;erale, vol. 1</source>. <publisher-name>Otto Harrassowitz Verlag</publisher-name>; <year>1989</year>.</mixed-citation></ref><ref id="pone.0321973.ref025"><label>25</label><mixed-citation publication-type="other">Mikolov T, Chen K, Corrado G, Dean J. <article-title>Efficient estimation of word representations in vector space</article-title>. <source>arXiv</source>. <year>2013</year>.</mixed-citation></ref><ref id="pone.0321973.ref026"><label>26</label><mixed-citation publication-type="other">Pennington J, Socher R, Manning CD. <article-title>Glove: global vectors for word representation.</article-title> In: <source>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</source>. <year>2014</year>. p. <fpage>1532</fpage>&#x02013;<lpage>43</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref027"><label>27</label><mixed-citation publication-type="other">Devlin J, Chang MW, Lee K, Toutanova K. <article-title>Bert: pre-training of deep bidirectional transformers for language understanding</article-title>. <source>arXiv</source>. <year>2018</year>.</mixed-citation></ref><ref id="pone.0321973.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Brown</surname><given-names>T</given-names></name>, <name><surname>Mann</surname><given-names>B</given-names></name>, <name><surname>Ryder</surname><given-names>N</given-names></name>, <name><surname>Subbiah</surname><given-names>M</given-names></name>, <name><surname>Kaplan</surname><given-names>JD</given-names></name>, <name><surname>Dhariwal</surname><given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Language models are few-shot learners</article-title>. <source>Adv Neural Inform Process Syst.</source>
<year>2020</year>;<volume>33</volume>:<fpage>1877</fpage>&#x02013;<lpage>901</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref029"><label>29</label><mixed-citation publication-type="other">Ouyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P, <etal>et al</etal>. <article-title>Training language models to follow instructions with human feedback</article-title>. <source>arXiv</source>. <year>2022</year>.</mixed-citation></ref><ref id="pone.0321973.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Hill</surname><given-names>F</given-names></name>, <name><surname>Reichart</surname><given-names>R</given-names></name>, <name><surname>Korhonen</surname><given-names>A</given-names></name>. <article-title>Simlex-999: evaluating semantic models with (genuine) similarity estimation</article-title>. <source>Comput Linguist</source>. <year>2015</year>;<volume>41</volume>(<issue>4</issue>):<fpage>665</fpage>&#x02013;<lpage>695</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/coli_a_00237</pub-id></mixed-citation></ref><ref id="pone.0321973.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Hills</surname><given-names>TT</given-names></name>, <name><surname>Jones</surname><given-names>MN</given-names></name>, <name><surname>Todd</surname><given-names>PM</given-names></name>. <article-title>Optimal foraging in semantic memory</article-title>. <source>Psychol Rev</source>. <year>2012</year>;<volume>119</volume>(<issue>2</issue>):<fpage>431</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/a0027373</pub-id>
<pub-id pub-id-type="pmid">22329683</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Bhatia</surname><given-names>S</given-names></name>. <article-title>Associative judgment and vector space semantics</article-title>. <source>Psychol Rev</source>. <year>2017</year>;<volume>124</volume>(<issue>1</issue>):<fpage>1</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/rev0000047</pub-id>
<pub-id pub-id-type="pmid">28004958</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref033"><label>33</label><mixed-citation publication-type="other">Hu J, Gauthier J, Qian P, Wilcox E, Levy RP. <article-title>A systematic assessment of syntactic generalization in neural language models</article-title>. <source>arXiv preprint arxiv</source>. <year>2020</year>.</mixed-citation></ref><ref id="pone.0321973.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Caron</surname><given-names>M</given-names></name>, <name><surname>Misra</surname><given-names>I</given-names></name>, <name><surname>Mairal</surname><given-names>J</given-names></name>, <name><surname>Goyal</surname><given-names>P</given-names></name>, <name><surname>Bojanowski</surname><given-names>P</given-names></name>, <name><surname>Joulin</surname><given-names>A</given-names></name>. <article-title>Unsupervised learning of visual features by contrasting cluster assignments</article-title>. <source>Adv Neural Inform Process Syst</source>. <year>2020</year>;<volume>33</volume>:<fpage>9912</fpage>&#x02013;<lpage>24</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Roads</surname><given-names>BD</given-names></name>, <name><surname>Love</surname><given-names>BC</given-names></name>. <article-title>Learning as the unsupervised alignment of conceptual systems</article-title>. <source>Nat Mach Intell</source>. <year>2020</year>;<volume>2</volume>(<issue>1</issue>):<fpage>76</fpage>&#x02013;<lpage>82</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s42256-019-0132-2</pub-id></mixed-citation></ref><ref id="pone.0321973.ref036"><label>36</label><mixed-citation publication-type="preprint"><name><surname>Krishna</surname><given-names>R</given-names></name>, <name><surname>Zhu</surname><given-names>Y</given-names></name>, <name><surname>Groth</surname><given-names>O</given-names></name>, <name><surname>Johnson</surname><given-names>J</given-names></name>, <name><surname>Hata</surname><given-names>K</given-names></name>, <name><surname>Kravitz</surname><given-names>J</given-names></name>, <etal>et al</etal>. Visual genome: connecting language and vision using crowdsourced dense image annotations. <year>2016</year>. Available from: <ext-link xlink:href="https://arxiv.org/abs/1602.07332" ext-link-type="uri">https://arxiv.org/abs/1602.07332</ext-link></mixed-citation></ref><ref id="pone.0321973.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Eimas</surname><given-names>PD</given-names></name>, <name><surname>Quinn</surname><given-names>PC</given-names></name>. <article-title>Studies on the formation of perceptually based basic-level categories in young infants</article-title>. <source>Child Dev</source>. <year>1994</year>;<volume>65</volume>(<issue>3</issue>):<fpage>903</fpage>&#x02013;<lpage>17</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/j.1467-8624.1994.tb00792.x</pub-id>
<pub-id pub-id-type="pmid">8045176</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Horst</surname><given-names>JS</given-names></name>, <name><surname>Parsons</surname><given-names>KL</given-names></name>, <name><surname>Bryan</surname><given-names>NM</given-names></name>. <article-title>Get the story straight: contextual repetition promotes word learning from storybooks</article-title>. <source>Front Psychol</source>. <year>2011</year>;<volume>2</volume>:<fpage>17</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00017</pub-id>
<pub-id pub-id-type="pmid">21713179</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>McDonough</surname><given-names>C</given-names></name>, <name><surname>Song</surname><given-names>L</given-names></name>, <name><surname>Hirsh-Pasek</surname><given-names>K</given-names></name>, <name><surname>Golinkoff</surname><given-names>RM</given-names></name>, <name><surname>Lannon</surname><given-names>R</given-names></name>. <article-title>An image is worth a thousand words: why nouns tend to dominate verbs in early word learning</article-title>. <source>Dev Sci</source>. <year>2011</year>;<volume>14</volume>(<issue>2</issue>):<fpage>181</fpage>&#x02013;<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/j.1467-7687.2010.00968.x</pub-id>
<pub-id pub-id-type="pmid">21359165</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref040"><label>40</label><mixed-citation publication-type="journal"><name><surname>Monfort</surname><given-names>M</given-names></name>, <name><surname>Andonian</surname><given-names>A</given-names></name>, <name><surname>Zhou</surname><given-names>B</given-names></name>, <name><surname>Ramakrishnan</surname><given-names>K</given-names></name>, <name><surname>Bargal</surname><given-names>SA</given-names></name>, <name><surname>Yan</surname><given-names>T</given-names></name>, <etal>et al</etal>. <article-title>Moments in time dataset: one million videos for event understanding</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2019</year>;<volume>42</volume>(<issue>2</issue>):<fpage>502</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TPAMI.2019.2901464</pub-id>
<pub-id pub-id-type="pmid">30802849</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Consortium</surname><given-names>M</given-names></name>. <article-title>Quantifying sources of variability in infancy research using the infant-directed-speech preference</article-title>. <source>Adv Methods Pract Psychol Sci</source>. <year>2020</year>;<volume>3</volume>(<issue>1</issue>):<fpage>24</fpage>&#x02013;<lpage>52</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Goodman</surname><given-names>JC</given-names></name>, <name><surname>Dale</surname><given-names>PS</given-names></name>, <name><surname>Li</surname><given-names>P</given-names></name>. <article-title>Does frequency count? Parental input and the acquisition of vocabulary</article-title>. <source>J Child Lang</source>. <year>2008</year>;<volume>35</volume>(<issue>3</issue>):<fpage>515</fpage>&#x02013;<lpage>31</lpage>.<pub-id pub-id-type="pmid">18588713</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref043"><label>43</label><mixed-citation publication-type="journal"><name><surname>Ebbinghaus</surname><given-names>H</given-names></name>. <article-title>Memory: A contribution to experimental psychology</article-title>. <source>Ann Neurosci</source>. <year>2013</year>;<volume>20</volume>(<issue>4</issue>):<fpage>155</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.5214/ans.0972.7531.200408</pub-id>
<pub-id pub-id-type="pmid">25206041</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref044"><label>44</label><mixed-citation publication-type="journal"><name><surname>Brown</surname><given-names>GD</given-names></name>, <name><surname>Watson</surname><given-names>FL</given-names></name>. <article-title>First in, first out: word learning age and spoken word frequency as predictors of word familiarity and word naming latency</article-title>. <source>Mem Cognit</source>. <year>1987</year>;<volume>15</volume>(<issue>3</issue>):<fpage>208</fpage>&#x02013;<lpage>216</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/bf03197718</pub-id>
<pub-id pub-id-type="pmid">3600260</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref045"><label>45</label><mixed-citation publication-type="journal"><name><surname>Ambridge</surname><given-names>B</given-names></name>, <name><surname>Kidd</surname><given-names>E</given-names></name>, <name><surname>Rowland</surname><given-names>CF</given-names></name>, <name><surname>Theakston</surname><given-names>AL</given-names></name>. <article-title>The ubiquity of frequency effects in first language acquisition</article-title>. <source>J Child Lang</source>. <year>2015</year>;<volume>42</volume>(<issue>2</issue>):<fpage>239</fpage>&#x02013;<lpage>73</lpage>.<pub-id pub-id-type="pmid">25644408</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref046"><label>46</label><mixed-citation publication-type="other">Chen T, Guestrin C. <article-title>Xgboost: A scalable tree boosting system.</article-title> In: <source>Proceedings of the 22nd ACM SIGKDD International conference on knowledge discovery and data mining</source>. <year>2016</year>. p. <fpage>785</fpage>&#x02013;<lpage>94</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref047"><label>47</label><mixed-citation publication-type="book"><name><surname>MacWhinney</surname><given-names>B</given-names></name>. <source>The CHILDES project: tools for analyzing talk, volume II: the database</source>. <publisher-name>Psychology Press</publisher-name>; <year>2014</year>.</mixed-citation></ref><ref id="pone.0321973.ref048"><label>48</label><mixed-citation publication-type="journal"><name><surname>Lundberg</surname><given-names>SM</given-names></name>, <name><surname>Lee</surname><given-names>SI</given-names></name>. <article-title>A unified approach to interpreting model predictions</article-title>. <source>Adv Neural Inform Process Syst</source>. <year>2017</year>;<volume>30</volume>.</mixed-citation></ref><ref id="pone.0321973.ref049"><label>49</label><mixed-citation publication-type="journal"><name><surname>Schrimpf</surname><given-names>M</given-names></name>, <name><surname>Blank</surname><given-names>IA</given-names></name>, <name><surname>Tuckute</surname><given-names>G</given-names></name>, <name><surname>Kauf</surname><given-names>C</given-names></name>, <name><surname>Hosseini</surname><given-names>EA</given-names></name>, <name><surname>Kanwisher</surname><given-names>N</given-names></name>, <etal>et al</etal>. <article-title>The neural architecture of language: integrative modeling converges on predictive processing</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2021</year>;<volume>118</volume>(<issue>45</issue>):e2105646118. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.2105646118</pub-id>
<pub-id pub-id-type="pmid">34737231</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref050"><label>50</label><mixed-citation publication-type="journal"><name><surname>Conwell</surname><given-names>C</given-names></name>, <name><surname>Prince</surname><given-names>JS</given-names></name>, <name><surname>Kay</surname><given-names>KN</given-names></name>, <name><surname>Alvarez</surname><given-names>GA</given-names></name>, <name><surname>Konkle</surname><given-names>T</given-names></name>. <article-title>A large-scale examination of inductive biases shaping high-level visual representation in brains and machines</article-title>. <source>Nat Commun</source>. <year>2024</year>;<volume>15</volume>(<issue>1</issue>):<fpage>9383</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41467-024-53147-y</pub-id>
<pub-id pub-id-type="pmid">39477923</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref051"><label>51</label><mixed-citation publication-type="book"><name><surname>Brown</surname><given-names>R</given-names></name>. <source>A first language</source>. <publisher-name>Harvard University Press</publisher-name>; <year>2013</year>.</mixed-citation></ref><ref id="pone.0321973.ref052"><label>52</label><mixed-citation publication-type="journal"><name><surname>Ninio</surname><given-names>A</given-names></name>. <article-title>Pathbreaking verbs in syntactic development and the question of prototypical transitivity</article-title>. <source>J Child Lang</source>. <year>1999</year>;<volume>26</volume>(<issue>3</issue>):<fpage>619</fpage>&#x02013;<lpage>53</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1017/s0305000999003931</pub-id>
<pub-id pub-id-type="pmid">10603698</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref053"><label>53</label><mixed-citation publication-type="journal"><name><surname>Saffran</surname><given-names>JR</given-names></name>, <name><surname>Aslin</surname><given-names>RN</given-names></name>, <name><surname>Newport</surname><given-names>EL</given-names></name>. <article-title>Statistical learning by 8-month-old infants</article-title>. <source>Science</source>. <year>1996</year>;<volume>274</volume>(<issue>5294</issue>):<fpage>1926</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.274.5294.1926</pub-id>
<pub-id pub-id-type="pmid">8943209</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref054"><label>54</label><mixed-citation publication-type="book"><name><surname>Nelson</surname><given-names>K</given-names></name>. <source>Young minds in social worlds: experience, meaning, and memory</source>. <publisher-name>Harvard University Press</publisher-name>; <year>2007</year>.</mixed-citation></ref><ref id="pone.0321973.ref055"><label>55</label><mixed-citation publication-type="book"><name><surname>Adamson</surname><given-names>LB</given-names></name>. <source>Communication development during infancy</source>. <publisher-name>Routledge</publisher-name>; <year>2018</year>.</mixed-citation></ref><ref id="pone.0321973.ref056"><label>56</label><mixed-citation publication-type="journal"><name><surname>Meltzoff</surname><given-names>AN</given-names></name>. <article-title>Understanding the intentions of others: re-enactment of intended acts by 18-month-old children</article-title>. <source>Dev Psychol</source>. <year>1995</year>;<volume>31</volume>(<issue>5</issue>):<fpage>838</fpage>. <pub-id pub-id-type="pmid">25147406</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref057"><label>57</label><mixed-citation publication-type="journal"><name><surname>Gergely</surname><given-names>G</given-names></name>, <name><surname>N&#x000e1;dasdy</surname><given-names>Z</given-names></name>, <name><surname>Csibra</surname><given-names>G</given-names></name>, <name><surname>B&#x000ed;r&#x000f3;</surname><given-names>S</given-names></name>. <article-title>Taking the intentional stance at 12 months of age</article-title>. <source>Cognition</source>. <year>1995</year>;<volume>56</volume>(<issue>2</issue>):<fpage>165</fpage>&#x02013;<lpage>93</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0010-0277(95)00661-h</pub-id>
<pub-id pub-id-type="pmid">7554793</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref058"><label>58</label><mixed-citation publication-type="journal"><name><surname>Longobardi</surname><given-names>E</given-names></name>, <name><surname>Spataro</surname><given-names>P</given-names></name>, <name><surname>Putnick</surname><given-names>DL</given-names></name>, <name><surname>Bornstein</surname><given-names>MH</given-names></name>. <article-title>Do early noun and verb production predict later verb and noun production? Theoretical implications</article-title>. <source>J Child Lang</source>. <year>2017</year>;<volume>44</volume>(<issue>2</issue>):<fpage>480</fpage>&#x02013;<lpage>95</lpage>.<pub-id pub-id-type="pmid">26880050</pub-id>
</mixed-citation></ref><ref id="pone.0321973.ref059"><label>59</label><mixed-citation publication-type="journal"><name><surname>Miller</surname><given-names>GA</given-names></name>. <article-title>WordNet: a lexical database for English</article-title>. <source>Communi ACM</source>. <year>1995</year>;<volume>38</volume>(<issue>11</issue>):<fpage>39</fpage>&#x02013;<lpage>41</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/219717.219748</pub-id></mixed-citation></ref><ref id="pone.0321973.ref060"><label>60</label><mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. <article-title>Deep residual learning for image recognition.</article-title> In: <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>. <year>2016</year>. p. <fpage>770</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0321973.ref061"><label>61</label><mixed-citation publication-type="other">Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, <etal>et al</etal>. <article-title>Huggingface&#x02019;s transformers: State-of-the-art natural language processing</article-title>. <source>arXiv</source>. <year>2019</year>:<fpage>191003771</fpage>.</mixed-citation></ref><ref id="pone.0321973.ref062"><label>62</label><mixed-citation publication-type="journal"><name><surname>Frank</surname><given-names>MC</given-names></name>, <name><surname>Braginsky</surname><given-names>M</given-names></name>, <name><surname>Yurovsky</surname><given-names>D</given-names></name>, <name><surname>Marchman</surname><given-names>VA</given-names></name>. <article-title>Wordbank: An open repository for developmental vocabulary data</article-title>. <source>J Child Lang</source>. <year>2017</year>;<volume>44</volume>(<issue>3</issue>):<fpage>677</fpage>&#x02013;<lpage>94</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1017/S0305000916000209</pub-id>
<pub-id pub-id-type="pmid">27189114</pub-id>
</mixed-citation></ref></ref-list></back></article>