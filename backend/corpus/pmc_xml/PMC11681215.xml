<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39730415</article-id><article-id pub-id-type="pmc">PMC11681215</article-id><article-id pub-id-type="publisher-id">75599</article-id><article-id pub-id-type="doi">10.1038/s41598-024-75599-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Parameter-efficient fine-tuning of large language models using semantic knowledge tuning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Prottasha</surname><given-names>Nusrat Jahan</given-names></name><address><email>jahannusratprotta@gmail.com</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Mahmud</surname><given-names>Asif</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Sobuj</surname><given-names>Md. Shohanur Islam</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Bhat</surname><given-names>Prakash</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Kowsher</surname><given-names>Md</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Yousefi</surname><given-names>Niloofar</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Garibay</surname><given-names>Ozlem Ozmen</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/036nfer12</institution-id><institution-id institution-id-type="GRID">grid.170430.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 2159 2859</institution-id><institution>University of Central Florida, </institution></institution-wrap>Orlando, FL 32816 USA </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05q9we431</institution-id><institution-id institution-id-type="GRID">grid.449503.f</institution-id><institution-id institution-id-type="ISNI">0000 0004 1798 7083</institution-id><institution>Noakhali Science &#x00026; Technology University, </institution></institution-wrap>Noakhali, 3814 Bangladesh </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00kvxt616</institution-id><institution-id institution-id-type="GRID">grid.443067.2</institution-id><institution>Hajee Mohammad Danesh Science &#x00026; Technology University, </institution></institution-wrap>Dinajpur, 5200 Bangladesh </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04mv4n011</institution-id><institution-id institution-id-type="GRID">grid.467171.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 0316 7795</institution-id><institution>Amazon, </institution></institution-wrap>New Jersey, USA </aff></contrib-group><pub-date pub-type="epub"><day>28</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>28</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><volume>14</volume><elocation-id>30667</elocation-id><history><date date-type="received"><day>18</day><month>2</month><year>2024</year></date><date date-type="accepted"><day>7</day><month>10</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply 2024</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Large Language Models (LLMs) are gaining significant popularity in recent years for specialized tasks using prompts due to their low computational cost. Standard methods like prefix tuning utilize special, modifiable tokens that lack semantic meaning and require extensive training for best performance, often falling short. In this context, we propose a novel method called Semantic Knowledge Tuning (SK-Tuning) for prompt and prefix tuning that employs meaningful words instead of random tokens. This method involves using a fixed LLM to understand and process the semantic content of the prompt through zero-shot capabilities. Following this, it integrates the processed prompt with the input text to improve the model&#x02019;s performance on particular tasks. Our experimental results show that SK-Tuning exhibits faster training times, fewer parameters, and superior performance on tasks such as text classification and understanding compared to other tuning methods. This approach offers a promising method for optimizing the efficiency and effectiveness of LLMs in processing language tasks.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Computer science</kwd><kwd>Mathematics and computing</kwd><kwd>Information technology</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">The domain of NLP has seen a remarkable transformation in recent years, primarily driven by the introduction of LLMs<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Transformer-based Language Models (TLMs) initially led about a revolution by showing outstanding capabilities in capturing extensive dependencies<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. However, the challenges connected with adapting TLMs to various tasks, combined with their resource-intensive training, resulted to the development of more powerful models, such as GPT-3<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. With billions of parameters, these LLMs have not only boosted performance benchmarks across various tasks but have also extended their applications into novel domains, including creative writing and multimodal learning<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>.</p><p id="Par3">Despite their notable achievements, a in-depth analysis of LLMs reveals several major limitations. The extensive computational resources required for their training raise questions about environmental sustainability and restrict accessibility to research facilities with sufficient equipment and resources<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>.</p><p id="Par4">To address this issue, there has been an increasing focus on the latest innovations in parameter-efficient fine-tuning methods<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>.As compared to retraining the entire model from scratch, fine-tuning LLMs has proven to be a more rapid and efficient approach. Nevertheless, the fine-tuning of all parameters of LLMs remains a challenge due to their vast size, which typically consists of billions of parameters. Despite this, the fine-tuning process still requires extensive computational resources, much like the pretraining technique.</p><p id="Par5">Adapting to this challenge, adapter training has gained importance as a more efficient approach<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. This approach involves introducing domain-specific parameters, referred to as adapters, into pretrained models. These adapters, which are made of small neural networks, are strategically inserted within or between the layers of the pretrained model. During the training process, only the parameters of these added adapters are updated, while the parameters of the pretrained model remain unchanged<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>.</p><p id="Par6">While adapters provide a more simple approach, they may not fully capture complex data patterns as effectively as fine-tuning the entire model. In addition, determining the optimal locations to insert adapters within the LLM can be challenging and may require experimentation. Nonetheless, prompt tuning complements adapter training by offering additional contextual information to guide the model&#x02019;s understanding of the task at hand.</p><p id="Par7">Prompt tuning<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> does not modify the underlying structure of the model, potentially resulting in quicker inference times and decreased resource consumption in contrast to utilizing adapters. Furthermore, prefix tuning<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> has been proposed as a method to improve performance. Unlike prompt tuning, which updates only a portion of a single layer, prefix tuning updates a section of every layer consistently, and it has shown improved performance in succeeding tasks.</p><p id="Par8">The use of prompts and prefix tuning techniques<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup> can pose challenges in terms of the effectiveness and interpretability of the employed prompts or prefixes. These methods generally utilize trainable virtual tokens within an adapter, which may not have essential semantic significance and require extensive training to acquire domain-specific knowledge efficiently. Consequently, the performance of these techniques may not be optimal, particularly when dealing with complex tasks, and extensive training is necessary to achieve optimal performance.</p><p id="Par9">To overcome these challenges, we propose SK-Tuning, a novel approach that focuses on improving the performance of fine-tuning LLMs for prompt and prefix tuning. Unlike standard techniques that depend on random virtual tokens, SK-Tuning utilizes genuine, semantically rich prompts or prefixes for adapter training. By employing the LLM&#x02019;s innate capacity to understand linguistic semantics, SK-Tuning strives to improve performance by integrating semantic knowledge directly from prompts or prefixes.</p><p id="Par10">LLMs display remarkable zero-shot capabilities, allowing them to perform tasks without explicit training, as shown in recent studies<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. To maximize the potential of these capabilities, SK-Tuning utilizes LLM&#x02019;s ability to understand prompts or instructions in a zero-shot manner. This approach speeds up the convergence process during fine-tuning because we concentrate only on refining the semantic representation of the prompt or prefix.</p><p id="Par11">The SK-Tuning method is presented in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, which displays the stages for prompt and prefix tuning. At first, the entire LLM is frozen to maintain its pretrained knowledge. Next, the frozen LLM is utilized to extract the semantic representation from the prompt or prefix text. This representation is then educated with a small adapter to improve its task-specific intelligence. Lastly, the revised representation is combined with the embedding of the input text, guaranteeing that the model effectively integrates both the semantic context provided by the prompt or prefix and the textual data of the input.</p><p id="Par12">We perform wide-ranging experimental evaluations across a variety of downstream tasks, including sequence classification, token classification, and NLI, to practically show the efficiency and excellence of SK-Tuning compared to traditional fine-tuning methods. Furthermore, we compare SK-Tuning with other parameter-efficient approaches, such as prompt tuning, prefix tuning, p-tuning, and LoRA, highlighting its unique advantages and contributions to the field of NLP.</p><p id="Par13">The major contributions of this paper are summarized as follows:<list list-type="bullet"><list-item><p id="Par14">This paper introduces SK-Tuning, a novel approach for fine-tuning LLMs using real, semantically meaningful prompt or prefix text.</p></list-item><list-item><p id="Par15">SK-Tuning improves training efficiency and convergence speed by utilizing the inherent semantic understanding of prompt or prefix text using LLM&#x02019;s zero-shot capabilities, as a result allowing rapid adaptation to new tasks.</p></list-item><list-item><p id="Par16">In numerous experiments covering a variety of tasks, including sequence classification, token classification, and NLI, SK-Tuning has continually exhibited significant improvements in performance metrics..</p></list-item><list-item><p id="Par17">The study includes a comprehensive evaluation against other parameter-efficient methods like prompt tuning, prefix tuning, p-tuning, and LoRA, highlighting SK-Tuning&#x02019;s superior effectiveness in terms of performance outcomes and computational efficiency.</p></list-item><list-item><p id="Par18">SK-Tuning reduces computational requirements and the number of trainable parameters compared to traditional fine-tuning approaches, making it a more resource-efficient solution for adapting LLMs.</p></list-item></list>The structure of this paper is as follows: section&#x000a0;&#x0201c;<xref rid="Sec2" ref-type="sec">Related work</xref>&#x0201d; reviews related work, situating our approach within the broader domain of parameter-efficient fine-tuning methods. Section&#x000a0;&#x0201c;<xref rid="Sec3" ref-type="sec">Background study</xref>&#x0201d; provides a background study on existing tuning techniques, setting the stage for our proposed method. In section&#x000a0;&#x0201c;<xref rid="Sec6" ref-type="sec">SK-tuning procedure</xref>&#x0201d;, we detail the SK-Tuning procedure, explaining its methodology and implementation. Section&#x000a0;&#x0201c;<xref rid="Sec13" ref-type="sec">Experiments</xref>&#x0201d; presents our experiments, showcasing the performance improvements achieved through SK-Tuning across various tasks. Section&#x000a0;&#x0201c;<xref rid="Sec30" ref-type="sec">Ablation study</xref>&#x0201d; offers an ablation study to further analyze the contributions of each component within SK-Tuning, reinforcing the paper&#x02019;s key contributions to NLP. Section&#x000a0;&#x0201c;<xref rid="Sec38" ref-type="sec">Discussion</xref>&#x0201d; provides a discussion on the implications and potential applications of SK-Tuning in practical settings. Section&#x000a0;&#x0201c;<xref rid="Sec39" ref-type="sec">Limitations</xref>&#x0201d; discusses the limitations and challenges experienced during the development and application of SK-Tuning. Finally, section&#x000a0;&#x0201c;<xref rid="Sec44" ref-type="sec">Conclusion</xref>&#x0201d; concludes the paper by summarizing the findings and highlighting future research directions in fine-tuning methods for LLMs.</p></sec><sec id="Sec2"><title>Related work</title><p id="Par19">The importance of parameter-efficient fine-tuning (PEFT) methods in the field of NLP is immense, considering the growing complexity of LLMs. These methods not only improve model performance but also significantly reduce computational and memory requirements, as demonstrated by recent academic research<sup><xref ref-type="bibr" rid="CR11">11</xref>&#x02013;<xref ref-type="bibr" rid="CR15">15</xref></sup>. The effectiveness of PEFT techniques is being thoroughly evaluated on a range of NLP tasks, as shown in<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. Moreover, an extensive body of research<sup><xref ref-type="bibr" rid="CR17">17</xref>&#x02013;<xref ref-type="bibr" rid="CR23">23</xref></sup> consistently indicates that PEFT strategies considerably enhance the performance of LLMs, even under limited-resource circumstances.</p><p id="Par20"><bold>Prompt tuning</bold> is a novel approach that improves NLP and generation tasks by fine-tuning learnable parameters within the model<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. This technique enhances the model&#x02019;s performance on specific roles by fine-tuning prompts, thereby optimizing its output. Improvements in prompt tuning have been achieved through the implementation of the residual connections to strengthen performance and stability<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. This technique has also been broadened to support continual learning environments, as illustrated in recent research<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>. Current research focuses on dynamic prompt tuning, which adapts prompts in real time based on evolving contexts, as well as hierarchical prompt tuning, which provides multilevel control over the model&#x02019;s responses<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>.</p><p id="Par21"><bold>Prefix tuning</bold> is another powerful technique that adds learnable parameters as prefixes to the input of pre-trained models, enabling modification to different applications with minimal changes to the model itself<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. This method enables efficient domain-specific fine-tuning without requiring the retraining of the entire model, particularly in resource-limited settings. Recent innovations introduce hierarchical prefix tuning, which organizes prefixes in a hierarchical manner to provide more detailed control over the model&#x02019;s responses<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Additionally, dynamic prefix tuning allows for real-time adaptation based on the input context, thereby improving the flexibility and adaptability of the model<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Techniques such as MixPrompt<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> and E2VPT<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> have also been introduced to combine and optimize the usage of input and key-value prompts, advancing the application of prefix tuning in natural language processing applications.</p><p id="Par22"><bold>Low-rank adaptation (LoRA)</bold> first proposed by<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, is a fine-tuning technique designed to optimize memory usage and has received considerable attention in the research community since its inception. The latest developments have expanded the range of applications for LoRA, particularly in the area of multitask learning, as illustrated by research conducted by<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>, and<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Practical applications of LoRA were further explored by<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, while<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> focused on optimizing its memory efficiency. A notable innovation, ReLoRA, introduced by<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, incorporates a full-rank warm-up phase.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> proposed adaptive approaches that dynamically adjust the low-rank adaptation parameters. Additionally,<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> presented the Low-Rank Kronecker Product (LoKr), and<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> developed ResLoRA, which integrates residual pathways. Further contributions include the Low-Rank Hadamard Product (LoHa) by<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, and the introduction of Orthogonal Finetuning (OFT) and OFT with butterfly factorization (BOFT) by<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> and<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, which utilize orthogonal matrices to transform pre-trained weight matrices, resulting in significant improvements in both fine-tuning efficiency and performance.</p><p id="Par23"><bold>Subspace learning</bold> has become a crucial area of research, with a focus on optimizing model weights within a low-dimensional space, thereby providing computational efficiency and improved performance in various machine learning tasks<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup>. This approach has been extensively utilized in meta-learning and continual learning frameworks, as shown by several studies<sup><xref ref-type="bibr" rid="CR44">44</xref>&#x02013;<xref ref-type="bibr" rid="CR49">49</xref></sup>. Latest improvements in adaptive subspace learning methods have demonstrated significant improvements in generalization and robustness, especially in challenging environments<sup><xref ref-type="bibr" rid="CR50">50</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup>. Furthermore, incorporation of subspace learning into neural architecture search has proven invaluable in identifying efficient and innovative architectures, optimizing both performance and resource utilization<sup><xref ref-type="bibr" rid="CR51">51</xref>&#x02013;<xref ref-type="bibr" rid="CR53">53</xref></sup>. The efficacy of subspace learning is further highlighted in scenarios requiring rapid adaptation to new tasks with limited data, such as few-shot learning and online learning, where it allows robust model performance despite data limitations<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>.</p><p id="Par24"><bold>Projected gradient descent (PGD)</bold> has been significantly improved by the development of advanced methodologies such as GaLore<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>. Unlike traditional approaches, which treat the objective function as a black box, GaLore creates gradients within multilayer neural networks, providing a more extensive and effective optimization process<sup><xref ref-type="bibr" rid="CR56">56</xref>,<xref ref-type="bibr" rid="CR57">57</xref></sup>. This approach has displayed notable enhancements in the convergence rate of neural network training, particularly in high-dimensional datasets, while also contributing to advanced stability during the training process<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. Furthermore, GaLore addresses the challenges of gradient sparsity and redundancy, resulting in significant gains in training efficiency<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>. These innovations have not only strengthened the robustness of neural networks against adversarial attacks but also ensured more stable and reliable training dynamics, marking a noteworthy improvement in the field<sup><xref ref-type="bibr" rid="CR59">59</xref>&#x02013;<xref ref-type="bibr" rid="CR61">61</xref></sup>.</p><p id="Par25"><bold>Memory-efficient optimization</bold> is a pivotal area of research within the development of adaptive optimization algorithms, particularly in the context of large-scale models where memory bounds are a significant challenge. Foundational studies by<sup><xref ref-type="bibr" rid="CR62">62</xref></sup> have proven the efficiency of quantization techniques and combined gradient computation in considerably reducing memory usage during training<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. Building upon these contributions, the latest innovations have introduced hierarchical memory management systems that enable dynamic memory allocation and sparse gradient updates, thereby further optimizing memory utilization, as highlighted by<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. Moreover,<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> proposed a memory-efficient fine-tuning approach, employing block-wise optimizing strategies that dynamically adjust memory allocation, achieving superior performance across several benchmarks. In a similar vein,<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> explored the use of low-rank factorization techniques to compress model parameters effectively while preserving model accuracy. Collectively, these innovations contribute to the deployment of large-scale models on resource-limited devices, ensuring computational efficiency and maintaining optimal performance.</p><p id="Par26">In contrast to previous techniques, our proposed SK-Tuning method introduces a novel strategy that utilizes authentic, semantically meaningful prompts or prefix texts during adapter training. This method capitalizes on the zero-shot capabilities of large language models (LLMs) and their fundamental understanding of linguistic semantics. As a result, SK-Tuning is designed to achieve faster convergence and enhance task performance. Through extensive experimental evaluations and comprehensive comparative analysis, we establish the superiority of SK-Tuning over existing fine-tuning techniques. These findings highlight the significant potential of SK-Tuning to advance fine-tuning methodologies in the field of NLP.</p></sec><sec id="Sec3"><title>Background study</title><p id="Par27">Prefix and prompt tuning are methods of adapting large pretrained language models to specific tasks or datasets with minimal updates to the model parameters. These techniques have gained prominence due to their efficiency and effectiveness, particularly in scenarios where updating the entire model is computationally expensive or impractical.</p><sec id="Sec4"><title>Prefix tuning</title><p id="Par28">Prefix tuning involves appending a sequence of tunable vectors, known as the prefix, to the input of each layer of the transformer model. Let us denote the transformer model as a function <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F$$\end{document}</tex-math><mml:math id="M2"><mml:mi>F</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq1.gif"/></alternatives></inline-formula> that maps an input sequence <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x$$\end{document}</tex-math><mml:math id="M4"><mml:mi>x</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq2.gif"/></alternatives></inline-formula> to an output <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y$$\end{document}</tex-math><mml:math id="M6"><mml:mi>y</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq3.gif"/></alternatives></inline-formula>, i.e., <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y = F(x)$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq4.gif"/></alternatives></inline-formula>. In prefix tuning, this mapping is modified to <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y = F(p \oplus x)$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02295;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq5.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p$$\end{document}</tex-math><mml:math id="M12"><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq6.gif"/></alternatives></inline-formula> represents the prefix and <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\oplus$$\end{document}</tex-math><mml:math id="M14"><mml:mo>&#x02295;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq7.gif"/></alternatives></inline-formula> denotes concatenation.</p><p id="Par29">Mathematically, if we consider a transformer model with <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K$$\end{document}</tex-math><mml:math id="M16"><mml:mi>K</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq8.gif"/></alternatives></inline-formula> layers, and each layer <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k$$\end{document}</tex-math><mml:math id="M18"><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq9.gif"/></alternatives></inline-formula> performs a transformation <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_l$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq10.gif"/></alternatives></inline-formula>, the modified transformation with prefix becomes:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F'_k(p_k, x) = F_k(p_k \oplus x) \end{aligned}$$\end{document}</tex-math><mml:math id="M22" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mi>k</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x02295;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p_k$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq11.gif"/></alternatives></inline-formula> is the prefix for layer <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k$$\end{document}</tex-math><mml:math id="M26"><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq12.gif"/></alternatives></inline-formula>. The prefixes <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{p_1, p_2, ..., p_K\}$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq13.gif"/></alternatives></inline-formula> are learnable parameters and are optimized during the training process.</p></sec><sec id="Sec5"><title>Prompt tuning</title><p id="Par30">Prompt tuning, on the other hand, leverages the concept of natural language prompts. Here, the model is fed a prompt that guides it to generate outputs tailored to a specific task. In mathematical terms, given a pretrained model <inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}$$\end{document}</tex-math><mml:math id="M30"><mml:mi mathvariant="script">M</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq14.gif"/></alternatives></inline-formula>, the objective is to find an optimal prompt <inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p^*$$\end{document}</tex-math><mml:math id="M32"><mml:msup><mml:mi>p</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq15.gif"/></alternatives></inline-formula> such that the model&#x02019;s performance on a task <inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T$$\end{document}</tex-math><mml:math id="M34"><mml:mi>T</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq16.gif"/></alternatives></inline-formula> is maximized when the prompt is used as an input.</p><p id="Par31">Formally, for a task <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T$$\end{document}</tex-math><mml:math id="M36"><mml:mi>T</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq17.gif"/></alternatives></inline-formula> and a set of task-specific examples <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{(x_i, y_i)\}$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq18.gif"/></alternatives></inline-formula>, prompt tuning aims to optimize the following:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} p^* = \arg \max _p \sum _i \log {\mathscr {M}}(y_i | p \oplus x_i) \end{aligned}$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mo>arg</mml:mo><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mi>p</mml:mi></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mo>log</mml:mo><mml:mi mathvariant="script">M</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02295;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>This objective function maximizes the likelihood of the correct outputs <inline-formula id="IEq19"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq19.gif"/></alternatives></inline-formula> given the inputs <inline-formula id="IEq20"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq20.gif"/></alternatives></inline-formula> concatenated with the optimal prompt <inline-formula id="IEq21"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p^*$$\end{document}</tex-math><mml:math id="M46"><mml:msup><mml:mi>p</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq21.gif"/></alternatives></inline-formula>. Unlike prefix tuning, prompt tuning does not modify the internal workings of the model but rather influences its outputs through carefully crafted input sequences.</p></sec></sec><sec id="Sec6"><title>SK-tuning procedure</title><p id="Par32">
<fig id="Fig1"><label>Fig. 1</label><caption><p>SK-Tuning approaches for Prefix (left) and Prompt (right). The dashed line represents the optimization path during the backward pass to the trainable adapter. Notably, in the context of prompt-tuning (on the right), the no sign signifies the discontinuation of the forward pass beyond a certain point. This is because we exclusively initialize layer-specific semantic information for the prompt, rendering the continuation of the forward pass unnecessary for the remaining layers.</p></caption><graphic xlink:href="41598_2024_75599_Fig1_HTML" id="MO1"/></fig>
</p><sec id="Sec7"><title>Problem definition</title><p id="Par33">Consider a downstream task that utilizes a pretrained LLM denoted as <inline-formula id="IEq22"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}$$\end{document}</tex-math><mml:math id="M48"><mml:mi mathvariant="script">M</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq22.gif"/></alternatives></inline-formula>. Let the training dataset be represented as <inline-formula id="IEq23"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {D}} = \{(x_i, y_i)\}_{i=1}^N$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq23.gif"/></alternatives></inline-formula>, where each training example <inline-formula id="IEq24"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(x_i, y_i)$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq24.gif"/></alternatives></inline-formula> consists of an input text <inline-formula id="IEq25"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq25.gif"/></alternatives></inline-formula> and its associated true label <inline-formula id="IEq26"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq26.gif"/></alternatives></inline-formula>.</p><p id="Par34">Our primary objective is to fine-tune the pretrained LLM <inline-formula id="IEq27"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}$$\end{document}</tex-math><mml:math id="M58"><mml:mi mathvariant="script">M</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq27.gif"/></alternatives></inline-formula> for these downstream tasks while keeping the model parameters <inline-formula id="IEq28"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Theta$$\end{document}</tex-math><mml:math id="M60"><mml:mi mathvariant="normal">&#x00398;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq28.gif"/></alternatives></inline-formula> frozen. Specifically, we aim to achieve this fine-tuning by introducing a small number of additional parameters, referred to as adapters or transformations, which enable task-specific adaptation without the need to retrain the entire LLM. Each instance in our system is a pair <inline-formula id="IEq29"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(x_i, y_i)$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq29.gif"/></alternatives></inline-formula> that defines a specific task configuration.</p><p id="Par35">Mathematically, our goal can be expressed as follows:</p><p id="Par36">Given a pretrained LLM with parameters <inline-formula id="IEq30"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Theta$$\end{document}</tex-math><mml:math id="M64"><mml:mi mathvariant="normal">&#x00398;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq30.gif"/></alternatives></inline-formula> and a dataset <inline-formula id="IEq31"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {D}}$$\end{document}</tex-math><mml:math id="M66"><mml:mi mathvariant="script">D</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq31.gif"/></alternatives></inline-formula>, we seek to find a set of trainable parameters <inline-formula id="IEq32"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi$$\end{document}</tex-math><mml:math id="M68"><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq32.gif"/></alternatives></inline-formula> for the adapters or transformations, such that:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \Phi ^* = \arg \min _\Phi {\mathscr {L}}({\mathscr {M}}_{\Theta , \Phi }, {\mathscr {D}}) \end{aligned}$$\end{document}</tex-math><mml:math id="M70" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="normal">&#x003a6;</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mo>arg</mml:mo><mml:munder><mml:mo movablelimits="true">min</mml:mo><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:munder><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where: - <inline-formula id="IEq33"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}_{\Theta , \Phi }$$\end{document}</tex-math><mml:math id="M72"><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq33.gif"/></alternatives></inline-formula> represents the fine-tuned model with frozen parameters <inline-formula id="IEq34"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Theta$$\end{document}</tex-math><mml:math id="M74"><mml:mi mathvariant="normal">&#x00398;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq34.gif"/></alternatives></inline-formula> and trainable adapters/transformations <inline-formula id="IEq35"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi$$\end{document}</tex-math><mml:math id="M76"><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq35.gif"/></alternatives></inline-formula>. - <inline-formula id="IEq36"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {L}}$$\end{document}</tex-math><mml:math id="M78"><mml:mi mathvariant="script">L</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq36.gif"/></alternatives></inline-formula> is a task-specific loss function that quantifies the alignment between model predictions and true labels across the training dataset <inline-formula id="IEq37"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {D}}$$\end{document}</tex-math><mml:math id="M80"><mml:mi mathvariant="script">D</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq37.gif"/></alternatives></inline-formula>.</p><p id="Par37">Our objective is to ascertain the veracity of the true labels <inline-formula id="IEq38"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><mml:math id="M82"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq38.gif"/></alternatives></inline-formula> for the corresponding input texts <inline-formula id="IEq39"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M84"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq39.gif"/></alternatives></inline-formula> by effectively training a small number of parameters (<inline-formula id="IEq40"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi$$\end{document}</tex-math><mml:math id="M86"><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq40.gif"/></alternatives></inline-formula>) without altering the pretrained model&#x02019;s core architecture or parameters (<inline-formula id="IEq41"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Theta$$\end{document}</tex-math><mml:math id="M88"><mml:mi mathvariant="normal">&#x00398;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq41.gif"/></alternatives></inline-formula>). This approach aims to achieve parameter efficiency while tailoring the LLM to specific downstream tasks.</p></sec><sec id="Sec8"><title>SK-tuning for prefix</title><p id="Par38">SK-Tuning for Prefix enhances the versatility and performance of the pretrained LLM for downstream tasks by judiciously incorporating semantic knowledge from prefixes into the fine-tuning process. In the context of prefix-tuning a pretrained LLM <inline-formula id="IEq42"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}$$\end{document}</tex-math><mml:math id="M90"><mml:mi mathvariant="script">M</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq42.gif"/></alternatives></inline-formula>, traditionally, a mapping function from virtual trainable tokens to the LLM&#x02019;s layer representation is employed to generate the layer&#x02019;s trainable parameters. However, in our proposed SK-Tuning approach, we adopt a different strategy. We leverage the power of a pretrained LLM <inline-formula id="IEq43"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}$$\end{document}</tex-math><mml:math id="M92"><mml:mi mathvariant="script">M</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq43.gif"/></alternatives></inline-formula>, with its parameters frozen, to directly acquire semantic knowledge embeddings from the prefix tokens.</p><p id="Par39">Let <italic>p</italic> denote a prefix comprising a sequence of semantic knowledge tokens, with a length of <italic>l</italic>. The LLM model is assumed to have a dimension of <italic>d</italic>. Our objective is to extract the semantic hidden representation from each layer of the LLM for the given input prefix <italic>p</italic>. Let <italic>m</italic> represent the total number of layers, which includes the attention mechanisms. For each layer, we obtain its hidden representation <inline-formula id="IEq44"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_j^p$$\end{document}</tex-math><mml:math id="M94"><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mi>p</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq44.gif"/></alternatives></inline-formula>. For <italic>m</italic> layers the representation can be defined as follows:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} h^p = {\mathscr {M}}_{\Theta _{\text {frozen}}}(p) \in {\mathbb {R}}^{l \times d}. \end{aligned}$$\end{document}</tex-math><mml:math id="M96" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:msub><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mtext>frozen</mml:mtext></mml:msub></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>Next, we introduce a trainable adapter <inline-formula id="IEq45"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {F}}$$\end{document}</tex-math><mml:math id="M98"><mml:mi mathvariant="script">F</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq45.gif"/></alternatives></inline-formula>, parameterized by <inline-formula id="IEq46"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi$$\end{document}</tex-math><mml:math id="M100"><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq46.gif"/></alternatives></inline-formula>, which takes <inline-formula id="IEq47"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h^p$$\end{document}</tex-math><mml:math id="M102"><mml:msup><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq47.gif"/></alternatives></inline-formula> as input and yields a semantic projection <italic>z</italic> with the same dimension:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} z = {\mathscr {F}}_{\Phi }(h^p) \in {\mathbb {R}}^{m \times l \times d}. \end{aligned}$$\end{document}</tex-math><mml:math id="M104" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>l</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>Now, we possess <italic>z</italic> as the semantic representation of prefix tokens for every layer of <inline-formula id="IEq48"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}$$\end{document}</tex-math><mml:math id="M106"><mml:mi mathvariant="script">M</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq48.gif"/></alternatives></inline-formula>. During the processing of input <inline-formula id="IEq49"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M108"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq49.gif"/></alternatives></inline-formula> in <inline-formula id="IEq50"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}_{\Theta }$$\end{document}</tex-math><mml:math id="M110"><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi mathvariant="normal">&#x00398;</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq50.gif"/></alternatives></inline-formula>, we concatenate <inline-formula id="IEq51"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_{j \in m}$$\end{document}</tex-math><mml:math id="M112"><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq51.gif"/></alternatives></inline-formula> to the processing layer for the <italic>j</italic>-th layer of <inline-formula id="IEq52"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}_{\Theta }$$\end{document}</tex-math><mml:math id="M114"><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mi mathvariant="normal">&#x00398;</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq52.gif"/></alternatives></inline-formula>. This operation allows the <italic>j</italic>-th layer to access the corresponding semantic information from the prefix text.</p><p id="Par40">Now, if <inline-formula id="IEq53"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_i$$\end{document}</tex-math><mml:math id="M116"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq53.gif"/></alternatives></inline-formula> represents the final hidden output of <inline-formula id="IEq54"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M118"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq54.gif"/></alternatives></inline-formula>, we can define:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} r_i = {\mathscr {M}}_{\Theta _{\text {frozen}}}(x_i, z). \end{aligned}$$\end{document}</tex-math><mml:math id="M120" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:msub><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mtext>frozen</mml:mtext></mml:msub></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>Consider a task-specific module <inline-formula id="IEq55"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {C}}$$\end{document}</tex-math><mml:math id="M122"><mml:mi mathvariant="script">C</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq55.gif"/></alternatives></inline-formula>, parameterized by <inline-formula id="IEq56"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\zeta$$\end{document}</tex-math><mml:math id="M124"><mml:mi>&#x003b6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq56.gif"/></alternatives></inline-formula>, which embodies a downstream task:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} o_i = {\mathscr {C}}_{\zeta }(r_i) \end{aligned}$$\end{document}</tex-math><mml:math id="M126" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">C</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>Here, <inline-formula id="IEq57"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_i$$\end{document}</tex-math><mml:math id="M128"><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq57.gif"/></alternatives></inline-formula> represents the output of our task.</p><p id="Par41">Our training objective aims to minimize the loss function <inline-formula id="IEq58"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {L}}$$\end{document}</tex-math><mml:math id="M130"><mml:mi mathvariant="script">L</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq58.gif"/></alternatives></inline-formula>, which quantifies the discrepancy between <inline-formula id="IEq59"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_i$$\end{document}</tex-math><mml:math id="M132"><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq59.gif"/></alternatives></inline-formula> and the target label <inline-formula id="IEq60"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><mml:math id="M134"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq60.gif"/></alternatives></inline-formula>, thereby indicating whether <inline-formula id="IEq61"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_i$$\end{document}</tex-math><mml:math id="M136"><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq61.gif"/></alternatives></inline-formula> correctly represents the label for <inline-formula id="IEq62"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M138"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq62.gif"/></alternatives></inline-formula>:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \min _{\Phi , \zeta } {\mathscr {L}}\left( {\mathscr {C}}_{\zeta }({\mathscr {M}}_{\Theta _{\text {frozen}}}(x_i, {\mathscr {F}}_{\Phi }({\mathscr {M}}_{\Theta _{\text {frozen}}}(p)))), y_i\right) . \end{aligned}$$\end{document}</tex-math><mml:math id="M140" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo movablelimits="true">min</mml:mo><mml:mrow><mml:mi mathvariant="normal">&#x003a6;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b6;</mml:mi></mml:mrow></mml:munder><mml:mi mathvariant="script">L</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi mathvariant="script">C</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:msub><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mtext>frozen</mml:mtext></mml:msub></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:msub><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mtext>frozen</mml:mtext></mml:msub></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>This approach allows the fine-tuning process to concentrate explicitly on the representation and comprehension of labels, while simultaneously harnessing the intrinsic knowledge embedded within <inline-formula id="IEq63"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Theta$$\end{document}</tex-math><mml:math id="M142"><mml:mi mathvariant="normal">&#x00398;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq63.gif"/></alternatives></inline-formula>. The adjustment of parameters <inline-formula id="IEq64"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi$$\end{document}</tex-math><mml:math id="M144"><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq64.gif"/></alternatives></inline-formula> and <inline-formula id="IEq65"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\zeta$$\end{document}</tex-math><mml:math id="M146"><mml:mi>&#x003b6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq65.gif"/></alternatives></inline-formula> empowers the model to further refine its ability to map textual inputs to their corresponding labels.</p></sec><sec id="Sec9"><title>SK-tuning for prompt</title><p id="Par42">SK-Tuning for prompts involves a systematic process of semantic knowledge embedding, trainable adapter integration, concatenation, and a training objective. This approach allows for fine-tuning the pretrained LLM to effectively leverage semantic knowledge from prompts for improved performance in various downstream tasks. In the SK-Tuning framework for prompts, we focus on enhancing the capabilities of a pretrained LLM (<inline-formula id="IEq66"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}$$\end{document}</tex-math><mml:math id="M148"><mml:mi mathvariant="script">M</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq66.gif"/></alternatives></inline-formula>) by incorporating semantic knowledge from sequential prompt tokens, denoted as <inline-formula id="IEq67"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p$$\end{document}</tex-math><mml:math id="M150"><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq67.gif"/></alternatives></inline-formula>, of length <inline-formula id="IEq68"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M152"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq68.gif"/></alternatives></inline-formula>. Let <inline-formula id="IEq69"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {E}}$$\end{document}</tex-math><mml:math id="M154"><mml:mi mathvariant="script">E</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq69.gif"/></alternatives></inline-formula> represent the token embedding layer of <inline-formula id="IEq70"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}$$\end{document}</tex-math><mml:math id="M156"><mml:mi mathvariant="script">M</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq70.gif"/></alternatives></inline-formula>, and consider <inline-formula id="IEq71"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_p \in {\mathbb {R}}^{l \times d}$$\end{document}</tex-math><mml:math id="M158"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq71.gif"/></alternatives></inline-formula> and <inline-formula id="IEq72"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_{x_i} \in {\mathbb {R}}^{n \times d}$$\end{document}</tex-math><mml:math id="M160"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq72.gif"/></alternatives></inline-formula> as the semantic embeddings for prompt <inline-formula id="IEq73"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p$$\end{document}</tex-math><mml:math id="M162"><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq73.gif"/></alternatives></inline-formula> and input text <inline-formula id="IEq74"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M164"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq74.gif"/></alternatives></inline-formula>, respectively. Here, <inline-formula id="IEq75"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><mml:math id="M166"><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq75.gif"/></alternatives></inline-formula> is the sequence length of input <inline-formula id="IEq76"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M168"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq76.gif"/></alternatives></inline-formula>.</p><p id="Par43">To obtain the semantic representation of the prompt <inline-formula id="IEq77"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p$$\end{document}</tex-math><mml:math id="M170"><mml:mi>p</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq77.gif"/></alternatives></inline-formula> and input text <inline-formula id="IEq78"><alternatives><tex-math id="M171">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M172"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq78.gif"/></alternatives></inline-formula>, we utilize the pretrained token embedding layer <inline-formula id="IEq79"><alternatives><tex-math id="M173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {E}}$$\end{document}</tex-math><mml:math id="M174"><mml:mi mathvariant="script">E</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq79.gif"/></alternatives></inline-formula> as follows:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} e_p = {\mathscr {E}}(p) \thicksim {\mathscr {M}}_{\Theta _{\text {frozen}}} \end{aligned}$$\end{document}</tex-math><mml:math id="M176" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="script">E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:msub><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mtext>frozen</mml:mtext></mml:msub></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>and<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M177">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} e_{x_i} = {\mathscr {E}}(x_i) \thicksim {\mathscr {M}}_{\Theta _{\text {frozen}}} \end{aligned}$$\end{document}</tex-math><mml:math id="M178" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="script">E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:msub><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mtext>frozen</mml:mtext></mml:msub></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>This operation yields <inline-formula id="IEq80"><alternatives><tex-math id="M179">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_p$$\end{document}</tex-math><mml:math id="M180"><mml:msub><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq80.gif"/></alternatives></inline-formula>, which encapsulates the semantic information of the prompt, while <inline-formula id="IEq81"><alternatives><tex-math id="M181">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}_{\Theta _{\text {frozen}}}$$\end{document}</tex-math><mml:math id="M182"><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:msub><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mtext>frozen</mml:mtext></mml:msub></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq81.gif"/></alternatives></inline-formula> ensures that the model parameters remain frozen during this process.</p><p id="Par44">To further enhance the representation of the prompt, we introduce a trainable adapter, denoted as <inline-formula id="IEq82"><alternatives><tex-math id="M183">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {G}}$$\end{document}</tex-math><mml:math id="M184"><mml:mi mathvariant="script">G</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq82.gif"/></alternatives></inline-formula>, which is parameterized by <inline-formula id="IEq83"><alternatives><tex-math id="M185">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><mml:math id="M186"><mml:mi>&#x003b3;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq83.gif"/></alternatives></inline-formula>. This adapter takes <inline-formula id="IEq84"><alternatives><tex-math id="M187">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_p$$\end{document}</tex-math><mml:math id="M188"><mml:msub><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq84.gif"/></alternatives></inline-formula> as input and produces an updated embedding <inline-formula id="IEq85"><alternatives><tex-math id="M189">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_p^\prime$$\end{document}</tex-math><mml:math id="M190"><mml:msubsup><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq85.gif"/></alternatives></inline-formula> as follows:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M191">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} e_p^\prime = {\mathscr {G}}_{\gamma }(e_p) \in {\mathbb {R}}^{l \times d} \end{aligned}$$\end{document}</tex-math><mml:math id="M192" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>&#x003b3;</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>The adapter <inline-formula id="IEq86"><alternatives><tex-math id="M193">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {G}}_{\gamma }$$\end{document}</tex-math><mml:math id="M194"><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>&#x003b3;</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq86.gif"/></alternatives></inline-formula> serves as a mechanism to refine the semantic knowledge captured in <inline-formula id="IEq87"><alternatives><tex-math id="M195">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_p$$\end{document}</tex-math><mml:math id="M196"><mml:msub><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq87.gif"/></alternatives></inline-formula> according to the specific downstream task requirements, allowing for fine-tuning without modifying the frozen model parameters.</p><p id="Par45">The task head, denoted as <inline-formula id="IEq88"><alternatives><tex-math id="M197">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {C}}$$\end{document}</tex-math><mml:math id="M198"><mml:mi mathvariant="script">C</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq88.gif"/></alternatives></inline-formula>, is designed to incorporate both the enhanced prompt representation <inline-formula id="IEq89"><alternatives><tex-math id="M199">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_p^\prime$$\end{document}</tex-math><mml:math id="M200"><mml:msubsup><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq89.gif"/></alternatives></inline-formula> and the semantic embeddings of the input text <inline-formula id="IEq90"><alternatives><tex-math id="M201">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_{x_i}$$\end{document}</tex-math><mml:math id="M202"><mml:msub><mml:mi>e</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq90.gif"/></alternatives></inline-formula>. We achieve this through concatenation:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M203">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} o_i = {\mathscr {C}}_{\zeta }({\mathscr {M}}_{\Theta _{\text {frozen}}}(e_p^\prime \oplus e_{x_i})) \end{aligned}$$\end{document}</tex-math><mml:math id="M204" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">C</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:msub><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mtext>frozen</mml:mtext></mml:msub></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:mo>&#x02295;</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>Here, <inline-formula id="IEq91"><alternatives><tex-math id="M205">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\oplus$$\end{document}</tex-math><mml:math id="M206"><mml:mo>&#x02295;</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq91.gif"/></alternatives></inline-formula> represents the concatenation operation, and <inline-formula id="IEq92"><alternatives><tex-math id="M207">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_i$$\end{document}</tex-math><mml:math id="M208"><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq92.gif"/></alternatives></inline-formula> serves as the output for the downstream task, allowing the model to leverage both prompt and input text information effectively.</p><p id="Par46">The training objective for SK-Tuning of the prompt involves minimizing a loss function <inline-formula id="IEq93"><alternatives><tex-math id="M209">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {L}}$$\end{document}</tex-math><mml:math id="M210"><mml:mi mathvariant="script">L</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq93.gif"/></alternatives></inline-formula>. This loss function quantifies the difference between the predicted output and the target label <inline-formula id="IEq94"><alternatives><tex-math id="M211">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><mml:math id="M212"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq94.gif"/></alternatives></inline-formula>, reflecting the model&#x02019;s performance on the specific task:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M213">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \min _{\gamma , \zeta } {\mathscr {L}}\left( {\mathscr {C}}_{\zeta }({\mathscr {M}}_{\Theta _{\text {frozen}}}({\mathscr {G}}_{\gamma }(e_p) \oplus e_{x_i}), y_i\right) \end{aligned}$$\end{document}</tex-math><mml:math id="M214" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo movablelimits="true">min</mml:mo><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b6;</mml:mi></mml:mrow></mml:munder><mml:mi mathvariant="script">L</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi mathvariant="script">C</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:msub><mml:mi mathvariant="normal">&#x00398;</mml:mi><mml:mtext>frozen</mml:mtext></mml:msub></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>&#x003b3;</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x02295;</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>Here, <inline-formula id="IEq95"><alternatives><tex-math id="M215">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><mml:math id="M216"><mml:mi>&#x003b3;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq95.gif"/></alternatives></inline-formula> and <inline-formula id="IEq96"><alternatives><tex-math id="M217">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\zeta$$\end{document}</tex-math><mml:math id="M218"><mml:mi>&#x003b6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq96.gif"/></alternatives></inline-formula> denote the parameters of the adapter <inline-formula id="IEq97"><alternatives><tex-math id="M219">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {G}}$$\end{document}</tex-math><mml:math id="M220"><mml:mi mathvariant="script">G</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq97.gif"/></alternatives></inline-formula> and the task head <inline-formula id="IEq98"><alternatives><tex-math id="M221">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {C}}$$\end{document}</tex-math><mml:math id="M222"><mml:mi mathvariant="script">C</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq98.gif"/></alternatives></inline-formula> respectively.</p></sec><sec id="Sec10"><title>Algorithms</title><p id="Par47">In this section, we describe two key algorithms that constitute the core of our proposed SK-Tuning approach for enhancing the fine-tuning of LLMs in the context of specific downstream tasks.</p><sec id="Sec11"><title>SK-tuning for prefix</title><p id="Par48">The first algorithm, outlined in Algorithm&#x000a0;1 (SK-Tuning for Prefix), details the process of incorporating semantic knowledge from prefixes into the fine-tuning of a pretrained LLM. The algorithm begins with inputs of a pretrained language model <inline-formula id="IEq99"><alternatives><tex-math id="M223">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}$$\end{document}</tex-math><mml:math id="M224"><mml:mi mathvariant="script">M</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq99.gif"/></alternatives></inline-formula> with frozen parameters <inline-formula id="IEq100"><alternatives><tex-math id="M225">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Theta$$\end{document}</tex-math><mml:math id="M226"><mml:mi mathvariant="normal">&#x00398;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq100.gif"/></alternatives></inline-formula>, a prompt text <italic>p</italic>, and a dataset <inline-formula id="IEq101"><alternatives><tex-math id="M227">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${(x_i, y_i)}_{i=1}^N$$\end{document}</tex-math><mml:math id="M228"><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq101.gif"/></alternatives></inline-formula>. The trainable parameters <inline-formula id="IEq102"><alternatives><tex-math id="M229">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi$$\end{document}</tex-math><mml:math id="M230"><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq102.gif"/></alternatives></inline-formula> and <inline-formula id="IEq103"><alternatives><tex-math id="M231">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\zeta$$\end{document}</tex-math><mml:math id="M232"><mml:mi>&#x003b6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq103.gif"/></alternatives></inline-formula> are initialized. For each input example <inline-formula id="IEq104"><alternatives><tex-math id="M233">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$({\textbf{x}}i, y_i)$$\end{document}</tex-math><mml:math id="M234"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq104.gif"/></alternatives></inline-formula>, the prompt text is processed through the frozen LLM to obtain <inline-formula id="IEq105"><alternatives><tex-math id="M235">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h^p$$\end{document}</tex-math><mml:math id="M236"><mml:msup><mml:mi>h</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq105.gif"/></alternatives></inline-formula>, which represents the hidden representation from each layer. This representation is then transformed using a trainable adapter <inline-formula id="IEq106"><alternatives><tex-math id="M237">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {F}}_{\Phi }$$\end{document}</tex-math><mml:math id="M238"><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq106.gif"/></alternatives></inline-formula> to yield <italic>z</italic>. Subsequently, the input text <inline-formula id="IEq107"><alternatives><tex-math id="M239">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M240"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq107.gif"/></alternatives></inline-formula> is processed, incorporating the generated <italic>z</italic> for improved task-specific adaptation. Finally, the classification head <inline-formula id="IEq108"><alternatives><tex-math id="M241">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {C}}_{\zeta }$$\end{document}</tex-math><mml:math id="M242"><mml:msub><mml:mi mathvariant="script">C</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq108.gif"/></alternatives></inline-formula> computes the output <inline-formula id="IEq109"><alternatives><tex-math id="M243">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_i$$\end{document}</tex-math><mml:math id="M244"><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq109.gif"/></alternatives></inline-formula> for the downstream task, and the loss <inline-formula id="IEq110"><alternatives><tex-math id="M245">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {L}}(o_i, y_i)$$\end{document}</tex-math><mml:math id="M246"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq110.gif"/></alternatives></inline-formula> is computed. The trainable parameters <inline-formula id="IEq111"><alternatives><tex-math id="M247">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Phi$$\end{document}</tex-math><mml:math id="M248"><mml:mi mathvariant="normal">&#x003a6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq111.gif"/></alternatives></inline-formula> and <inline-formula id="IEq112"><alternatives><tex-math id="M249">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\zeta$$\end{document}</tex-math><mml:math id="M250"><mml:mi>&#x003b6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq112.gif"/></alternatives></inline-formula> are updated iteratively to minimize the loss.</p><p id="Par49">
<fig position="anchor" id="Figa"><label>Algorithm 1</label><caption><p>SK-tuning for prefix</p></caption><graphic position="anchor" xlink:href="41598_2024_75599_Figa_HTML" id="MO2"/></fig>
</p></sec><sec id="Sec12"><title>SK-tuning for prompt</title><p id="Par50">The second algorithm, described in Algorithm 2 (SK-Tuning for Prompt), focuses on leveraging semantic knowledge from sequential prompt tokens to enhance fine-tuning. It begins with inputs of a pretrained language model <inline-formula id="IEq113"><alternatives><tex-math id="M251">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {M}}$$\end{document}</tex-math><mml:math id="M252"><mml:mi mathvariant="script">M</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq113.gif"/></alternatives></inline-formula> with frozen parameters <inline-formula id="IEq114"><alternatives><tex-math id="M253">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Theta$$\end{document}</tex-math><mml:math id="M254"><mml:mi mathvariant="normal">&#x00398;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq114.gif"/></alternatives></inline-formula>, a prompt text <italic>p</italic>, and a dataset <inline-formula id="IEq115"><alternatives><tex-math id="M255">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${({\textbf{x}}i, y_i)}_{i=1}^N$$\end{document}</tex-math><mml:math id="M256"><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq115.gif"/></alternatives></inline-formula>. Trainable parameters <inline-formula id="IEq116"><alternatives><tex-math id="M257">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><mml:math id="M258"><mml:mi>&#x003b3;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq116.gif"/></alternatives></inline-formula> and <inline-formula id="IEq117"><alternatives><tex-math id="M259">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\zeta$$\end{document}</tex-math><mml:math id="M260"><mml:mi>&#x003b6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq117.gif"/></alternatives></inline-formula> are initialized. For each input example <inline-formula id="IEq118"><alternatives><tex-math id="M261">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(x_i, y_i)$$\end{document}</tex-math><mml:math id="M262"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq118.gif"/></alternatives></inline-formula>, the embeddings of the prompt text <italic>p</italic> and input text <inline-formula id="IEq119"><alternatives><tex-math id="M263">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\textbf{x}}i$$\end{document}</tex-math><mml:math id="M264"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq119.gif"/></alternatives></inline-formula> are obtained through the pretrained token embedding layer <inline-formula id="IEq120"><alternatives><tex-math id="M265">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {E}}$$\end{document}</tex-math><mml:math id="M266"><mml:mi mathvariant="script">E</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq120.gif"/></alternatives></inline-formula> while ensuring the core LLM parameters remain frozen. The prompt embedding <inline-formula id="IEq121"><alternatives><tex-math id="M267">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_p$$\end{document}</tex-math><mml:math id="M268"><mml:msub><mml:mi>e</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq121.gif"/></alternatives></inline-formula> and text embedding <inline-formula id="IEq122"><alternatives><tex-math id="M269">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e{x_i}$$\end{document}</tex-math><mml:math id="M270"><mml:mrow><mml:mi>e</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq122.gif"/></alternatives></inline-formula> are then utilized to create an enhanced prompt representation <inline-formula id="IEq123"><alternatives><tex-math id="M271">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_p^\prime$$\end{document}</tex-math><mml:math id="M272"><mml:msubsup><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq123.gif"/></alternatives></inline-formula> using a trainable adapter <inline-formula id="IEq124"><alternatives><tex-math id="M273">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {G}}_{\gamma }$$\end{document}</tex-math><mml:math id="M274"><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>&#x003b3;</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq124.gif"/></alternatives></inline-formula>. The classification head <inline-formula id="IEq125"><alternatives><tex-math id="M275">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {C}}_{\zeta }$$\end{document}</tex-math><mml:math id="M276"><mml:msub><mml:mi mathvariant="script">C</mml:mi><mml:mi>&#x003b6;</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq125.gif"/></alternatives></inline-formula> combines this enhanced prompt representation with the input text embedding and computes the output <inline-formula id="IEq126"><alternatives><tex-math id="M277">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o_i$$\end{document}</tex-math><mml:math id="M278"><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq126.gif"/></alternatives></inline-formula> for the downstream task. As in the previous algorithm, the loss <inline-formula id="IEq127"><alternatives><tex-math id="M279">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr {L}}(o_i, y_i)$$\end{document}</tex-math><mml:math id="M280"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq127.gif"/></alternatives></inline-formula> is computed, and the trainable parameters <inline-formula id="IEq128"><alternatives><tex-math id="M281">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><mml:math id="M282"><mml:mi>&#x003b3;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq128.gif"/></alternatives></inline-formula> and <inline-formula id="IEq129"><alternatives><tex-math id="M283">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\zeta$$\end{document}</tex-math><mml:math id="M284"><mml:mi>&#x003b6;</mml:mi></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq129.gif"/></alternatives></inline-formula> are updated iteratively to minimize the loss.</p><p id="Par51">
<fig position="anchor" id="Figb"><label>Algorithm 2</label><caption><p>SK-tuning for prompt</p></caption><graphic position="anchor" xlink:href="41598_2024_75599_Figb_HTML" id="MO3"/></fig>
</p></sec></sec></sec><sec id="Sec13"><title>Experiments</title><sec id="Sec14"><title>Experimental setup</title><p id="Par52">Our experiments utilize a computational setup with two NVIDIA RTX H100 GPUs (80GB VRAM each), an Intel&#x000ae;Xeon&#x000ae;Gold 6448Y 2.1 GHz 32 Core Processor. This system includes 128GB of RAM and a Dell 7.68TB Enterprise NVMe Read Intensive Drive, providing the necessary computational power and storage for efficient model training and evaluation.</p><p id="Par53">For the implementation, we employed the PyTorch<sup><xref ref-type="bibr" rid="CR65">65</xref></sup> deep learning framework for the implementation of our experiments. Additionally, we leveraged the Transformers library developed by Hugging Face<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>. This library offers a comprehensive set of tools and pretrained models for NLP tasks, facilitating the training and evaluation of LLMs on a variety of datasets.</p><p id="Par54">The combination of these resources and software frameworks allowed us to conduct extensive experiments, enabling us to assess the performance and effectiveness of our proposed SK-Tuning approach across a range of downstream tasks.</p></sec><sec id="Sec15"><title>LM results</title><p id="Par56"><bold>Datasets:</bold> We evaluate our SK-Tuning on CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI and RTE of the GLUE Benchmarks<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>. We compute the accuracy using the Matthews correlation for CoLA, accuracy/F1 score for MRPC and QQP, Pearson/Spearman correlation for STS-B, average matched accuracy for MNLI, and accuracy for other NLU tasks in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.</p><p id="Par55">
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Performance comparison of RoBERTa models on GLUE tasks: metrics include MCC for CoLA, accuracy for SST-2, accuracy/F1-score for MRPC and QQP, Pearson/Spearman correlation for STS-B, and Accuracy for MNLI, QNLI, and RTE.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">PEFT method</th><th align="left"># TPs</th><th align="left">CoLA</th><th align="left">SST2</th><th align="left">MRPC</th><th align="left">STS-B</th><th align="left">QQP</th><th align="left">MNLI</th><th align="left">QNLI</th><th align="left">RTE</th><th align="left">Avg.</th></tr></thead><tbody><tr><td align="left" rowspan="14">RoB<sub>B</sub></td><td align="left">FT</td><td align="left">124.6M</td><td align="left">67.07</td><td align="left">95.89</td><td align="left">90.24/93.98</td><td align="left">92.87/91.61</td><td align="left">91.18/89.02</td><td align="left">88.27</td><td align="left">92.67</td><td align="left">78.20</td><td align="left">87.04/91.53</td></tr><tr><td align="left">Adapter<sup>S</sup></td><td align="left">7.41M</td><td align="left">
<italic>63.32</italic>
</td><td align="left">94.31</td><td align="left">
<italic>90.44/93.18</italic>
</td><td align="left">91.25/<italic>90.94</italic></td><td align="left">
<italic>90.81/86.55</italic>
</td><td align="left"><bold>87</bold>.<bold>33</bold></td><td align="left">92.06</td><td align="left">73.56</td><td align="left">
<italic>85.38/90.22</italic>
</td></tr><tr><td align="left">Prompt tuning</td><td align="left">
<italic>0.61M</italic>
</td><td align="left">49.37</td><td align="left">92.09</td><td align="left">70.83/81.72</td><td align="left">82.44/83.11</td><td align="left">82.99/78.35</td><td align="left">80.57</td><td align="left">80.03</td><td align="left">58.12</td><td align="left">74.55/81.06</td></tr><tr><td align="left">Prefix-tuning</td><td align="left">0.96M</td><td align="left">59.31</td><td align="left">93.81</td><td align="left">87.25/91.03</td><td align="left">88.48/88.32</td><td align="left">87.75/84.09</td><td align="left">85.21</td><td align="left">90.77</td><td align="left">54.51</td><td align="left">80.88/87.81</td></tr><tr><td align="left">(IA)<sup>3</sup></td><td align="left">0.66M</td><td align="left">59.58</td><td align="left">93.92</td><td align="left">87.00/90.52</td><td align="left">90.30/90.32</td><td align="left">87.99/84.10</td><td align="left">83.95</td><td align="left">90.88</td><td align="left">71.12</td><td align="left">83.09/88.31</td></tr><tr><td align="left">BitFit</td><td align="left">0.69M</td><td align="left">61.32</td><td align="left">94.72</td><td align="left">89.22/92.41</td><td align="left">90.34/90.27</td><td align="left">88.12/84.11</td><td align="left">84.64</td><td align="left">91.09</td><td align="left"><bold>77</bold>.<bold>98</bold></td><td align="left">84.67/88.93</td></tr><tr><td align="left">LoRA</td><td align="left">0.89M</td><td align="left">62.09</td><td align="left">94.04</td><td align="left">87.50/90.68</td><td align="left">90.66/90.83</td><td align="left">88.83/85.21</td><td align="left">86.54</td><td align="left">92.02</td><td align="left">72.92</td><td align="left">84.32/88.90</td></tr><tr><td align="left">AdaLoRA</td><td align="left">1.03M</td><td align="left">59.82</td><td align="left">93.92</td><td align="left">87.99/91.33</td><td align="left">90.83/90.73</td><td align="left">88.58/84.98</td><td align="left">86.26</td><td align="left">91.43</td><td align="left">70.04</td><td align="left">83.60/89.01</td></tr><tr><td align="left">MAM Adapter</td><td align="left">46.78M</td><td align="left">61.42</td><td align="left">
<italic>94.87</italic>
</td><td align="left">89.31/92.21</td><td align="left">90.74/90.42</td><td align="left">88.31/83.20</td><td align="left">86.63</td><td align="left">90.19</td><td align="left">72.62</td><td align="left">84.26/88.61</td></tr><tr><td align="left">PROPETL <sub>Adapter</sub></td><td align="left">1.87M</td><td align="left"><bold>66</bold>.<bold>33</bold></td><td align="left">93.85</td><td align="left">87.25/90.82</td><td align="left"><italic>91.33</italic>/<bold>91</bold>.<bold>04</bold></td><td align="left">89.22/85.79</td><td align="left">86.49</td><td align="left">
<italic>92.56</italic>
</td><td align="left">75.54</td><td align="left">85.32/89.21</td></tr><tr><td align="left">PROPETL <sub>Prefix</sub></td><td align="left">10.49M</td><td align="left">61.79</td><td align="left">94.30</td><td align="left">88.73/91.98</td><td align="left">90.30/90.19</td><td align="left">88.54/85.05</td><td align="left">86.22</td><td align="left">91.51</td><td align="left">63.31</td><td align="left">83.08/89.07</td></tr><tr><td align="left">PROPETL <sub>LoRA</sub></td><td align="left">1.77M</td><td align="left">60.38</td><td align="left">94.11</td><td align="left">87.42/90.87</td><td align="left">90.76/90.55</td><td align="left">88.90/85.55</td><td align="left">
<italic>86.84</italic>
</td><td align="left">92.04</td><td align="left">67.39</td><td align="left">83.48/88.99</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left">
<bold>0.60M</bold>
</td><td align="left">60.21</td><td align="left"><bold>94</bold>.<bold>88</bold></td><td align="left">
<bold>89.73/92.47</bold>
</td><td align="left"><bold>91</bold>.<bold>30</bold>/90.19</td><td align="left">
<bold>90.83/87.82</bold>
</td><td align="left">86.24</td><td align="left"><bold>92</bold>.<bold>60</bold></td><td align="left">
<italic>76.91</italic>
</td><td align="left">
<bold>85.45/90.37</bold>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">0.84M</td><td align="left">61.83</td><td align="left">93.72</td><td align="left">90.21/90.04</td><td align="left">90.11/89.92</td><td align="left">88.67/87.12</td><td align="left">85.83</td><td align="left">92.09</td><td align="left">75.32</td><td align="left">83.83/89.29</td></tr><tr><td align="left" rowspan="14">RoB<sub>L</sub></td><td align="left">FT</td><td align="left">355.3M</td><td align="left">69.78</td><td align="left">97.54</td><td align="left">92.22/94.28</td><td align="left">93.74/92.96</td><td align="left">93.30/89.68</td><td align="left">92.42</td><td align="left">96.61</td><td align="left">89.23</td><td align="left">90.60/92.30</td></tr><tr><td align="left">Adapter<sup>S</sup></td><td align="left">19.77M</td><td align="left">67.03</td><td align="left">96.37</td><td align="left">89.94/92.54</td><td align="left"><italic>92.58</italic>/92.42</td><td align="left">
<italic>92.19/88.50</italic>
</td><td align="left">
<italic>91.00</italic>
</td><td align="left">94.31</td><td align="left">85.25</td><td align="left">88.58/<italic>91.15</italic></td></tr><tr><td align="left">Prompt-tuning</td><td align="left">
<italic>1.07M</italic>
</td><td align="left">61.13</td><td align="left">94.61</td><td align="left">73.04/81.29</td><td align="left">78.51/78.99</td><td align="left">80.74/75.16</td><td align="left">68.15</td><td align="left">89.13</td><td align="left">60.29</td><td align="left">75.70/78.48</td></tr><tr><td align="left">Prefix-tuning</td><td align="left">2.03M</td><td align="left">59.01</td><td align="left">95.76</td><td align="left">88.24/91.37</td><td align="left">90.92/91.07</td><td align="left">88.88/85.45</td><td align="left">89.30</td><td align="left">93.32</td><td align="left">74.01</td><td align="left">84.93/89.29</td></tr><tr><td align="left">(IA)<sup>3</sup></td><td align="left">1.22M</td><td align="left">61.15</td><td align="left">94.61</td><td align="left">86.52/90.33</td><td align="left">92.22/86.25</td><td align="left">89.45/86.25</td><td align="left">88.63</td><td align="left">94.25</td><td align="left">81.23</td><td align="left">86.00/87.61</td></tr><tr><td align="left">Bitfit</td><td align="left">1.32M</td><td align="left"><bold>68</bold>.<bold>01</bold></td><td align="left">96.10</td><td align="left">
<italic>90.93/93.38</italic>
</td><td align="left">91.93/<bold>93</bold>.<bold>38</bold></td><td align="left">89.48/86.43</td><td align="left">89.98</td><td align="left">94.47</td><td align="left">
<italic>87.73</italic>
</td><td align="left">88.57/91.06</td></tr><tr><td align="left">LoRA</td><td align="left">1.84M</td><td align="left">64.47</td><td align="left">
<italic>96.67</italic>
</td><td align="left">87.50/91.19</td><td align="left">91.66/91.44</td><td align="left">90.15/86.91</td><td align="left">90.76</td><td align="left">95.00</td><td align="left">79.78</td><td align="left">86.99/89.84</td></tr><tr><td align="left">AdaLoRA</td><td align="left">2.23M</td><td align="left">65.85</td><td align="left">94.95</td><td align="left">89.46/92.34</td><td align="left">92.05/91.80</td><td align="left">89.60/86.30</td><td align="left">90.36</td><td align="left">94.62</td><td align="left">77.98</td><td align="left">86.85/90.14</td></tr><tr><td align="left">MAM Adapter</td><td align="left">122.20M</td><td align="left">67.39</td><td align="left">95.81</td><td align="left">90.12/92.77</td><td align="left">92.44/92.18</td><td align="left">90.87/86.65</td><td align="left">90.62</td><td align="left">94.31</td><td align="left">86.62</td><td align="left">88.52/90.53</td></tr><tr><td align="left">PROPETL <sub>Adapter</sub></td><td align="left">5.40M</td><td align="left">65.55</td><td align="left">96.27</td><td align="left">89.71/92.54</td><td align="left">91.92/91.67</td><td align="left">90.67/87.74</td><td align="left"><bold>91</bold>.<bold>37</bold></td><td align="left">
<italic>95.20</italic>
</td><td align="left"><bold>88</bold>.<bold>89</bold></td><td align="left"><italic>88.69</italic>/90.65</td></tr><tr><td align="left">PROPETL <sub>Prefix</sub></td><td align="left">26.85M</td><td align="left">62.24</td><td align="left">96.17</td><td align="left">90.04/92.92</td><td align="left">90.70/90.49</td><td align="left">89.30/86.30</td><td align="left">90.33</td><td align="left">94.73</td><td align="left">79.71</td><td align="left">86.65/89.90</td></tr><tr><td align="left">PROPETL <sub>LoRA</sub></td><td align="left">4.19M</td><td align="left">61.90</td><td align="left">95.93</td><td align="left">89.06/92.19</td><td align="left">91.66/91.38</td><td align="left">90.93/88.05</td><td align="left">90.53</td><td align="left">94.93</td><td align="left">83.57</td><td align="left">87.31/90.54</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left">
<bold>1.02M</bold>
</td><td align="left">
<italic>67.13</italic>
</td><td align="left"><bold>96</bold>.<bold>43</bold></td><td align="left">
<bold>91.10/93.22</bold>
</td><td align="left"><bold>92</bold>.<bold>54</bold>/<italic>92.11</italic></td><td align="left">
<bold>92.10/88.73</bold>
</td><td align="left">90.42</td><td align="left">95.42</td><td align="left">87.11</td><td align="left">
<bold>89.01/91.34</bold>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">1.94M</td><td align="left">66.33</td><td align="left">96.08</td><td align="left">90.96/93.09</td><td align="left">91.87/90.68</td><td align="left">90.23/87.93</td><td align="left">89.97</td><td align="left"><bold>96</bold>.<bold>10</bold></td><td align="left">86.99</td><td align="left">86.86/89.66</td></tr></tbody></table><table-wrap-foot><p>Significant values are in [bold, italics].</p></table-wrap-foot></table-wrap>
</p><p id="Par57"><bold>Model selection &#x00026; hyperparameter:</bold> For the GLUE benchmark, the models we select for fine-tuning are RoBERTa-base <inline-formula id="IEq130"><alternatives><tex-math id="M285">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$RoB_B$$\end{document}</tex-math><mml:math id="M286"><mml:mrow><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq130.gif"/></alternatives></inline-formula> with 125M parameters and RoBERTa-large <inline-formula id="IEq131"><alternatives><tex-math id="M287">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$RoB_L$$\end{document}</tex-math><mml:math id="M288"><mml:mrow><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq131.gif"/></alternatives></inline-formula> with 355M parameters from<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>. Dropout, attention dropout, and weight decay rates are uniformly maintained at 0.2 across all tasks. The initial learning rate was <inline-formula id="IEq132"><alternatives><tex-math id="M289">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 10^{-4}$$\end{document}</tex-math><mml:math id="M290"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq132.gif"/></alternatives></inline-formula>, subsequently fine-tuned to <inline-formula id="IEq133"><alternatives><tex-math id="M291">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2 \times 10^{-5}$$\end{document}</tex-math><mml:math id="M292"><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq133.gif"/></alternatives></inline-formula> and <inline-formula id="IEq134"><alternatives><tex-math id="M293">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2 \times 10^{-6}$$\end{document}</tex-math><mml:math id="M294"><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq134.gif"/></alternatives></inline-formula>. All datasets have been trained over 10 epochs.</p><p id="Par58"><bold>Results:</bold> Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> presents a detailed performance comparison of various parameter-efficient fine-tuning (PEFT) methods applied to two versions of the RoBERTa model on GLUE tasks, highlighting the SK-Tuning methods as particularly effective. These methods achieve competitive or superior performance across several metrics while utilizing significantly fewer parameters-demonstrated by as low as 0.60M parameters for <inline-formula id="IEq135"><alternatives><tex-math id="M295">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$RoB_B$$\end{document}</tex-math><mml:math id="M296"><mml:mrow><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq135.gif"/></alternatives></inline-formula> and 1.02M for <inline-formula id="IEq136"><alternatives><tex-math id="M297">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$RoB_L$$\end{document}</tex-math><mml:math id="M298"><mml:mrow><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq136.gif"/></alternatives></inline-formula>. Notably, SK-Tuning (Prompt) and SK-Tuning (Prefix) consistently perform well across different task types, such as SST2 and QQP, demonstrating a compelling balance between model efficiency and task performance. This efficiency makes SK-Tuning an attractive option for scenarios requiring deployment in resource-constrained environments or where fast inference is crucial. The results underscore the potential of small, well-tuned models to match or even surpass the performance of larger, fully fine-tuned counterparts, suggesting a promising direction for future research in NLP model optimization.</p></sec><sec id="Sec16"><title>LLM results</title><p id="Par59">We conducted experiments on a diverse set of datasets to evaluate the performance of SK-Tuning across various NLP tasks, including sequence classification, token classification, and NLI. Our goal was to compare the performance of SK-Tuning with existing models on these tasks. Subsequently, we provide extensive details on the datasets utilized in our experiments.</p><sec id="Sec17"><title>Classification datasets</title><p id="Par60">Sequence classification, a common task in NLP, involves labeling or categorizing text. In our study, we utilized five datasets: CoLA, SST2 from the GLUE benchmark, along with the Emotion dataset, and the Fake News Filipino dataset.<list list-type="bullet"><list-item><p id="Par61"><bold>Cola</bold> (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/glue/viewer/cola/">https://huggingface.co/datasets/glue/viewer/cola/</ext-link>): The Corpus of Linguistic Acceptability (CoLA)<sup><xref ref-type="bibr" rid="CR69">69</xref></sup> consists of 10,657 sentences curated from 23 linguistic publications. Each sentence has been meticulously annotated by its original author for grammaticality or acceptability. The publicly available version of the dataset includes 9,594 sentences for training and validation, while 1063 sentences are reserved for a separate held-out test set.</p></list-item><list-item><p id="Par63"><bold>SST-2</bold> (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/sst2">https://huggingface.co/datasets/sst2</ext-link>): The Stanford Sentiment Treebank is a dataset featuring fully labeled parse trees, enabling a comprehensive examination of how sentiment compositionally influences language. Derived from the dataset presented by Pang and Lee<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, the corpus comprises 11,855 individual sentences extracted from movie reviews. Employing the Stanford parser, the dataset encompasses a total of 215,154 distinct phrases derived from these parse trees, with each phrase annotated by three human judges. The experiments involving binary classification on complete sentences (distinguishing between negative or somewhat negative versus somewhat positive or positive, with neutral sentences excluded) are denoted by the dataset acronym SST-2. The publicly available version includes 67,349 sentences designated for training along with 872 for validation set, while 1,821 sentences are for the test set.</p></list-item><list-item><p id="Par65"><bold>Emotion</bold> (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/dair-ai/emotion">https://huggingface.co/datasets/dair-ai/emotion</ext-link>): Emotion is a dataset<sup><xref ref-type="bibr" rid="CR71">71</xref></sup> of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. The publicly available version includes 16,000 sentences designated for training along with 2000 for validation set, while 2000 sentences are for the test set.</p></list-item><list-item><p id="Par67"><bold>Fake News Filipino</bold> (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/fake_news_filipino">https://huggingface.co/datasets/fake_news_filipino</ext-link>): A unique initiative in low-resource fake news detection dataset<sup><xref ref-type="bibr" rid="CR72">72</xref></sup> for the Filipino language. Comprising 3,206 meticulously labeled news samples, evenly divided between authentic and fabricated content, this dataset represents a pioneering effort. We partitioned the dataset into 70%, 10% and 20% for training, validation, and testing purposes.</p></list-item></list></p></sec><sec id="Sec18"><title>Token classification datasets</title><p id="Par69">Token classification involves labeling individual tokens within a sentence. Named Entity Recognition (NER) is a prevalent task in token classification, aiming to assign labels to entities in a sentence, which may include individuals, locations, or organizations. We have used 3 token classification datasets: CoNLL 2003, NCBI Disease, and WikiAnn dataset.<list list-type="bullet"><list-item><p id="Par70"><bold>CoNLL 2003</bold> (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/conll2003">https://huggingface.co/datasets/conll2003</ext-link>): CoNLL-2003 serves as a named entity recognition dataset<sup><xref ref-type="bibr" rid="CR73">73</xref></sup> introduced within the framework of the CoNLL-2003 shared task, focusing on language-independent named entity recognition. This dataset comprises eight files that encompass two languages: English and German. We have utilized the English dataset and &#x0201c;ner tags&#x0201d; as labels for our experiment. The publicly available version includes 14,041 examples designated for training along with 3250 for validation examples, while 3453 examples are for testing.</p></list-item><list-item><p id="Par72"><bold>NCBI Disease</bold> (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/ncbi_disease">https://huggingface.co/datasets/ncbi_disease</ext-link>): The dataset<sup><xref ref-type="bibr" rid="CR74">74</xref></sup> includes annotations for disease names and concepts from the NCBI disease corpus, which is a compilation of 793 PubMed abstracts extensively annotated at both the mention and concept levels. There are 3 labels, 0 indicates no disease mentioned, 1 signals the first token of a disease, and 2 the subsequent disease tokens. The publicly available version includes 5433 examples designated for training along with 924 for validation examples, while 941 examples are for testing.</p></list-item><list-item><p id="Par74"><bold>WikiAnn</bold> (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/wikiann">https://huggingface.co/datasets/wikiann</ext-link>): WikiANN, also known as PAN-X, is a multilingual dataset<sup><xref ref-type="bibr" rid="CR75">75</xref></sup> for named entity recognition. It comprises Wikipedia articles annotated with location (LOC), person (PER), and organization (ORG) tags using the IOB2 format. This specific version aligns with the balanced train, validation, and test splits of 20,000, 10,000, and 10,000, respectively introduced by Rahimi et al. (2019), covering 176 out of the 282 languages featured in the original WikiANN corpus.</p></list-item></list></p></sec><sec id="Sec19"><title>Entailment datasets</title><p id="Par76">NLI involves the challenge of determining the truth (entailment), falsity (contradiction), or undetermined status (neutral) of a &#x0201c;hypothesis&#x0201d; based on a provided &#x0201c;premise.&#x0201d; We have used 3 NLI datasets for this task: RTE, SNLI, and MRPC.<list list-type="bullet"><list-item><p id="Par77"><bold>RTE</bold> (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/glue/viewer/rte">https://huggingface.co/datasets/glue/viewer/rte</ext-link>): The Recognizing Textual Entailment (RTE) datasets originate from a series of annual challenges focused on textual entailment. The creators of the benchmark amalgamated data from RTE1<sup><xref ref-type="bibr" rid="CR76">76</xref></sup>, RTE2<sup><xref ref-type="bibr" rid="CR77">77</xref></sup>, RTE3<sup><xref ref-type="bibr" rid="CR78">78</xref></sup>, and RTE5<sup><xref ref-type="bibr" rid="CR79">79</xref></sup>. Constructed examples are derived from news and Wikipedia text. To maintain consistency, the benchmark creators transformed all datasets into a two-class split, collapsing neutral and contradiction into &#x0201c;not entailment&#x0201d; for three-class datasets. The publicly available version includes 2490 examples designated for training along with 277 for validation examples, while 3,000 examples are for testing.</p></list-item><list-item><p id="Par79"><bold>MRPC</bold> (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/glue/viewer/mrpc">https://huggingface.co/datasets/glue/viewer/mrpc</ext-link>): The Microsoft Research Paraphrase Corpus (MRPC)<sup><xref ref-type="bibr" rid="CR80">80</xref></sup> comprises 5801 pairs of sentences extracted from newswire articles. Human annotators have labeled each pair to indicate whether it is a paraphrase or not. The entire dataset is split into a training subset, consisting of 4076 sentence pairs (with 2753 identified as paraphrases), and a test subset, containing 1725 pairs (with 1147 recognized as paraphrases).</p></list-item><list-item><p id="Par81"><bold>SNLI</bold> (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/snli">https://huggingface.co/datasets/snli</ext-link>): The Stanford NLI (SNLI) corpus<sup><xref ref-type="bibr" rid="CR81">81</xref></sup> is an assemblage of 570,000 pairs of English sentences crafted by humans. These sentence pairs have been meticulously labeled to achieve balanced classification, with the labels entailment, contradiction, and neutral. This corpus is designed to facilitate the task of NLI. The publicly available version includes 550,152 examples designated for training along with 10,000 for validation examples, while 10,000 examples are for testing.</p></list-item></list>These datasets collectively cover a wide spectrum of NLP tasks, enabling comprehensive evaluations of SK-Tuning&#x02019;s performance across various domains and challenges.</p></sec></sec><sec id="Sec20"><title>Large language models</title><p id="Par83">In our analysis, we utilized multiple Large Language Models (LLMs) to obtain extensive and detailed results. Specifically, we employed Bloom 7b, Llama2 7b, Mistral 7b, Falcon 7b, and Phi-2 2.7b, each offering unique strengths and capabilities that complemented one another.<list list-type="bullet"><list-item><p id="Par84"><bold>Bloom:</bold> A 7B parameter LLM from BigScience, trained on an extensive corpus of text and code. Bloom displays robust performance on various NLP tasks and offers several variants, including Bloom Text-to-Text and Bloom Code<sup><xref ref-type="bibr" rid="CR82">82</xref></sup>.</p></list-item><list-item><p id="Par85"><bold>Llama2:</bold> Meta AI has introduced Llama 2, its most advanced LLM to date. Llama 2 showcases a diverse array of capabilities and potential applications, with model sizes ranging from 7 billion to 70 billion parameters. This release provides access to both model weights and initial code for pretrained and fine-tuned Llama models, including variants such as Llama Chat (specialized for dialogue) and Code Llama (optimized for programming tasks)<sup><xref ref-type="bibr" rid="CR83">83</xref></sup>.</p></list-item><list-item><p id="Par86"><bold>Mistral:</bold> Mistral 7B is a freely available, open-source language model comprising 7.3 billion parameters that demonstrates exceptional performance. Released in September 2023, it exhibits competitive results in comparison to Meta&#x02019;s LLaMA models, outperforming the 13B version on all benchmarks evaluated and equaling the 34B version on numerous metrics. Developed using the transformers architecture and accessible via BitTorrent and Hugging Face, Mistral 7B presents a robust and accessible option for researchers and developers seeking a high-performing LLM<sup><xref ref-type="bibr" rid="CR84">84</xref></sup>.</p></list-item><list-item><p id="Par87"><bold>Falcon:</bold> The Falcon Large Language Model (LLM) is a generative LLM designed to advance applications and use cases for future-proofing our world. Currently, the Falcon 180B, 40B, 7B, and 1.3B parameter artificial intelligence models, along with the high-quality REFINEDWEB dataset, constitute a comprehensive suite of offerings<sup><xref ref-type="bibr" rid="CR85">85</xref></sup>.</p></list-item><list-item><p id="Par88"><bold>phi-2:</bold> Phi-2, the most recent small language model (SLM) developed by Microsoft Research, is a 2.7 billion parameter model that showcases superior reasoning and language understanding capabilities compared to its predecessors, Phi-1 and Phi-1.5<sup><xref ref-type="bibr" rid="CR86">86</xref></sup>. The model was trained on a diverse dataset, comprising &#x0201c;textbook quality&#x0201d; web data and synthetic textbooks/exercises generated using GPT-3.5. Phi-2 exhibits exceptional performance in various tasks, including Python code generation<sup><xref ref-type="bibr" rid="CR87">87</xref></sup>. It is noteworthy that Phi-2 surpasses the performance of models up to 25 times larger in size. Furthermore, Phi-2 has been released under an MIT License, permitting its utilization in commercial applications.</p></list-item></list></p></sec><sec id="Sec21"><title>Baseline methods</title><p id="Par89">We established the following baseline methods to evaluate the performance of our proposed approach:<list list-type="bullet"><list-item><p id="Par90"><bold>Full fine-tuning:</bold> This methodology<sup><xref ref-type="bibr" rid="CR88">88</xref></sup> involves the adjustment of all parameters within the pretrained language model to adapt it to the specific task at hand. It functions as a comprehensive adaptation approach; however, it can be computationally intensive.</p></list-item><list-item><p id="Par91"><bold>Prefix tuning:</bold> This lightweight method<sup><xref ref-type="bibr" rid="CR89">89</xref></sup> introduces trainable continuous vectors termed &#x0201c;prefixes&#x0201d; to the input of each transformer layer, while the original model parameters remain fixed. Prefix-tuning is predicated on the concept of prompting in language models, enabling ensuing tokens to attend to this prefix as if it were composed of &#x0201c;virtual tokens&#x0201d;. It presents a more efficient alternative to complete fine-tuning, particularly in low-data scenarios.</p></list-item><list-item><p id="Par92"><bold>Prompt tuning:</bold> This method<sup><xref ref-type="bibr" rid="CR90">90</xref></sup> employs natural language prompts called &#x0201c;soft prompts&#x0201d; to guide the model&#x02019;s behavior without modifying its internal parameters. This provides a flexible method to adapt models to different tasks without additional training.</p></list-item><list-item><p id="Par93"><bold>P tuning:</bold> This method<sup><xref ref-type="bibr" rid="CR91">91</xref></sup> introduces an optimized prompt tuning method, which exhibits efficacy across a diverse spectrum of model scales and natural language tasks. The method addresses the suboptimal performance associated with prompt tuning when applied to pretrained models of typical size. Moreover, it endeavors to rectify the limitations observed in the performance of prompt tuning, particularly its inefficacy in challenging sequence labeling tasks.</p></list-item><list-item><p id="Par94"><bold>LoRA:</bold> LoRA<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that involves learning low-rank matrices to adapt the model while freezing most of its original parameters in a fixed state. This study investigated LoRA with rank 2 and rank 4 to evaluate its capability in optimizing the balance between performance and efficiency.</p></list-item></list>These baseline methods represent a diverse range of fine-tuning strategies, allowing us to measure the comparative performance of our proposed approach.</p></sec><sec id="Sec22"><title>Evaluation metrics</title><p id="Par95">Evaluation metrics measure the performance of a model on a specific dataset by comparing the model&#x02019;s predictions with ground truth labels. Various tasks have specific metrics, and we used accuracy and F1 score in our experiments.</p><sec id="Sec23"><title>Accuracy</title><p id="Par96">Accuracy is a metric that measures the overall correctness of a model&#x02019;s predictions. It is calculated as the ratio of correct predictions to the total number of predictions made by the model:<disp-formula id="Equ14"><alternatives><tex-math id="M299">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Accuracy} = \frac{\text {True Positives (TP) + True Negatives (TN)}}{\text {Total Predictions}} \end{aligned}$$\end{document}</tex-math><mml:math id="M300" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>True Positives (TP) + True Negatives (TN)</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Total Predictions</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec24"><title>F1 score</title><p id="Par97">The F1 score is the harmonic mean of precision and recall, providing a balanced measure considering both false positives and false negatives:<disp-formula id="Equ15"><alternatives><tex-math id="M301">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {F1 Score} = \frac{2 \times \text {Precision} \times \text {Recall}}{\text {Precision + Recall}} \end{aligned}$$\end{document}</tex-math><mml:math id="M302" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>F1 Score</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mtext>Precision</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mtext>Recall</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Precision + Recall</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_75599_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>The F1 score ranges from 0 to 1, with higher values indicating better overall performance in terms of precision and recall.</p><p id="Par98">In these formulas, TP, TN, FP, and FN represent the counts of true positives, true negatives, false positives, and false negatives, respectively.</p></sec></sec><sec id="Sec25"><title>Hyperparameters setting</title><p id="Par99">In our experiments, we carefully selected hyperparameters to ensure consistent and effective training across various datasets and tasks.</p><p id="Par100">For the maximum sequence length, we set it to 128 for all datasets except for RTE, where we did not impose a maximum sequence length.</p><p id="Par101">Regarding learning rates, we employed the following values:<list list-type="bullet"><list-item><p id="Par102">For the sequence classification datasets, the learning rate was set to <inline-formula id="IEq137"><alternatives><tex-math id="M303">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 10^{-3}$$\end{document}</tex-math><mml:math id="M304"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq137.gif"/></alternatives></inline-formula>.</p></list-item><list-item><p id="Par103">For the token classification datasets, a learning rate of <inline-formula id="IEq138"><alternatives><tex-math id="M305">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 10^{-5}$$\end{document}</tex-math><mml:math id="M306"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq138.gif"/></alternatives></inline-formula> was used.</p></list-item><list-item><p id="Par104">For the NLI datasets, the learning rate was set to <inline-formula id="IEq139"><alternatives><tex-math id="M307">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 10^{-4}$$\end{document}</tex-math><mml:math id="M308"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_75599_Article_IEq139.gif"/></alternatives></inline-formula>.</p></list-item></list>In terms of the number of training epochs:<list list-type="bullet"><list-item><p id="Par105">Sequence classification datasets were trained for 5 epochs.</p></list-item><list-item><p id="Par106">Token classification datasets were trained for 10 epochs.</p></list-item><list-item><p id="Par107">NLI datasets were trained for 10 epochs, with the exception of the SNLI dataset, which was trained for 2 epochs on each model.</p></list-item></list>For all our datasets, regardless of the task or tuning method (P-Tuning, Prefix Tuning, or Prompt Tuning), we consistently used 20 virtual tokens during training.</p><p id="Par108">We employ the Adaptive Moment Estimation with Weight Decay (ADAMW)<sup><xref ref-type="bibr" rid="CR92">92</xref></sup> optimizer for all experiments. The ADAMW optimizer is an improved version of the traditional ADAM optimizer, which incorporates weight decay directly into the optimization process to better handle regularization. Additionally, we set the weight decay value to 0.01 across all experiments to control regularization and ensure stable model training.</p></sec><sec id="Sec26"><title>Result analysis</title><p id="Par109">See Tables <xref rid="Tab2" ref-type="table">2</xref>, <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref>, <xref rid="Tab5" ref-type="table">5</xref> and <xref rid="Tab6" ref-type="table">6</xref>.</p><p>
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Sequence classification results for the bloom model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">Fake News Filipino</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">95.02</td><td align="left">93.83</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03493</td><td align="left">70.99</td><td align="left">68.18</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00701</td><td align="left">74.31</td><td align="left">72.23</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01582</td><td align="left">72.97</td><td align="left">70.19</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01413</td><td align="left">90.13</td><td align="left">88.87</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.05794</td><td align="left"><bold>93</bold>.<bold>56</bold></td><td align="left">
<italic>90.05</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00035</italic>
</td><td align="left">
<italic>92.86</italic>
</td><td align="left"><bold>90</bold>.<bold>63</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00016</bold></td><td align="left">91.03</td><td align="left">89.13</td></tr><tr><td align="left" rowspan="8">Emotion</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">90.31</td><td align="left">87.52</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03521</td><td align="left">74.75</td><td align="left">68.11</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00813</td><td align="left">79.12</td><td align="left">71.07</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01593</td><td align="left">69.45</td><td align="left">70.23</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.02413</td><td align="left">86.76</td><td align="left">80.23</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.06831</td><td align="left">
<italic>87.52</italic>
</td><td align="left">82.01</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00161</italic>
</td><td align="left"><bold>88</bold>.<bold>21</bold></td><td align="left"><bold>82</bold>.<bold>64</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00104</bold></td><td align="left">86.82</td><td align="left">
<italic>82.14</italic>
</td></tr><tr><td align="left" rowspan="8">SST2</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">97.93</td><td align="left">97.81</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03493</td><td align="left">85.78</td><td align="left">86.31</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00715</td><td align="left">92.45</td><td align="left">92.78</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01653</td><td align="left">91.34</td><td align="left">91.75</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01456</td><td align="left">92.27</td><td align="left">92.77</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.02831</td><td align="left">94.36</td><td align="left">94.83</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00082</italic>
</td><td align="left"><bold>96</bold>.<bold>85</bold></td><td align="left"><bold>96</bold>.<bold>65</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00034</bold></td><td align="left">
<italic>96.53</italic>
</td><td align="left">
<italic>96.19</italic>
</td></tr><tr><td align="left" rowspan="8">Cola</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">87.05</td><td align="left">89.93</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03495</td><td align="left">73.72</td><td align="left">83.69</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00723</td><td align="left">82.74</td><td align="left"><bold>87</bold>.<bold>70</bold></td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01615</td><td align="left">70.32</td><td align="left">81.12</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01415</td><td align="left">81.13</td><td align="left">83.03</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.02797</td><td align="left">84.33</td><td align="left">85.21</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00083</italic>
</td><td align="left"><bold>84</bold>.<bold>91</bold></td><td align="left">
<italic>86.01</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00052</bold></td><td align="left">
<italic>84.51</italic>
</td><td align="left">85.92</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Sequence classification results for the Llama2 model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">Fake News Filipino</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">95.22</td><td align="left">93.90</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03983</td><td align="left">70.06</td><td align="left">68.57</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00743</td><td align="left">73.72</td><td align="left">72.07</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01731</td><td align="left">71.54</td><td align="left">70.63</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01601</td><td align="left">90.38</td><td align="left">87.62</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.03213</td><td align="left">
<italic>92.14</italic>
</td><td align="left"><bold>90</bold>.<bold>86</bold></td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left"><bold>0</bold>.<bold>00024</bold></td><td align="left"><bold>92</bold>.<bold>26</bold></td><td align="left">
<italic>89.90</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left">
<italic>0.00035</italic>
</td><td align="left">90.83</td><td align="left">88.23</td></tr><tr><td align="left" rowspan="8">Emotion</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">91.11</td><td align="left">87.92</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03994</td><td align="left">84.31</td><td align="left">82.78</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00864</td><td align="left">85.37</td><td align="left">82.50</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01781</td><td align="left">83.05</td><td align="left">81.88</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01624</td><td align="left">86.49</td><td align="left">82.86</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.03233</td><td align="left">
<italic>88.56</italic>
</td><td align="left"><bold>84</bold>.<bold>18</bold></td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00174</italic>
</td><td align="left"><bold>88</bold>.<bold>72</bold></td><td align="left">
<italic>83.51</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00122</bold></td><td align="left">85.92</td><td align="left">82.87</td></tr><tr><td align="left" rowspan="8">SST2</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">97.32</td><td align="left">97.69</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.04855</td><td align="left">85.78</td><td align="left">86.31</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00712</td><td align="left">94.24</td><td align="left"><bold>97</bold>.<bold>26</bold></td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01753</td><td align="left">95.55</td><td align="left">
<italic>96.62</italic>
</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01607</td><td align="left">86.97</td><td align="left">81.93</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.03191</td><td align="left">87.11</td><td align="left">82.03</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00088</italic>
</td><td align="left"><bold>96</bold>.<bold>52</bold></td><td align="left">96.48</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00037</bold></td><td align="left">
<italic>96.50</italic>
</td><td align="left">96.35</td></tr><tr><td align="left" rowspan="8">Cola</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">88.22</td><td align="left">89.64</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03984</td><td align="left">71.18</td><td align="left">83.29</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00757</td><td align="left">73.27</td><td align="left">85.26</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01751</td><td align="left">69.12</td><td align="left">81.74</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01603</td><td align="left">82.25</td><td align="left">83.43</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.03213</td><td align="left">84.18</td><td align="left">83.88</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00092</italic>
</td><td align="left"><bold>85</bold>.<bold>01</bold></td><td align="left"><bold>86</bold>.<bold>22</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00060</bold></td><td align="left">
<italic>84.36</italic>
</td><td align="left">
<italic>85.85</italic>
</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Sequence classification results for the Falcon model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">Fake News Filipino</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">94.05</td><td align="left">92.93</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03821</td><td align="left">69.57</td><td align="left">68.19</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00732</td><td align="left">72.35</td><td align="left">70.78</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01797</td><td align="left">70.23</td><td align="left">69.15</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.00972</td><td align="left">88.31</td><td align="left">85.14</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.05784</td><td align="left"><bold>91</bold>.<bold>89</bold></td><td align="left"><bold>89</bold>.<bold>44</bold></td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00075</italic>
</td><td align="left">90.11</td><td align="left">
<italic>88.93</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00029</bold></td><td align="left">
<italic>90.21</italic>
</td><td align="left">87.23</td></tr><tr><td align="left" rowspan="8">Emotion</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">88.53</td><td align="left">85.94</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03836</td><td align="left">81.14</td><td align="left">80.61</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00841</td><td align="left">
<italic>87.25</italic>
</td><td align="left">
<italic>84.19</italic>
</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01803</td><td align="left">81.76</td><td align="left">79.14</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01194</td><td align="left">84.17</td><td align="left">82.34</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.05781</td><td align="left"><bold>88</bold>.<bold>79</bold></td><td align="left"><bold>86</bold>.<bold>13</bold></td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00204</italic>
</td><td align="left">86.91</td><td align="left">82.11</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00113</bold></td><td align="left">86.29</td><td align="left">82.03</td></tr><tr><td align="left" rowspan="8">SST2</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">96.23</td><td align="left">95.76</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03818</td><td align="left">90.18</td><td align="left">91.36</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00605</td><td align="left">93.56</td><td align="left">93.75</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01781</td><td align="left">90.33</td><td align="left">91.26</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01193</td><td align="left">91.13</td><td align="left">92.07</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.05789</td><td align="left">91.72</td><td align="left">92.17</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00093</italic>
</td><td align="left">
<italic>94.73</italic>
</td><td align="left">
<italic>94.02</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00038</bold></td><td align="left"><bold>95</bold>.<bold>02</bold></td><td align="left"><bold>95</bold>.<bold>01</bold></td></tr><tr><td align="left" rowspan="8">Cola</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">85.22</td><td align="left">87.39</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03826</td><td align="left">70.03</td><td align="left">82.23</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00711</td><td align="left">71.45</td><td align="left">84.47</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01792</td><td align="left">68.07</td><td align="left">81.73</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.00973</td><td align="left">82.14</td><td align="left">82.38</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.05741</td><td align="left"><bold>84</bold>.<bold>66</bold></td><td align="left"><bold>85</bold>.<bold>33</bold></td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00094</italic>
</td><td align="left">83.74</td><td align="left">85.03</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00065</bold></td><td align="left">
<italic>84.01</italic>
</td><td align="left">
<italic>85.11</italic>
</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Sequence classification results for the Mistral model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">Fake News Filipino</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">97.92</td><td align="left">94.72</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03651</td><td align="left">71.26</td><td align="left">70.91</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00169</td><td align="left">74.12</td><td align="left">72.27</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01753</td><td align="left">71.37</td><td align="left">71.95</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.07502</td><td align="left">91.28</td><td align="left">
<italic>90.05</italic>
</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.17129</td><td align="left">92.19</td><td align="left">89.18</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00019</italic>
</td><td align="left"><bold>94</bold>.<bold>06</bold></td><td align="left"><bold>91</bold>.<bold>92</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00027</bold></td><td align="left">
<italic>92.44</italic>
</td><td align="left">90.02</td></tr><tr><td align="left" rowspan="8">Emotion</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">93.53</td><td align="left">89.09</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03683</td><td align="left">82.19</td><td align="left">79.24</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00736</td><td align="left">86.17</td><td align="left">81.77</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01783</td><td align="left">83.14</td><td align="left">80.01</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01539</td><td align="left">84.37</td><td align="left">80.08</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.01731</td><td align="left">88.45</td><td align="left">
<italic>84.23</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00162</italic>
</td><td align="left">
<italic>88.72</italic>
</td><td align="left">82.51</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00115</bold></td><td align="left"><bold>89</bold>.<bold>13</bold></td><td align="left"><bold>84</bold>.<bold>94</bold></td></tr><tr><td align="left" rowspan="8">SST2</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">98.09</td><td align="left">98.98</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03673</td><td align="left">91.20</td><td align="left">92.28</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00618</td><td align="left">93.14</td><td align="left">93.47</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01764</td><td align="left">90.76</td><td align="left">91.15</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01512</td><td align="left">92.65</td><td align="left">93.03</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.01726</td><td align="left">94.53</td><td align="left">94.67</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00080</italic>
</td><td align="left"><bold>96</bold>.<bold>97</bold></td><td align="left">
<italic>97.07</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00031</bold></td><td align="left">
<italic>96.93</italic>
</td><td align="left"><bold>97</bold>.<bold>14</bold></td></tr><tr><td align="left" rowspan="8">Cola</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">87.75</td><td align="left">89.90</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03652</td><td align="left">72.21</td><td align="left">80.43</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00639</td><td align="left">74.13</td><td align="left">81.66</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01754</td><td align="left">71.23</td><td align="left">79.76</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01505</td><td align="left">83.44</td><td align="left">84.65</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.01712</td><td align="left">
<italic>85.32</italic>
</td><td align="left">86.04</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00082</italic>
</td><td align="left"><bold>85</bold>.<bold>92</bold></td><td align="left">
<italic>86.22</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00046</bold></td><td align="left">85.01</td><td align="left"><bold>86</bold>.<bold>36</bold></td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Sequence classification results for the Phi-2 model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">Fake News Filipino</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">92.43</td><td align="left">90.71</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.83914</td><td align="left">66.28</td><td align="left">66.31</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.14124</td><td align="left">68.15</td><td align="left">67.22</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.15824</td><td align="left">67.33</td><td align="left">66.87</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.13741</td><td align="left">83.35</td><td align="left">81.46</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.71651</td><td align="left">86.67</td><td align="left">84.29</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left"><bold>0</bold>.<bold>04263</bold></td><td align="left"><bold>89</bold>.<bold>36</bold></td><td align="left"><bold>88</bold>.<bold>63</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left">
<italic>0.04924</italic>
</td><td align="left">
<italic>88.64</italic>
</td><td align="left">
<italic>87.11</italic>
</td></tr><tr><td align="left" rowspan="8">Emotion</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">87.95</td><td align="left">84.78</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.86523</td><td align="left">77.27</td><td align="left">76.25</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.14234</td><td align="left">82.16</td><td align="left">80.43</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.15845</td><td align="left">77.36</td><td align="left">75.81</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.13748</td><td align="left">82.67</td><td align="left">80.25</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.71656</td><td align="left">
<italic>85.03</italic>
</td><td align="left">
<italic>82.66</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">0.06271</td><td align="left">82.63</td><td align="left">80.64</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>02423</bold></td><td align="left"><bold>85</bold>.<bold>82</bold></td><td align="left"><bold>83</bold>.<bold>14</bold></td></tr><tr><td align="left" rowspan="8">SST2</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">94.63</td><td align="left">94.24</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.83721</td><td align="left">86.24</td><td align="left">87.13</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.14231</td><td align="left">88.19</td><td align="left">88.04</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.15851</td><td align="left">85.43</td><td align="left">87.68</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.13753</td><td align="left">86.21</td><td align="left">87.18</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.71668</td><td align="left">86.75</td><td align="left">88.28</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.02745</italic>
</td><td align="left"><bold>96</bold>.<bold>85</bold></td><td align="left"><bold>96</bold>.<bold>65</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>01479</bold></td><td align="left">
<italic>96.53</italic>
</td><td align="left">
<italic>96.19</italic>
</td></tr><tr><td align="left" rowspan="8">Cola</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">84.23</td><td align="left">85.13</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.82621</td><td align="left">66.24</td><td align="left">70.16</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.14123</td><td align="left">67.47</td><td align="left">70.81</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.15833</td><td align="left">64.36</td><td align="left">68.38</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.13744</td><td align="left">78.55</td><td align="left">80.26</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.71654</td><td align="left">80.39</td><td align="left">
<italic>82.43</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left"><bold>0</bold>.<bold>03675</bold></td><td align="left">
<italic>80.91</italic>
</td><td align="left">82.01</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left">
<italic>0.05143</italic>
</td><td align="left"><bold>81</bold>.<bold>31</bold></td><td align="left"><bold>82</bold>.<bold>64</bold></td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p></sec><sec id="Sec27"><title>Sequence classification</title><p id="Par110">In the realm of classification, we conducted a comprehensive evaluation across various LLMs, including Bloom, Llama2, Falcon, Mistral, and Phi-2, employing different fine-tuning techniques. For each model, we examined the effectiveness of traditional approaches such as Finetuning, Prefix Tuning, Prompt Tuning, PTuning, Lora Rank 2, and Lora Rank 4, and compared them to our proposed SK-Tuning methods, both for Prefix and Prompt. Notably, SK-Tuning consistently outperforms traditional methods across different datasets, showcasing its superior efficiency and effectiveness. The performances of various models on different datasets are documented in Tables&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>,&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>,&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>,&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>, and&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>.</p><p id="Par111">Across the &#x0201c;Fake News Filipino&#x0201d; dataset, SK-Tuning, especially when applied as SK-Tuning (Prefix), demonstrates remarkable performance improvements compared to traditional approaches. It achieves the highest accuracy and F1-score, emphasizing its capability to efficiently adapt LLMs to specific tasks while minimizing trainable parameters. In the &#x0201c;Emotion&#x0201d; dataset, SK-Tuning consistently outperforms other methods, indicating its robustness across different classification tasks. The same trend is observed in the &#x0201c;SST2&#x0201d; dataset, where SK-Tuning invariably achieves superior results. Lastly, in the &#x0201c;Cola&#x0201d; dataset, SK-Tuning (Prefix) and SK-Tuning (Prompt) perpetually outperform other approaches, underscoring their potential for enhancing sequence classification tasks.</p><p id="Par112">Comparatively, traditional methods like Prefix Tuning and Prompt Tuning, although efficient in terms of parameters compared to Fine-tuning, tend to lag behind SK-Tuning in terms of accuracy and F1-score. Furthermore, SK-Tuning requires fewer trainable parameters, making it an attractive choice for practitioners aiming to optimize performance while maintaining efficiency.</p></sec><sec id="Sec28"><title>Token classification</title><p id="Par113">In this comprehensive analysis of token classification across various datasets, we conducted an extensive evaluation of five distinct models: Bloom, Llama2, Falcon, Mistral, and Phi-2, while exploring a range of fine-tuning techniques to understand their impact on performance, documented in Tables&#x000a0;<xref rid="Tab7" ref-type="table">7</xref>,&#x000a0;<xref rid="Tab8" ref-type="table">8</xref>,&#x000a0;<xref rid="Tab9" ref-type="table">9</xref>,&#x000a0;<xref rid="Tab10" ref-type="table">10</xref> and&#x000a0;<xref rid="Tab11" ref-type="table">11</xref>. The datasets used for evaluation included conll03, ncbi disease, and wiki ann, each representing different challenges in token classification.</p><p>
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Token classification results for the Bloom model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">conll03</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">98.53</td><td align="left">82.47</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03534</td><td align="left">83.55</td><td align="left">24.86</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00843</td><td align="left">85.23</td><td align="left">28.73</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01583</td><td align="left">83.22</td><td align="left">26.34</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01403</td><td align="left">91.12</td><td align="left">68.24</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.06795</td><td align="left">93.23</td><td align="left">71.33</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00071</italic>
</td><td align="left">
<italic>94.08</italic>
</td><td align="left">
<italic>71.59</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00052</bold></td><td align="left"><bold>94</bold>.<bold>11</bold></td><td align="left"><bold>71</bold>.<bold>60</bold></td></tr><tr><td align="left" rowspan="8">NCBI disease</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">98.53</td><td align="left">92.46</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03492</td><td align="left">89.09</td><td align="left">60.06</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00742</td><td align="left">91.17</td><td align="left">75.34</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01572</td><td align="left">90.22</td><td align="left">81.23</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01417</td><td align="left">92.86</td><td align="left">80.00</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.06797</td><td align="left">
<italic>96.12</italic>
</td><td align="left">
<italic>83.49</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00093</italic>
</td><td align="left"><bold>96</bold>.<bold>17</bold></td><td align="left"><bold>84</bold>.<bold>85</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00068</bold></td><td align="left">95.32</td><td align="left">82.18</td></tr><tr><td align="left" rowspan="8">WikiAnn</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">90.50</td><td align="left">60.14</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03527</td><td align="left">71.67</td><td align="left">22.18</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00732</td><td align="left">76.23</td><td align="left">31.78</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01577</td><td align="left">70.65</td><td align="left">24.33</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01408</td><td align="left">82.23</td><td align="left">41.23</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.06791</td><td align="left"><bold>85</bold>.<bold>13</bold></td><td align="left"><bold>45</bold>.<bold>14</bold></td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00083</italic>
</td><td align="left">
<italic>83.19</italic>
</td><td align="left">
<italic>42.13</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00044</bold></td><td align="left">82.59</td><td align="left">42.01</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab8"><label>Table 8</label><caption><p>Token classification results for the Llama2 model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">conll03</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">98.75</td><td align="left">80.77</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03964</td><td align="left">82.28</td><td align="left">66.56</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00638</td><td align="left">86.65</td><td align="left">69.91</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01731</td><td align="left">80.11</td><td align="left">65.11</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01426</td><td align="left">88.67</td><td align="left">63.34</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.07122</td><td align="left">91.32</td><td align="left">69.03</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left"><bold>0</bold>.<bold>00044</bold></td><td align="left"><bold>93</bold>.<bold>63</bold></td><td align="left"><bold>70</bold>.<bold>83</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left">
<italic>0.00063</italic>
</td><td align="left">
<italic>93.02</italic>
</td><td align="left">
<italic>70.19</italic>
</td></tr><tr><td align="left" rowspan="8">NCBI disease</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">98.32</td><td align="left">93.38</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03976</td><td align="left">88.23</td><td align="left">68.23</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00712</td><td align="left">91.22</td><td align="left">78.24</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01733</td><td align="left">90.15</td><td align="left">77.23</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01424</td><td align="left">92.48</td><td align="left">80.18</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.07125</td><td align="left">95.34</td><td align="left">82.87</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00083</italic>
</td><td align="left">
<italic>96.22</italic>
</td><td align="left">
<italic>84.73</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00061</bold></td><td align="left"><bold>96</bold>.<bold>28</bold></td><td align="left"><bold>84</bold>.<bold>89</bold></td></tr><tr><td align="left" rowspan="8">WikiAnn</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">91.49</td><td align="left">63.21</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03986</td><td align="left">81.15</td><td align="left">35.17</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00712</td><td align="left">83.23</td><td align="left">44.19</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01743</td><td align="left">81.29</td><td align="left">38.11</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01434</td><td align="left">84.82</td><td align="left">47.90</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.07125</td><td align="left">86.56</td><td align="left">49.39</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00082</italic>
</td><td align="left"><bold>86</bold>.<bold>99</bold></td><td align="left"><bold>50</bold>.<bold>61</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00052</bold></td><td align="left">
<italic>86.69</italic>
</td><td align="left">
<italic>49.58</italic>
</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab9"><label>Table 9</label><caption><p>Token classification results for the Falcon model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">conll03</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">97.82</td><td align="left">79.03</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03772</td><td align="left">90.57</td><td align="left">67.62</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00832</td><td align="left">91.26</td><td align="left">70.15</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01762</td><td align="left">89.23</td><td align="left">66.02</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01942</td><td align="left">90.21</td><td align="left">68.96</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.09752</td><td align="left">93.25</td><td align="left">71.19</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00071</italic>
</td><td align="left">
<italic>94.21</italic>
</td><td align="left">
<italic>71.73</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00055</bold></td><td align="left"><bold>94</bold>.<bold>82</bold></td><td align="left"><bold>72</bold>.<bold>02</bold></td></tr><tr><td align="left" rowspan="8">NCBI disease</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">97.93</td><td align="left">90.88</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03763</td><td align="left">89.23</td><td align="left">69.33</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00721</td><td align="left">92.05</td><td align="left">82.28</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01752</td><td align="left">88.15</td><td align="left">70.36</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01936</td><td align="left">90.55</td><td align="left">80.25</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.09754</td><td align="left">94.41</td><td align="left">
<italic>83.19</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00085</italic>
</td><td align="left">
<italic>95.83</italic>
</td><td align="left">82.18</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00056</bold></td><td align="left"><bold>96</bold>.<bold>01</bold></td><td align="left"><bold>84</bold>.<bold>48</bold></td></tr><tr><td align="left" rowspan="8">WikiAnn</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">89.23</td><td align="left">62.09</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03772</td><td align="left">82.67</td><td align="left">36.55</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00836</td><td align="left">83.33</td><td align="left">
<italic>43.32</italic>
</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01768</td><td align="left">81.14</td><td align="left">35.21</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01983</td><td align="left">80.47</td><td align="left">41.58</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.09752</td><td align="left">86.61</td><td align="left"><bold>48</bold>.<bold>03</bold></td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00063</italic>
</td><td align="left"><bold>82</bold>.<bold>99</bold></td><td align="left">42.51</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00044</bold></td><td align="left">
<italic>82.96</italic>
</td><td align="left">42.49</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab10"><label>Table 10</label><caption><p>Token classification results for the Mistral model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">conll03</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">98.89</td><td align="left">84.60</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03634</td><td align="left">83.31</td><td align="left">58.54</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00741</td><td align="left">87.77</td><td align="left">62.19</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01743</td><td align="left">81.15</td><td align="left">67.59</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01494</td><td align="left">88.32</td><td align="left">68.14</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.08694</td><td align="left">92.05</td><td align="left">70.66</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00062</italic>
</td><td align="left">
<italic>95.29</italic>
</td><td align="left">
<italic>72.70</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00044</bold></td><td align="left"><bold>95</bold>.<bold>89</bold></td><td align="left"><bold>72</bold>.<bold>03</bold></td></tr><tr><td align="left" rowspan="8">NCBI disease</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">98.52</td><td align="left">93.39</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03627</td><td align="left">88.49</td><td align="left">74.25</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00696</td><td align="left">92.03</td><td align="left">80.11</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01735</td><td align="left">87.13</td><td align="left">63.29</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01483</td><td align="left">94.58</td><td align="left">82.37</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.08698</td><td align="left">96.88</td><td align="left">83.15</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00081</italic>
</td><td align="left">
<italic>96.98</italic>
</td><td align="left">
<italic>85.06</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00053</bold></td><td align="left"><bold>97</bold>.<bold>01</bold></td><td align="left"><bold>85</bold>.<bold>15</bold></td></tr><tr><td align="left" rowspan="8">WikiAnn</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">92.15</td><td align="left">63.09</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03633</td><td align="left">81.91</td><td align="left">36.03</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00752</td><td align="left">84.48</td><td align="left">45.31</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01733</td><td align="left">81.04</td><td align="left">35.02</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01495</td><td align="left">82.08</td><td align="left">42.22</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.08692</td><td align="left">85.33</td><td align="left">
<italic>45.95</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00095</italic>
</td><td align="left"><bold>86</bold>.<bold>73</bold></td><td align="left"><bold>46</bold>.<bold>21</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00052</bold></td><td align="left">
<italic>85.99</italic>
</td><td align="left">45.52</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab11"><label>Table 11</label><caption><p>Token classification results for the Phi-2 model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">conll03</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">98.13</td><td align="left">79.02</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.83844</td><td align="left">78.27</td><td align="left">56.63</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.14124</td><td align="left">80.38</td><td align="left">58.27</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.15814</td><td align="left">76.54</td><td align="left">56.07</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.13746</td><td align="left">84.44</td><td align="left">62.15</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.71649</td><td align="left">86.56</td><td align="left">65.43</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00951</italic>
</td><td align="left">
<italic>90.42</italic>
</td><td align="left">
<italic>71.73</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00838</bold></td><td align="left"><bold>91</bold>.<bold>15</bold></td><td align="left"><bold>71</bold>.<bold>98</bold></td></tr><tr><td align="left" rowspan="8">NCBI disease</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">95.82</td><td align="left">91.19</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.83823</td><td align="left">82.42</td><td align="left">63.38</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.13939</td><td align="left">85.61</td><td align="left">65.44</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.15794</td><td align="left">81.17</td><td align="left">67.63</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.13748</td><td align="left">86.23</td><td align="left">78.45</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.71493</td><td align="left">87.34</td><td align="left">78.26</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00994</italic>
</td><td align="left">
<italic>89.22</italic>
</td><td align="left">
<italic>80.63</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00438</bold></td><td align="left"><bold>90</bold>.<bold>82</bold></td><td align="left"><bold>81</bold>.<bold>95</bold></td></tr><tr><td align="left" rowspan="8">WikiAnn</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">88.92</td><td align="left">58.21</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.83832</td><td align="left">74.37</td><td align="left">31.57</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.01416</td><td align="left">78.86</td><td align="left">38.32</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.15812</td><td align="left">75.23</td><td align="left">32.26</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.13748</td><td align="left">79.04</td><td align="left">39.88</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.71649</td><td align="left">81.53</td><td align="left"><bold>44</bold>.<bold>47</bold></td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00851</italic>
</td><td align="left">
<italic>82.03</italic>
</td><td align="left">42.93</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00693</bold></td><td align="left"><bold>83</bold>.<bold>18</bold></td><td align="left">
<italic>43.06</italic>
</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p id="Par114">First and foremost, we observed that Full Fine-tuning consistently achieved high accuracy across all models and datasets. However, it also required a substantial percentage of parameters, potentially making it less feasible for resource-constrained environments.</p><p id="Par115">To address the trade-off between model efficiency and performance, we investigated several fine-tuning techniques. Prefix Tuning, Prompt Tuning, and P-Tuning, which involve introducing a small fraction of parameters, showcased mixed results. While these techniques achieved decent accuracy in some cases, they often lagged behind in terms of F1-score, indicating challenges in maintaining a balance between precision and recall.</p><p id="Par116">Remarkably, Lora Rank 2 and Lora Rank 4, with a moderate percentage of parameters, consistently delivered a strong performance, especially in terms of the F1-score. These results underscore the importance of considering the architecture of the model when optimizing for token classification tasks, with Lora Rank models demonstrating their effectiveness.</p><p id="Par117">Finally, SK-Tuning techniques, both Prefix and Prompt variants, stood out as noteworthy approaches. They required an extremely minimal percentage of additional parameters yet yielded competitive accuracy and remarkable F1 scores. This suggests that these techniques have the potential to strike a favorable balance between model efficiency and task effectiveness.</p></sec><sec id="Sec29"><title>Entailment detection</title><p id="Par118">The results of entailment detection using various models, including Bloom, Llama2, Falcon, Mistral, and Phi-2, are presented in Tables&#x000a0;<xref rid="Tab12" ref-type="table">12</xref>,&#x000a0;<xref rid="Tab13" ref-type="table">13</xref>,&#x000a0;<xref rid="Tab14" ref-type="table">14</xref>,&#x000a0;<xref rid="Tab15" ref-type="table">15</xref> and&#x000a0;<xref rid="Tab16" ref-type="table">16</xref> . Across all three datasets (RTE, MRPC, SNLI), full fine-tuning consistently achieves the highest accuracy and F1-score, with Bloom and Mistral models demonstrating remarkable results. This underscores the value of fine-tuning the entire model&#x02019;s parameters to adapt to specific entailment tasks, as it allows the model to capture intricate patterns and nuances in the data.</p><p>
<table-wrap id="Tab12"><label>Table 12</label><caption><p>Entailment classification results for the Bloom model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">RTE</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">92.31</td><td align="left">87.19</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03493</td><td align="left">70.03</td><td align="left">64.06</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00714</td><td align="left">65.34</td><td align="left">62.20</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01584</td><td align="left">71.11</td><td align="left">69.23</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01402</td><td align="left">80.25</td><td align="left">80.01</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.05804</td><td align="left">
<italic>84.45</italic>
</td><td align="left">
<italic>83.26</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00076</italic>
</td><td align="left">83.88</td><td align="left">82.76</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00053</bold></td><td align="left"><bold>84</bold>.<bold>92</bold></td><td align="left"><bold>83</bold>.<bold>87</bold></td></tr><tr><td align="left" rowspan="8">MRPC</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">90.01</td><td align="left">91.13</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03494</td><td align="left">73.56</td><td align="left">81.70</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00773</td><td align="left">81.39</td><td align="left">86.01</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01562</td><td align="left">78.08</td><td align="left">84.38</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01393</td><td align="left">80.21</td><td align="left">82.29</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.05799</td><td align="left">83.88</td><td align="left">84.84</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00082</italic>
</td><td align="left">
<italic>88.99</italic>
</td><td align="left">
<italic>86.28</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00054</bold></td><td align="left"><bold>89</bold>.<bold>03</bold></td><td align="left"><bold>86</bold>.<bold>37</bold></td></tr><tr><td align="left" rowspan="8">SNLI</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">95.62</td><td align="left">95.78</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03492</td><td align="left">87.32</td><td align="left">87.26</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00803</td><td align="left">88.88</td><td align="left">88.87</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01594</td><td align="left">86.22</td><td align="left">86.54</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01412</td><td align="left">91.37</td><td align="left">91.36</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.05813</td><td align="left">
<italic>93.23</italic>
</td><td align="left">
<italic>93.68</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00085</italic>
</td><td align="left">92.54</td><td align="left">92.98</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00060</bold></td><td align="left"><bold>93</bold>.<bold>75</bold></td><td align="left"><bold>94</bold>.<bold>02</bold></td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab13"><label>Table 13</label><caption><p>Entailment classification results for the Llama2 model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">RTE</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">93.51</td><td align="left">88.92</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03982</td><td align="left">70.15</td><td align="left">65.23</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00737</td><td align="left">62.81</td><td align="left">66.00</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01753</td><td align="left">67.24</td><td align="left">66.21</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01612</td><td align="left">81.04</td><td align="left">80.67</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.03224</td><td align="left">
<italic>83.43</italic>
</td><td align="left">81.44</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00077</italic>
</td><td align="left"><bold>85</bold>.<bold>73</bold></td><td align="left"><bold>84</bold>.<bold>01</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00052</bold></td><td align="left">83.72</td><td align="left">
<italic>83.43</italic>
</td></tr><tr><td align="left" rowspan="8">MRPC</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">92.25</td><td align="left">92.95</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03973</td><td align="left">79.41</td><td align="left">80.01</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00724</td><td align="left">80.18</td><td align="left">80.37</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01745</td><td align="left">74.56</td><td align="left">82.67</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01601</td><td align="left">80.48</td><td align="left">82.02</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.03218</td><td align="left">81.89</td><td align="left">83.11</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00082</italic>
</td><td align="left"><bold>85</bold>.<bold>97</bold></td><td align="left"><bold>86</bold>.<bold>37</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00051</bold></td><td align="left">
<italic>85.03</italic>
</td><td align="left">
<italic>85.37</italic>
</td></tr><tr><td align="left" rowspan="8">SNLI</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">93.31</td><td align="left">94.03</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03986</td><td align="left">86.34</td><td align="left">86.33</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00736</td><td align="left">87.02</td><td align="left">87.41</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01752</td><td align="left">85.17</td><td align="left">86.27</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01613</td><td align="left">90.21</td><td align="left">90.87</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.03228</td><td align="left">
<italic>91.15</italic>
</td><td align="left">
<italic>91.85</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00095</italic>
</td><td align="left"><bold>91</bold>.<bold>43</bold></td><td align="left"><bold>91</bold>.<bold>98</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00069</bold></td><td align="left">90.97</td><td align="left">91.04</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab14"><label>Table 14</label><caption><p>Entailment classification results for the Falcon model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">RTE</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">93.22</td><td align="left">87.67</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03822</td><td align="left">64.23</td><td align="left">63.38</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00813</td><td align="left">66.51</td><td align="left">66.02</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01794</td><td align="left">53.42</td><td align="left">53.09</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01138</td><td align="left">73.28</td><td align="left">70.15</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.01774</td><td align="left">78.33</td><td align="left">73.42</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00084</italic>
</td><td align="left">
<italic>80.12</italic>
</td><td align="left">
<italic>79.73</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00065</bold></td><td align="left"><bold>80</bold>.<bold>25</bold></td><td align="left"><bold>79</bold>.<bold>78</bold></td></tr><tr><td align="left" rowspan="8">MRPC</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">90.21</td><td align="left">90.83</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03813</td><td align="left">74.13</td><td align="left">78.22</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00715</td><td align="left">80.04</td><td align="left">80.19</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01783</td><td align="left">80.43</td><td align="left">79.59</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.00983</td><td align="left">80.82</td><td align="left">82.21</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">
<italic>0.01763</italic>
</td><td align="left">82.52</td><td align="left">83.01</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">0.00079</td><td align="left">82.68</td><td align="left">83.68</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00054</bold></td><td align="left">83.03</td><td align="left">85.37</td></tr><tr><td align="left" rowspan="8">SNLI</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">92.53</td><td align="left">92.97</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03822</td><td align="left">84.33</td><td align="left">84.98</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00843</td><td align="left">86.13</td><td align="left">86.93</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01782</td><td align="left">83.31</td><td align="left">83.66</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01163</td><td align="left">87.05</td><td align="left">87.29</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.06773</td><td align="left">89.21</td><td align="left">89.88</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00072</italic>
</td><td align="left"><bold>90</bold>.<bold>86</bold></td><td align="left">
<italic>91.00</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00053</bold></td><td align="left"><bold>90</bold>.<bold>86</bold></td><td align="left">
<italic>91.00</italic>
</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab15"><label>Table 15</label><caption><p>Entailment classification results for the Mistral model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">RTE</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">94.67</td><td align="left">89.82</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03663</td><td align="left">76.22</td><td align="left">74.45</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00732</td><td align="left">80.34</td><td align="left">80.17</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01778</td><td align="left">75.12</td><td align="left">75.86</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01521</td><td align="left">83.39</td><td align="left">82.25</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.06739</td><td align="left">
<italic>85.65</italic>
</td><td align="left">83.12</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00083</italic>
</td><td align="left">84.73</td><td align="left">
<italic>83.87</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00065</bold></td><td align="left"><bold>85</bold>.<bold>94</bold></td><td align="left"><bold>84</bold>.<bold>67</bold></td></tr><tr><td align="left" rowspan="8">MRPC</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">93.02</td><td align="left">94.21</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03654</td><td align="left">75.28</td><td align="left">77.03</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00722</td><td align="left">80.34</td><td align="left">82.17</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01715</td><td align="left">76.19</td><td align="left">80.31</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01513</td><td align="left">82.83</td><td align="left">83.41</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.06724</td><td align="left"><bold>86</bold>.<bold>47</bold></td><td align="left">
<italic>87.02</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00082</italic>
</td><td align="left">85.63</td><td align="left">85.17</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00055</bold></td><td align="left">
<italic>86.31</italic>
</td><td align="left"><bold>87</bold>.<bold>98</bold></td></tr><tr><td align="left" rowspan="8">SNLI</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">94.21</td><td align="left">95.32</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.03666</td><td align="left">85.55</td><td align="left">85.78</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.00744</td><td align="left">86.35</td><td align="left">86.21</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.01774</td><td align="left">85.37</td><td align="left">86.05</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.01524</td><td align="left">84.12</td><td align="left">84.76</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.06736</td><td align="left">89.11</td><td align="left">89.77</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00089</italic>
</td><td align="left">
<italic>91.62</italic>
</td><td align="left">
<italic>91.31</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00068</bold></td><td align="left"><bold>92</bold>.<bold>56</bold></td><td align="left"><bold>91</bold>.<bold>86</bold></td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p>
<table-wrap id="Tab16"><label>Table 16</label><caption><p>Entailment classification results for the Phi-2 model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Type</th><th align="left">Parameters (%)</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="8">RTE</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">90.37</td><td align="left">85.74</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.83872</td><td align="left">59.54</td><td align="left">58.27</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.14234</td><td align="left">61.18</td><td align="left">61.84</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.15834</td><td align="left">58.61</td><td align="left">56.38</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.13746</td><td align="left">66.52</td><td align="left">65.82</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.71658</td><td align="left">72.25</td><td align="left">70.45</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00424</italic>
</td><td align="left">
<italic>76.44</italic>
</td><td align="left">
<italic>75.92</italic>
</td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00252</bold></td><td align="left"><bold>76</bold>.<bold>53</bold></td><td align="left"><bold>76</bold>.<bold>11</bold></td></tr><tr><td align="left" rowspan="8">MRPC</td><td align="left">Full Fine-tuning</td><td align="left">100.000</td><td align="left">89.31</td><td align="left">90.21</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.83822</td><td align="left">71.15</td><td align="left">72.78</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.14345</td><td align="left">73.16</td><td align="left">75.28</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.15842</td><td align="left">70.48</td><td align="left">71.21</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.13747</td><td align="left">80.53</td><td align="left">81.33</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.71659</td><td align="left">
<italic>83.19</italic>
</td><td align="left">
<italic>84.23</italic>
</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00742</italic>
</td><td align="left"><bold>83</bold>.<bold>63</bold></td><td align="left"><bold>84</bold>.<bold>72</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00349</bold></td><td align="left">82.54</td><td align="left">83.42</td></tr><tr><td align="left" rowspan="8">SNLI</td><td align="left">Full Fine-tuning</td><td align="left">100.00</td><td align="left">90.54</td><td align="left">91.02</td></tr><tr><td align="left">Prefix Tuning</td><td align="left">0.83844</td><td align="left">79.27</td><td align="left">79.82</td></tr><tr><td align="left">Prompt Tuning</td><td align="left">0.14149</td><td align="left">81.30</td><td align="left">81.80</td></tr><tr><td align="left">P-Tuning</td><td align="left">0.15823</td><td align="left">78.56</td><td align="left">77.96</td></tr><tr><td align="left">Lora Rank 2</td><td align="left">0.13745</td><td align="left">82.45</td><td align="left">82.67</td></tr><tr><td align="left">Lora Rank 4</td><td align="left">0.71656</td><td align="left">84.36</td><td align="left">84.89</td></tr><tr><td align="left">SK-Tuning (Prefix)</td><td align="left">
<italic>0.00609</italic>
</td><td align="left"><bold>89</bold>.<bold>21</bold></td><td align="left"><bold>90</bold>.<bold>51</bold></td></tr><tr><td align="left">SK-Tuning (Prompt)</td><td align="left"><bold>0</bold>.<bold>00589</bold></td><td align="left">
<italic>88.62</italic>
</td><td align="left">
<italic>88.95</italic>
</td></tr></tbody></table><table-wrap-foot><p>The best results are highlighted in bold, and the second-best result is italics for clarity except full fine-tuning.</p></table-wrap-foot></table-wrap>
</p><p id="Par119">In contrast, prefix tuning and prompt tuning techniques, which involve fine-tuning only a small fraction of the model&#x02019;s parameters, tend to yield significantly lower accuracy and F1-scores. This suggests that limiting parameter updates to specific prefixes or prompts may not be sufficient for optimal entailment classification performance, as these methods may struggle to capture the diverse and complex relationships present in the data.</p><p id="Par120">The Lora Rank 2 and Lora Rank 4 models deliver competitive results, particularly evident in the RTE dataset, where they outperform other techniques. This indicates that techniques like Lora Rank, which involve a moderate amount of parameter modification, can strike a balance between model adaptation and computational efficiency.</p><p id="Par121">However, SK-Tuning, whether applied to prefixes or prompts, consistently performs well across datasets, demonstrating its effectiveness as an alternative fine-tuning strategy. SK-Tuning achieves strong results with a minimal increase in the number of parameters, making it a promising approach for entailment classification tasks where computational resources are a concern.</p></sec></sec><sec id="Sec30"><title>Ablation study</title><sec id="Sec31"><title>Efficiency</title><p id="Par123">Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> illustrates that SK-Tuning methods for Prefix and Prompt, demonstrate superior memory efficiency with the lowest memory cost among the compared PEFT methods, making them ideal for resource-constrained environments. Despite their minimal memory footprint, these methods maintain competitive training efficiency, balancing low parameter percentages with moderate training times, which highlights their effectiveness in achieving lightweight and fast fine-tuning. Compared to other methods like LoKr, LoHa, and LoRA, which show higher memory costs and varying degrees of training efficiency, SK-Tuning stands out as a robust approach that optimizes both memory and computational resources, making it particularly advantageous for scenarios where efficiency is paramount.</p><p id="Par122">
<fig id="Fig2"><label>Fig. 2</label><caption><p>Comparison of memory efficiency (left) and training efficiency (right) across various PEFT methods. S-Prefix and S-Prompt represent SK-Tuning applied to prefix tuning and prompt tuning, respectively. The left chart shows the memory cost in GB, highlighting the model weights and optimizations, while the right chart displays the percentage of parameters, total training time in hours, and iteration time per second.</p></caption><graphic xlink:href="41598_2024_75599_Fig2_HTML" id="MO4"/></fig>
</p></sec><sec id="Sec32"><title>Faster convergence with SK-tuning</title><p id="Par124">In this section, we present an ablation study comparing the convergence speed and performance of SK-Tuning with traditional prompt and prefix tuning methods on three different downstream tasks: Token Classification, Sequence Classification, and NLI. We hypothesize that SK-Tuning, leveraging semantic knowledge, will lead to faster convergence due to the inherent zero-shot capabilities of LLMs<sup><xref ref-type="bibr" rid="CR93">93</xref></sup>.</p><sec id="Sec33"><title>Accelerated convergence in token classification</title><p id="Par125">In the context of token classification tasks, we conducted a comprehensive comparison between SK-Tuning and traditional tuning methods. We utilized two benchmark datasets, namely Wikiann and Conll03, both featuring token-level labels. Our primary objective was to analyze the convergence behavior, measured in terms of loss reduction, as training steps progressed.</p><p id="Par126">Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> visually presents the convergence trajectories for SK-Tuning and traditional methods. Notably, we observed a remarkable disparity in the convergence speed between these approaches. SK-Tuning, whether applied to prefixes or prompts, demonstrated a strikingly swift convergence compared to the conventional tuning method.</p><p>
<fig id="Fig3"><label>Fig. 3</label><caption><p>Convergence comparison for token classification.</p></caption><graphic xlink:href="41598_2024_75599_Fig3_HTML" id="MO5"/></fig>
</p><p id="Par127">This accelerated convergence showcased in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> serves as compelling evidence of the significant advantages brought about by the incorporation of semantic knowledge. It underscores the ability of SK-Tuning to facilitate rapid adaptation to the intricacies of token classification tasks, emphasizing the practical utility of this approach.</p></sec><sec id="Sec34"><title>Accelerated convergence in sequence classification</title><p id="Par128">For the evaluation of SK-Tuning in sequence classification tasks, we conducted a comparative analysis against traditional tuning methods. Our experimentation leveraged two benchmark datasets: Fake News and SST2, both featuring sequences with corresponding labels. Our primary objective was to assess the convergence performance, measured in terms of loss reduction, as the model underwent training iterations.</p><p id="Par129">Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> offers a visual representation of the convergence patterns observed during sequence classification. Notably, the results depicted in the figure demonstrate the accelerated convergence achieved with SK-Tuning when compared to conventional tuning methods.</p><p>
<fig id="Fig4"><label>Fig. 4</label><caption><p>Convergence comparison for sequence classification.</p></caption><graphic xlink:href="41598_2024_75599_Fig4_HTML" id="MO6"/></fig>
</p><p id="Par130">The swift convergence illustrated in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> underscores the significant advantages bestowed by the integration of semantic knowledge into the fine-tuning process. This enhancement enables the model to quickly adapt to the nuances of the specific sequence classification task, reaffirming the effectiveness of SK-Tuning in practical scenarios.</p></sec><sec id="Sec35"><title>Accelerated convergence in NLI</title><p id="Par131">In the realm of NLI tasks, we conducted a comparative analysis pitting SK-Tuning against traditional tuning methods. Our evaluation incorporated well-established datasets, including MRPC and SNLI, which consist of premise-hypothesis pairs and their corresponding entailment labels. The primary objective was to assess convergence speed, measured in terms of training steps.</p><p id="Par132">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> visually illustrates the convergence dynamics observed during NLI tasks. Notably, the findings showcased in the figure reveal the expedited convergence achieved through SK-Tuning when compared to traditional tuning approaches.</p><p>
<fig id="Fig5"><label>Fig. 5</label><caption><p>Convergence comparison for sequence classification.</p></caption><graphic xlink:href="41598_2024_75599_Fig5_HTML" id="MO7"/></fig>
</p><p id="Par133">The swift convergence depicted in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> underscores the substantial advantages conferred by the integration of semantic knowledge into the fine-tuning process. This augmentation enhances the model&#x02019;s adaptability, enabling it to quickly grasp the nuances of NLI tasks and reaffirming the practical utility of SK-Tuning in advancing NLI model performance.</p><p id="Par134">Our ablation study clearly demonstrates that SK-Tuning outperforms traditional prompt and prefix tuning methods in terms of convergence speed across a range of downstream tasks. The incorporation of semantic knowledge, along with the zero-shot capabilities of LLMs, contributes to faster task adaptation. Additionally, SK-Tuning consistently leads to better performance, as shown in subsequent sections.</p></sec></sec><sec id="Sec36"><title>Adapter layers</title><p id="Par135">In this study, we investigate the impact of adapter layer complexity on the performance of fine-tuned models. Specifically, we analyze how increasing the complexity of adapter layers affects various factors, including the percentage of parameters, computational cost, and convergence speed. We conducted experiments using the Mistral 7B model on the SST2 dataset, and the results are presented in Table&#x000a0;<xref rid="Tab17" ref-type="table">17</xref>.</p><p>
<table-wrap id="Tab17"><label>Table 17</label><caption><p>Exploring the Trade-offs&#x02014;adapter complexity vs. performance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Number of Layers</th><th align="left">Parameters</th><th align="left">Accuracy (%)</th><th align="left">F1-score (%)</th><th align="left">Convergence steps</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">0.00031</td><td align="left">96.93</td><td align="left">97.14</td><td align="left">1500</td></tr><tr><td align="left">3</td><td align="left">0.02236</td><td align="left">95.32</td><td align="left">97.02</td><td align="left">7900</td></tr><tr><td align="left">5</td><td align="left">1.18583</td><td align="left">96.98</td><td align="left">97.25</td><td align="left">16600</td></tr><tr><td align="left">7</td><td align="left">3.46256</td><td align="left">97.21</td><td align="left">97.54</td><td align="left">23900</td></tr><tr><td align="left">9</td><td align="left">6.28583</td><td align="left">97.16</td><td align="left">97.37</td><td align="left">53200</td></tr><tr><td align="left">11</td><td align="left">9.89372</td><td align="left">97.29</td><td align="left">97.87</td><td align="left">79400</td></tr></tbody></table></table-wrap>
</p><p id="Par136">As shown in Table&#x000a0;<xref rid="Tab17" ref-type="table">17</xref>, increasing the number of adapter layers leads to a proportional increase in the number of parameters. This rise in complexity comes at the cost of increased computational resources and slower convergence. While the performance of the model does show marginal improvements with more complex adapter layers, it is essential to note that these gains are relatively modest.</p><p id="Par137">For instance, with just one adapter layer, the model exhibits a relatively small number of parameters, efficient convergence, and high accuracy. However, as we progressively increase the complexity with additional layers, the number of parameters surges significantly, computational requirements escalate, and convergence becomes substantially slower. Notably, the performance gains achieved by complex adapter layers are relatively modest.</p><p id="Par138">The observed trend suggested that as the complexity of the adapter layers increased, the computational demands and training time also increased substantially. This phenomenon can be attributed to the need for extensive training to capture and leverage semantic information effectively.</p></sec><sec id="Sec37"><title>Effect of prompt and prefix text</title><p id="Par139">In this ablation study, we investigate the influence of prompt and prefix text length on the performance of SK-Tuning for sentiment classification using the SST-2 dataset. Our goal is to demonstrate that well-crafted prompt or prefix texts can outperform longer, less informative alternatives, despite the latter offering a larger number of trainable parameters.</p><p id="Par140">We conducted experiments with various prompt and prefix texts and evaluated their corresponding accuracy on sentiment classification tasks using the Mistral model, which boasts 7 billion parameters. The table below summarizes the results.</p><p id="Par141">The results presented in Table&#x000a0;<xref rid="Tab18" ref-type="table">18</xref> clearly illustrate that a concise and informative prompt text outperforms longer and less focused alternatives. Despite the fact that longer prompts or prefixes provide more trainable parameters, our findings underscore the significance of crafting prompts that offer clear task instructions and context, resulting in enhanced model performance.</p><p>
<table-wrap id="Tab18"><label>Table 18</label><caption><p>Effect of prompt and prefix length on sentiment classification accuracy.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Prompt / Prefix (Text)</th><th align="left">Prefix</th><th align="left">Prompt</th><th align="left">Token length</th></tr></thead><tbody><tr><td align="left">&#x0201c;Classify the sentiment of the following text&#x0201d;</td><td align="left">93.41</td><td align="left">92.83</td><td align="left">9</td></tr><tr><td align="left">&#x0201c;I have a piece of text and I need to understand its emotional tone. Could you classify the sentiment of the text:&#x0201d;</td><td align="left">
<italic>96.04</italic>
</td><td align="left">
<italic>95.41</italic>
</td><td align="left">29</td></tr><tr><td align="left">&#x0201c;Considering the context and tone, can you classify the sentiment of the following text? Here&#x02019;s the text&#x0201d;</td><td align="left">95.13</td><td align="left">94.82</td><td align="left">24</td></tr><tr><td align="left">&#x0201c;Focus on the emotional cues present in the text:&#x0201d;</td><td align="left">92.93</td><td align="left">93.09</td><td align="left">12</td></tr><tr><td align="left">&#x0201c;Let&#x02019;s analyze the sentiment of this text together. I&#x02019;ll provide the text, and you classify the sentiment of the text. Here&#x02019;s the text:&#x0201d;</td><td align="left">94.78</td><td align="left">94.17</td><td align="left">35</td></tr><tr><td align="left">&#x0201c;Classify the positive or negative sentiment of the text:&#x0201d;</td><td align="left"><bold>96</bold>.<bold>83</bold></td><td align="left"><bold>96</bold>.<bold>52</bold></td><td align="left">11</td></tr></tbody></table><table-wrap-foot><p>Significant values are in [bold, italics].</p></table-wrap-foot></table-wrap>
</p><p id="Par142">Furthermore, to visualize the relationship between the prompt text and the input text, we analyzed the attention scores of the last layer. Specifically, we used the prompt text <italic>Classify the positive or negative sentiment of the text.</italic> in conjunction with input texts <italic> I love this movie.</italic> and <italic>I hate this movie.</italic> The figures in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> depict the attention scores, highlighting the sentimental connection between the prompt and the text. In the left figure, the prompt text, particularly the word <italic>positive</italic> exhibits a strong attention score with <italic>love</italic> Conversely, in the right figure, the prompt word <italic>negative</italic> shows a pronounced attention score with <italic>hate</italic> This observation suggests that including words like <italic>positive</italic> and <italic>negative</italic> in the prompt text significantly aids the model in making informed decisions, thereby emphasizing the importance of crafting effective prompt texts.</p><p>
<fig id="Fig6"><label>Fig. 6</label><caption><p>Attentional Insights&#x02014;exploring the Sentimental Connection between Prompt Text and Input Text. The left side of the figure reveals the attention scores between the prompt text, particularly the word &#x02018;positive,&#x02019; and the input text &#x02018;I love this movie.&#x02019; On the right side, the attention scores depict the relationship between the prompt word &#x02018;negative&#x02019; and the input text &#x02019;I hate this movie.&#x02019; These attention patterns shed light on how well-crafted prompt texts enhance the model&#x02019;s decision-making process.</p></caption><graphic xlink:href="41598_2024_75599_Fig6_HTML" id="MO8"/></fig>
</p></sec></sec><sec id="Sec38"><title>Discussion</title><p id="Par143">We present a comprehensive comparison of our proposed SK-Tuning method with established parameter-efficient fine-tuning techniques, including Prompt Tuning, Prefix Tuning, P-Tuning, LORA Rank 2, and LORA Rank 4. Our evaluation encompassed a diverse set of downstream tasks across various domains within NLP. Notably, SK-Tuning for both prompts and prefixes consistently outperformed these traditional methods across several key metrics, including accuracy, F1 score, and parameter efficiency.</p><p id="Par144">One of the key takeaways from our comparison is the remarkable performance gains achieved by SK-Tuning. In terms of accuracy and F1 score, SK-Tuning consistently delivered superior results across the spectrum of tasks. This improvement underscores the effectiveness of leveraging semantically meaningful information in the fine-tuning process, as opposed to relying on arbitrary virtual tokens.</p><p id="Par145">Equally noteworthy is the efficiency of SK-Tuning. By minimizing the number of trainable parameters required for adaptation, our approach demonstrates a substantial reduction in computational resources while maintaining or even enhancing task performance. This efficiency is particularly crucial in practical applications, where resource constraints often play a significant role.</p><p id="Par146">Another noteworthy aspect of our study is the extensive evaluation across five different pretrained LLMs: Bloom (7B), Falcon (7B), LLAMA2 (7B), Mistral (7B), and Phi2 (2.7B). Our results consistently indicate that SK-Tuning is a robust and versatile technique that can be applied to various LLM architectures, demonstrating its broad applicability and effectiveness across different model sizes and complexities.</p></sec><sec id="Sec39"><title>Limitations</title><p id="Par147">While SK-Tuning offers significant advantages in terms of performance and parameter efficiency, there are several key limitations that should be considered:</p><sec id="Sec40"><title>Training and inference time overhead</title><p id="Par148">One of the primary limitations of SK-Tuning is the potential increase in inference or training time. Since it utilizes the pre-trained LLM twice during the forward pass: once to obtain semantic information from the prompt or prefix and again for processing the input data to get output. This dual usage of the LLM can lead to longer training and inference time.</p></sec><sec id="Sec41"><title>Dependency on pretrained models</title><p id="Par149">SK-Tuning relies heavily on the quality and capabilities of the underlying pretrained LLM. The success of prompt or prefix text tuning is linked to the zero-shot capabilities of the LLM. If the pretrained model does not have a strong grasp of semantic knowledge or lacks certain linguistic skills, the effectiveness of SK-Tuning could be reduced. It needs significant training to accurately understand the semantic meaning of the prompt or prefix text.</p></sec><sec id="Sec42"><title>Semantic knowledge acquisition</title><p id="Par150">The effectiveness of SK-Tuning depends on using prompts or prefixes that are meaningful. The more relevant the prompt is to the task, the better the performance, described in section&#x000a0;&#x0201c;<xref rid="Sec37" ref-type="sec">Effect of prompt and prefix text</xref>&#x0201d;. However, creating or finding these meaningful prompts can be difficult and might require specific knowledge about the domain. This challenge could limit how useful SK-Tuning is for certain tasks or datasets.</p></sec><sec id="Sec43"><title>Tuning hyperparameters</title><p id="Par151">Like other fine-tuning approaches, SK-Tuning involves hyperparameter tuning, including the design of the adapter architecture, the choice of semantic knowledge text, and the adjustment of task-specific modules. Identifying the optimal hyperparameters can be a time-consuming and computationally intensive process.</p></sec></sec><sec id="Sec44"><title>Conclusion</title><p id="Par152">In conclusion, our work introduces SK-Tuning as a pioneering approach to fine-tuning LLMs for specific downstream tasks, with a strong emphasis on parameter efficiency. We have shown that traditional methods, relying on learnable virtual tokens in adapters while keeping the LLM&#x02019;s core parameters frozen, often fall short in terms of both efficiency and performance.</p><p id="Par153">SK-Tuning, on the other hand, revolutionizes the fine-tuning process by replacing arbitrary virtual tokens with real, semantically meaningful prefixes. This innovation allows LLMs to tap into their intrinsic semantic knowledge, significantly reducing the need for extensive training iterations. Our experimental results across a range of downstream tasks, including sequence classification, token classification, and NLI, provide compelling evidence that SK-Tuning outperforms traditional approaches. Notably, this improvement is achieved with a reduced number of trainable parameters, emphasizing the efficiency of our method.</p><p id="Par154">By prioritizing parameter efficiency and harnessing the latent semantic understanding of LLMs, SK-Tuning opens up new possibilities for efficient model adaptation across various real-world applications. We believe that our approach holds great promise for advancing the field of NLP, offering researchers and practitioners a valuable tool for achieving enhanced task performance while optimizing computational resources. As LLMs continue to play a pivotal role in NLP, SK-Tuning represents a significant step forward in harnessing their full potential.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Asif Mahmud and Md. Shohanur Islam Sobuj.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization, M.K.; Methodology, N.J, M.K., A.M., and M.S.I.S; software, N.J, M.K., A.M., M.S.I.S, and N.J; formal analysis, M.K, P.B; investigation, A.M., M.S.I.S, M.K. and N.J. resources P.B, J.X. and A.M., data collection M.S.I.S, A.M, and M.K, writing original draft preparation M.K., A.M., M.S.I.S, and N.J; writing review and editing, P.B, N.Y, O.O.G, and M.K.; visualization M.K., A.M., M.S.I.S, and N.J; supervision, N.Y, O.O.G, and P.B. All authors reviewed the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availibility</title><p>Data information and URL are provided within the manuscript.</p></notes><notes id="FPar002" notes-type="COI-statement"><title>Competing interests</title><p>The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Zhu, Y. et al. Large language models for information retrieval: A survey. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2308.07107">arXiv:2308.07107</ext-link> (2023).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Greco, C.&#x000a0;M. &#x00026; Tagarelli, A. Bringing order into the realm of transformer-based language models for artificial intelligence and law. <italic>Artif. Intell. Law</italic><italic>2023</italic>, 1&#x02013;148 (2023).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Brown, T.&#x000a0;B. et al. Language models are few-shot learners (2020). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2005.14165">arXiv: 2005.14165</ext-link>.</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">McIntosh, T.&#x000a0;R., Susnjak, T., Liu, T., Watters, P. &#x00026; Halgamuge, M.&#x000a0;N. From google gemini to openai q*(q-star): A survey of reshaping the generative artificial intelligence (ai) research landscape. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2312.10868">arXiv:2312.10868</ext-link> (2023).</mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Raiaan, M. A.&#x000a0;K. et al. A review on large language models: Architectures, applications, taxonomies, open issues and challenges. <italic>Authorea Preprints</italic> (2023).</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Liu, H. et al. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning (2022). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2205.05638">arXiv: 2205.05638</ext-link>.</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Houlsby, N. et al. Parameter-efficient transfer learning for NLP. In <italic>Proceedings of the 36th International Conference on Machine Learning</italic>, vol.&#x000a0;97 of <italic>Proceedings of Machine Learning Research</italic> (eds. Chaudhuri, K. &#x00026; Salakhutdinov, R.) 2790&#x02013;2799 (PMLR, 2019).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Lester, B., Al-Rfou, R. &#x00026; Constant, N. The power of scale for parameter-efficient prompt tuning (2021). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2104.08691">arXiv: 2104.08691</ext-link>.</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Li, X.&#x000a0;L. &#x00026; Liang, P. Prefix-tuning: Optimizing continuous prompts for generation (2021). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2101.00190">arXiv: 2101.00190</ext-link>.</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Huang, W., Abbeel, P., Pathak, D. &#x00026; Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In <italic>International Conference on Machine Learning</italic> 9118&#x02013;9147 (PMLR, 2022).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Xin, Y. et al. Parameter-efficient fine-tuning for pre-trained vision models: A survey. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2402.02242">arXiv:2402.02242</ext-link> (2024).</mixed-citation></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H</given-names></name><etal/></person-group><article-title>Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>1950</fpage><lpage>1965</lpage></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Liu, H. et al. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>35</bold>, 1950&#x02013;1965 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Nguyen, M., Kishan, K., Nguyen, T., Chadha, A. &#x00026; Vu, T. Efficient fine-tuning large language models for knowledge-aware response planning. In <italic>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</italic> 593&#x02013;611 (Springer, 2023).</mixed-citation></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Hern&#x000e1;ndez</surname><given-names>A</given-names></name><name><surname>Ortega-Mendoza</surname><given-names>RM</given-names></name><name><surname>Villatoro-Tello</surname><given-names>E</given-names></name><name><surname>Camacho-Bello</surname><given-names>CJ</given-names></name><name><surname>P&#x000e9;rez-Cort&#x000e9;s</surname><given-names>O</given-names></name></person-group><article-title>Natural language understanding for navigation of service robots in low-resource domains and languages: Scenarios in spanish and nahuatl</article-title><source>Mathematics</source><year>2024</year><volume>12</volume><fpage>1136</fpage><pub-id pub-id-type="doi">10.3390/math12081136</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Hern&#x000e1;ndez, A., Ortega-Mendoza, R. M., Villatoro-Tello, E., Camacho-Bello, C. J. &#x00026; P&#x000e9;rez-Cort&#x000e9;s, O. Natural language understanding for navigation of service robots in low-resource domains and languages: Scenarios in spanish and nahuatl. <italic>Mathematics</italic><bold>12</bold>, 1136 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Chow, K., Tang, Y., Lyu, Z., Rajput, A. &#x00026; Ban, K. Performance optimization in the llm world 2024. In <italic>Companion of the 15th ACM/SPEC International Conference on Performance Engineering</italic> 156&#x02013;157 (2024).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T. &#x00026; Neubig, G. Towards a unified view of parameter-efficient transfer learning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2110.04366">arXiv:2110.04366</ext-link> (2021).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Liu, X. et al. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2110.07602">arXiv:2110.07602</ext-link> (2021).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Liu, X. et al. Gpt understands, too. <italic>AI Open</italic> (2023).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Zhang, Q. et al. Adaptive budget allocation for parameter-efficient fine-tuning. In <italic>The Eleventh International Conference on Learning Representations</italic> (2023).</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Hu, E.&#x000a0;J. et al. Lora: Low-rank adaptation of large language models. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2106.09685">arXiv:2106.09685</ext-link> (2021).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Li, X.&#x000a0;L. &#x00026; Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2101.00190">arXiv:2101.00190</ext-link> (2021).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Zaken, E.&#x000a0;B., Ravfogel, S. &#x00026; Goldberg, Y. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2106.10199">arXiv:2106.10199</ext-link> (2021).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Ding, H., Zhao, X., Abdullah, S.&#x000a0;N., Dewi, D.&#x000a0;A. &#x00026; Jiang, Z. Dynamic adaptive optimization for effective sentiment analysis fine-tuning on large language models. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2408.11856">arXiv:2408.11856</ext-link> (2024).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Razdaibiedina, A. et al. Residual prompt tuning: Improving prompt tuning with residual reparameterization. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2305.03937">arXiv:2305.03937</ext-link> (2023).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Razdaibiedina, A. et al. Progressive prompts: Continual learning for language models. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2301.12314">arXiv:2301.12314</ext-link> (2023).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Yang, Y. et al. Recent advances of foundation language models-based continual learning: A survey. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2405.18653">arXiv:2405.18653</ext-link> (2024).</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Yang, X. et al. Dynamic prompting: A unified framework for prompt tuning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2303.02909">arXiv:2303.02909</ext-link> (2023).</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Zhao, H., He, R., Xiao, M. &#x00026; Xu, J. Infusing hierarchical guidance into prompt tuning: A parameter-efficient framework for multi-level implicit discourse relation recognition. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2402.15080">arXiv:2402.15080</ext-link> (2024).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Chen, L., Chou, H. &#x00026; Zhu, X. Developing prefix-tuning models for hierarchical text classification. In <italic>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track</italic> 390&#x02013;397 (2022).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Liu, X., Huang, H., Shi, G. &#x00026; Wang, B. Dynamic prefix-tuning for generative template-based event extraction. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2205.06166">arXiv:2205.06166</ext-link> (2022).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Yang, L. et al. Mixpave: Mix-prompt tuning for few-shot product attribute value extraction. In <italic>Findings of the Association for Computational Linguistics: ACL</italic> 9978&#x02013;9991 (2023).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Han, C. et al. E&#x000a0;<inline-graphic xlink:href="41598_2024_75599_Article_IEq141.gif"/> 2vpt: An effective and efficient approach for visual prompt tuning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2307.13770">arXiv:2307.13770</ext-link> (2023).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Renduchintala, A., Konuk, T. &#x00026; Kuchaiev, O. Tied-lora: Enhacing parameter efficiency of lora with weight tying. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2311.09578">arXiv:2311.09578</ext-link> (2023).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Sheng, Y. et al. S-lora: Serving thousands of concurrent lora adapters. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2311.03285">arXiv:2311.03285</ext-link> (2023).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Xia, W., Qin, C. &#x00026; Hazan, E. Chain of lora: Efficient fine-tuning of language models via residual learning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2401.04151">arXiv:2401.04151</ext-link> (2024).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Wang, Y., Lin, Y., Zeng, X. &#x00026; Zhang, G. Multilora: Democratizing lora for better multi-task learning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2311.11501">arXiv:2311.11501</ext-link> (2023).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Dettmers, T., Pagnoni, A., Holtzman, A. &#x00026; Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>36</bold>, 56 (2024).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Lialin, V., Muckatira, S., Shivagunde, N. &#x00026; Rumshisky, A. Relora: High-rank training through low-rank updates. In <italic>Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)</italic> (2023).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Edalati, A. et al. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2212.10650">arXiv:2212.10650</ext-link> (2022).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Shi, S. et al. Reslora: Identity residual mapping in low-rank adaption. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2402.18039">arXiv:2402.18039</ext-link> (2024).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Hyeon-Woo, N., Ye-Bin, M. &#x00026; Oh, T.-H. Fedpara: Low-rank hadamard product for communication-efficient federated learning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2108.06098">arXiv:2108.06098</ext-link> (2021).</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Qiu, Z. et al. Controlling text-to-image diffusion by orthogonal finetuning (2024). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2306.07280">arXiv: 2306.07280</ext-link>.</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Liu, W. et al. Parameter-efficient orthogonal finetuning via butterfly factorization (2024). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2311.06243">arXiv: 2311.06243</ext-link>.</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Larsen, B.&#x000a0;W., Fort, S., Becker, N. &#x00026; Ganguli, S. How many degrees of freedom do we need to train deep networks: a loss landscape perspective. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2107.05802">arXiv:2107.05802</ext-link> (2021).</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Gur-Ari, G., Roberts, D.&#x000a0;A. &#x00026; Dyer, E. Gradient descent happens in a tiny subspace. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1812.04754">arXiv:1812.04754</ext-link> (2018).</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Lee, Y. &#x00026; Choi, S. Gradient-based meta-learning with learned layerwise metric and subspace. In <italic>International Conference on Machine Learning</italic> 2927&#x02013;2936 (PMLR, 2018).</mixed-citation></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Chaudhry</surname><given-names>A</given-names></name><name><surname>Khan</surname><given-names>N</given-names></name><name><surname>Dokania</surname><given-names>P</given-names></name><name><surname>Torr</surname><given-names>P</given-names></name></person-group><article-title>Continual learning in low-rank orthogonal subspaces</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>9900</fpage><lpage>9911</lpage></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Chaudhry, A., Khan, N., Dokania, P. &#x00026; Torr, P. Continual learning in low-rank orthogonal subspaces. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>33</bold>, 9900&#x02013;9911 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Finn, C., Abbeel, P. &#x00026; Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In <italic>International Conference on Machine Learning</italic> 1126&#x02013;1135 (PMLR, 2017).</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Frikha, A., Krompa&#x000df;, D., K&#x000f6;pken, H.-G. &#x00026; Tresp, V. Few-shot one-class classification via meta-learning. In <italic>Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35</italic> 7448&#x02013;7456 (2021).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Nunez, E. et al. Lcs: Learning compressible subspaces for efficient, adaptive, real-time network compression at inference time. In <italic>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</italic> 3818&#x02013;3827 (2023).</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Pham, H., Guan, M., Zoph, B., Le, Q. &#x00026; Dean, J. Efficient neural architecture search via parameters sharing. In <italic>International conference on machine learning</italic> 4095&#x02013;4104 (PMLR, 2018).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Chen, Y. et al. Automatic subspace evoking for efficient neural architecture search. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2210.17180">arXiv:2210.17180</ext-link> (2022).</mixed-citation></ref><ref id="CR53"><label>53.</label><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>P</given-names></name><etal/></person-group><article-title>A comprehensive survey of neural architecture search: Challenges and solutions</article-title><source>ACM Comput. Surv. (CSUR)</source><year>2021</year><volume>54</volume><fpage>1</fpage><lpage>34</lpage></element-citation><mixed-citation id="mc-CR53" publication-type="journal">Ren, P. et al. A comprehensive survey of neural architecture search: Challenges and solutions. <italic>ACM Comput. Surv. (CSUR)</italic><bold>54</bold>, 1&#x02013;34 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Rajeswaran, A., Finn, C., Kakade, S.&#x000a0;M. &#x00026; Levine, S. Meta-learning with implicit gradients. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>32</bold>, 859 (2019).</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Zhao, J. et al. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2403.03507">arXiv:2403.03507</ext-link> (2024).</mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Chen, Y. &#x00026; Wainwright, M.&#x000a0;J. Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1509.03025">arXiv:1509.03025</ext-link> (2015).</mixed-citation></ref><ref id="CR57"><label>57.</label><citation-alternatives><element-citation id="ec-CR57" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Raskutti</surname><given-names>G</given-names></name><name><surname>Yuan</surname><given-names>M</given-names></name></person-group><article-title>Non-convex projected gradient descent for generalized low-rank tensor regression</article-title><source>J. Mach. Learn. Res.</source><year>2019</year><volume>20</volume><fpage>1</fpage><lpage>37</lpage></element-citation><mixed-citation id="mc-CR57" publication-type="journal">Chen, H., Raskutti, G. &#x00026; Yuan, M. Non-convex projected gradient descent for generalized low-rank tensor regression. <italic>J. Mach. Learn. Res.</italic><bold>20</bold>, 1&#x02013;37 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Zhang, T. &#x00026; Fan, X. Projected gradient descent algorithm for low-rank matrix estimation. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2403.02704">arXiv:2403.02704</ext-link> (2024).</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">M&#x00105;dry, A., Makelov, A., Schmidt, L., Tsipras, D. &#x00026; Vladu, A. Towards deep learning models resistant to adversarial attacks. <italic>stat</italic><bold>1050</bold> (2017).</mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Croce, F. &#x00026; Hein, M. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In <italic>International Conference on Machine Learning</italic> 2206&#x02013;2216 (PMLR, 2020).</mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">Wong, E., Rice, L. &#x00026; Kolter, J.&#x000a0;Z. Fast is better than free: Revisiting adversarial training. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2001.03994">arXiv:2001.03994</ext-link> (2020).</mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="other">Shazeer, N. &#x00026; Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In <italic>International Conference on Machine Learning</italic> 4596&#x02013;4604 (PMLR, 2018).</mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Li, B., Chen, J. &#x00026; Zhu, J. Memory efficient optimizers with 4-bit states. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>36</bold>, 56 (2024).</mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Deshpande, A. et al. Spartan: sparse hierarchical memory for parameter-efficient transformers. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2211.16634">arXiv:2211.16634</ext-link> (2022).</mixed-citation></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="other">Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library (2019). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1912.01703">arXiv: 1912.01703</ext-link>.</mixed-citation></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="other">Wolf, T. et al. Huggingface&#x02019;s transformers: State-of-the-art natural language processing (2020). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1910.03771">arXiv: 1910.03771</ext-link>.</mixed-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">Wang, A. et al. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1804.07461">arXiv:1804.07461</ext-link> (2018).</mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">Liu, Y. et al. Roberta: A robustly optimized bert pretraining approach. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1907.11692">arXiv:1907.11692</ext-link> (2019).</mixed-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Warstadt, A., Singh, A. &#x00026; Bowman, S.&#x000a0;R. Neural network acceptability judgments (2019). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1805.12471">arXiv: 1805.12471</ext-link>.</mixed-citation></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="other">Socher, R. et al. Recursive deep models for semantic compositionality over a sentiment treebank. In <italic>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</italic> (eds. Yarowsky, D. et al.) 1631&#x02013;1642 (Association for Computational Linguistics, 2013).</mixed-citation></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Saravia, E., Liu, H.-C.&#x000a0;T., Huang, Y.-H., Wu, J. &#x00026; Chen, Y.-S. CARER: Contextualized affect representations for emotion recognition. In <italic>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</italic> (eds. Riloff, E., et al.) 3687&#x02013;3697 (Association for Computational Linguistics, 2018). 10.18653/v1/D18-1404.</mixed-citation></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="other">Cruz, J. C.&#x000a0;B., Tan, J.&#x000a0;A. &#x00026; Cheng, C. Localization of fake news detection via multitask transfer learning. In <italic>Proceedings of the Twelfth Language Resources and Evaluation Conference</italic> (eds. Calzolari, N. et al.) 2596&#x02013;2604 (European Language Resources Association, 2020).</mixed-citation></ref><ref id="CR73"><label>73.</label><mixed-citation publication-type="other">Tjong Kim Sang, E. F. &#x00026; De Meulder, F. Introduction to the CoNLL-2003 shared task. Language-independent named entity recognition. In <italic>Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL</italic> 142&#x02013;147 (2003).</mixed-citation></ref><ref id="CR74"><label>74.</label><mixed-citation publication-type="other">Zhao, S., Liu, T., Zhao, S. &#x00026; Wang, F. A neural multi-task learning framework to jointly model medical named entity recognition and normalization (2018). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1812.06081">arXiv: 1812.06081</ext-link>.</mixed-citation></ref><ref id="CR75"><label>75.</label><mixed-citation publication-type="other">Pan, X. et al. Cross-lingual name tagging and linking for 282 languages. In <italic>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</italic> (eds. Barzilay, R. &#x00026; Kan, M.-Y.) 1946&#x02013;1958 (Association for Computational Linguistics, 2017). 10.18653/v1/P17-1178 .</mixed-citation></ref><ref id="CR76"><label>76.</label><mixed-citation publication-type="other">Dagan, I., Glickman, O. &#x00026; Magnini, B. The pascal recognising textual entailment challenge. In <italic>Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment</italic> (eds. Qui&#x000f1;onero-Candela, J. et al.) 177&#x02013;190 (Springer, 2006).</mixed-citation></ref><ref id="CR77"><label>77.</label><mixed-citation publication-type="other">Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L. &#x00026; Giampiccolo, D. The second pascal recognising textual entailment challenge. In <italic>Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</italic> (2006).</mixed-citation></ref><ref id="CR78"><label>78.</label><mixed-citation publication-type="other">Giampiccolo, D., Magnini, B., Dagan, I. &#x00026; Dolan, W.&#x000a0;B. The third pascal recognizing textual entailment challenge. In <italic>ACL-PASCAL@ACL</italic> (2007).</mixed-citation></ref><ref id="CR79"><label>79.</label><citation-alternatives><element-citation id="ec-CR79" publication-type="journal"><person-group person-group-type="author"><name><surname>Bentivogli</surname><given-names>L</given-names></name><name><surname>Clark</surname><given-names>P</given-names></name><name><surname>Dagan</surname><given-names>I</given-names></name><name><surname>Giampiccolo</surname><given-names>D</given-names></name></person-group><article-title>The fifth pascal recognizing textual entailment challenge</article-title><source>TAC</source><year>2009</year><volume>7</volume><fpage>8</fpage></element-citation><mixed-citation id="mc-CR79" publication-type="journal">Bentivogli, L., Clark, P., Dagan, I. &#x00026; Giampiccolo, D. The fifth pascal recognizing textual entailment challenge. <italic>TAC</italic><bold>7</bold>, 8 (2009).</mixed-citation></citation-alternatives></ref><ref id="CR80"><label>80.</label><mixed-citation publication-type="other">Dolan, W.&#x000a0;B. &#x00026; Brockett, C. Automatically constructing a corpus of sentential paraphrases. In <italic>Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</italic> (2005).</mixed-citation></ref><ref id="CR81"><label>81.</label><mixed-citation publication-type="other">Bowman, S.&#x000a0;R., Angeli, G., Potts, C. &#x00026; Manning, C.&#x000a0;D. A large annotated corpus for learning natural language inference (2015). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1508.05326">arXiv: 1508.05326</ext-link>.</mixed-citation></ref><ref id="CR82"><label>82.</label><mixed-citation publication-type="other">Workshop, B. et al. Bloom: A 176b-parameter open-access multilingual language model (2023). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2211.05100">arXiv: 2211.05100</ext-link>.</mixed-citation></ref><ref id="CR83"><label>83.</label><mixed-citation publication-type="other">Geng, X. &#x00026; Liu, H <italic>An Open Reproduction of llama</italic> (Openllama, 2023).</mixed-citation></ref><ref id="CR84"><label>84.</label><mixed-citation publication-type="other">Jiang, A.&#x000a0;Q. et al. Mistral 7b (2023). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2310.06825">arXiv: 2310.06825</ext-link>.</mixed-citation></ref><ref id="CR85"><label>85.</label><mixed-citation publication-type="other">Penedo, G. et al. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only (2023). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2306.01116">arXiv: 2306.01116</ext-link>.</mixed-citation></ref><ref id="CR86"><label>86.</label><mixed-citation publication-type="other">Li, Y. et al. Textbooks are all you need ii: phi-1.5 technical report (2023). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2309.05463">arXiv: 2309.05463</ext-link>.</mixed-citation></ref><ref id="CR87"><label>87.</label><mixed-citation publication-type="other">Gunasekar, S. et al. Textbooks are all you need (2023). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2306.11644">arXiv: 2306.11644</ext-link>.</mixed-citation></ref><ref id="CR88"><label>88.</label><mixed-citation publication-type="other">Howard, J. &#x00026; Ruder, S. Universal language model fine-tuning for text classification (2018). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1801.06146">arXiv: 1801.06146</ext-link>.</mixed-citation></ref><ref id="CR89"><label>89.</label><mixed-citation publication-type="other">Li, X.&#x000a0;L. &#x00026; Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In <italic>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</italic> (eds. Zong, C. et al.) 4582&#x02013;4597 (Association for Computational Linguistics, 2021). 10.18653/v1/2021.acl-long.353.</mixed-citation></ref><ref id="CR90"><label>90.</label><mixed-citation publication-type="other">Lester, B., Al-Rfou, R. &#x00026; Constant, N. The power of scale for parameter-efficient prompt tuning. In <italic>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</italic> (eds. Moens, M.-F. et al.) 3045&#x02013;3059 (Association for Computational Linguistics, 2021). 10.18653/v1/2021.emnlp-main.243</mixed-citation></ref><ref id="CR91"><label>91.</label><mixed-citation publication-type="other">Liu, X. et al. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In <italic>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</italic> (eds. Muresan S. et al.) 61&#x02013;68. 10.18653/v1/2022.acl-short.8 (Association for Computational Linguistics, 2022).</mixed-citation></ref><ref id="CR92"><label>92.</label><mixed-citation publication-type="other">Loshchilov, I. &#x00026; Hutter, F. Decoupled weight decay regularization (2019). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1711.05101">arXiv: 1711.05101</ext-link>.</mixed-citation></ref><ref id="CR93"><label>93.</label><citation-alternatives><element-citation id="ec-CR93" publication-type="journal"><person-group person-group-type="author"><name><surname>Kojima</surname><given-names>T</given-names></name><name><surname>Gu</surname><given-names>SS</given-names></name><name><surname>Reid</surname><given-names>M</given-names></name><name><surname>Matsuo</surname><given-names>Y</given-names></name><name><surname>Iwasawa</surname><given-names>Y</given-names></name></person-group><article-title>Large language models are zero-shot reasoners</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>22199</fpage><lpage>22213</lpage></element-citation><mixed-citation id="mc-CR93" publication-type="journal">Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. &#x00026; Iwasawa, Y. Large language models are zero-shot reasoners. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>35</bold>, 22199&#x02013;22213 (2022).</mixed-citation></citation-alternatives></ref></ref-list></back></article>