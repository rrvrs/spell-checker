<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Heliyon</journal-id><journal-id journal-id-type="iso-abbrev">Heliyon</journal-id><journal-title-group><journal-title>Heliyon</journal-title></journal-title-group><issn pub-type="epub">2405-8440</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11786643</article-id><article-id pub-id-type="pii">S2405-8440(25)00147-1</article-id><article-id pub-id-type="doi">10.1016/j.heliyon.2025.e41767</article-id><article-id pub-id-type="publisher-id">e41767</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Develop an emotion recognition system using jointly connectivity between electroencephalogram and electrocardiogram signals</article-title></title-group><contrib-group><contrib contrib-type="author" id="au1"><name><surname>Sedehi</surname><given-names>Javid Farhadi</given-names></name></contrib><contrib contrib-type="author" id="au2"><name><surname>Dabanloo</surname><given-names>Nader Jafarnia</given-names></name><email>jafarnia@srbiau.ac.ir</email><xref rid="cor1" ref-type="corresp">&#x0204e;</xref></contrib><contrib contrib-type="author" id="au3"><name><surname>Maghooli</surname><given-names>Keivan</given-names></name></contrib><contrib contrib-type="author" id="au4"><name><surname>Sheikhani</surname><given-names>Ali</given-names></name></contrib><aff id="aff1">Department of Biomedical Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran</aff></contrib-group><author-notes><corresp id="cor1"><label>&#x0204e;</label>Corresponding author. <email>jafarnia@srbiau.ac.ir</email></corresp></author-notes><pub-date pub-type="pmc-release"><day>08</day><month>1</month><year>2025</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="collection"><day>30</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="epub"><day>08</day><month>1</month><year>2025</year></pub-date><volume>11</volume><issue>2</issue><elocation-id>e41767</elocation-id><history><date date-type="received"><day>25</day><month>4</month><year>2024</year></date><date date-type="rev-recd"><day>3</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>6</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 The Authors</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p></license></permissions><abstract id="abs0010"><p>This study pioneers an innovative approach to improve the accuracy and dependability of emotion recognition (ER) systems by integrating electroencephalogram (EEG) with electrocardiogram (ECG) data. We propose a novel method of estimating effective connectivity (EC) to capture the dynamic interplay between the heart and brain during emotions of happiness, disgust, fear, and sadness. Leveraging three EC estimation techniques (Granger causality (GC), partial directed coherence (PDC) and directed transfer function (DTF)), we feed the resulting EC representations as inputs into convolutional neural networks (CNNs), namely ResNet-18 and MobileNetV2, known for their swift and superior performance. To evaluate this approach, we have used EEG and ECG data from the public MAHNOB-HCI database through 5-fold cross-validation criterion. Remarkably, our approach achieves an average accuracy of 97.34&#x000a0;&#x000b1;&#x000a0;1.19 and 96.53&#x000a0;&#x000b1;&#x000a0;3.54 for DTF images within the alpha frequency band using ResNet-18 and MobileNetV2, respectively. Comparative analyses in comparison to cutting-edge research unequivocally prove the efficacy of augmenting ECG with EEG, showcasing substantial improvements in ER performance across the spectrum of emotions studied.</p></abstract><kwd-group id="kwrds0010"><title>Keywords</title><kwd>Coupling EEG-ECG</kwd><kwd>Convolutional neural network (CNN)</kwd><kwd>Effective connectivity</kwd><kwd>Emotion recognition</kwd><kwd>Transfer learning</kwd></kwd-group></article-meta></front><body><sec id="sec1"><label>1</label><title>Introduction</title><p id="p0010">Emotions are fundamental to human existence, influencing our cognition, decision-making, and interpersonal relationships. They deeply affect how we interpret our surroundings and play a vital role in maintaining both our psychological and bodily well-being. Recognizing and understanding emotions in others is crucial in fields such as psychology, marketing, and artificial intelligence (AI) [<xref rid="bib1" ref-type="bibr">[1]</xref>, <xref rid="bib2" ref-type="bibr">[2]</xref>, <xref rid="bib3" ref-type="bibr">[3]</xref>]. Emotion recognition technologies have found applications in improving service of customers, developing smarter AI, and improving diagnosis of mental health. To measure emotional states, researchers use techniques like electrocardiography (ECG) [<xref rid="bib4" ref-type="bibr">[4]</xref>, <xref rid="bib5" ref-type="bibr">[5]</xref>, <xref rid="bib6" ref-type="bibr">[6]</xref>] and electroencephalography (EEG) [<xref rid="bib7" ref-type="bibr">[7]</xref>, <xref rid="bib8" ref-type="bibr">[8]</xref>, <xref rid="bib9" ref-type="bibr">[9]</xref>, <xref rid="bib10" ref-type="bibr">[10]</xref>, <xref rid="bib11" ref-type="bibr">[11]</xref>]. EEG monitors the electrical activity in the brain to detect patterns that correspond to different emotional states, while ECG assesses heart rate variability which can indicate relaxation levels or stress. These technologies facilitate a more profound insight into the biological foundations of emotions and aid in designing responses that are empathetically attuned to human sentiments.</p><p id="p0015">There are some brain analysis tools such as connectivity that estimate information flow of lobes to distinguish mental states. This tool has three categories, structural, effective and functional [<xref rid="bib12" ref-type="bibr">12</xref>]. If we consider anatomical structures and compute connectivity between different brain regions, it's structural category [<xref rid="bib12" ref-type="bibr">12</xref>,<xref rid="bib13" ref-type="bibr">13</xref>]. It is typically measured using techniques such as diffusion tensor imaging (DTI) which tracks the movement of water molecules along white matter tracts in the brain. This measure provides insights into the physical wiring of the brain, and offers information about the underlying architecture of brain networks. Also, it allows for the investigation of abnormalities in brain connectivity associated with neurological and psychiatric disorders. but it has disadvantages as limited in providing information about the dynamic functional interactions between brain regions, vulnerable to artifacts and limitations of imaging techniques. Also, it may not fully capture the complexity of brain function since structural connections do not always equate to functional connections. The second one, functional connectivity (FC) refers to the statistical correlation or synchronization of neural activity between different brain regions. It is typically measured using techniques such as functional magnetic resonance imaging (fMRI) or EEG [<xref rid="bib12" ref-type="bibr">12</xref>,<xref rid="bib14" ref-type="bibr">14</xref>]. In this technique, correlation does not imply causation, so functional connectivity measures may not reveal the directionality of interactions. Also, FC is susceptible to noise, artifacts, and confounding factors that can influence correlation measures and is limited in providing information about the underlying structural connections of the brain. The third technique, effective connectivity (EC) describes the influence one brain region exerts over another, often in a directed or causal manner [<xref rid="bib12" ref-type="bibr">12</xref>,<xref rid="bib15" ref-type="bibr">15</xref>]. It focuses on understanding the directional influence or causal relationships between brain regions. This technique provides insights into the directionality and causality of interactions between brain regions, enables the investigation of network dynamics and how information flows within the brain. Also, it is useful for modeling and understanding cognitive processes and brain disorders.</p><p id="p0020">Convolutional Neural Networks (CNNs) have emerged as the most frequently used tool in EEG studies, primarily due to their robust ability to process and analyze spatial and temporal data [<xref rid="bib16" ref-type="bibr">16</xref>,<xref rid="bib17" ref-type="bibr">17</xref>]. This popularity is underpinned by CNNs&#x02019; exceptional proficiency in handling the complex and high-dimensional data typical of EEG recordings. Researchers favor CNNs because they effectively extract useful patterns and features from the raw EEG signals without requiring extensive handy crafted feature engineering. This capability is particularly advantageous in neuroscientific research, where accurate interpret of brain signals is crucial. CNNs are applied in a variety of EEG-related tasks such as classify specific pattern of wave, recognize classes of emotions [<xref rid="bib7" ref-type="bibr">[7]</xref>, <xref rid="bib8" ref-type="bibr">[8]</xref>, <xref rid="bib9" ref-type="bibr">[9]</xref>, <xref rid="bib10" ref-type="bibr">[10]</xref>, <xref rid="bib11" ref-type="bibr">[11]</xref>], and diagnose neurological disorders [<xref rid="bib18" ref-type="bibr">[18]</xref>, <xref rid="bib19" ref-type="bibr">[19]</xref>, <xref rid="bib20" ref-type="bibr">[20]</xref>, <xref rid="bib21" ref-type="bibr">[21]</xref>]. Their widespread adoption in the field is also facilitated by the availability of powerful computational tools and libraries that support CNN implementations, making them accessible to a broad spectrum of researchers and practitioners in neuroscience.</p><p id="p0025">Generating 2D images for CNN applications frequently relates to transforming single-channel EEG into time-frequency (TF) representations. Commonly used single-channel techniques include the continuous wavelet transform (CWT) [<xref rid="bib7" ref-type="bibr">7</xref>,<xref rid="bib18" ref-type="bibr">18</xref>,<xref rid="bib22" ref-type="bibr">[22]</xref>, <xref rid="bib23" ref-type="bibr">[23]</xref>, <xref rid="bib24" ref-type="bibr">[24]</xref>, <xref rid="bib25" ref-type="bibr">[25]</xref>], the Wigner-Ville distribution (WVD) [<xref rid="bib18" ref-type="bibr">18</xref>], and the synchrosqueezing wavelet transform (SSWT) [<xref rid="bib10" ref-type="bibr">10</xref>]. While these methods remain vital for signal analysis, multi-channel approaches, such as connectivity analysis of brain, have gained prominence for their broader applicability and richer insights into EEG data. These multi-channel methods provide a more integrated perspective on neural interactions in contrast to single-channel analyses.</p><p id="p0030">TF representation methods, including CWT, WVD, and SSWT, specialize in examining the evolving frequency components of EEG and ECG signals over time. These approaches are particularly effective for single-channel data, enabling detailed exploration of temporal and frequency dynamics linked to diverse cognitive and physiological states. They offer a focused, granular understanding of neural activity.</p><p id="p0035">On the other hand, brain connectivity analysis leverages data from multiple EEG channels to evaluate interactions across different brain regions. This macroscopic approach uncovers complex network dynamics and regional interplay during specific tasks. The comprehensive insights offered by connectivity analysis are particularly valuable for understanding how brain regions coordinate, an essential factor in diagnosing and treating neurological disorders.</p><p id="p0040">In summary, while TF representation methods provide specific frequency analysis, brain connectivity approaches excel in delivering comprehensive insights into the networked activity of the brain, leading to more significant discoveries in EEG research.</p><sec id="sec1.1"><label>1.1</label><title>Purpose of this study</title><p id="p0045">The purpose of this study stems from the critical role emotions play in shaping human experiences, influencing cognitive processes, behaviors, and social dynamics. Accurately recognizing and understanding emotions is essential among different domains, including marketing, psychology and artificial intelligence. Emotion recognition technologies, particularly those utilizing EEG and ECG, offer profound insights into the physiological basis of emotions, enhancing applications in customer service, AI development, and mental health diagnostics. The study aims to advance these technologies by leveraging the capabilities of CNNs to process complex EEG data. CNNs are favored for their effectiveness in extracting meaningful patterns from high-dimensional data without extensive manual intervention. This research further explores the potential of multi-channel EEG approaches, such as brain connectivity analysis, over traditional single-channel time-frequency representation methods. By focusing on the interactions across multiple brain regions, this study seeks to provide a more comprehensive understanding of neural dynamics, ultimately contributing to improved diagnostic and therapeutic strategies for neurological disorders.</p></sec><sec id="sec1.2"><label>1.2</label><title>Contribution of this study</title><p id="p0050">Contribution of this study is as follows:<list list-type="simple" id="ulist0010"><list-item id="u0010"><label>-</label><p id="p0055">Estimate coupling of information flow between multi-channels of ECG and EEG signals while emotional processing task. In this regards, three EC method named partial directed coherence (PDC), Graner causality (GC) and direct transfer function (DTF) are extracted from 3 ECG channels and 32 EEG channels of four emotions (happy, fear, disgust and sad) from the public and well-known MAHNOB-HCI database. In our knowledge, this is the first time that ECG is added to EEG to estimate emotional connectivity from MAHNOB-HCI database.</p></list-item><list-item id="u0015"><label>-</label><p id="p0060">Extract deep features from incorporation of EEG-ECG windows and represent the novel interaction multichannel EC images (IMEC) in four mentioned emotional classes.</p></list-item></list></p></sec><sec id="sec1.3"><label>1.3</label><title>Related studies</title><p id="p0065">Li et al. [<xref rid="bib26" ref-type="bibr">26</xref>] computed the phase locking value (PLV), a functional connectivity (FC) metric, from 32 channels across DEAP and MAHNOB-HCI databases. They discriminate emotional classes&#x02014; positive, neutral and negative &#x02014;using the Graph Regularized Extreme Learning Machine and SVM techniques. Their approach achieved classification accuracies of 68&#x000a0;% for the MAHNOB-HCI dataset and 62&#x000a0;% for the DEAP dataset.</p><p id="p0070">Moon et al. [<xref rid="bib27" ref-type="bibr">27</xref>] introduced a CNN framework to capture neural activity patterns through a matrix of connectivity generated from three methods of transfer entropy (TE), phase locking value (PLV) and Pearson correlation coefficient (PCC). To enhance performance, the connectivity matrix was structured using two strategies to evaluate the resemblance and variation between EEG signals from various spatial regions. The system was investigated on EEG data from the DEAP dataset, achieving an accuracy value of 80.73&#x000a0;% with the PLV metric. The findings also revealed that the effectiveness of emotional video classification strongly depends on how accurately the connectivity matrix represents emotional valence, emphasizing that valence-specific connectivity improves classification outcomes.</p><p id="p0075">Bagherzadeh et al. in Ref. [<xref rid="bib9" ref-type="bibr">9</xref>] used two EC techniques of PDC and direct directed transfer function (dDTF) to discriminate five emotions. They achieved the average accuracy of 99.43&#x000a0;% and 96.26&#x000a0;% on MAHNOB-HCI dataset using ResNet-50 on dDTF and PDC images of alpha frequency band, respectively. This underscores the efficiency of the EC-based CNN model in detecting emotions from EEG signals. In Ref. [<xref rid="bib8" ref-type="bibr">8</xref>], brain connectivity was assessed using PDC across five frequency bands to generate EC images. These images were then input into six different CNN models: ResNet-18, AlexNet, ShuffleNet, DarkNet-19, Xception and Inception-V3. The ResNet-18 model, achieved the best accuracy of 94.27&#x000a0;% on DEAP and 95.25&#x000a0;% for MAHNOB-HCI dataset. In Ref. [<xref rid="bib11" ref-type="bibr">11</xref>] three EC methods of PDC, TE and dDTF are fused to created specific image to be used for combined pre-trained CNN models with long-short term memory (LSTM) models. Finally, they achieved the accuracy of 98.76&#x000a0;% and 98.86&#x000a0;% to classify emotions from the DEAP and MAHNOB-HCI datasets, respectively. Also, these methods are used to detect depression [<xref rid="bib28" ref-type="bibr">28</xref>], schizophrenia [<xref rid="bib29" ref-type="bibr">29</xref>] and mental workload [<xref rid="bib30" ref-type="bibr">30</xref>] from EEG signals. For example, Mirjebreili et al. [<xref rid="bib28" ref-type="bibr">28</xref>], generated EC images to feed hybrid pre-trained CNNs such as EfficientnetB0 and LSTM to predict response of MDD patients to treatment with selective serotonin reuptake inhibitors (SSRIs) antidepressants. Accuracy of this study was higher than 98&#x000a0;% on 30 Major Depressive Disorder (MDD) patients. This approach offers significant potential as a diagnostic tool for early MDD diagnosis and treatment planning. Bagherzadeh and Shalbaf in Ref. [<xref rid="bib29" ref-type="bibr">29</xref>], estimated three EC measures of PDC, dDTF and TE and generate a fused image to feed pre-trained CNNs. They detect schizophrenia from EEG signals with the accuracy of higher than 95&#x000a0;%. Safari et al. in Ref. [<xref rid="bib30" ref-type="bibr">30</xref>], estimated dDTF connectivity measure from EEG signals during low-task and high-task in mental workload. Then, selected best features by the minimum-redundancy-maximum-relevance (mRMR) technique and applied support vector machine (SVM) and achieved the accuracy of 89.53&#x000a0;%.</p><p id="p0080">In our previous study [<xref rid="bib31" ref-type="bibr">31</xref>], we have estimated GC from EEG-ECG with respect to eye-tracking data as gate. Then, we have generated images to feed to ResNet-18 and classify sadness and happiness from MAHNOB-HCI database [<xref rid="bib32" ref-type="bibr">32</xref>]. The mean accuracy and area under the curve (AUC) achieved 91&#x000a0;% and 0.97, respectively. In this study, we consider more specific connectivity measures (PDC and DTF) along with GC without eye-tracking data. Also, we increase emotional classes from 2 to 4, sadness, happiness, fear and disgust.</p></sec></sec><sec id="sec2"><label>2</label><title>Material and methods</title><sec id="sec2.1"><label>2.1</label><title>MAHNOB-HCI dataset</title><p id="p0085">This dataset represents a groundbreaking collection of EEG and ECG recordings obtained from 27 (16 female/11 male) various healthy participants. Data were captured over 32 channels as the participants viewed 20 distinct video clips [<xref rid="bib32" ref-type="bibr">32</xref>]. The group included varying educations, ranging in age from 19 to 40 years (mean age&#x000a0;=&#x000a0;26.06, SD&#x000a0;=&#x000a0;4.39). Recordings adhered to local regulations and institutional protocols at Imperial College London, negating the need for ethical review and approval. EEG signals were collected using the advanced Biosemi Active II system, adhering to the internationally accepted 10&#x02013;20 electrode placement standard, with a sampling frequency of 256&#x000a0;Hz. The electrodes were positioned at the following sites: Fp1, Af3, F3, F7, Fc6, Fc2, Cz, C4, Cp6, Cp2, Fc5, Fc1, P3, P7, P4, P8, Po3, O1, Oz, Pz, Fp2, C3, T7, Cp5, Cp1, Af4, Fz, F4, F8, T8, Po4, and O2. Additionally, three ECG leads were recorded concurrently from the upper right and left chest, and lower left abdomen, all sampled at the same frequency.</p><p id="p0090">A distinctive three-phase protocol was devised to provoke specific emotional reactions. The procedure started with a neutral video to minimize any existing emotional conditions, continued with the display of emotionally stimulating clips, and ended with a self-report task. Each session spanned roughly 50&#x000a0;min, with personal evaluations lasting about two and a half minutes. The video segments varied in length, ranging from 34 to 117&#x000a0;s, ensuring participants remained attentive and experienced diverse emotional states. Participants assessed their emotions based on arousal and valence scales, as well as provided descriptive labels. For this research, samples were categorized by arousal and valence levels, ultimately centering on four emotions: happiness, disgust, fear, and sadness.</p></sec><sec id="sec2.2"><label>2.2</label><title>Proposed ER system using inter connectivity of EEG-ECG</title><p id="p0095">In this study, the interplay between multiple ECG signals and EEG signals are considered in four basic emotional classes. These interactions are estimated through EC methods. Comparison of these methods show which one is more effective in recognition of these emotional classes. Flowchart of this study is shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. First, both signals are filtered to remove noise and artifacts. Then, sampling frequency is reduced to lower computational burden and costs. After that, reference of EEG channels is changed by mean of all ones. After passing the preprocessing block, the EC measures are estimated. To estimate DTF, PDC and GC measures, stationarity of signals must be satisfied, therefore, signals are segmented into smaller parts. After that, each mentioned EC is estimated using a particular order of model and validated through defined test. Then, PDC, GC and DTF are visualized in color scales as images. Images are split into five-folds and two quick and high performance (MobileNetV2 and ResNet-18) pre-trained CNN models are re-trained. This approach is TL, as the knowledge from pre-trained CNN models is applied to our new task. This process involves replacing the fully connected layer with a new one containing 4 neurons, corresponding to the number of emotions in this study. Additionally, the classifier layer associated with the previous fully connected layer is substituted with a new one.<fig id="fig1"><label>Fig. 1</label><caption><p>Flowchart of proposed IMEC-TL method to recognize four emotional classes.</p></caption><alt-text id="alttext0010">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p></sec><sec id="sec2.3"><label>2.3</label><title>Estimate effective connectivity</title><p id="p0100">EC serves as a valuable approach for examining multiple EEG signals to uncover the relationships between brain channels, enabling the differentiation of various brain states. Each EC technique evaluates these interactions through its unique perspective. GC, DTF and PDC are the most used EC methods in emotion recognition [<xref rid="bib8" ref-type="bibr">8</xref>,<xref rid="bib9" ref-type="bibr">9</xref>,<xref rid="bib11" ref-type="bibr">11</xref>], detection of schizophrenia [<xref rid="bib33" ref-type="bibr">33</xref>], object detection [<xref rid="bib34" ref-type="bibr">34</xref>], detection of epilepsy [<xref rid="bib35" ref-type="bibr">35</xref>] and vegetative state patients [<xref rid="bib36" ref-type="bibr">36</xref>]. These three methods are employed here to uncover the interaction between three ECG channels and 32 EEG channels across four emotional states: fear, happiness, sadness, and disgust.</p><sec id="sec2.3.1"><label>2.3.1</label><title>Classical linear GC</title><p id="p0105">During the simultaneous examination of two signals, denoted as <inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M2" altimg="si2.svg"><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the second signal is considered to have a causal effect on the first if incorporating its historical data significantly enhances the prediction accuracy of the first signal compared to using only its own past information [<xref rid="bib36" ref-type="bibr">36</xref>]. Within the framework of a univariate autoregressive (AR) model, this association is expressed by Eq <xref rid="fd1" ref-type="disp-formula">(1)</xref>, where <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the coefficients of the model (commonly estimated via the least squares approach), <inline-formula><mml:math id="M4" altimg="si4.svg"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> indicates the order of the autoregressive (AR) model, and <inline-formula><mml:math id="M5" altimg="si5.svg"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the residuals. Notably, the forecast for each signal (<inline-formula><mml:math id="M6" altimg="si6.svg"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M7" altimg="si7.svg"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula>) depends solely on its own past values [<xref rid="bib37" ref-type="bibr">37</xref>].<disp-formula id="fd1"><label>(1)</label><mml:math id="M8" altimg="si8.svg" alttext="Equation 1."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">&#x02212;</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mrow><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">&#x02212;</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mrow><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="p0110">The GC from y to x is calculate by Eq <xref rid="fd2" ref-type="disp-formula">(2)</xref> as follow:<disp-formula id="fd2"><label>(2)</label><mml:math id="M9" altimg="si9.svg" alttext="Equation 2."><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="badbreak" stretchy="true">&#x02192;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x0203e;</mml:mo></mml:mover></mml:mrow></mml:msub><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x0203e;</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="p0115">The variances of these residuals are determined as <inline-formula><mml:math id="M10" altimg="si10.svg"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x0203e;</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M11" altimg="si11.svg"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x0203e;</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The GC metric ranges from zero to infinity. As highlighted earlier, GC provides the benefit of asymmetry, making it suitable for detecting directional interactions. However, being a linear parametric approach, it relies on an autoregressive model with a specified order <inline-formula><mml:math id="M12" altimg="si4.svg"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p id="p0120">For this research, GC was computed using MATLAB 2023b in combination with the HERMES connectivity toolbox, with the autoregressive model order set to 10.</p></sec><sec id="sec2.3.2"><label>2.3.2</label><title>PDC</title><p id="p0125">PDC is a method used to determine the directional flow of information between two EEG signals within the frequency domain. To estimate the PDC from the <inline-formula><mml:math id="M13" altimg="si12.svg"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> th EEG channel to the <inline-formula><mml:math id="M14" altimg="si13.svg"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> th channel at a specific frequency <inline-formula><mml:math id="M15" altimg="si14.svg"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula>, Eq <xref rid="fd3" ref-type="disp-formula">(3)</xref> is employed, as referenced in literature [<xref rid="bib34" ref-type="bibr">34</xref>,<xref rid="bib38" ref-type="bibr">[38]</xref>, <xref rid="bib39" ref-type="bibr">[39]</xref>, <xref rid="bib40" ref-type="bibr">[40]</xref>]:<disp-formula id="fd3"><label>(3)</label><mml:math id="M16" altimg="si15.svg" alttext="Equation 3."><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>&#x02217;</mml:mo></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula>In this equation, <inline-formula><mml:math id="M17" altimg="si16.svg"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the frequency-specific autoregressive coefficient from matrix A (denoted by <inline-formula><mml:math id="M18" altimg="si17.svg"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>), which is determined by Eq <xref rid="fd4" ref-type="disp-formula">(4)</xref>:<disp-formula id="fd4"><label>(4)</label><mml:math id="M19" altimg="si18.svg" alttext="Equation 4."><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">&#x02212;</mml:mo><mml:mrow><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>j</mml:mi><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>f</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p id="p0130">Similarly, <inline-formula><mml:math id="M20" altimg="si4.svg"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> stands for the order of the model, here we preset it to 10. The PDC is calculated using MATLAB (version 2023b) with the HERMES connectivity toolbox.</p></sec><sec id="sec2.3.3"><label>2.3.3</label><title>DTF</title><p id="p0135">DTF is estimated similarly as PDC [<xref rid="bib37" ref-type="bibr">37</xref>]. Eq <xref rid="fd5" ref-type="disp-formula">(5)</xref> estimate DTF. <inline-formula><mml:math id="M21" altimg="si19.svg"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula> is the transfer function matrix H (PDC uses those of &#x01fb9;). Its value is ranging from zero (no coupling between the two EEG signals) to one (complete coupling of the two EEG signals).<disp-formula id="fd5"><label>(5)</label><mml:math id="M22" altimg="si20.svg" alttext="Equation 5."><mml:mrow><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mi>H</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula>In a similar manner, DTF was calculated using MATLAB (version 2023b) via HERMES connectivity toolbox, with the <inline-formula><mml:math id="M23" altimg="si4.svg"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> fixed at 10.</p></sec></sec><sec id="sec2.4"><label>2.4</label><title>Transfer learning and pre-trained CNNs</title><p id="p0140">Transfer learning is extensively applied in deep learning, leveraging pre-trained models as a starting point for addressing new challenges. Adapting a network through transfer learning is typically quicker and requires fewer resources compared to building a model from the ground up with randomly initialized parameters. This approach facilitates the effective utilization of pre-learned features for a novel task, even when dealing with a constrained set of training images [<xref rid="bib8" ref-type="bibr">[8]</xref>, <xref rid="bib9" ref-type="bibr">[9]</xref>, <xref rid="bib10" ref-type="bibr">[10]</xref>, <xref rid="bib11" ref-type="bibr">[11]</xref>,<xref rid="bib41" ref-type="bibr">41</xref>].</p><p id="p0145">With the variety of transfer learning models available, it is crucial to assess trade-offs and ensure compatibility with the study's objectives. After thorough experimentation to determine the most appropriate models for our needs, MobileNet-v2 [<xref rid="bib42" ref-type="bibr">42</xref>] and ResNet-18 [<xref rid="bib43" ref-type="bibr">43</xref>] were chosen for their optimal balance between storage efficiency, computational demands, and practical applicability. ResNet-18 includes an initial 7&#x000a0;&#x000d7;&#x000a0;7 convolutional layers, followed by eight sequential residual blocks (each consisting of two convolutional layers with a filter size of 3&#x000a0;&#x000d7;&#x000a0;3 and a shortcut connection), and ends with a fully connected layer comprising 1000 neurons. Its structure is depicted in <xref rid="fig2" ref-type="fig">Fig. 2</xref>(a). MobileNet-v2 employs clipped rectified linear units (cReLU) with a value of 6 and builds upon residual blocks by incorporating depthwise convolutional layers. For detailed information about both CNN architectures, refer to Refs. [<xref rid="bib42" ref-type="bibr">42</xref>,<xref rid="bib43" ref-type="bibr">43</xref>]. <xref rid="fig2" ref-type="fig">Fig. 2 (b)</xref> illustrate simple structure of MobileNetV2.<fig id="fig2"><label>Fig. 2</label><caption><p>Simplified structure of (a) ResNet-18 and (b) MobileNetV2.</p></caption><alt-text id="alttext0015">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p></sec><sec id="sec2.5"><label>2.5</label><title>Evaluation statistics</title><p id="p0150">A 5-fold CV approach is employed to evaluate the classification performance of our IMEC-CNN method. For each EC category, the images are divud into 4 folds for re-training the two pre-trained CNNs and 1-fold for testing. This process is repeated five times, ensuring that every image is included in the testing phase exactly once. The mean accuracy and AUC are calculated as performance metrics using Eq <xref rid="fd6" ref-type="disp-formula">(6)</xref> and Eq <xref rid="fd7" ref-type="disp-formula">(7)</xref> for the 4-class emotion recognition task, with AUC computed in a binary format [<xref rid="bib44" ref-type="bibr">44</xref>].<disp-formula id="fd6"><label>(6)</label><mml:math id="M24" altimg="si21.svg" alttext="Equation 6."><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mi>l</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd7"><label>(7)</label><mml:math id="M25" altimg="si22.svg" alttext="Equation 7."><mml:mrow><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mi>C</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo linebreak="badbreak">+</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Where, in Eq <xref rid="fd6" ref-type="disp-formula">(6)</xref>, <inline-formula><mml:math id="M26" altimg="si23.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M27" altimg="si24.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> the true positive and true negative for 4 classes (<inline-formula><mml:math id="M28" altimg="si25.svg"><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="true">)</mml:mo><mml:mtext>.</mml:mtext></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="M29" altimg="si26.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M30" altimg="si27.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are false positive and false negative for four emotional classes. In Eq <xref rid="fd7" ref-type="disp-formula">(7)</xref>, TP as the true positive, means correct decisions of class 1 against other three emotional classes, and TN as the true negative is true decisions of other three emotional classes against class 1. FN as false negative means wrong decision of three emotional classes against class 1 and FP as false positive means wrong decision of class 1 against other three emotional classes. This process is repeated for each four emotional classes to be as positive class.</p></sec></sec><sec id="sec3"><label>3</label><title>Results</title><p id="p0155">32 EEGs and 3 ECGs from the MAHNOB-HCI (27 participants) dataset underwent initial preprocessing via the EEGLAB toolbox within MATLAB software. Samples from three participants were removed due to technical mistakes. To deduce the GC, PDC and DTF measures among the cleaned EEGs from the MAHNOB-HCI (24 participants) database the procedure was employed. The window length was determined by the variance ratio test to ensure the selection of stationary segments. A consistent 5000&#x000a0;ms sliding window (with 50&#x000a0;% overlap) was chosen across all database to create a substantial number of images for models of pre-trained CNNs. The MVAR model order of 10, guided by the Akaike Information Criterion (AIC) [<xref rid="bib12" ref-type="bibr">12</xref>], was uniformly used. The suitability of the MVAR was then verified through assessments of consistency, stability and residual whiteness measures [<xref rid="bib12" ref-type="bibr">12</xref>]. Following these validations, GC, DTF and PDC EC measures were computed within five recognized frequency bands&#x02014;theta, delta, beta, alpha, and gamma&#x02014;using the 5000&#x000a0;ms windows. The resultant GC, DTF and PDC, represented as 35&#x000a0;&#x000d7;&#x000a0;35 matrices was used as an image for input into various pre-trained CNNs. Consequently, 2853 (sadness), 4391 (happiness), 2281 (fear) and 2750 (disgust), EC images were generated per frequency band from the MAHNOB-HCI database. <xref rid="fig3" ref-type="fig">Fig. 3</xref> present DTF images (left), GC images (middle) and PDC images (right) from one participant across four emotions of happiness, disgust, fear and sadness in the best (alpha) frequency band from up to down, respectively. The color scales represent the transfer of data, where smaller values appear in cooler shades of blue, and larger values are shown in warmer hues of red. The first 32 arrays in each axis are for EEG (PO3, O1, C4,F7, FC5, FC1, Fp1, CP1, FC2, Pz, Fp2, AF4, Cz, T8, CP6, AF3, P3, P7, Oz, F8, CP5, FC6, F3, C3, Fz, F4, CP2, P4, P8, PO4, O2, T7) and the last three are for ECG (EXG1-3 are upper right and left corners of chest and left of abdomen, respectively) channels. Based on <xref rid="fig3" ref-type="fig">Fig. 3</xref>, DTF images illustrate the most interconnections between EEG and ECG than GC and PDC. Also, for this kind of image, these interactions are most in happiness (<xref rid="fig3" ref-type="fig">Fig. 3 (a)</xref>) and disgust (<xref rid="fig3" ref-type="fig">Fig. 3 (b)</xref>). Moreover, based on DTF images, there are high interactions between the third ECG signal and most EEG channels for happiness (right side of <xref rid="fig3" ref-type="fig">Fig. 3 (a)</xref>) and disgust (right side of <xref rid="fig3" ref-type="fig">Fig. 3 (b)</xref>)) emotional class than fear (right side of <xref rid="fig3" ref-type="fig">Fig. 3 (c)</xref>)) and sadness (right side of <xref rid="fig3" ref-type="fig">Fig. 3 (d)</xref>).<fig id="fig3"><label>Fig. 3</label><caption><p>An example of DTF (left), GC (middle) and PDC (right) images for one subject at four emotions of (a) happiness, (b) disgust, (c) fear and (d) sadness. The color scales represent the information flow, where cooler blue shades correspond to lower values and warmer red hues indicate higher values. The initial 32 arrays along each axis correspond to EEG, while the final three represent ECG channels positioned at the upper right chest, upper left chest, and abdomen left side.</p></caption><alt-text id="alttext0020">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p><p id="p0160">Subsequently, a comprehensive fine-tuning of two advanced pre-trained CNN models, SqueezeNet and ResNet-18 was undertaken using these images from the training set (4-folds) to identify four mentioned emotional classes, with test performed on 1-fold from the test set. Following this, average accuracy and AUC performance metrics were calculated for the test set images. This process was iterated for 5-times, thereby treating the images of each fold as unseen data to classify the four designated emotional classes. The overall performance was summarized using the average and standard deviation of the computed metrics. The training process utilized the cross-entropy loss function. Also, the adaptive moment estimation (ADAM) optimizer [<xref rid="bib45" ref-type="bibr">45</xref>] was used. Key training hyperparameters featured a starting learning rate of 0.0008, a mini-batch size of 32, a gradient decay factor of 0.99 and a maximum of 30 epochs. All implementations and computations were performed using MATLAB 2023b on a system equipped with an Intel(R) Core(TM) i7-10610U processor (1.80&#x000a0;GHz) and an Nvidia Quadro P520 graphics card.</p><p id="p0165">As mentioned in section <xref rid="sec2.4" ref-type="sec">2.4</xref>, the first convolutional layer of ResNet-18 extracts 64 feature maps for an input image. Activations of this layer for the happy-alpha-DTF image is shown in <xref rid="fig4" ref-type="fig">Fig. 4 (a)</xref>. As it can be observed, all of these feature maps are not appropriate. <xref rid="fig4" ref-type="fig">Fig. 4 (b)</xref> shows activation of the specific last layers name &#x02018;pool5&#x02019; for that image.<fig id="fig4"><label>Fig. 4</label><caption><p>(a) 64 extracted deep feature maps from first convolutional layer (conv1) and (b) pool5 layer of ResNet-18 on a DTF images from sadness for one subject. This image illustrate that each filter is not appropriate.</p></caption><alt-text id="alttext0025">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig></p><p id="p0170"><xref rid="fig5" ref-type="fig">Fig. 5</xref> shows the results of the gradient-weighted class activation mapping (Grad-CAM) method applied to alpha-DTF images for the sadness class (left side of <xref rid="fig3" ref-type="fig">Fig. 3</xref>). Grad-CAM generates a gradient-weighted class activation map that shows how changes in the classification score of input X affect the network's evaluation of a specific class [<xref rid="bib46" ref-type="bibr">46</xref>]. This tool is useful for elucidating how a network makes its predictions, ensuring it concentrates on the relevant segments of the data. This technique leverages the gradients of the network's classification score relative to its last convolutional feature map. Regions in the data that have high values on the Grad-CAM map are critical in influencing the network's score for a particular class. Therefore, <xref rid="fig5" ref-type="fig">Fig. 5</xref> shows how ResNet-18 decides which belongs to each emotional class. Also, this figure reveals that interaction of EEG and ECG channels are effective on the final decision about recognition of emotions. For example, for disgust class, the ResNet-18 observes upper right side of image, for happy observes lower right side of DTF, for fear observes middle right side and for sad observes bigger parts from right side, from up to down right side. According to <xref rid="fig5" ref-type="fig">Fig. 5 (a)</xref> the second and third ECG channels (EXG2 and EXG3) are coupled with all EEG channels. Also, most of EEG channels are coupled with EXG3 according to <xref rid="fig5" ref-type="fig">Fig. 5 (b)</xref>. According to <xref rid="fig5" ref-type="fig">Fig. 5 (c)</xref>, there is low coupling between ECG and EEG channels in fear. Two channels from the frontal, i.e., Fp2 and Fp1, are connected with all EEG and ECG channels during sadness (<xref rid="fig5" ref-type="fig">Fig. 5 (d)</xref>).<fig id="fig5"><label>Fig. 5</label><caption><p>An example from Grad-Cam activation functions maps of ResNet-18 for alpha (best) frequency band on alpha-DTF images (a) disgust, (b) happiness, (c) fear and (d) sadness for a subject.</p></caption><alt-text id="alttext0030">Fig. 5</alt-text><graphic xlink:href="gr5"/></fig></p><p id="p0175"><xref rid="fig6" ref-type="fig">Fig. 6</xref> shows topographic images of DTF at alpha frequency band for disgust (a), happiness (b), fear (c) and sadness (d) for 3rd participant. Duration of signal in this image is 5&#x000a0;s. This pattern of interconnection through EEG and ECG was alike for majority of participants. As evident from <xref rid="fig6" ref-type="fig">Fig. 6</xref>(a) and (b), there are coupling between Fp1, Fp2 and Fc1 EEG channels and EXG3 channel (ECG). <xref rid="fig6" ref-type="fig">Fig. 6 (c) and 6 (d)</xref> reveals no interaction between ECG and EEG channels in this moment.<fig id="fig6"><label>Fig. 6</label><caption><p>Topographic representation of DTF at alpha frequency band for disgust (a), happiness (b), fear (c) and sadness (d) for 3rd subject at one-five second segment.</p></caption><alt-text id="alttext0035">Fig. 6</alt-text><graphic xlink:href="gr6"/></fig></p><p id="p0180"><xref rid="tbl1" ref-type="table">Table 1</xref>, <xref rid="tbl2" ref-type="table">Table 2</xref> report accuracies obtained using the ResNET-18 and MobileNetV2 on all kinds of IMEC images per frequency bands, respectively. Based on <xref rid="tbl1" ref-type="table">Table 1</xref>, the highest accuracy obtained via alpha-DTF and ResNet-18 equal to 97.34 with standard deviation of 1.19. After that, beta and gamma frequency bands for DTF images using this CNN achieved high accuracies of 95.94&#x000a0;% and 95,26&#x000a0;%, respectively.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Average accuracy (mean&#x000a0;&#x000b1;&#x000a0;std) of ResNet-18 on IMEC images from five frequency bands.</p></caption><alt-text id="alttext0050">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Image</bold></th><th><bold>BAND</bold></th><th><bold>Accuracy (%)(mean&#x000a0;&#x000b1;&#x000a0;std)</bold></th></tr></thead><tbody><tr><td rowspan="5" align="left">PDC</td><td align="left">Delta</td><td align="char">84.65&#x000a0;&#x000b1;&#x000a0;3.45</td></tr><tr><td align="left">Theta</td><td align="char">86.35&#x000a0;&#x000b1;&#x000a0;3.62</td></tr><tr><td align="left">Alpha</td><td align="char">88.54&#x000a0;&#x000b1;&#x000a0;2.07</td></tr><tr><td align="left">Beta</td><td align="char">86.76&#x000a0;&#x000b1;&#x000a0;2.80</td></tr><tr><td align="left">gamma</td><td align="char">86.45&#x000a0;&#x000b1;&#x000a0;2.65</td></tr><tr><td rowspan="5" align="left">GC</td><td align="left">Delta</td><td align="char">85.53&#x000a0;&#x000b1;&#x000a0;3.87</td></tr><tr><td align="left">Theta</td><td align="char">86.08&#x000a0;&#x000b1;&#x000a0;3.65</td></tr><tr><td align="left">Alpha</td><td align="char">89.54&#x000a0;&#x000b1;&#x000a0;2.12</td></tr><tr><td align="left">Beta</td><td align="char">87.56&#x000a0;&#x000b1;&#x000a0;3.56</td></tr><tr><td align="left">gamma</td><td align="char">87.28&#x000a0;&#x000b1;&#x000a0;2.76</td></tr><tr><td rowspan="5" align="left">DTF</td><td align="left">Delta</td><td align="char">88.87&#x000a0;&#x000b1;&#x000a0;3.63</td></tr><tr><td align="left">Theta</td><td align="char">90.69&#x000a0;&#x000b1;&#x000a0;2.98</td></tr><tr><td align="left">Alpha</td><td align="left">97.34&#x000a0;&#x000b1;&#x000a0;1.19</td></tr><tr><td align="left">Beta</td><td align="char">95.94&#x000a0;&#x000b1;&#x000a0;1.57</td></tr><tr><td align="left">gamma</td><td align="char">95.26&#x000a0;&#x000b1;&#x000a0;2.86</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Average accuracy of MobileNetV2 on IMEC images from five frequency bands.</p></caption><alt-text id="alttext0055">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Image</bold></th><th><bold>BAND</bold></th><th><bold>Accuracy (%)(mean&#x000a0;&#x000b1;&#x000a0;std)</bold></th></tr></thead><tbody><tr><td rowspan="5" align="left">PDC</td><td align="left">Delta</td><td align="char">82.89&#x000a0;&#x000b1;&#x000a0;3.01</td></tr><tr><td align="left">Theta</td><td align="char">86.12&#x000a0;&#x000b1;&#x000a0;2.28</td></tr><tr><td align="left">Alpha</td><td align="char">87.29&#x000a0;&#x000b1;&#x000a0;3.01</td></tr><tr><td align="left">Beta</td><td align="char">84.92&#x000a0;&#x000b1;&#x000a0;2.30</td></tr><tr><td align="left">gamma</td><td align="char">83.47&#x000a0;&#x000b1;&#x000a0;3.25</td></tr><tr><td rowspan="5" align="left">GC</td><td align="left">Delta</td><td align="char">84.22&#x000a0;&#x000b1;&#x000a0;3.20</td></tr><tr><td align="left">Theta</td><td align="char">85.38&#x000a0;&#x000b1;&#x000a0;2.80</td></tr><tr><td align="left">Alpha</td><td align="char">89.09&#x000a0;&#x000b1;&#x000a0;3.24</td></tr><tr><td align="left">Beta</td><td align="char">88.13&#x000a0;&#x000b1;&#x000a0;2.22</td></tr><tr><td align="left">gamma</td><td align="char">86.63&#x000a0;&#x000b1;&#x000a0;2.60</td></tr><tr><td rowspan="5" align="left">DTF</td><td align="left">Delta</td><td align="char">86.72&#x000a0;&#x000b1;&#x000a0;3.63</td></tr><tr><td align="left">Theta</td><td align="char">89.28&#x000a0;&#x000b1;&#x000a0;3.09</td></tr><tr><td align="left">Alpha</td><td align="left"><bold>96.53</bold>&#x000a0;&#x000b1;&#x000a0;<bold>3.54</bold></td></tr><tr><td align="left">Beta</td><td align="char">94.78&#x000a0;&#x000b1;&#x000a0;2.03</td></tr><tr><td align="left">gamma</td><td align="char">93.83&#x000a0;&#x000b1;&#x000a0;3.26</td></tr></tbody></table></table-wrap></p><p id="p0185">Based on <xref rid="tbl2" ref-type="table">Table 2</xref>, the highest accuracy value obtained using alpha-DTF images and MobileNetV2 equal to 96.53&#x000a0;% (std&#x000a0;=&#x000a0;&#x000b1; 3.54). After that, beta and gamma frequency bands for DTF images using this CNN achieved high accuracies of 94.78&#x000a0;% and 93.83&#x000a0;%, respectively. <xref rid="fig7" ref-type="fig">Fig. 7 (a)</xref> and (b) shows ROC for best result, alpha-DTF-ResNet-18 and the second-best model, MobileNetV2, respectively. Based on <xref rid="fig7" ref-type="fig">Fig. 7 (a)</xref>, AUC for happiness, sadness, fear and disgust for this setup are 0.92, 0.91, 0.99 and 0.91. According to <xref rid="fig7" ref-type="fig">Fig. 7 (b)</xref>, MobileNetV2 achieved the AUC values of 0.97, 0.81, 0.80 and 0.77 for fear, happy, disgust and sad, respectively.<fig id="fig7"><label>Fig. 7</label><caption><p>ROC curves for DTF images of alpha frequency band using (a) ResNet-18 (winner model) and (b) MobileNetv2 (runner model).</p></caption><alt-text id="alttext0040">Fig. 7</alt-text><graphic xlink:href="gr7"/></fig></p><p id="p0190"><xref rid="fig8" ref-type="fig">Fig. 8 (a) and 8 (b)</xref> illustrates progress of training based on accuracy (percent) and loss function across epochs, respectively. This figure shows the result of training ResNet-18 on DTF from the alpha (optimal) frequency band. Observing this figure demonstrate superiority of ResNet-18 against MobileNetV2 based on accuracy and loss values.<fig id="fig8"><label>Fig. 8</label><caption><p>(a) Accuracy and (b) loss curves for 30 epochs for ResNet-18 (blue color) and MobileNetV2 (brown color) on training set of alpha-DTF images.</p></caption><alt-text id="alttext0045">Fig. 8</alt-text><graphic xlink:href="gr8"/></fig></p></sec><sec id="sec4"><label>4</label><title>Discussion</title><p id="p0195">The aim of this study is developing an ER system based on interaction of multichannel EEG and ECG signals in point of connectivity measures. To our knowledge, this is the first time that connectivity measures are estimated jointly from EEG and ECG for ER domain. EEG and photoplethysmogram (PPG) has been used to calculate both gPDC and dDTF during deception in interview [<xref rid="bib47" ref-type="bibr">47</xref>,<xref rid="bib48" ref-type="bibr">48</xref>].</p><p id="p0200">The coupling of ECG and EEG measurements in emotion processing tasks provides a more comprehensive perception of the physiological basis of emotions. Latest study highlights the potential of combining these two modalities to achieve a richer, multidimensional insight into emotional states. EEG, which measures brain electrical activity, and ECG, which records the heart's electrical signals, are both influenced by emotional stimuli. EEG captures the brain's direct response to emotional inputs through various frequency bands like alpha, beta, and gamma, which have been linked to different emotional processes. In contrast, ECG provides data on the autonomic nervous system's response to emotions. Moreover, integrating ECG and EEG is beneficial for instantaneous emotion classification frameworks, such as those used in mental health monitoring and neuromarketing. These systems benefit from the rapid response detection of EEG and the stability and simplicity of ECG measurements, making them more robust and user-friendly in everyday applications. The continuous advancements in sensor technology and data analysis techniques are likely to further enhance the efficacy and practicality of EEG-ECG combined studies in emotion research, offering deeper undertesting about the intricate relationship between the brain and the heart in emotions.</p><p id="p0205">Comparing three kinds of EC images based on <xref rid="tbl1" ref-type="table">Table 1</xref>, <xref rid="tbl2" ref-type="table">Table 2</xref>, DTF outperforms the other two. DTF's advantage lies in its comprehensive frequency-domain analysis, ability to normalize interactions, and distinguishing direct versus indirect influences, making it particularly valuable in detailed network connectivity analyses in neuroscience and other fields that handle complex signal systems.</p><p id="p0210">Our analysis revealed that the alpha frequency wave stands out as the most significant among various frequency bands, based on <xref rid="tbl1" ref-type="table">Table 1</xref>, <xref rid="tbl2" ref-type="table">Table 2</xref> This frequency band yielded the highest accuracy in recognizing four specific emotional classes from MAHNOB-HCI database, using highly optimized CNNs. Other frequency bands, such as beta, gamma, theta, and delta, followed in subsequent ranks. These results align with findings from other research [<xref rid="bib8" ref-type="bibr">8</xref>,<xref rid="bib15" ref-type="bibr">15</xref>,<xref rid="bib49" ref-type="bibr">49</xref>,<xref rid="bib50" ref-type="bibr">50</xref>], which indicate that the alpha, beta, and gamma bands are closely linked to emotional processes. Furthermore, multiple psychological studies have emphasized the alpha band's key role in emotional processing, noting its association with relaxed states [<xref rid="bib51" ref-type="bibr">[51]</xref>, <xref rid="bib52" ref-type="bibr">[52]</xref>, <xref rid="bib53" ref-type="bibr">[53]</xref>, <xref rid="bib54" ref-type="bibr">[54]</xref>].</p><p id="p0215">According to <xref rid="fig5" ref-type="fig">Fig. 5</xref>, ECG channels are coupled with most frontal EEG channels, some parietal (P8, Po4), central (Cp5, Cp1,C3) and temporal (T7). Frontal region has been introduced as the origin of emotional processing area in physiological studies. Also, these channels are similar with introduced channels in Refs. [<xref rid="bib7" ref-type="bibr">7</xref>,<xref rid="bib9" ref-type="bibr">9</xref>,<xref rid="bib10" ref-type="bibr">10</xref>].</p><p id="p0220">When comparing ResNet-18 and MobileNetV2 across all IMEC images, ResNet-18 outperforms MobileNetV2 in terms of performance. Although both CNNs utilize a residual unit design, MobileNetV2 incorporates a rectified linear unit (ReLU) clipped at 6. This modification results in fewer parameters, making it faster and more suitable for mobile applications. Additionally, MobileNetV2 employs depthwise convolution. Despite these advantages, as evidenced by the data in <xref rid="tbl1" ref-type="table">Table 1</xref>, <xref rid="tbl2" ref-type="table">Table 2</xref> and <xref rid="fig7" ref-type="fig">Fig. 7</xref>, ResNet-18 demonstrates superior accuracy and AUC values across all frequency bands.</p><sec id="sec4.1"><label>4.1</label><title>Comparison with recent researches on MAHNOB-HCI</title><p id="p0225"><xref rid="tbl3" ref-type="table">Table 3</xref> compares our results with recent ER researches on same dataset (MAHNOB-HCI). The discrepancies between this study and them lie in emotional classes, modalities and evaluation criteria. We have integrated the ECG modality into our ER framework to explore the interconnection between the brain and the heart through various emotions. Additionally, we propose a multimodal framework for recognizing emotional states by simultaneously processing both EEG and ECG modalities. We used the GC connectivity measure to analyze the information flow between brain regions and the heart during basic emotions.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Comparison of ER studies on MAHNOB-HCI database.</p></caption><alt-text id="alttext0060">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Ref</bold></th><th><bold>Modality</bold></th><th><bold>Method</bold></th><th><bold>Number of classes</bold></th><th><bold>Evaluation criterion</bold></th><th><bold>Accuracy (%) (m&#x000a0;&#x000b1;&#x000a0;std)</bold></th></tr></thead><tbody><tr><td align="left">[<xref rid="bib8" ref-type="bibr">8</xref>]</td><td align="left">EEG</td><td align="left">Alpha-PDC</td><td align="char">5</td><td align="left">LOSO<xref rid="tbl3fna" ref-type="table-fn">a</xref></td><td align="left">95.25&#x000a0;&#x000b1;&#x000a0;2.23</td></tr><tr><td align="left">[<xref rid="bib9" ref-type="bibr">9</xref>]</td><td align="left">EEG</td><td align="left">Alpha-dDTF, ResNet-50</td><td align="char">5</td><td align="left">10-fold CV</td><td align="left">96.26&#x000a0;&#x000b1;&#x000a0;0.53</td></tr><tr><td align="left">[<xref rid="bib11" ref-type="bibr">11</xref>]</td><td align="left">EEG</td><td align="left">Fusion of TE, PDC, dDTF, CNN-LSTM</td><td align="char">5</td><td align="left">LOSO</td><td align="left">98.86&#x000a0;&#x000b1;&#x000a0;0.57</td></tr><tr><td align="left">[<xref rid="bib55" ref-type="bibr">55</xref>]</td><td align="left">EEG</td><td align="left">MSST, deep CNN</td><td align="char">4</td><td align="left">Hold out</td><td align="left">88.17</td></tr><tr><td align="left">[<xref rid="bib56" ref-type="bibr">56</xref>]</td><td align="left">ECG</td><td align="left">1DCNN-LSTM</td><td align="char">2<xref rid="tbl3fnb" ref-type="table-fn">b</xref></td><td align="left">Hold out</td><td align="left">73.33 (valence), 60 (arousal)</td></tr><tr><td align="left">[<xref rid="bib57" ref-type="bibr">57</xref>]</td><td align="left">EEG</td><td align="left">Entropies, Sugeno-fuzzy inference system</td><td align="char">4</td><td align="left">Hold out</td><td align="left">85.20&#x000a0;%</td></tr><tr><td align="left"><bold>Ours</bold></td><td align="left">EEG-ECG</td><td align="left">IMEC, <bold>ResNet-18</bold>, MobileNetV2</td><td align="char">4</td><td align="left">5-fold CV</td><td align="left">97.34&#x000a0;&#x000b1;&#x000a0;1.19</td></tr></tbody></table><table-wrap-foot><fn id="tbl3fna"><label>a</label><p id="ntpara0010">LOSO&#x000a0;=&#x000a0;leave-one-subject-out.</p></fn></table-wrap-foot><table-wrap-foot><fn id="tbl3fnb"><label>b</label><p id="ntpara0015">valence (high valence/low valence), and arousal (high arousal/low arousal).</p></fn></table-wrap-foot></table-wrap></p><p id="p0230">Unlike other strategies that generate time-frequency images of multivariate synchrosqueezing transform (MSST) from a single channel [<xref rid="bib55" ref-type="bibr">55</xref>] or use raw single channel data [<xref rid="bib56" ref-type="bibr">56</xref>], our approach is multichannel. This multichannel method provides a more comprehensive representation of EEG signals, which is then used to train pre-trained CNNs.</p><p id="p0235">According to <xref rid="tbl3" ref-type="table">Table 3</xref>, our results demonstrate significant improvement. When compared to Ref. [<xref rid="bib56" ref-type="bibr">56</xref>], which used ECG signals to classify valence and arousal classes, our accuracy is notably higher (97.34&#x000a0;% vs. 73.33&#x000a0;% for valence and 60&#x000a0;% for arousal). Similarly, when compared to Refs. [<xref rid="bib8" ref-type="bibr">8</xref>,<xref rid="bib9" ref-type="bibr">9</xref>,<xref rid="bib55" ref-type="bibr">55</xref>,<xref rid="bib57" ref-type="bibr">57</xref>], which used EEG signals, our method also outperforms them (97.34&#x000a0;% vs. 95.25&#x000a0;%, 96.26&#x000a0;%, 85.2&#x000a0;% and 88.17&#x000a0;%, respectively).</p><p id="p0240">Based on <xref rid="tbl3" ref-type="table">Table 3</xref>, our results surpass all of these studies except for Bagherzadeh et al. [<xref rid="bib11" ref-type="bibr">11</xref>], who achieved higher accuracy by generating brain connectivity images using a fusion of three methods (TE, PDC, and dDTF) and combining CNN and LSTM. Our use of multichannel and multimodal approaches has led to higher accuracy in our ER framework compared to recent studies.</p></sec><sec id="sec4.2"><label>4.2</label><title>Advantage, disadvantage and limitation of this study</title><p id="p0245">Advantages of this study are as follows:<list list-type="simple" id="ulist0015"><list-item id="u0020"><label>&#x02022;</label><p id="p0250">Estimate coupling of ECG and EEG WHILE emotions as a way to improve ER systems performance.</p></list-item><list-item id="u0025"><label>&#x02022;</label><p id="p0255">Jointly represent EC of multichannel ECG and EEG to investigate their involvement in sadness, happiness, disgust and fear.</p></list-item><list-item id="u0030"><label>&#x02022;</label><p id="p0260">Use transfer learning approach to recognize emotions from physiological data due to insufficient number of samples for train from the first.</p></list-item></list></p><p id="p0265">Here, a restriction is that it processes entire segments pertaining to emotional clips, which may not always be relevant. This is because there may be instances where the subject's attention diverges from the clip, leading to distraction, or situations where multiple emotions are elicited simultaneously.</p><p id="p0270">The second restriction is that, examination of variability of heart rate is a common approach in several ER researches. Since HRV directly change during emotions such as horror, anger and disgust. Therefore, this could be a useful metric in our study, but its frequency range is below 4&#x000a0;Hz, and the corresponding EEG range is delta. Delta is not the specific frequency band in emotion classification procedure. Hence, HRV is not considered in this study. The other limitation is number of samples, the database has 27 subject and each one has watched 18 emotional video clips.</p></sec></sec><sec id="sec5"><label>5</label><title>Conclusion and upcoming work</title><p id="p0275">This study pivots on enhancing the accuracy of emotion classification frameworks by integrating EEG data with ECG data to calculate EC, capturing the brain-heart interaction during emotions such as happiness, fear, sadness and disgust. To achieve this, three EC estimation techniques&#x02014;GC, DTF, and PDC&#x02014;were applied, and their outputs were used as inputs for advanced pre-trained CNNs, specifically ResNet-18 and MobileNetV2. The highest average accuracy recorded was 97.34&#x000a0;&#x000b1;&#x000a0;1.19&#x000a0;% with DTF images in the alpha frequency band using ResNet-18, and 96.53&#x000a0;&#x000b1;&#x000a0;3.54&#x000a0;% with MobileNetV2. These findings highlight that combining EEG with ECG data significantly improves emotion recognition performance compared to existing methods, particularly for emotions like happiness, disgust, fear, and sadness.</p><p id="p0280">In the future, we would use the eye track data as a gate to choose most effective parts of EEG and ECG data during sadness, happiness, disgust and fear. Also, we would combine other deep learning techniques like LSTM to add time information to our proposed ER system.</p></sec><sec id="sec6"><title>CRediT authorship contribution statement</title><p id="p0285"><bold>Javid Farhadi Sedehi:</bold> Writing &#x02013; review &#x00026; editing, Writing &#x02013; original draft, Visualization, Validation, Software, Methodology, Investigation, Formal analysis, Conceptualization. <bold>Nader Jafarnia Dabanloo:</bold> Writing &#x02013; review &#x00026; editing, Validation, Supervision, Methodology, Investigation, Conceptualization. <bold>Keivan Maghooli:</bold> Writing &#x02013; review &#x00026; editing, Validation, Supervision, Methodology, Investigation, Conceptualization. <bold>Ali Sheikhani:</bold> Validation, Methodology, Conceptualization.</p></sec><sec sec-type="data-availability" id="sec7"><title>Data availability</title><p id="p0290">In this study, the public available MAHNOB-HCI database was analyzed. The databases can be found here: <ext-link ext-link-type="uri" xlink:href="https://mahnob-db.eu/hci-tagging/" id="intref0010">https://mahnob-db.eu/hci-tagging/</ext-link>.</p></sec><sec id="sec8"><title>Ethics declaration</title><p id="p0295">In this study, the public available MAHNOB-HCI database was analyzed. The study involving human participants was conducted in compliance with local legislation and institutional guidelines for of Imperial College London, thereby exempting it from ethical review and approval.</p></sec><sec sec-type="COI-statement" id="coi0010"><title>Declaration of competing interest</title><p id="p0300">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></sec></body><back><ref-list id="cebib0010"><title>References</title><ref id="bib1"><label>1</label><element-citation publication-type="journal" id="sref1"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Song</surname><given-names>W.</given-names></name><name><surname>Tao</surname><given-names>W.</given-names></name><name><surname>Liotta</surname><given-names>A.</given-names></name><name><surname>Yang</surname><given-names>D.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Gao</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>Y.</given-names></name><name><surname>Ge</surname><given-names>W.</given-names></name><name><surname>Zhang</surname><given-names>W.</given-names></name><name><surname>Zhang</surname><given-names>W.</given-names></name></person-group><article-title>A systematic review on affective computing: emotion models, databases, and recent advances</article-title><source>Inf. Fusion</source><volume>83</volume><year>2022 Jul 1</year><fpage>19</fpage><lpage>52</lpage></element-citation></ref><ref id="bib2"><label>2</label><element-citation publication-type="journal" id="sref2"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>M.M.</given-names></name><name><surname>Sarkar</surname><given-names>A.K.</given-names></name><name><surname>Hossain</surname><given-names>M.A.</given-names></name><name><surname>Hossain</surname><given-names>M.S.</given-names></name><name><surname>Islam</surname><given-names>M.R.</given-names></name><name><surname>Hossain</surname><given-names>M.B.</given-names></name><name><surname>Quinn</surname><given-names>J.M.</given-names></name><name><surname>Moni</surname><given-names>M.A.</given-names></name></person-group><article-title>Recognition of human emotions using EEG signals: a review</article-title><source>Comput. Biol. Med.</source><volume>136</volume><year>2021 Sep 1</year><object-id pub-id-type="publisher-id">104696</object-id></element-citation></ref><ref id="bib3"><label>3</label><element-citation publication-type="journal" id="sref3"><person-group person-group-type="author"><name><surname>Jafari</surname><given-names>M.</given-names></name><name><surname>Shoeibi</surname><given-names>A.</given-names></name><name><surname>Khodatars</surname><given-names>M.</given-names></name><name><surname>Bagherzadeh</surname><given-names>S.</given-names></name><name><surname>Shalbaf</surname><given-names>A.</given-names></name><name><surname>Garc&#x000ed;a</surname><given-names>D.L.</given-names></name><name><surname>Gorriz</surname><given-names>J.M.</given-names></name><name><surname>Acharya</surname><given-names>U.R.</given-names></name></person-group><article-title>Emotion recognition in EEG signals using deep learning methods: a review</article-title><source>Comput. Biol. Med.</source><year>2023 Sep 9</year><object-id pub-id-type="publisher-id">107450</object-id></element-citation></ref><ref id="bib4"><label>4</label><element-citation publication-type="journal" id="sref4"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>T.</given-names></name><name><surname>Qiu</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Zhao</surname><given-names>H.</given-names></name><name><surname>Jiang</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Xu</surname><given-names>J.</given-names></name><name><surname>Sun</surname><given-names>T.</given-names></name><name><surname>Jiang</surname><given-names>N.</given-names></name></person-group><article-title>A new deep convolutional neural network incorporating attentional mechanisms for ECG emotion recognition</article-title><source>Comput. Biol. Med.</source><volume>159</volume><year>2023 Jun 1</year><object-id pub-id-type="publisher-id">106938</object-id></element-citation></ref><ref id="bib5"><label>5</label><element-citation publication-type="journal" id="sref5"><person-group person-group-type="author"><name><surname>O&#x0011f;uz</surname><given-names>F.E.</given-names></name><name><surname>Alkan</surname><given-names>A.</given-names></name><name><surname>Sch&#x000f6;ler</surname><given-names>T.</given-names></name></person-group><article-title>Emotion detection from ECG signals with different learning algorithms and automated feature engineering</article-title><source>Signal, Image and Video Processing</source><volume>17</volume><issue>7</issue><year>2023 Oct</year><fpage>3783</fpage><lpage>3791</lpage></element-citation></ref><ref id="bib6"><label>6</label><element-citation publication-type="journal" id="sref6"><person-group person-group-type="author"><name><surname>Hasnul</surname><given-names>M.A.</given-names></name><name><surname>Ab Aziz</surname><given-names>N.A.</given-names></name><name><surname>Abd</surname><given-names>Aziz A.</given-names></name></person-group><article-title>Augmenting ECG data with multiple filters for a better emotion recognition system</article-title><source>Arabian J. Sci. Eng.</source><volume>48</volume><issue>8</issue><year>2023 Aug</year><fpage>10313</fpage><lpage>10334</lpage></element-citation></ref><ref id="bib7"><label>7</label><element-citation publication-type="journal" id="sref7"><person-group person-group-type="author"><name><surname>Bagherzadeh</surname><given-names>S.</given-names></name><name><surname>Maghooli</surname><given-names>K.</given-names></name><name><surname>Shalbaf</surname><given-names>A.</given-names></name><name><surname>Maghsoudi</surname><given-names>A.</given-names></name></person-group><article-title>Emotion recognition using continuous wavelet transform and ensemble of convolutional neural networks through transfer learning from electroencephalogram signal</article-title><source>Frontiers in Biomedical Technologies</source><volume>10</volume><issue>1</issue><year>2023</year><fpage>47</fpage><lpage>56</lpage></element-citation></ref><ref id="bib8"><label>8</label><element-citation publication-type="journal" id="sref8"><person-group person-group-type="author"><name><surname>Bagherzadeh</surname><given-names>S.</given-names></name><name><surname>Maghooli</surname><given-names>K.</given-names></name><name><surname>Shalbaf</surname><given-names>A.</given-names></name><name><surname>Maghsoudi</surname><given-names>A.</given-names></name></person-group><article-title>Recognition of emotional states using frequency effective connectivity maps through transfer learning approach from electroencephalogram signals</article-title><source>Biomed. Signal Process Control</source><volume>75</volume><year>2022 May 1</year><object-id pub-id-type="publisher-id">103544</object-id></element-citation></ref><ref id="bib9"><label>9</label><element-citation publication-type="journal" id="sref9"><person-group person-group-type="author"><name><surname>Bagherzadeh</surname><given-names>S.</given-names></name><name><surname>Maghooli</surname><given-names>K.</given-names></name><name><surname>Shalbaf</surname><given-names>A.</given-names></name><name><surname>Maghsoudi</surname><given-names>A.</given-names></name></person-group><article-title>Emotion recognition using effective connectivity and pre-trained convolutional neural networks in EEG signals</article-title><source>Cognitive Neurodynamics</source><volume>16</volume><issue>5</issue><year>2022 Oct</year><fpage>1087</fpage><lpage>1106</lpage><pub-id pub-id-type="pmid">36237402</pub-id>
</element-citation></ref><ref id="bib10"><label>10</label><element-citation publication-type="journal" id="sref10"><person-group person-group-type="author"><name><surname>Bagherzadeh</surname><given-names>S.</given-names></name><name><surname>Norouzi</surname><given-names>M.R.</given-names></name><name><surname>Hampa</surname><given-names>S.B.</given-names></name><name><surname>Ghasri</surname><given-names>A.</given-names></name><name><surname>Kouroshi</surname><given-names>P.T.</given-names></name><name><surname>Hosseininasab</surname><given-names>S.</given-names></name><name><surname>Zadeh</surname><given-names>M.A.</given-names></name><name><surname>Nasrabadi</surname><given-names>A.M.</given-names></name></person-group><article-title>A subject-independent portable emotion recognition system using synchrosqueezing wavelet transform maps of EEG signals and ResNet-18</article-title><source>Biomed. Signal Process Control</source><volume>90</volume><year>2024 Apr 1</year><object-id pub-id-type="publisher-id">105875</object-id></element-citation></ref><ref id="bib11"><label>11</label><element-citation publication-type="journal" id="sref10d"><person-group person-group-type="author"><name><surname>Bagherzadeh</surname><given-names>S.</given-names></name><name><surname>Shalbaf</surname><given-names>A.</given-names></name><name><surname>Shoeibi</surname><given-names>A.</given-names></name><name><surname>Jafari</surname><given-names>M.</given-names></name><name><surname>San Tan</surname><given-names>R.</given-names></name><name><surname>Acharya</surname><given-names>U.R.</given-names></name></person-group><article-title>Developing an EEG-based emotion recognition using ensemble deep learning methods and fusion of brain effective connectivity maps</article-title><source>IEEE Access</source><volume>12</volume><year>2024</year><fpage>50949</fpage><lpage>50965</lpage></element-citation></ref><ref id="bib12"><label>12</label><element-citation publication-type="journal" id="sref12"><person-group person-group-type="author"><name><surname>Mullen</surname><given-names>T.</given-names></name></person-group><article-title>Source information flow toolbox (SIFT)</article-title><source>Swartz Center Comput Neurosci</source><volume>15</volume><year>2010 Dec 15</year><fpage>1</fpage><lpage>69</lpage></element-citation></ref><ref id="bib13"><label>13</label><element-citation publication-type="book" id="sref13"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>D.K.</given-names></name></person-group><part-title>Diffusion MRI</part-title><year>2010 Nov 11</year><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib14"><label>14</label><element-citation publication-type="book" id="sref14"><person-group person-group-type="author"><name><surname>Buxton</surname><given-names>R.B.</given-names></name></person-group><part-title>Introduction to Functional Magnetic Resonance Imaging: Principles and Techniques</part-title><year>2009 Aug 27</year><publisher-name>Cambridge university press</publisher-name></element-citation></ref><ref id="bib15"><label>15</label><element-citation publication-type="journal" id="sref15"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K.J.</given-names></name><name><surname>Harrison</surname><given-names>L.</given-names></name><name><surname>Penny</surname><given-names>W.</given-names></name></person-group><article-title>Dynamic causal modelling</article-title><source>Neuroimage</source><volume>19</volume><issue>4</issue><year>2003 Aug 1</year><fpage>1273</fpage><lpage>1302</lpage><pub-id pub-id-type="pmid">12948688</pub-id>
</element-citation></ref><ref id="bib16"><label>16</label><element-citation publication-type="journal" id="sref16"><person-group person-group-type="author"><name><surname>Craik</surname><given-names>A.</given-names></name><name><surname>He</surname><given-names>Y.</given-names></name><name><surname>Contreras-Vidal</surname><given-names>J.L.</given-names></name></person-group><article-title>Deep learning for electroencephalogram (EEG) classification tasks: a review</article-title><source>J. Neural. Eng.</source><volume>16</volume><issue>3</issue><year>2019 Apr 9</year><object-id pub-id-type="publisher-id">031001</object-id></element-citation></ref><ref id="bib17"><label>17</label><element-citation publication-type="journal" id="sref17"><person-group person-group-type="author"><name><surname>Rajwal</surname><given-names>S.</given-names></name><name><surname>Aggarwal</surname><given-names>S.</given-names></name></person-group><article-title>Convolutional neural network-based EEG signal analysis: a systematic review</article-title><source>Arch. Comput. Methods Eng.</source><volume>30</volume><issue>6</issue><year>2023 Jul</year><fpage>3585</fpage><lpage>3615</lpage></element-citation></ref><ref id="bib18"><label>18</label><element-citation publication-type="journal" id="sref18"><person-group person-group-type="author"><name><surname>Khare</surname><given-names>S.K.</given-names></name><name><surname>Bajaj</surname><given-names>V.</given-names></name><name><surname>Acharya</surname><given-names>U.R.</given-names></name></person-group><article-title>SPWVD-CNN for automated detection of schizophrenia patients using EEG signals</article-title><source>IEEE Trans. Instrum. Meas.</source><volume>70</volume><year>2021 Apr 2</year><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">33776080</pub-id>
</element-citation></ref><ref id="bib19"><label>19</label><element-citation publication-type="journal" id="sref19"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>D.M.</given-names></name><name><surname>Yahya</surname><given-names>N.</given-names></name><name><surname>Kamel</surname><given-names>N.</given-names></name><name><surname>Faye</surname><given-names>I.</given-names></name></person-group><article-title>Automated diagnosis of major depressive disorder using brain effective connectivity and 3D convolutional neural network</article-title><source>IEEE Access</source><volume>9</volume><year>2021 Jan 5</year><fpage>8835</fpage><lpage>8846</lpage></element-citation></ref><ref id="bib20"><label>20</label><element-citation publication-type="journal" id="sref20"><person-group person-group-type="author"><name><surname>Loh</surname><given-names>H.W.</given-names></name><name><surname>Ooi</surname><given-names>C.P.</given-names></name><name><surname>Aydemir</surname><given-names>E.</given-names></name><name><surname>Tuncer</surname><given-names>T.</given-names></name><name><surname>Dogan</surname><given-names>S.</given-names></name><name><surname>Acharya</surname><given-names>U.R.</given-names></name></person-group><article-title>Decision support system for major depression detection using spectrogram and convolution neural network with EEG signals</article-title><source>Expet Syst.</source><volume>39</volume><issue>3</issue><year>2022 Mar</year><object-id pub-id-type="publisher-id">e12773</object-id></element-citation></ref><ref id="bib21"><label>21</label><element-citation publication-type="journal" id="sref20v"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Wu</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name></person-group><article-title>An end-to-end deep learning model for EEG-based major depressive disorder classification</article-title><source>IEEE Access</source><volume>11</volume><year>2023</year><fpage>41337</fpage><lpage>41347</lpage></element-citation></ref><ref id="bib22"><label>22</label><element-citation publication-type="book" id="sref22"><person-group person-group-type="author"><name><surname>Tokmak</surname><given-names>F.</given-names></name><name><surname>Subasi</surname><given-names>A.</given-names></name><name><surname>Qaisar</surname><given-names>S.M.</given-names></name></person-group><part-title>Artificial intelligence-based emotion recognition using ECG signals</part-title><series>Applications of Artificial Intelligence Healthcare and Biomedicine</series><volume>vol. 1</volume><year>2024 Jan</year><publisher-name>Academic Press</publisher-name><fpage>37</fpage><lpage>67</lpage></element-citation></ref><ref id="bib23"><label>23</label><element-citation publication-type="journal" id="sref23"><person-group person-group-type="author"><name><surname>Ozaltin</surname><given-names>O.</given-names></name><name><surname>Yeniay</surname><given-names>O.</given-names></name></person-group><article-title>A novel proposed CNN&#x02013;SVM architecture for ECG scalograms classification</article-title><source>Soft Comput.</source><volume>27</volume><issue>8</issue><year>2023 Apr</year><fpage>4639</fpage><lpage>4658</lpage><pub-id pub-id-type="pmid">36536664</pub-id>
</element-citation></ref><ref id="bib24"><label>24</label><element-citation publication-type="journal" id="sref24"><person-group person-group-type="author"><name><surname>Al Rahhal</surname><given-names>M.M.</given-names></name><name><surname>Bazi</surname><given-names>Y.</given-names></name><name><surname>Al Zuair</surname><given-names>M.</given-names></name><name><surname>Othman</surname><given-names>E.</given-names></name><name><surname>BenJdira</surname><given-names>B.</given-names></name></person-group><article-title>Convolutional neural networks for electrocardiogram classification</article-title><source>J. Med. Biol. Eng.</source><volume>38</volume><year>2018 Dec</year><fpage>1014</fpage><lpage>1025</lpage></element-citation></ref><ref id="bib25"><label>25</label><element-citation publication-type="journal" id="sref25"><person-group person-group-type="author"><name><surname>Rashed-Al-Mahfuz</surname><given-names>M.</given-names></name><name><surname>Moni</surname><given-names>M.A.</given-names></name><name><surname>Lio</surname><given-names>P.</given-names></name><name><surname>Islam</surname><given-names>S.M.</given-names></name><name><surname>Berkovsky</surname><given-names>S.</given-names></name><name><surname>Khushi</surname><given-names>M.</given-names></name><name><surname>Quinn</surname><given-names>J.M.</given-names></name></person-group><article-title>Deep convolutional neural networks based ECG beats classification to diagnose cardiovascular conditions</article-title><source>Biomedical engineering letters</source><volume>11</volume><year>2021 May</year><fpage>147</fpage><lpage>162</lpage><pub-id pub-id-type="pmid">34150350</pub-id>
</element-citation></ref><ref id="bib26"><label>26</label><element-citation publication-type="journal" id="sref26"><person-group person-group-type="author"><name><surname>Li</surname><given-names>P.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Si</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>F.</given-names></name><name><surname>Zhu</surname><given-names>X.</given-names></name><name><surname>Huang</surname><given-names>X.</given-names></name><name><surname>Zeng</surname><given-names>Y.</given-names></name><name><surname>Yao</surname><given-names>D.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Xu</surname><given-names>P.</given-names></name></person-group><article-title>EEG based emotion recognition by combining functional connectivity network and local activations</article-title><source>IEEE Trans. Biomed. Eng.</source><volume>66</volume><issue>10</issue><year>2019</year><fpage>2869</fpage><lpage>2881</lpage><pub-id pub-id-type="pmid">30735981</pub-id>
</element-citation></ref><ref id="bib27"><label>27</label><element-citation publication-type="journal" id="sref27"><person-group person-group-type="author"><name><surname>Moon</surname><given-names>S.E.</given-names></name><name><surname>Chen</surname><given-names>C.J.</given-names></name><name><surname>Hsieh</surname><given-names>C.J.</given-names></name><name><surname>Wang</surname><given-names>J.L.</given-names></name><name><surname>Lee</surname><given-names>J.S.</given-names></name></person-group><article-title>Emotional EEG classification using connectivity features and convolutional neural networks</article-title><source>Neural Network.</source><volume>132</volume><year>2020 Dec 1</year><fpage>96</fpage><lpage>107</lpage></element-citation></ref><ref id="bib28"><label>28</label><element-citation publication-type="journal" id="sref28"><person-group person-group-type="author"><name><surname>Mirjebreili</surname><given-names>S.M.</given-names></name><name><surname>Shalbaf</surname><given-names>R.</given-names></name><name><surname>Shalbaf</surname><given-names>A.</given-names></name></person-group><article-title>Prediction of treatment response in major depressive disorder using a hybrid of convolutional recurrent deep neural networks and effective connectivity based on EEG signal</article-title><source>Physical and Engineering Sciences in Medicine</source><volume>15</volume><year>2024 Feb</year><comment>1-0</comment></element-citation></ref><ref id="bib29"><label>29</label><element-citation publication-type="journal" id="sref29"><person-group person-group-type="author"><name><surname>Bagherzadeh</surname><given-names>S.</given-names></name><name><surname>Shalbaf</surname><given-names>A.</given-names></name></person-group><article-title>EEG-based schizophrenia detection using fusion of effective connectivity maps and convolutional neural networks with transfer learning</article-title><source>Cognitive Neurodynamics</source><volume>9</volume><year>2024 May</year><fpage>1</fpage><lpage>2</lpage></element-citation></ref><ref id="bib30"><label>30</label><element-citation publication-type="journal" id="sref30"><person-group person-group-type="author"><name><surname>Safari</surname><given-names>M.</given-names></name><name><surname>Shalbaf</surname><given-names>R.</given-names></name><name><surname>Bagherzadeh</surname><given-names>S.</given-names></name><name><surname>Shalbaf</surname><given-names>A.</given-names></name></person-group><article-title>Classification of mental workload using brain connectivity and machine learning on electroencephalogram data</article-title><source>Sci. Rep.</source><volume>14</volume><issue>1</issue><year>2024 Apr 21</year><fpage>9153</fpage><pub-id pub-id-type="pmid">38644365</pub-id>
</element-citation></ref><ref id="bib31"><label>31</label><element-citation publication-type="journal" id="sref31"><person-group person-group-type="author"><name><surname>Sedehi</surname><given-names>J.F.</given-names></name><name><surname>Dabanloo</surname><given-names>N.J.</given-names></name><name><surname>Maghooli</surname><given-names>K.</given-names></name><name><surname>Sheikhani</surname><given-names>A.</given-names></name></person-group><article-title>Multimodal insights into granger causality connectivity: integrating physiological signals and gated eye-tracking data for emotion recognition using convolutional neural network</article-title><source>Heliyon</source><volume>10</volume><issue>16</issue><year>2024 Aug 30</year><object-id pub-id-type="publisher-id">e36411</object-id></element-citation></ref><ref id="bib32"><label>32</label><element-citation publication-type="journal" id="sref32"><comment>[database]</comment><person-group person-group-type="author"><name><surname>Soleymani</surname><given-names>M.</given-names></name><name><surname>Lichtenauer</surname><given-names>J.</given-names></name><name><surname>Pun</surname><given-names>T.</given-names></name><name><surname>Pantic</surname><given-names>M.</given-names></name></person-group><article-title>A multimodal database for affect recognition and implicit tagging</article-title><source>IEEE transactions on affective computing</source><volume>3</volume><issue>1</issue><year>2011 Aug 4</year><fpage>42</fpage><lpage>55</lpage></element-citation></ref><ref id="bib33"><label>33</label><element-citation publication-type="journal" id="sref33"><person-group person-group-type="author"><name><surname>Bagherzadeh</surname><given-names>S.</given-names></name><name><surname>Shahabi</surname><given-names>M.S.</given-names></name><name><surname>Shalbaf</surname><given-names>A.</given-names></name></person-group><article-title>Detection of schizophrenia using hybrid of deep learning and brain effective connectivity image from electroencephalogram signal</article-title><source>Comput. Biol. Med.</source><volume>146</volume><year>2022 Jul 1</year><object-id pub-id-type="publisher-id">105570</object-id></element-citation></ref><ref id="bib34"><label>34</label><element-citation publication-type="journal" id="sref34"><person-group person-group-type="author"><name><surname>Tafreshi</surname><given-names>T.F.</given-names></name><name><surname>Daliri</surname><given-names>M.R.</given-names></name><name><surname>Ghodousi</surname><given-names>M.</given-names></name></person-group><article-title>Functional and effective connectivity based features of EEG signals for object recognition</article-title><source>Cognitive neurodynamics</source><volume>13</volume><issue>6</issue><year>2019 Dec</year><fpage>555</fpage><lpage>566</lpage><pub-id pub-id-type="pmid">31741692</pub-id>
</element-citation></ref><ref id="bib35"><label>35</label><element-citation publication-type="journal" id="sref35"><person-group person-group-type="author"><name><surname>Varotto</surname><given-names>G.</given-names></name><name><surname>Visani</surname><given-names>E.</given-names></name><name><surname>Canafoglia</surname><given-names>L.</given-names></name><name><surname>Franceschetti</surname><given-names>S.</given-names></name><name><surname>Avanzini</surname><given-names>G.</given-names></name><name><surname>Panzica</surname><given-names>F.</given-names></name></person-group><article-title>Enhanced frontocentral EEG connectivity in photosensitive generalized epilepsies: a partial directed coherence study</article-title><source>Epilepsia</source><volume>53</volume><issue>2</issue><year>2012 Feb</year><fpage>359</fpage><lpage>367</lpage><pub-id pub-id-type="pmid">22191664</pub-id>
</element-citation></ref><ref id="bib36"><label>36</label><element-citation publication-type="journal" id="sref36"><person-group person-group-type="author"><name><surname>Varotto</surname><given-names>G.</given-names></name><name><surname>Fazio</surname><given-names>P.</given-names></name><name><surname>Sebastiano</surname><given-names>D.R.</given-names></name><name><surname>Duran</surname><given-names>D.</given-names></name><name><surname>D'Incerti</surname><given-names>L.</given-names></name><name><surname>Parati</surname><given-names>E.</given-names></name><name><surname>Sattin</surname><given-names>D.</given-names></name><name><surname>Leonardi</surname><given-names>M.</given-names></name><name><surname>Franceschetti</surname><given-names>S.</given-names></name><name><surname>Panzica</surname><given-names>F.</given-names></name></person-group><article-title>Altered resting state effective connectivity in long-standing vegetative state patients: an EEG study</article-title><source>Clin. Neurophysiol.</source><volume>125</volume><issue>1</issue><year>2014 Jan 1</year><fpage>63</fpage><lpage>68</lpage><pub-id pub-id-type="pmid">23927942</pub-id>
</element-citation></ref><ref id="bib37"><label>37</label><element-citation publication-type="journal" id="sref37"><person-group person-group-type="author"><name><surname>Niso</surname><given-names>G.</given-names></name><name><surname>Bru&#x000f1;a</surname><given-names>R.</given-names></name><name><surname>Pereda</surname><given-names>E.</given-names></name><name><surname>Guti&#x000e9;rrez</surname><given-names>R.</given-names></name><name><surname>Bajo</surname><given-names>R.</given-names></name><name><surname>Maest&#x000fa;</surname><given-names>F.</given-names></name><name><surname>Del-Pozo</surname><given-names>F.</given-names></name></person-group><article-title>HERMES: towards an integrated toolbox to characterize functional and effective brain connectivity</article-title><source>Neuroinformatics</source><volume>11</volume><year>2013 Oct</year><fpage>405</fpage><lpage>434</lpage><pub-id pub-id-type="pmid">23812847</pub-id>
</element-citation></ref><ref id="bib38"><label>38</label><element-citation publication-type="journal" id="sref38"><person-group person-group-type="author"><name><surname>Astolfi</surname><given-names>L.</given-names></name><name><surname>Cincotti</surname><given-names>F.</given-names></name><name><surname>Mattia</surname><given-names>D.</given-names></name><name><surname>Marciani</surname><given-names>M.G.</given-names></name><name><surname>Baccala</surname><given-names>L.A.</given-names></name><etal/></person-group><article-title>Comparison of different cortical connectivity estimators for high-resolution EEG recordings</article-title><source>Hum. Brain Mapp.</source><volume>28</volume><issue>2</issue><year>2007</year><fpage>143</fpage><lpage>157</lpage><pub-id pub-id-type="pmid">16761264</pub-id>
</element-citation></ref><ref id="bib39"><label>39</label><element-citation publication-type="journal" id="sref39"><person-group person-group-type="author"><name><surname>Astolfi</surname><given-names>L.</given-names></name><name><surname>Fallani</surname><given-names>F.D.V.</given-names></name><name><surname>Cincotti</surname><given-names>F.</given-names></name><name><surname>Mattia</surname><given-names>D.</given-names></name><name><surname>Marciani</surname><given-names>M.G.</given-names></name><etal/></person-group><article-title>Estimation of effective and functional cortical connectivity from neuroelectric and hemodynamic recordings</article-title><source>IEEE Trans. Neural Syst. Rehabil. Eng.</source><volume>17</volume><issue>3</issue><year>2008</year><fpage>224</fpage><lpage>233</lpage><pub-id pub-id-type="pmid">19273037</pub-id>
</element-citation></ref><ref id="bib40"><label>40</label><element-citation publication-type="journal" id="sref40"><person-group person-group-type="author"><name><surname>Baccal&#x000e1;</surname><given-names>L.A.</given-names></name><name><surname>Sameshima</surname><given-names>K.</given-names></name></person-group><article-title>Partial directed coherence: a new concept in neural structure determination</article-title><source>Biol. Cybern.</source><volume>84</volume><issue>6</issue><year>2001 May</year><fpage>463</fpage><lpage>474</lpage><pub-id pub-id-type="pmid">11417058</pub-id>
</element-citation></ref><ref id="bib41"><label>41</label><element-citation publication-type="journal" id="sref41"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>F.</given-names></name><name><surname>Qi</surname><given-names>Z.</given-names></name><name><surname>Duan</surname><given-names>K.</given-names></name><name><surname>Xi</surname><given-names>D.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name><name><surname>Zhu</surname><given-names>H.</given-names></name><name><surname>Xiong</surname><given-names>H.</given-names></name><name><surname>He</surname><given-names>Q.</given-names></name></person-group><article-title>A comprehensive survey on transfer learning</article-title><source>Proc. IEEE</source><volume>109</volume><issue>1</issue><year>2020 Jul 7</year><fpage>43</fpage><lpage>76</lpage></element-citation></ref><ref id="bib42"><label>42</label><element-citation publication-type="book" id="sref42"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><part-title>Deep residual learning for image recognition</part-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2016</year><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="bib43"><label>43</label><element-citation publication-type="other" id="sref43"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>Andrew G.</given-names></name><name><surname>Zhu</surname><given-names>Menglong</given-names></name><name><surname>Chen</surname><given-names>Bo</given-names></name><name><surname>Kalenichenko</surname><given-names>Dmitry</given-names></name><name><surname>Wang</surname><given-names>Weijun</given-names></name><name><surname>Weyand</surname><given-names>Tobias</given-names></name><name><surname>Andreetto</surname><given-names>Marco</given-names></name><name><surname>Adam</surname><given-names>Hartwig</given-names></name></person-group><article-title>MobileNets: efficient convolutional neural networks for mobile vision applications</article-title><pub-id pub-id-type="doi">10.48550/arXiv.1704.04861</pub-id><year>2017</year></element-citation></ref><ref id="bib44"><label>44</label><element-citation publication-type="journal" id="sref44"><person-group person-group-type="author"><name><surname>Sokolova</surname><given-names>M.</given-names></name><name><surname>Lapalme</surname><given-names>G.</given-names></name></person-group><article-title>A systematic analysis of performance measures for classification tasks</article-title><source>Inf. Process. Manag.</source><volume>45</volume><issue>4</issue><year>2009 Jul 1</year><fpage>427</fpage><lpage>437</lpage></element-citation></ref><ref id="bib45"><label>45</label><element-citation publication-type="book" id="sref45"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D.P.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name></person-group><part-title>Adam: A Method for Stochastic Optimization</part-title><year>2014</year><comment>arXiv preprint arXiv:1412.6980</comment></element-citation></ref><ref id="bib46"><label>46</label><element-citation publication-type="book" id="sref46"><person-group person-group-type="author"><name><surname>Selvaraju</surname><given-names>R.R.</given-names></name><name><surname>Cogswell</surname><given-names>M.</given-names></name><name><surname>Das</surname><given-names>A.</given-names></name><name><surname>Vedantam</surname><given-names>R.</given-names></name><name><surname>Parikh</surname><given-names>D.</given-names></name><name><surname>Batra</surname><given-names>D.</given-names></name></person-group><part-title>Grad-cam: visual explanations from deep networks via gradient-based localization</part-title><source>InProceedings of the IEEE International Conference on Computer Vision</source><year>2017</year><fpage>618</fpage><lpage>626</lpage></element-citation></ref><ref id="bib47"><label>47</label><element-citation publication-type="journal" id="sref47"><person-group person-group-type="author"><name><surname>Daneshi Kohan</surname><given-names>M.</given-names></name><name><surname>Motie Nasrabadi</surname><given-names>A.</given-names></name><name><surname>Shamsollahi</surname><given-names>M.B.</given-names></name><name><surname>Sharifi</surname><given-names>A.</given-names></name></person-group><article-title>EEG/PPG effective connectivity fusion for analyzing deception in interview</article-title><source>Signal, Image and Video Processing</source><volume>14</volume><issue>5</issue><year>2020 Jul</year><fpage>907</fpage><lpage>914</lpage></element-citation></ref><ref id="bib48"><label>48</label><element-citation publication-type="journal" id="sref48"><person-group person-group-type="author"><name><surname>Kohan</surname><given-names>M.D.</given-names></name><name><surname>Nasrabadi</surname><given-names>A.M.</given-names></name><name><surname>Shamsollahi</surname><given-names>M.B.</given-names></name></person-group><article-title>Interview based connectivity analysis of EEG in order to detect deception</article-title><source>Med. Hypotheses</source><volume>136</volume><year>2020 Mar 1</year><object-id pub-id-type="publisher-id">109517</object-id></element-citation></ref><ref id="bib49"><label>49</label><element-citation publication-type="journal" id="sref49"><person-group person-group-type="author"><name><surname>Demir</surname><given-names>F.</given-names></name><name><surname>Sobahi</surname><given-names>N.</given-names></name><name><surname>Siuly</surname><given-names>S.</given-names></name><name><surname>Sengur</surname><given-names>A.</given-names></name></person-group><article-title>Exploring deep learning features for automatic classification of human emotion using EEG rhythms</article-title><source>IEEE Sensor. J.</source><volume>21</volume><issue>13</issue><year>2021 Apr 1</year><fpage>14923</fpage><lpage>14930</lpage></element-citation></ref><ref id="bib50"><label>50</label><element-citation publication-type="journal" id="sref50"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>W.L.</given-names></name><name><surname>Lu</surname><given-names>B.L.</given-names></name></person-group><article-title>Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks</article-title><source>IEEE Transactions on autonomous mental development</source><volume>7</volume><issue>3</issue><year>2015 May 8</year><fpage>162</fpage><lpage>175</lpage></element-citation></ref><ref id="bib51"><label>51</label><element-citation publication-type="journal" id="sref51"><person-group person-group-type="author"><name><surname>Jau&#x00161;ovec</surname><given-names>N.</given-names></name><name><surname>Jau&#x00161;ovec</surname><given-names>K.</given-names></name><name><surname>Gerli&#x0010d;</surname><given-names>I.</given-names></name></person-group><article-title>Differences in event-related and induced EEG patterns in the theta and alpha frequency bands related to human emotional intelligence</article-title><source>Neurosci. Lett.</source><volume>311</volume><issue>2</issue><year>2001 Sep 28</year><fpage>93</fpage><lpage>96</lpage><pub-id pub-id-type="pmid">11567786</pub-id>
</element-citation></ref><ref id="bib52"><label>52</label><element-citation publication-type="journal" id="sref52"><person-group person-group-type="author"><name><surname>Klimesch</surname><given-names>W.</given-names></name></person-group><article-title>Alpha-band oscillations, attention, and controlled access to stored information</article-title><source>Trends Cognit. Sci.</source><volume>16</volume><issue>12</issue><year>2012 Dec 1</year><fpage>606</fpage><lpage>617</lpage><pub-id pub-id-type="pmid">23141428</pub-id>
</element-citation></ref><ref id="bib53"><label>53</label><element-citation publication-type="journal" id="sref53"><person-group person-group-type="author"><name><surname>Ba&#x0015f;ar</surname><given-names>E.</given-names></name></person-group><article-title>A review of alpha activity in integrative brain function: fundamental physiology, sensory coding, cognition and pathology</article-title><source>Int. J. Psychophysiol.</source><volume>86</volume><issue>1</issue><year>2012 Oct 1</year><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="pmid">22820267</pub-id>
</element-citation></ref><ref id="bib54"><label>54</label><element-citation publication-type="journal" id="sref54"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>J.J.</given-names></name><name><surname>Keune</surname><given-names>P.M.</given-names></name><name><surname>Sch&#x000f6;nenberg</surname><given-names>M.</given-names></name><name><surname>Nusslock</surname><given-names>R.</given-names></name></person-group><article-title>Frontal EEG alpha asymmetry and emotion: from neural underpinnings and methodological considerations to psychopathology and social cognition</article-title><source>Psychophysiology</source><volume>55</volume><issue>1</issue><year>2018 Jan</year><object-id pub-id-type="publisher-id">e13028</object-id></element-citation></ref><ref id="bib55"><label>55</label><element-citation publication-type="journal" id="sref55"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Cheng</surname><given-names>C.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>Multimodal emotion recognition based on manifold learning and convolution neural network</article-title><source>Multimed. Tool. Appl.</source><volume>81</volume><issue>23</issue><year>2022 Sep</year><fpage>33253</fpage><lpage>33268</lpage></element-citation></ref><ref id="bib56"><label>56</label><element-citation publication-type="journal" id="sref56"><person-group person-group-type="author"><name><surname>Mellouk</surname><given-names>W.</given-names></name><name><surname>Handouzi</surname><given-names>W.</given-names></name></person-group><article-title>CNN-LSTM for automatic emotion recognition using contactless photoplythesmographic signals</article-title><source>Biomed. Signal Process Control</source><volume>85</volume><year>2023 Aug 1</year><object-id pub-id-type="publisher-id">104907</object-id></element-citation></ref><ref id="bib57"><label>57</label><element-citation publication-type="journal" id="sref56m"><person-group person-group-type="author"><name><surname>Sedehi</surname><given-names>J.F.</given-names></name><name><surname>Dabanloo</surname><given-names>N.J.</given-names></name><name><surname>Maghooli</surname><given-names>K.</given-names></name><name><surname>Sheikhani</surname><given-names>A.</given-names></name></person-group><article-title>Recognition of emotional categories using mined fuzzy rules from electroencephalogram signals with gated eye track data approach</article-title><source>IEEE Access</source><volume>12</volume><year>2024</year><fpage>118122</fpage><lpage>118140</lpage></element-citation></ref></ref-list><ack id="ack0010"><title>Acknowledgment</title><p id="p0305">We would like to thank the Science and Research Branch, <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100002660</institution-id><institution>Islamic Azad University</institution></institution-wrap></funding-source>, Tehran, Iran due to their support.</p></ack></back></article>