<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Nat Commun</journal-id><journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id><journal-title-group><journal-title>Nature Communications</journal-title></journal-title-group><issn pub-type="epub">2041-1723</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40188094</article-id><article-id pub-id-type="pmc">PMC11972378</article-id><article-id pub-id-type="publisher-id">56989</article-id><article-id pub-id-type="doi">10.1038/s41467-025-56989-2</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Benchmarking large language models for biomedical natural language processing applications and recommendations</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6036-1516</contrib-id><name><surname>Chen</surname><given-names>Qingyu</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Hu</surname><given-names>Yan</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Peng</surname><given-names>Xueqing</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9588-7454</contrib-id><name><surname>Xie</surname><given-names>Qianqian</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1268-7239</contrib-id><name><surname>Jin</surname><given-names>Qiao</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Gilson</surname><given-names>Aidan</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9583-3846</contrib-id><name><surname>Singer</surname><given-names>Maxwell B.</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Ai</surname><given-names>Xuguang</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lai</surname><given-names>Po-Ting</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Zhizheng</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Keloth</surname><given-names>Vipina K.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Raja</surname><given-names>Kalpana</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Jimin</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>He</surname><given-names>Huan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lin</surname><given-names>Fongci</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0322-4566</contrib-id><name><surname>Du</surname><given-names>Jingcheng</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8258-3585</contrib-id><name><surname>Zhang</surname><given-names>Rui</given-names></name><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7411-6047</contrib-id><name><surname>Zheng</surname><given-names>W. Jim</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Adelman</surname><given-names>Ron A.</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9998-916X</contrib-id><name><surname>Lu</surname><given-names>Zhiyong</given-names></name><address><email>zhiyong.lu@nih.gov</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Xu</surname><given-names>Hua</given-names></name><address><email>hua.xu@yale.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03v76x132</institution-id><institution-id institution-id-type="GRID">grid.47100.32</institution-id><institution-id institution-id-type="ISNI">0000000419368710</institution-id><institution>Department of Biomedical Informatics and Data Science, Yale School of Medicine, </institution><institution>Yale University, </institution></institution-wrap>New Haven, CT USA </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01cwqze88</institution-id><institution-id institution-id-type="GRID">grid.94365.3d</institution-id><institution-id institution-id-type="ISNI">0000 0001 2297 5165</institution-id><institution>National Library of Medicine, </institution><institution>National Institutes of Health, </institution></institution-wrap>Bethesda, MD USA </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03gds6c39</institution-id><institution-id institution-id-type="GRID">grid.267308.8</institution-id><institution-id institution-id-type="ISNI">0000 0000 9206 2401</institution-id><institution>McWilliams School of Biomedical Informatics, </institution><institution>University of Texas Health Science at Houston, </institution></institution-wrap>Houston, TX USA </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03v76x132</institution-id><institution-id institution-id-type="GRID">grid.47100.32</institution-id><institution-id institution-id-type="ISNI">0000000419368710</institution-id><institution>Department of Ophthalmology and Visual Science, Yale School of Medicine, </institution><institution>Yale University, </institution></institution-wrap>New Haven, CT USA </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/017zqws13</institution-id><institution-id institution-id-type="GRID">grid.17635.36</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8657</institution-id><institution>Division of Computational Health Sciences, Department of Surgery, Medical School, </institution><institution>University of Minnesota, </institution></institution-wrap>Minneapolis, MN USA </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/017zqws13</institution-id><institution-id institution-id-type="GRID">grid.17635.36</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8657</institution-id><institution>Center for Learning Health System Sciences, </institution><institution>University of Minnesota, </institution></institution-wrap>Minneapolis, MN 55455 USA </aff></contrib-group><pub-date pub-type="epub"><day>6</day><month>4</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>6</day><month>4</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>16</volume><elocation-id>3280</elocation-id><history><date date-type="received"><day>17</day><month>11</month><year>2023</year></date><date date-type="accepted"><day>7</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">The rapid growth of biomedical literature poses challenges for manual knowledge curation and synthesis. Biomedical Natural Language Processing (BioNLP) automates the process. While Large Language Models (LLMs) have shown promise in general domains, their effectiveness in BioNLP tasks remains unclear due to limited benchmarks and practical guidelines. We perform a systematic evaluation of four LLMs&#x02014;GPT and LLaMA representatives&#x02014;on 12 BioNLP benchmarks across six applications. We compare their zero-shot, few-shot, and fine-tuning performance with the traditional fine-tuning of BERT or BART models. We examine inconsistencies, missing information, hallucinations, and perform cost analysis. Here, we show that traditional fine-tuning outperforms zero- or few-shot LLMs in most tasks. However, closed-source LLMs like GPT-4 excel in reasoning-related tasks such as medical question answering. Open-source LLMs still require fine-tuning to close performance gaps. We find issues like missing information and hallucinations in LLM outputs. These results offer practical insights for applying LLMs in BioNLP.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">Baseline performance, benchmarks, and guidance for LLMs in biomedicine are limited. The authors assess four LLMs on 12 tasks, establish baselines, examine hallucinations, and provide recommendations for optimal LLM use.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Data mining</kwd><kwd>Health care</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/100000092</institution-id><institution>U.S. Department of Health &#x00026; Human Services | NIH | U.S. National Library of Medicine (NLM)</institution></institution-wrap></funding-source><award-id>1K99LM01402</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Qingyu</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par3">Biomedical literature presents direct obstacles to curation, interpretation, and knowledge discovery due to its vast volume and domain-specific challenges. PubMed alone sees an increase of approximately 5000 articles every day, totaling over 36 million as of March 2024<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. In specialized fields such as COVID-19, roughly 10,000 dedicated articles are added each month, bringing the total to over 0.4 million as of March 2024<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. In addition to volume, the biomedical domain also poses challenges with ambiguous language. For example, a single entity such as Long COVID can be referred to using 763 different terms<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Additionally, the same term can describe different entities, as seen with the term AP2, which can refer to a gene, a chemical, or a cell line<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Beyond entities, identifying novel biomedical relations and capturing semantics in biomedical literature present further challenges<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>.</p><p id="Par4">To overcome these challenges, biomedical natural language processing (BioNLP) techniques are used to assist with manual curation, interpretation, and knowledge discovery. Biomedical language models are considered as the backbone of BioNLP methods; they leverage massive amounts of biomedical literature and capture biomedical semantic representations in an unsupervised or self-supervised manner. Early biomedical language models are non-contextual embeddings (e.g., word2vec and fastText) that use fully connected neural networks such as BioWordVec and BioSentVec<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. Since the inception of transformers, biomedical language models have adopted their architecture, and can be categorized into (1) encoder-based, masked language models using the encoder from the transformer architecture such as the biomedical bidirectional encoder representations from transformers (BERT) family including BioBERT and PubMedBERT<sup><xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR11">11</xref></sup>, (2) decoder-based, generative language models using the decoder from the transformer architecture such as the generative pre-trained transformer (GPT) family including BioGPT and BioMedLM<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>, and (3) encoder-decoder-based, using both encoders and decoders such as BioBART and Scifive<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>. BioNLP studies fine-tuned those language models and demonstrated that they achieved the SOTA performance in various BioNLP applications<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>, and those models have been successfully employed in PubMed-scale downstream applications such as biomedical sentence search<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> and COVID-19 literature mining<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>.</p><p id="Par5">Recently, the latest closed-source GPT models, including GPT-3 and, more notably, GPT-4, have made significant strides and garnered considerable attention from society. A key characteristic of these models is the exponential growth of their parameters. For instance, GPT-3 has ~175 billion parameters, which is hundreds larger than GPT-2. Models of this magnitude are commonly referred to as Large Language Models (LLMs)<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Moreover, the enhancement of LLMs is achieved through reinforcement learning with human feedback, thereby aligning text generation with human preferences<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. For instance, GPT-3.5 builds upon the foundation of GPT-3 using reinforcement learning techniques, resulting in significantly improved performance in natural language understanding<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. The launch of ChatGPT&#x02014;a chatbot using GPT-3.5 and GPT-4&#x02014;has marked a milestone in generative artificial intelligence. It has demonstrated strong capabilities in the tasks that its predecessors fail to do; for instance, GPT-4 passed over 20 academic and professional exams, including the Uniform Bar Exam, SAT Evidence-Based Reading &#x00026; Writing, and Medical Knowledge Self-Assessment Program<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. The remarkable advancements have sparked extensive discussions among society, with excitement and concerns alike. In addition to closed-source LLMs, open-source LLMs, such as LLaMA<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> and Mixtral<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> have been widely adopted in downstream applications and also used as the basis for continuous pretraining domain-specific resources. In the biomedical domain, PMC LLaMA (7B and 13B) is one of the first biomedical domain-specific LLMs that continuously pre-trained LLaMA on 4.8&#x02009;M biomedical papers and 30&#x02009;K medical textbooks<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Meditron (7B and 70B), a more recent biomedical domain-specific LLM, employed a similar continuous pretraining strategy on LLaMA 2.</p><p id="Par6">Pioneering studies have conducted early experiments on LLMs in the biomedical domain and reported encouraging results. For instance, Bubeck et al. studied the ability of GPT-4 in a wide spectrum, such as coding, mathematics, and interactions with humans. This early study reported biomedical-related results, indicating that GPT-4 achieved an accuracy of approximately 80% in the US Medical Licensing Exam (Step 1, 2, and 3), along with an example of using GPT-4 to verify claims in a medical note. Lee et al. also demonstrated use cases of GPT-4 for answering medical questions, generating summaries from patient reports, assisting clinical decision-making, and creating educational materials<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Wong et al. conducted a study on GPT-3.5 and GPT-4 for end-to-end clinical trial matching, handling complex eligibility criteria, and extracting complex matching logic<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Liu et al. explored the performance of GPT-4 on radiology domain-specific use cases<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Nori et al. further found that general-domain LLMs with advanced prompt engineering can achieve the highest accuracy in medical question answering without fine-tuning<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Recent reviews also summarize related studies in detail<sup><xref ref-type="bibr" rid="CR28">28</xref>&#x02013;<xref ref-type="bibr" rid="CR30">30</xref></sup>.</p><p id="Par7">These results demonstrate the potential of using LLMs in BioNLP applications, particularly when minimal manually curated gold standard data is available and fine-tuning or retraining for every new task is not required. In the biomedical domain, a primary challenge is the limited availability of labeled datasets, which have a significantly lower scale than those in the general domain (e.g., a biomedical sentence similarity dataset only has 100 labeled instances in total<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>)<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. This challenges the fine-tuning approach because (1) models fine-tuned on limited labeled datasets may not be generalizable, and (2) it becomes more challenging to fine-tune the models with a larger size.</p><p id="Par8">Motivated by the early experiments, it is important to systematically assess the effectiveness of LLMs in BioNLP tasks and comprehend their impact on BioNLP method development and downstream users. Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> provides a detailed comparison of representative studies in this context. While our primary focus is on the biomedical domain, specifically the evaluation of LLMs using biomedical literature, we have also included two representative studies in the clinical domain (evaluating LLMs using clinical records) for reference. There are several primary limitations. First, most evaluation studies primarily assessed GPT-3 or GPT-3.5, which may not provide a full spectrum of representative LLMs from different categories. For instance, few studies evaluated more advanced closed-source LLMs such as GPT-4, LLM representatives from the general domain such as LLaMA<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, and biomedical domain-specific LLMs such as PMC-LLaMA<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. Second, the existing studies mostly assessed extraction tasks where the gold standard is fixed. Few of these studies evaluated generative tasks such as text summarization and text simplification where the gold standard is free-text. Arguably, existing transformer models have demonstrated satisfactory performance in extractive tasks, while generative tasks remain a challenge in terms of achieving similar levels of proficiency. Therefore, it is imperative to assess how effective LLMs are in the context of generative tasks in BioNLP, examining whether they can complement existing models. Third, most existing studies only reported quantitative assessments such as the F1-score, with limited emphasis on qualitative evaluations. However, conducting qualitative evaluations (e.g., assessing the quality of LLM-generated text and categorizing inconsistent or hallucinated responses) to understand of the errors and impacts of LLMs on downstream applications in the biomedical domain are arguably more critical than mere quantitative metrics. For instance, studies on LLMs found a relatively low correlation between human judgments and automatic measures, such as ROUGE-L, commonly applied to text summarization tasks in the clinical domain<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Finally, it is worth noting that several studies did not provide public access to their associated data or codes. For example, few studies have made the prompts or selected examples for few-shot learning available. This hinders reproducibility and also presents challenges in evaluating new LLMs using the same setting for a fair comparison.<table-wrap id="Tab1"><label>Table 1</label><caption><p>A comparison of key elements from representative studies assessing large language models (LLMs) in the biomedical and clinical domains as of March 2024</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Study</th><th rowspan="2">Domain</th><th rowspan="2">Model</th><th rowspan="2">Tasks</th><th colspan="2">Evaluation scope<sup>a</sup></th><th colspan="2">Evaluation setting</th><th colspan="3">Evaluation measures</th><th>Availability</th></tr><tr><th>Extractive/<break/>Classification</th><th>Generative</th><th>Zero/<break/>Few-shot</th><th>Fine-tuning<sup>c</sup></th><th>Quantitative</th><th>Qualitative<sup>d</sup></th><th>Cost analysis</th><th/></tr></thead><tbody><tr><td><sup><xref ref-type="bibr" rid="CR80">80</xref></sup></td><td>Clinical</td><td><p>T5,</p><p>GPT-3</p></td><td><p>Clinical language inference,</p><p>Radiology question answering,</p><p>Discharge summary classification</p></td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>Y</td></tr><tr><td><sup><xref ref-type="bibr" rid="CR76">76</xref></sup></td><td>Clinical</td><td>GPT-3</td><td><p>Clinical sense disambiguation,</p><p>Biomedical evidence extraction,</p><p>Coreference resolution,</p><p>Medication status extraction,</p><p>Medication attribute extraction</p></td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr><tr><td><sup><xref ref-type="bibr" rid="CR43">43</xref></sup></td><td>Biomedical</td><td><p>BERT,</p><p>GPT-3</p></td><td><p>Named entity recognition,</p><p>Relation extraction,</p></td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>Y</td></tr><tr><td><sup><xref ref-type="bibr" rid="CR44">44</xref></sup></td><td>Biomedical</td><td><p>BERT,</p><p>GPT-3.5</p></td><td>Relation extraction</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr><tr><td><sup><xref ref-type="bibr" rid="CR27">27</xref></sup></td><td>Biomedical</td><td><p>Med-PaLM,</p><p>GPT-4</p></td><td>Question answering</td><td>N</td><td>Y</td><td>Y</td><td>N</td><td>Y</td><td>Y</td><td>N</td><td>N</td></tr><tr><td><sup><xref ref-type="bibr" rid="CR81">81</xref></sup></td><td>Biomedical</td><td><p>BERT,</p><p>GPT-3.5,</p><p>GPT-4</p></td><td><p>Biomedical reasoning,</p><p>Document classification</p></td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr><tr><td><sup><xref ref-type="bibr" rid="CR82">82</xref></sup></td><td>Biomedical</td><td><p>BERT,</p><p>GPT-3.5</p></td><td><p>Named entity recognition,</p><p>Relation extraction,</p><p>Document classification,</p><p>Question answering</p></td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>Y</td><td>N</td><td>N</td><td>N</td></tr><tr><td>Ours</td><td>Biomedical</td><td><p>BERT, BART,</p><p>LLaMA 2,</p><p>PMC LLaMA,</p><p>GPT-3.5,</p><p>GPT-4</p></td><td><p>Named entity recognition,</p><p>Relation extraction,</p><p>Document classification,</p><p>Question answering,</p><p>Text summarization,</p><p>Text simplification</p></td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td><td>Y</td></tr></tbody></table><table-wrap-foot><p>The table categorizes each study by its domain of focus (Biomedical or Clinical), the models evaluated, the evaluation scope including extractive tasks such as named entity recognition (NER) and generative tasks such as text summarization, the evaluation measures including quantitative evaluation metrics (such as the F1-score), qualitative evaluation metrics (such as the completeness in a scale of 1&#x02014;5), and the accessibility of data, prompts, and codes to the public. <sup>a</sup>Extractive or classification: the tasks where the gold standard is fixed, e.g., relation extraction. <sup>b</sup>Generative: text summarization and text simplification tasks where the gold standard is free-text. <sup>c</sup>Fine-tuning: an LLM is further tuned on specific datasets. <sup>d</sup>Qualitative: tasks such as manual validations on the quality of LLM-generated text.</p></table-wrap-foot></table-wrap></p><p id="Par9">In this study, we conducted a comprehensive evaluation of LLMs in BioNLP applications to examine their great potentials as well as their limitations and errors. Our study has three main contributions.</p><p id="Par10">First, we performed comprehensive evaluations on four representative LLMs: GPT-3.5 and GPT-4 (representatives from closed-source LLMs), LLaMA 2 (a representative from open-sourced LLMs), and PMC LLaMA (a representative from biomedical domain-specific LLMs). We evaluated them on 12 BioNLP datasets across six applications: (1) named entity recognition, which extracts biological entities of interest from free-text, (2) relation extraction, which identifies relations among entities, (3) multi-label document classification, which categorizes documents into broad categories, (4) question answering, which provides answers to medical questions, (5) text summarization, which produces a coherent summary of an input text, and (6) text simplification, which generates understandable content of an input text. The models were evaluated under four settings: zero-shot, static few-shot, dynamic K-nearest few-shot, and fine-tuning where applicable. We compared these models against the state-of-the-art (SOTA) approaches that use fine-tuned, domain-specific BERT or BART models. Both BERT and BART models are well-established in BioNLP research.</p><p id="Par11">Our results suggest that SOTA fine-tuning approaches outperformed zero- and few-shot LLMs in most of the BioNLP tasks. These approaches achieved a macro-average approximately 15% higher than the best zero- and few-shot LLM performance across 12 benchmarks (0.65 vs. 0.51) and over 40% higher in information extraction tasks, such as relation extraction (0.79 vs. 0.33). However, closed-source LLMs such as GPT-3.5 and GPT-4 demonstrated better zero- and few-shot performance in reasoning-related tasks such as medical question answering, where they outperformed the SOTA fine-tuning approaches. In addition, they exhibited lower-than-SOTA but reasonable performance in generation-related tasks such as text summarization and simplification, showing competitive accuracy and readability, as well as showing potential in semantic understanding tasks such as document-level classification. Among the LLMs, GPT-4 showed the overall highest performance, especially due to its remarkable reasoning capability. However, it comes with a trade-off, being 60 to 100 times more expensive than GPT-3.5. In contrast, open-sourced LLMs such as LLaMA 2 did not demonstrate robust zero- and few-shot performance &#x02013; they still require fine-tuning to bridge the performance gap for BioNLP applications.</p><p id="Par12">Second, we conducted a thorough manual validation on collectively over hundreds of thousands of sample outputs from the LLMs. For extraction and classification tasks where the gold standard is fixed (e.g., relation extraction and multi-label document classification), we examined (1) missing output, when LLMs fail to provide the requested output, (2) inconsistent output, when LLMs produce different outputs for similar instances, and (3) hallucinated output, when LLMs fail to address the user input and may contain repetitions and misinformation in the output<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. For text summarization tasks, two healthcare professionals performed manual evaluations assessing Accuracy, Completeness, and Readability. The results revealed prevalent cases of missing, inconsistent, and hallucinated outputs, especially for LLaMA 2 under the zero-shot setting. For instance, it had over 102 hallucinated cases (32% of the total testing instances) and 69 inconsistent cases (22%) for a multi-label document classification dataset.</p><p id="Par13">Finally, we provided recommendations for downstream users on the best practice to use LLMs in BioNLP applications. We also noted two open problems. First, the current data and evaluation paradigms in BioNLP are tailored to supervised methods and may not be fair to LLMs. For instance, the results showed that automatic metrics for text summarization may not align with manual evaluations. Also, the datasets that specifically target tasks where LLMs excel, such as reasoning, are limited in the biomedical domain. Revisiting data and evaluation paradigms in BioNLP are key to maximizing the benefits of LLMs in BioNLP applications. Second, addressing errors, missing information, and inconsistencies is crucial to minimize the risks associated with LLMs in biomedical and clinical applications. We strongly encourage a community effort to find better solutions to mitigate these issues.</p><p id="Par14">We believe that the findings of this study will be beneficial for BioNLP downstream users and will also contribute to further enhancing the performance of LLMs in BioNLP applications. The established benchmarks and baseline performance could serve as the basis for evaluating new LLMs in the biomedical domain. To ensure reproducibility and facilitate benchmarking, we have made the relevant data, models, and results publicly accessible through 10.5281/zenodo.14025500<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Quantitative evaluations</title><p id="Par15">Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> illustrates the primary evaluation metric results and their macro-averages of the LLMs under zero/few-shot (static one- and five-shot) and fine-tuning settings over the 12 datasets. The results on specific datasets were consistent with those independently reported by other studies, such as an accuracy of 0.4462 and 0.7471 on MedQA for GPT-3.5 zero-shot and GPT-4 zero-shot, respectively (0.4988 and 0.7156 in our study, respectively)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Similarly, a micro-F1 of 0.6224 and 0.6720 on HoC and LitCovid for GPT-3.5 zero-shot was reported, respectively (0.6605 and 0.6707 in our study, respectively)<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. An accuracy of 0.7790 on PubMedQA was also reported for the fine-tuned PMC LLaMA 13B (combined multiple question answering datasets for fine-tuning)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>; our study also reported a similar accuracy of 0.7680 using the PubMedQA training set only. We further summarized detailed results in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S2</xref> Quantitative evaluation results, including secondary metric results in S2.2, performance mean, variance, and confidence intervals in S2.3, statistical test results in S2.4, and dynamic K-nearest few-shot results in S2.5.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Quantitative evaluations of the LLMs on the 12 benchmarks under zero/few-shot (including static one- and five-shot)) and fine-tuned settings</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="3" colspan="2"/><th rowspan="3">SOTA results<break/>before the LLMs<break/>(Foundation model)</th><th colspan="9">Zero/Few-shot</th><th colspan="2">Fine-tuned</th></tr><tr><th colspan="3">Zero-shot</th><th colspan="3">One-shot</th><th colspan="3">Five-shot</th><th colspan="2"/></tr><tr><th>GPT-3.5</th><th>GPT-4</th><th>LLaMA 2 13B</th><th>GPT-3.5</th><th>GPT-4</th><th>LLaMA 2<break/>13B</th><th>GPT-3.5</th><th>GPT-4</th><th>LLaMA 2<break/>13B<sup>b</sup></th><th>LLaMA 2<break/>13B</th><th>PMC LLaMA 13B</th></tr></thead><tbody><tr><td colspan="14">Named entity recognition</td></tr><tr><td>BC5CDR-chemical</td><td>Entity F1</td><td><p>0.9500<sup><xref ref-type="bibr" rid="CR83">83</xref></sup></p><p>(PubMedBERT)</p></td><td>0.6274</td><td>0.7993</td><td>0.3944</td><td>0.7133</td><td>0.8327*</td><td>0.6276</td><td>0.7228</td><td>0.7979</td><td>0.5530</td><td>0.9149</td><td>0.9063</td></tr><tr><td>NCBI Disease</td><td>Entity F1</td><td><p>0.9090<sup><xref ref-type="bibr" rid="CR83">83</xref></sup></p><p>(PubMedBERT)</p></td><td>0.4060</td><td>0.5827</td><td>0.2211</td><td>0.4817</td><td>0.5988</td><td>0.3811</td><td>0.4309</td><td>0.6389*</td><td>0.4847</td><td>0.8682*</td><td>0.8353</td></tr><tr><td colspan="14">Relation extraction</td></tr><tr><td>ChemProt</td><td>Macro F1</td><td><p>0.7344<sup><xref ref-type="bibr" rid="CR84">84</xref></sup></p><p>(BioBERT)</p></td><td>0.1345</td><td>0.3250</td><td>0.1392</td><td>0.1280</td><td>0.3391</td><td>0.0718</td><td>0.1758</td><td>0.3756</td><td>0.0967</td><td>0.4612*</td><td>0.3111</td></tr><tr><td>DDI2013</td><td>Macro F1</td><td><p>0.7919<sup><xref ref-type="bibr" rid="CR85">85</xref></sup></p><p>(BioBERT)</p></td><td>0.2004</td><td>0.2968</td><td>0.1305</td><td>0.2126</td><td>0.3312</td><td>0.1779</td><td>0.1706</td><td>0.3276</td><td>0.1663</td><td>0.6218</td><td>0.5700</td></tr><tr><td colspan="14">Multi-label document classification</td></tr><tr><td>HoC</td><td>Macro F1</td><td><p>0.8882<sup><xref ref-type="bibr" rid="CR86">86</xref></sup></p><p>(BioBERT)</p></td><td>0.6722</td><td>0.7109</td><td>0.1285</td><td>0.6671</td><td>0.7093</td><td>0.3072</td><td>0.6994</td><td>0.7099</td><td>0.1797</td><td>0.6957*</td><td>0.4221</td></tr><tr><td>LitCovid</td><td>Macro F1</td><td><p>0.8921<sup><xref ref-type="bibr" rid="CR86">86</xref></sup></p><p>(BioBERT)</p></td><td>0.5967</td><td>0.5883</td><td>0.3825</td><td>0.6009</td><td>0.5901</td><td>0.4808</td><td>0.6179</td><td>0.6077</td><td>0.3305</td><td>0.5725*</td><td>0.4273</td></tr><tr><td colspan="14">Question answering</td></tr><tr><td>MedQA (5-Option)</td><td>Accuracy</td><td><p>0.4195<sup>a</sup><sup><xref ref-type="bibr" rid="CR87">87</xref></sup></p><p>(BioLinkBERT)</p></td><td>0.4988</td><td>0.7156</td><td>0.2522</td><td>0.5161</td><td>0.7439</td><td>0.2899</td><td>0.5208</td><td>0.7651*</td><td>0.3504</td><td>0.4462*</td><td>0.3975</td></tr><tr><td>PubMedQA</td><td>Accuracy</td><td><p>0.7340<sup><xref ref-type="bibr" rid="CR87">87</xref></sup></p><p>(BioLinkBERT)</p></td><td>0.6560</td><td>0.6280</td><td>0.5520</td><td>0.4600</td><td>0.7100</td><td>0.2660</td><td>0.6920</td><td>0.7580*</td><td>0.6000</td><td>0.8040*</td><td>0.7680</td></tr><tr><td colspan="14">Text summarization</td></tr><tr><td>PubMed</td><td>Rouge-L</td><td><p>0.4316<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p><p>(BART)</p></td><td>0.2274</td><td>0.2419</td><td>0.1190</td><td>0.2351</td><td>0.2427</td><td>0.0989</td><td>0.2423</td><td>0.2444</td><td>0.1629</td><td>0.1857*</td><td>0.1684</td></tr><tr><td>MS^2</td><td>Rouge-L</td><td><p>0.2080<sup><xref ref-type="bibr" rid="CR50">50</xref></sup></p><p>(BART)</p></td><td>0.0889</td><td>0.1224</td><td>0.0948</td><td>0.1132</td><td>0.1248</td><td>0.0320</td><td>0.1013</td><td>0.1218</td><td>0.1205</td><td>0.0934*</td><td>0.0059</td></tr><tr><td colspan="14">Text simplification</td></tr><tr><td>Cochrane PLS</td><td>Rouge-L</td><td><p>0.4476<sup><xref ref-type="bibr" rid="CR88">88</xref></sup></p><p>(BART)</p></td><td>0.2365</td><td>0.2375</td><td>0.2081</td><td>0.2447</td><td>0.2385</td><td>0.2207</td><td>0.2470</td><td>0.2469</td><td>0.2283</td><td>0.2355</td><td>0.2370</td></tr><tr><td>PLOS</td><td>Rouge-L</td><td><p>0.4368<sup><xref ref-type="bibr" rid="CR70">70</xref></sup></p><p>(BART)</p></td><td>0.2323</td><td>0.2253</td><td>0.2121</td><td>0.2449*</td><td>0.2386</td><td>0.1836</td><td>0.2416</td><td>0.2409</td><td>0.1656</td><td>0.2583</td><td>0.2577</td></tr><tr><td>Macro-average</td><td/><td>0.6536</td><td>0.3814</td><td>0.4561</td><td>0.2362</td><td>0.3848</td><td>0.4750</td><td>0.2614</td><td>0.4052</td><td>0.4862</td><td>0.2866</td><td>0.5131</td><td>0.4422</td></tr></tbody></table><table-wrap-foot><p>The primary metric results are reported. State-of-the-art (SOTA) results, representing the reported best performance of studies using fine-tuned (domain-specific) language models before the LLMs and their backbone models, are also provided. The SOTA results are directly extracted from the studies. <sup>a</sup>the study reported accuracy on MedQA (4-option); we applied the released model for inference on MedQA (5-option). <sup>b</sup>The inputs for LLaMA 2 were truncated for question answering, text summarization, and text simplification tasks under the five-shot setting due to its input token length limit detailed in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref> Prompt engineering. The highest performance under either zero/few-shot or fine-tuned settings is marked in bold. For instance, GPT-4 one-shot achieved the highest performance under the zero/few-shot setting, and LLaMA 2 13B fine-tuned achieved the highest performance under the fine-tuned setting in the BC5CDR-chemical dataset. A two-tailed Wilcoxon rank-sum test with bootstrapping, using a subsample size of 30 and 100 repetitions at a 95% confidence interval, was conducted for both zero/few-shot and fine-tuning settings. An asterisk (*) indicates if the P-value of the best performance is less than 0.05 for all the models under either the zero/few-shot or fine-tuned settings. Continuing with the BC5CDR-chemical example, the P-value of GPT-4 one-shot was less than 0.05 for all others under the zero/few-shot setting, whereas LLaMA 2 13B fine-tuned was not under the fine-tuned setting. Detailed results, including performance mean and variance, statistical test results, dynamic few-shot, and secondary metrics are provided in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S2</xref> Quantitative evaluation results S2.4.</p></table-wrap-foot></table-wrap></p><p id="Par16">SOTA vs. LLMs. The results of SOTA fine-tuning approaches for comparison are provided in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>. Recall that the SOTA approaches utilized fine-tuned (domain-specific) language models. For the extractive and classification tasks, the SOTA approaches fine-tuned biomedical domain-specific BERT models such as BioBERT and PubMedBERT. For text summarization and simplification tasks, the SOTA approaches fine-tuned BART models.</p><p id="Par17">As demonstrated in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, the SOTA fine-tuning approaches had a macro-average of 0.6536 across the 12 datasets, whereas the best LLM counterparts were 0.4561, 0.4750, 0.4862, and 0.5131 under zero-shot, one-shot, five-shot, and fine-tuning settings, respectively. It outperformed the zero- and few-shot of LLMs in 10 out of the 12 datasets. It had much higher performance especially in information extraction tasks. For instance, for NCBI Disease, the SOTA approach achieved an entity-level F1-score of 0.9090, whereas the best results of LLMs (GPT-4) under zero- and one-shot settings were 30% lower (0.5988). The performance of LLMs is closer under the fine-tuning setting, with LLaMA 2 13B achieving an entity-level F1-score of 0.8682, but it is still lower. Notably, the SOTA fine-tuning approaches are very strong baselines &#x02013; they were much more sophisticated than simple fine-tuning over a foundation model. Continuing with the example of NCBI Disease, the SOTA fine-tuning approach generated large-scale weak labeled examples and used contrastive learning to learn a general representation.</p><p id="Par18">In contrast, the LLMs outperformed the SOTA fine-tuning approaches in question answering. For MedQA, the SOTA approach had an accuracy of 0.4195. GPT-4 under the zero-shot setting had almost 30% higher accuracy in absolute difference (0.7156), and GPT-3.5 also had approximately 8% higher accuracy (0.4988) under the zero-shot setting. For PubMedQA, the SOTA approach had an accuracy of 0.7340. GPT-4 under the one-shot setting had a similar accuracy (0.7100) and showed higher accuracy with more shots (0.7580 under the five-shot setting), as we will show later. Both LLaMA 2 13B and PMC LLaMA 13B also had higher accuracy under the fine-tuning setting (0.8040 and 0.7680, respectively). In this case, GPT-3.5 did not achieve higher accuracy over the SOTA approach, but it already had a competitive accuracy (0.6950) under the five-shot setting.</p><p id="Par19">Comparisons among the LLMs. Comparing among the LLMs, under zero/few-shot settings, the results demonstrate that GPT-4 consistently had the highest performance. Under the zero-shot setting, the macro-average of GPT-4 was 0.4561, which is approximately 7% higher than GPT-3.5 (0.3814) and almost double than LLaMA 2 13B (0.2362). It achieved the highest performance in nine out of the 12 datasets, and its performance was also within 3% of the best result for the remaining three datasets. The one-shot and five-shot settings showed very similar patterns.</p><p id="Par20">In addition, LLaMA2 13B exhibited substantially lower performance than GPT-3.5 (15% lower and 10% lower) and GPT-4 (22% lower and 17% lower) under zero- and one-shot settings. It had up to six times lower performance in specific datasets compared to the best LLM results; for example, 0.1286 vs. 0.7109 for HoC under the zero-shot setting. These results suggest that LLaMA2 13B still requires fine-tuning to achieve similar performance and bridge the performance gap. Fine-tuning improved LLaMA 2 13B&#x02019;s macro-average from 0.2837 to 0.5131. Notably, its performance under the fine-tuning setting is slightly higher than the zero- and few-shot performance of GPT-4. Fine-tuning LLaMA 2 13B generally improved its performance in all tasks except text summarization and text simplification. A key reason for its performance limitation is that the datasets have much longer input context than its allowed input tokens (4096) such that fine-tuning did not help in this case. This observation also motivates further research efforts on extending LLMs&#x02019; context window<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>.</p><p id="Par21">Under the fine-tuning setting, the results also indicate that PMC LLaMA 13B, as a continuously pretrained biomedical domain-specific LLM, did not achieve an overall higher performance than LLaMA 2 13B. Fine-tuned LLaMA 2 13B had better performance than that of PMC LLaMA 13B in 10 out of the 12 datasets. As mentioned, we reproduced similar results reported in PMC LLaMA study<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. For instance, it reported an accuracy of 0.7790 on PubMedQA with fine-tuning multiple question answering datasets together. We got a very similar accuracy of 0.7680 when fine-tuning PMC LLaMA 13B on the PubMedQA dataset only. However, we also found that directly fine-tuning of LLaMA 2 13B using the exact same setting resulted in better or at least similar performance.</p><sec id="Sec4"><title>Few-shot and cost analysis</title><p id="Par22">Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> further illustrates the performance of the dynamic K-nearest few-shot and the associated cost with the increasing number of shots. The detailed results are also provided in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S2</xref>. Dynamic K-nearest few-shot was conducted for K values of one, two, and five. For comparison, we also provided the zero-shot and static one-shot performance in the figure. The results suggest that dynamic K-nearest few-shot is most effective for multi-label document classification and question answering. For instance, for the LitCovid dataset, GPT-4 had a macro-F1 of 0.5901 under the static one-shot setting; in contrast, its macro-F1 under dynamic one-nearest shot was 0.6500 and further increased to 0.7055 with five-nearest shots. Similarly, GPT-3.5 exhibited improvements, with its macro-F1 under the static one-shot setting at 0.6009, compared to 0.6364 and 0.6484 for dynamic one-shot and five-shot, respectively. For question answering, the improvement was not as high as for multi-label document classification, but the overall trend showed a steady increase, especially considering that GPT-4 already had similar or higher performance than SOTA approaches with zero-shot. For instance, its accuracy on PubMedQA was 0.71 with a static one-shot; the accuracy increased to 0.72 and 0.75 under dynamic one-shot and five-shot, respectively.<fig id="Fig1"><label>Fig. 1</label><caption><title>Dynamic K-nearest few-shot results (<italic>K</italic>&#x02009;=&#x02009;1, 2, and 5) shown in line charts, with associated costs (dollars per 100 instances) depicted in bar charts for each benchmark.</title><p>The input and output types for each benchmark are displayed at the bottom of each subplot. Detailed methods for the few-shot and cost analysis are summarized in the Data and Methods section. Dynamic K-nearest few-shot involves selecting the K closest training instances as examples for each testing instance. Additionally, the performance of static one-shot (using the same one-shot example for each testing instance) is shown as a dashed horizontal line for comparison. Detailed performance in digits is also provided in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S2</xref>.</p></caption><graphic xlink:href="41467_2025_56989_Fig1_HTML" id="d33e1447"/></fig></p><p id="Par23">In contrast, the results show that dynamic K-nearest few-shot was less effective for other tasks. For instance, the dynamic one-shot performance is lower than the static one-shot performance for both GPT models on the two named entity recognition datasets, and by increasing the number of dynamic shots does not help either. Similar findings are also observed in relation extraction. For text summarization and text simplification tasks, the dynamic K-nearest few-shot performance was slightly higher in two datasets, but in general, it was very similar to the static one-shot performance. In addition, the results also suggest that increasing the number of shots does not necessarily improve the performance. For instance, GPT-4 with dynamic five-shot did not have the highest performance in eight out of the 12 datasets. Similar findings were reported in other studies, where the performance of GPT-3.5 with five-shot learning was lower than that of zero-shot learning for natural language inference tasks<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>.</p><p id="Par24">Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> further compares the costs per 100 instances of using GPT-3.5 and GPT-4. The cost is calculated based on the number of input and output tokens with unit price. We used gpt-4-0613 for extractive tasks and gpt-4-32k-0613 for generative tasks because the input and output context are much longer especially with more shots. GPT-4 generally exhibited the highest performance, as shown in both Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> and Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>; however, the cost analysis results also demonstrate a clear trade-off, with GPT-4 being 60 to 100 times more expensive. For extractive and classification tasks, the actual cost per 100 instances of GPT-4 for five-shots ranges from approximately $2 for sentence-level inputs to around $10 for abstract-level inputs. This cost is 60 to 70 times higher than that of GPT-3.5, which costs approximately $0.03 for sentence-level inputs and around $0.16 for abstract-level inputs with five-shots. For generative tasks, the cost difference is even more pronounced, scaling to 100 times or more expensive. One reason is that GPT-4 32&#x02009;K has a higher unit price, and tasks like text summarization involve much longer input and output tokens. Taking the PubMed Text Summarization dataset as an example, GPT-4 cost $84.02 per 100 instances with five-shots, amounting to approximately $5600 to inference the entire testing set. In comparison, GPT-3 only cost $0.71 per 100 instances for five-shots, totaling around $48 for the entire testing set.</p><p id="Par25">Based on both performance and cost results, it indicates that the cost difference does not necessarily scale to the performance difference, except for question answering tasks. GPT-4 exhibited 20% to 30% higher accuracy than GPT-3.5 in question-answering tasks, and higher than the SOTA approaches; for other tasks, the performance difference is much smaller with a significantly higher cost. For instance, the performance of GPT-4 on both text simplification tasks was within 2% of that of GPT-3.5, but the actual cost was more than 100 times higher.</p></sec></sec><sec id="Sec5"><title>Qualitative evaluations</title><sec id="Sec6"><title>Error analysis on named entity recognition</title><p id="Par26">Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2A</xref> further shows an error analysis on the named entity recognition benchmark NCBI Disease, where the performance of LLMs under zero- and few-shot settings was substantially lower than SOTA results (e.g., the LLaMA 2 13B zero-shot performance is almost 70% lower). Recall that named entity recognition extracts entities from free text, and the benchmarks evaluate the accuracy of these extracted entities. We examined all the predictions on full test sets and categorized into four types: (1) correct entities, where the predicted entities are correct with both text spans and entity types, (2) wrong entities, where the predicted entities are incorrect, (3) missing entities, where the true entities are not predicted, and (4) boundary issues, where the predicted entities are correct but with different text spans than the gold standard, as shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2A</xref>. The results reveal that the LLMs can predict up to 512 entities correctly out of 960 in total, explaining the low F1-score. As the SOTA model is not publicly available, we used an alternate fine-tuned BioBERT model on NCBI Disease from an independent study (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/ugaray96/biobert_ncbi_disease_ner">https://huggingface.co/ugaray96/biobert_ncbi_disease_ner</ext-link>), which had an entity-level F1-score of 0.8920 for comparison. It predicted 863 entities out of 960 correctly. The wrong entities, missing entities, and boundary issues were 111, 97, and 269, respectively.<fig id="Fig2"><label>Fig. 2</label><caption><title>Qualitative evaluation results on inconsistency, missing information, and hallucinations.</title><p><bold>A</bold> Error analysis on the named entity recognition benchmark NCBI Disease. Correct entities: the predicted entities are correct with both text spans and entity types; Wrong entities: the predicted entities are incorrect; Missing entities: true entities are not predicted; and Boundary issues: the predicted entities are correct but with different text spans than the gold standard. <bold>B</bold>&#x02013;<bold>D</bold> Qualitative evaluation on ChemProt, HoC, and MedQA where the gold standard is a fixed classification type or multiple-choice option. Inconsistent responses: the responses are in different formats; Missingness: the responses are missing; and Hallucinations, where LLMs fail to address the prompt and may contain repetitions and misinformation in the output.</p></caption><graphic xlink:href="41467_2025_56989_Fig2_HTML" id="d33e1499"/></fig></p><p id="Par27">In addition, Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2A</xref> also shows that GPT-4 had the lowest number of wrong entities, whereas other categories have a similar prevalence to GPT-3.5, which explains its higher F1-score overall. Furthermore, providing one shot did not alter the errors for GPT-3.5 and GPT-4 compared to their zero-shot settings, but it dramatically changed the results for LLaMA 2 13B. Under one-shot, LLaMA 2 13B had 449 correctly predicted entities, compared to 148 under zero-shot. Additionally, its missing entities also reduced from 812 to 511 with one-shot, but it also had a trade-off of more boundary issues and wrong entities.</p></sec><sec id="Sec7"><title>Evaluations on inconsistencies, missing information, and hallucinations</title><p id="Par28">Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2B&#x02013;D</xref> present the qualitative evaluation results on ChemProt, HoC, and MedQA, respectively. Recall that we categorized inconsistencies, missing information, and hallucinations on the tasks where the gold standard is a fixed classification type or a multiple-choice option. Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> also provides detailed examples. The findings show prevalent inconsistent, missing, or hallucinated responses, particularly in LLaMA 2 13B zero-shot responses. For instance, it exhibited 506 hallucinated responses (~3% out of the total 16,943 instances) and 2376 inconsistent responses (14%) for ChemProt. In the case of HoC, there were 102 (32%) hallucinated responses and 69 (22%) inconsistent responses. Similarly, for MedQA, there were 402 (32%) inconsistent responses. In comparison, GPT-3.5 and GPT-4 exhibited substantially fewer cases. GPT-3.5 showed a small number of inconsistent responses for ChemProt and HoC, and a few missing responses for MedQA. On the other hand, GPT-4 did not exhibit any such cases for ChemProt and HoC, while displaying a few missing responses for MedQA.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Examples of inconsistent, missing, and hallucinated responses</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Type</th><th>Example</th></tr></thead><tbody><tr><td>1. Inconsistency</td><td><p>Example 1. GPT-3 zero-shot on ChemProt</p><p><underline>Required output</underline>: one of the six relation types: CPR:3, CPR:4, CPR:5, CPR:6, CPR:9, or false</p><p><underline>Actual and expected output</underline>: CPR:9 [<italic>the expected output</italic>]</p><p><underline>Inconsistent output 1</underline>: SUBSTRATE [<italic>it might refer to CPR:9</italic>]</p><p><underline>Inconsistent output 2</underline>: SUBSTRATE PRODUCT OF [<italic>it might refer to CPR:9</italic>]</p><p>Example 2. LLaMA 2 zero-shot on MedQA</p><p><underline>Required output</underline>: a multiple-choice option from A to E</p><p><underline>Actual and somewhat expected output</underline>: I&#x02019;m happy to help! Based on the information provided, the correct answer is E. Age above 50 years. Aspirin is contraindicated&#x02026; [<italic>it directly provided the option at the start, which is similar to the expected output</italic>]</p><p><underline>Inconsistent output 1</underline>: I&#x02019;m happy to help! Based on the patient&#x02019;s symptoms and physical examination findings, the most likely diagnosis is pyloric stenosis. &#x02026; [<italic>it did not provide the answer option</italic>]</p><p><underline>Inconsistent output 2</underline>: Great! Let&#x02019;s break down the question and the options. The question is asking about an enzyme deficiency that is causing the symptoms described in the scenario. &#x02026; Therefore, option E, &#x02026;, is the correct answer. [<italic>it went through all the options and provided the answer option at last</italic>]</p><p><underline>Inconsistent output 3</underline>: Great question! To confirm the most likely diagnosis of this patient, we need to consider the following options: Options A, C, and D &#x02026; Therefore, the correct answer is: E. &#x02026; It is important to consider all possible causes&#x02026;</p></td></tr><tr><td>2. Missingness</td><td><p>Example 1. GPT-3.5 one-shot on MedQA</p><p><underline>Required output</underline>: a multiple-choice option from A to E</p><p><underline>Actual and expected output:</underline> C [<italic>the expected output</italic>]</p><p><underline>Missing output 1</underline>: [<italic>no answer</italic>]</p></td></tr><tr><td>3. Hallucinations</td><td><p>Example 1. LLaMA 2 zero-shot on HoC</p><p><underline>Required output</underline>: one or more document topic labels out of 10 in total separated by semicolons</p><p><underline>Actual and somewhat expected output</underline>: Based on the input article, the following topics are relevant: * Sustaining proliferative signaling * Resisting cell death * Enabling replicative immortality * Activating invasion and metastasis * Genomic instability and mutation * Tumor promoting inflammation * Cellular energetics * Avoiding immune destruction Please note that the article is related to multiple topics, and the output is a semicolon-separated list of relevant topics. [<italic>it provides the topics like the expected format though not separated by semicolons</italic>]</p><p><underline>Hallucinated output 1</underline>: Please provide the list of topics that are relevant to the input article.</p><p>Example 2. LLaMA 2 zero-shot on MedQA</p><p><underline>Required output</underline>: a multiple-choice option from A to E</p><p>Actual and expected out example is provided above</p><p><underline>Hallucinated output 1</underline>: Great! You have selected the correct answer. Let me explain why. &#x02026;</p><p><underline>Hallucinated output 2</underline>: That&#x02019;s correct! Tetralogy of Fallot is a congenital heart defect &#x02026;</p><p><underline>Hallucinated output 3</underline>: Great question! Based on the patient&#x02019;s symptoms and physical examination findings, the most likely impaired structure is the ________________. &#x02026; [<italic>it asks to fill in the blank</italic>]</p><p><underline>Hallucinated output 4</underline>: Please select one of the options from A to E.</p></td></tr></tbody></table><table-wrap-foot><p>Text in square brackets represents annotated explanations. Unnecessary detail is omitted due to space constraints.</p></table-wrap-foot></table-wrap></p><p id="Par29">It is worth noting that inconsistent responses do not necessarily imply that they fail to address the prompts; rather, the responses answer the prompt but in different formats. In contrast, hallucinated cases do not address the prompts and may repeat the prompts or contain irrelevant information. All such instances pose challenges for automatic extraction or postprocessing and may require manual review. As a potential solution, we observed that adding just one shot could significantly reduce such cases, especially for LLaMA 2 13B, which exhibited prevalent instances in zero-shot. As illustrated in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2B</xref>, LLaMA 2 13B one-shot dramatically reduced these cases in ChemProt and MedQA. Similarly, its hallucinated responses decreased from 102 to 0, and inconsistent cases decreased from 69 to 23 in HoC with one-shot. Another solution is fine-tuning, which we did not find any such cases during the manual examination, albeit with a trade-off of computational resources.</p></sec><sec id="Sec8"><title>Evaluations on accuracy, completeness, and readability</title><p id="Par30">Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> presents the qualitative evaluation results on the PubMed Text Summarization dataset. In Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3A</xref>, the overall results in accuracy, completeness, and readability for the four models on 50 random samples are depicted. The evaluation results in digits are further demonstrated in Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> for complementary. Detailed results with statistical analysis and examples are available in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S3</xref>. The fine-tuned BART model used in the SOTA approach<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, serving as the baseline, achieved an accuracy of 4.76 (out of 5), a completeness of 4.02, and a readability of 4.05. In contrast, both GPT-3.5 and GPT-4 demonstrated similar and slightly higher accuracy (4.79 and 4.83, respectively) and statistically significantly higher readability than the fine-tuned BART model (4.66 and 4.73), but statistically significantly lower completeness (3.61 and 3.57) under the zero-shot setting. The LLaMA 2 13B zero-shot performance is substantially lower in all three aspects.<fig id="Fig3"><label>Fig. 3</label><caption><title>Qualitative evaluation results on accuracy, completeness, and readability.</title><p><bold>A</bold> The overall results of the fine-tuned BART, GPT-3.5 zero-shot, GPT-4 zero-shot, and LLaMA 2 zero-shot models on a scale of 1 to 5, based on random 50 testing instances from the PubMed Text Summarization dataset. <bold>B</bold> and <bold>C</bold> display the number of winning, tying, and losing cases when comparing GPT-4 zero-shot to GPT-3.5 zero-shot and GPT-4 zero-shot to the fine-tuned BART model, respectively. Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> shows the results in digits for complementary. Detailed results, including statistical tests and examples, are provided in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S3</xref>.</p></caption><graphic xlink:href="41467_2025_56989_Fig3_HTML" id="d33e1708"/></fig><table-wrap id="Tab4"><label>Table 4</label><caption><p>Qualitative evaluation results on accuracy, completeness, and readability of the generated text for the fine-tuned BART, GPT-3.5 zero-shot, GPT-4 zero-shot, and LLaMA 2 zero-shot models on a scale of 1 to 5, based on random 50 testing instances from the PubMed Text Summarization dataset, to complement Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref></p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Fine-tuned BART</th><th>GPT-3.5 zero-shot</th><th>GPT-4 zero-shot</th><th>LLaMA 2 zero-shot</th></tr></thead><tbody><tr><td>Accuracy</td><td>4.76</td><td>4.79</td><td>4.83</td><td>3.42</td></tr><tr><td>Completeness</td><td>4.02</td><td>3.61</td><td>3.57</td><td>2.20</td></tr><tr><td>Readability</td><td>4.05</td><td>3.57</td><td>4.73</td><td>2.53</td></tr></tbody></table></table-wrap></p><p id="Par31">Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3B</xref> further compares GPT-4 to GPT-3.5 and the fine-tuned BART model in detail. In the comparison between GPT-4 and GPT-3.5, GPT-4 had a slightly higher number of winning cases in the three aspects (4 winning cases vs. 1 losing case for accuracy, 17 vs. 13 for completeness, and 13 vs. 6 for readability). Most of the cases resulted in a tie. When comparing GPT-4 to the fine-tuned BART model, GPT-4 had significantly more winning cases for readability (34 vs. 1) with much fewer winning cases for completeness (9 vs. 22).</p></sec></sec></sec><sec id="Sec9" sec-type="discussion"><title>Discussions</title><p id="Par32">First, the SOTA fine-tuning approaches outperformed zero- and few-shot performance of LLMs in most of BioNLP applications. As demonstrated in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, it had the best performance in 10 out of the 12 benchmarks. In particular, it outperformed zero- and few-shot LLMs by a large margin in information extraction and classification tasks such as named entity recognition and relation extraction, which is consistent to the existing studies<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup>. In contrast to, other tasks such as medical question answering, named entity recognition, and relation extraction require limited reasoning and extract information directly from inputs at the sentence-level. Zero- and few-shot learning may not be appropriate or sufficient for these conditions. For those tasks, arguably, fine-tuned biomedical domain-specific language models are still the first choice and have already set a high bar, according to the literature<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>.</p><p id="Par33">In addition, closed-source LLMs such as GPT-3.5 and GPT-4 demonstrated reasonable zero- and few-shot capabilities for three BioNLP tasks. The most promising task that outperformed the SOTA fine-tuning approaches is medical question answering, which involves reasoning<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. As shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> and Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, GPT-4 already outperformed previous fine-tuned SOTA approaches in MedQA and PubMedQA with zero- or few-shot learning. This is also supported by the existing studies on medical question answering<sup><xref ref-type="bibr" rid="CR38">38</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. The second potential use case is text summarization and simplification. As shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, those tasks are still less favored by the automatic evaluation measures; however, manual evaluation results show both GPT-3.5 and GPT-4 had higher readability and competitive accuracy compared to the SOTA fine-tuning approaches. Other studies reported similar findings regarding the low correlation between automatic and manual evaluations<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup>. The third possible use case &#x02013; though still underperformed by previous fine-tuned SOTA approaches &#x02013; document-level classification, which involves semantic understanding. As shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, GPT-4 achieved over a 0.7 F1-score with dynamic K-nearest shot for both multi-label document-level classification benchmarks.</p><p id="Par34">In addition to closed-source LLMs, open-source LLMs such as LLaMA 2 do not demonstrate strong zero- and few-shot capabilities. While there are other open-source LLMs available, LLaMA 2 remains as a strong representative<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. Results in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> suggest that its overall zero-shot performance is 15% and 22% lower than that of GPT-3.5 and GPT-4, respectively, and up to 60% lower in specific BioNLP tasks. Not only does it exhibit suboptimal performance, but the results in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> also demonstrate that its zero-shot responses frequently contain inconsistencies, missing elements, and hallucinations, accounting for up to 30% of the full testing set instances. Therefore, fine-tuning open-source LLMs for BioNLP tasks is still necessary to bridge the gap. Only through fine-tuning LLaMA 2, its overall performance is slightly higher than the one-shot GPT-4 (4%). However, it is worth noting that the model sizes of LLaMA 2 and PMC LLaMA are significantly smaller than those of GPT-3.5 and GPT-4, making it challenging to evaluate them on the same level. Additionally, open-source LLMs have the advantage of continued development and local deployment.</p><p id="Par35">Another primary finding on open-source LLMs is that the results do not indicate significant performance improvement from continuously biomedical pre-trained LLMs (PMC LLaMA 13B vs. LLaMA 2 13B). As mentioned, our study reproduced similar results reported in PMC LLaMA 13B; however, we also found that directly fine-tuning LLaMA 2 yielded better or at least similar performance&#x02014;and this is consistent across all 12 benchmarks. In the biomedical domain, representative foundation LLMs such as PMC LLaMA used 32 A100 GPUs<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, and Meditron used 128 A100 GPUs to continuously pretrain from LLaMA or LLaMA 2<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. Our evaluation did not find significant performance improvement for PMC LLaMA; the Meditron study also only reported ~3% improvement itself and only evaluated on question answering datasets. At a minimum, the results suggest the need for a more effective and sustainable approach to developing biomedical domain-specific LLMs.</p><p id="Par36">The automatic metrics for text summarization and simplification tasks may not align with manual evaluations. As the quantitative results on text summarization and generation demonstrated, commonly used automatic evaluations such as Rouge, BERT, and BART scores consistently favored the fine-tuned BART&#x02019;s generated text, while manual evaluations show different results, indicating that GPT-3.5 and GPT-4 had competitive accuracy and much higher readability even under the zero-shot setting. Existing studies also reported that the automatic measures on LLM-generated text may not correlate to human preference<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup>. The MS^2 benchmark used in the study also discussed the limitation of automatic measures, specifically for text summarization<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. Additionally, the results highlight that completeness is a primary limitation when adapting GPT models to biomedical text generation tasks despite its competitive accuracy and readability scores.</p><p id="Par37">Last, our evaluation on both performance and cost demonstrates a clear trade-off when using LLMs in practice. GPT-4 had the overall best performance in the 12 benchmarks, with an 8% improvement over GPT-3.5 but also at a higher cost (60 to 100 times higher than GPT-3.5). Notably, GPT-4 showed significantly higher performance, particularly in question-answering tasks that involve reasoning, such as over 20% improvement in MedQA compared to GPT-3.5. This observation is consistent with findings from other studies<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR38">38</xref></sup>. Note that newer versions of GPT-4, such as GPT-4 Turbo, may further reduce the cost of using GPT-4.</p><p id="Par38">These findings lead to recommendations for downstream users to apply LLMs in BioNLP applications, summarized in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>. It provides suggestions on which BioNLP applications are recommended (or not) for LLMs, categorized by conditions (e.g., the zero/few-shot setting when computational resources are limited) and additional tips (e.g., when advanced prompt engineering is more effective).<fig id="Fig4"><label>Fig. 4</label><caption><title>Recommendations for using LLMs in BioNLP applications.</title><p>It presents specific task-based recommendations across different settings and offers general guidance on effectively applying LLMs in BioNLP.</p></caption><graphic xlink:href="41467_2025_56989_Fig4_HTML" id="d33e1880"/></fig></p><p id="Par39">We also recognize the following two open problems and encourage a community effort for better usage of LLMs in BioNLP applications.</p><p id="Par40">Adapting both data and evaluation paradigms is essential to maximize the benefits of LLMs in BioNLP applications. Arguably, the current datasets and evaluation settings in BioNLP are tailored to supervised (fine-tuning) methods and is not fair for LLMs. Those issues challenge the direct comparison between the fine-tuned biomedical domain-specific language models and zero/few shot of LLMs. The datasets for the tasks where LLMs excel are also limited in the biomedical domain. Further, the manual measures on biomedical text summarization also showed different results than that of all three automatic measures. These collectively suggest the current BioNLP evaluation frameworks have limitations when they are applied to LLMs<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup>. They may not be able to accurately assess the full benefits of LLMs in biomedical applications, calling for the development of new evaluation datasets and methods for LLMs in bioNLP tasks.</p><p id="Par41">Addressing inconsistencies, missingness, and hallucinations produced by LLMs is critical. The prevalence of inconsistencies, missingness, and hallucinations generated by LLMs is of concern, and we argue that they must be addressed for deployment. Our results demonstrate that providing just one shot could significantly reduce the occurrence of such issues, offering a simple solution. However, thorough examination in real-world scenario validations is still necessary. Additionally, more advanced approaches for validating LLMs&#x02019; responses are expected for further improvement of their reliability and usability<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>.</p><p id="Par42">This study also has several limitations that should be acknowledged. While this study examined strong LLM representatives from each category (closed-source, open-source, and biomedical domain-specific), it is important to note that there are other LLMs, such as BARD<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> and Mistral<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, that have demonstrated strong performance in the literature. Additionally, while we investigated zero-shot, one-shot, dynamic K-nearest few-shot, and fine-tuning techniques, each of them has variations, and there are also new approaches<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>. Given the rapidly growing nature of this area, our study cannot cover all of them. Instead, our aim is to establish baseline performance on the main BioNLP applications using commonly used LLMs and methods as representatives, and to make the datasets, methods, codes, and results publicly available. This enables downstream users to understand when and how to apply LLMs in their own use cases and to compare new LLMs and associated methods on the same benchmarks. In the future, we also plan to assess LLMs in real-world scenarios in the biomedical domain to further broaden the scope of the study.</p></sec><sec id="Sec10"><title>Methods</title><sec id="Sec11"><title>Evaluation tasks, datasets, and metrics</title><p id="Par43">Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref> presents a summary of the evaluation tasks, datasets, and metrics. We benchmarked the models on the full testing sets of the twelve datasets from six BioNLP applications, which are BC5CDR-chemical and NCBI-disease for Named Entity Recognition, ChemProt and DDI2013 for relation extraction, HoC and LitCovid for multi-label document classification, and MedQA and PubMedQA for question answering, PubMed Text Summarization and MS^2 for text summarization, and Cochrane PLS and PLOS Text Simplification for text simplification. These datasets have been widely used in benchmarking biomedical text mining challenges<sup><xref ref-type="bibr" rid="CR55">55</xref>&#x02013;<xref ref-type="bibr" rid="CR57">57</xref></sup> and evaluating biomedical language models<sup><xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>. The datasets are also available in the repository. We evaluated the datasets using the official evaluation metrics provided by the original dataset description papers, as well as commonly used metrics for method development or applications with the datasets, as documented in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>. Note that it is challenging to have a single one-size-fits-all metric, and some datasets and related studies used multiple evaluation metrics. Therefore, we also adopted secondary metrics for additional evaluations. A detailed description is below.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Evaluation datasets, dataset size, and evaluation metrics</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Training</th><th>Validation</th><th>Testing</th><th>Primary metrics</th><th>Secondary metrics</th></tr></thead><tbody><tr><td colspan="6">Named entity recognition</td></tr><tr><td>BC5CDR-chemical<sup><xref ref-type="bibr" rid="CR59">59</xref></sup></td><td>4560</td><td>4581</td><td>4797</td><td>Entity-level F1<sup><xref ref-type="bibr" rid="CR59">59</xref>,<xref ref-type="bibr" rid="CR89">89</xref></sup></td><td/></tr><tr><td>NCBI-disease<sup><xref ref-type="bibr" rid="CR60">60</xref></sup></td><td>5424</td><td>923</td><td>940</td><td>Entity-level F1<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR60">60</xref></sup></td><td/></tr><tr><td colspan="6">Relation extraction</td></tr><tr><td>ChemProt<sup><xref ref-type="bibr" rid="CR55">55</xref></sup></td><td>19,460</td><td>11,820</td><td>16,943</td><td>Macro F1<sup><xref ref-type="bibr" rid="CR90">90</xref></sup></td><td>Micro F1<sup><xref ref-type="bibr" rid="CR55">55</xref>,<xref ref-type="bibr" rid="CR90">90</xref></sup></td></tr><tr><td>DDI2013<sup><xref ref-type="bibr" rid="CR62">62</xref></sup></td><td>18,779</td><td>7244</td><td>5761</td><td>Macro F1<sup><xref ref-type="bibr" rid="CR62">62</xref>,<xref ref-type="bibr" rid="CR85">85</xref></sup></td><td>Micro F1<sup><xref ref-type="bibr" rid="CR16">16</xref></sup></td></tr><tr><td colspan="6">Multi-label document classification</td></tr><tr><td>HoC<sup><xref ref-type="bibr" rid="CR64">64</xref></sup></td><td>1108</td><td>157</td><td>315</td><td>Macro F1<sup><xref ref-type="bibr" rid="CR64">64</xref>,<xref ref-type="bibr" rid="CR86">86</xref></sup></td><td>Micro F1<sup><xref ref-type="bibr" rid="CR86">86</xref></sup></td></tr><tr><td>LitCovid<sup><xref ref-type="bibr" rid="CR56">56</xref></sup></td><td>24,960</td><td>6239</td><td>2500</td><td>Macro F1<sup><xref ref-type="bibr" rid="CR56">56</xref></sup></td><td>Micro F1<sup><xref ref-type="bibr" rid="CR56">56</xref></sup></td></tr><tr><td colspan="6">Question answering</td></tr><tr><td>MedQA 5-option<sup><xref ref-type="bibr" rid="CR66">66</xref></sup></td><td>10,178</td><td>1272</td><td>1273</td><td>Accuracy<sup><xref ref-type="bibr" rid="CR66">66</xref></sup></td><td>Macro F1<sup><xref ref-type="bibr" rid="CR91">91</xref></sup></td></tr><tr><td>PubMedQA<sup><xref ref-type="bibr" rid="CR67">67</xref></sup></td><td>190,142</td><td>21,127</td><td>500</td><td>Accuracy<sup><xref ref-type="bibr" rid="CR67">67</xref></sup></td><td>Macro F1<sup><xref ref-type="bibr" rid="CR91">91</xref></sup></td></tr><tr><td colspan="6">Text summarization</td></tr><tr><td>PubMed Text Summarization<sup>a</sup><sup><xref ref-type="bibr" rid="CR68">68</xref></sup></td><td>117,108</td><td>6631</td><td>6658</td><td>Rouge-L<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></td><td>BERT Score<sup><xref ref-type="bibr" rid="CR92">92</xref></sup>, BART Score<sup><xref ref-type="bibr" rid="CR93">93</xref></sup></td></tr><tr><td>MS^2<sup>b</sup><sup><xref ref-type="bibr" rid="CR50">50</xref></sup></td><td>14,188</td><td>2021</td><td>-</td><td>Rouge-L<sup><xref ref-type="bibr" rid="CR50">50</xref></sup></td><td>BERT Score<sup><xref ref-type="bibr" rid="CR94">94</xref></sup>, BART Score<sup><xref ref-type="bibr" rid="CR28">28</xref></sup></td></tr><tr><td colspan="6">Text simplification</td></tr><tr><td>Cochrane PLS<sup><xref ref-type="bibr" rid="CR69">69</xref></sup></td><td>3568</td><td>411</td><td>480</td><td>Rouge-L<sup><xref ref-type="bibr" rid="CR69">69</xref></sup></td><td>FKGL<sup><xref ref-type="bibr" rid="CR95">95</xref></sup>, DCRS<sup><xref ref-type="bibr" rid="CR96">96</xref></sup></td></tr><tr><td>PLOS Text Simplification<sup><xref ref-type="bibr" rid="CR70">70</xref></sup></td><td>26,124</td><td>1000</td><td>1000</td><td>Rouge-L<sup><xref ref-type="bibr" rid="CR70">70</xref></sup></td><td>FKGL<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, DCRS<sup><xref ref-type="bibr" rid="CR70">70</xref></sup></td></tr></tbody></table><table-wrap-foot><p>The related studies using the metrics are also provided. <sup>a</sup>We filtered the noisy instances with less than 50 words for the training and validation sets and kept the testing set untouched. <sup>b</sup>The gold standard of the testing set of MS^2 is not publicly available; we used the validation set instead.</p></table-wrap-foot></table-wrap></p><p id="Par44">Named entity recognition. Named entity recognition is a task that involves identifying entities of interest from free text. The biomedical entities can be described in various ways, and resolving the ambiguities is crucial<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. Named entity recognition is typically a sequence labeling task, where each token is classified into a specific entity type. BC5CDR-chemical<sup><xref ref-type="bibr" rid="CR59">59</xref></sup> and NCBI-disease<sup><xref ref-type="bibr" rid="CR60">60</xref></sup> are manually annotated named entity recognition datasets for chemicals and diseases mentioned in biomedical literature, respectively. The exact match (that is, the predicted tokens must have the same text spans as the gold standard) F1-score was used to quantify the model performance.</p><p id="Par45">Relation extraction. Relation extraction involves identifying the relationships between entities, which is important for drug repurposing and knowledge discovery<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. Relation extraction is typically a multi-class classification problem, where a sentence or passage is given with identified entities and the goal is to classify the relation type between them. ChemProt<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> and DDI2013<sup><xref ref-type="bibr" rid="CR62">62</xref></sup> are manually curated relation extraction datasets for protein-protein interactions and drug-drug interactions from biomedical literature, respectively. Macro and micro F1-scores were used to quantify the model performance.</p><p id="Par46">Multi-label document classification. Multi-label document classification identifies semantic categories at the document-level. The semantic categories are effective for grasping the main topics and searching for relevant literature in the biomedical domain<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. Unlike multi-class classification, which assigns only one label to an instance, multi-label classification can assign up to N labels to an instance. HoC<sup><xref ref-type="bibr" rid="CR64">64</xref></sup> and LitCovid<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> are manually annotated multi-label document classification datasets for hallmarks of cancer (10 labels) and COVID-19 topics (7 labels), respectively. Macro and Micro F1 scores were used as the primary and secondary evaluation metrics, respectively.</p><p id="Par47">Question answering. Question answering evaluates the knowledge and reasoning capabilities of a system in answering a given biomedical question with or without associated contexts<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. Biomedical QA datasets such as MedQA and PubMedQA have been widely used in the evaluation of language models<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. The MedQA dataset is collected from questions in the United States Medical License Examination (USMLE), where each instance contains a question (usually a patient description) and five answer choices (e.g., five potential diagnoses)<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>. The PubMedQA dataset includes biomedical research questions from PubMed, and the task is to use yes, no, or maybe to answer these questions with the corresponding abstracts<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>. Accuracy and macro F1-score are used as the primary and secondary evaluation metrics, respectively.</p><p id="Par48">Text summarization. Text summarization produces a concise and coherent summary of a longer documents or multiple documents while preserving its essential content. We used two primary biomedical text summarization datasets: the PubMed text summarization benchmark<sup><xref ref-type="bibr" rid="CR68">68</xref></sup> and MS^2<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. The PubMed text summarization benchmark focuses on single document summarization where the input is a full PubMed article, and the gold standard output is its abstract. M2^2 in contrast, focuses on multi-document summarization where the input is a collection of PubMed articles, and the gold standard output is the abstract of a systematic review study that cites those articles. Both benchmarks used the ROUGE-L score as the primary evaluation metric; BERT score and BART score were used as secondary evaluation metrics.</p><p id="Par49">Text simplification. Text simplification rephrases complex texts into simpler language while maintaining the original meaning, making the information more accessible to a broader audience. We used two primary biomedical text simplification datasets: Cochrane PLS<sup><xref ref-type="bibr" rid="CR69">69</xref></sup> and the PLOS text simplification benchmark<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>. Cochrane PLS consists of the medical documents from the Cochrane Database of Systematic Reviews and the corresponding plain-language summary (PLS) written by the authors. The PLOS text simplification benchmark consists of articles from PLOS journals and the corresponding technical summary and PLS written by the authors. The ROUGE-L score was used as the primary evaluation metric. Flesch-Kincaid Grade Level (FKGL) and Dale-Chall Readability Score (DCRS), two commonly used evaluation metrics on readability<sup><xref ref-type="bibr" rid="CR71">71</xref></sup> were used as the secondary evaluation metrics.</p></sec><sec id="Sec12"><title>Baselines</title><p id="Par50">For each dataset, we reported the reported SOTA fine-tuning result before the rise of LLMs as the baseline. The SOTA approaches involved fine-tuning (domain-specific) language models such as PubMedBERT<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, BioBERT<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, or BART<sup><xref ref-type="bibr" rid="CR72">72</xref></sup> as the backbone. The fine-tuning still requires scalable manually labeled instances, which is challenging in the biomedical domain<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. In contrast, LLMs may have the advantage when minimal manually labeled instances are available, and they do not require fine-tuning or retraining for every new task through zero/few-shot learning. Therefore, we used the existing SOTA results achieved by the fine-tuning approaches to quantify the benefits and challenges of LLMs in BioNLP applications.</p></sec><sec id="Sec13"><title>Large language models</title><p id="Par51">Representative LLMs and their versions. Both GPT-3.5 and GPT-4 have been regularly updated. For reproducibility, we used the snapshots gpt-3.5-turbo-16k-0613 and gpt-4-0613 for extractive tasks, and gpt-4-32k-0613 for generative tasks, considering their input and output token sizes. Regarding LLaMA 2, it is available in 7B, 13B, and 70B versions. We evaluated LLaMA 2 13B based on the computational resources required for fine-tuning, which is arguably the most common scenario applicable to BioNLP downstream applications. For PMC LLaMA, both 7B and 13B versions are available. Similarly, we used PMC LLaMA 13B, specifically evaluating it under the fine-tuning setting &#x02013; the same setting used in its original study<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. In the original study, PMC LLaMA was only evaluated on medical question answering tasks, combining multiple question answering datasets for fine-tuning. In our case, we fine-tuned each dataset separately and reported the results individually.</p><p id="Par52">Prompts. To date, prompt design remains an open research problem<sup><xref ref-type="bibr" rid="CR73">73</xref>&#x02013;<xref ref-type="bibr" rid="CR75">75</xref></sup>. We developed a prompt template that can be used across different tasks based on existing literature<sup><xref ref-type="bibr" rid="CR74">74</xref>&#x02013;<xref ref-type="bibr" rid="CR77">77</xref></sup>. An annotated prompt example is provided in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref> Prompt engineering, and we have made all the prompts publicly available in the repository. The prompt template contains (1) task descriptions (e.g., classifying relations), (2) input specifications (e.g., a sentence with labeled entities), (3) output specifications (e.g., the relation type), (4) task guidance (e.g., detailed descriptions or documentations on relation types), and (5) example demonstrations if examples from training sets are provided. This approach aligns with previous studies in the biomedical domain, which have demonstrated that incorporating task guidance into the prompt leads to improved performance<sup><xref ref-type="bibr" rid="CR74">74</xref>,<xref ref-type="bibr" rid="CR76">76</xref></sup> and was also employed and evaluated in our previous study, specifically focusing on named entity recognition<sup><xref ref-type="bibr" rid="CR77">77</xref></sup>. We also adapted the SOTA example selection approach in the biomedical domain described below<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>.</p><p id="Par53">Zero-shot and static few-shot. We comparatively evaluated the zero-shot, one-shot, and five-shot learning performances. Only a few studies have made the selected examples available. For reproducibility and benchmarking, we first randomly selected the required number of examples in training sets, used the same selected examples for few-shot learning, and made the selected examples publicly available.</p><p id="Par54">Dynamic K-nearest few-shot. In addition to zero- or static few-shot learning where fixed instructions are used for each instance, we further evaluated the LLMs under a dynamic few-shot learning setting. The dynamic few-shot learning is based on the MedPrompt approach, the SOTA method that demonstrated robust performance in medical question answering tasks without fine-tuning<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. The essence is to use K training instances that are most similar to the test instance as the selected examples. We denote this setting as dynamic K-nearest few-shot, as the prompts for different test instances differ. Specifically, for each dataset, we used the SOTA text embedding model text-embedding-ada-002<sup><xref ref-type="bibr" rid="CR54">54</xref></sup> to encode the instances and used cosine similarity as the metric for finding similar training instances to a testing instance. We tested dynamic K-nearest few-shot prompts with K equals to one, two, and five.</p><p id="Par55">Parameters for prompt engineering. For zero-, one-, and few-shot approaches, we used a temperature parameter of 0 to minimize variance for both GPT and LLaMA-based models. Additionally, for LLaMA models, we maintained other parameters unchanged, set the maximum number of generated tokens per task, and truncated the instances due to the input length limit for the five-shot setting. Further details are provided in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref> Prompt engineering, and the related codes are available in the repository.</p><p id="Par56">Fine-tuning. We further conducted instruction fine-tuning on LLaMA 2 13B and PMC-LLaMA 13B. For each dataset, we fine-tuned LLaMA 2 13B and PMC- LLaMA 13B using its training set. The goal of instruction fine-tuning is defined by the objective function: <inline-formula id="IEq1"><alternatives><tex-math id="d33e2483">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\arg }{\max }_{\theta }{\sum}_{\left({x}^{i},{y}^{i}\right)\epsilon (X,Y)}{logp}({y}^{i}|{x}^{i};\theta )$$\end{document}</tex-math><mml:math id="d33e2488"><mml:mi>arg</mml:mi><mml:msub><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02223;</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41467_2025_56989_Article_IEq1.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq2"><alternatives><tex-math id="d33e2543">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}^{i}$$\end{document}</tex-math><mml:math id="d33e2548"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2025_56989_Article_IEq2.gif"/></alternatives></inline-formula> represents the input instruction, <inline-formula id="IEq3"><alternatives><tex-math id="d33e2556">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}^{i}$$\end{document}</tex-math><mml:math id="d33e2561"><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41467_2025_56989_Article_IEq3.gif"/></alternatives></inline-formula> is the ground truth response, and <inline-formula id="IEq4"><alternatives><tex-math id="d33e2569">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta$$\end{document}</tex-math><mml:math id="d33e2574"><mml:mi>&#x003b8;</mml:mi></mml:math><inline-graphic xlink:href="41467_2025_56989_Article_IEq4.gif"/></alternatives></inline-formula> is the parameter set of the model. This function aims to maximize the likelihood of accurately predicting responses based on the given instructions. The fine-tuning is performed on eight H100 80G GPUs, over three epochs with a learning rate of 1e&#x02212;5, a weight decay of 1e&#x02212;5, a warmup ratio of 0.01, and Low-Rank Adaptation (LoRA) for parameter-effective tuning<sup><xref ref-type="bibr" rid="CR78">78</xref></sup>.</p><p id="Par57">Output parsing. For extractive and classification tasks, we extracted the targeted predictions (e.g., classification types or multiple-choice options) from the raw outputs of LLMs with a combination of manual and automatic processing. We manually reviewed the processed outputs. Manual review showed that LLMs provided answers in inconsistent formats in some cases. For example, when presenting multiple-choice option C, the raw output examples included variations such as: &#x0201c;Based on the information provided, the most likely &#x02026; is C. The thyroid gland is a common site for metastasis, and &#x02026;&#x0201d;, &#x0201c;Great! Let&#x02019;s go through the options. A. &#x02026; B. &#x02026;Therefore, the most likely diagnosis is C.&#x0201d;, and &#x0201c;I&#x02019;m happy to help! Based on the patient&#x02019;s symptoms and examination findings, &#x02026; Therefore, option A is incorrect. &#x02026;, so option D is incorrect. The correct answer is option C.&#x0201d; (adapted from real responses with unnecessary details omitted). In such cases, automatic processing might overlook the answer, potentially lowering LLM accuracy. Thus, we manually extracted outputs in these instances to ensure fair credit. Additionally, we qualitatively evaluated the prevalence of such cases (providing responses in inconsistent formats), which will be introduced below.</p></sec><sec id="Sec14"><title>Evaluations</title><p id="Par58">Quantitative evaluations. We summarized the evaluation metrics in Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref> under zero-shot, static few-shot, dynamic K-nearest few-shot, and fine-tuning settings. The metrics are applicable to the entire testing sets of 12 datasets. We further conducted bootstrapping using a subsample size of 30 and repeated 100 times at a 95% confidence interval to report performance variance and performed a two-tailed Wilcoxon rank-sum test using SciPy<sup><xref ref-type="bibr" rid="CR79">79</xref></sup>. Further details are provided in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S2</xref> Quantitative evaluation results (S2.1. Result reporting).</p><p id="Par59">Qualitative evaluations on inconsistency, missing information, and hallucinations. For the tasks where the gold standard is fixed, e.g., a classification type or multiple-choice option, we conducted qualitative evaluations on collectively hundreds of thousands of raw outputs of the LLMs (the raw outputs from three LLMs under zero- and one-shot conditions across three benchmarks) to categorize errors beyond inaccurate predictions. Specifically, we examined (1) inconsistent responses, where the responses are in different formats, (2) missingness, where the responses are missing, and (3) hallucinations, where LLMs fail to address the prompt and may contain repetitions and misinformation in the output<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. We evaluated and reported the results in selected datasets: ChemProt, HoC, and MedQA.</p><p id="Par60">Qualitative evaluations on accuracy, completeness, and readability. For the tasks with free-text gold standards, such as summaries, we conducted qualitative evaluations on the quality of generated text. Specifically, one senior resident and one junior resident evaluated four models: the fine-tuned BART model reported in the SOTA approach, GPT-3.5 zero-shot, GPT-4 zero-shot, and LLaMA 2 13B zero-shot on 50 random samples from the PubMed Text Summarization benchmark. Each annotator was provided with 600 annotations. To mitigate potential bias, the model outputs were all lowercased, their orders were randomly shuffled, and the annotators were unaware of the models being evaluated. They assessed three dimensions on a scale of 1&#x02014;5: (1) accuracy, does the generated text contain correct information from the original input, (2) completeness, does the generated text capture the key information from the original input, and (3) readability, is the generated text easy to read. The detailed evaluation guideline is provided in Supplementary Information&#x000a0;<xref rid="MOESM1" ref-type="media">S3</xref> Qualitative evaluation on the PubMed Text Summarization Benchmark.</p><sec id="Sec15"><title>Cost analysis</title><p id="Par61">We further conducted a cost analysis to quantify the trade-off between cost and accuracy when using GPT models. The cost of GPT models is determined by the number of input and output tokens. We tracked the tokens in the input prompts and output completions using the official model tokenizers provided by OpenAI (<ext-link ext-link-type="uri" xlink:href="https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken">https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken</ext-link>) and used the pricing table (<ext-link ext-link-type="uri" xlink:href="https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/">https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/</ext-link>) to compute the overall cost.</p></sec></sec><sec id="Sec16"><title>Reporting summary</title><p id="Par62">Further information on research design is available in the&#x000a0;<xref rid="MOESM2" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p></sec></sec><sec id="Sec17" sec-type="supplementary-material"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41467_2025_56989_MOESM1_ESM.docx"><caption><p>Supplementary Information</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="41467_2025_56989_MOESM2_ESM.pdf"><caption><p>Reporting Summary</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="41467_2025_56989_MOESM3_ESM.pdf"><caption><p>Description of Additional Supplementary Files</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM4"><media xlink:href="41467_2025_56989_MOESM4_ESM.zip"><caption><p>Supplementary Data 1</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM5"><media xlink:href="41467_2025_56989_MOESM5_ESM.pdf"><caption><p>Transparent Peer Review file</p></caption></media></supplementary-material>
</p></sec><sec id="Sec18" sec-type="supplementary-material"><title>Source data</title><p>
<supplementary-material content-type="local-data" id="MOESM6"><media xlink:href="41467_2025_56989_MOESM6_ESM.xlsx"><caption><p>Source Data</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Zhiyong Lu, Hua Xu.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s41467-025-56989-2.</p></sec><ack><title>Acknowledgements</title><p>This study is supported by the following National Institutes of Health grants: 1R01LM014604 (Q.C., R.A.A., and H.X), 4R00LM014024 (Q.C.), R01AG078154 (R.Z., and H.X), 1R01AG066749 (W.J.Z), W81XWH-22-1-0164 (W.J.Z), and the Intramural Research Program of the National Library of Medicine (Q.C., Q.J., P.L., Z.W., and Z.L).</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Q.C., Z.L., and H.X. designed the research. Q.C., Y.H., X.P., Q.X., Q.J., A.G., M.B.S., X.A., P.L., Z.W., V.K.K., K.P., J.H., H.H., F.L., and J.D. performed experiments and data analysis. Q.C., Z.L., and H.X. wrote and edited the manuscript. All authors contributed to the discussion and manuscript preparation.</p></notes><notes notes-type="peer-review"><title>Peer review</title><sec id="FPar1"><title>Peer review information</title><p id="Par63"><italic>Nature Communications</italic> thanks the anonymous reviewers for their contribution to the peer review of this work. A peer review file is available.</p></sec></notes><notes notes-type="funding-information"><title>Funding</title><p>Open access funding provided by the National Institutes of Health.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>All data supporting the findings of this study, including source data, are available in the article and Supplementary Information, and can be accessed publicly via 10.5281/zenodo.14025500<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Additional data or requests for data can also be obtained from the corresponding authors upon request.&#x000a0;<xref ref-type="sec" rid="Sec18">Source data</xref> are provided with this paper.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The codes are publicly available via 10.5281/zenodo.14025500<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>.</p></notes><notes id="FPar2" notes-type="COI-statement"><title>Competing interests</title><p id="Par64">Dr. Jingcheng Du and Dr. Hua Xu have research-related financial interests at Melax Technologies Inc. The remaining authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Sayers</surname><given-names>EW</given-names></name><etal/></person-group><article-title>Database resources of the National Center for Biotechnology Information in 2023</article-title><source>Nucleic Acids Res.</source><year>2023</year><volume>51</volume><fpage>D29</fpage><lpage>D38</lpage><pub-id pub-id-type="pmid">36370100</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Sayers, E. W. et al. Database resources of the National Center for Biotechnology Information in 2023. <italic>Nucleic Acids Res.</italic><bold>51</bold>, D29&#x02013;D38 (2023).<pub-id pub-id-type="pmid">36370100</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Q</given-names></name><etal/></person-group><article-title>LitCovid in 2022: an information resource for the COVID-19 literature</article-title><source>Nucleic Acids Res.</source><year>2023</year><volume>51</volume><fpage>D1512</fpage><lpage>D1518</lpage><pub-id pub-id-type="pmid">36350613</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Chen, Q. et al. LitCovid in 2022: an information resource for the COVID-19 literature. <italic>Nucleic Acids Res.</italic><bold>51</bold>, D1512&#x02013;D1518 (2023).<pub-id pub-id-type="pmid">36350613</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Leaman, R. et al. Comprehensively identifying long COVID articles with human-in-the-loop machine learning. <italic>Patterns</italic><bold>4</bold>, 100659 (2023).</mixed-citation></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Q</given-names></name><etal/></person-group><article-title>BioConceptVec: creating and evaluating literature-based biomedical concept embeddings on a large scale</article-title><source>PLoS Comput. Biol.</source><year>2020</year><volume>16</volume><fpage>e1007617</fpage><pub-id pub-id-type="pmid">32324731</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Chen, Q. et al. BioConceptVec: creating and evaluating literature-based biomedical concept embeddings on a large scale. <italic>PLoS Comput. Biol.</italic><bold>16</bold>, e1007617 (2020).<pub-id pub-id-type="pmid">32324731</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Blake</surname><given-names>C</given-names></name></person-group><article-title>Beyond genes, proteins, and abstracts: Identifying scientific claims from full-text biomedical articles</article-title><source>J. Biomed. Inform.</source><year>2010</year><volume>43</volume><fpage>173</fpage><lpage>189</lpage><pub-id pub-id-type="pmid">19900574</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Blake, C. Beyond genes, proteins, and abstracts: Identifying scientific claims from full-text biomedical articles. <italic>J. Biomed. Inform.</italic><bold>43</bold>, 173&#x02013;189 (2010).<pub-id pub-id-type="pmid">19900574</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Su</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Deep learning joint models for extracting entities and relations in biomedical: a survey and comparison</article-title><source>Brief. Bioinforma.</source><year>2022</year><volume>23</volume><fpage>bbac342</fpage></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Su, Y. et al. Deep learning joint models for extracting entities and relations in biomedical: a survey and comparison. <italic>Brief. Bioinforma.</italic><bold>23</bold>, bbac342 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Zhang, Y., Chen, Q., Yang, Z., Lin, H., &#x00026; Lu, Z. BioWordVec, improving biomedical word embeddings with subword information and MeSH. <italic>Sci. Data.</italic><bold>6</bold>, 1&#x02013;9 (2019).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Chen, Q., Peng, Y. &#x00026; Lu, Z. BioSentVec: creating sentence embeddings for biomedical texts.In <italic>2019 IEEE International Conference on Healthcare Informatics (ICHI)</italic> 1&#x02013;5 (IEEE, 2019).</mixed-citation></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J</given-names></name><etal/></person-group><article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title><source>Bioinformatics</source><year>2020</year><volume>36</volume><fpage>1234</fpage><lpage>1240</lpage><pub-id pub-id-type="pmid">31501885</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Lee, J. et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. <italic>Bioinformatics</italic><bold>36</bold>, 1234&#x02013;1240 (2020).<pub-id pub-id-type="pmid">31501885</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Peng, Y., Yan, S., &#x00026; Lu, Z. Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets. In <italic>Proc. 18th BioNLP Workshop and Shared Task</italic>, 58&#x02013;65 (Association for Computational Linguistics, Florence, Italy, 2019).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Fang, L., Chen, Q., Wei, C.-H., Lu, Z. &#x00026; Wang, K. Bioformer: an efficient transformer language model for biomedical text mining, arXiv preprint arXiv:2302.01588 (2023).</mixed-citation></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>R</given-names></name><etal/></person-group><article-title>BioGPT: generative pre-trained transformer for biomedical text generation and mining</article-title><source>Brief. Bioinforma.</source><year>2022</year><volume>23</volume><fpage>bbac409</fpage></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text generation and mining. <italic>Brief. Bioinforma.</italic><bold>23</bold>, bbac409 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Venigalla, A., Frankle, J., &#x00026; Carbin, M. Biomedlm: a domain-specific large language model for biomedical text, MosaicML<italic>.</italic> Accessed: Dec, 23 (2022).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Yuan, H. et al. BioBART: Pretraining and evaluation of a biomedical generative language model. In <italic>Proc. 21st Workshop on Biomedical Language Processing</italic>, 97&#x02013;109 (2022).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Phan, L.N. et al. Scifive: a text-to-text transformer model for biomedical literature, arXiv preprint arXiv:2106.03598 (2021).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Gu, Y. et al. Domain-specific language model pretraining for biomedical natural language processing. <italic>ACM Trans. Comput. Healthc. HEALTH</italic>, <bold>3</bold>, 1&#x02013;23 (2021).</mixed-citation></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Allot</surname><given-names>A</given-names></name><etal/></person-group><article-title>LitSense: making sense of biomedical literature at sentence level</article-title><source>Nucleic Acids Res.</source><year>2019</year><volume>47</volume><fpage>W594</fpage><lpage>W599</lpage><pub-id pub-id-type="pmid">31020319</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Allot, A. et al. LitSense: making sense of biomedical literature at sentence level. <italic>Nucleic Acids Res.</italic><bold>47</bold>, W594&#x02013;W599 (2019).<pub-id pub-id-type="pmid">31020319</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Zhao, W. X. et al. A survey of large language models, arXiv preprint arXiv:2303.18223 (2023).</mixed-citation></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>L</given-names></name><etal/></person-group><article-title>Training language models to follow instructions with human feedback</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>27730</fpage><lpage>27744</lpage></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Ouyang, L. et al. Training language models to follow instructions with human feedback. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>35</bold>, 27730&#x02013;27744 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Chen, X. et al. How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks, arXiv preprint arXiv:2303.00293 (2023).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">OpenAI, GPT-4 Technical Report, ArXiv, abs/2303.08774, (2023).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Touvron, H. et al. Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288 (2023).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Jiang, A. Q. et al. Mixtral of experts arXiv preprint arXiv:2401.04088, 2024.</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Lee, P, Goldberg, C. &#x00026; Kohane, I. The AI revolution in medicine: GPT-4 and beyond (Pearson, 2023).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Wong, C. et al. Scaling clinical trial matching using large language models: A case study in oncology. In <italic>Machine Learning for Healthcare Conference</italic> 846&#x02013;862 (PMLR, 2023).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Liu, Q. et al. Exploring the Boundaries of GPT-4 in Radiology. In <italic>Proc. of the 2023 Conference on Empirical Methods in Natural Language Processing</italic> 14414&#x02013;14445 (2023).</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Nori, H. et al. Can generalist foundation models outcompete special-purpose tuning? Case study in medicine, arXiv preprint arXiv:2311.16452 (2023).</mixed-citation></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>S</given-names></name><etal/></person-group><article-title>Opportunities and challenges for ChatGPT and large language models in biomedicine and health</article-title><source>Brief. Bioinforma.</source><year>2024</year><volume>25</volume><fpage>bbad493</fpage></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Tian, S. et al. Opportunities and challenges for ChatGPT and large language models in biomedicine and health. <italic>Brief. Bioinforma.</italic><bold>25</bold>, bbad493 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">He, K. et al. A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics, arXiv preprint arXiv:2310.05694 (2023).</mixed-citation></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Omiye</surname><given-names>JA</given-names></name><name><surname>Gui</surname><given-names>H</given-names></name><name><surname>Rezaei</surname><given-names>SJ</given-names></name><name><surname>Zou</surname><given-names>J</given-names></name><name><surname>Daneshjou</surname><given-names>R</given-names></name></person-group><article-title>Large language models in medicine: the potentials and pitfalls: a narrative review</article-title><source>Ann. Intern. Med.</source><year>2024</year><volume>177</volume><fpage>210</fpage><lpage>220</lpage><pub-id pub-id-type="pmid">38285984</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">Omiye, J. A., Gui, H., Rezaei, S. J., Zou, J. &#x00026; Daneshjou, R. Large language models in medicine: the potentials and pitfalls: a narrative review. <italic>Ann. Intern. Med.</italic><bold>177</bold>, 210&#x02013;220 (2024).<pub-id pub-id-type="pmid">38285984</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>So&#x0011f;anc&#x00131;o&#x0011f;lu</surname><given-names>G</given-names></name><name><surname>&#x000d6;zt&#x000fc;rk</surname><given-names>H</given-names></name><name><surname>&#x000d6;zg&#x000fc;r</surname><given-names>A</given-names></name></person-group><article-title>BIOSSES: a semantic sentence similarity estimation system for the biomedical domain</article-title><source>Bioinformatics</source><year>2017</year><volume>33</volume><fpage>i49</fpage><lpage>i58</lpage><pub-id pub-id-type="pmid">28881973</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">So&#x0011f;anc&#x00131;o&#x0011f;lu, G., &#x000d6;zt&#x000fc;rk, H. &#x00026; &#x000d6;zg&#x000fc;r, A. BIOSSES: a semantic sentence similarity estimation system for the biomedical domain. <italic>Bioinformatics</italic><bold>33</bold>, i49&#x02013;i58 (2017).<pub-id pub-id-type="pmid">28881973</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Tinn, R. et al. Fine-tuning large neural language models for biomedical natural language processing. <italic>Patterns.</italic><bold>4</bold>, 100729 (2023).</mixed-citation></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Rankine</surname><given-names>A</given-names></name><name><surname>Peng</surname><given-names>Y</given-names></name><name><surname>Aghaarabi</surname><given-names>E</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name></person-group><article-title>Benchmarking effectiveness and efficiency of deep learning models for semantic textual similarity in the clinical domain: validation study</article-title><source>JMIR Med. Inform.</source><year>2021</year><volume>9</volume><fpage>e27386</fpage><pub-id pub-id-type="pmid">34967748</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">Chen, Q., Rankine, A., Peng, Y., Aghaarabi, E. &#x00026; Lu, Z. Benchmarking effectiveness and efficiency of deep learning models for semantic textual similarity in the clinical domain: validation study. <italic>JMIR Med. Inform.</italic><bold>9</bold>, e27386 (2021).<pub-id pub-id-type="pmid">34967748</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Wu, C. et al. PMC-LLaMA: toward building open-source language models for medicine, <italic>J. Am. Med. Inform. Associat</italic>. ocae045 (2024).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Fleming, S. L. et al. MedAlign: A clinician-generated dataset for instruction following with electronic medical records. In <italic>Proc. AAAI Conference on Artificial Intelligence</italic> Vol. 38 22021&#x02013;22030 (2023).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Zhang, Y. et al. Siren&#x02019;s song in the AI ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219 (2023).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Chen, Q. et al. A systematic evaluation of large language models for biomedical natural language processing: benchmarks, baselines, and recommendations. 10.5281/zenodo.14025500 (2024).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Nori, H., King, N., McKinney, S. M., Carignan, D. &#x00026; Horvitz, E. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375 (2023).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Labrak, Y., Rouvier, M. &#x00026; Dufour, R. A zero-shot and few-shot study of instruction-finetuned large language models applied to clinical and biomedical tasks. In <italic>Proc. 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</italic> 2049&#x02013;2066 (ELRA and ICCL, 2024).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Jin, H. et al. Llm maybe longlm: Self-extend llm context window without tuning. In <italic>Proc. of Machine Learning Research</italic>, <bold>235</bold> 22099&#x02013;22114 (2024).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Ding, Y. et al. LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens, arXiv preprint arXiv:2402.13753 (2024).</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Xie, Q., Huang, J., Saha, T. &#x00026; Ananiadou, S. Gretel: Graph contrastive topic enhanced language model for long document extractive summarization. In <italic>Proc. 29th International Conference on Computational Linguistics</italic>, 6259&#x02013;6269 (International Committee on Computational Linguistics, 2022).</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Jimenez Gutierrez, B. et al. Thinking about GPT-3 in-context learning for biomedical IE? Think again. In <italic>Findings of the Association for Computational Linguistics: EMNLP 2022</italic>, 4497&#x02013;4512 (Association for Computational Linguistics, 2022).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Rehana, H. et al. Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text, arXiv preprint arXiv:2303.17728 (2023).</mixed-citation></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Biomedical question answering: a survey of approaches and challenges</article-title><source>ACM Comput. Surv. (CSUR)</source><year>2022</year><volume>55</volume><fpage>1</fpage><lpage>36</lpage></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Jin, Q. et al. Biomedical question answering: a survey of approaches and challenges. <italic>ACM Comput. Surv. (CSUR)</italic><bold>55</bold>, 1&#x02013;36 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Singhal, K. et al. Large language models encode clinical knowledge, <italic>Nature</italic><bold>620</bold>, 1&#x02013;9 (2023).</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Chang, Y. et al. A survey on evaluation of large language models, <italic>ACM Trans. Intell. Syst. Technol</italic>. (2023).</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Minaee, S. et al. Large language models: A survey, arXiv preprint arXiv:2402.06196 (2024).</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Chen, Z. et al. Meditron-70b: Scaling medical pretraining for large language models, arXiv preprint arXiv:2311.16079 (2023).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">DeYoung, J., Beltagy, I., van Zuylen, M., Kuehl, B. &#x00026; Wang, L. L. Ms2: Multi-document summarization of medical studies. In <italic>Proc. 2021 Conference on Empirical Methods in Natural Language Processing</italic>, 7494&#x02013;7513 (2021).</mixed-citation></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Wornow</surname><given-names>M</given-names></name><etal/></person-group><article-title>The shaky foundations of large language models and foundation models for electronic health records</article-title><source>npj Digit. Med.</source><year>2023</year><volume>6</volume><fpage>135</fpage><pub-id pub-id-type="pmid">37516790</pub-id>
</element-citation><mixed-citation id="mc-CR51" publication-type="journal">Wornow, M. et al. The shaky foundations of large language models and foundation models for electronic health records. <italic>npj Digit. Med.</italic><bold>6</bold>, 135 (2023).<pub-id pub-id-type="pmid">37516790</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Manyika, J. An overview of Bard: an early experiment with generative AI. <ext-link ext-link-type="uri" xlink:href="https://ai.google/static/documents/google-about-bard.pdf">https://ai.google/static/documents/google-about-bard.pdf</ext-link> (2023).</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Jiang, A. Q. et al. Mistral 7B, arXiv preprint arXiv:2310.06825, (2023).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Neelakantan, A. et al. Text and code embeddings by contrastive pre-training, arXiv preprint arXiv:2201.10005 (2022).</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Krallinger, M. et al. Overview of the BioCreative VI chemical-protein interaction Track. In <italic>Proc. of the sixth BioCreative challenge evaluation workshop</italic> Vol. 1, 141&#x02013;146 (2017).</mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Chen, Q. et al. Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations, <italic>Database</italic><bold>2022</bold>, baac069 (2022).</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Islamaj Do&#x0011f;an, R. et al. Overview of the BioCreative VI Precision Medicine Track: mining protein interactions and mutations for precision medicine, <italic>Database</italic><bold>2019</bold>, bay147 (2019).</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">International Society for Biocuration, Biocuration: Distilling data into knowledge, <italic>Plos Biol.</italic>, <bold>16</bold>, e2002846 (2018).</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Li, J. et al. BioCreative V CDR task corpus: a resource for chemical disease relation extraction, <italic>Database</italic>, 2016 (2016).</mixed-citation></ref><ref id="CR60"><label>60.</label><citation-alternatives><element-citation id="ec-CR60" publication-type="journal"><person-group person-group-type="author"><name><surname>Do&#x0011f;an</surname><given-names>RI</given-names></name><name><surname>Leaman</surname><given-names>R</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name></person-group><article-title>NCBI disease corpus: a resource for disease name recognition and concept normalization</article-title><source>J. Biomed. Inform.</source><year>2014</year><volume>47</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmid">24393765</pub-id>
</element-citation><mixed-citation id="mc-CR60" publication-type="journal">Do&#x0011f;an, R. I., Leaman, R. &#x00026; Lu, Z. NCBI disease corpus: a resource for disease name recognition and concept normalization. <italic>J. Biomed. Inform.</italic><bold>47</bold>, 1&#x02013;10 (2014).<pub-id pub-id-type="pmid">24393765</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR61"><label>61.</label><citation-alternatives><element-citation id="ec-CR61" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Rousseau</surname><given-names>JF</given-names></name><name><surname>Ding</surname><given-names>Y</given-names></name><name><surname>Song</surname><given-names>M</given-names></name><name><surname>Lu</surname><given-names>W</given-names></name></person-group><article-title>Understanding drug repurposing from the perspective of biomedical entities and their evolution: Bibliographic research using aspirin</article-title><source>JMIR Med. Inform.</source><year>2020</year><volume>8</volume><fpage>e16739</fpage><pub-id pub-id-type="pmid">32543442</pub-id>
</element-citation><mixed-citation id="mc-CR61" publication-type="journal">Li, X., Rousseau, J. F., Ding, Y., Song, M. &#x00026; Lu, W. Understanding drug repurposing from the perspective of biomedical entities and their evolution: Bibliographic research using aspirin. <italic>JMIR Med. Inform.</italic><bold>8</bold>, e16739 (2020).<pub-id pub-id-type="pmid">32543442</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="other">Segura-Bedmar, I., Mart&#x000ed;nez, P. &#x00026; Herrero-Zazo, M. Semeval-2013 task 9: extraction of drug-drug interactions from biomedical texts (ddiextraction 2013). In <italic>Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 2: Proc. Seventh International Workshop on Semantic Evaluation (SemEval 2013)</italic> 341&#x02013;350 (Association for Computational Linguistics, 2013).</mixed-citation></ref><ref id="CR63"><label>63.</label><citation-alternatives><element-citation id="ec-CR63" publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>J</given-names></name><etal/></person-group><article-title>ML-Net: multi-label classification of biomedical texts with deep neural networks</article-title><source>J. Am. Med. Inform. Assoc.</source><year>2019</year><volume>26</volume><fpage>1279</fpage><lpage>1285</lpage><pub-id pub-id-type="pmid">31233120</pub-id>
</element-citation><mixed-citation id="mc-CR63" publication-type="journal">Du, J. et al. ML-Net: multi-label classification of biomedical texts with deep neural networks. <italic>J. Am. Med. Inform. Assoc.</italic><bold>26</bold>, 1279&#x02013;1285 (2019).<pub-id pub-id-type="pmid">31233120</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR64"><label>64.</label><citation-alternatives><element-citation id="ec-CR64" publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>S</given-names></name><etal/></person-group><article-title>Automatic semantic classification of scientific literature according to the hallmarks of cancer</article-title><source>Bioinformatics</source><year>2016</year><volume>32</volume><fpage>432</fpage><lpage>440</lpage><pub-id pub-id-type="pmid">26454282</pub-id>
</element-citation><mixed-citation id="mc-CR64" publication-type="journal">Baker, S. et al. Automatic semantic classification of scientific literature according to the hallmarks of cancer. <italic>Bioinformatics</italic><bold>32</bold>, 432&#x02013;440 (2016).<pub-id pub-id-type="pmid">26454282</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="other">Kaddari, Z., Mellah, Y., Berrich, J., Bouchentouf, T. &#x00026; Belkasmi, M. G. Biomedical question answering: A survey of methods and datasets. In <italic>2020 Fourth International Conference On Intelligent Computing in Data Sciences (ICDS)</italic> 1&#x02013;8 (IEEE, 2020).</mixed-citation></ref><ref id="CR66"><label>66.</label><citation-alternatives><element-citation id="ec-CR66" publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>D</given-names></name><etal/></person-group><article-title>What disease does this patient have? A large-scale open domain question answering dataset from medical exams</article-title><source>Appl. Sci.</source><year>2021</year><volume>11</volume><fpage>6421</fpage></element-citation><mixed-citation id="mc-CR66" publication-type="journal">Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. <italic>Appl. Sci.</italic><bold>11</bold>, 6421 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">Jin, Q., Dhingra, B., Liu, Z., Cohen, W. &#x00026; Lu, X. Pubmedqa: A dataset for biomedical research question answering. In <italic>Proc. 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</italic>, 2567&#x02013;2577 (EMNLP-IJCNLP, 2019).</mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">Cohan, A. et al. A discourse-aware attention model for abstractive summarization of long documents. <italic>In Proc. 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Languag Technologies</italic> Vol. 2, 615&#x02013;621 (2018).</mixed-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Devaraj, A., Wallace, B. C., Marshall, I. J. &#x00026; Li, J. J. Paragraph-level simplification of medical texts. In <italic>Proc. 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</italic> 4972&#x02013;4984 (Association for Computational Linguistics, 2021).</mixed-citation></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="other">Luo, Z., Xie, Q., &#x00026; Ananiadou, S. Readability controllable biomedical document summarization. In <italic>Findings of the Association for Computational Linguistics: EMNLP</italic>, 4667&#x02013;4680 (2022).</mixed-citation></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Goldsack, T. et al. Overview of the biolaysumm 2024 shared task on lay summarization of biomedical research articles. In <italic>Proc. 23rd Workshop on Biomedical Natural Language Processing</italic> 122&#x02013;131 (2024).</mixed-citation></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="other">Lewis, M. et al. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In <italic>Proc. 58th Annual Meeting of the Association for Computational Linguistics</italic>, 7871&#x02013;7880 (2020).</mixed-citation></ref><ref id="CR73"><label>73.</label><citation-alternatives><element-citation id="ec-CR73" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>P</given-names></name><etal/></person-group><article-title>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</article-title><source>ACM Comput. Surv.</source><year>2023</year><volume>55</volume><fpage>1</fpage><lpage>35</lpage></element-citation><mixed-citation id="mc-CR73" publication-type="journal">Liu, P. et al. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. <italic>ACM Comput. Surv.</italic><bold>55</bold>, 1&#x02013;35 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR74"><label>74.</label><mixed-citation publication-type="other">Hu, Y. et al. Improving large language models for clinical named entity recognition via prompt engineering, <italic>J. Am. Med. Inform. Assoc.</italic><bold>31</bold>, ocad259 (2024).</mixed-citation></ref><ref id="CR75"><label>75.</label><citation-alternatives><element-citation id="ec-CR75" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><etal/></person-group><article-title>Investigating the impact of prompt engineering on the performance of large language models for standardizing obstetric diagnosis text: comparative study</article-title><source>JMIR Format Res.</source><year>2024</year><volume>8</volume><fpage>e53216</fpage></element-citation><mixed-citation id="mc-CR75" publication-type="journal">Wang, L. et al. Investigating the impact of prompt engineering on the performance of large language models for standardizing obstetric diagnosis text: comparative study. <italic>JMIR Format Res.</italic><bold>8</bold>, e53216 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR76"><label>76.</label><mixed-citation publication-type="other">Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. &#x00026; Sontag, D. Large language models are few-shot clinical information extractors. In <italic>Proc. 2022 Conference on Empirical Methods in Natural Language Processing</italic>, 1998&#x02013;2022 (2022).</mixed-citation></ref><ref id="CR77"><label>77.</label><citation-alternatives><element-citation id="ec-CR77" publication-type="journal"><person-group person-group-type="author"><name><surname>Keloth</surname><given-names>VK</given-names></name><etal/></person-group><article-title>Advancing entity recognition in biomedicine via instruction tuning of large language models</article-title><source>Bioinformatics</source><year>2024</year><volume>40</volume><fpage>btae163</fpage><pub-id pub-id-type="pmid">38514400</pub-id>
</element-citation><mixed-citation id="mc-CR77" publication-type="journal">Keloth, V. K. et al. Advancing entity recognition in biomedicine via instruction tuning of large language models. <italic>Bioinformatics</italic><bold>40</bold>, btae163 (2024).<pub-id pub-id-type="pmid">38514400</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR78"><label>78.</label><mixed-citation publication-type="other">Hu, E. J. et al. Lora: Low-rank adaptation of large language models, arXiv preprint arXiv:2106.09685 (2021).</mixed-citation></ref><ref id="CR79"><label>79.</label><citation-alternatives><element-citation id="ec-CR79" publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><etal/></person-group><article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nat. Methods</source><year>2020</year><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="pmid">32015543</pub-id>
</element-citation><mixed-citation id="mc-CR79" publication-type="journal">Virtanen, P. et al. SciPy 1.0: fundamental algorithms for scientific computing in Python. <italic>Nat. Methods</italic><bold>17</bold>, 261&#x02013;272 (2020).<pub-id pub-id-type="pmid">32015543</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR80"><label>80.</label><mixed-citation publication-type="other">Lehman, E. et al. Do we still need clinical language models? <italic>In Conference on health, inference, and learning</italic>, 578&#x02013;597 (PMLR, 2023).</mixed-citation></ref><ref id="CR81"><label>81.</label><mixed-citation publication-type="other">Chen, S. et al. Evaluating the ChatGPT family of models for biomedical reasoning and classification. <italic>J. Am. Med. Inform. Assoc.</italic><bold>31</bold>, ocad256 (2024).</mixed-citation></ref><ref id="CR82"><label>82.</label><mixed-citation publication-type="other">Chen, Q. et al. A comprehensive benchmark study on biomedical text generation and mining with ChatGPT, <italic>bioRxiv</italic>, pp. 2023.04. 19.537463 (2023).</mixed-citation></ref><ref id="CR83"><label>83.</label><mixed-citation publication-type="other">Zhang, S., Cheng, H., Gao, J. &#x00026; Poon H. Optimizing bi-encoder for named entity recognition via contrastive learning. In <italic>Proc. 11th International Conference on Learning Representations</italic>, (ICLR, 2023).</mixed-citation></ref><ref id="CR84"><label>84.</label><mixed-citation publication-type="other">He, J. et al. Chemical-protein relation extraction with pre-trained prompt tuning. <italic>Proc IEEE Int. Conf. Healthc. Inform</italic>. <bold>2022</bold>, 608&#x02013;609 (2022).</mixed-citation></ref><ref id="CR85"><label>85.</label><mixed-citation publication-type="other">Mingliang, D., Jijun, T. &#x00026; Fei, G. Document-level DDI relation extraction with document-entity embedding. pp. 392&#x02013;397.</mixed-citation></ref><ref id="CR86"><label>86.</label><mixed-citation publication-type="other">Chen, Q., Du, J., Allot, A. &#x00026; Lu, Z. LitMC-BERT: transformer-based multi-label classification of biomedical literature with an application on COVID-19 literature curation, <italic>IEEE/ACM Trans. Comput. Biol. Bioinform</italic>. <bold>19</bold>, 2584&#x02013;2595 (2022).</mixed-citation></ref><ref id="CR87"><label>87.</label><citation-alternatives><element-citation id="ec-CR87" publication-type="journal"><person-group person-group-type="author"><name><surname>Yasunaga</surname><given-names>M</given-names></name><etal/></person-group><article-title>Deep bidirectional language-knowledge graph pretraining</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2022</year><volume>35</volume><fpage>37309</fpage><lpage>37323</lpage></element-citation><mixed-citation id="mc-CR87" publication-type="journal">Yasunaga, M. et al. Deep bidirectional language-knowledge graph pretraining. <italic>Adv. Neural Inf. Process. Syst.</italic><bold>35</bold>, 37309&#x02013;37323 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR88"><label>88.</label><mixed-citation publication-type="other">Flores, L. J. Y., Huang, H., Shi, K., Chheang, S. &#x00026; Cohan, A. Medical text simplification: optimizing for readability with unlikelihood training and reranked beam search decoding. In <italic>Findings of the Association for Computational Linguistics: EMNLP</italic>, 4859&#x02013;4873 (2023).</mixed-citation></ref><ref id="CR89"><label>89.</label><citation-alternatives><element-citation id="ec-CR89" publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>C-H</given-names></name><etal/></person-group><article-title>Assessing the state of the art in biomedical relation extraction: overview of the BioCreative V chemical-disease relation (CDR) task</article-title><source>Database</source><year>2016</year><volume>2016</volume><fpage>baw032</fpage><pub-id pub-id-type="pmid">26994911</pub-id>
</element-citation><mixed-citation id="mc-CR89" publication-type="journal">Wei, C.-H. et al. Assessing the state of the art in biomedical relation extraction: overview of the BioCreative V chemical-disease relation (CDR) task. <italic>Database</italic><bold>2016</bold>, baw032 (2016).<pub-id pub-id-type="pmid">26994911</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR90"><label>90.</label><mixed-citation publication-type="other">He, J. et al. Prompt tuning in biomedical relation extraction, <italic>J. Healthcare Inform. Res.</italic><bold>8</bold>, 1&#x02013;19 (2024).</mixed-citation></ref><ref id="CR91"><label>91.</label><mixed-citation publication-type="other">Guo, Z., Wang, P., Wang, Y. &#x00026; Yu, S. Improving small language models on PubMedQA via Generative Data Augmentation, <italic>arXiv</italic>, <bold>12</bold> (2023).</mixed-citation></ref><ref id="CR92"><label>92.</label><citation-alternatives><element-citation id="ec-CR92" publication-type="journal"><person-group person-group-type="author"><name><surname>Koh</surname><given-names>HY</given-names></name><name><surname>Ju</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Pan</surname><given-names>S</given-names></name></person-group><article-title>An empirical survey on long document summarization: Datasets, models, and metrics</article-title><source>ACM Comput. Surv.</source><year>2022</year><volume>55</volume><fpage>1</fpage><lpage>35</lpage></element-citation><mixed-citation id="mc-CR92" publication-type="journal">Koh, H. Y., Ju, J., Liu, M. &#x00026; Pan, S. An empirical survey on long document summarization: Datasets, models, and metrics. <italic>ACM Comput. Surv.</italic><bold>55</bold>, 1&#x02013;35 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR93"><label>93.</label><mixed-citation publication-type="other">Bishop, J. A., Xie, Q. &#x00026; Ananiadou, S. LongDocFACTScore: Evaluating the factuality of long document abstractive summarisation. In <italic>Proc. of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</italic> 10777&#x02013;10789 (2024).</mixed-citation></ref><ref id="CR94"><label>94.</label><mixed-citation publication-type="other">Wang, L. L, DeYoung, J. &#x00026; Wallace, B. Overview of MSLR2022: A shared task on multidocument summarization for literature reviews. In <italic>Proc. Third Workshop on Scholarly Document Processing</italic> 175&#x02013;180 (Association for Computational Linguistics, 2022).</mixed-citation></ref><ref id="CR95"><label>95.</label><citation-alternatives><element-citation id="ec-CR95" publication-type="journal"><person-group person-group-type="author"><name><surname>Ondov</surname><given-names>B</given-names></name><name><surname>Attal</surname><given-names>K</given-names></name><name><surname>Demner-Fushman</surname><given-names>D</given-names></name></person-group><article-title>A survey of automated methods for biomedical text simplification</article-title><source>J. Am. Med. Inform. Assoc.</source><year>2022</year><volume>29</volume><fpage>1976</fpage><lpage>1988</lpage><pub-id pub-id-type="pmid">36083212</pub-id>
</element-citation><mixed-citation id="mc-CR95" publication-type="journal">Ondov, B., Attal, K. &#x00026; Demner-Fushman, D. A survey of automated methods for biomedical text simplification. <italic>J. Am. Med. Inform. Assoc.</italic><bold>29</bold>, 1976&#x02013;1988 (2022).<pub-id pub-id-type="pmid">36083212</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR96"><label>96.</label><citation-alternatives><element-citation id="ec-CR96" publication-type="journal"><person-group person-group-type="author"><name><surname>Stricker</surname><given-names>J</given-names></name><name><surname>Chasiotis</surname><given-names>A</given-names></name><name><surname>Kerwer</surname><given-names>M</given-names></name><name><surname>G&#x000fc;nther</surname><given-names>A</given-names></name></person-group><article-title>Scientific abstracts and plain language summaries in psychology: A comparison based on readability indices</article-title><source>PLoS One</source><year>2020</year><volume>15</volume><fpage>e0231160</fpage><pub-id pub-id-type="pmid">32240246</pub-id>
</element-citation><mixed-citation id="mc-CR96" publication-type="journal">Stricker, J., Chasiotis, A., Kerwer, M. &#x00026; G&#x000fc;nther, A. Scientific abstracts and plain language summaries in psychology: A comparison based on readability indices. <italic>PLoS One</italic><bold>15</bold>, e0231160 (2020).<pub-id pub-id-type="pmid">32240246</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>