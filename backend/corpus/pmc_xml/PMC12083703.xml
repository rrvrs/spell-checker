<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">ArXiv</journal-id><journal-id journal-id-type="iso-abbrev">ArXiv</journal-id><journal-id journal-id-type="publisher-id">arxiv</journal-id><journal-title-group><journal-title>ArXiv</journal-title></journal-title-group><issn pub-type="epub">2331-8422</issn><publisher><publisher-name>Cornell University</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40386570</article-id><article-id pub-id-type="pmc">PMC12083703</article-id><article-id pub-id-type="arxiv">arXiv:2505.05736v1</article-id><article-id pub-id-type="publisher-id">2505.05736</article-id><article-version-alternatives><article-version article-version-type="number">1</article-version><article-version article-version-type="status">preprint</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Da</given-names></name><xref rid="A1" ref-type="aff">1</xref><xref rid="FN1" ref-type="author-notes">!</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Zhanliang</given-names></name><xref rid="A1" ref-type="aff">1</xref><xref rid="A2" ref-type="aff">2</xref><xref rid="FN1" ref-type="author-notes">!</xref></contrib><contrib contrib-type="author"><name><surname>Nguyen</surname><given-names>Quan</given-names></name><xref rid="A1" ref-type="aff">1</xref><xref rid="A3" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Zhuoran</given-names></name><xref rid="A1" ref-type="aff">1</xref><xref rid="A4" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Kai</given-names></name><xref rid="A1" ref-type="aff">1</xref><xref rid="A5" ref-type="aff">5</xref><xref rid="CR1" ref-type="corresp">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Raymond G. Perelman Center for Cellular and Molecular Therapeutics, Children&#x02019;s Hospital of Philadelphia, Philadelphia, PA 19104, USA</aff><aff id="A2"><label>2</label>Applied Mathematics and Computational Science Graduate Program, University of Pennsylvania, Philadelphia, PA, 19104, USA</aff><aff id="A3"><label>3</label>Bioengineering Graduate Program, University of Pennsylvania, Philadelphia, PA 19104, USA</aff><aff id="A4"><label>4</label>Genomics and Computational Biology Graduate Program, University of Pennsylvania, Philadelphia, PA 19104, USA</aff><aff id="A5"><label>5</label>Department of Pathology and Laboratory Medicine, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA 19104, USA</aff><author-notes><corresp id="CR1"><label>*</label>: correspondence should be addressed to <email>wangk@chop.edu</email>.</corresp><fn fn-type="equal" id="FN1"><label>!</label><p id="P1">: equal contribution.</p></fn></author-notes><pub-date pub-type="epub"><day>9</day><month>5</month><year>2025</year></pub-date><elocation-id>arXiv:2505.05736v1</elocation-id><permissions><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</ext-link>, which allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. The license allows for commercial use.</license-p></license></permissions><self-uri content-type="pdf">nihpp-2505.05736v1.pdf</self-uri><abstract id="ABS1"><p id="P2">The scarcity of high-quality multimodal biomedical data limits the ability to effectively fine-tune pretrained Large Language Models (LLMs) for specialized biomedical tasks. To address this challenge, we introduce MINT (Multimodal Integrated kNowledge Transfer), a framework that aligns unimodal large decoder models with domain-specific decision patterns from high-quality multimodal biomedical data through preference optimization. While MINT supports different optimization techniques, we primarily implement it with the Odds Ratio Preference Optimization (ORPO) framework as its backbone. This strategy enables the aligned LLMs to perform predictive tasks using text-only or image-only inputs while retaining knowledge learnt from multimodal data. MINT leverages an upstream multimodal machine learning (MML) model trained on high-quality multimodal data to transfer domain-specific insights to downstream text-only or image-only LLMs. We demonstrate MINT&#x02019;s effectiveness through two key applications: (1) Rare genetic disease prediction from texts, where MINT uses a multimodal encoder model, trained on facial photos and clinical notes, to generate a preference dataset for aligning a lightweight decoder-based text-only LLM (Llama 3.2&#x02013;3B-Instruct). Despite relying on text input only, the MINT-derived model outperforms models trained with Supervised Fine-Tuning (SFT), Retrieval-Augmented Generation (RAG), or direct preference optimization (DPO), and even outperforms much larger foundation model (Llama 3.1-405B-Instruct). (2) Tissue type classification using cell nucleus images, where MINT uses a vision-language foundation model as the preference generator, containing knowledge learnt from both text and histopathological images to align downstream image-only models. The resulting MINT-derived model significantly improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type classification. In summary, MINT provides an effective strategy to align unimodal LLMs with high-quality multimodal expertise through preference optimization. Our study also highlights a hybrid strategy that grafts the strength of encoder models in classification tasks into large decoder models to enhance reasoning, improve predictive tasks and reduce hallucination in biomedical applications.</p></abstract><kwd-group><kwd>Large Language Models</kwd><kwd>Human Phenotype Ontology</kwd><kwd>Rare Genetic Disorders</kwd><kwd>Supervised Fine-Tuning</kwd><kwd>Retrieval Augmented Generation</kwd><kwd>Direct Preference Optimization</kwd><kwd>Odds Ratio Preference Optimization</kwd></kwd-group></article-meta></front><body><sec id="S1"><title>INTRODUCTION</title><p id="P3">Language Models (LLMs) have achieved remarkable advances in natural language understanding and reasoning, powered by the scaling properties of the Transformer architecture<sup><xref rid="R1" ref-type="bibr">1</xref>,<xref rid="R2" ref-type="bibr">2</xref></sup> and pretraining on large-scale, general-purpose text corpora<sup><xref rid="R3" ref-type="bibr">3</xref>&#x02013;<xref rid="R5" ref-type="bibr">5</xref></sup>. Compared to earlier natural language processing (NLP) models, such as those from the Bidirectional Encoder Representations from Transformers (BERT)<sup><xref rid="R6" ref-type="bibr">6</xref></sup>, LLMs exhibit superior linguistic capabilities, more coherent long-range dependencies, context handling<sup><xref rid="R6" ref-type="bibr">6</xref>&#x02013;<xref rid="R9" ref-type="bibr">9</xref></sup> and improved generalization across diverse tasks. Recent models such as Llama 3.1<sup>7</sup>/3.2<sup>8</sup>/3.3<sup>9</sup> and DeepSeek V3<sup><xref rid="R10" ref-type="bibr">10</xref></sup>/R1<sup><xref rid="R11" ref-type="bibr">11</xref></sup> extend these capabilities further with context windows of up to 128K tokens, enabling more accurate processing of lengthy, detailed, and often noisy biomedical documents. This progress has expanded their potential for a broad range of biomedical applications, including summarizing noisy clinical documents<sup><xref rid="R12" ref-type="bibr">12</xref>&#x02013;<xref rid="R15" ref-type="bibr">15</xref></sup>, managing patient scheduling<sup><xref rid="R16" ref-type="bibr">16</xref>&#x02013;<xref rid="R18" ref-type="bibr">18</xref></sup>, and addressing highly specialized tasks such as rare disease prediction and gene prioritization<sup><xref rid="R19" ref-type="bibr">19</xref>,<xref rid="R20" ref-type="bibr">20</xref></sup>. Despite these advances, adapting LLMs to domain-specific biomedical tasks remains challenging, especially when multimodal data are scarce, supervision is sparse, or the task requires specialized clinical reasoning.</p><p id="P4">Supervised fine-tuning (SFT)<sup><xref rid="R21" ref-type="bibr">21</xref>,<xref rid="R22" ref-type="bibr">22</xref></sup> is the most commonly employed approach for adapting LLMs to domain-specific tasks using task-specific datasets that are typically much smaller than those used during pretraining. SFT has proven effective for linguistically driven applications<sup><xref rid="R23" ref-type="bibr">23</xref>&#x02013;<xref rid="R25" ref-type="bibr">25</xref></sup>. For example, Yang et al. developed PhenoGPT<sup><xref rid="R26" ref-type="bibr">26</xref></sup> to derive phenotypes from long and complex clinical notes by fine-tuning LLMs on data annotated with Human Phenotype Ontology (HPO) terms, achieving improved performance over the foundation LLMs. While such applications demonstrate benefits of SFT in biomedical NLP tasks, the approach faces notable limitations when applied to more complex tasks that require structured prediction and complex logical reasoning, such as rare disease diagnosis or gene prioritization<sup><xref rid="R27" ref-type="bibr">27</xref>&#x02013;<xref rid="R29" ref-type="bibr">29</xref></sup>. These tasks often involve generating precise medical terminologies rather than selecting from a predefined set, which is especially challenging for auto-regressive LLMs that generates responses token by token<sup><xref rid="R2" ref-type="bibr">2</xref></sup>. The challenge is amplified when labeled data are scarce, as is often the case in biomedical domains. Moreover, indiscriminate fine-tuning on medical datasets can have unintended consequences, potentially weakening the model&#x02019;s inherent linguistic and logical reasoning abilities significantly<sup><xref rid="R30" ref-type="bibr">30</xref>&#x02013;<xref rid="R32" ref-type="bibr">32</xref></sup>. These limitations becomes even more pronounced in zero-shot or few-shot scenarios, where models must handle previously unseen disease classes or tissue types, which are increasingly encountered in real-world clinical practice<sup><xref rid="R33" ref-type="bibr">33</xref></sup>.</p><p id="P5">To tackle these challenges, we propose Multimodal Integrated Knowledge Transfer (MINT), a framework that facilitates aligning LLMs with domain-specific patterns from multimodal biomedical databases (e.g., images, audios, videos, clinical texts) while preserving their original linguistic and reasoning capabilities. MINT can be implemented with different preference optimization techniques including Direct Preference Optimization (DPO)<sup><xref rid="R53" ref-type="bibr">53</xref></sup> and Odds Ratio Preference Optimization (ORPO)<sup><xref rid="R34" ref-type="bibr">34</xref></sup>. In this paper, we primarily use MINT with ORPO as our default backbone, which we refer to simply as &#x0201c;MINT&#x0201d; hereafter for brevity. The MINT framework comprises two key components: upstream preference learning dataset construction and downstream LLM model training. In the upstream pipeline, a multimodal ML model is trained to generate a preference learning dataset, including a list of preferred responses and a list of unfavored responses. In the downstream pipeline, these preference-labeled inputs are used for aligning LLM using ORPO to prioritize preferred responses over unfavored ones while preserving its general linguistic and reasoning capabilities.</p><p id="P6">To showcase its power, we demonstrate that the MINT framework significantly enhances LLM performance in two important biomedical tasks: phenotype-driven rare disease prediction and image-based tissue type classification. For the first task, we utilize GestaltMML<sup><xref rid="R35" ref-type="bibr">35</xref></sup>, a multimodal Transformer model trained on facial images, clinical texts, and demographic information, to construct a preference learning dataset. Our results show that the MINT-aligned model outperforms widely used methods such as SFT and Retrieval-Augmented Generation (RAG)<sup><xref rid="R36" ref-type="bibr">36</xref>&#x02013;<xref rid="R38" ref-type="bibr">38</xref></sup> in the task of performing text -only rare disease prediction, and even demonstrates strong performance on previously unseen disease classes in zero-shot settings. The second application of MINT focuses on tissue type classification from cell nucleus images using the PanNuke Database<sup><xref rid="R39" ref-type="bibr">39</xref>,<xref rid="R40" ref-type="bibr">40</xref></sup> and Pathology Language-Image Pretraining (PLIP)<sup><xref rid="R41" ref-type="bibr">41</xref></sup> model as the upstream multimodal preference dataset generator. By aligning Llama 3.2-Vision-11B-Instruct with preferences generated from PLIP, MINT outperforms other widely used techniques for image-based tissue type classification, such as Supervised Fine-Tuning (SFT). Together, we believe that the MINT framework can be readily extended to other biomedical applications, enabling the efficient transfer of knowledge from domain-specific multimodal datasets to LLMs or other large decoder models. This approach serves as a similar but alternative technique to RAG, which combines the strength of encoder in classification tasks with large decoder models.</p></sec><sec id="S2"><title>RESULTS</title><sec id="S3"><title>Overview</title><p id="P7">In MINT, each task utilizes an upstream multimodal machine learning (MML) model to generate a preference learning dataset, which consists of most-likely and least-likely labels for each sample. The downstream large language model (LLM) then serves as the target for multimodal knowledge transfer (<xref rid="F1" ref-type="fig">Figure 1</xref>). Throughout the <xref rid="S2" ref-type="sec">Results</xref> section, we refer to our default implementation (MINT with ORPO) simply as &#x02018;MINT&#x02019; for brevity, while the alternative implementation with DPO is referred to as &#x0201c;DPO&#x0201d;.</p><p id="P8">To demonstrate the effectiveness of the MINT framework, we evaluate its performance on two essential tasks: rare genetic disease prediction and tissue type classification. A summary of datasets, upstream multimodal models and downstream LLMs used in this study is given in <xref rid="T1" ref-type="table">Table 1</xref>. For rare disease prediction, the upstream model is GestaltMML, a Vision-and-Language Transformer-based multimodal ML model, while the downstream model is the text-only Llama-3.2&#x02013;3B-Instruct since our input will be clinical texts only. For tissue type classification, the upstream model is Pathology Language-Image Pretraining (PLIP), a vision-language foundation model designed for pathology AI, whereas the downstream model is Llama 3.2-Vision-11B-Instruct, whose input will be images with a trivial question such as &#x0201c;What is shown in Figure?&#x0201d;</p><p id="P9">Phenotype-driven disease prediction<sup><xref rid="R42" ref-type="bibr">42</xref>&#x02013;<xref rid="R45" ref-type="bibr">45</xref></sup> is a task that makes predictions of rare diseases based on patients&#x02019; clinical phenotypes, and such predictions can assist physicians in selecting diagnostic modalities or to facilitate clinical labs in finding causal genes/mutations<sup><xref rid="R46" ref-type="bibr">46</xref>,<xref rid="R47" ref-type="bibr">47</xref></sup>. Such clinical phenotypes can include facial photos, clinical notes, lab tests, and other imaging modalities. Several machine learning models have been developed to utilize facial photos for disease prediction, such as DeepGestalt<sup><xref rid="R48" ref-type="bibr">48</xref></sup> and GestaltMatcher<sup><xref rid="R49" ref-type="bibr">49</xref></sup>, which are based on facial image analysis, as well as GestaltMML<sup><xref rid="R35" ref-type="bibr">35</xref></sup>, a multimodal model that integrates facial images, demographic information, and clinical phenotypes such as Human Phenotype Ontology (HPO) terms<sup><xref rid="R47" ref-type="bibr">47</xref></sup>.</p><p id="P10">GestaltMML is a Transformer-based multimodal machine learning (ML) model that combines the ViLT (Vision-and-Language Transformer<sup><xref rid="R50" ref-type="bibr">50</xref></sup>) with the GestaltMatcher Database (GMDB)<sup><xref rid="R49" ref-type="bibr">49</xref>,<xref rid="R51" ref-type="bibr">51</xref></sup>. Unlike earlier multimodal models<sup><xref rid="R50" ref-type="bibr">50</xref></sup> that use distinct image and text models with simple fusion, GestaltMML employs a unified Transformer encoder<sup><xref rid="R2" ref-type="bibr">2</xref></sup> to model interactions between facial images and structured clinical texts.</p><p id="P11">PanNuke is a cell nuclei instance segmentation and classification dataset with exhaustive nuclei labels across 19 different tissue types. The dataset consists of 481 visual fields sampled from more than 20K whole slide images at different magnifications, with a total of 205,343 labeled nuclei, each with an instance segmentation mask. PLIP, built upon the widely recognized Contrastive Language-Image Pre-training (CLIP) model<sup><xref rid="R52" ref-type="bibr">52</xref></sup>, is the first vision-and-language foundation model designed for pathology AI.</p><p id="P12">Our goal here is not nuclei segmentation but predicting the tissue type (such as &#x0201c;bile duct&#x0201d; and &#x0201c;colon&#x0201d;) from cell nucleus images. We show that MINT, when applied to Llama 3.2-Vision-11B-Instruct, outperforms other widely used techniques for image-based tissue type classification, such as Supervised Fine-Tuning (SFT).</p><p id="P13">We assess MINT&#x02019;s performance by comparing it against the foundation LLM and other approaches, including supervised fine-tuning (SFT), retrieval-augmented generation (RAG), and direct preference optimization (DPO) on four key evaluation metrics: Hallucination Free Accuracy (HFA), Top-N (N= 5 or 10) Accuracy, Top-1 Accuracy, and Coverage-Avoidance Ratio (CAR). Hallucination-Free Accuracy measures the rate at which a language model does not produce hallucinations, with 100% indicating no hallucinations at all. For &#x0201c;Hallucination&#x0201d;, we mean the model either does not follow the instruction at all or produces fabricated labels. The CAR measures the model&#x02019;s capability to distinguish a list of preferred responses over the least-likely responses. Note that RAG is implemented and evaluated for the text-based rare disease prediction task only in our study.</p></sec><sec id="S4"><title>MINT Enhances Rare Disease Prediction Accuracy by LLM</title><p id="P14">We first evaluate it on a rare disease prediction task using text-based clinical phenotypes. In this task, the downstream LLM is given a short clinical summary (a few hundred words) that includes demographic information such as the patient&#x02019;s age, gender, and ethnicity, along with observed phenotype features. For instance, a typical input might describe a 1-year-old female patient of Caucasian descent presenting with developmental delay, overgrowth, scoliosis and cardiovascular abnormalities. Based on this input, the model generates a ranked list of ten possible rare diseases that best match the provided features. The output is structured in a consistent format, with each entry containing the full disease name and, when applicable, a commonly used abbreviation. Examples of generated predictions include conditions such as &#x0201c;Phelan-McDermid syndrome (PHMDS)&#x0201d; and &#x0201c;Williams-Beuren syndrome (WBS).&#x0201d; This structured response helps clinicians quickly interpret model outputs and assess diagnostic relevance.</p><p id="P15">To benchmark prediction performance, we compared MINT against the base model without any domain knowledge enhancement, as well as several existing model enhancement techniques, including Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and SFT followed by Direct Preference Optimization (DPO), which we refer to as DPO hereafter for simplicity. <xref rid="F2" ref-type="fig">Figure 2</xref> presents the evaluation results of our best-performing configuration, using the Llama 3.2&#x02013;3B-Instruct enhanced with MINT under a balanced preference setting (AoR = 1.0), i.e., 10 accepted vs. 10 rejected responses. The AoR is defined as the ratio of accepted to rejected responses when building the preference-learning datasets (see the <xref rid="S8" ref-type="sec">Methods</xref> section for details). For this evaluation, we utilize a dataset of 6522 training samples and 386 testing samples compiled from the GestaltMatcher Database (GMDB). The choice of how many responses to accept and reject was determined through considerations of the total number of disease labels (~520). The figure summarizes diagnostic accuracy, robustness under different preference ratios, and general language understanding capability.</p><p id="P16">We compare various enhancement strategies applied to base Llama 3.2&#x02013;3B-Instruct model, using our four key metrics: Hallucination-Free Accuracy (HFA), Top-10 accuracy, Top-1 accuracy, and Coverage-Avoidance Rate (CAR) (<xref rid="F2" ref-type="fig">Figure 2a</xref>). Our proposed method, MINT, consistently achieves the best performance across all metrics. Notably, MINT raises Top-10 accuracy from 5.19% (base model) to 52.99%, outperforming other methods such as RAG (6.52%), SFT (37.53%) and DPO (38.49%). The Coverage-Avoidance Rate (CAR) metric further highlights MINT&#x02019;s strength. CAR is a harmonic mean between Top-k coverage and Bottom-q avoidance, assessing whether the model can prioritize likely diseases while avoiding implausible ones. On this metric, MINT achieves a CAR of 0.2877, significantly higher than both SFT (0.1665) and DPO (0.2808). This indicates that MINT not only improves recall of true conditions in the top ranks, but also minimizes the inclusion of clinically irrelevant diseases, thereby improving reliability for real-world diagnostic support.</p><p id="P17">We also examine the robustness of MINT under varying Acceptance-over-Rejection (AoR) ratios, which reflect the balance between accepted and rejected samples in the preference optimization phase (<xref rid="F2" ref-type="fig">Figure 2b</xref>). We observe that performance improves as the AoR approaches a balanced ratio (10 accepted vs. 10 rejected). This trend suggests that a well-balanced preference dataset provides clearer and more stable gradient signals for learning disease ranking, while overly imbalanced settings such as too many rejected candidates, may dilute the optimization signal and reduce learning efficiency. These results offer practical guidance for designing preference-based supervision strategies in downstream medical applications.</p><p id="P18">To address the possibility that MINT-enhanced models may lose the general language understanding capabilities, we evaluate it on the H6-benchmark, a comprehensive evaluation suite comprising six-widely-used datasets: MMLU<sup><xref rid="R54" ref-type="bibr">54</xref></sup>, TruthfulQA<sup><xref rid="R55" ref-type="bibr">55</xref></sup>, HellaSwag<sup><xref rid="R56" ref-type="bibr">56</xref></sup>, Winogrande, ARC<sup><xref rid="R57" ref-type="bibr">57</xref></sup> and GSM8k<sup><xref rid="R58" ref-type="bibr">58</xref></sup>. In total, the H6-benchmark contains approximately 113,000 diverse samples that test various aspects of language understanding and reasoning through multiple-choice questions. The results demonstrate that MINT does not compromise the base model&#x02019;s generalization ability. Performance is maintained or modestly changed across all benchmarks, suggesting that MINT&#x02019;s improvements in rare disease prediction are not at the expense of broad language or reasoning skills. This balance is especially important for models expected to operate in open-domain or multi-task settings.</p><p id="P19">To validate the generality of MINT across model sizes and architectures, we report full results in <xref rid="T2" ref-type="table">Table S2</xref>, which includes experiments on Llama 3.2&#x02013;1B, 3B, 8B, Gemma 2&#x02013;2B, 9B, and Llama 3.1&#x02013;8B,70B, 405B. Due to computational constraints, only inference is performed on the largest models (70B and 405B). Across all scales and families, MINT consistently improves both the Top-10 and Top-1 accuracy. For example, on Llama 3.2&#x02013;1B-Instruct, Top-10 accuracy increases from 4.24% to 34.28%, yet on Gemma-2&#x02013;9B, Top-10 accuracy improves from 12.37% to 33.80%. Similarly, we observed a large increase of CAR values under different MINT model sizes and architectures, outperforming all the corresponding baselines. These gains are consistently accompanied by near-perfect HFA values, often exceeding 99%, indicating that MINT substantially enhances accuracy while maintaining high factual consistency and avoiding hallucinated outputs. This is to be expected since the upstream model GestaltMML is an encoder model that puts strong constraints on the prediction to ~520 disease labels.</p><sec id="S5"><title>Additional Validation and Case Study for Rare Disease Prediction</title><p id="P20">Our results presented in the previous section is performed by splitting the original GMDB datasets into training and testing subsets. We recognize that cross-validation can be susceptible to overfitting, especially if the data used for validation is not representative of the true distribution of data the model will encounter in real-world scenarios. Indeed, the clinical phenotypes from the GMDB database often comprises of a few phenotype terms in the form of HPO, rather than natural texts that are written by human experts. To assess the robustness and generalizability of MINT in real-world clinical contexts, we conducted external validation using Phenopacket-derived clinical notes<sup><xref rid="R59" ref-type="bibr">59</xref></sup>. This dataset consists of 5980 high-quality synthesized clinical notes that simulate realistic clinical narratives by converting standardized phenotype profiles (&#x0201c;Phenopackets&#x0201d;) into natural language. This introduces the linguistic variability and contextual noise commonly observed in clinical practice. We divided this dataset into two subsets: one containing set of diseases that overlap with the GMDB training set (1638 samples with 72 overlapping diseases) and another that are non-overlapping (disjoint) from the training set (4342 samples with 456 disjoint diseases). This division allows us to assess both in-distribution performance and zero-shot generalization to unseen disease classes.</p><p id="P21">As shown in <xref rid="T2" ref-type="table">Table 2</xref>, MINT demonstrates strong performance across both subsets. For the overlapping disease subset, MINT substantially outperforms all baselines, achieving 66.91% Top-10 accuracy and 47.56% Top-1 accuracy, compared to the base model&#x02019;s 5.13% and 3.42%, respectively. The performance gap is particularly notable compared to other enhancement techniques: DPO (60.99% Top-10, 26.56% Top-1), SFT (46.40% Top-10, 14.65% Top-1), and RAG (30.11% Top-10, 26.45% Top-1).</p><p id="P22">For the more challenging disjoint disease subset, which evaluates zero-shot generalization to completely unseen disease classes, performance decreases across all methods as expected. MINT achieves 10.48% Top-10 accuracy and 7.00% Top-1 accuracy. Interestingly, in this zero-shot scenario, RAG shows competitive performance with 24.17% Top-10 accuracy and 13.80% Top-1 accuracy, highlighting the complementary strengths of retrieval-based approaches when dealing with previously unseen diseases. This suggests that combining MINT with retrieval mechanisms could yield further improvements for zero-shot prediction scenarios. All methods maintain near-perfect Hallucination-Free Accuracy (HFA) rates above 99%, indicating strong factual consistency across approaches.</p><p id="P23">These extensive validation experiments demonstrate that MINT improves rare disease prediction across diverse text sources when diseases overlap with the training distribution. For zero-shot scenarios with previously unseen diseases, retrieval augmented approaches offer complementary strengths, suggesting that combining preference optimization with retrieval mechanisms could be particularly effective for comprehensive rare disease prediction systems.</p></sec></sec><sec id="S6"><title>MINT Substantially Enhances Tissue Type Classification Accuracy by LLM</title><p id="P24">We further assess the capability of MINT on a vision-centric biomedical classification task: tissue type prediction using the PanNuke dataset. The objective is to identify the most likely tissue of origin given a histological image, selected from a fixed set of 19 tissue types. This task is cast into a vision-language interface by providing each model with two inputs: (i) an image patch extracted from a nucleus-level H&#x00026;E-stained slide, and (ii) a standardized prompt that asks, <italic toggle="yes">&#x0201c;Based on the morphological features observed in the provided nucleus image, which tissue types could this nucleus originate from?&#x0201d;</italic> The model is instructed to generate a ranked list of exactly five candidate tissue types. While the prompt follows the format of a typical Visual Question Answering (VQA) task, it is semantically constant across all samples and thus does not provide additional context. As a result, the task relies entirely on visual modality from decision-making, making it a faithful benchmark of visual reasoning capacity.</p><p id="P25">We summarize the results of applying MINT to the downstream Llama 3.2&#x02013;11B-Vision-Instruct model and three common enhancement baselines: SFT, DPO, and the base model (<xref rid="F3" ref-type="fig">Figure 3a</xref>). For this evaluation, we utilize 5436 training images and 2330 testing images from the PanNuke dataset. Full numerical results with standard deviations across three random seeds are provided in <bold>Table S3</bold>. The base vision-language model achieves moderate baseline performance, with a Top-5 accuracy of 32.21% and Top-1 accuracy of 16.96%, likely reflecting pre-existing visual understanding from its large-scale pretraining. Upon applying MINT, we observe a substantial performance gain where Top-5 accuracy rises to 57.58% and Top-1 accuracy improves to 28.41%, representing a nearly doubling of accuracy compared to the base model. In addition to accuracy gains, MINT achieves the highest Coverage-Avoidance Rate (CAR) of 0.5203, slightly surpassing the best baseline (DPO, 0.5196) and substantially exceeding the base model (0.2965). The CAR is computed using 5 accepted responses and 5 rejected responses due to the small label set (19 classes), reflects the model&#x02019;s ability to both surface relevant tissue types in the top predictions and avoid implausible categories in the bottom ranks.</p><p id="P26">Other techniques, including SFT, DPO, show moderate gains; For example, both SFT and DPO improve Top-5 accuracy to 41.16%, yet fall short of MINT across all key metrics. These results reinforce MINT&#x02019;s strength in enhancing multimodal models across both text-based and image-based biomedical tasks. Note that due to the small size of the label space, we do not explore varying Acceptance-over-Rejection (AoR) ratios in this task and instead fix the preference optimization setting to a balanced 5 accepted vs. 5 rejected labels.</p><p id="P27">To examine whether MINT and other fine-tuning strategies compromise the general capabilities of vision-language models (VLMs), we further evaluate their performance on SEED-Bench<sup><xref rid="R60" ref-type="bibr">60</xref></sup>, a large-scale benchmark consists of 19,000 multiple choice questions with high-quality human annotations SEED-Bench spans ten core evaluation dimensions, including instance interaction, instance identity, instance attributes, instance location, instance counting, scene understanding, spatial relation, text understanding, and visual reasoning. As shown in <xref rid="F3" ref-type="fig">Figure 3b</xref>, the performance of all fine-tuned models&#x02014;MINT, SFT, and DPO&#x02014;closely aligns with that of the base model across all axes. These findings indicate that none of the fine-tuning approaches, including MINT, degrade the general-purpose visual-language reasoning ability of the foundation model, while MINT simultaneously delivers substantial performance improvements on the domain-specific task of tissue type classification.</p></sec><sec id="S7"><title>Case Study: MINT Enhanced LLM Can Differentiate Between Colon and Bile Duct Tissue Images</title><p id="P28">We next present a detailed case study demonstrating MINT&#x02019;s effectiveness in distinguishing between Colon<sup><xref rid="R61" ref-type="bibr">61</xref></sup> and Bile Duct<sup><xref rid="R62" ref-type="bibr">62</xref>&#x02013;<xref rid="R64" ref-type="bibr">64</xref></sup> tissue images (<xref rid="F4" ref-type="fig">Figure 4</xref>), which represent a particularly challenging discrimination task due to their histological similarities. Both tissues exhibit comparable epithelia structures and are functionally related within the digestive system<sup><xref rid="R65" ref-type="bibr">65</xref>,<xref rid="R66" ref-type="bibr">66</xref></sup>. Colon epithelium consists of columnar cells with microvilli that enhance nutrient absorption and water reabsorption, while bile ducts are lined by cholangiocytes with similar microvilli that aid bile modification and transport<sup><xref rid="R67" ref-type="bibr">67</xref>,<xref rid="R68" ref-type="bibr">68</xref></sup>.</p><p id="P29">To illustrate MINT&#x02019;s superior discriminative capability, we select representative training and testing samples that highlight the visual similarity between these tissue types as shown in <xref rid="F4" ref-type="fig">Figure 4</xref>. The testing results demonstrate MINT&#x02019;s enhanced discriminative ability. For bile duct tissue samples (Top panel in <xref rid="F4" ref-type="fig">Figure 4</xref>), MINT consistently assigns rank 1 to the correct tissue type across all test samples, while pushing the visually similar colon tissue to substantially lower rank with average in 5.75. In comparison, SFT shows considerable confusion, assigning an average of 1.25 to ground truth bile duct but also assigning a higher rank of 2.00 to colon which is the confused class. This indicates that while SFT can identify the correct tissue, it fails to decisively differentiate it from the visually similar tissue. Similarly, for colon tissue samples (Bottom panel in <xref rid="F4" ref-type="fig">Figure 4</xref>), MINT assigns rank 1 to the correct tissue across all test samples while relegating bile duct to much lower positions with average rank 4.00. In contrast, SFT shows significant confusion with an average rank of 1.25 for ground truth class colon and 1.50 for confused class bile duct. This situation indicates that SFT frequently ranks the incorrect confused tissue even higher than the correct one for colon samples. The base model performs considerably worse than both fine-tuned models, showing minimal discrimination between these similar tissue types. For bile duct samples, the base model assigns average ranks of 4.75 to bile duct (correct) and 2.25 to colon (incorrect), frequently ranking the wrong tissue higher. For colon samples, it assigns average ranks of 4.25 to colon (correct) and 1.50 to bile duct (incorrect), consistently preferring the incorrect tissue.</p><p id="P30">This improved discriminative capability stems directly from MINT&#x02019;s preference learning approach. Unlike SFT, which learns morphological features solely from positive samples without explicitly accounting for confusing tissue classes. MINT, on the other hand, is explicitly trained to both recognize the correct tissue class and distinguish it from visually similar, potentially confusing tissue classes. For instance, when learning bile duct tissue, MINT is trained to recognize morphological features of colon tissue as a rejected class, and vice versa. This targeted contrastive learning enables MINT to better differentiate between closely related tissue types.</p><p id="P31">By explicitly incorporating both chosen (positive) and rejected (negative) tissue types during training, MINT learns subtle morphological features that differentiate between visually similar tissues. The odds ratio loss function used in MINT&#x02019;s training emphasizes the importance of maximizing the separation between correct and incorrect predictions, resulting in more decisive classification for challenging cases.</p><p id="P32">The case study demonstrates that MINT&#x02019;s approach is particularly valuable for histopathology applications, where subtle morphological differences between tissue types can have significant diagnostic implications. While SFT improves over the base model in identifying correct tissues, MINT substantially outperforms both in discriminating between visually similar tissue types.</p></sec></sec><sec id="S8"><title>METHODS</title><sec id="S9"><title>Evaluation Metric</title><p id="P33">In this paper, we assess the capabilities of large language models (LLMs) on rare genetic diseases prediction using text description of clinical phenotypes across three main outcomes: (HFA: Hallucination-Free Accuracy) whether the LLM can produce a meaningful response to a query, specifically a list of real disease names; (Top-N) if (HFA) is successful, whether the LLM&#x02019;s response of N choices includes the correct disease prediction (true disease name); (Top-1) if (HFA) is successful, whether the LLM can accurately select the correct disease prediction. These tasks follow a logical progression: outcomes (Top-N) and (Top-1) are only applicable if (HFA) is achieved, meaning if (HFA) fails, (Top-N) and (Top-1) will also fail. For rare disease prediction, we choose N = 10, while for tissue type classification, we chose N = 5.</p><p id="P34">Motivated by the preference optimization framework (DPO and ORPO), we also evaluate the model&#x02019;s ability to prioritize likely diseases in the Top-<inline-formula><mml:math id="M1" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> preferred responses while avoiding Bottom- <inline-formula><mml:math id="M2" display="inline"><mml:mi>q</mml:mi></mml:math></inline-formula> unlikely responses. Let <inline-formula><mml:math id="M3" display="inline"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the <italic toggle="yes">i</italic>th response by LLM, consisting of a list of rare disease names. Let <inline-formula><mml:math id="M4" display="inline"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denote the top-<inline-formula><mml:math id="M5" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> disease names (<inline-formula><mml:math id="M6" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> labels with highest probabilities) predicted by upstream Multimodal ML model. If the true label if not at the first position, then we will swap the true label with the one first position. Let <inline-formula><mml:math id="M7" display="inline"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denote the bottom-<inline-formula><mml:math id="M8" display="inline"><mml:mi>q</mml:mi></mml:math></inline-formula> disease names predicted by upstream Multimodal ML model. Similarly, it consists of diseases with <inline-formula><mml:math id="M9" display="inline"><mml:mi>q</mml:mi></mml:math></inline-formula> lowest probabilities among all the diseases.</p><p id="P35">Next, we define the Top-&#x1d458; coverage rate and Bottom-&#x1d45e; avoidance rate, denoted by <inline-formula><mml:math id="M10" display="inline"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M11" display="inline"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> respectively, by
<disp-formula id="FD1">
<mml:math id="M12" display="block"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mspace width="1em"/><mml:mtext>and</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02229;</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math>
</disp-formula>
respectively. Finally, we define the overall Coverage-Avoidance Rate (CAR) to be the weighted harmonic mean between Top-<inline-formula><mml:math id="M13" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> coverage rate and Bottom-<inline-formula><mml:math id="M14" display="inline"><mml:mi>q</mml:mi></mml:math></inline-formula> avoidance rate. Mathematically, for <inline-formula><mml:math id="M15" display="inline"><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>,
<disp-formula id="FD2">
<mml:math id="M16" display="block"><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math>
</disp-formula>
The weighting parameter <inline-formula><mml:math id="M17" display="inline"><mml:mi>&#x003bb;</mml:mi></mml:math></inline-formula> can be adjusted further to balance the importance between Top-<inline-formula><mml:math id="M18" display="inline"><mml:mi>k</mml:mi></mml:math></inline-formula> coverage and Bottom-<inline-formula><mml:math id="M19" display="inline"><mml:mi>q</mml:mi></mml:math></inline-formula> avoidance rate. Note that when <inline-formula><mml:math id="M20" display="inline"><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, both <inline-formula><mml:math id="M21" display="inline"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M22" display="inline"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are equally weighted, and CAR is just the naive harmonic mean. When <inline-formula><mml:math id="M23" display="inline"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula>, we can set <inline-formula><mml:math id="M24" display="inline"><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to give <inline-formula><mml:math id="M25" display="inline"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> more weight. Likewise, when <inline-formula><mml:math id="M26" display="inline"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula>, we can set <inline-formula><mml:math id="M27" display="inline"><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to give <inline-formula><mml:math id="M28" display="inline"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> more weight. We also denote <inline-formula><mml:math id="M29" display="inline"><mml:mrow><mml:mi>k</mml:mi><mml:mo>/</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula> by Acceptance-over-Rejection (AoR) ratio.</p><p id="P36">Finally, we benchmark all the add-on techniques using six widely recognized datasets, including ARC (Abstract Science Reasoning)<sup><xref rid="R57" ref-type="bibr">57</xref></sup>, HellaSwag (Commonsense Inference)<sup><xref rid="R56" ref-type="bibr">56</xref></sup>, MMLU (Massive Multitask Language Understanding)<sup><xref rid="R54" ref-type="bibr">54</xref></sup>, TruthfulQA (Detecting False Information)<sup><xref rid="R55" ref-type="bibr">55</xref></sup>, Winogrande (Context-Based Inference)<sup><xref rid="R69" ref-type="bibr">69</xref></sup>, and GSM8K (Mathematical Reasoning)<sup><xref rid="R58" ref-type="bibr">58</xref></sup>. ARC (AI2 Reasoning Challenge) assesses scientific reasoning with grade-school science questions. HellaSwag tests commonsense inference through narrative continuation tasks. MMLU (Massive Multitask Language Understanding) evaluates multitask knowledge and reasoning across 57 diverse subjects. TruthfulQA measures the model&#x02019;s ability to avoid reproducing falsehoods. Winogrande focuses on commonsense reasoning with ambiguous pronoun resolution tasks. GSM8K asks the model with multi-step grade school math problems. All evaluations use a 5-shot setting to ensure consistency and rigor, which verifies that the techniques retain the original language understanding and reasoning capabilities of the base model after fine-tuning. This evaluation aims to determine whether the add-on technique retains the original language and reasoning capabilities of the base model.</p></sec><sec id="S10"><title>Automated Evaluation Program</title><p id="P37">Given the extensive number of experiments involved, we develop an automated evaluation framework, LLM-Eval, to systematically and rigorously assess the responses generated by LLMs. The evaluation process comprises multiple steps to ensure the plausibility, relevance, and ranking quality of the model&#x02019;s diagnoses. It begins with HFA, which checks whether the model generates plausible disease names by filtering out extreme hallucinations, such as nonsensical phrases or invalid outputs. Once HFA is passed, Top-N evaluates whether the ground truth disease is included in the ranked list of <inline-formula><mml:math id="M30" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> possible diseases generated by the model, while Top-1 checks if the top-ranked disease matches the ground truth. Additionally, we compute the CAR to measure whether the model prioritizes the top-&#x1d458; diseases and avoids the bottom-<inline-formula><mml:math id="M31" display="inline"><mml:mi>q</mml:mi></mml:math></inline-formula> diseases recommended by the upstream multimodal ML model.</p><p id="P38">To account for variations in phrasing or formatting that retain the same semantic meaning, we employ the SequenceMatcher algorithm<sup><xref rid="R70" ref-type="bibr">70</xref></sup>. This technique calculates a similarity ratio between two strings, effectively identifying matches even when textual representations differ slightly. For instance, it can align abbreviations with full names (e.g., &#x0201c;CdLS&#x0201d; and &#x0201c;Cornelia de Lange Syndrome&#x0201d;) or account for other minor variations in disease name representation (e.g., &#x0201c;Simpson-golabi-behmel Syndrome, type 1&#x0201d; and &#x0201c;Type 1 Simpson-golabi-behmel Syndrome&#x0201d;.</p><p id="P39">We set the similarity threshold to 0.6 for HFA to allow greater flexibility, as this stage focuses on detecting plausible outputs rather than assessing strict correctness. This step ensures minimizes false negatives during the plausibility check. Conversely, stricter thresholds of 0.8 are applied for Top-10, Top-1, and CAR, as these later stages require higher precision to avoid false positives and ensure accurate ranking and matching. This balance between flexibility and stringency ensures a robust evaluation framework.</p></sec><sec id="S11"><title>Data sources</title><p id="P40">For rare disease prediction, we primarily utilize the GMDB database (v1.0.9)<sup><xref rid="R49" ref-type="bibr">49</xref>,<xref rid="R51" ref-type="bibr">51</xref></sup>, a multimodal medical resource containing frontal facial images of patients along with corresponding textual metadata, including demographic information and clinical HPO terms. This database is accessible to researchers in the medical field but requires an application process to obtain access to the data.</p><p id="P41">For external validation, we employ two independent data sources beyond the primary GMDB database, the Phenopacket-derived clinical notes<sup><xref rid="R59" ref-type="bibr">59</xref></sup>. This dataset comprises 5980 high quality synthesized clinical notes created by converting standardized phenotype profiles (called &#x0201c;Phenopackets&#x0201d;) into natural language narratives. These notes simulate realistic clinical documentation by transforming structured phenotypic data into flowing text with the linguistic variability commonly observed in medical records. We divide this dataset into two subsets: one containing set of diseases that overlap with our GMDB training set (1638 samples representing 72 diseases) and another with diseases that are completely disjoint from our training set. (4342 samples representing 456 diseases)</p><p id="P42">For tissue type classification, we employ the PanNuke database, a comprehensive pan-cancer dataset designed for nuclei instance segmentation and classification. This dataset encompasses 19 distinct tissue types, annotated through a semi-automated process and rigorously quality-checked by clinical pathologists. As a result, it offers a dataset whose characteristics closely mirror those encountered in clinical settings, with minimal selection bias.</p></sec><sec id="S12"><title>Supervised Fine-tuning (SFT) of LLMs</title><sec id="S13"><title>SFT Data Construction:</title><p id="P43">Although this is a multimodality dataset, for the purpose of fine-tuning LLMs, we only use the texts component of GMDB. We work with the samples that have non-null present features, or equivalently, at least one HPO id in the &#x0201c;present features&#x0201d; column of the metadata. In this case, we will do the following two steps: (1) Transform the HPO id(s) into real text data via the standard HPO dictionary and then concatenate them with a comma &#x0201c;,&#x0201d; in between. For instance, the &#x0201c;HP:0000486; HP:0001263; HP:0010864&#x0201d; will become &#x0201c;Strabismus, Global developmental delay, Intellectual disability, severe.&#x0201d; (2) Add patients&#x02019; demographic information in the front. The image metadata of GMDB database contains patients&#x02019; sex, age, and ethnicity information, which will be combined for LLM fine-tuning. For tissue type classification, we format our fine-tuning data in the simple Visual Question Answering (VQA) format. <bold>Table S4</bold> contains the sample SFT data used for both rare disease prediction and tissue type classification.</p></sec><sec id="S14"><title>LoRA fine-tuning of LLMs:</title><p id="P44">For fine-tuning, we utilize a single node with 4 NVIDIA A100 GPUs (40GB) and employ Low-Rank Adaptation (LoRA)<sup><xref rid="R71" ref-type="bibr">71</xref></sup> to reduce memory overhead. Our LoRA configuration is set with 16-bit precision, and the parameters are tailored to model size: for Llama 3.1&#x02013;8B-Instruct and Gemma-2&#x02013;9B-Instruct, we set the rank to 128 and scaling factor (lora alpha) to 64; for Llama 3.2&#x02013;1B-Instruct, Llama 3.2&#x02013;3B-Instruct and Gemma-2&#x02013;2B-Instruct, we increase the rank to 256 and scaling factor to 128. A consistent LoRA dropout rate of 0.05 is set across all models to mitigate overfitting. For tissue type classification task, we set the rank to 128 and scaling factor to 64 for Llama3.2-Vision-11B-Instruct model.</p></sec></sec><sec id="S15"><title>Inference via Retrieval Augmented Generation (RAG)</title><p id="P45">Retrieval-Augmented Generation (RAG) combines retrieval systems with LLMs to enhance the relevance and contextual accuracy of generated text, making it particularly well-suited for domain-specific tasks such as disease prediction. In this work, we utilize LlamaIndex<sup><xref rid="R72" ref-type="bibr">72</xref></sup> to construct our RAG database, as it offers a flexible interface for connecting both structured and unstructured medical knowledge sources to LLMs, enabling efficient and accurate retrieval. The database is built from two key biomedical resources: OMIM<sup><xref rid="R73" ref-type="bibr">73</xref></sup> and HPO<sup><xref rid="R74" ref-type="bibr">74</xref></sup>. To improve retrieval precision, we preprocess these datasets by systematically mapping HPO and OMIM terms to their corresponding disease names, ensuring that phenotypic and disease-related terminology is accurately aligned.</p><p id="P46">To further enhance the embedding representations for diseases and phenotypes, we adopt NeuML/pubmedbert-base-embeddings<sup><xref rid="R75" ref-type="bibr">75</xref></sup>, a biomedical sentence transformer specifically trained on large-scale biomedical literature. This model captures the complex relationships and semantics of medical terms, allowing our system to understand the nuances of disease and phenotype descriptions more effectively. Together, these components enable our RAG pipeline to retrieve domain-specific, high-quality information, which is then used by the LLM to generate accurate and contextually appropriate outputs tailored to the disease prediction scenario.</p></sec><sec id="S16"><title>Preference Optimization Frameworks</title><sec id="S17"><title>Overview:</title><p id="P47">MINT is a framework for aligning Large Language Models with multimodal biomedical expertise through preference optimization. In this study, we explore two implementations of the MINT framework: MINT with DPO (Direct Preference Optimization) and MINT with ORPO (Odds Ratio Preference Optimization). While traditional Reinforcement Learning from Human Feedback (RLHF) approaches like PPO<sup><xref rid="R76" ref-type="bibr">76</xref>&#x02013;<xref rid="R78" ref-type="bibr">78</xref></sup> require reward model training, extensive hyperparameter tuning, and complex sampling procedures, DPO offers a more direct approach to preference alignment. The process begins with a base model, followed by SFT (Supervised Fine-tuning) to align the model&#x02019;s responses with specific requirements. The final step introduces a direct optimization objective that leverages the SFT model as a reference point to optimize the preference alignment between chosen and rejected responses. This approach enhances the model&#x02019;s domain-specific skills by learning from a dataset of human preferences without the complexity of reward modeling and policy optimization typically associated with RLHF methods.</p><p id="P48">ORPO, on the other hand, integrates preference alignment directly into the SFT phase in a single step. Its objective function leverages the odds ratio, offering a stable and effective approach to distinguish between preferred and non-preferred responses. This method can be applied effectively across various model sizes and demonstrates superior performance.</p></sec><sec id="S18"><title>Preference Learning Dataset Construction:</title><p id="P49">Due to the substantial time and resources required for manual data labeling, we employ the previously developed GestaltMML and PLIP models to build a dataset based on preference learning.</p><p id="P50">In each instance within the GMDB or PanNuke database, GestaltMML or PLIP produces a ranked list of potential labels (either disease names or tissue types). From this list, the top k labels (for example, k = 5, 10) are identified as preferred (&#x0201c;chosen&#x0201d;) outputs, indicative of probable diagnoses, while the bottom q labels (for example, q = 10, 50) are classified as non-preferred (&#x0201c;rejected&#x0201d;) outputs, suggesting fewer probable diagnoses. Generally, q is set larger than k to reflect that potential diagnoses are fewer than improbable ones. If the most probable prediction (Top-1 prediction) from GestaltMML or PLIP does not align with the actual prediction, it is substituted with the true prediction to ensure its presence at the top of the &#x0201c;chosen&#x0201d; outputs.</p><p id="P51">Thus, each case is structured into a pair of prompts: the input data sample is followed by the &#x0201c;chosen&#x0201d; and &#x0201c;rejected&#x0201d; labels, with the accurate prediction consistently presented first in the list. The sample of preference learning dataset are illustrated in <bold>Table S4</bold>.</p></sec><sec id="S19"><title>Prompting Strategies for Inference:</title><p id="P52">To optimize MINT for rare disease prediction and tissue type classification, we implement a dual-phase prompt strategy to prevent model collapse and catastrophic forgetting. This approach allows the model to strengthen diagnostic accuracy while ensuring outputs that are clear and clinically useful.</p><p id="P53">During the inference stage, we introduce a more detailed prompt format to ensure the model&#x02019;s outputs are presented in a consistent and structured way, making them easier to interpret for clinicians and data analysts. This phase&#x02019;s prompt requires a numbered list of diseases, with precise formatting guidelines that are easy to follow and interpret. The concrete prompts for inference are presented in <bold>Table S4</bold>.</p><p id="P54">The added structure in the inference prompt help MINT produce outputs that are not only diagnostically accurate but also neatly formatted and suitable for clinical review. By implementing this dual-phase prompt strategy, we ensure that MINT could generate flexible and insightful responses during training while providing reliable, structured outputs in inference, which is the key for supporting human interpretation in clinical workflows.</p></sec><sec id="S20"><title>Direct Preference Optimization (DPO):</title><p id="P55">DPO is a preference-based learning framework built upon a SFT model, designed to align model predictions with task-specific preferences. DPO builds directly on the SFT model by leveraging preference signals to prioritize chosen diagnoses <inline-formula><mml:math id="M32" display="inline"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> over rejected diagnoses <inline-formula><mml:math id="M33" display="inline"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, ensuring the model aligns its outputs to reflect user-defined preferences. To achieve this, DPO compares the output probabilities of the trainable actor model <inline-formula><mml:math id="M34" display="inline"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with those of a frozen reference model <inline-formula><mml:math id="M35" display="inline"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which is a static copy of the SFT model. The reference model ensures that the preference-aligned actor model does not deviate too far from the base SFT model, thus preventing issues such as hallucinations or overly biased outputs. The DPO objective function is expressed as follows:
<disp-formula id="FD3">
<mml:math id="M36" display="block"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">E</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="italic">log &#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math>
</disp-formula>
Here, <inline-formula><mml:math id="M37" display="inline"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the conditional probability of the actor model, <inline-formula><mml:math id="M38" display="inline"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02223;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the frozen reference model, <inline-formula><mml:math id="M39" display="inline"><mml:mi>x</mml:mi></mml:math></inline-formula> is the input phenotypic description, and <inline-formula><mml:math id="M40" display="inline"><mml:mi>&#x003b2;</mml:mi></mml:math></inline-formula> is a scaling factor to control the distance between actor model and reference model. By maximizing the relative log likelihood of chosen response over rejected response while referencing the base SFT model, DPO ensures the effective alignment with preferences without compromising the foundational consistency of the model.</p></sec><sec id="S21"><title>MINT:</title><p id="P56"><underline>Our default implementation, MINT with ORPO,</underline> utilizes the Odds Ratio Preference Optimization (ORPO). Compared to DPO, the ORPO is a framework that combines Supervised Fine-Tuning (SFT) and preference learning in one stage.</p><p id="P57">Given a set of paired diagnoses <inline-formula><mml:math id="M41" display="inline"><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> for each patient case, ORPO minimizes the preference loss <inline-formula><mml:math id="M42" display="inline"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> which encourages higher scores for chosen diagnoses:
<disp-formula id="FD4">
<mml:math id="M43" display="block"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math>
</disp-formula>
where <inline-formula><mml:math id="M44" display="inline"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>F</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the conventional causal language model negative log-likelihood (NLL) loss function to maximize the likelihood of generating the reference tokens. <inline-formula><mml:math id="M45" display="inline"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> maximizes the odds ratio between the likelihood of generating the favored response <inline-formula><mml:math id="M46" display="inline"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> over disfavored response <inline-formula><mml:math id="M47" display="inline"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. More specifically, the odds of <inline-formula><mml:math id="M48" display="inline"><mml:mi>y</mml:mi></mml:math></inline-formula> given <inline-formula><mml:math id="M49" display="inline"><mml:mi>x</mml:mi></mml:math></inline-formula> is defined by
<disp-formula id="FD5">
<mml:math id="M50" display="block"><mml:mrow><mml:mi>O</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math>
</disp-formula>
Following the above, the odds ratio of a chosen response <inline-formula><mml:math id="M51" display="inline"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> over <inline-formula><mml:math id="M52" display="inline"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, denote by <inline-formula><mml:math id="M53" display="inline"><mml:mrow><mml:mi>O</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>, is defined by
<disp-formula id="FD6">
<mml:math id="M54" display="block"><mml:mrow><mml:mi>O</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>O</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>O</mml:mi><mml:mi>d</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula>
and the <inline-formula><mml:math id="M55" display="inline"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="italic">log &#x003c3;</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="italic">log OR</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>. In other words, the log odds ratio was wrapped with the log sigmoid function so that <inline-formula><mml:math id="M56" display="inline"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> could be minimized by increasing the log odds ratio between <inline-formula><mml:math id="M57" display="inline"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>&#x003c9;</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M58" display="inline"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p id="P58">Finally, in <inline-formula><mml:math id="M59" display="inline"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>R</mml:mi><mml:mi>P</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the <inline-formula><mml:math id="M60" display="inline"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is weighted with <inline-formula><mml:math id="M61" display="inline"><mml:mi>&#x003b2;</mml:mi></mml:math></inline-formula> adapts the pretrained model to the domain-specific task and meanwhile enforces separation between chosen and rejected outputs (see <bold>Figure S2</bold>), compelling the model to push disfavored generations further down.</p></sec><sec id="S22"><title>Benchmark experiments</title><p id="P59">For rare disease prediction, we perform five methodological comparisons on Llama 3.2&#x02013;3B-Instruct: (1) direct inference using the base model, (2) base model with RAG support, (3) base model after supervised fine-tuning (SFT), (4) DPO, and (5) MINT. Additionally, we also assess other four relatively small open-source models of different sizes: Llama 3.2&#x02013;1B-Instruct, Llama 3.1&#x02013;8B-Instruct, Gemma-2&#x02013;2B-Instruct, and Gemma-2&#x02013;9B-Instruct. For larger models like Llama 3.1&#x02013;70B-Instruct and Llama 3.1&#x02013;405B-Instruct, we limit our evaluation to their direct inference capabilities using the base model, as implementing additional techniques is not feasible due to computational resource constraints faced by the research community. All the results besides Llama 3.2&#x02013;3B-Instruct are presented in <xref rid="T1" ref-type="table">Table S1</xref>.</p><p id="P60">Beyond these eight experiments, we also evaluate other LLMs, including the closed-source GPT-4o (access time Nov. 2024), on one-shot learning using CoT prompts. Sample responses are included in the Supplementary Material.</p><p id="P61">For tissue type classification, we perform four methodological comparisons on Llama 3.2-Vision-11B-Instruct: (1) direct inference using the base model, (2) base model after supervised fine-tuning (SFT), (3) DPO, and (4) MINT.</p></sec></sec></sec><sec id="S23"><title>DISCUSSION</title><p id="P62">In this study, we introduce MINT (Multimodal Integrated kNowledge Transfer), a novel and effective framework for aligning unimodal LLMs with multimodal biomedical data through preference optimization, with ORPO serving as the default backbone mechanism in our implementation. Our comprehensive experiments demonstrate MINT&#x02019;s effectiveness in two complex biomedical tasks: rare disease prediction from clinical texts and tissue type classification from histological images. These results support MINT&#x02019;s potential as a versatile approach for enhancing domain-specific capabilities of large language models (LLMs) without compromising their general language and visual understanding.</p><p id="P63">Despite MINT&#x02019;s promising results, several important limitations must be acknowledged. First, MINT&#x02019;s performance diminishes substantially in zero-shot scenarios with completely unseen disease classes, as evidenced by the lower accuracy on disjoint disease subsets in our external validation. While this is expected for any fine-tuning approach, it highlights the need for complementary techniques such as RAG when applying MINT in real-world clinical settings where extremely rare, previously unseen diseases may be encountered. Second, the quality of the upstream multimodal model significantly influences MINT&#x02019;s effectiveness. MINT inherits both the strengths and the weaknesses of its upstream model. If the upstream model contains biases or errors, these may propagate to MINT-enhanced LLM. This risk is particularly relevant in medical applications where demographic or geographic biases in training data could lead to disparities in diagnostic accuracy. Third, while MINT significantly improves specific task performance, it still falls short of specialized encoder-based classification models in some scenarios. This gap reflects the fundamental architectural differences between encoder models optimized for classification and decoder models designed for generative tasks. The auto-regressive nature of text generation introduces additional complexity compared to direct label selection, which may limit the upper bound of MINT&#x02019;s performance on classification tasks.</p><p id="P64">The core innovation of MINT lies in its unified approach to aligning unimodal models with multimodal expertise through preference optimization. Rather than using traditional knowledge transfer methods that require parallel training of teacher and student models, MINT uses multimodal models as preference data generators to create structured datasets of likely and unlikely predictions. These preference datasets then guide the alignment of downstream LLMs through ORPO, enabling them to benefit from patterns identified in multimodal data while preserving their general capabilities. This approach offers several distinct advantages: First, MINT effectively bridges the modality gap between multimodal machine learning models and text-only or image-only LLMs. Our rare disease prediction experiments demonstrate that MINT can align LLMs with patterns from GestaltMML, which utilizes facial images, demographic information and clinical phenotypes. Although the downstream LLMs consume only text, they still benefit from facial image data indirectly through the preference-learning dataset, which encapsulates inter-modal relationships learned by GestaltMML. Second, MINT demonstrates similarly impressive performance in the tissue type classification task with preference data from PLIP, helping models prioritize relevant tissue types and avoid implausible ones more effectively than baseline approaches. Third, MINT&#x02019;s preference optimization approach enables more nuanced learning than traditional supervised fine-tuning by incorporating both positive and negative examples during training, developing a more discriminative understanding of subtle features that differentiate similar conditions, as evidenced in our tissue type classification case study. Fourth, MINT-enhanced decoder-based LLMs show promising generalization capabilities compared to traditional encoder-only classifiers, particularly in zero-shot scenarios with previously unseen disease classes, demonstrating the decoder architecture&#x02019;s inherent flexibility and reasoning advantages. Finally, key design choices contribute to MINT&#x02019;s success: the balanced Acceptance-over-Rejection ratio provides clear contrastive learning signals; the odds ratio loss delivers stable gradients even with limited training data; and the approach preserves general model capabilities while enhancing domain-specific performance, making it particularly valuable for biomedical applications.</p><p id="P65">Looking forward, the MINT framework opens several promising directions for future research in biomedical AI. First, hybrid approaches combining MINT with retrieval mechanisms could address the zero-shot limitations while preserving MINT&#x02019;s strong in-distribution performance. Our external validation results on disjoint disease subsets, where RAG outperformed MINT, suggest that such hybrid approaches might be particularly valuable for rare disease prediction, where unseen presentations are common. Second, extending MINT to additional biomedical tasks beyond disease prediction and tissue type classification could demonstrate its broader utility. Potential applications include medical image analysis, genomic interpretation, drug discovery, and clinical decision support. Each domain would require domain-specific upstream models but could benefit from MINT&#x02019;s flexible knowledge transfer approach. Third, investigating the interpretability of MINT-enhanced models represents an important clinical research direction. While MINT improves predictive performance, understanding the reasoning behind its predictions remains crucial for clinical trust and adoption. Future work could explore techniques for extracting and visualizing the diagnostic patterns learned through preference optimization. For tissue classification specifically, integrating region-of-interest highlighting could help pathologists understand which histological features most influenced the model&#x02019;s predictions. Finally, prospective clinical evaluations in real-world healthcare settings will be essential to validate MINT&#x02019;s practical utility. Such studies should assess not only diagnostic accuracy but also impact on clinical decision-making, time-to-prediction for rare conditions, and overall patient outcomes.</p><p id="P66">In conclusion, MINT represents an advancement in biomedical AI by bridging the gap between specialized multimodal models and general-purpose large language models. By leveraging preference optimization to transfer domain-specific knowledge, MINT enhances the capabilities of open-source LLMs in critical biomedical tasks while maintaining their inherent flexibility and reasoning capabilities. The framework demonstrated consistent effectiveness across multiple tasks, models, and external validation datasets suggests broad applicability in the biomedical domain. As LLMs continue to evolve as interfaces for healthcare information, approaches like MINT that can effectively integrate specialized domain knowledge while preserving general capabilities will be increasingly valuable for advancing precision medicine and improving patient care.</p></sec></body><back><ack id="S24"><title>ACKNOWLEDGEMENTS</title><p id="P67">We thank the GestaltMatcher Database (GMDB) which provides a collection of curated medical photography of genetic syndromes for training the multimodal model used in the current study. We thank patients and their families for contributing facial photos and phenotype descriptions to enable the establishment of computational models. This project is supported by NIH grant HG013031 and the CHOP Research Institute.</p></ack><fn-group><fn id="FN2"><p id="P69">DECLARATIONS</p></fn><fn fn-type="COI-statement" id="FN3"><p id="P70">Competing interests</p><p id="P71">The authors declare no competing interests.</p></fn></fn-group><sec sec-type="data-availability" id="S25"><title>Availability of data and materials</title><p id="P68">The GMDB (v1.0.9) database used in the current study can be obtained from <ext-link xlink:href="https://db.gestaltmatcher.org/" ext-link-type="uri">https://db.gestaltmatcher.org/</ext-link>. All the software tools and computational workflow (as Jupyter Notebook) can be found at <ext-link xlink:href="https://github.com/WGLab/MINT-LLM" ext-link-type="uri">https://github.com/WGLab/MINT-LLM</ext-link>. This study did not generate any new material.</p></sec><ref-list><title>REFERENCES</title><ref id="R1"><label>1.</label><mixed-citation publication-type="other"><name><surname>Wolf</surname><given-names>T</given-names></name>, <name><surname>Debut</surname><given-names>L</given-names></name>, <name><surname>Sanh</surname><given-names>V</given-names></name>, <etal/>
<source>Transformers: State-of-the-Art Natural Language Processing</source>. <year>2019</year>:</mixed-citation></ref><ref id="R2"><label>2.</label><mixed-citation publication-type="other"><name><surname>Vaswani</surname><given-names>A</given-names></name>, <name><surname>Shazeer</surname><given-names>N</given-names></name>, <name><surname>Parmar</surname><given-names>N</given-names></name>, <etal/>
<article-title>Attention is all you need</article-title>. <source>Advances in neural information processing systems</source>. <year>2017</year>;<fpage>30</fpage></mixed-citation></ref><ref id="R3"><label>3.</label><mixed-citation publication-type="other"><name><surname>McKinzie</surname><given-names>B</given-names></name>, <name><surname>Gan</surname><given-names>Z</given-names></name>, <name><surname>Fauconnier</surname><given-names>J-P</given-names></name>, <etal/>
<article-title>MM1: methods, analysis and insights from multimodal LLM pre-training</article-title>. <source>Springer</source>; <year>2024</year>:<fpage>304</fpage>&#x02013;<lpage>323</lpage>.</mixed-citation></ref><ref id="R4"><label>4.</label><mixed-citation publication-type="journal"><name><surname>Tirumala</surname><given-names>K</given-names></name>, <name><surname>Simig</surname><given-names>D</given-names></name>, <name><surname>Aghajanyan</surname><given-names>A</given-names></name>, <name><surname>Morcos</surname><given-names>A</given-names></name>. <article-title>D4: Improving llm pretraining via document deduplication and diversification</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2023</year>;<volume>36</volume>:<fpage>53983</fpage>&#x02013;<lpage>53995</lpage>.</mixed-citation></ref><ref id="R5"><label>5.</label><mixed-citation publication-type="other"><name><surname>Shi</surname><given-names>W</given-names></name>, <name><surname>Ajith</surname><given-names>A</given-names></name>, <name><surname>Xia</surname><given-names>M</given-names></name>, <etal/>
<article-title>Detecting pretraining data from large language models</article-title>. <source>arXiv preprint arXiv:231016789</source>. <year>2023</year>;</mixed-citation></ref><ref id="R6"><label>6.</label><mixed-citation publication-type="other"><name><surname>Devlin</surname><given-names>J</given-names></name>, <name><surname>Chang</surname><given-names>M-W</given-names></name>, <name><surname>Lee</surname><given-names>K</given-names></name>, <name><surname>Toutanova</surname><given-names>K</given-names></name>. <article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>. <source>arXiv preprint arXiv:181004805</source>. <year>2018</year>;</mixed-citation></ref><ref id="R7"><label>7.</label><mixed-citation publication-type="other"><name><surname>Dubey</surname><given-names>A</given-names></name>, <name><surname>Jauhri</surname><given-names>A</given-names></name>, <name><surname>Pandey</surname><given-names>A</given-names></name>, <etal/>
<article-title>The llama 3 herd of models</article-title>. <source>arXiv preprint arXiv:240721783</source>. <year>2024</year>;</mixed-citation></ref><ref id="R8"><label>8.</label><mixed-citation publication-type="webpage"><source>Meta. Llama 3.2 Model Card</source>. <date-in-citation>Accessed 12/02/2024</date-in-citation>, <year>2024</year>. models/llama3_2/MODEL_CARD.md</mixed-citation></ref><ref id="R9"><label>9.</label><mixed-citation publication-type="webpage"><source>Meta. Llama 3.3 Model Card</source>. <date-in-citation>Accessed 02/03/2025</date-in-citation>, <year>2025</year>. models/llama3_3/MODEL_CARD.md</mixed-citation></ref><ref id="R10"><label>10.</label><mixed-citation publication-type="other"><name><surname>Liu</surname><given-names>A</given-names></name>, <name><surname>Feng</surname><given-names>B</given-names></name>, <name><surname>Xue</surname><given-names>B</given-names></name>, <etal/>
<article-title>Deepseek-v3 technical report</article-title>. <source>arXiv preprint arXiv:241219437</source>. <year>2024</year>;</mixed-citation></ref><ref id="R11"><label>11.</label><mixed-citation publication-type="other"><name><surname>Guo</surname><given-names>D</given-names></name>, <name><surname>Yang</surname><given-names>D</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <etal/>
<article-title>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</article-title>. <source>arXiv preprint arXiv:250112948</source>. <year>2025</year>;</mixed-citation></ref><ref id="R12"><label>12.</label><mixed-citation publication-type="journal"><name><surname>Afantenos</surname><given-names>S</given-names></name>, <name><surname>Karkaletsis</surname><given-names>V</given-names></name>, <name><surname>Stamatopoulos</surname><given-names>P</given-names></name>. <article-title>Summarization from medical documents: a survey</article-title>. <source>Artificial intelligence in medicine</source>. <year>2005</year>;<volume>33</volume>(<issue>2</issue>):<fpage>157</fpage>&#x02013;<lpage>177</lpage>.<pub-id pub-id-type="pmid">15811783</pub-id>
</mixed-citation></ref><ref id="R13"><label>13.</label><mixed-citation publication-type="journal"><name><surname>Mamykina</surname><given-names>L</given-names></name>, <name><surname>Vawdrey</surname><given-names>DK</given-names></name>, <name><surname>Stetson</surname><given-names>PD</given-names></name>, <name><surname>Zheng</surname><given-names>K</given-names></name>, <name><surname>Hripcsak</surname><given-names>G</given-names></name>. <article-title>Clinical documentation: composition or synthesis?</article-title>
<source>Journal of the American Medical Informatics Association</source>. <year>2012</year>;<volume>19</volume>(<issue>6</issue>):<fpage>1025</fpage>&#x02013;<lpage>1031</lpage>.<pub-id pub-id-type="pmid">22813762</pub-id>
</mixed-citation></ref><ref id="R14"><label>14.</label><mixed-citation publication-type="journal"><name><surname>Mishra</surname><given-names>R</given-names></name>, <name><surname>Bian</surname><given-names>J</given-names></name>, <name><surname>Fiszman</surname><given-names>M</given-names></name>, <etal/>
<article-title>Text summarization in the biomedical domain: a systematic review of recent research</article-title>. <source>Journal of biomedical informatics</source>. <year>2014</year>;<volume>52</volume>:<fpage>457</fpage>&#x02013;<lpage>467</lpage>.<pub-id pub-id-type="pmid">25016293</pub-id>
</mixed-citation></ref><ref id="R15"><label>15.</label><mixed-citation publication-type="journal"><name><surname>Gulden</surname><given-names>C</given-names></name>, <name><surname>Kirchner</surname><given-names>M</given-names></name>, <name><surname>Sch&#x000fc;ttler</surname><given-names>C</given-names></name>, <etal/>
<article-title>Extractive summarization of clinical trial descriptions</article-title>. <source>International journal of medical informatics</source>. <year>2019</year>;<volume>129</volume>:<fpage>114</fpage>&#x02013;<lpage>121</lpage>.<pub-id pub-id-type="pmid">31445245</pub-id>
</mixed-citation></ref><ref id="R16"><label>16.</label><mixed-citation publication-type="journal"><name><surname>Tripathi</surname><given-names>S</given-names></name>, <name><surname>Sukumaran</surname><given-names>R</given-names></name>, <name><surname>Cook</surname><given-names>TS</given-names></name>. <article-title>Efficient healthcare with large language models: optimizing clinical workflow and enhancing patient care</article-title>. <source>Journal of the American Medical Informatics Association</source>. <year>2024</year>;<volume>31</volume>(<issue>6</issue>):<fpage>1436</fpage>&#x02013;<lpage>1440</lpage>.<pub-id pub-id-type="pmid">38273739</pub-id>
</mixed-citation></ref><ref id="R17"><label>17.</label><mixed-citation publication-type="journal"><name><surname>Qiu</surname><given-names>J</given-names></name>, <name><surname>Lam</surname><given-names>K</given-names></name>, <name><surname>Li</surname><given-names>G</given-names></name>, <etal/>
<article-title>LLM-based agentic systems in medicine and healthcare</article-title>. <source>Nature Machine Intelligence</source>. <year>2024</year>;<volume>6</volume>(<issue>12</issue>):<fpage>1418</fpage>&#x02013;<lpage>1420</lpage>.</mixed-citation></ref><ref id="R18"><label>18.</label><mixed-citation publication-type="book"><name><surname>Gebreab</surname><given-names>SA</given-names></name>, <name><surname>Salah</surname><given-names>K</given-names></name>, <name><surname>Jayaraman</surname><given-names>R</given-names></name>, <name><surname>ur Rehman</surname><given-names>MH</given-names></name>, <name><surname>Ellaham</surname><given-names>S</given-names></name>. <source>Llm-based framework for administrative task automation in healthcare</source>. <publisher-name>IEEE</publisher-name>; <year>2024</year>:<fpage>1</fpage>&#x02013;<lpage>7</lpage>.</mixed-citation></ref><ref id="R19"><label>19.</label><mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>K</given-names></name>, <name><surname>Weng</surname><given-names>C</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>. <article-title>Assessing the utility of large language models for phenotype-driven gene prioritization in the diagnosis of rare genetic disease</article-title>. <source>The American Journal of Human Genetics</source>. <year>2024</year>;<volume>111</volume>(<issue>10</issue>):<fpage>2190</fpage>&#x02013;<lpage>2202</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.ajhg.2024.08.010</pub-id><pub-id pub-id-type="pmid">39255797</pub-id>
</mixed-citation></ref><ref id="R20"><label>20.</label><mixed-citation publication-type="other"><name><surname>do Olmo</surname><given-names>J</given-names></name>, <name><surname>Logrono</surname><given-names>J</given-names></name>, <name><surname>Mascias</surname><given-names>C</given-names></name>, <name><surname>Martinez</surname><given-names>M</given-names></name>, <name><surname>Isla</surname><given-names>J</given-names></name>. <article-title>Assessing DxGPT: Diagnosing Rare Diseases with Various Large Language Models</article-title>. <source>medRxiv</source>. <year>2024</year>:2024.05. 08.24307062.</mixed-citation></ref><ref id="R21"><label>21.</label><mixed-citation publication-type="other"><name><surname>Gunel</surname><given-names>B</given-names></name>, <name><surname>Du</surname><given-names>J</given-names></name>, <name><surname>Conneau</surname><given-names>A</given-names></name>, <name><surname>Stoyanov</surname><given-names>V</given-names></name>. <article-title>Supervised contrastive learning for pre-trained language model fine-tuning</article-title>. <source>arXiv preprint arXiv:201101403</source>. <year>2020</year>;</mixed-citation></ref><ref id="R22"><label>22.</label><mixed-citation publication-type="other"><name><surname>Chen</surname><given-names>T</given-names></name>, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Chang</surname><given-names>S</given-names></name>, <name><surname>Cheng</surname><given-names>Y</given-names></name>, <name><surname>Amini</surname><given-names>L</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>. <source>Adversarial robustness: From self-supervised pre-training to fine-tuning</source>. <year>2020</year>:<fpage>699</fpage>&#x02013;<lpage>708</lpage>.</mixed-citation></ref><ref id="R23"><label>23.</label><mixed-citation publication-type="journal"><name><surname>Al-Moslmi</surname><given-names>T</given-names></name>, <name><surname>Oca&#x000f1;a</surname><given-names>MG</given-names></name>, <name><surname>Opdahl</surname><given-names>AL</given-names></name>, <name><surname>Veres</surname><given-names>C</given-names></name>. <article-title>Named entity extraction for knowledge graphs: A literature overview</article-title>. <source>IEEE Access</source>. <year>2020</year>;<volume>8</volume>:<fpage>32862</fpage>&#x02013;<lpage>32881</lpage>.</mixed-citation></ref><ref id="R24"><label>24.</label><mixed-citation publication-type="journal"><name><surname>Etzioni</surname><given-names>O</given-names></name>, <name><surname>Cafarella</surname><given-names>M</given-names></name>, <name><surname>Downey</surname><given-names>D</given-names></name>, <etal/>
<article-title>Unsupervised named-entity extraction from the web: An experimental study</article-title>. <source>Artificial intelligence</source>. <year>2005</year>;<volume>165</volume>(<issue>1</issue>):<fpage>91</fpage>&#x02013;<lpage>134</lpage>.</mixed-citation></ref><ref id="R25"><label>25.</label><mixed-citation publication-type="other"><name><surname>Daiber</surname><given-names>J</given-names></name>, <name><surname>Jakob</surname><given-names>M</given-names></name>, <name><surname>Hokamp</surname><given-names>C</given-names></name>, <name><surname>Mendes</surname><given-names>PN</given-names></name>. <source>Improving efficiency and accuracy in multilingual entity extraction</source>. <year>2013</year>:<fpage>121</fpage>&#x02013;<lpage>124</lpage>.</mixed-citation></ref><ref id="R26"><label>26.</label><mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>, <name><surname>Deng</surname><given-names>W</given-names></name>, <etal/>
<article-title>Enhancing phenotype recognition in clinical notes using large language models: PhenoBCBERT and PhenoGPT</article-title>. <source>Patterns</source>. <year>2024</year>;<volume>5</volume>(<issue>1</issue>)</mixed-citation></ref><ref id="R27"><label>27.</label><mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>D</given-names></name>, <name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>K</given-names></name>. <article-title>Exploring the reversal curse and other deductive logical reasoning in BERT and GPT-based large language models</article-title>. <source>Patterns</source>. <year>2024</year>;<volume>5</volume>(<issue>9</issue>)</mixed-citation></ref><ref id="R28"><label>28.</label><mixed-citation publication-type="other"><name><surname>Berglund</surname><given-names>L</given-names></name>, <name><surname>Tong</surname><given-names>M</given-names></name>, <name><surname>Kaufmann</surname><given-names>M</given-names></name>, <etal/>
<article-title>The Reversal Curse: LLMs trained on&#x0201d; A is B&#x0201d; fail to learn&#x0201d; B is A&#x0201d;</article-title>. <source>arXiv preprint arXiv:230912288</source>. <year>2023</year>;</mixed-citation></ref><ref id="R29"><label>29.</label><mixed-citation publication-type="journal"><name><surname>Zhu</surname><given-names>H</given-names></name>, <name><surname>Huang</surname><given-names>B</given-names></name>, <name><surname>Zhang</surname><given-names>S</given-names></name>, <etal/>
<article-title>Towards a Theoretical Understanding of the&#x02019;Reversal Curse&#x02019;via Training Dynamics</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2024</year>;<volume>37</volume>:<fpage>90473</fpage>&#x02013;<lpage>90513</lpage>.</mixed-citation></ref><ref id="R30"><label>30.</label><mixed-citation publication-type="other"><name><surname>Huang</surname><given-names>T</given-names></name>, <name><surname>Hu</surname><given-names>S</given-names></name>, <name><surname>Ilhan</surname><given-names>F</given-names></name>, <name><surname>Tekin</surname><given-names>SF</given-names></name>, <name><surname>Liu</surname><given-names>L</given-names></name>. <article-title>Harmful fine-tuning attacks and defenses for large language models: A survey</article-title>. <source>arXiv preprint arXiv:240918169</source>. <year>2024</year>;</mixed-citation></ref><ref id="R31"><label>31.</label><mixed-citation publication-type="other"><name><surname>Qi</surname><given-names>X</given-names></name>, <name><surname>Zeng</surname><given-names>Y</given-names></name>, <name><surname>Xie</surname><given-names>T</given-names></name>, <etal/>
<source>Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:231003693</source>. <year>2023</year>;</mixed-citation></ref><ref id="R32"><label>32.</label><mixed-citation publication-type="journal"><name><surname>Vrban&#x0010d;i&#x0010d;</surname><given-names>G</given-names></name>, <name><surname>Podgorelec</surname><given-names>V</given-names></name>. <article-title>Transfer learning with adaptive fine-tuning</article-title>. <source>IEEE Access</source>. <year>2020</year>;<volume>8</volume>:<fpage>196197</fpage>&#x02013;<lpage>196211</lpage>.</mixed-citation></ref><ref id="R33"><label>33.</label><mixed-citation publication-type="confproc"><name><surname>Wortsman</surname><given-names>M</given-names></name>, <name><surname>Ilharco</surname><given-names>G</given-names></name>, <name><surname>Li</surname><given-names>M</given-names></name>, <etal/>
<source>Robust fine-tuning of zero-shot models</source>. <conf-name>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name> (<publisher-name>CVPR</publisher-name>). <year>2021</year>:<fpage>7949</fpage>&#x02013;<lpage>7961</lpage>.</mixed-citation></ref><ref id="R34"><label>34.</label><mixed-citation publication-type="other"><name><surname>Hong</surname><given-names>J</given-names></name>, <name><surname>Lee</surname><given-names>N</given-names></name>, <name><surname>Thorne</surname><given-names>J</given-names></name>. <source>Orpo: Monolithic preference optimization without reference model</source>. <year>2024</year>:<fpage>11170</fpage>&#x02013;<lpage>11189</lpage>.</mixed-citation></ref><ref id="R35"><label>35.</label><mixed-citation publication-type="other"><name><surname>Wu</surname><given-names>D</given-names></name>, <name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>, <etal/>
<article-title>GestaltMML: Enhancing Rare Genetic Disease Diagnosis through Multimodal Machine Learning Combining Facial Images and Clinical Texts</article-title>. <source>ArXiv</source>. <year>2024</year>;</mixed-citation></ref><ref id="R36"><label>36.</label><mixed-citation publication-type="journal"><name><surname>Lewis</surname><given-names>P</given-names></name>, <name><surname>Perez</surname><given-names>E</given-names></name>, <name><surname>Piktus</surname><given-names>A</given-names></name>, <etal/>
<article-title>Retrieval-augmented generation for knowledge-intensive nlp tasks</article-title>. <source>Advances in neural information processing systems</source>. <year>2020</year>;<volume>33</volume>:<fpage>9459</fpage>&#x02013;<lpage>9474</lpage>.</mixed-citation></ref><ref id="R37"><label>37.</label><mixed-citation publication-type="other"><name><surname>Gao</surname><given-names>Y</given-names></name>, <name><surname>Xiong</surname><given-names>Y</given-names></name>, <name><surname>Gao</surname><given-names>X</given-names></name>, <etal/>
<article-title>Retrieval-augmented generation for large language models: A survey</article-title>. <source>arXiv preprint arXiv:231210997</source>. <year>2023</year>;</mixed-citation></ref><ref id="R38"><label>38.</label><mixed-citation publication-type="other"><name><surname>Jiang</surname><given-names>Z</given-names></name>, <name><surname>Xu</surname><given-names>FF</given-names></name>, <name><surname>Gao</surname><given-names>L</given-names></name>, <etal/>
<article-title>Active retrieval augmented generation</article-title>. <source>arXiv preprint arXiv:230506983</source>. <year>2023</year>;</mixed-citation></ref><ref id="R39"><label>39.</label><mixed-citation publication-type="book"><name><surname>Gamper</surname><given-names>J</given-names></name>, <name><surname>Alemi Koohbanani</surname><given-names>N</given-names></name>, <name><surname>Benet</surname><given-names>K</given-names></name>, <name><surname>Khuram</surname><given-names>A</given-names></name>, <name><surname>Rajpoot</surname><given-names>N</given-names></name>. <source>Pannuke: an open pan-cancer histology dataset for nuclei instance segmentation and classification</source>. <publisher-name>Springer</publisher-name>; <year>2019</year>:<fpage>11</fpage>&#x02013;<lpage>19</lpage>.</mixed-citation></ref><ref id="R40"><label>40.</label><mixed-citation publication-type="other"><name><surname>Gamper</surname><given-names>J</given-names></name>, <name><surname>Koohbanani</surname><given-names>NA</given-names></name>, <name><surname>Benes</surname><given-names>K</given-names></name>, <etal/>
<article-title>Pannuke dataset extension, insights and baselines</article-title>. <source>arXiv preprint arXiv:200310778</source>. <year>2020</year>;</mixed-citation></ref><ref id="R41"><label>41.</label><mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>Z</given-names></name>, <name><surname>Bianchi</surname><given-names>F</given-names></name>, <name><surname>Yuksekgonul</surname><given-names>M</given-names></name>, <name><surname>Montine</surname><given-names>TJ</given-names></name>, <name><surname>Zou</surname><given-names>J</given-names></name>. <article-title>A visual&#x02013;language foundation model for pathology image analysis using medical twitter</article-title>. <source>Nature medicine</source>. <year>2023</year>;<volume>29</volume>(<issue>9</issue>):<fpage>2307</fpage>&#x02013;<lpage>2316</lpage>.</mixed-citation></ref><ref id="R42"><label>42.</label><mixed-citation publication-type="journal"><name><surname>Zemojtel</surname><given-names>T</given-names></name>, <name><surname>K&#x000f6;hler</surname><given-names>S</given-names></name>, <name><surname>Mackenroth</surname><given-names>L</given-names></name>, <etal/>
<article-title>Effective diagnosis of genetic disease by computational phenotype analysis of the disease-associated genome</article-title>. <source>Science translational medicine</source>. <year>2014</year>;<volume>6</volume>(<issue>252</issue>):252ra123&#x02013;252ra123.</mixed-citation></ref><ref id="R43"><label>43.</label><mixed-citation publication-type="journal"><name><surname>Jacobsen</surname><given-names>JO</given-names></name>, <name><surname>Kelly</surname><given-names>C</given-names></name>, <name><surname>Cipriani</surname><given-names>V</given-names></name>, <etal/>
<article-title>Phenotype-driven approaches to enhance variant prioritization and diagnosis of rare disease</article-title>. <source>Human mutation</source>. <year>2022</year>;<volume>43</volume>(<issue>8</issue>):<fpage>1071</fpage>&#x02013;<lpage>1081</lpage>.<pub-id pub-id-type="pmid">35391505</pub-id>
</mixed-citation></ref><ref id="R44"><label>44.</label><mixed-citation publication-type="journal"><name><surname>Trakadis</surname><given-names>YJ</given-names></name>, <name><surname>Buote</surname><given-names>C</given-names></name>, <name><surname>Therriault</surname><given-names>J-F</given-names></name>, <name><surname>Jacques</surname><given-names>P-&#x000c9;</given-names></name>, <name><surname>Larochelle</surname><given-names>H</given-names></name>, <name><surname>L&#x000e9;vesque</surname><given-names>S</given-names></name>. <article-title>PhenoVar: a phenotype-driven approach in clinical genomics for the diagnosis of polymalformative syndromes</article-title>. <source>BMC medical genomics</source>. <year>2014</year>;<volume>7</volume>:<fpage>1</fpage>&#x02013;<lpage>13</lpage>.<pub-id pub-id-type="pmid">24397966</pub-id>
</mixed-citation></ref><ref id="R45"><label>45.</label><mixed-citation publication-type="journal"><name><surname>Smedley</surname><given-names>D</given-names></name>, <name><surname>Robinson</surname><given-names>PN</given-names></name>. <article-title>Phenotype-driven strategies for exome prioritization of human Mendelian disease genes</article-title>. <source>Genome medicine</source>. <year>2015</year>;<volume>7</volume>:<fpage>1</fpage>&#x02013;<lpage>11</lpage>.<pub-id pub-id-type="pmid">25606059</pub-id>
</mixed-citation></ref><ref id="R46"><label>46.</label><mixed-citation publication-type="journal"><name><surname>Ferreira</surname><given-names>CR</given-names></name>. <article-title>The burden of rare diseases. American journal of medical genetics</article-title>
<source>Part A</source>. <year>2019</year>;<volume>179</volume>(<issue>6</issue>):<fpage>885</fpage>&#x02013;<lpage>892</lpage>.</mixed-citation></ref><ref id="R47"><label>47.</label><mixed-citation publication-type="journal"><name><surname>Bauskis</surname><given-names>A</given-names></name>, <name><surname>Strange</surname><given-names>C</given-names></name>, <name><surname>Molster</surname><given-names>C</given-names></name>, <name><surname>Fisher</surname><given-names>C</given-names></name>. <article-title>The diagnostic odyssey: insights from parents of children living with an undiagnosed condition</article-title>. <source>Orphanet journal of rare diseases</source>. <year>2022</year>;<volume>17</volume>(<issue>1</issue>):<fpage>233</fpage>.<pub-id pub-id-type="pmid">35717227</pub-id>
</mixed-citation></ref><ref id="R48"><label>48.</label><mixed-citation publication-type="other"><name><surname>Hustinx</surname><given-names>A</given-names></name>, <name><surname>Hellmann</surname><given-names>F</given-names></name>, <name><surname>S&#x000fc;mer</surname><given-names>&#x000d6;</given-names></name>, <etal/>
<source>Improving Deep Facial Phenotyping for Ultra-rare Disorder Verification Using Model Ensembles</source>. <year>2023</year>:<fpage>5018</fpage>&#x02013;<lpage>5028</lpage>.</mixed-citation></ref><ref id="R49"><label>49.</label><mixed-citation publication-type="other"><name><surname>Lesmann</surname><given-names>H</given-names></name>, <name><surname>Lyon</surname><given-names>GJ</given-names></name>, <name><surname>Caro</surname><given-names>P</given-names></name>, <etal/>
<article-title>GestaltMatcher Database-a FAIR database for medical imaging data of rare disorders</article-title>. <source>MedRxiv</source>. <year>2023</year>;</mixed-citation></ref><ref id="R50"><label>50.</label><mixed-citation publication-type="other"><name><surname>Kim</surname><given-names>W</given-names></name>, <name><surname>Son</surname><given-names>B</given-names></name>, <name><surname>Kim</surname><given-names>I</given-names></name>. <article-title>Vilt: Vision-and-language transformer without convolution or region supervision</article-title>. <source>PMLR</source>; <year>2021</year>:<fpage>5583</fpage>&#x02013;<lpage>5594</lpage>.</mixed-citation></ref><ref id="R51"><label>51.</label><mixed-citation publication-type="journal"><name><surname>Hsieh</surname><given-names>T-C</given-names></name>, <name><surname>Bar-Haim</surname><given-names>A</given-names></name>, <name><surname>Moosa</surname><given-names>S</given-names></name>, <etal/>
<article-title>GestaltMatcher facilitates rare disease matching using facial phenotype descriptors</article-title>. <source>Nature genetics</source>. <year>2022</year>;<volume>54</volume>(<issue>3</issue>):<fpage>349</fpage>&#x02013;<lpage>357</lpage>.<pub-id pub-id-type="pmid">35145301</pub-id>
</mixed-citation></ref><ref id="R52"><label>52.</label><mixed-citation publication-type="other"><name><surname>Radford</surname><given-names>A</given-names></name>, <name><surname>Kim</surname><given-names>JW</given-names></name>, <name><surname>Hallacy</surname><given-names>C</given-names></name>, <etal/>
<article-title>Learning transferable visual models from natural language supervision</article-title>. <source>PmLR</source>; <year>2021</year>:<fpage>8748</fpage>&#x02013;<lpage>8763</lpage>.</mixed-citation></ref><ref id="R53"><label>53.</label><mixed-citation publication-type="other"><name><surname>Rafailov</surname><given-names>R</given-names></name>, <name><surname>Sharma</surname><given-names>A</given-names></name>, <name><surname>Mitchell</surname><given-names>E</given-names></name>, <name><surname>Manning</surname><given-names>CD</given-names></name>, <name><surname>Ermon</surname><given-names>S</given-names></name>, <name><surname>Finn</surname><given-names>C</given-names></name>. <article-title>Direct preference optimization: Your language model is secretly a reward model</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2024</year>;<fpage>36</fpage></mixed-citation></ref><ref id="R54"><label>54.</label><mixed-citation publication-type="other"><name><surname>Hendrycks</surname><given-names>D</given-names></name>, <name><surname>Burns</surname><given-names>C</given-names></name>, <name><surname>Basart</surname><given-names>S</given-names></name>, <etal/>
<article-title>Measuring massive multitask language understanding</article-title>. <source>arXiv preprint arXiv:200903300</source>. <year>2020</year>;</mixed-citation></ref><ref id="R55"><label>55.</label><mixed-citation publication-type="other"><name><surname>Lin</surname><given-names>S</given-names></name>, <name><surname>Hilton</surname><given-names>J</given-names></name>, <name><surname>Evans</surname><given-names>O</given-names></name>. <article-title>Truthfulqa: Measuring how models mimic human falsehoods</article-title>. <source>arXiv preprint arXiv:210907958</source>. <year>2021</year>;</mixed-citation></ref><ref id="R56"><label>56.</label><mixed-citation publication-type="other"><name><surname>Zellers</surname><given-names>R</given-names></name>, <name><surname>Holtzman</surname><given-names>A</given-names></name>, <name><surname>Bisk</surname><given-names>Y</given-names></name>, <name><surname>Farhadi</surname><given-names>A</given-names></name>, <name><surname>Choi</surname><given-names>Y</given-names></name>. <article-title>Hellaswag: Can a machine really finish your sentence?</article-title>
<source>arXiv preprint arXiv:190507830</source>. <year>2019</year>;</mixed-citation></ref><ref id="R57"><label>57.</label><mixed-citation publication-type="other"><name><surname>Chollet</surname><given-names>F</given-names></name>. <article-title>On the measure of intelligence</article-title>. <source>arXiv preprint arXiv:191101547</source>. <year>2019</year>;</mixed-citation></ref><ref id="R58"><label>58.</label><mixed-citation publication-type="other"><name><surname>Cobbe</surname><given-names>K</given-names></name>, <name><surname>Kosaraju</surname><given-names>V</given-names></name>, <name><surname>Bavarian</surname><given-names>M</given-names></name>, <etal/>
<article-title>Training verifiers to solve math word problems</article-title>. <source>arXiv preprint arXiv:211014168</source>. <year>2021</year>;</mixed-citation></ref><ref id="R59"><label>59.</label><mixed-citation publication-type="other"><name><surname>Wu</surname><given-names>D</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Nguyen</surname><given-names>Q</given-names></name>, <name><surname>Wang</surname><given-names>K</given-names></name>. <article-title>Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances Rare Disease Diagnosis from Clinical Notes</article-title>. <source>arXiv preprint arXiv:250312286</source>. <year>2025</year>;</mixed-citation></ref><ref id="R60"><label>60.</label><mixed-citation publication-type="other"><name><surname>Li</surname><given-names>B</given-names></name>, <name><surname>Wang</surname><given-names>R</given-names></name>, <name><surname>Wang</surname><given-names>G</given-names></name>, <name><surname>Ge</surname><given-names>Y</given-names></name>, <name><surname>Ge</surname><given-names>Y</given-names></name>, <name><surname>Shan</surname><given-names>Y</given-names></name>. <article-title>Seed-bench: Benchmarking multimodal llms with generative comprehension</article-title>. <source>arXiv preprint arXiv:230716125</source>. <year>2023</year>;</mixed-citation></ref><ref id="R61"><label>61.</label><mixed-citation publication-type="journal"><name><surname>Zonios</surname><given-names>GI</given-names></name>, <name><surname>Cothren</surname><given-names>RM</given-names></name>, <name><surname>Arendt</surname><given-names>JT</given-names></name>, <etal/>
<article-title>Morphological model of human colon tissue fluorescence</article-title>. <source>IEEE Transactions on biomedical engineering</source>. <year>1996</year>;<volume>43</volume>(<issue>2</issue>):<fpage>113</fpage>&#x02013;<lpage>122</lpage>.<pub-id pub-id-type="pmid">8682522</pub-id>
</mixed-citation></ref><ref id="R62"><label>62.</label><mixed-citation publication-type="journal"><name><surname>Akwari</surname><given-names>OE</given-names></name>, <name><surname>Van Heerden</surname><given-names>JA</given-names></name>, <name><surname>Foulk</surname><given-names>WT</given-names></name>, <name><surname>Baggenstoss</surname><given-names>AH</given-names></name>. <article-title>Cancer of the bile ducts associated with ulcerative colitis</article-title>. <source>Annals of Surgery</source>. <year>1975</year>;<volume>181</volume>(<issue>3</issue>):<fpage>303</fpage>.<pub-id pub-id-type="pmid">165791</pub-id>
</mixed-citation></ref><ref id="R63"><label>63.</label><mixed-citation publication-type="journal"><name><surname>Mir-Madjlessi</surname><given-names>SH</given-names></name>, <name><surname>Farmer</surname><given-names>RG</given-names></name>, <name><surname>Sivak</surname><given-names>MV</given-names></name>. <article-title>Bile duct carcinoma in patients with ulcerative colitis: Relationship to sclerosing cholangitis: Report of six cases and review of the literature</article-title>. <source>Digestive diseases and sciences</source>. <year>1987</year>;<volume>32</volume>:<fpage>145</fpage>&#x02013;<lpage>154</lpage>.<pub-id pub-id-type="pmid">3542446</pub-id>
</mixed-citation></ref><ref id="R64"><label>64.</label><mixed-citation publication-type="journal"><name><surname>Nakanuma</surname><given-names>Y</given-names></name>, <name><surname>Tsuneyama</surname><given-names>K</given-names></name>, <name><surname>Harada</surname><given-names>K</given-names></name>. <article-title>Pathology and pathogenesis of intrahepatic bile duct loss</article-title>. <source>Journal of hepato-biliary-pancreatic surgery</source>. <year>2001</year>;<volume>8</volume>:<fpage>303</fpage>&#x02013;<lpage>315</lpage>.<pub-id pub-id-type="pmid">11521175</pub-id>
</mixed-citation></ref><ref id="R65"><label>65.</label><mixed-citation publication-type="journal"><name><surname>Hinck</surname><given-names>L</given-names></name>, <name><surname>N&#x000e4;thke</surname><given-names>I</given-names></name>. <article-title>Changes in cell and tissue organization in cancer of the breast and colon</article-title>. <source>Current opinion in cell biology</source>. <year>2014</year>;<volume>26</volume>:<fpage>87</fpage>&#x02013;<lpage>95</lpage>.<pub-id pub-id-type="pmid">24529250</pub-id>
</mixed-citation></ref><ref id="R66"><label>66.</label><mixed-citation publication-type="journal"><name><surname>Schomacker</surname><given-names>KT</given-names></name>, <name><surname>Frisoli</surname><given-names>JK</given-names></name>, <name><surname>Compton</surname><given-names>CC</given-names></name>, <etal/>
<source>Ultraviolet laser-induced fluorescence of colonic tissue: basic biology and diagnostic potential. Lasers in surgery and medicine</source>. <year>1992</year>;<volume>12</volume>(<issue>1</issue>):<fpage>63</fpage>&#x02013;<lpage>78</lpage>.<pub-id pub-id-type="pmid">1614265</pub-id>
</mixed-citation></ref><ref id="R67"><label>67.</label><mixed-citation publication-type="journal"><name><surname>Yasuda</surname><given-names>T</given-names></name>, <name><surname>Shiozaki</surname><given-names>H</given-names></name>. <article-title>Esophageal reconstruction with colon tissue</article-title>. <source>Surgery today</source>. <year>2011</year>;<volume>41</volume>:<fpage>745</fpage>&#x02013;<lpage>753</lpage>.<pub-id pub-id-type="pmid">21626317</pub-id>
</mixed-citation></ref><ref id="R68"><label>68.</label><mixed-citation publication-type="journal"><name><surname>Das</surname><given-names>KM</given-names></name>, <name><surname>Vecchi</surname><given-names>M</given-names></name>, <name><surname>Sakamaki</surname><given-names>S</given-names></name>. <article-title>A shared and unique epitope (s) on human colon, skin, and biliary epithelium detected by a monoclonal antibody</article-title>. <source>Gastroenterology</source>. <year>1990</year>;<volume>98</volume>(<issue>2</issue>):<fpage>464</fpage>&#x02013;<lpage>469</lpage>.<pub-id pub-id-type="pmid">1688539</pub-id>
</mixed-citation></ref><ref id="R69"><label>69.</label><mixed-citation publication-type="journal"><name><surname>Sakaguchi</surname><given-names>K</given-names></name>, <name><surname>Bras</surname><given-names>RL</given-names></name>, <name><surname>Bhagavatula</surname><given-names>C</given-names></name>, <name><surname>Choi</surname><given-names>Y</given-names></name>. <article-title>Winogrande: An adversarial winograd schema challenge at scale</article-title>. <source>Communications of the ACM</source>. <year>2021</year>;<volume>64</volume>(<issue>9</issue>):<fpage>99</fpage>&#x02013;<lpage>106</lpage>.</mixed-citation></ref><ref id="R70"><label>70.</label><mixed-citation publication-type="confproc"><name><surname>Sree Ram Kiran Nag</surname><given-names>M</given-names></name>, <name><surname>Srinivas</surname><given-names>G</given-names></name>, <name><surname>Venkata Rao</surname><given-names>K</given-names></name>, <name><surname>Vakkalanka</surname><given-names>S</given-names></name>, <name><surname>Nagendram</surname><given-names>S</given-names></name>. <source>An efficient procedure for identifying the similarity between French and English languages with sequence matcher technique</source>. <conf-name>Advances in Data Science and Management: Proceedings of ICDSM 2021</conf-name>. <publisher-name>Springer</publisher-name>; <year>2022</year>:<fpage>29</fpage>&#x02013;<lpage>40</lpage>.</mixed-citation></ref><ref id="R71"><label>71.</label><mixed-citation publication-type="other"><name><surname>Hu</surname><given-names>EJ</given-names></name>, <name><surname>Shen</surname><given-names>Y</given-names></name>, <name><surname>Wallis</surname><given-names>P</given-names></name>, <etal/>
<article-title>Lora: Low-rank adaptation of large language models</article-title>. <source>arXiv preprint arXiv:210609685</source>. <year>2021</year>;</mixed-citation></ref><ref id="R72"><label>72.</label><mixed-citation publication-type="webpage"><name><surname>Liu</surname><given-names>J.</given-names></name>
<source>LlamaIndex</source>. <year>2024</year>. <ext-link xlink:href="https://github.com/jerryjliu/llama_index" ext-link-type="uri">https://github.com/jerryjliu/llama_index</ext-link></mixed-citation></ref><ref id="R73"><label>73.</label><mixed-citation publication-type="journal"><name><surname>Zeeberg</surname><given-names>BR</given-names></name>, <name><surname>Qin</surname><given-names>H</given-names></name>, <name><surname>Narasimhan</surname><given-names>S</given-names></name>, <etal/>
<article-title>High-Throughput GoMiner, an &#x02018;industrial-strength&#x02019; integrative gene ontology tool for interpretation of multiple-microarray experiments, with application to studies of Common Variable Immune Deficiency (CVID)</article-title>. <source>BMC Bioinformatics</source>. <year>2005</year>;<volume>6</volume>:<fpage>168</fpage>.<pub-id pub-id-type="pmid">15998470</pub-id>
</mixed-citation></ref><ref id="R74"><label>74.</label><mixed-citation publication-type="journal"><name><surname>Zhong</surname><given-names>S</given-names></name>, <name><surname>Storch</surname><given-names>KF</given-names></name>, <name><surname>Lipan</surname><given-names>O</given-names></name>, <name><surname>Kao</surname><given-names>MC</given-names></name>, <name><surname>Weitz</surname><given-names>CJ</given-names></name>, <name><surname>Wong</surname><given-names>WH</given-names></name>. <article-title>GoSurfer: a graphical interactive tool for comparative analysis of large gene sets in Gene Ontology space</article-title>. <source>Appl Bioinformatics</source>. <year>2004</year>;<volume>3</volume>(<issue>4</issue>):<fpage>261</fpage>&#x02013;<lpage>4</lpage>.<pub-id pub-id-type="pmid">15702958</pub-id>
</mixed-citation></ref><ref id="R75"><label>75.</label><mixed-citation publication-type="journal"><name><surname>Gu</surname><given-names>Y</given-names></name>, <name><surname>Tinn</surname><given-names>R</given-names></name>, <name><surname>Cheng</surname><given-names>H</given-names></name>, <etal/>
<article-title>Domain-specific language model pretraining for biomedical natural language processing</article-title>. <source>ACM Transactions on Computing for Healthcare (HEALTH)</source>. <year>2021</year>;<volume>3</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>23</lpage>.</mixed-citation></ref><ref id="R76"><label>76.</label><mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>C</given-names></name>, <name><surname>Velu</surname><given-names>A</given-names></name>, <name><surname>Vinitsky</surname><given-names>E</given-names></name>, <etal/>
<article-title>The surprising effectiveness of ppo in cooperative multi-agent games</article-title>. <source>Advances in neural information processing systems</source>. <year>2022</year>;<volume>35</volume>:<fpage>24611</fpage>&#x02013;<lpage>24624</lpage>.</mixed-citation></ref><ref id="R77"><label>77.</label><mixed-citation publication-type="other"><name><surname>Engstrom</surname><given-names>L</given-names></name>, <name><surname>Ilyas</surname><given-names>A</given-names></name>, <name><surname>Santurkar</surname><given-names>S</given-names></name>, <etal/>
<article-title>Implementation matters in deep policy gradients: A case study on ppo and trpo</article-title>. <source>arXiv preprint arXiv:200512729</source>. <year>2020</year>;</mixed-citation></ref><ref id="R78"><label>78.</label><mixed-citation publication-type="other"><name><surname>Engstrom</surname><given-names>L</given-names></name>, <name><surname>Ilyas</surname><given-names>A</given-names></name>, <name><surname>Santurkar</surname><given-names>S</given-names></name>, <etal/>
<source>Implementation matters in deep rl: A case study on ppo and trpo</source>. <year>2019</year>:</mixed-citation></ref></ref-list></back><floats-group><fig position="float" id="F1"><label>Figure 1.</label><caption><title>Overview of the MINT framework for transferring multimodal knowledge to Large Language Models.</title><p id="P72">The framework consists of several pipelines: <bold>(1) Upstream Pipeline</bold>: A multimodal classifier integrates test and image modality input data to generate top-k most-likely and bottom-q least-likely predictions, which are organized into chosen (preferred) and rejected (non-preferred) responses in natural language to form a preference dataset. <bold>(2) Downstream Pipeline-SFT</bold>: Standard supervised fine-tuning of base language or vision-language. <bold>(3) Downstream Pipeline-MINT with DPO</bold>: Direct Preference Optimization approach that uses a frozen reference model (initialized from SFT) and a trainable policy model, optimizing with KL-divergence and maximum likelihood objectives. <bold>(4) Downstream Pipeline-MINT with ORPO (default)</bold>: Our proposed unified framework combining negative log likelihood and odds ratio loss in a single step, directly optimizing the relative probabilities between chosen and rejected responses without requiring a separate reference model.</p></caption><graphic xlink:href="nihpp-2505.05736v1-f0001" position="float"/></fig><fig position="float" id="F2"><label>Figure 2.</label><caption><title>Performance evaluation of rare disease prediction techniques using Llama 3.2&#x02013;3B-Instruct model.</title><p id="P73"><bold>(a)</bold> Comparison of model performance across four evaluation metrics: Hallucination-Free Accuracy (HFA), Top-10 accuracy, Top-1 accuracy, and Coverage-Avoidance Ratio (CAR) for five approaches: Base Model, RAG, SFT, MINT with DPO, and MINT with ORPO (color from light to dark respectively).<bold>(b)</bold> Effect of varying Acceptance-over-Rejection (AoR) ratios on MINT performance, showing optimal performance at balanced ratio (AoR=1). <bold>(c)</bold> Radar chart comparing performance on six language understanding benchmarks, demonstrating preserved general capabilities across all fine-tuning techniques.</p></caption><graphic xlink:href="nihpp-2505.05736v1-f0002" position="float"/></fig><fig position="float" id="F3"><label>Figure 3.</label><caption><title>Performance on tissue type classification using different fine-tuning techniques on the Llama 3.2-Vision-11B-Instruct foundation model.</title><p id="P74"><bold>(a)</bold> Bar Chart showing performance metrics across four evaluation criteria: Hallucination Free Accuracy (%), Top-5 Accuracy (%), Top-1 Accuracy (%), and CAR (Coverage-Avoidance Ratio). Four fine-tuning approaches are compared: Base Model, SFT, MINT with DPO, MINT with ORPO (color from light to dark respectively). <bold>(b)</bold> Radar chart comparing performance across multiple general vision-language capabilities for different fine-tuning techniques.</p></caption><graphic xlink:href="nihpp-2505.05736v1-f0003" position="float"/></fig><fig position="float" id="F4"><label>Figure 4.</label><caption><title>Comparative analysis of tissue type classification performance between Base model, SFT, and MINT for similar-looking bile duct and colon tissues.</title><p id="P75">The figure demonstrates how MINT improves discrimination between histologically similar tissues by leveraging both positive and negative training examples. Top panel shows bile duct tissue classification: a representative training sample with corresponding chosen and rejected tissue types (left), and four testing samples with their respective ranks assigned by Base, SFT, and MINT (Right). Bottom panel shows the same analysis for colon tissue classification. Green values represent ranking for the ground truth tissue class, while red values indication rankings for the visually similar confused class. Lower values represent higher confidence (rank 1 is the highest). Average ranks across all test samples are shown in both panels. Average ranks across all test samples are shown at the bottom of each panel. <bold>&#x02018;MINT&#x02019; refers to our default implementation of the MINT framework using ORPO.</bold></p></caption><graphic xlink:href="nihpp-2505.05736v1-f0004" position="float"/></fig><table-wrap position="float" id="T1"><label>Table 1.</label><caption><p id="P76">Summary of datasets, upstream multimodal ML model and downstream LLM for the two biomedical tasks evaluated in this study.</p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th colspan="3" align="center" valign="top" rowspan="1">Detailed Components of MINT Framework</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Task</td><td align="left" valign="top" rowspan="1" colspan="1">Rare Disease Prediction</td><td align="left" valign="top" rowspan="1" colspan="1">Tissue Type Classification</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Dataset</td><td align="left" valign="top" rowspan="1" colspan="1">GestaltMatcher Database</td><td align="left" valign="top" rowspan="1" colspan="1">PanNuke Database</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Images</td><td align="left" valign="top" rowspan="1" colspan="1">Frontal Facial Image</td><td align="left" valign="top" rowspan="1" colspan="1">Nucleus Image</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Texts</td><td align="left" valign="top" rowspan="1" colspan="1">Age, Sex, Ethnicity, HPO terms</td><td align="left" valign="top" rowspan="1" colspan="1">Tissue type</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Upstream Multimodal ML Model</td><td align="left" valign="top" rowspan="1" colspan="1">GestaltMML</td><td align="left" valign="top" rowspan="1" colspan="1">Pathology Language and Image Pre-Training (PLIP)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Downstream LLM</td><td align="left" valign="top" rowspan="1" colspan="1">Llama 3.2&#x02013;3B-Instruct</td><td align="left" valign="top" rowspan="1" colspan="1">Llama 3.2-Vision-11B-Instruct</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1"># of Training Sample</td><td align="left" valign="top" rowspan="1" colspan="1">6522</td><td align="left" valign="top" rowspan="1" colspan="1">5436</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1"># of Testing Sample</td><td align="left" valign="top" rowspan="1" colspan="1">386</td><td align="left" valign="top" rowspan="1" colspan="1">2330</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1"># of Labels (Categories)</td><td align="left" valign="top" rowspan="1" colspan="1">528</td><td align="left" valign="top" rowspan="1" colspan="1">19</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T2"><label>Table 2.</label><caption><title>Evaluation of Llama 3.2&#x02013;3B-Instruct variants on Phenopacket-derived clinical notes.</title><p id="P77">Results show three evaluation metrics: Hallucination Free Accuracy (%), Top-10 Accuracy, and Top-1 Accuracy for five model variants based on the Llama 3.2&#x02013;3B-Instruct architecture. Each row represents a different approach: Base Model, RAG, SFT, DPO and MINT. Results are presented for three data configurations: overlapping diseases (1638 samples with 72 diseases shared with training data), disjoint diseases (4342 samples with 456 diseases unseen in training data), and the combined dataset (5980 samples). Bold values indicate best performance for each metric in each configuration. <bold>&#x02018;MINT&#x02019; refers to our default implementation of the MINT framework using ORPO.</bold></p></caption><table frame="box" rules="all"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th colspan="4" align="center" valign="top" rowspan="1">Performance on Phenopacket-derived Clinical Notes (N=1638) (72 overlapping diseases)</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">Model (Llama 3.2&#x02013;3B-Instruct)</td><td align="center" valign="top" rowspan="1" colspan="1">HFA (%)</td><td align="center" valign="top" rowspan="1" colspan="1">Top-10 Accuracy (%)</td><td align="center" valign="top" rowspan="1" colspan="1">Top-1 Accuracy (%)</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Base</td><td align="center" valign="top" rowspan="1" colspan="1">99.94</td><td align="center" valign="top" rowspan="1" colspan="1">5.13</td><td align="center" valign="top" rowspan="1" colspan="1">3.42</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">RAG</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>100.00</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">30.11</td><td align="center" valign="top" rowspan="1" colspan="1">26.45</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">SFT</td><td align="center" valign="top" rowspan="1" colspan="1">99.94</td><td align="center" valign="top" rowspan="1" colspan="1">46.40</td><td align="center" valign="top" rowspan="1" colspan="1">14.65</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">DPO</td><td align="center" valign="top" rowspan="1" colspan="1">99.94</td><td align="center" valign="top" rowspan="1" colspan="1">60.99</td><td align="center" valign="top" rowspan="1" colspan="1">26.56</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">MINT</td><td align="center" valign="top" rowspan="1" colspan="1">99.89</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>66.91</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>47.56</bold>
</td></tr><tr><th colspan="4" align="center" valign="top" rowspan="1">Performance on Phenopacket-derived Clinical Notes (N=4342) (456 disjoint diseases)</th></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Model (Llama 3.2&#x02013;3B-Instruct)</td><td align="center" valign="top" rowspan="1" colspan="1">HFA (%)</td><td align="center" valign="top" rowspan="1" colspan="1">Top-10 Accuracy (%)</td><td align="center" valign="top" rowspan="1" colspan="1">Top-1 Accuracy (%)</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Base</td><td align="center" valign="top" rowspan="1" colspan="1">99.99</td><td align="center" valign="top" rowspan="1" colspan="1">20.00</td><td align="center" valign="top" rowspan="1" colspan="1">11.29</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">RAG</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>100.00</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>24.17</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>13.80</bold>
</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">SFT</td><td align="center" valign="top" rowspan="1" colspan="1">100.00</td><td align="center" valign="top" rowspan="1" colspan="1">9.16</td><td align="center" valign="top" rowspan="1" colspan="1">8.46</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">DPO</td><td align="center" valign="top" rowspan="1" colspan="1">99.99</td><td align="center" valign="top" rowspan="1" colspan="1">9.48</td><td align="center" valign="top" rowspan="1" colspan="1">7.71</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">MINT</td><td align="center" valign="top" rowspan="1" colspan="1">99.90</td><td align="center" valign="top" rowspan="1" colspan="1">10.48</td><td align="center" valign="top" rowspan="1" colspan="1">7.00</td></tr><tr><th colspan="4" align="center" valign="top" rowspan="1">Performance on Phenopacket-derived Clinical Notes (N=5980)</th></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Model (Llama 3.2&#x02013;3B-Instruct)</td><td align="center" valign="top" rowspan="1" colspan="1">HFA (%)</td><td align="center" valign="top" rowspan="1" colspan="1">Top-10 Accuracy (%)</td><td align="center" valign="top" rowspan="1" colspan="1">Top-1 Accuracy (%)</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">Base</td><td align="center" valign="top" rowspan="1" colspan="1">99.98</td><td align="center" valign="top" rowspan="1" colspan="1">15.58</td><td align="center" valign="top" rowspan="1" colspan="1">8.98</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">RAG</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>100.00</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">25.96</td><td align="center" valign="top" rowspan="1" colspan="1">17.62</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">SFT</td><td align="center" valign="top" rowspan="1" colspan="1">99.98</td><td align="center" valign="top" rowspan="1" colspan="1">20.25</td><td align="center" valign="top" rowspan="1" colspan="1">10.18</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">DPO</td><td align="center" valign="top" rowspan="1" colspan="1">99.98</td><td align="center" valign="top" rowspan="1" colspan="1">24.24</td><td align="center" valign="top" rowspan="1" colspan="1">13.47</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">MINT</td><td align="center" valign="top" rowspan="1" colspan="1">99.90</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>27.05</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>20.23</bold>
</td></tr></tbody></table></table-wrap></floats-group></article>