<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Med (Lausanne)</journal-id><journal-id journal-id-type="iso-abbrev">Front Med (Lausanne)</journal-id><journal-id journal-id-type="publisher-id">Front. Med.</journal-id><journal-title-group><journal-title>Frontiers in Medicine</journal-title></journal-title-group><issn pub-type="epub">2296-858X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40400634</article-id><article-id pub-id-type="pmc">PMC12092233</article-id><article-id pub-id-type="doi">10.3389/fmed.2025.1555380</article-id><article-categories><subj-group subj-group-type="heading"><subject>Medicine</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Multimodal AI diagnostic system for neuromyelitis optica based on ultrawide-field fundus photography</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Gu</surname><given-names>Simin</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="fn0001" ref-type="author-notes">
<sup>&#x02020;</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/2938175/overview"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Bao</surname><given-names>Tiancheng</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="fn0001" ref-type="author-notes">
<sup>&#x02020;</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Tao</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/1512362/overview"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Yuan</surname><given-names>Qiting</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Weiguang</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Lin</surname><given-names>Jiayi</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/2632832/overview"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Zhu</surname><given-names>Haocheng</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/3023358/overview"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Cui</surname><given-names>Shihai</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Yi</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/2210705/overview"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Jia</surname><given-names>Xiuhua</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Huang</surname><given-names>Lina</given-names></name><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Ling</surname><given-names>Shiqi</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/785499/overview"/><role content-type="https://credit.niso.org/contributor-roles/project-administration/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Department of Ophthalmology, The Third Affiliated Hospital, Sun Yat-Sen University</institution>, <addr-line>Guangzhou</addr-line>, <country>China</country></aff><aff id="aff2"><sup>2</sup><institution>Zeemo Technology Company Limited</institution>, <addr-line>Shenzhen</addr-line>, <country>China</country></aff><aff id="aff3"><sup>3</sup><institution>Shenzhen Eye Hospital, Jinan University</institution>, <addr-line>Shenzhen</addr-line>, <country>China</country></aff><author-notes><fn fn-type="edited-by" id="fn0002"><p>Edited by: Haoyu Chen, The Chinese University of Hong Kong, Hong Kong SAR, China</p></fn><fn fn-type="edited-by" id="fn0003"><p>Reviewed by: Gilbert Yong San Lim, SingHealth, Singapore</p><p>Ling-Ping Cen, Guangdong Medical University, China</p><p>Md Mahmudul Hasan, University of New South Wales, Australia</p></fn><corresp id="c001">*Correspondence: Lina Huang, <email>lina_h@126.com</email>; Shiqi Ling, <email>lingshiqi123@163.com</email></corresp><fn fn-type="equal" id="fn0001"><p><sup>&#x02020;</sup>These authors have contributed equally to this work and share first authorship</p></fn></author-notes><pub-date pub-type="epub"><day>07</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>12</volume><elocation-id>1555380</elocation-id><history><date date-type="received"><day>04</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>14</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025 Gu, Bao, Wang, Yuan, Yu, Lin, Zhu, Cui, Sun, Jia, Huang and Ling.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Gu, Bao, Wang, Yuan, Yu, Lin, Zhu, Cui, Sun, Jia, Huang and Ling</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><sec id="sec1"><title>Purpose</title><p>While deep learning (DL) has demonstrated significant utility in ocular diseases, no clinically validated algorithm currently exists for diagnosing neuromyelitis optica (NMO). This study aimed to develop a proof-of-concept multimodal artificial intelligence (AI) diagnostic model that synergistically integrates ultrawide field fundus photographs (UWFs) with clinical examination data for predicting the onset and stage of suspected NMO.</p></sec><sec id="sec2"><title>Methods</title><p>The study utilized the UWFs of 330 eyes from 285 NMO patients and 1,288 eyes from 770 non-NMO participants, along with clinical examination reports, to develop an AI model for predicting the onset or stage of suspected NMO. The performance of the AI model was evaluated based on the area under the receiver operating characteristic curve (AUC), sensitivity, and specificity.</p></sec><sec id="sec3"><title>Results</title><p>The multimodal AI diagnostic model achieved an AUC of 0.9923, a maximum Youden index of 0.9389, a sensitivity of 97.0% and a specificity of 96.9% in predicting the prevalence of NMO on test data set.</p></sec><sec id="sec4"><title>Conclusion</title><p>Our study demonstrates the feasibility of DL algorithms in diagnosing and predicting of NMO.</p></sec></abstract><kwd-group><kwd>neuromyelitis optic</kwd><kwd>neuromyelitis optica spectrum disorders</kwd><kwd>deep learning</kwd><kwd>artificial intelligence</kwd><kwd>diagnostic model</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that financial support was received for the research and/or publication of this article. This research was funded by Guangzhou Huangpu International Science and Technology Collaboration Project, grant number &#x0201c;2021GH08,&#x0201d; Guangzhou Science and Technology Planning Project, Grant number &#x0201c;2024A03J0278.&#x0201d;</funding-statement></funding-group><counts><fig-count count="8"/><table-count count="4"/><equation-count count="1"/><ref-count count="28"/><page-count count="11"/><word-count count="6375"/></counts><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Ophthalmology</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec5"><label>1</label><title>Introduction</title><p>Neuromyelitis optica (NMO), also known as Devic syndrome, is an idiopathic neuroinflammatory disorder. This rare relapsing clinical syndrome characterized by astrocytopathy of the central nervous system, with a predilection for the optic nerves and spinal cord (<xref rid="ref1" ref-type="bibr">1</xref>), disease occurs globally and affects individuals of all ethnicities (<xref rid="ref2" ref-type="bibr">2</xref>). Its primarily manifestations include acute optic neuritis (ON) (<xref rid="ref3" ref-type="bibr">3</xref>) and transverse myelitis (TM) (<xref rid="ref4" ref-type="bibr">4</xref>), both of which cause blindness and paralysis. NMO has a poor prognosis and has long been considered a clinical variant of multiple sclerosis (MS). However, the disease is now studied as a prototypic autoimmune disorder on the basis of the discovery of a novel and pathogenic anti-astrocytic serum autoantibody that targets aquaporin-4 (AQP4-Ab) and IgG autoantibodies to myelin oligodendrocyte glycoprotein (MOG-IgG). Pathogenetic AQP4-IgG is responsible for more than 80% of NMO cases (<xref rid="ref5" ref-type="bibr">5</xref>), while approximately 10&#x02013;40% of individuals with NMO lack AQP4-IgG and instead exhibit pathogenetic MOG-IgG (<xref rid="ref6" ref-type="bibr">6</xref>). Although the AQP4 antibody is highly specific for this disorder, some patients harboring this antibody may present with isolated ON or TM. Magnetic resonance imaging (MRI) is commonly used to identify and characterize lesions in suspected NMO cases, helping to distinguish between NMO and MS. To encompass the broader clinical spectrum, including atypical or incomplete presentations, the term &#x02018;neuromyelitis optica spectrum disorders&#x02019; (NMOSD) was subsequently introduced to refer to NMO and its formes frustes (<xref rid="ref7" ref-type="bibr">7</xref>).</p><p>A subset of patients with NMO exhibit a variety of different symptoms indicating brain or brainstem involvement; however, ON and TM are the predominant manifestations of the disease. Ocular complications frequently include reduced visual acuity, a decline in high-contrast visual acuity, color desaturation (<xref rid="ref8" ref-type="bibr">8</xref>), scotoma, and ocular pain during eye movement (<xref rid="ref9" ref-type="bibr">9</xref>). These visual disturbances frequently serve as critical diagnostic indicators of NMO. If left untreated, NMO can lead to severe and irreversible visual impairment as well as significant motor dysfunction owing to incomplete recovery from acute attacks. Hence, advancements in early and accurate diagnosis are crucial, as they would facilitate prompt therapeutic intervention and significantly improve long-term clinical outcomes for patients with NMO.</p><p>Currently, the clinical diagnosis of NMO is conducted by qualified ophthalmologists or neurologists based on clinical presentation, disease course, and the detection of specific autoantibodies, supplemented by MRI scanning (<xref rid="ref10" ref-type="bibr">10</xref>). Notably, the confirmation process requires specialized clinical expertise, posing a difficult challenge in resource-limited settings such as developing countries or rural communities where access to trained ophthalmologists or neurologists is scarce. Taken together, existing approaches for detecting NMO have substantial limitations in terms of accessibility and widespread implementation. Given the increasing global health burden associated with this disease, there is an urgent need to develop innovative diagnostic strategies that can overcome these constraints, enhance early detection, and ultimately improve patient outcomes.</p><p>Advancements in artificial intelligence (AI), particularly in machine learning (ML) and deep learning (DL), has driven remarkable breakthroughs in different areas of research (<xref rid="ref11" ref-type="bibr">11&#x02013;13</xref>). In ophthalmology, the widespread availability of fundus digital imaging has sparked growing interest in leveraging AI for early diagnosis and treatment in retinal and optic nerve disorders (<xref rid="ref14" ref-type="bibr">14</xref>). Among these imaging modalities, ultrawide field fundus photographs (UWFs) have gained prominence in medical applications, particularly for ocular disease detection. UWFs offers various merits, including their suitability for nondilated pupils, the speed of acquisition, and the wide retinal imaging ranges (up to 200&#x000b0;) (<xref rid="ref15" ref-type="bibr">15</xref>). Consequently, a variety of studies have explored the integration of AI with UWFs for diagnosing and treating of various ocular conditions such as diabetic retinopathy (<xref rid="ref16" ref-type="bibr">16</xref>), age-related macular degeneration (<xref rid="ref17" ref-type="bibr">17</xref>), cataracts (<xref rid="ref18" ref-type="bibr">18</xref>), glaucoma (<xref rid="ref19" ref-type="bibr">19</xref>, <xref rid="ref20" ref-type="bibr">20</xref>) and myopia (<xref rid="ref21" ref-type="bibr">21</xref>). DL methodologies based on UWFs have become instrumental in reshaping diagnostic strategies in ophthalmology. Notably, DL methodologies based on UWFs have emerged as transformative tools, reshaping diagnostic paradigms in ophthalmology and paving the way for more efficient, accurate, and accessible disease detection strategies.</p><p>Standard color fundus photographs provides a 30 to 50-degree image whereas UWFs provide an encompassing view of the retina, allowing examination of not only the central retinal area but also the peripheral zones, which able to detect predominantly peripheral lesions in eyes with its wide coverage (<xref rid="ref22" ref-type="bibr">22</xref>). The analysis of ultrawide field fundus could be of value in screening, given the prognostic importance of peripheral lesions in distinguishing NMOSD from other fundus disease progression. However, despite NMOSD being a major cause of visual impairment worldwide, the application of AI in its diagnosis remains relatively unexplored. This raises an intriguing question: could NMOSD be accurately diagnosed solely through UWF imaging, without the need for multiple diagnostic modalities? Exploring this possibility could pave the way for more accessible and efficient diagnostic strategies in clinical practice.</p></sec><sec sec-type="methods" id="sec6"><label>2</label><title>Methods</title><sec id="sec7"><label>2.1</label><title>Subject characteristics</title><p>This clinical study adhered to the tenets of the Declaration of Helsinki and was approved by the research ethics committee of the Third Affiliated Hospital of Sun Yat-sen University.</p><p>The study utilized the UWFs of 330 eyes from 285 NMO patients and 1,288 eyes from 770 non-NMO participants, along with clinical examination reports were initially selected for the study. All the enrolled patients were referred to the Third Affiliated Hospital of Sun Yat-sen University from January 2022 to April 2024. The whole process was performed at the hospital, and informed consent was obtained from all study participants. The inclusion criteria included a diagnosis of NMO confirmed by a qualified neurosurgeon from the Third Affiliated Hospital of Sun Yat-sen University, based on diagnostic criteria established in previous population-based studies. Patients excluded from the possibility of NMO were classified as non-NMO, however, they could present with other ocular pathologies, such as diabetic retinopathy, glaucoma, retinal detachment, or retinal degeneration.</p></sec><sec id="sec8"><label>2.2</label><title>Multimodal dataset preparation</title><p>The dataset used in this study contains the data from two modalities: UWF fundus images, captured using scanning laser ophthalmoscopy (SLO, Optos Daytona) and clinical examination reports. To ensure the quality of the research, stringent image quality control criteria were implemented. All images were captured by extensively trained and specialized technicians, ensuring that the scanning quality of each image adheres to the standards of clarity and visual interpretability, absented from significant motion artifacts, and with precise centration on the optic nerve head or macula. Images failing to meet these criteria were excluded. During data preprocessing, all images were assessed for quality using both automated algorithms and manual inspection by experienced ophthalmologists. Images with severe artifacts that could compromise feature extraction were excluded from analysis. All UWFs analyzed in the present study were restricted to a single, highest-quality capture per patient, as determined by standardized ophthalmic imaging quality metrics (focus clarity &#x02265;70%, illumination uniformity, and absence of artifacts). Six ophthalmologists with clinical experience were recruited to evaluate the images and clinical examination reports. To ensure privacy, all images were first deidentified to remove any patient-related information. Subsequently, the images were preprocessed using a pretrained deep learning segmentation model to remove extraneous &#x0201c;image borders,&#x0201d; including the eyelids and the edges of the scanning machine, which are irrelevant to the current classification task. The segmentation model removes these image borders by generating a predicted mask of the detected border region while retaining the retina region. The output image of the segmentation model is padded using black pixels to produce a square shape with the retina area centered vertically for model input. This approach is well-justified, particularly in tasks that prioritize critical retinal regions, such as the optic disc and macula. By employing this method, interference such as eyelids, and machine edges can be minimized, data quality can be significantly improved, and overall model performance can be enhanced. This represents a widely accepted and validated practice in the field. <xref rid="fig1" ref-type="fig">Figure 1</xref> shows the original SLO image and the corresponding preprocessed &#x0201c;borderless&#x0201d; image.</p><fig position="float" id="fig1"><label>Figure 1</label><caption><p>Original SLO image (LHS) and processed image (RHS). <bold>(A1&#x02013;A3)</bold> Original SLO image; <bold>(B1&#x02013;B3)</bold> excessive &#x0201c;image border&#x0201d;removed image.</p></caption><graphic xlink:href="fmed-12-1555380-g001" position="float"/></fig><p>Three types of clinical examination reports, including those with data from MRI of the optic nerve, MRI of the spine and AQP4 antibody tests, were collected and analyzed. The model prediction probabilities and clinical data are integrated using a conditional decision approach. Specifically, the image score, obtained from the DL module, and the clinical score, derived from the clinical criteria, are weighted and input into the model to generate the final NMO prediction. The reports for the optic nerve MRI scans and the AQP4 antibody test were considered either positive or negative. Categorical variables were first converted into two dummy variables (1 for positive and 0 for negative) through manual coding, followed by one-hot encoding for model input. The record for spine MRI is an unstructured text description of the MRI result. Thus, the spine MRI results were preprocessed into dummy variables as follows: (a) the results were encoded as 1 (positive) if demyelinating disease was observed across three consecutive vertebrae; (b) the results were encoded as 0 (negative) if demyelinating disease was not observed or if it did not affect three consecutive vertebrae. Finally, we created a new dummy variable named &#x0201c;clinical result,&#x0201d; which was set to 1 if any of the dummy variables above was positive and 0 otherwise. The composite clinical outcome was coded as positive (1) if any one of the following criteria were present: AQP4-IgG seropositivity, orbital MRI demonstrating characteristic features diagnostic of NMO, or spinal cord MRI showing lesions spanning &#x02265;3 vertebral segments. Certain cases exhibiting characteristic optic nerve abnormalities, including optic disc hyperemia, edema, indistinct margins, and a partially increased cup-to-disc (C/D) ratio suggestive of optic nerve atrophy (<xref rid="fig2" ref-type="fig">Figure 2</xref>) were classified as high-risk for NMO. For all 330 included fundus images, matched clinical data were available. Cases with missing data were classified as positive if at least one available criterion was met; if all criteria were missing, the AI prediction alone determined the outcome.</p><fig position="float" id="fig2"><label>Figure 2</label><caption><p>Representative &#x0201c;positive&#x0201d; NMO UWF images. <bold>(A1,B1)</bold> The right eye of a 34-year-old female with NMO demonstrated a partially increased cup-to-disc (C/D) ratio. <bold>(A2,B2)</bold> The left eye of a 31-year-old female with NMO exhibited optic disc hyperemia, edema, and indistinct margins.</p></caption><graphic xlink:href="fmed-12-1555380-g002" position="float"/></fig><p>The image dataset comprised 1,618 images in total (330 or 20.4% NMO positive and 1,288 or 79.6% NMO negative), among which 330 were associated with clinical records. The dataset was split into training, validation and test datasets at a ratio of 80:10:10 on the basis of a stratified-group-split approach, i.e., stratification by NMO labels and grouping by subject ID.</p></sec><sec id="sec9"><label>2.3</label><title>Model design</title><p>We developed a multimodal AI diagnosis system for NMO (MAiDS-NMO), the structure of which is depicted in <xref rid="fig3" ref-type="fig">Figure 3</xref>. MAiDS-NMO takes inputs from two modalities: fundus imaging and clinical results. The fundus images are fed to a deep neural network model for NMO likelihood inference, and the clinical result is used together with the predicted NMO probability from the fundus images to make a positive or negative prediction of NMO. The deep model is Inception-V3 with Log-Sum-Exp (LSE) pooling (replacing the original adaptive average pooling method) (<xref rid="ref23" ref-type="bibr">23</xref>), enabling pixels with similar scores to have similar weights in the pooling process during training.</p><disp-formula id="E1">
<mml:math id="M1" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>_</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mfrac><mml:mo>log</mml:mo><mml:mfenced open="[" close="]"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mfenced open="(" close=")" separators=","><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mfenced></mml:munder><mml:mi>r</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi mathvariant="italic">where</mml:mi><mml:mspace width="0.25em"/><mml:mi>r</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="italic">is</mml:mi><mml:mspace width="0.25em"/><mml:mi>a</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal"/><mml:mi mathvariant="italic">hyperparameter</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mfenced open="(" close=")" separators=","><mml:mi>h</mml:mi><mml:mi>w</mml:mi></mml:mfenced><mml:mi mathvariant="italic">is</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="italic">input</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="italic">shape</mml:mi><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula><fig position="float" id="fig3"><label>Figure 3</label><caption><p>The MAiDS-NMO system structure. MAiDS-NMO takes two input modalities of fundus image and clinical result. The fundus image is then fed to a deep neural network model for NMO likelihood inference. Finally, the clinical result is used together with the predicted NMO probability (based on fundus image) to reach a positive or negative prediction of NMO.</p></caption><graphic xlink:href="fmed-12-1555380-g003" position="float"/></fig></sec><sec id="sec10"><label>2.4</label><title>Model development</title><p>We use the PyTorch implementation of the Inception-V3 model (LSE pooling) with the pretrained weight file &#x0201c;inception_v3_google,&#x0201d; which was developed by Google using the ImageNet dataset (ILSVRC-2012-CLS dataset for image classification) under the TensorFlow framework. One NVIDIA GeForce RTX 3090 Ti (24&#x0202f;GB) GPU was used for model development.</p><p>The training dataset was used to fine-tune the ImageNet pretrained Inception-V3 model, while the validation dataset was used to select the best weights (those which yielded the minimum validation loss). We adopted a three-step &#x0201c;warm-up&#x0201d; procedure during training, i.e., we froze all the layers (blocks) except for the fully connected dense layer of the model for the first several epochs of training and gradually unfroze Block 7, then Blocks 5 &#x00026; 6, and finally the remaining blocks of the model. This method effectively prevents the training process from being trapped by a local minimum before it finds a global minimum.</p><p>The diagnosis of NMO can be reliably established through serological antibody testing (AQP4-IgG/MOG-IgG) and characteristic neuroimaging findings on MRI. Our diagnostic model developed exclusively using UWFs parameters demonstrated comparable diagnostic accuracy (AUROC 0.97, specificity 0.92). After training the DL model with the dataset of 1,618 fundus photos, including 330 NMO images, our diagnostic model developed exclusively using UWFs parameters demonstrated high confidence in the results, with an area under the curve (AUC)&#x0202f;&#x0003e;&#x0202f;0.97 and a specificity &#x0003e; 92% in distinguishing NMO from non-NMO fundus photos. Notably, the model incorporating comprehensive clinical metadata showed enhanced diagnostic performance, with an area under the curve (AUC)&#x0202f;&#x0003e;&#x0202f;0.99 and a specificity &#x0003e; 96%, though comparative analysis revealed no statistically significant difference between the two models.</p><p>Five-fold cross-validation was performed to rigorously evaluate the robustness and generalizability of the model. This methodological approach entails partitioning the dataset into five equal subsets, wherein the model is iteratively trained on four subsets and validated on the remaining one. By systematically rotating the test set across all five folds, this technique facilitates a comprehensive assessment, mitigating potential biases associated with a single training-test split.</p></sec><sec id="sec11"><label>2.5</label><title>Comparative test</title><p>A panel of six ophthalmologists with different levels of experience (comprising two neuro-ophthalmology specialists, two attending ophthalmologists, and two resident ophthalmologists) were recruited to participated in a diagnostic validation study involving 200 UWFs at the Third Affiliated Hospital of Sun Yat-sen University between January 2022 to April 2024. While the fundus photos were being presented, the AI model identified whether the UWFs depicted NMO. Simultaneously, the ophthalmologists were asked to independently complete the same test as the AI model without access to the clinical examination reports. In the subsequent validation phase, both the MAiDS-NMO and human experts repeated the evaluation with full access to multimodal clinical parameters. The diagnostic performances of the AI model and ophthalmologists were recorded and subjected to comparative statistical analysis. The functional architecture and training pipeline of the AI model and the development process are represented in <xref rid="fig4" ref-type="fig">Figure 4</xref>.</p><fig position="float" id="fig4"><label>Figure 4</label><caption><p>Functional architecture and training pipeline of multimodal AI diagnosis system for NMO. <bold>(A)</bold> Study dataset comprising two modalities: ultrawide-field fundus images (UWFs) and corresponding clinical examination reports. <bold>(B)</bold> Schematic of the NMO diagnostic algorithm workflow. <bold>(C)</bold> Development of the deep learning (DL) system using annotated datasets for training and validation. <bold>(D)</bold> Comparative performance evaluation of MAiDS-NMO for NMO diagnosis against clinical standards.</p></caption><graphic xlink:href="fmed-12-1555380-g004" position="float"/></fig></sec></sec><sec sec-type="results" id="sec12"><label>3</label><title>Results</title><sec id="sec13"><label>3.1</label><title>Definitions of NMOSD</title><p>The diagnostic criteria for possible NMOSD were based on the following published international consensus diagnostic criteria: (<xref rid="ref10" ref-type="bibr">10</xref>) an AQP4-IgG-positive serostatus, the presence of at least one of the &#x0201c;core characteristics,&#x0201d; and the exclusion of alternative diagnoses. The diagnostic and exclusion criteria are detailed in <xref rid="box1" ref-type="boxed-text">Box 1</xref>, <xref rid="box2" ref-type="boxed-text">2</xref>, respectively.</p><boxed-text id="box1" position="float"><label>BOX 1</label><caption><title>2015 IPND criteria for NMOSD with AQP4-IgG.</title></caption><p>
<table-wrap position="anchor" id="tab1"><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Criterion A: Aquaporin 4 (AQP4)-IgG-positive serostatus</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Criterion B: (At least one of the following &#x02018;core characteristics&#x02019; (which may be the result of one or more clinical attacks))<break/><list list-type="order"><list-item><p>Clinical evidence for acute opticneuritis</p></list-item><list-item><p>Clinical evidence for acute myelitis</p></list-item><list-item><p>Clinical evidence for acute area postrema syndrome</p></list-item><list-item><p>Clinical evidence for acute brainstem encephalitis other than area postrema syndrome</p></list-item><list-item><p>Clinical evidence for acute diencephalitis or symptomatic narcolepsy plus MRI evidence of</p></list-item></list>
<list list-type="bullet"><list-item><p>a periependymal lesion at the level of the third ventricle, or</p></list-item><list-item><p>a lesion in the thalamus or hypothalamus</p></list-item></list>
<list list-type="order"><list-item><p>Clinical evidence for acute (tel) encephalitis plus MRI evidence of</p></list-item></list>
<list list-type="bullet"><list-item><p>an extensive periependymal lesion at the level of the lateral ventricles, or</p></list-item><list-item><p>a large/confluent deep or subcortical white matter lesion (often with gadolinium enhancement), or</p></list-item><list-item><p>a longitudinally extensive (&#x02265;1/2 of its length), diffuse, heterogeneous or oedematous corpus callosum lesion, or</p></list-item><list-item><p>a longitudinally extensive (contiguously from the internal capsule to the cerebral peduncles) corticospinal tract lesion</p></list-item></list></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Criterion C: Exclusion of alternative diagnoses</td></tr></tbody></table></table-wrap>
</p></boxed-text><boxed-text id="box2" position="float"><label>BOX 2</label><caption><title>Exclusion criteria for NMOSD based on 2015IPND criteria.</title></caption><p>
<table-wrap position="anchor" id="tab2"><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Criterion A: Clinical Exclusion Criteria</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">
<list list-type="order"><list-item><p>Isolated Syndromes Without Core Features</p></list-item></list>
</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">
<list list-type="bullet"><list-item><p>Pure brainstem symptoms (e.g., isolated vertigo)</p></list-item><list-item><p>Isolated cortical syndromes (e.g., seizures)</p></list-item><list-item><p>Chronic progressive myelopathy (&#x0003e;3&#x0202f;months progression)</p></list-item></list>
<list list-type="order"><list-item><p>Atypical Presentations</p></list-item></list>
<list list-type="bullet"><list-item><p>Peripheral nervous system involvement (e.g., radiculopathy)</p></list-item><list-item><p>Non-specific encephalopathy without MRI lesions</p></list-item></list>
</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Criterion B: Neuroimaging Exclusion Criteria: Location Exclusionary Findings<break/><list list-type="order"><list-item><p>Spinal Cord: Short-segment myelitis (&#x0003c;3 vertebral segments)</p></list-item><list-item><p>Optic Nerve: Unilateral involvement with &#x0003c;50% nerve length affected</p></list-item><list-item><p>Brain: Dawson&#x02019;s fingers, ovoid periventricular lesions (MS-like)</p></list-item><list-item><p>Orbit: Isolated anterior optic nerve involvement (MOGAD-pattern)</p></list-item></list></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Criterion C: Laboratory Exclusion Criteria<break/><list list-type="order"><list-item><p>Positive MOG-IgG (by cell-based assay, CBA) with compatible clinical/MOGAD features</p></list-item><list-item><p>CSF-Specific Oligoclonal Bands (OCBs) (strongly suggestive of MS)</p></list-item><list-item><p>Seropositivity for Alternative Diagnoses:</p></list-item></list><list list-type="bullet"><list-item><p>Infectious (HIV, syphilis, HTLV-1)</p></list-item><list-item><p>Paraneoplastic (anti-CRMP5, anti-amphiphysin)</p></list-item><list-item><p>Metabolic (vitamin B12 deficiency, copper deficiency)</p></list-item></list></td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Criterion D: Disease-Specific Exclusions<break/><list list-type="order"><list-item><p>Multiple Sclerosis (MS):</p></list-item></list><list list-type="bullet"><list-item><p>Meeting 2017 McDonald MRI criteria for dissemination in space/time</p></list-item><list-item><p>"Central vein sign&#x0201d; on high-resolution MRI (&#x0003e;40% perivenous lesions)</p></list-item></list><list list-type="order"><list-item><p>MOG Antibody-Associated Disease (MOGAD):</p></list-item></list><list list-type="bullet"><list-item><p>Bilateral optic neuritis with optic disc edema</p></list-item><list-item><p>Short-segment or conus-predominant myelitis</p></list-item></list><list list-type="order"><list-item><p>Other Mimics:</p></list-item></list><list list-type="bullet"><list-item><p>Spinal dural arteriovenous fistula (MRI: flow voids, cord edema)</p></list-item><list-item><p>Neurosarcoidosis (hilar lymphadenopathy, elevated ACE)</p></list-item></list></td></tr></tbody></table></table-wrap>
</p></boxed-text></sec><sec id="sec14"><label>3.2</label><title>Image datasets and patient clinical characteristics</title><p>We established a dataset composed of UWFs and clinical examination reports collected in Guangzhou, Third Affiliated Hospital of Sun Yat-sen University, from January 2022 to April 2024. The demographic and clinical information of the study participants is summarized in <xref rid="tab3" ref-type="table">Table 1</xref>.</p><table-wrap position="float" id="tab3"><label>Table 1</label><caption><p>Baseline characteristics of the study participants in the data sets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2" colspan="1">Group</th><th align="center" valign="top" colspan="3" rowspan="1">NMO</th><th align="center" valign="top" colspan="3" rowspan="1">non-NMO</th></tr><tr><th align="center" valign="top" rowspan="1" colspan="1">Training</th><th align="center" valign="top" rowspan="1" colspan="1">Validation</th><th align="center" valign="top" rowspan="1" colspan="1">Test</th><th align="center" valign="top" rowspan="1" colspan="1">Training</th><th align="center" valign="top" rowspan="1" colspan="1">Validation</th><th align="center" valign="top" rowspan="1" colspan="1">Test</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Participants</td><td align="center" valign="top" rowspan="1" colspan="1">219</td><td align="center" valign="top" rowspan="1" colspan="1">33</td><td align="center" valign="top" rowspan="1" colspan="1">33</td><td align="center" valign="top" rowspan="1" colspan="1">798</td><td align="center" valign="top" rowspan="1" colspan="1">127</td><td align="center" valign="top" rowspan="1" colspan="1">130</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Eyes</td><td align="center" valign="top" rowspan="1" colspan="1">264</td><td align="center" valign="top" rowspan="1" colspan="1">33</td><td align="center" valign="top" rowspan="1" colspan="1">33</td><td align="center" valign="top" rowspan="1" colspan="1">1,031</td><td align="center" valign="top" rowspan="1" colspan="1">127</td><td align="center" valign="top" rowspan="1" colspan="1">130</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Images</td><td align="center" valign="top" rowspan="1" colspan="1">264</td><td align="center" valign="top" rowspan="1" colspan="1">33</td><td align="center" valign="top" rowspan="1" colspan="1">33</td><td align="center" valign="top" rowspan="1" colspan="1">1,031</td><td align="center" valign="top" rowspan="1" colspan="1">127</td><td align="center" valign="top" rowspan="1" colspan="1">130</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Male/female</td><td align="center" valign="top" rowspan="1" colspan="1">59/160</td><td align="center" valign="top" rowspan="1" colspan="1">7/26</td><td align="center" valign="top" rowspan="1" colspan="1">8/25</td><td align="center" valign="top" rowspan="1" colspan="1">231/567</td><td align="center" valign="top" rowspan="1" colspan="1">38/89</td><td align="center" valign="top" rowspan="1" colspan="1">40/90</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Ages</td><td align="center" valign="top" rowspan="1" colspan="1">38.11 (&#x000b1;11.8)</td><td align="center" valign="top" rowspan="1" colspan="1">41.4 (&#x000b1;12.1)</td><td align="center" valign="top" rowspan="1" colspan="1">40.5 (&#x000b1;12.7)</td><td align="center" valign="top" rowspan="1" colspan="1">39.9 (&#x000b1;13.1)</td><td align="center" valign="top" rowspan="1" colspan="1">40.2 (&#x000b1;13.9)</td><td align="center" valign="top" rowspan="1" colspan="1">41.2 (&#x000b1;12.5)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">AQP4-IgG seropositivity</td><td align="center" valign="top" rowspan="1" colspan="1">183</td><td align="center" valign="top" rowspan="1" colspan="1">27</td><td align="center" valign="top" rowspan="1" colspan="1">29</td><td align="center" valign="top" rowspan="1" colspan="1">/</td><td align="center" valign="top" rowspan="1" colspan="1">/</td><td align="center" valign="top" rowspan="1" colspan="1">/</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Optic nerve MRI (consistent with NMO)</td><td align="center" valign="top" rowspan="1" colspan="1">171</td><td align="center" valign="top" rowspan="1" colspan="1">20</td><td align="center" valign="top" rowspan="1" colspan="1">23</td><td align="center" valign="top" rowspan="1" colspan="1">/</td><td align="center" valign="top" rowspan="1" colspan="1">/</td><td align="center" valign="top" rowspan="1" colspan="1">/</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Spinal cord MRI (consistent with NMO)</td><td align="center" valign="top" rowspan="1" colspan="1">219</td><td align="center" valign="top" rowspan="1" colspan="1">33</td><td align="center" valign="top" rowspan="1" colspan="1">33</td><td align="center" valign="top" rowspan="1" colspan="1">/</td><td align="center" valign="top" rowspan="1" colspan="1">/</td><td align="center" valign="top" rowspan="1" colspan="1">/</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Underlying conditions (N)</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Hypertension</td><td align="center" valign="top" rowspan="1" colspan="1">9 (4.1%)</td><td align="center" valign="top" rowspan="1" colspan="1">1 (3.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">2 (6.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">81 (10.2%)</td><td align="center" valign="top" rowspan="1" colspan="1">11 (8.6%)</td><td align="center" valign="top" rowspan="1" colspan="1">15 (11.5%)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Diabetes</td><td align="center" valign="top" rowspan="1" colspan="1">12 (5.4%)</td><td align="center" valign="top" rowspan="1" colspan="1">2 (6.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">2 (6.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">90 (11.3%)</td><td align="center" valign="top" rowspan="1" colspan="1">10 (7.8%)</td><td align="center" valign="top" rowspan="1" colspan="1">14 (14.7%)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Ocular comorbidities (eyes)</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Glaucoma</td><td align="center" valign="top" rowspan="1" colspan="1">6 (2.3%)</td><td align="center" valign="top" rowspan="1" colspan="1">2 (6.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">3 (9.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">51 (4.9%)</td><td align="center" valign="top" rowspan="1" colspan="1">10 (7.8%)</td><td align="center" valign="top" rowspan="1" colspan="1">14 (10.7%)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Diabetic retinopathy</td><td align="center" valign="top" rowspan="1" colspan="1">7 (2.6%)</td><td align="center" valign="top" rowspan="1" colspan="1">2 (6.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">2 (6.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">91 (8.8%)</td><td align="center" valign="top" rowspan="1" colspan="1">11 (8.6%)</td><td align="center" valign="top" rowspan="1" colspan="1">15 (11.5%)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Retinal detachment</td><td align="center" valign="top" rowspan="1" colspan="1">1 (0.3%)</td><td align="center" valign="top" rowspan="1" colspan="1">1 (0.3%)</td><td align="center" valign="top" rowspan="1" colspan="1">2 (6.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">33 (3.2%)</td><td align="center" valign="top" rowspan="1" colspan="1">5 (3.9%)</td><td align="center" valign="top" rowspan="1" colspan="1">6 (4.6%)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Retinal degeneration</td><td align="center" valign="top" rowspan="1" colspan="1">30 (11.4%)</td><td align="center" valign="top" rowspan="1" colspan="1">4 (12.1%)</td><td align="center" valign="top" rowspan="1" colspan="1">3 (9.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">67 (6.5%)</td><td align="center" valign="top" rowspan="1" colspan="1">13 (10.2%)</td><td align="center" valign="top" rowspan="1" colspan="1">17 (13.0%)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Retinal vein obstruction</td><td align="center" valign="top" rowspan="1" colspan="1">8 (3.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">2 (6.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">1 (3.0%)</td><td align="center" valign="top" rowspan="1" colspan="1">27 (2.6%)</td><td align="center" valign="top" rowspan="1" colspan="1">2 (1.6%)</td><td align="center" valign="top" rowspan="1" colspan="1">4 (3.0%)</td></tr></tbody></table></table-wrap><p>First, we developed a model to diagnose possible NMOSD on the basis of 1,618 UWFs from 1,055 individuals seen at the Third Affiliated Hospital of Sun Yat-sen University ophthalmology department. The images were split into training, validation and test datasets at a ratio of 80:10:10 on the basis of a stratified-group-split approach. Among these images, 330 images were associated with clinical records.</p></sec><sec id="sec15"><label>3.3</label><title>Diagnostic performance of MAiDS-NMO based on UWFs captured by SLO</title><p>Several performance metrics for selected fine-tuned weights during the training, validation and test processes without use of the clinical results. Weight No. 1 showed the best overall performance among the four selected weights, with an AUC of 0.9751 and a maximum Youden&#x02019;s index of 0.8783 on the test dataset (<xref rid="fig5" ref-type="fig">Figure 5</xref> shows the ROC curve of the model on the test dataset). This weight was therefore selected as the final weight for the MAiDS-NMO system based on maximization of the Youden index.</p><fig position="float" id="fig5"><label>Figure 5</label><caption><p>ROC curve of weight no. 1 on test data set. Weight no. 1 shows an AUROC of 0.9751, maximized Youden&#x02019;s index of 0.8783, sensitivity of 90.9% and specificity of 92.%.</p></caption><graphic xlink:href="fmed-12-1555380-g005" position="float"/></fig><p>The MAiDS-NMO system was then evaluated in the test dataset using the optimum cut-off threshold value (0.84) and with multimodal input data. The system achieved an AUC of 0.9923 (ROC curve shown in <xref rid="fig6" ref-type="fig">Figure 6</xref>), a maximum Youden index of 0.9389, a sensitivity of 97.0% and a specificity of 96.9%.</p><fig position="float" id="fig6"><label>Figure 6</label><caption><p>ROC curve of MAiDS-NMO with multimodal input on test data set. The system achieved an AUROC of 0.9923, maximized Youden&#x02019;s index of 0.9389, sensitivity of 97.0% and specificity of 96.9%.</p></caption><graphic xlink:href="fmed-12-1555380-g006" position="float"/></fig><p>The results obtained from five-fold cross-validation provide a more robust and reliable assessment of the MAiDS-NMO system overall predictive performance, mitigating potential biases that may arise from a single training-test split. By employing a systematic approach to data partitioning and iterative validation, this technique ensures the model&#x02019;s stability and generalizability across diverse data distributions, thereby enhancing its applicability and scalability in real-world clinical settings. The detailed results are presented in <xref rid="tab4" ref-type="table">Table 2</xref>.</p><table-wrap position="float" id="tab4"><label>Table 2</label><caption><p>Results of five-fold cross-validation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1">Fold ID</th><th align="left" valign="top" rowspan="1" colspan="1">Training</th><th align="left" valign="top" rowspan="1" colspan="1">Validation</th><th align="left" valign="top" rowspan="1" colspan="1">Test</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="4" colspan="1">Fold 1</td><td align="left" valign="top" rowspan="2" colspan="1">Sensitivity: 87.8%</td><td align="left" valign="top" rowspan="1" colspan="1">AUC: 0.971</td><td align="left" valign="top" rowspan="1" colspan="1">AUC: 0.9734</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Youden: 0.838</td><td align="left" valign="top" rowspan="1" colspan="1">Youden: 0.8249</td></tr><tr><td align="left" valign="top" rowspan="2" colspan="1">Specificity: 89.5%</td><td align="left" valign="top" rowspan="1" colspan="1">Sensitivity: 88.1%</td><td align="left" valign="top" rowspan="1" colspan="1">Sensitivity: 87.9%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Specificity: 93.1%</td><td align="left" valign="top" rowspan="1" colspan="1">Specificity: 94.6%</td></tr><tr><td align="left" valign="top" rowspan="4" colspan="1">Fold 2</td><td align="left" valign="top" rowspan="2" colspan="1">Sensitivity: 85.3%</td><td align="left" valign="top" rowspan="1" colspan="1">AUC: 0.960</td><td align="left" valign="top" rowspan="1" colspan="1">AUC: 0.9774</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Youden: 0.8298</td><td align="left" valign="top" rowspan="1" colspan="1">Youden: 0.8331</td></tr><tr><td align="left" valign="top" rowspan="2" colspan="1">Specificity: 89.0%</td><td align="left" valign="top" rowspan="1" colspan="1">Sensitivity: 75.0%</td><td align="left" valign="top" rowspan="1" colspan="1">Sensitivity: 84.8%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Specificity: 93.8%</td><td align="left" valign="top" rowspan="1" colspan="1">Specificity: 96.9%</td></tr><tr><td align="left" valign="top" rowspan="4" colspan="1">Fold 3</td><td align="left" valign="top" rowspan="2" colspan="1">Sensitivity: 92.2%</td><td align="left" valign="top" rowspan="1" colspan="1">AUC: 0.9311</td><td align="left" valign="top" rowspan="1" colspan="1">AUC: 0.9641</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Youden: 0.7208</td><td align="left" valign="top" rowspan="1" colspan="1">Youden: 0.8471</td></tr><tr><td align="left" valign="top" rowspan="2" colspan="1">Specificity: 93.4%</td><td align="left" valign="top" rowspan="1" colspan="1">Sensitivity: 75.8%</td><td align="left" valign="top" rowspan="1" colspan="1">Sensitivity: 78.9%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Specificity: 94.3%</td><td align="left" valign="top" rowspan="1" colspan="1">Specificity: 96.9%</td></tr><tr><td align="left" valign="top" rowspan="4" colspan="1">Fold 4</td><td align="left" valign="top" rowspan="2" colspan="1">Sensitivity: 92.4%</td><td align="left" valign="top" rowspan="1" colspan="1">AUC: 0.9318</td><td align="left" valign="top" rowspan="1" colspan="1">AUC: 0.9834</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Youden: 0.726</td><td align="left" valign="top" rowspan="1" colspan="1">Youden: 0.8702</td></tr><tr><td align="left" valign="top" rowspan="2" colspan="1">Specificity: 94.3%</td><td align="left" valign="top" rowspan="1" colspan="1">Sensitivity: 83.6%</td><td align="left" valign="top" rowspan="1" colspan="1">Sensitivity: 84.8%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Specificity: 87.7%</td><td align="left" valign="top" rowspan="1" colspan="1">Specificity: 94.6%</td></tr><tr><td align="left" valign="top" rowspan="4" colspan="1">Fold 5</td><td align="left" valign="top" rowspan="2" colspan="1">Sensitivity: 88.0%</td><td align="left" valign="top" rowspan="1" colspan="1">AUC: 0.9554</td><td align="left" valign="top" rowspan="1" colspan="1">AUC: 0.9753</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Youden: 0.782</td><td align="left" valign="top" rowspan="1" colspan="1">Youden: 0.8322</td></tr><tr><td align="left" valign="top" rowspan="2" colspan="1">Specificity: 90.7%</td><td align="left" valign="top" rowspan="1" colspan="1">Sensitivity: 80.9%</td><td align="left" valign="top" rowspan="1" colspan="1">Sensitivity: 84.8%</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Specificity: 90.2%</td><td align="left" valign="top" rowspan="1" colspan="1">Specificity: 93.8%</td></tr></tbody></table></table-wrap></sec><sec id="sec16"><label>3.4</label><title>Heatmap</title><p>Heatmaps were generated and superimposed onto the fundus images to visualize the relative contributions of each pixel in predicting the grading for each image. In-NMO-positive cases, the regions exhibiting the highest signals intensity&#x02014;indicating the most significant influence on the grading prediction&#x02014;were predominantly localized to the optic nerve and peripapillary areas, aligning with known pathological sites of the disease. In contrast, NMO-negative cases displayed no areas of high signal intensity in the optic nerve and its surrounding regions, suggesting an absence of characteristic NMO-related features. Representative examples of these heatmaps are shown in <xref rid="fig7" ref-type="fig">Figure 7</xref>.</p><fig position="float" id="fig7"><label>Figure 7</label><caption><p>Original SLO image (right) and heatmap (left). <bold>(A1)</bold> Positive case. <bold>(A2)</bold> Negative case. <bold>(B1)</bold> In the NMO-positive case, the areas of high signal intensity closely corresponded to the anatomical distribution of the optic nerve and peripapillary region. <bold>(B2)</bold> In the negative case, no areas of high signal intensity were observed in these anatomical regions.</p></caption><graphic xlink:href="fmed-12-1555380-g007" position="float"/></fig></sec><sec id="sec17"><label>3.5</label><title>Comparative test</title><p>To verify the performance of MAiDS-NMO in assessing NMO, a comparative test involving 200 UWFs was conducted between the model and six ophthalmologists. In the initial assessment, the precision rates of the proposed system in the two tests were 90.2 and 97.8% across two tests, demonstrating stable and reliable performance of the neural network. Indicating that the neural network achieved relatively stable training results. In contrast, the accuracy rates of experts, attending physicians, and residents in the first test were 78.3, 56.8, and 33.3%, respectively. However, when provided with clinical information the accuracy rates improved to 98, 85.7, and 70.2%, respectively. Results of the comparative test were shown in <xref rid="fig6" ref-type="fig">Figure 6</xref>. These results highlight the potential of MAiDS-NMO in assisting clinical diagnosis, particularly in settings where access to specialized expertise is limited. The detailed results of the comparative test are presented in <xref rid="fig8" ref-type="fig">Figure 8</xref>.</p><fig position="float" id="fig8"><label>Figure 8</label><caption><p>Diagnostic performance comparison (AI vs. ophthalmologists at different levels): AI model for NMO detection; L1: specialists; L2: attendings; L3: residents. L1: senior specialists [&#x0003e;10&#x0202f;years&#x02019; experience]; L2: attendings [3&#x02013;10&#x0202f;years]; L3: residents [&#x0003c;3&#x0202f;years].</p></caption><graphic xlink:href="fmed-12-1555380-g008" position="float"/></fig></sec></sec><sec sec-type="discussion" id="sec18"><label>4</label><title>Discussion</title><p>NMOSD occurs worldwide and affects individuals of all ethnic backgrounds (<xref rid="ref24" ref-type="bibr">24</xref>). This severe and widespread disorder has a profound impact on global health, imposing a heavy global burden. Without timely treatment, approximately 50% of NMOSD patients may become disabled and blind, and one-third die within five years of their initial attack (<xref rid="ref25" ref-type="bibr">25</xref>). Diagnosing NMOSD and monitoring treatment is challenging, burdensome, and clinically time-consuming due to the disease&#x02019;s variable and often insidious manifestations. Furthermore, access to essential diagnostic tools such as MRI scanning and tests for AQP4-IgG and MOG-IgG is limited in many regions, and some patients are seronegative. Thus, it may be challenging to distinguish NMOSD solely based on brain MRI at disease onset. Misdiagnosis of NMOSD is common, leading to delays in appropriate treatment and potentially resulting in severe or irreversible vision loss. Additionally, NMOSD lesions may be missed if patients are evaluated outside the acute phase or present with atypical clinical symptoms. Differences among ethnic populations, selection bias, and the availability of expert knowledge further complicate accurate diagnosis and analysis.</p><p>Screening for NMOSD, coupled with timely referral and treatment, is a widely recognized strategy for preventing blindness. Thus, there is high clinical demand for an efficient and reliable AI model that capable of overcoming the abovementioned obstacles and aiding in the early detection of NMOSD. Such a model could facilitate prompt intervention and improve patient outcomes. Deep learning algorithms have been widely applied in the diagnosis of ophthalmic diseases, demonstrating outstanding diagnostic performance (<xref rid="ref26" ref-type="bibr">26</xref>, <xref rid="ref27" ref-type="bibr">27</xref>). However, to our knowledge, few studies have explored the effectiveness of DL- or AI-based methods in NMOSD diagnosis.</p><p>In this study, our AI model achieved exceptional performance in diagnosing NMOSD using UWFs as the input. First, the training dataset was constructed entirely based on the ophthalmologist&#x02019;s grading of all color fundus images, enabling the development of a DL algorithm with robust accuracy in identifying individuals suspected of having NMOSD at various stages. The model achieved an AUC&#x0202f;&#x0003e;&#x0202f;0.97, sensitivity &#x0003e; 90%, and specificity &#x0003e; 92% across all datasets with available data. Second, beyond using UWFs as input, the AI model is well-suited for integrating additional clinical examinations and MRI reports of the optic nerve and spinal cord. This allows for NMOSD subtype classification and disease progression tracking, incorporating multimodal data to enhance predictive performance. Third, the model was trained on both high-quality and low-quality UWFs, ensuring its applicability to images affected by eye movement, media opacities, or other conditions that may obscure image clarity. Lastly, in a comparative test between the model and ophthalmologists, results indicated that while the neural network achieved relatively stable training results, ophthalmologists demonstrated superior diagnostic performance when provided with clinical data for reference.</p><p>The shortage of ophthalmologists and neurologists in rural areas has led to delays in NMOSD diagnosis and treatment, leaving many patients without timely medical care. UWFs is noninvasive and user-friendly tool that playing a significant role in telemedicine, proving invaluable in regions with limited access to specialized ophthalmological services. By enabling remote diagnostics and facilitating prompt medical interventions (<xref rid="ref28" ref-type="bibr">28</xref>), UWF helps bridge healthcare gaps in underserved areas. Our AI-driven algorithm and platform, designed to diagnose NMOSD using UWF images, streamline the diagnostic process without requiring direct involvement from ophthalmologists or neurologists. This innovation has the potential to assist governments in providing more accurate and timely medical support to economically disadvantaged populations, improving healthcare accessibility and outcomes.</p><p>There are several limitations to this study. First, the dataset included a relatively small number of patients with NMSOD, largely due to the rarity of the disease in the general population. This limited sample size may have affected the model&#x02019;s training and its overall predictive performance. Second, all the data were derived exclusively from a single-center, hospital-based Chinese population, without representation from broader community-based or multi-ethnic cohorts. This homogeneity, coupled with the lack of an external validation dataset, restricts the generalizability and external applicability of our findings. Validation using data from multiple centers and more diverse ethnic populations will be essential to confirm the robustness and adaptability of the model. Third, the current application is designed specifically for the early identification or onset prediction of NMOSD but does not address disease progression or recurrence was limited to onset prediction and cannot predict progression, the latter being an essential part of NMOSD management. Despite this limitation, the model still provides clinical value, especially in assisting with the challenging differential diagnosis during the early stages of the disease. Future research with larger, more diverse datasets and longitudinal follow-up will be essential to develop a fully automated and comprehensive deep learning framework capable of both diagnosing and monitoring NMOSD.</p></sec><sec sec-type="conclusions" id="sec19"><label>5</label><title>Conclusion</title><p>The emergence and advancement of AI have provided new opportunities for improving novel systems and strategies for detecting NMOSD. Our study demonstrated that the proposed DL system exhibits high sensitivity and specificity in identifying NMOSD patients, while also showing potential for predicting disease onset and progression. With improved computing capabilities and advanced database technologies, this model could potentially be developed into a comprehensive virtual screening system for NMOSD in clinical practice.</p></sec></body><back><sec sec-type="data-availability" id="sec20"><title>Data availability statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.</p></sec><sec sec-type="ethics-statement" id="sec21"><title>Ethics statement</title><p>Written informed consent was obtained from the individual(s), and minor(s)&#x02019; legal guardian/next of kin, for the publication of any potentially identifiable images or data included in this article.</p></sec><sec sec-type="author-contributions" id="sec22"><title>Author contributions</title><p>SG: Writing &#x02013; original draft, Conceptualization, Formal analysis. TB: Data curation, Writing &#x02013; review &#x00026; editing. TW: Software, Visualization, Writing &#x02013; review &#x00026; editing. QY: Formal analysis, Writing &#x02013; review &#x00026; editing. WY: Software, Writing &#x02013; review &#x00026; editing. JL: Software, Writing &#x02013; review &#x00026; editing. HZ: Methodology, Writing &#x02013; review &#x00026; editing. SC: Data curation, Writing &#x02013; review &#x00026; editing. YS: Methodology, Writing &#x02013; review &#x00026; editing. XJ: Methodology, Writing &#x02013; review &#x00026; editing. LH: Supervision, Writing &#x02013; review &#x00026; editing. SL: Project administration, Supervision, Writing &#x02013; review &#x00026; editing.</p></sec><sec sec-type="COI-statement" id="sec24"><title>Conflict of interest</title><p>QY and WY were employed by Zeemo Technology Company Limited.</p><p>The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="ai-statement" id="sec25"><title>Generative AI statement</title><p>The author(s) declare that no Gen AI was used in the creation of this manuscript.</p></sec><sec sec-type="disclaimer" id="sec26"><title>Publisher&#x02019;s note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id="ref1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jarius</surname><given-names>S</given-names></name><name><surname>Paul</surname><given-names>F</given-names></name><name><surname>Weinshenker</surname><given-names>BG</given-names></name><name><surname>Levy</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>HJ</given-names></name><name><surname>Wildemann</surname><given-names>B</given-names></name></person-group>. <article-title>Neuromyelitis optica</article-title>. <source>Nat Rev Dis Primers</source>. (<year>2020</year>) <volume>6</volume>:<fpage>85</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41572-020-0214-9</pub-id><pub-id pub-id-type="pmid">33093467</pub-id>
</mixed-citation></ref><ref id="ref2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salama</surname><given-names>S</given-names></name><name><surname>Marouf</surname><given-names>H</given-names></name><name><surname>Ihab Reda</surname><given-names>M</given-names></name><name><surname>Mansour</surname><given-names>AR</given-names></name></person-group>. <article-title>Clinical and radiological characteristics of neuromyelitis optica spectrum disorder in the north Egyptian Nile Delta</article-title>. <source>J Neuroimmunol</source>. (<year>2018</year>) <volume>324</volume>:<fpage>22</fpage>&#x02013;<lpage>5</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jneuroim.2018.08.014</pub-id>, PMID: <pub-id pub-id-type="pmid">30199734</pub-id>
</mixed-citation></ref><ref id="ref3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jarius</surname><given-names>S</given-names></name><name><surname>Wildemann</surname><given-names>B</given-names></name></person-group>. <article-title>The history of neuromyelitis optica. Part 2: 'Spinal amaurosis', or how it all began</article-title>. <source>J Neuroinflammation</source>. (<year>2019</year>) <volume>16</volume>:<fpage>280</fpage>. doi: <pub-id pub-id-type="doi">10.1186/s12974-019-1594-1</pub-id>, PMID: <pub-id pub-id-type="pmid">31883522</pub-id>
</mixed-citation></ref><ref id="ref4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jarius</surname><given-names>S</given-names></name><name><surname>Wildemann</surname><given-names>B</given-names></name></person-group>. <article-title>Devic's index case: a critical reappraisal - AQP4-IgG-mediated neuromyelitis optica spectrum disorder, or rather MOG encephalomyelitis?</article-title>
<source>J Neurol Sci</source>. (<year>2019</year>) <volume>407</volume>:<fpage>116396</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jns.2019.07.014</pub-id><pub-id pub-id-type="pmid">31726278</pub-id>
</mixed-citation></ref><ref id="ref5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lennon</surname><given-names>VA</given-names></name><name><surname>Wingerchuk</surname><given-names>DM</given-names></name><name><surname>Kryzer</surname><given-names>TJ</given-names></name><name><surname>Pittock</surname><given-names>SJ</given-names></name><name><surname>Lucchinetti</surname><given-names>CF</given-names></name><name><surname>Fujihara</surname><given-names>K</given-names></name><etal/></person-group>. <article-title>A serum autoantibody marker of neuromyelitis optica: distinction from multiple sclerosis</article-title>. <source>Lancet</source>. (<year>2004</year>) <volume>364</volume>:<fpage>2106</fpage>&#x02013;<lpage>12</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S0140-6736(04)17551-X</pub-id>, PMID: <pub-id pub-id-type="pmid">15589308</pub-id>
</mixed-citation></ref><ref id="ref6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mader</surname><given-names>S</given-names></name><name><surname>Gredler</surname><given-names>V</given-names></name><name><surname>Schanda</surname><given-names>K</given-names></name><name><surname>Rostasy</surname><given-names>K</given-names></name><name><surname>Dujmovic</surname><given-names>I</given-names></name><name><surname>Pfaller</surname><given-names>K</given-names></name><etal/></person-group>. <article-title>Complement activating antibodies to myelin oligodendrocyte glycoprotein in neuromyelitis optica and related disorders</article-title>. <source>J Neuroinflammation</source>. (<year>2011</year>) <volume>8</volume>:<fpage>184</fpage>. doi: <pub-id pub-id-type="doi">10.1186/1742-2094-8-184</pub-id>, PMID: <pub-id pub-id-type="pmid">22204662</pub-id>
</mixed-citation></ref><ref id="ref7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wingerchuk</surname><given-names>DM</given-names></name><name><surname>Lennon</surname><given-names>VA</given-names></name><name><surname>Lucchinetti</surname><given-names>CF</given-names></name><name><surname>Pittock</surname><given-names>SJ</given-names></name><name><surname>Weinshenker</surname><given-names>BG</given-names></name></person-group>. <article-title>The spectrum of neuromyelitis optica</article-title>. <source>Lancet Neurol</source>. (<year>2007</year>) <volume>6</volume>:<fpage>805</fpage>&#x02013;<lpage>15</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S1474-4422(07)70216-8</pub-id>, PMID: <pub-id pub-id-type="pmid">17706564</pub-id>
</mixed-citation></ref><ref id="ref8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakajima</surname><given-names>H</given-names></name><name><surname>Hosokawa</surname><given-names>T</given-names></name><name><surname>Sugino</surname><given-names>M</given-names></name><name><surname>Kimura</surname><given-names>F</given-names></name><name><surname>Sugasawa</surname><given-names>J</given-names></name><name><surname>Hanafusa</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>Visual field defects of optic neuritis in neuromyelitis optica compared with multiple sclerosis</article-title>. <source>BMC Neurol</source>. (<year>2010</year>) <volume>10</volume>:<fpage>45</fpage>. doi: <pub-id pub-id-type="doi">10.1186/1471-2377-10-45</pub-id>, PMID: <pub-id pub-id-type="pmid">20565857</pub-id>
</mixed-citation></ref><ref id="ref9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>JJ</given-names></name><name><surname>Flanagan</surname><given-names>EP</given-names></name><name><surname>Jitprapaikulsan</surname><given-names>J</given-names></name><name><surname>L&#x000f3;pez-Chiriboga</surname><given-names>ASS</given-names></name><name><surname>Fryer</surname><given-names>JP</given-names></name><name><surname>Leavitt</surname><given-names>JA</given-names></name><etal/></person-group>. <article-title>Myelin oligodendrocyte glycoprotein antibody-positive optic neuritis: clinical characteristics, radiologic clues, and outcome</article-title>. <source>Am J Ophthalmol</source>. (<year>2018</year>) <volume>195</volume>:<fpage>8</fpage>&#x02013;<lpage>15</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.ajo.2018.07.020</pub-id>, PMID: <pub-id pub-id-type="pmid">30055153</pub-id>
</mixed-citation></ref><ref id="ref10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wingerchuk</surname><given-names>DM</given-names></name><name><surname>Banwell</surname><given-names>B</given-names></name><name><surname>Bennett</surname><given-names>JL</given-names></name><name><surname>Cabre</surname><given-names>P</given-names></name><name><surname>Carroll</surname><given-names>W</given-names></name><name><surname>Chitnis</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>International consensus diagnostic criteria for neuromyelitis optica spectrum disorders</article-title>. <source>Neurology</source>. (<year>2015</year>) <volume>85</volume>:<fpage>177</fpage>&#x02013;<lpage>89</lpage>. doi: <pub-id pub-id-type="doi">10.1212/WNL.0000000000001729</pub-id>, PMID: <pub-id pub-id-type="pmid">26092914</pub-id>
</mixed-citation></ref><ref id="ref11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ankolekar</surname><given-names>A</given-names></name><name><surname>Eppings</surname><given-names>L</given-names></name><name><surname>Bottari</surname><given-names>F</given-names></name><name><surname>Pinho</surname><given-names>IF</given-names></name><name><surname>Howard</surname><given-names>K</given-names></name><name><surname>Baker</surname><given-names>R</given-names></name><etal/></person-group>. <article-title>Using artificial intelligence and predictive modelling to enable learning healthcare systems (LHS) for pandemic preparedness</article-title>. <source>Comput Struct Biotechnol J</source>. (<year>2024</year>) <volume>24</volume>:<fpage>412</fpage>&#x02013;<lpage>9</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.csbj.2024.05.014</pub-id>, PMID: <pub-id pub-id-type="pmid">38831762</pub-id>
</mixed-citation></ref><ref id="ref12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Fan</surname><given-names>X</given-names></name><name><surname>Meng</surname><given-names>Y</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name></person-group>. <article-title>AI-driven generalized polynomial transformation models for unsupervised fundus image registration</article-title>. <source>Front Med</source>. (<year>2024</year>) <volume>11</volume>:<fpage>1421439</fpage>. doi: <pub-id pub-id-type="doi">10.3389/fmed.2024.1421439</pub-id>, PMID: <pub-id pub-id-type="pmid">39081694</pub-id>
</mixed-citation></ref><ref id="ref13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keenan</surname><given-names>TDL</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Agr&#x000f3;n</surname><given-names>E</given-names></name><name><surname>Tham</surname><given-names>YC</given-names></name><name><surname>Goh</surname><given-names>JHL</given-names></name><name><surname>Lei</surname><given-names>X</given-names></name><etal/></person-group>. <article-title>DeepLensNet: deep learning automated diagnosis and quantitative classification of cataract type and severity</article-title>. <source>Ophthalmology</source>. (<year>2022</year>) <volume>129</volume>:<fpage>571</fpage>&#x02013;<lpage>84</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.ophtha.2021.12.017</pub-id>, PMID: <pub-id pub-id-type="pmid">34990643</pub-id>
</mixed-citation></ref><ref id="ref14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goh</surname><given-names>JHL</given-names></name><name><surname>Lim</surname><given-names>ZW</given-names></name><name><surname>Fang</surname><given-names>X</given-names></name><name><surname>Anees</surname><given-names>A</given-names></name><name><surname>Nusinovici</surname><given-names>S</given-names></name><name><surname>Rim</surname><given-names>TH</given-names></name><etal/></person-group>. <article-title>Artificial intelligence for cataract detection and management</article-title>. <source>Asia Pac J Ophthalmol</source>. (<year>2020</year>) <volume>9</volume>:<fpage>88</fpage>&#x02013;<lpage>95</lpage>. doi: <pub-id pub-id-type="doi">10.1097/01.APO.0000656988.16221.04</pub-id></mixed-citation></ref><ref id="ref15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Zhong</surname><given-names>J</given-names></name><name><surname>Yi</surname><given-names>Z</given-names></name></person-group>. <article-title>DeepUWF: an automated ultra-wide-field fundus screening system via deep learning</article-title>. <source>IEEE J Biomed Health Inform</source>. (<year>2021</year>) <volume>25</volume>:<fpage>2988</fpage>&#x02013;<lpage>96</lpage>. doi: <pub-id pub-id-type="doi">10.1109/JBHI.2020.3046771</pub-id>, PMID: <pub-id pub-id-type="pmid">33361011</pub-id>
</mixed-citation></ref><ref id="ref16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulshan</surname><given-names>V</given-names></name><name><surname>Peng</surname><given-names>L</given-names></name><name><surname>Coram</surname><given-names>M</given-names></name><name><surname>Stumpe</surname><given-names>MC</given-names></name><name><surname>Wu</surname><given-names>D</given-names></name><name><surname>Narayanaswamy</surname><given-names>A</given-names></name><etal/></person-group>. <article-title>Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</article-title>. <source>JAMA</source>. (<year>2016</year>) <volume>316</volume>:<fpage>2402</fpage>&#x02013;<lpage>10</lpage>. doi: <pub-id pub-id-type="doi">10.1001/jama.2016.17216</pub-id>, PMID: <pub-id pub-id-type="pmid">27898976</pub-id>
</mixed-citation></ref><ref id="ref17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>Y</given-names></name><name><surname>Qiao</surname><given-names>L</given-names></name><name><surname>Du</surname><given-names>M</given-names></name><name><surname>Qu</surname><given-names>C</given-names></name><name><surname>Wan</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>Age-related macular degeneration: epidemiology, genetics, pathophysiology, diagnosis, and targeted therapy</article-title>. <source>Genes Dis</source>. (<year>2022</year>) <volume>9</volume>:<fpage>62</fpage>&#x02013;<lpage>79</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.gendis.2021.02.009</pub-id>, PMID: <pub-id pub-id-type="pmid">35005108</pub-id>
</mixed-citation></ref><ref id="ref18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Niu</surname><given-names>K</given-names></name><name><surname>Xiong</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>He</surname><given-names>Z</given-names></name><name><surname>Song</surname><given-names>H</given-names></name></person-group>. <article-title>Automatic cataract grading methods based on deep learning</article-title>. <source>Comput Methods Prog Biomed</source>. (<year>2019</year>) <volume>182</volume>:<fpage>104978</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cmpb.2019.07.006</pub-id>, PMID: <pub-id pub-id-type="pmid">31450174</pub-id>
</mixed-citation></ref><ref id="ref19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hood</surname><given-names>DC</given-names></name><name><surname>De Moraes</surname><given-names>CG</given-names></name></person-group>. <article-title>Efficacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs</article-title>. <source>Ophthalmology</source>. (<year>2018</year>) <volume>125</volume>:<fpage>1207</fpage>&#x02013;<lpage>8</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.ophtha.2018.04.020</pub-id>, PMID: <pub-id pub-id-type="pmid">30032794</pub-id>
</mixed-citation></ref><ref id="ref20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasan</surname><given-names>MM</given-names></name><name><surname>Phu</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Sowmya</surname><given-names>A</given-names></name><name><surname>Meijering</surname><given-names>E</given-names></name><name><surname>Kalloniatis</surname><given-names>M</given-names></name></person-group>. <article-title>Predicting visual field global and local parameters from OCT measurements using explainable machine learning</article-title>. <source>Sci Rep</source>. (<year>2025</year>) <volume>15</volume>:<fpage>5685</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-025-89557-1</pub-id><pub-id pub-id-type="pmid">39956834</pub-id>
</mixed-citation></ref><ref id="ref21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>TE</given-names></name><name><surname>Anees</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><etal/></person-group>. <article-title>Retinal photograph-based deep learning algorithms for myopia and a blockchain platform to facilitate artificial intelligence medical research: a retrospective multicohort study</article-title>. <source>Lancet Digit Health</source>. (<year>2021</year>) <volume>3</volume>:<fpage>e317</fpage>&#x02013;<lpage>29</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S2589-7500(21)00055-8</pub-id>, PMID: <pub-id pub-id-type="pmid">33890579</pub-id>
</mixed-citation></ref><ref id="ref22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aiello</surname><given-names>LP</given-names></name><name><surname>Odia</surname><given-names>I</given-names></name><name><surname>Glassman</surname><given-names>AR</given-names></name><name><surname>Melia</surname><given-names>M</given-names></name><name><surname>Jampol</surname><given-names>LM</given-names></name><name><surname>Bressler</surname><given-names>NM</given-names></name><etal/></person-group>. <article-title>Comparison of early treatment diabetic retinopathy study standard 7-field imaging with Ultrawide-field imaging for determining severity of diabetic retinopathy</article-title>. <source>JAMA Ophthalmol</source>. (<year>2019</year>) <volume>137</volume>:<fpage>65</fpage>&#x02013;<lpage>73</lpage>. doi: <pub-id pub-id-type="doi">10.1001/jamaophthalmol.2018.4982</pub-id>, PMID: <pub-id pub-id-type="pmid">30347105</pub-id>
</mixed-citation></ref><ref id="ref23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calafiore</surname><given-names>GC</given-names></name><name><surname>Gaubert</surname><given-names>S</given-names></name><name><surname>Possieri</surname><given-names>C</given-names></name></person-group>. <article-title>Log-sum-Exp neural networks and Posynomial models for convex and log-log-convex data</article-title>. <source>IEEE Trans Neural Netw Learn Syst</source>. (<year>2020</year>) <volume>31</volume>:<fpage>827</fpage>&#x02013;<lpage>38</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TNNLS.2019.2910417</pub-id>, PMID: <pub-id pub-id-type="pmid">31095500</pub-id>
</mixed-citation></ref><ref id="ref24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mori</surname><given-names>M</given-names></name><name><surname>Kuwabara</surname><given-names>S</given-names></name><name><surname>Paul</surname><given-names>F</given-names></name></person-group>. <article-title>Worldwide prevalence of neuromyelitis optica spectrum disorders</article-title>. <source>J Neurol Neurosurg Psychiatry</source>. (<year>2018</year>) <volume>89</volume>:<fpage>555</fpage>&#x02013;<lpage>6</lpage>. doi: <pub-id pub-id-type="doi">10.1136/jnnp-2017-317566</pub-id>, PMID: <pub-id pub-id-type="pmid">29436488</pub-id>
</mixed-citation></ref><ref id="ref25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huda</surname><given-names>S</given-names></name><name><surname>Whittam</surname><given-names>D</given-names></name><name><surname>Bhojak</surname><given-names>M</given-names></name><name><surname>Chamberlain</surname><given-names>J</given-names></name><name><surname>Noonan</surname><given-names>C</given-names></name><name><surname>Jacob</surname><given-names>A</given-names></name></person-group>. <article-title>Neuromyelitis optica spectrum disorders</article-title>. <source>Clin Med</source>. (<year>2019</year>) <volume>19</volume>:<fpage>169</fpage>&#x02013;<lpage>76</lpage>. doi: <pub-id pub-id-type="doi">10.7861/clinmedicine.19-2-169</pub-id>, PMID: <pub-id pub-id-type="pmid">30872305</pub-id>
</mixed-citation></ref><ref id="ref26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Su</surname><given-names>Y</given-names></name><name><surname>Lin</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Nie</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>A deep-learning system predicts glaucoma incidence and progression using retinal photographs</article-title>. <source>J Clin Invest</source>. (<year>2022</year>) <volume>132</volume>:<fpage>e157968</fpage>. doi: <pub-id pub-id-type="doi">10.1172/JCI157968</pub-id>, PMID: <pub-id pub-id-type="pmid">35642636</pub-id>
</mixed-citation></ref><ref id="ref27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasan</surname><given-names>MM</given-names></name><name><surname>Phu</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Sowmya</surname><given-names>A</given-names></name><name><surname>Kalloniatis</surname><given-names>M</given-names></name><name><surname>Meijering</surname><given-names>E</given-names></name></person-group>. <article-title>OCT-based diagnosis of glaucoma and glaucoma stages using explainable machine learning</article-title>. <source>Sci Rep</source>. (<year>2025</year>) <volume>15</volume>:<fpage>3592</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41598-025-87219-w</pub-id><pub-id pub-id-type="pmid">39875492</pub-id>
</mixed-citation></ref><ref id="ref28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>TD</given-names></name><name><surname>Le</surname><given-names>DT</given-names></name><name><surname>Bum</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Song</surname><given-names>SJ</given-names></name><name><surname>Choo</surname><given-names>H</given-names></name></person-group>. <article-title>Retinal disease diagnosis using deep learning on ultra-wide-field fundus images</article-title>. <source>Diagnostics</source>. (<year>2024</year>):<fpage>14</fpage>. doi: <pub-id pub-id-type="doi">10.3390/diagnostics14010105</pub-id><pub-id pub-id-type="pmid">39795541</pub-id>
</mixed-citation></ref></ref-list></back></article>