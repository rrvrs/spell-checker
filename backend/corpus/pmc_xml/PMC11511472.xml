<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39460228</article-id><article-id pub-id-type="pmc">PMC11511472</article-id>
<article-id pub-id-type="doi">10.3390/s24206748</article-id><article-id pub-id-type="publisher-id">sensors-24-06748</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Student Facial Expression Recognition Model Based on Multi-Scale and Deep Fine-Grained Feature Attention Enhancement</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2429-5782</contrib-id><name><surname>Shou</surname><given-names>Zhaoyu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-24-06748" ref-type="aff">1</xref><xref rid="af2-sensors-24-06748" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Yi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-24-06748" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Dongxu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-24-06748" ref-type="aff">1</xref><xref rid="c1-sensors-24-06748" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Feng</surname><given-names>Cheng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="af1-sensors-24-06748" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Huibing</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><xref rid="af3-sensors-24-06748" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Lin</surname><given-names>Yuming</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af3-sensors-24-06748" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Guangxiang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af4-sensors-24-06748" ref-type="aff">4</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Travieso-Gonz&#x000e1;lez</surname><given-names>Carlos M.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-24-06748"><label>1</label>School of Information and Communication, Guilin University of Electronic Technology, Guilin 541004, China; <email>guilinshou@guet.edu.cn</email> (Z.S.); <email>22022303042@mails.guet.edu.cn</email> (Y.H.); <email>fengcheng@mails.guet.edu.cn</email> (C.F.)</aff><aff id="af2-sensors-24-06748"><label>2</label>Guangxi Wireless Broadband Communication and Signal Processing Key Laboratory, Guilin University of Electronic Technology, Guilin 541004, China</aff><aff id="af3-sensors-24-06748"><label>3</label>School of Computer and Information Security, Guilin University of Electronic Technology, Guilin 541004, China; <email>zhanghuibing@guet.edu.cn</email> (H.Z.); <email>ymlin@guet.edu.cn</email> (Y.L.)</aff><aff id="af4-sensors-24-06748"><label>4</label>34th Research Institute of China Electronics Technology Group Corporation, Guilin 541004, China; <email>wuguangxiang@cetc.com.cn</email></aff><author-notes><corresp id="c1-sensors-24-06748"><label>*</label>Correspondence: <email>ldx@mails.guet.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>20</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="collection"><month>10</month><year>2024</year></pub-date><volume>24</volume><issue>20</issue><elocation-id>6748</elocation-id><history><date date-type="received"><day>18</day><month>8</month><year>2024</year></date><date date-type="rev-recd"><day>05</day><month>10</month><year>2024</year></date><date date-type="accepted"><day>18</day><month>10</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 by the authors.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>In smart classroom environments, accurately recognizing students&#x02019; facial expressions is crucial for teachers to efficiently assess students&#x02019; learning states, timely adjust teaching strategies, and enhance teaching quality and effectiveness. In this paper, we propose a student facial expression recognition model based on multi-scale and deep fine-grained feature attention enhancement (SFER-MDFAE) to address the issues of inaccurate facial feature extraction and poor robustness of facial expression recognition in smart classroom scenarios. Firstly, we construct a novel multi-scale dual-pooling feature aggregation module to capture and fuse facial information at different scales, thereby obtaining a comprehensive representation of key facial features; secondly, we design a key region-oriented attention mechanism to focus more on the nuances of facial expressions, further enhancing the representation of multi-scale deep fine-grained feature; finally, the fusion of multi-scale and deep fine-grained attention-enhanced features is used to obtain richer and more accurate facial key information and realize accurate facial expression recognition. The experimental results demonstrate that the proposed SFER-MDFAE outperforms the existing state-of-the-art methods, achieving an accuracy of 76.18% on FER2013, 92.75% on FERPlus, 92.93% on RAF-DB, 67.86% on AffectNet, and 93.74% on the real smart classroom facial expression dataset (SCFED). These results validate the effectiveness of the proposed method.</p></abstract><kwd-group><kwd>facial expression recognition</kwd><kwd>smart classroom</kwd><kwd>multi-scale features</kwd><kwd>deep fine-grained features</kwd><kwd>key region-oriented attention mechanism</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62177012</award-id><award-id>62267003</award-id></award-group><award-group><funding-source>Guangxi Natural Science Foundation</funding-source><award-id>No.2024GXNSFDA010048</award-id></award-group><award-group><funding-source>Project of Guangxi Wireless Broadband Communication and Signal Processing Key Laboratory</funding-source><award-id>GXKL06240107</award-id></award-group><award-group><funding-source>Innovation Project of Guangxi Graduate Education</funding-source><award-id>YCBZ2024160</award-id></award-group><funding-statement>This work was supported by the National Natural Science Foundation of China (62177012, 62267003), Guangxi Natural Science Foundation under Grant No.2024GXNSFDA010048, the Project of Guangxi Wireless Broadband Communication and Signal Processing Key Laboratory (GXKL06240107), and Innovation Project of Guangxi Graduate Education (YCBZ2024160).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-24-06748"><title>1. Introduction</title><p>Facial expression recognition plays an important role in the field of education, which is mainly reflected in two aspects: firstly, it helps teachers to assess students&#x02019; emotional states and attention levels in real time [<xref rid="B1-sensors-24-06748" ref-type="bibr">1</xref>], further adjusting teaching strategies. Secondly, it supports personalized teaching; specifically, teachers can gain insights into their learning situation and needs through analyzing students&#x02019; facial expressions and emotional states to provide targeted teaching and counseling [<xref rid="B2-sensors-24-06748" ref-type="bibr">2</xref>]. However, in the classroom environment, factors such as lighting variations, viewpoint diversity, and distance differences can severely affect the accuracy of facial expression recognition. Therefore, enhancing the robustness and accuracy of facial expression recognition models to cope with complex classroom environments has become a critical problem to be solved.</p><p>In the past few decades, advancements in VR technology have enabled researchers to create specific virtual environments designed to elicit emotional responses, thereby improving the recognition of facial expressions. Castiblanco et al. [<xref rid="B3-sensors-24-06748" ref-type="bibr">3</xref>] conducted a study that utilized virtual reality environments combined with electroencephalography (EEG) and machine learning techniques to explore the effectiveness of EEG in emotional representation. The results showed a significant consistency between self-assessments and classifier outcomes, further supporting the feasibility of using EEG as a tool for emotional representation. Mar&#x000ed;n-Morales et al. [<xref rid="B4-sensors-24-06748" ref-type="bibr">4</xref>] proposed an emotion recognition system that evokes emotional states through immersive virtual environments. Based on EEG and electrocardiogram (ECG) data from 60 participants, the system achieved high accuracy using a support vector machine classifier, validating the potential application of immersive virtual environments in emotion recognition. However, despite the progress made in virtual environment research, exploring emotional responses and facial expression recognition in real-world settings remains a significant challenge for researchers. Saurav et al. [<xref rid="B5-sensors-24-06748" ref-type="bibr">5</xref>] used the AdaBoost feature selection algorithm to select key facial features from the original HOG features, thereby improving the accuracy of facial expression recognition. Shi et al. [<xref rid="B6-sensors-24-06748" ref-type="bibr">6</xref>] addressed the variability in the number and location of key points in the traditional SIFT algorithm by proposing an improved version. This new method optimizes key point positions through shape decomposition and a key point constraint algorithm, while also extracting regional gradient information for emotion recognition. Niu et al. [<xref rid="B7-sensors-24-06748" ref-type="bibr">7</xref>] proposed a facial expression recognition algorithm combining ORB and LBP features. The algorithm extracts effective features through face detection, employs region partitioning to enhance computational speed, and finally uses SVM for classifying the combined features to recognize different facial expressions. Lakshmi et al. [<xref rid="B8-sensors-24-06748" ref-type="bibr">8</xref>] proposed a modified HOG and LBP feature descriptor for extracting features from the detected eye, nose, and mouth regions, thus recognizing facial expressions. The above studies use traditional manual features to recognize facial expressions in real environments, which saves computational resources. However, these methods rely on manually designed features, which are time-consuming and laborious, and the models have limited recognition accuracy and weak generalization performance. Consequently, in recent years, researchers have gradually shifted their focus to the field of deep learning. Shou et al. [<xref rid="B9-sensors-24-06748" ref-type="bibr">9</xref>] proposed a residual channel-crossing transformer mask network to accurately extract the key features of students&#x02019; facial expressions. Xue et al. [<xref rid="B10-sensors-24-06748" ref-type="bibr">10</xref>] proposed a facial expression recognition network with smooth prediction from coarse to fine. In the first stage, it identifies facial expressions that are relatively similar and categorizes them into the same class. In the second stage, it captures unique fine-grained facial features and reclassifies the expression categories from the first stage. Nan et al. [<xref rid="B11-sensors-24-06748" ref-type="bibr">11</xref>] introduced an attention mechanism to enhance the extraction of facial features in the MobileNetV1 model. By combining center loss with softmax loss, they optimized the model parameters to reduce intra-class distance and increase inter-class distance, thereby improving the accuracy of facial expression classification. Farzaneh et al. [<xref rid="B12-sensors-24-06748" ref-type="bibr">12</xref>] proposed a deep attention center loss method that enhances discriminative power by adaptively selecting a subset of important feature elements. By embedding an attention mechanism, this method adaptively achieves intra-class compactness and inter-class separation, effectively extracting relevant facial information. Zhang et al. [<xref rid="B13-sensors-24-06748" ref-type="bibr">13</xref>] proposed a novel method of rebalancing attention mapping, which regularizes the model to extract transformation-invariant information from minor classes within the training samples. Additionally, they introduced rebalanced smooth labels to adjust the cross-entropy loss, leveraging the label distribution in imbalanced data to improve the model&#x02019;s performance in imbalanced facial expression recognition (FER) tasks. Yu et al. [<xref rid="B14-sensors-24-06748" ref-type="bibr">14</xref>] addressed the limitations of FER dataset size and class imbalance by generating pseudo-labels through semi-supervised learning, employing uniform sampling, and introducing temporal encoders. These strategies significantly improved the accuracy of facial expression recognition.</p><p>Although the current facial expression recognition methods based on deep learning have achieved certain results, there are still some shortcomings: (1) existing models ignore the problem of scale variation, which leads to the possibility of losing some key information when extracting facial features, thus misclassifying similar expressions at different scales as different expressions; (2) the extraction of deep fine-grained features is insufficient, making it difficult to distinguish expressions with subtle differences; (3) previous methods mainly rely on the overall or relatively rough local analysis, which only capture limited features and are easily affected by external factors, thereby reducing the accuracy and robustness of recognition.</p><p>To address the above issues, inspired by multi-scale feature extraction [<xref rid="B15-sensors-24-06748" ref-type="bibr">15</xref>] and attention mechanisms [<xref rid="B16-sensors-24-06748" ref-type="bibr">16</xref>], we propose a student facial expression recognition model based on multi-scale and deep fine-grained feature attention enhancement. The model captures and fuses different scales of facial information through the constructed multi-scale dual-pooling feature aggregation module to obtain a comprehensive representation of key facial features. Additionally, a key region-oriented attention mechanism is designed to focus on the nuances of facial expressions to further enhance the multi-scale deep fine-grained feature representation. Finally, the fusion of multi-scale and deep fine-grained attention-enhanced features is used to obtain richer and more accurate facial key information and achieve accurate facial expression recognition.</p><p>The main contributions of this paper are as follows:<list list-type="order"><list-item><p>A new multi-scale dual-pooling feature aggregation module is constructed, which captures and fuses facial information at different scales, thereby obtaining a comprehensive representation of key facial features.</p></list-item><list-item><p>A key region-oriented attention mechanism is designed, which enhances the ability to capture fine-grained features by directing key features, aiming to focus more on the nuances of facial expressions and less on the common areas of the face as a way to reduce the common interferences and thus improve the accuracy of recognition.</p></list-item><list-item><p>A smart classroom facial expression dataset (SCFED) during learning is created, and a large number of experiments are conducted on this dataset and four other public datasets, FER2013, FERPlus, AffectNet, and RAF-DB. The experimental results prove that the SFER-MDFAE proposed in this paper has certain advantages over the state-of-the-art facial expression recognition methods.</p></list-item></list></p><p>The rest of the paper is organized as follows: <xref rid="sec2-sensors-24-06748" ref-type="sec">Section 2</xref> briefly describes the related work of this paper. <xref rid="sec3-sensors-24-06748" ref-type="sec">Section 3</xref> describes the proposed method in detail. <xref rid="sec4-sensors-24-06748" ref-type="sec">Section 4</xref> shows the datasets and the comparison and analysis of experimental performance. <xref rid="sec5-sensors-24-06748" ref-type="sec">Section 5</xref> summarizes the work and gives an outlook.</p></sec><sec id="sec2-sensors-24-06748"><title>2. Related Work</title><sec id="sec2dot1-sensors-24-06748"><title>2.1. Facial Expression Recognition Based on Attention Mechanisms</title><p>With the development of computer vision technology, many researchers have begun to conduct facial expression recognition research in the field of deep learning. Jeong et al. [<xref rid="B17-sensors-24-06748" ref-type="bibr">17</xref>] applied 3D convolution to extract spatial&#x02013;temporal features, selected 23 facial landmarks to represent the movement of facial muscles, and finally used the designed joint classifier to output facial expression recognition results. Li et al. [<xref rid="B18-sensors-24-06748" ref-type="bibr">18</xref>] proposed a novel method for facial cropping and rotation to preprocess input images and simplified the convolutional neural network, thereby improving the accuracy and speed of facial feature extraction. Fard et al. [<xref rid="B19-sensors-24-06748" ref-type="bibr">19</xref>] proposed an Adaptive Correlation (Ad-Corre) loss, which guides the network to generate embedded feature vectors with high intra-class correlation and low inter-class correlation, thereby effectively enhancing the discriminative power of feature representations. Vignesh et al. [<xref rid="B20-sensors-24-06748" ref-type="bibr">20</xref>] proposed a unique convolutional neural network that suppresses redundant information by embedding U-Net between VGG layers, allowing the model to extract more critical features.</p><p>The above studies mainly extracted expression features through a large amount of training data, without considering the varying importance of different facial regions in facial expression recognition. To address this limitation, researchers have incorporated attention mechanisms into facial expression recognition. Minaee et al. [<xref rid="B21-sensors-24-06748" ref-type="bibr">21</xref>] proposed an approach based on attentional convolutional networks to capture facial key region features. Tang et al. [<xref rid="B22-sensors-24-06748" ref-type="bibr">22</xref>] found that different facial regions contribute differently to expression recognition, and therefore proposed an approach combining bidirectional gated recurrent units (BiGRUs) with an attention mechanism. The BiGRUs model long-range dependencies of the region feature maps obtained using the window cropping strategy, while the attention mechanism adaptively adjusts the weight of each region, achieving precise facial expression recognition. Yu et al. [<xref rid="B23-sensors-24-06748" ref-type="bibr">23</xref>] introduced the Channel Collaborative Attention Module (CCAM) and Spatial Collaborative Attention Module (SCAM) to accurately capture key facial expression features. Zheng et al. [<xref rid="B16-sensors-24-06748" ref-type="bibr">16</xref>] proposed a cross-attention mechanism where landmark features and image features serve as mutual queries, reducing the attention to the common facial regions and highlighting the facial regions of interest. Although these methods have demonstrated excellent performance in facial expression recognition, they still fall short in extracting local fine-grained features in complex scenarios, leading to difficulties in distinguishing different classes of expressions with subtle differences. Consequently, improving the model&#x02019;s ability to capture fine-grained features in complex environments remains an important direction for future research.</p></sec><sec id="sec2dot2-sensors-24-06748"><title>2.2. Facial Expression Recognition Based on Multi-Scale</title><p>Multi-scale facial expression feature extraction plays an important role in facial expression recognition. Fan et al. [<xref rid="B24-sensors-24-06748" ref-type="bibr">24</xref>] systematically extracted kernel-scale information, network-scale information, and knowledge-scale information to enhance the representation of facial features of interest. Wen et al. [<xref rid="B25-sensors-24-06748" ref-type="bibr">25</xref>] introduced a multi-head attention mechanism consisting of spatial and channel attention to address the limitations of a single attention head in simultaneously focusing on different facial regions. Here, spatial attention is used to capture features at multiple scales, and combining spatial attention with channel attention highlights subtle differences between expressions and reduces common interference, which leads to a more accurate categorization of expressions. Zhao et al. [<xref rid="B26-sensors-24-06748" ref-type="bibr">26</xref>] utilized a multi-scale module to fuse features from different receptive fields, effectively reducing the sensitivity of deep convolution to pose variations. Karnati et al. [<xref rid="B27-sensors-24-06748" ref-type="bibr">27</xref>] analyzed texture and residual features at different scales during feature extraction to capture more comprehensive facial expression information, which enhances the accuracy and robustness of facial expression recognition. Mao et al. [<xref rid="B15-sensors-24-06748" ref-type="bibr">15</xref>] directly extracted three-layer multi-scale landmark features and three-layer multi-scale image features from the facial landmark detectors and image backbone, and added a small visual transformer network to fuse these features. The methods for extracting and fusing multi-scale features mentioned above effectively address the issue of scale variation. Consequently, in this paper, we propose a multi-scale dual-pooling feature aggregation method to extract and fuse key facial information at different scales, thereby enhancing the model&#x02019;s recognition performance and robustness in various environments.</p></sec></sec><sec id="sec3-sensors-24-06748"><title>3. SFER-MDFAE Model</title><p>The overall framework of the SFER-MDFAE model is shown in <xref rid="sensors-24-06748-f001" ref-type="fig">Figure 1</xref>. It mainly consists of four parts: the IR50 [<xref rid="B28-sensors-24-06748" ref-type="bibr">28</xref>] backbone network, the multi-scale dual-pooling feature aggregation module (MDPFA), the key region-oriented attention feature enhancement module (KROAFE), and feature fusion. First, the given facial image is input into the IR50 backbone network to extract features at four different scales; second, these multi-scale features are input into the MDPFA module to obtain multi-scale channel attention aggregation features, which represent richer global semantic information and can improve the robustness of the model; then, the fourth layer of multi-scale features is input into the KROAFE module, where a key region-oriented attention mechanism captures high-dimensional deep fine-grained features, focusing on the nuances of facial expressions and reducing common interference; finally, the multi-scale channel attention aggregation features and the deep fine-grained features are fused to obtain richer and more accurate facial expression features, which are then input into the fully connected layer for facial expression classification.</p><sec id="sec3dot1-sensors-24-06748"><title>3.1. Multi-Scale Feature Extraction</title><p>Existing facial expression recognition models mainly extract facial features at a single scale, neglecting factors such as lighting variations, viewpoint diversity, and distance differences between the target object and the camera, which can lead to the difficulty of the model in effectively capturing facial expression features at a single scale, thereby reducing the accuracy of facial expression recognition. To address the above issues, we employ IR50 as the backbone network to extract multi-scale features from the input images. The IR50 backbone network is composed of multiple different convolutional layers and pooling layers corresponding to different scales. Among them, convolution kernels of varying sizes have different receptive fields. Smaller convolution kernels mainly extract shallow image edge and texture information, while larger convolution kernels can capture a broader range of contextual information, thereby extracting deeper semantic features. Through this multi-scale feature extraction mechanism, IR50 can capture facial expression information at different layers. In addition, we propose two improvements based on related work [<xref rid="B15-sensors-24-06748" ref-type="bibr">15</xref>,<xref rid="B16-sensors-24-06748" ref-type="bibr">16</xref>,<xref rid="B29-sensors-24-06748" ref-type="bibr">29</xref>]. First, the MobileNet backbone network used for extracting multi-scale landmark features is removed, retaining only the IR50 single backbone network. This avoids the problem of inaccurate landmark feature localization, further improves the accuracy, and simplifies the model structure. Second, multi-scale features&#x000a0;<inline-formula>
<mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;are extracted from the images using the IR50 backbone network. In particular, the fourth-layer high-dimensional feature&#x000a0;<inline-formula>
<mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;is retained to better represent the deep semantic information of facial expressions and to further explore the subtle differences in facial expressions. Therefore, in the subsequent work, the multiscale features&#x000a0;<inline-formula>
<mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;are fed into the MDPFA module for extracting multiscale channel attention aggregation features, and the fourth-layer high-dimensional feature&#x000a0;<inline-formula>
<mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;is fed into the KROAFE module to capture high-dimensional deep fine-grained attention-enhanced features.</p></sec><sec id="sec3dot2-sensors-24-06748"><title>3.2. Multi-Scale Dual-Pooling Feature Aggregation Module</title><p>To effectively capture comprehensive facial key features, we design a multi-scale dual-pooling feature aggregation module. The module mainly consists of two parts: channel attention feature selection and channel attention feature aggregation, which can effectively extract and fuse facial key information at different scales. This design significantly enhances the model&#x02019;s recognition performance and robustness in different environments, thereby improving the accuracy and reliability of facial expression recognition.</p><sec id="sec3dot2dot1-sensors-24-06748"><title>3.2.1. Channel Attention Feature Selection</title><p>The multi-scale features&#x000a0;<inline-formula>
<mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;obtained by the IR50 backbone network still contain irrelevant or redundant information unrelated to facial expressions, and direct aggregation of these multiscale features will lead to an excessive number of features, which is not conducive to the processing of subsequent prediction layers. Therefore, a channel attention mechanism (CA) has been introduced. This mechanism dynamically adjusts the weights of channels to effectively capture shallow image edges, texture information, and deep semantic information across different scale features. By selectively extracting the most representative and effective expression information from each channel while minimizing information loss, it enhances the model&#x02019;s feature representation capabilities and overall performance. The structure of the channel attention mechanism is shown in <xref rid="sensors-24-06748-f002" ref-type="fig">Figure 2</xref>.</p><p>Firstly, global maximum pooling and global average pooling are applied to the input facial expression features&#x000a0;<inline-formula>
<mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;at different scales to extract the most significant features and global average features in each channel; next, the features of each channel and their importance are learned through a shared multilayer perceptron (Shared MLP); then, the outputs of Shared MLP are summed up, and then the result after summing is mapped using the sigmoid activation function to obtain the channel attention weights&#x000a0;<inline-formula>
<mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>, as shown in Equation (1).
<disp-formula id="FD1-sensors-24-06748">
<label>(1)</label>
<mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula></p><p>Here,&#x000a0;<inline-formula>
<mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to maximum average pooling and&#x000a0;<inline-formula>
<mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to global average pooling.</p><p>Finally, the input feature&#x000a0;<inline-formula>
<mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;is multiplied by the channel attention weight&#x000a0;<inline-formula>
<mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;to obtain the channel attention selection feature&#x000a0;<inline-formula>
<mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>, as shown in Equation (2).
<disp-formula id="FD2-sensors-24-06748">
<label>(2)</label>
<mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</disp-formula></p></sec><sec id="sec3dot2dot2-sensors-24-06748"><title>3.2.2. Channel Attention Feature Aggregation</title><p>The multi-scale features&#x000a0;<inline-formula>
<mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;extracted from the IR50 backbone network are processed through the channel attention mechanism to obtain the channel attention selection features&#x000a0;<inline-formula>
<mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>. Subsequently, these features are individually fed into the global average pooling layer, which compresses the spatial dimensions of each channel into a single value; that is, the global average of each channel is computed to obtain&#x000a0;<inline-formula>
<mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>. This is carried out to capture the global information of the entire image and reduces the spatial dimensions of the feature maps, thereby reducing the computational and parametric quantities of the model. To better leverage the key shallow image edges, texture features, and the deep semantic features from these different scale representations, a vector concatenation method is employed for their integration. This process generates a multi-scale channel attention aggregation feature&#x000a0;<inline-formula>
<mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;that simultaneously captures both global visual information and fine-grained visual details, as illustrated in Equation (3).
<disp-formula id="FD3-sensors-24-06748">
<label>(3)</label>
<mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula></p><p>Here,&#x000a0;<inline-formula>
<mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to vector concatenation.</p></sec></sec><sec id="sec3dot3-sensors-24-06748"><title>3.3. Key Region-Oriented Attention Feature Enhancement Module</title><p>In order to focus on the nuances of facial expressions in a more detailed way, reduce the focus on the common areas of the face, and reduce the impact of common interference on facial expression recognition, we propose a key region-oriented attention feature enhancement module. Unlike the related work [<xref rid="B15-sensors-24-06748" ref-type="bibr">15</xref>], this paper eliminates the MobileNet backbone network. The reason for this removal is that the facial landmark features extracted by the MobileNet backbone are sometimes inaccurate, particularly in smart classroom settings where uneven lighting and diverse viewpoints can significantly affect the ability of the attention mechanism to capture crucial facial region information. Therefore, instead of using the landmark features extracted by MobileNet as prior knowledge for attention feature fusion with multi-scale facial features, the designed key region-oriented attention feature enhancement module is innovatively applied to the deep fine-grained feature&#x000a0;<inline-formula>
<mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>. The deep feature map&#x000a0;<inline-formula>
<mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;contains richer deep semantic information while reducing interference from shallow noise. This allows the key region-oriented attention mechanism (KROA) to focus more efficiently on fine-grained feature areas, thereby enhancing its ability to capture these fine-grained features. The design not only improves the accuracy of the model but also significantly reduces the computation and memory consumption of the model. The specific structure of KROA is shown in <xref rid="sensors-24-06748-f003" ref-type="fig">Figure 3</xref>.</p><p>Traditional attention mechanisms need to compute pairwise token interactions across all spatial locations, which generates a huge computational burden and memory usage; sparse attention mechanisms restrict attention operations to local windows [<xref rid="B30-sensors-24-06748" ref-type="bibr">30</xref>], extended windows [<xref rid="B31-sensors-24-06748" ref-type="bibr">31</xref>], and axially striped regions [<xref rid="B32-sensors-24-06748" ref-type="bibr">32</xref>], which reduces computation and memory usage, but the key regions are not selected in a sufficiently accurate and flexible manner. In contrast to these methods, the key region-oriented attention mechanism dynamically selects the&#x000a0;<inline-formula>
<mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;regions that are the most closely associated with each area and applies a multi-head self-attention mechanism among these regions. This approach enables more efficient extraction of critical facial expression features. The specific implementation method is as follows:</p><p><bold>Linear mapping:</bold> For each patch in the input feature map&#x000a0;<inline-formula>
<mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;(<inline-formula>
<mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:math>
</inline-formula>, and&#x000a0;<inline-formula>
<mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:math>
</inline-formula>&#x000a0;refer to the number of channels, height, and width of the input feature map, respectively), linear transformations are applied to the queries, keys, and values, as shown in Equations (4)&#x02013;(6).
<disp-formula id="FD4-sensors-24-06748">
<label>(4)</label>
<mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math>
</disp-formula>
<disp-formula id="FD5-sensors-24-06748">
<label>(5)</label>
<mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math>
</disp-formula>
<disp-formula id="FD6-sensors-24-06748">
<label>(6)</label>
<mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math>
</disp-formula></p><p>Here,&#x000a0;<inline-formula>
<mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to the <italic toggle="yes">i</italic>-th patch;&#x000a0;<inline-formula>
<mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to the <italic toggle="yes">l</italic>-th attention head;&#x000a0;<inline-formula>
<mml:math id="mm4440" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refer to the query, key, and value matrices of each patch; and&#x000a0;<inline-formula>
<mml:math id="mm4441" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refer to the weight matrices of&#x000a0;<inline-formula>
<mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;, respectively.</p><p><bold>Region division and mean calculation:</bold> The feature map is divided into regions, and region-level queries and keys are computed based on all patches within each region. Specifically, the input feature map&#x000a0;<inline-formula>
<mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;is divided into&#x000a0;<inline-formula>
<mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;non-overlapping regions so that each region contains&#x000a0;<inline-formula>
<mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>/</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;vectors and the mean values of the query and key of all patches in each region are used as the query and key of that region, as shown in Equations (7) and (8).
<disp-formula id="FD7-sensors-24-06748">
<label>(7)</label>
<mml:math id="mm45" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math>
</disp-formula>
<disp-formula id="FD8-sensors-24-06748">
<label>(8)</label>
<mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math>
</disp-formula></p><p>Here, <italic toggle="yes">j</italic> refers to the <italic toggle="yes">j</italic>-th region and&#x000a0;<inline-formula>
<mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to the query and key of the <italic toggle="yes">j</italic>-th region.&#x000a0;<inline-formula>
<mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;represents the number of patches in each region.</p><p><bold>Correlation calculation and key region selection:</bold> Different from the traditional sparse attention mechanism, this attention mechanism is more flexible and accurate for the selection of key regions. Specifically, a matrix multiplication operation is performed between the queries of each region and the key values of all other regions to calculate the degree of association&#x000a0;<inline-formula>
<mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;between the various regions. The&#x000a0;<inline-formula>
<mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;regions with the highest degrees of association are retained for each region using the row indexing operator&#x000a0;<inline-formula>
<mml:math id="mm500" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">topIndex</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>, as shown in Equations (9) and (10).
<disp-formula id="FD9-sensors-24-06748">
<label>(9)</label>
<mml:math id="mm511" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi mathvariant="italic">al</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi mathvariant="italic">al</mml:mi><mml:mi>j</mml:mi></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math>
</disp-formula>
<disp-formula id="FD10-sensors-24-06748">
<label>(10)</label>
<mml:math id="mm51" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula></p><p>In the equations,&#x000a0;<inline-formula>
<mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to the matrix transposition and&#x000a0;<inline-formula>
<mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x02115;</mml:mo><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;represents the index of the&#x000a0;<inline-formula>
<mml:math id="mm501" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;most relevant regions for any query region. For example, the <italic toggle="yes">j</italic>-th row of&#x000a0;<inline-formula>
<mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x02115;</mml:mo><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;contains the indices of the&#x000a0;<inline-formula>
<mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;regions that are the most relevant to the <italic toggle="yes">j</italic>-th region, where&#x000a0;<inline-formula>
<mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02264;</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>.</p><p><bold>Patch-to-patch attention:</bold> After obtaining the index matrix&#x000a0;<inline-formula>
<mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x02115;</mml:mo><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>, each query patch in any query region&#x000a0;<inline-formula>
<mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math>
</inline-formula>&#x000a0;only focuses on all the patches within the&#x000a0;<inline-formula>
<mml:math id="mm60" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;regions indexed by&#x000a0;<inline-formula>
<mml:math id="mm61" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x02115;</mml:mo><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>. Therefore, multi-head self-attention is performed only between all patches in region&#x000a0;<inline-formula>
<mml:math id="mm62" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math>
</inline-formula>&#x000a0;and the key&#x02013;value pairs within these&#x000a0;<inline-formula>
<mml:math id="mm63" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;related regions. The detailed process is shown in Equations (11)&#x02013;(14).
<disp-formula id="FD11-sensors-24-06748">
<label>(11)</label>
<mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</disp-formula>
<disp-formula id="FD12-sensors-24-06748">
<label>(12)</label>
<mml:math id="mm665" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="italic">head</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">Attention</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mi mathvariant="italic">il</mml:mi><mml:mi>q</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msup><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi mathvariant="italic">il</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msup><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi mathvariant="italic">il</mml:mi><mml:mi>q</mml:mi></mml:msubsup></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>
</disp-formula>
<disp-formula id="FD13-sensors-24-06748">
<label>(13)</label>
<mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>H</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="italic">il</mml:mi></mml:msub><mml:msup><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi mathvariant="italic">il</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup><mml:msup><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="italic">il</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math>
</disp-formula>
<disp-formula id="FD14-sensors-24-06748">
<label>(14)</label>
<mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>H</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula></p><p>Here,&#x000a0;<inline-formula>
<mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to the single-head attention function and&#x000a0;<inline-formula>
<mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;represent all the keys and values collected from the&#x000a0;<inline-formula>
<mml:math id="mm559" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;regions, respectively.&#x000a0;<inline-formula>
<mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to the normalization function,&#x000a0;<inline-formula>
<mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to a scaling factor to prevent the gradient from disappearing,&#x000a0;<inline-formula>
<mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to the output attention score of the <italic toggle="yes">l</italic>-th attention head, and&#x000a0;<inline-formula>
<mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;and&#x000a0;<inline-formula>
<mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msubsup></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;represent the weight matrices for&#x000a0;<inline-formula>
<mml:math id="mm772" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;and&#x000a0;<inline-formula>
<mml:math id="mm773" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>, respectively.&#x000a0;<inline-formula>
<mml:math id="mm74" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>H</mml:mi><mml:mi>S</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to the output multi-head self-attention value;&#x000a0;<inline-formula>
<mml:math id="mm75" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;signifies vector concatenation, which aggregates the output attention scores from all attention heads to obtain the final attention score;&#x000a0;<inline-formula>
<mml:math id="mm76" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;is the weight matrix;&#x000a0;<inline-formula>
<mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to the output of the&#x000a0;<inline-formula>
<mml:math id="mm777" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;after applying the key region-oriented attention mechanism, and&#x000a0;<inline-formula>
<mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;[<xref rid="B33-sensors-24-06748" ref-type="bibr">33</xref>] refers to the local position enhancement encoding, which is essentially a deep downsampling convolution with a convolution kernel of 5.</p><p>After obtaining the attention feature&#x000a0;<inline-formula>
<mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>, it is fed into a global average pooling layer, which aims to compress the spatial dimensions of each channel into a single value, that is, to compute the global average of each channel and thus extract the global average feature vector. Subsequently, the pooled features are flattened into a one-dimensional feature vector&#x000a0;<inline-formula>
<mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>, which represents the deep fine-grained attention-enhanced feature.</p></sec><sec id="sec3dot4-sensors-24-06748"><title>3.4. Feature Fusion</title><p>The multi-scale channel attention aggregation feature&#x000a0;<inline-formula>
<mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;and the deep fine-grained attention-enhanced feature&#x000a0;<inline-formula>
<mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;are fused through vector concatenation. This fusion strategy not only obtains a comprehensive representation of the facial key features but also pays more attention to the nuances of different classes of expressions, thus obtaining richer and more accurate facial key information&#x000a0;<inline-formula>
<mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>, as shown in Equation (15).
<disp-formula id="FD15-sensors-24-06748">
<label>(15)</label>
<mml:math id="mm84" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula></p><p>Here,&#x000a0;<inline-formula>
<mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;refers to vector concatenation.</p><p><bold>Algorithm 1 illustrates the basic steps of SFER-MDFAE.</bold>
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm&#x000a0;1:</bold> SFER-MDFAE Algorithm</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Input:</bold>&#x000a0;Each batch contains 64 facial images from a smart classroom, with each image having a size of&#x000a0;<inline-formula>
<mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>224</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>224</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>.<break/><bold>Output:</bold> Predict the probability distribution for each image&#x02019;s facial expression category (happy, dedicated, neutral, confused, tired, bored).
<list list-type="simple"><list-item><p>&#x02003;&#x02003;&#x000a0;&#x000a0;1: Initialize the model architecture and its parameters.</p></list-item><list-item><p>&#x02003;&#x02003;&#x000a0;&#x000a0;2: <bold>While</bold> SFER-MDFAE Not Convergence <bold>do</bold>:</p></list-item><list-item><p>&#x02003;&#x02003;&#x000a0;&#x000a0;3:&#x02003;&#x02003;&#x02003;Use the pre-trained IR50 as the backbone network to extract multi-scale features, namely&#x000a0;<inline-formula>
<mml:math id="mm877" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm88" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>, and&#x000a0;<inline-formula>
<mml:math id="mm8999" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>.</p></list-item><list-item><p>&#x02003;&#x02003;&#x000a0;&#x000a0;4:&#x02003;&#x02003;&#x02003;<bold>While</bold>&#x000a0;<inline-formula>
<mml:math id="mm991" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x0003c;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;<bold>do</bold>:</p></list-item><list-item><p>&#x02003;&#x02003;&#x000a0;&#x000a0;5:&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;For each layer of the multi-scale feature&#x000a0;<inline-formula>
<mml:math id="mm992" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>, selectively extract the most representative and effective facial expression information using the channel attention mechanism, resulting in the channel attention selection feature&#x000a0;<inline-formula>
<mml:math id="mm993" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>.</p></list-item><list-item><p>&#x02003;&#x02003;&#x000a0;&#x000a0;6:&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;Perform global average pooling on the&#x000a0;<inline-formula>
<mml:math id="mm994" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;to reduce the spatial dimensions of the channel, obtaining&#x000a0;<inline-formula>
<mml:math id="mm9994" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>.</p></list-item><list-item><p>&#x02003;&#x02003;&#x000a0;&#x000a0;7:&#x02003;&#x02003;&#x02003;<bold>end while</bold></p></list-item><list-item><p>&#x02003;&#x02003;&#x000a0;&#x000a0;8:&#x02003;&#x02003;&#x02003;Fuse the&#x000a0;<inline-formula>
<mml:math id="mm995" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm996" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm997" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>, and&#x000a0;<inline-formula>
<mml:math id="mm9978" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;using vector concatenation to obtain the multi-scale channel attention aggregation feature&#x000a0;<inline-formula>
<mml:math id="mm998" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>.</p></list-item><list-item><p>&#x02003;&#x02003;&#x000a0;&#x000a0;9:&#x02003;&#x02003;&#x02003;Linearly map the high-dimensional feature&#x000a0;<inline-formula>
<mml:math id="mm979" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;to queries, keys, and values, and perform down-sampling on values to obtain the local position enhancement feature.</p></list-item><list-item><p>&#x02003;&#x02003;10:&#x02003;&#x02003;&#x02003;Calculate the similarity&#x000a0;<inline-formula>
<mml:math id="mm99" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math>
</inline-formula>&#x000a0;between each query and key, and select the&#x000a0;<inline-formula>
<mml:math id="mm10011" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;key regions index&#x000a0;<inline-formula>
<mml:math id="mm100999" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;based on the similarity.</p></list-item><list-item><p>&#x02003;&#x02003;11:&#x02003;&#x02003;&#x02003;Collect the&#x000a0;<inline-formula>
<mml:math id="mm101" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;key&#x02013;value pairs corresponding to the regions indicated by&#x000a0;<inline-formula>
<mml:math id="mm1000" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>.</p></list-item><list-item><p>&#x02003;&#x02003;12:&#x02003;&#x02003;&#x02003;Use the multi-head attention mechanism to extract the key features and overlay the local positional enhancement features to finally obtain the attention feature&#x000a0;<inline-formula>
<mml:math id="mm103" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>.</p></list-item><list-item><p>&#x02003;&#x02003;13:&#x02003;&#x02003;&#x02003;Perform global average pooling and flattening operations on&#x000a0;<inline-formula>
<mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;to obtain the deep fine-grained feature &#x000a0;<inline-formula>
<mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>.</p></list-item><list-item><p>&#x02003;&#x02003;14:&#x02003;&#x02003;&#x02003;Fuse the extracted&#x000a0;<inline-formula>
<mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;and&#x000a0;<inline-formula>
<mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;features to obtain richer facial key information&#x000a0;<inline-formula>
<mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>.</p></list-item><list-item><p>&#x02003;&#x02003;15:&#x02003;&#x02003;&#x02003;Map the&#x000a0;<inline-formula>
<mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>;&#x000a0;to the output space through a fully connected (FC) layer to obtain the probabilities for each class.</p></list-item><list-item><p>&#x02003;&#x02003;16: Compute the loss using the cross-entropy loss function and optimize the model using the gradient descent algorithm.</p></list-item><list-item><p>&#x02003;&#x02003;17: <bold>end while</bold></p></list-item></list>
</td></tr></tbody></array></p></sec></sec><sec id="sec4-sensors-24-06748"><title>4. Experiments</title><sec id="sec4dot1-sensors-24-06748"><title>4.1. Datasets</title><p>To validate the advancement and effectiveness of the proposed SFER-MDFAE model, experiments are conducted on four classical facial expression datasets and one real smart classroom expression dataset, namely, the FER2013 [<xref rid="B34-sensors-24-06748" ref-type="bibr">34</xref>], FERPlus [<xref rid="B35-sensors-24-06748" ref-type="bibr">35</xref>], RAF-DB [<xref rid="B36-sensors-24-06748" ref-type="bibr">36</xref>], AffectNet [<xref rid="B37-sensors-24-06748" ref-type="bibr">37</xref>], and SCFEDs, respectively.</p><p>FER2013: The FER2013 dataset contains seven basic expressions (angry, disgust, fear, happy, sad, surprise, and neutral), with a total of 35,887 facial expression images, of which 28,709 are used for training, 3589 for validation, and 3589 for testing.</p><p>FERPlus: FERPlus is extended from FER2013 and reclassifies FER2013 into eight basic expressions (anger, contempt, disgust, fear, happy, neutral, sad, and surprise), which is more accurate and reasonable than FER2013. There are 31,341 facial expression images, of which 28,085 are used for training and 3256 for validation.</p><p>RAF-DB: The RAF-DB dataset contains seven basic expressions (surprise, fear, disgust, happy, sad, anger, and neutral), with a total of 15,339 facial expression images, of which 12,271 are used for training and 3068 for validation.</p><p>AffectNet: AffectNet is currently the largest publicly available facial expression dataset. AffectNet-7 includes seven basic expressions (neutral, happy, sad, surprise, fear, disgust, and anger), comprising a total of 287,401 facial images. Of these, 283,901 images are used for training, and 3500 are reserved for validation.</p><p>SCFED: The SCFED is captured using a SONY AX60 HD camera in a smart classroom scenario, containing six basic expressions (happy, dedicated, neutral, confused, tired, and bored), with a total of 36,226 facial expression images, of which 28,979 are used for training, 3619 for validation, and 3628 for testing. Specifically, for the high-resolution sequential images of students collected in a smart classroom environment, we employed the MTCNN (Multi-task Convolutional Neural Network) [<xref rid="B38-sensors-24-06748" ref-type="bibr">38</xref>] algorithm for face detection and cropping, thereby obtaining individual facial images of the students. The expression labels were divided into six categories: happy, dedicated, neutral, confused, tired, and bored. During the annotation process, we invited several experts in education, psychology, and experienced teachers to manually label the facial images. If a particular expression label was agreed upon by more than 50% of the annotators, the image was classified under that label. Ultimately, we successfully constructed the SCFED.</p></sec><sec id="sec4dot2-sensors-24-06748"><title>4.2. Baseline Models and Evaluation Metrics</title><p>In order to evaluate the performance of the proposed SFER-MDFAE model, accuracy (ACC) is used as the evaluation metric, while the complexity of the model is assessed based on the number of parameters and computational cost, and the SFER-MDFAE model is compared with five state-of-the-art baseline models on five datasets, as detailed in <xref rid="sensors-24-06748-t001" ref-type="table">Table 1</xref>.</p></sec><sec id="sec4dot3-sensors-24-06748"><title>4.3. Experimental Environment and Experimental Setup</title><p>The experimental environment of this paper is shown in <xref rid="sensors-24-06748-t002" ref-type="table">Table 2</xref>.</p><p>The SFER-MDFAE model proposed in this paper uses different parameter settings for five different datasets.</p><p>FER2013: The learning rate is set to 1 &#x000d7; 10<sup>&#x02212;5</sup>, the regularization coefficient is set to 1 &#x000d7; 10<sup>&#x02212;4</sup>, the momentum is set to 0.9, and the batch size is set to 64.</p><p>FERPlus: The learning rate is set to 1 &#x000d7; 10<sup>&#x02212;5</sup>, the regularization coefficient is set to 1 &#x000d7; 10<sup>&#x02212;4</sup>, the momentum is set to 0.9, and the batch size is set to 100.</p><p>RAF-DB: The learning rate is set to 3 &#x000d7; 10<sup>&#x02212;5</sup>, the regularization coefficient is set to 1 &#x000d7; 10<sup>&#x02212;4</sup>, the momentum is set to 0.9, and the batch size is set to 64.</p><p>AffectNet: The learning rate is set to 2 &#x000d7; 10<sup>&#x02212;6</sup>, the regularization coefficient is set to 5 &#x000d7; 10<sup>&#x02212;5</sup>, the momentum is set to 0.9, and the batch size is set to 64.</p><p>SCFED: The learning rate is set to 1 &#x000d7; 10<sup>&#x02212;5</sup>, the regularization coefficient is set to 1 &#x000d7; 10<sup>&#x02212;4</sup>, the momentum is set to 0.9, and the batch size is set to 64.</p><p>Similar to POSTER [<xref rid="B16-sensors-24-06748" ref-type="bibr">16</xref>], we also utilize the IR50 network pre-trained on the Ms-Celeb-1M [<xref rid="B42-sensors-24-06748" ref-type="bibr">42</xref>] dataset as the backbone network. Gradient descent optimization is performed on all five datasets using the Adam optimizer with random level flipping and random erasure during training [<xref rid="B43-sensors-24-06748" ref-type="bibr">43</xref>]. The number of&#x000a0;<inline-formula>
<mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;regions retaining the maximum degree of the association is set to 15. In addition, to prevent overfitting, an early stopping strategy is used. For the FER2013, FERPlus, RAF-DB, and SCFEDs, training is stopped if the accuracy on the validation set does not improve for 50 consecutive epochs. For the AffectNet dataset, training is stopped if there is no improvement in validation accuracy for 12 consecutive epochs.</p></sec><sec id="sec4dot4-sensors-24-06748"><title>4.4. Experiments and Analysis of Results</title><p>The accuracy curves of the validation sets for each model on the FER2013, FERPlus, RAF-DB, AffectNet, and SCFEDs are shown in <xref rid="sensors-24-06748-f004" ref-type="fig">Figure 4</xref>a&#x02013;e.</p><p>As can be seen from <xref rid="sensors-24-06748-f004" ref-type="fig">Figure 4</xref>a&#x02013;e, for both the FER2013 dataset and SCFED, the performance of the SFER-MDFAE model proposed in this paper outperforms the other baseline models after the 15th epoch of training and reaches convergence at the 30th epoch. For the FERPlus dataset, the SFER-MDFAE model outperforms the other baseline models after the 25th epoch of training and reaches convergence at the 30th epoch. For the RAF-DB dataset, the SFER-MDFAE model outperforms the other baseline models and reaches convergence after the 25th epoch of training. For the AffectNet dataset, the SFER-MFAE model outperforms the other baseline models after the fifth epoch of training and reaches convergence by the tenth epoch. It can be seen that the SFER-MDFAE model proposed in this paper shows fast convergence speed and excellent performance on the FER2013, FERPlus, RAF-DB, AffectNet, and SCFEDs, which indicates that the design and training strategy of the model has significant advantages, and is able to learn effective facial expression features in a shorter period of time with good generalization performance.</p><p>The performance metrics of the SFER-MDFAE model on the FER2013, FERPlus, RAF-DB, AffectNet, and SCFEDs, compared with other baseline models, are presented in <xref rid="sensors-24-06748-t003" ref-type="table">Table 3</xref>. The computational complexity and the number of parameters for each model are shown in <xref rid="sensors-24-06748-t004" ref-type="table">Table 4</xref>. The detailed analysis is as follows:</p><list list-type="order"><list-item><p>As can be seen from <xref rid="sensors-24-06748-t003" ref-type="table">Table 3</xref>, the SFER-MDFAE model proposed in this paper significantly outperforms other existing methods on the FER2013 dataset. Specifically, the SFER-MDFAE achieves an accuracy of 76.18% on the FER2013 dataset, which represents improvements of 3.40%, 2.40%, 4.15%, 1.31%, and 1.45% over the EAC (72.78%), EmoNeXt (73.78%), DDAMFN (72.03%), POSTER (74.87%), and POSTER++ (74.73%) methods, respectively. These results indicate that despite the presence of some mislabeled data in the FER2013 dataset, the SFER-MDFAE model is still capable of achieving superior experimental outcomes.</p></list-item><list-item><p>As shown in <xref rid="sensors-24-06748-t003" ref-type="table">Table 3</xref>, the SFER-MDFAE model proposed in this paper significantly outperforms other existing methods on the FERPlus dataset. Specifically, the SFER-MDFAE achieves an accuracy of 92.75% on the FERPlus dataset, representing improvements of 2.77%, 2.52%, 2.30%, 1.20%, and 0.83% over the EAC (89.98%), EmoNeXt (90.23%), DDAMFN (90.45%), POSTER (91.55%), and POSTER++ (91.92%) methods, respectively. Overall, the SFER-MDFAE model proposed in this paper shows a significant improvement over the state-of-the-art POSTER++ model, further demonstrating the superiority of our approach.</p></list-item><list-item><p>As can be seen from <xref rid="sensors-24-06748-t003" ref-type="table">Table 3</xref>, the SFER-MDFAE model achieves an accuracy of 92.93% on the RAF-DB dataset, significantly outperforming other state-of-the-art methods. Specifically, the SFER-MDFAE model improves upon the EAC (90.16%), EmoNeXt (87.25%), DDAMFN (90.97%), POSTER (92.14%), and POSTER++ (92.17%) methods by 2.77%, 5.68%, 1.96%, 0.79%, and 0.76%, respectively. Compared to the FER2013 and FERPlus datasets, the RAF-DB dataset has a different distribution and quantity of facial expressions. The improvement of our model on the RAF-DB dataset further demonstrates its superior performance on public datasets and indicates its broad applicability.</p></list-item><list-item><p>As shown in <xref rid="sensors-24-06748-t003" ref-type="table">Table 3</xref>, the accuracy of the SFER-MDFAE model on the AffectNet dataset reached 67.86%, significantly outperforming other state-of-the-art methods. Compared to EAC (64.74%), EmoNeXt (65.54%), DDAMFN (66.72%), POSTER (67.23%), and POSTER++ (67.34%), it achieved improvements of 3.12%, 2.32%, 1.14%, 0.63%, and 0.52%, respectively. These results demonstrate that the SFER-MDFAE model can achieve substantial performance gains even on large and complex datasets that are challenging to train on, further validating the model&#x02019;s superiority and applicability.</p></list-item><list-item><p>As shown in <xref rid="sensors-24-06748-t003" ref-type="table">Table 3</xref>, the SFER-MDFAE model performs exceptionally well on the SCFED, which represents the real smart classroom scenario. The model achieves an accuracy of 93.74%, outperforming other state-of-the-art methods. Compared to EAC (92.03%), EmoNeXt (92.42%), DDAMFN (93.00%), POSTER (93.25%), and POSTER++ (93.11%), the SFER-MDFAE model offers improvements of 1.71%, 1.32%, 0.74%, 0.49%, and 0.63%, respectively. Compared to other public datasets, in the smart classroom scenario, students&#x02019; expressions do not change significantly and are highly susceptible to occlusion by the environment and other factors, making it difficult for the model to recognize expressions. Despite these challenges, the SFER-MDFAE model still achieves a slight improvement over the advanced POSTER model. This indicates that the SFER-MDFAE model has a significant advantage in capturing facial expression features, particularly fine-grained features. Furthermore, its performance on the SCFED further validates the applicability and robustness of the SFER-MDFAE model in the specific context of smart classrooms.</p></list-item><list-item><p>As can be seen from <xref rid="sensors-24-06748-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-24-06748-t004" ref-type="table">Table 4</xref>, compared with the state-of-the-art POSTER and POSTER++ models, the SFER-MDFAE model significantly reduces the complexity of the model and the consumption of computational resources while maintaining a high level of accuracy and possesses the advantages of high efficiency and practicality.</p></list-item></list><p>In this paper, the confusion matrix is used to evaluate the classification performance of the SFER-MDFAE model on various facial expression categories in order to exhaustively reveal the model&#x02019;s recognition accuracy on different facial expression categories. By analyzing the confusion matrix, it is possible not only to quantify the overall recognition effectiveness of the model but also to gain insight into its correct classification and misclassification on each specific expression category. The confusion matrices of the SFER-MDFAE model on the FER2013, FERPlus, RAF-DB, AffectNet, and SCFEDs are shown in <xref rid="sensors-24-06748-f005" ref-type="fig">Figure 5</xref>a&#x02013;e.</p><p>As indicated by the confusion matrix in <xref rid="sensors-24-06748-f005" ref-type="fig">Figure 5</xref>, the SFER-MDFAE model achieved the best performance in the &#x0201c;happy&#x0201d; expression category across the five expression datasets. This suggests that the model can relatively easily and accurately capture the facial features associated with &#x0201c;happy&#x0201d; expressions. In addition, the model shows high accuracy and low misclassification rate in recognizing all classes of expressions in the five expression datasets, which proves that the SFER-MDFAE model is able to capture the subtle changes in facial expressions and effectively differentiate between different classes of more similar expressions. Especially for the SCFED, the model does not recognize &#x0201c;dedicated&#x0201d; as &#x0201c;neutral&#x0201d; or &#x0201c;dedicated&#x0201d; as &#x0201c;neutral&#x0201d; when recognizing the two similar expressions, and the model shows very high accuracy in recognizing each class of expression. This shows that the SFER-MDFAE model is still able to efficiently recognize students&#x02019; learning expressions even in a complex classroom environment.</p></sec><sec id="sec4dot5-sensors-24-06748"><title>4.5. Results and Analysis of Ablation Experiments</title><p>To validate the effectiveness of the multi-scale feature extraction, the multi-scale dual-pooling feature aggregation module, and the key region-oriented attention feature enhancement module in the SFER-MDFAE model, four variant models are designed and evaluated through ablation experiments across five datasets. Specifically, SFER-MDFAE-nm denotes the removal of multi-scale features&#x000a0;<inline-formula>
<mml:math id="mm1188" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm1189" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>,&#x000a0;<inline-formula>
<mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>, using only the deep semantic feature&#x000a0;<inline-formula>
<mml:math id="mm91" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>; SFER-MDFAE-np indicates the removal of the multi-scale dual-pooling feature aggregation module; SFER-MDFAE-na represents the removal of the key region-oriented attention mechanism; and SFER-MDFAE-rw replaces the proposed key region-oriented attention mechanism with a window-based cross-attention mechanism [<xref rid="B13-sensors-24-06748" ref-type="bibr">13</xref>]. The experimental results are shown in <xref rid="sensors-24-06748-f006" ref-type="fig">Figure 6</xref>a&#x02013;d.</p><p>As can be seen from the ablation experiment results in <xref rid="sensors-24-06748-f006" ref-type="fig">Figure 6</xref>, the accuracy of SFER-MDFAE-nm is significantly reduced compared to that of SFER-MDFAE, indicating the importance of multi-scale features for a comprehensive understanding of faces. By capturing facial expression features at different scales in the image, the model can more effectively comprehend shallow edge and texture features and deep semantic features present in expression images. This multi-scale approach significantly improves the model&#x02019;s feature expression ability, leading to improved recognition accuracy; the accuracy of SFER-MDFAE-np is lower than that of SFER-MDFAE, indicating the effectiveness of the proposed multi-scale dual-pooling feature aggregation module. This module extracts effective features at different scales through the channel feature selection mechanism and fuses them to obtain comprehensive representations of key facial features, which helps the model maintain high recognition accuracy even when recognizing noisy expression images; the accuracy of SFER-MDFAE surpasses that of its two variant models, SFER-MDFAE-na and SFER-MDFAE-rw, indicating that the proposed key region-oriented attention mechanism is not only effective but also superior to the traditional window-based cross-attention mechanism. This advantage arises because the key region-oriented attention mechanism dynamically selects the most relevant regions for each key area and applies a multi-head self-attention mechanism between these regions. This allows the model to accurately capture the global correlations between regions, rather than being limited to information within a local window, thus improving the extraction of key facial expression features. Furthermore, removing any of the modules leads to a decrease in model performance, which suggests that on all five datasets, all three modules introduced are valid and play a complementary role in the model.</p></sec><sec id="sec4dot6-sensors-24-06748"><title>4.6. Parametric Analysis</title><p>In order to verify the influence of the value of the number of&#x000a0;<inline-formula>
<mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>, which retains the number of regions with the maximum degree of association, on the SFER-MDFAE model, we conducted experiments with different values of&#x000a0;<inline-formula>
<mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;(<inline-formula>
<mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>15</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn><mml:mo>,</mml:mo><mml:mn>49</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>). Since the experimental results are consistent across datasets, we analyze the smart classroom facial expression dataset SCFED as an example, and the results are shown in <xref rid="sensors-24-06748-f007" ref-type="fig">Figure 7</xref>.</p><p>As can be seen from <xref rid="sensors-24-06748-f007" ref-type="fig">Figure 7</xref>, the value of&#x000a0;<inline-formula>
<mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;has a significant effect on the validation set accuracy and test set accuracy of the model. The model performs best when&#x000a0;<inline-formula>
<mml:math id="mm96" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>, reaching 93.92% accuracy in the validation set and 93.74% in the test set. Lower&#x000a0;<inline-formula>
<mml:math id="mm97" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;values (e.g.,&#x000a0;<inline-formula>
<mml:math id="mm98" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>) do not allow the model to adequately capture the deep semantic information of facial features, resulting in insufficient attention to the nuances of facial expressions, which in turn affects the performance of the model. Higher&#x000a0;<inline-formula>
<mml:math id="mm999" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;values (e.g.,&#x000a0;<inline-formula>
<mml:math id="mm100" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn><mml:mo>,</mml:mo><mml:mn>49</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>), on the other hand, may reintroduce noise, causing the model to pay too much attention to common regions of different classes of facial expressions, which in turn reduces the effectiveness of the key region-oriented attention mechanism. Therefore, an appropriate&#x000a0;<inline-formula>
<mml:math id="mm1019" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;value allows the model to focus more on the nuances of facial expressions and less on the common regions of the face, thereby improving the model&#x02019;s performance.</p></sec><sec id="sec4dot7-sensors-24-06748"><title>4.7. Visualization Analysis</title><p>In order to better explain the effect of the key region-oriented attention mechanism in the SFER-MDFAE model, we randomly selected seven different kinds of facial images from the RAF-DB and SCFEDs and used the GradCAM [<xref rid="B44-sensors-24-06748" ref-type="bibr">44</xref>] visualization method to generate the attention heatmaps, and the results are shown in <xref rid="sensors-24-06748-f008" ref-type="fig">Figure 8</xref>. In <xref rid="sensors-24-06748-f008" ref-type="fig">Figure 8</xref>, rows (a)&#x02013;(g) represent seven classes of expressions: surprise, fear, happy, tired, disgust, doubt, and neutral, respectively. Column (I) is the original facial image, column (II) is the attention visualization result of the fourth layer of high-dimensional features&#x000a0;<inline-formula>
<mml:math id="mm1029" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;output by IR50, column (III) is the attention visualization result output by the baseline model [<xref rid="B15-sensors-24-06748" ref-type="bibr">15</xref>] based on the window-based cross-attention mechanism, and column (IV) is the attention visualization result output by the KROAFE module proposed in this paper.</p><p>As can be seen from <xref rid="sensors-24-06748-f008" ref-type="fig">Figure 8</xref>, although column (II) effectively recognizes the facial region in the original images, it is unable to recognize the fine-grained features of the face, which has some limitations. For example, in the category of disgusted expressions in row (e) of <xref rid="sensors-24-06748-f008" ref-type="fig">Figure 8</xref>, column (II) only recognizes the universal regions and fails to effectively capture the key regions that help recognize disgusted expressions, such as the forehead and between the eyebrows. In contrast, column (IV) shows that KROAFE is able to accurately focus on the key regions ignored in column (II), including eyebrows, forehead, eyes, and mouth, etc., and the attention result is significantly better than that in column (II), which suggests that KROAFE is able to efficiently extract the fine-grained features of a complex face. It is worth noting that the attention regions in column (IV) are larger due to the fact that KROAFE residually joins the extracted fine-grained feature with the downsampled feature&#x000a0;<inline-formula>
<mml:math id="mm1039" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>. From another perspective, the attention features extracted in column (III) are less stable compared to those in the fourth column (IV), and column (III) often focuses on non-facial invalid regions in complex facial expressions, as shown in rows (c), (f), and (g). In particular, in the confused expression category in row (f), due to the presence of the hand interference at the bottom, column (III) incorrectly focuses on the hand interference region instead of the face region, which may lead to a final misjudgment. Although this is an isolated case, such instances may have a significant impact on the overall performance of the model in the complex environments of a real-world smart classroom. On the contrary, column (IV) is not affected by the interfering regions, and the overall visualization of the face in the examples is significantly better than column (III). This is not only due to the superiority of the KROAFE module but also due to the fact that the MobileNet network is removed in this paper, which avoids the problem of inaccurate landmark feature localization leading to inaccurate extraction of key regions. In summary, the above results further validate the effectiveness of the KROAFE method proposed in this paper.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-24-06748"><title>5. Conclusions</title><p>In this paper, we propose a student facial expression recognition model based on multi-scale and deep fine-grained feature attention enhancement. The facial expression information at different scales is captured and fused through a multi-scale dual-pooling feature aggregation module as a way to obtain a comprehensive representation of key facial features and improve the robustness of the model. In addition, a key region-oriented attention mechanism is designed to pay more attention to the nuances of facial expressions, further enhancing the representation of multi-scale deep fine-grained features. Compared with the traditional window-based cross-attention mechanism, this mechanism can dynamically select and focus on key regions in the image without the limitation of a fixed window, enhancing the expression of facial features. The multi-scale and deep fine-grained attention-enhanced features are fused to obtain richer and more accurate facial key information and realize the accurate classification of facial expressions. The experimental results on five datasets demonstrate that the SFER-MDFAE model proposed outperforms the other five baseline models. In addition, this paper conducts a large number of ablation experiments to demonstrate the effectiveness of the multi-scale feature extraction, multi-scale dual-pooling feature aggregation module, and the key region-oriented attention feature enhancement module. In future research, we will focus on further optimizing the key region-oriented attention mechanism to enhance its accuracy and efficiency in capturing complex emotional features. Meanwhile, we aim to promote facial expression recognition technology in smart classrooms, integrating information such as students&#x02019; head poses to better characterize their learning emotions. This will assist teachers in evaluating students&#x02019; learning states more effectively. Additionally, as our research group&#x02019;s plans deepen and expand, we intend to utilize virtual reality (VR) technology to simulate diverse smart teaching scenarios and incorporate it into wearable devices. This approach aims to better address the varied demands of smart education and facilitate the widespread application of emotion recognition technology.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, Z.S. and Y.H.; methodology, Y.H.; software, Y.H.; validation, D.L., C.F. and H.Z.; formal analysis, H.Z.; investigation, Y.L.; resources, Z.S.; data curation, Y.H.; writing&#x02014;original draft preparation, Y.H.; writing&#x02014;review and editing, G.W.; visualization, Y.H.; supervision, Z.S.; project administration, Z.S.; funding acquisition, Z.S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The datasets are available at the following link: FER2013: <uri xlink:href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data">https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data</uri> (accessed on 1 October 2024), FERPlus: <uri xlink:href="https://www.worldlink.com.cn/osdir/ferplus.html">https://www.worldlink.com.cn/osdir/ferplus.html</uri> (accessed on 1 October 2024), RAF-DB: <uri xlink:href="http://www.whdeng.cn/RAF/model1.html">http://www.whdeng.cn/RAF/model1.html</uri> (accessed on 1 October 2024), AffectNet: <uri xlink:href="http://mohammadmahoor.com/affectnet/">http://mohammadmahoor.com/affectnet/</uri> (accessed on 1 October 2024). The SCFED will be made available upon reasonable request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Author Guangxiang Wu was employed by the company 34th Research Institute of China Electronics Technology Group Corporation. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-24-06748"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Goldberg</surname><given-names>P.</given-names></name>
<name><surname>S&#x000fc;mer</surname><given-names>&#x000d6;.</given-names></name>
<name><surname>St&#x000fc;rmer</surname><given-names>K.</given-names></name>
<name><surname>Wagner</surname><given-names>W.</given-names></name>
<name><surname>G&#x000f6;llner</surname><given-names>R.</given-names></name>
<name><surname>Gerjets</surname><given-names>P.</given-names></name>
<name><surname>Kasneci</surname><given-names>E.</given-names></name>
<name><surname>Trautwein</surname><given-names>U.</given-names></name>
</person-group><article-title>Attentive or not? Toward a machine learning approach to assessing students&#x02019; visible engagement in classroom instruction</article-title><source>Educ. Psychol. Rev.</source><year>2021</year><volume>33</volume><fpage>27</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1007/s10648-019-09514-z</pub-id></element-citation></ref><ref id="B2-sensors-24-06748"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Munna</surname><given-names>A.S.</given-names></name>
<name><surname>Kalam</surname><given-names>M.A.</given-names></name>
</person-group><article-title>Teaching and learning process to enhance teaching effectiveness: A literature review</article-title><source>Int. J. Humanit. Innov. (IJHI)</source><year>2021</year><volume>4</volume><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.33750/ijhi.v4i1.102</pub-id></element-citation></ref><ref id="B3-sensors-24-06748"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Castiblanco Jimenez</surname><given-names>I.A.</given-names></name>
<name><surname>Olivetti</surname><given-names>E.C.</given-names></name>
<name><surname>Vezzetti</surname><given-names>E.</given-names></name>
<name><surname>Moos</surname><given-names>S.</given-names></name>
<name><surname>Celeghin</surname><given-names>A.</given-names></name>
<name><surname>Marcolin</surname><given-names>F.</given-names></name>
</person-group><article-title>Effective affective EEG-based indicators in emotion-evoking VR environments: An evidence from machine learning</article-title><source>Neural Comput. Appl.</source><year>2024</year><pub-id pub-id-type="doi">10.1007/s00521-024-10240-z</pub-id></element-citation></ref><ref id="B4-sensors-24-06748"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mar&#x000ed;n-Morales</surname><given-names>J.</given-names></name>
<name><surname>Higuera-Trujillo</surname><given-names>J.L.</given-names></name>
<name><surname>Greco</surname><given-names>A.</given-names></name>
<name><surname>Guixeres</surname><given-names>J.</given-names></name>
<name><surname>Llinares</surname><given-names>C.</given-names></name>
<name><surname>Scilingo</surname><given-names>E.P.</given-names></name>
<name><surname>Alca&#x000f1;iz</surname><given-names>M.</given-names></name>
<name><surname>Valenza</surname><given-names>G.</given-names></name>
</person-group><article-title>Affective computing in virtual reality: Emotion recognition from brain and heartbeat dynamics using wearable sensors</article-title><source>Sci. Rep.</source><year>2018</year><volume>8</volume><elocation-id>13657</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-32063-4</pub-id><pub-id pub-id-type="pmid">30209261</pub-id>
</element-citation></ref><ref id="B5-sensors-24-06748"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Saurav</surname><given-names>S.</given-names></name>
<name><surname>Saini</surname><given-names>R.</given-names></name>
<name><surname>Singh</surname><given-names>S.</given-names></name>
</person-group><article-title>Fast facial expression recognition using boosted histogram of oriented gradient (BHOG) features</article-title><source>Pattern Anal. Appl.</source><year>2023</year><volume>26</volume><fpage>381</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1007/s10044-022-01112-0</pub-id></element-citation></ref><ref id="B6-sensors-24-06748"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shi</surname><given-names>Y.</given-names></name>
<name><surname>Lv</surname><given-names>Z.</given-names></name>
<name><surname>Bi</surname><given-names>N.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
</person-group><article-title>An improved SIFT algorithm for robust emotion recognition under various face poses and illuminations</article-title><source>Neural Comput. Appl.</source><year>2020</year><volume>32</volume><fpage>9267</fpage><lpage>9281</lpage><pub-id pub-id-type="doi">10.1007/s00521-019-04437-w</pub-id></element-citation></ref><ref id="B7-sensors-24-06748"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Niu</surname><given-names>B.</given-names></name>
<name><surname>Gao</surname><given-names>Z.</given-names></name>
<name><surname>Guo</surname><given-names>B.</given-names></name>
</person-group><article-title>Facial expression recognition with LBP and ORB features</article-title><source>Comput. Intell. Neurosci.</source><year>2021</year><volume>2021</volume><fpage>8828245</fpage><pub-id pub-id-type="doi">10.1155/2021/8828245</pub-id><pub-id pub-id-type="pmid">33505453</pub-id>
</element-citation></ref><ref id="B8-sensors-24-06748"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lakshmi</surname><given-names>D.</given-names></name>
<name><surname>Ponnusamy</surname><given-names>R.</given-names></name>
</person-group><article-title>Facial emotion recognition using modified HOG and LBP features with deep stacked autoencoders</article-title><source>Microprocess. Microsyst.</source><year>2021</year><volume>82</volume><fpage>103834</fpage><pub-id pub-id-type="doi">10.1016/j.micpro.2021.103834</pub-id></element-citation></ref><ref id="B9-sensors-24-06748"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shou</surname><given-names>Z.</given-names></name>
<name><surname>Zhu</surname><given-names>N.</given-names></name>
<name><surname>Wen</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Mo</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
</person-group><article-title>A Method for Analyzing Learning Sentiment Based on Classroom Time-Series Images</article-title><source>Math. Probl. Eng.</source><year>2023</year><volume>2023</volume><fpage>6955772</fpage><pub-id pub-id-type="doi">10.1155/2023/6955772</pub-id></element-citation></ref><ref id="B10-sensors-24-06748"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xue</surname><given-names>F.</given-names></name>
<name><surname>Tan</surname><given-names>Z.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Ma</surname><given-names>Z.</given-names></name>
<name><surname>Guo</surname><given-names>G.</given-names></name>
</person-group><article-title>Coarse-to-fine cascaded networks with smooth predicting for video facial expression recognition</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#x02013;24 June 2022</conf-date><fpage>2412</fpage><lpage>2418</lpage></element-citation></ref><ref id="B11-sensors-24-06748"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nan</surname><given-names>Y.</given-names></name>
<name><surname>Ju</surname><given-names>J.</given-names></name>
<name><surname>Hua</surname><given-names>Q.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>B.</given-names></name>
</person-group><article-title>A-MobileNet: An approach of facial expression recognition</article-title><source>Alex. Eng. J.</source><year>2022</year><volume>61</volume><fpage>4435</fpage><lpage>4444</lpage><pub-id pub-id-type="doi">10.1016/j.aej.2021.09.066</pub-id></element-citation></ref><ref id="B12-sensors-24-06748"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Farzaneh</surname><given-names>A.H.</given-names></name>
<name><surname>Qi</surname><given-names>X.</given-names></name>
</person-group><article-title>Facial expression recognition in the wild via deep attentive center loss</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Virtual</conf-loc><conf-date>5&#x02013;9 January 2021</conf-date><fpage>2402</fpage><lpage>2411</lpage></element-citation></ref><ref id="B13-sensors-24-06748"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Deng</surname><given-names>W.</given-names></name>
</person-group><article-title>Leave no stone unturned: Mine extra knowledge for imbalanced facial expression recognition</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2024</year><volume>36</volume><fpage>14414</fpage><lpage>14426</lpage></element-citation></ref><ref id="B14-sensors-24-06748"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>J.</given-names></name>
<name><surname>Wei</surname><given-names>Z.</given-names></name>
<name><surname>Cai</surname><given-names>Z.</given-names></name>
<name><surname>Zhao</surname><given-names>G.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Xie</surname><given-names>G.</given-names></name>
<name><surname>Zhu</surname><given-names>J.</given-names></name>
<name><surname>Zhu</surname><given-names>W.</given-names></name>
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<etal/>
</person-group><article-title>Exploring Facial Expression Recognition through Semi-Supervised Pre-training and Temporal Modeling</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>17&#x02013;21 June 2024</conf-date><fpage>4880</fpage><lpage>4887</lpage></element-citation></ref><ref id="B15-sensors-24-06748"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mao</surname><given-names>J.</given-names></name>
<name><surname>Xu</surname><given-names>R.</given-names></name>
<name><surname>Yin</surname><given-names>X.</given-names></name>
<name><surname>Chang</surname><given-names>Y.</given-names></name>
<name><surname>Nie</surname><given-names>B.</given-names></name>
<name><surname>Huang</surname><given-names>A.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
</person-group><article-title>POSTER++: A simpler and stronger facial expression recognition network</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2301.12149</pub-id><pub-id pub-id-type="doi">10.1016/j.patcog.2024.110951</pub-id></element-citation></ref><ref id="B16-sensors-24-06748"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>C.</given-names></name>
<name><surname>Mendieta</surname><given-names>M.</given-names></name>
<name><surname>Chen</surname><given-names>C.</given-names></name>
</person-group><article-title>Poster: A pyramid cross-fusion transformer network for facial expression recognition</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Paris, France</conf-loc><conf-date>2&#x02013;3 October 2023</conf-date><fpage>3146</fpage><lpage>3155</lpage></element-citation></ref><ref id="B17-sensors-24-06748"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jeong</surname><given-names>D.</given-names></name>
<name><surname>Kim</surname><given-names>B.G.</given-names></name>
<name><surname>Dong</surname><given-names>S.Y.</given-names></name>
</person-group><article-title>Deep joint spatiotemporal network (DJSTN) for efficient facial expression recognition</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>1936</elocation-id><pub-id pub-id-type="doi">10.3390/s20071936</pub-id><pub-id pub-id-type="pmid">32235662</pub-id>
</element-citation></ref><ref id="B18-sensors-24-06748"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>K.</given-names></name>
<name><surname>Jin</surname><given-names>Y.</given-names></name>
<name><surname>Akram</surname><given-names>M.W.</given-names></name>
<name><surname>Han</surname><given-names>R.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
</person-group><article-title>Facial expression recognition with convolutional neural networks via a new face cropping and rotation strategy</article-title><source>Vis. Comput.</source><year>2020</year><volume>36</volume><fpage>391</fpage><lpage>404</lpage><pub-id pub-id-type="doi">10.1007/s00371-019-01627-4</pub-id></element-citation></ref><ref id="B19-sensors-24-06748"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fard</surname><given-names>A.P.</given-names></name>
<name><surname>Mahoor</surname><given-names>M.H.</given-names></name>
</person-group><article-title>Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>26756</fpage><lpage>26768</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3156598</pub-id></element-citation></ref><ref id="B20-sensors-24-06748"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vignesh</surname><given-names>S.</given-names></name>
<name><surname>Savithadevi</surname><given-names>M.</given-names></name>
<name><surname>Sridevi</surname><given-names>M.</given-names></name>
<name><surname>Sridhar</surname><given-names>R.</given-names></name>
</person-group><article-title>A novel facial emotion recognition model using segmentation VGG-19 architecture</article-title><source>Int. J. Inf. Technol.</source><year>2023</year><volume>15</volume><fpage>1777</fpage><lpage>1787</lpage><pub-id pub-id-type="doi">10.1007/s41870-023-01184-z</pub-id></element-citation></ref><ref id="B21-sensors-24-06748"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Minaee</surname><given-names>S.</given-names></name>
<name><surname>Minaei</surname><given-names>M.</given-names></name>
<name><surname>Abdolrashidi</surname><given-names>A.</given-names></name>
</person-group><article-title>Deep-emotion: Facial expression recognition using attentional convolutional network</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>3046</elocation-id><pub-id pub-id-type="doi">10.3390/s21093046</pub-id><pub-id pub-id-type="pmid">33925371</pub-id>
</element-citation></ref><ref id="B22-sensors-24-06748"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tang</surname><given-names>C.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
<name><surname>Tian</surname><given-names>Q.</given-names></name>
</person-group><article-title>Convolutional Neural Network&#x02013;Bidirectional Gated Recurrent Unit Facial Expression Recognition Method Fused with Attention Mechanism</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>12418</elocation-id><pub-id pub-id-type="doi">10.3390/app132212418</pub-id></element-citation></ref><ref id="B23-sensors-24-06748"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>W.</given-names></name>
<name><surname>Xu</surname><given-names>H.</given-names></name>
</person-group><article-title>Co-attentive multi-task convolutional neural network for facial expression recognition</article-title><source>Pattern Recognit.</source><year>2022</year><volume>123</volume><fpage>108401</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2021.108401</pub-id></element-citation></ref><ref id="B24-sensors-24-06748"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fan</surname><given-names>X.</given-names></name>
<name><surname>Jiang</surname><given-names>M.</given-names></name>
<name><surname>Shahid</surname><given-names>A.R.</given-names></name>
<name><surname>Yan</surname><given-names>H.</given-names></name>
</person-group><article-title>Hierarchical scale convolutional neural network for facial expression recognition</article-title><source>Cogn. Neurodyn.</source><year>2022</year><volume>16</volume><fpage>847</fpage><lpage>858</lpage><pub-id pub-id-type="doi">10.1007/s11571-021-09761-3</pub-id><pub-id pub-id-type="pmid">35847532</pub-id>
</element-citation></ref><ref id="B25-sensors-24-06748"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wen</surname><given-names>Z.</given-names></name>
<name><surname>Lin</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>T.</given-names></name>
<name><surname>Xu</surname><given-names>G.</given-names></name>
</person-group><article-title>Distract your attention: Multi-head cross attention network for facial expression recognition</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2109.07270</pub-id><pub-id pub-id-type="pmid">37218785</pub-id>
</element-citation></ref><ref id="B26-sensors-24-06748"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
</person-group><article-title>Learning deep global multi-scale and local attention features for facial expression recognition in the wild</article-title><source>IEEE Trans. Image Process.</source><year>2021</year><volume>30</volume><fpage>6544</fpage><lpage>6556</lpage><pub-id pub-id-type="doi">10.1109/TIP.2021.3093397</pub-id><pub-id pub-id-type="pmid">34224355</pub-id>
</element-citation></ref><ref id="B27-sensors-24-06748"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Karnati</surname><given-names>M.</given-names></name>
<name><surname>Seal</surname><given-names>A.</given-names></name>
<name><surname>Yazidi</surname><given-names>A.</given-names></name>
<name><surname>Krejcar</surname><given-names>O.</given-names></name>
</person-group><article-title>Flepnet: Feature level ensemble parallel network for facial expression recognition</article-title><source>IEEE Trans. Affect. Comput.</source><year>2022</year><volume>13</volume><fpage>2058</fpage><lpage>2070</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2022.3208309</pub-id></element-citation></ref><ref id="B28-sensors-24-06748"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Deng</surname><given-names>J.</given-names></name>
<name><surname>Guo</surname><given-names>J.</given-names></name>
<name><surname>Xue</surname><given-names>N.</given-names></name>
<name><surname>Zafeiriou</surname><given-names>S.</given-names></name>
</person-group><article-title>Arcface: Additive angular margin loss for deep face recognition</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#x02013;20 June 2019</conf-date><fpage>4690</fpage><lpage>4699</lpage></element-citation></ref><ref id="B29-sensors-24-06748"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>R.</given-names></name>
<name><surname>Huang</surname><given-names>A.</given-names></name>
<name><surname>Hu</surname><given-names>Y.</given-names></name>
<name><surname>Feng</surname><given-names>X.</given-names></name>
</person-group><article-title>GFFT: Global-local feature fusion transformers for facial expression recognition in the wild</article-title><source>Image Vis. Comput.</source><year>2023</year><volume>139</volume><fpage>104824</fpage><pub-id pub-id-type="doi">10.1016/j.imavis.2023.104824</pub-id></element-citation></ref><ref id="B30-sensors-24-06748"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Lin</surname><given-names>Y.</given-names></name>
<name><surname>Cao</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>H.</given-names></name>
<name><surname>Wei</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Lin</surname><given-names>S.</given-names></name>
<name><surname>Guo</surname><given-names>B.</given-names></name>
</person-group><article-title>Swin transformer: Hierarchical vision transformer using shifted windows</article-title><source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source><conf-loc>Virtually</conf-loc><conf-date>11&#x02013;17 October 2021</conf-date><fpage>10012</fpage><lpage>10022</lpage></element-citation></ref><ref id="B31-sensors-24-06748"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tu</surname><given-names>Z.</given-names></name>
<name><surname>Talebi</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Yang</surname><given-names>F.</given-names></name>
<name><surname>Milanfar</surname><given-names>P.</given-names></name>
<name><surname>Bovik</surname><given-names>A.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group><article-title>Maxvit: Multi-axis vision transformer</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#x02013;27 October 2022</conf-date><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>459</fpage><lpage>479</lpage></element-citation></ref><ref id="B32-sensors-24-06748"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Qiu</surname><given-names>Q.</given-names></name>
<name><surname>Chen</surname><given-names>L.</given-names></name>
<name><surname>Wu</surname><given-names>B.</given-names></name>
<name><surname>Lin</surname><given-names>B.</given-names></name>
<name><surname>He</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>W.</given-names></name>
</person-group><article-title>Crossformer++: A versatile vision transformer hinging on cross-scale attention</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>3123</fpage><lpage>3136</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2023.3341806</pub-id><pub-id pub-id-type="pmid">38113150</pub-id>
</element-citation></ref><ref id="B33-sensors-24-06748"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Zhou</surname><given-names>D.</given-names></name>
<name><surname>He</surname><given-names>S.</given-names></name>
<name><surname>Feng</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
</person-group><article-title>Shunted self-attention via multi-scale token aggregation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#x02013;24 June 2022</conf-date><fpage>10853</fpage><lpage>10862</lpage></element-citation></ref><ref id="B34-sensors-24-06748"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Goodfellow</surname><given-names>I.J.</given-names></name>
<name><surname>Erhan</surname><given-names>D.</given-names></name>
<name><surname>Carrier</surname><given-names>P.L.</given-names></name>
<name><surname>Courville</surname><given-names>A.</given-names></name>
<name><surname>Mirza</surname><given-names>M.</given-names></name>
<name><surname>Hamner</surname><given-names>B.</given-names></name>
<name><surname>Cukierski</surname><given-names>W.</given-names></name>
<name><surname>Tang</surname><given-names>Y.</given-names></name>
<name><surname>Thaler</surname><given-names>D.</given-names></name>
<name><surname>Lee</surname><given-names>D.H.</given-names></name>
<etal/>
</person-group><article-title>Challenges in representation learning: A report on three machine learning contests</article-title><source>Proceedings of the Neural Information Processing: 20th International Conference, ICONIP 2013</source><conf-loc>Daegu, Republic of Korea</conf-loc><conf-date>3&#x02013;7 November 2013</conf-date><comment>Proceedings, Part III 20</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2013</year><fpage>117</fpage><lpage>124</lpage></element-citation></ref><ref id="B35-sensors-24-06748"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Barsoum</surname><given-names>E.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Ferrer</surname><given-names>C.C.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
</person-group><article-title>Training deep networks for facial expression recognition with crowd-sourced label distribution</article-title><source>Proceedings of the 18th ACM International Conference on Multimodal Interaction</source><conf-loc>Tokyo, Japan</conf-loc><conf-date>12&#x02013;16 November 2016</conf-date><fpage>279</fpage><lpage>283</lpage></element-citation></ref><ref id="B36-sensors-24-06748"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Deng</surname><given-names>W.</given-names></name>
</person-group><article-title>Deep facial expression recognition: A survey</article-title><source>IEEE Trans. Affect. Comput.</source><year>2020</year><volume>13</volume><fpage>1195</fpage><lpage>1215</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2020.2981446</pub-id></element-citation></ref><ref id="B37-sensors-24-06748"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mollahosseini</surname><given-names>A.</given-names></name>
<name><surname>Hasani</surname><given-names>B.</given-names></name>
<name><surname>Mahoor</surname><given-names>M.H.</given-names></name>
</person-group><article-title>Affectnet: A database for facial expression, valence, and arousal computing in the wild</article-title><source>IEEE Trans. Affect. Comput.</source><year>2017</year><volume>10</volume><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1109/TAFFC.2017.2740923</pub-id></element-citation></ref><ref id="B38-sensors-24-06748"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Qiao</surname><given-names>Y.</given-names></name>
</person-group><article-title>Joint face detection and alignment using multitask cascaded convolutional networks</article-title><source>IEEE Signal Process. Lett.</source><year>2016</year><volume>23</volume><fpage>1499</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1109/LSP.2016.2603342</pub-id></element-citation></ref><ref id="B39-sensors-24-06748"><label>39.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>C.</given-names></name>
<name><surname>Ling</surname><given-names>X.</given-names></name>
<name><surname>Deng</surname><given-names>W.</given-names></name>
</person-group><article-title>Learn from all: Erasing attention consistency for noisy label facial expression recognition</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#x02013;27 October 2022</conf-date><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2022</year><fpage>418</fpage><lpage>434</lpage></element-citation></ref><ref id="B40-sensors-24-06748"><label>40.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>El Boudouri</surname><given-names>Y.</given-names></name>
<name><surname>Bohi</surname><given-names>A.</given-names></name>
</person-group><article-title>EmoNeXt: An Adapted ConvNeXt for Facial Emotion Recognition</article-title><source>Proceedings of the 2023 IEEE 25th International Workshop on Multimedia Signal Processing (MMSP)</source><conf-loc>Poitiers, France</conf-loc><conf-date>27&#x02013;29 September 2023</conf-date><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="B41-sensors-24-06748"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Song</surname><given-names>Z.</given-names></name>
</person-group><article-title>A dual-direction attention mixed feature network for facial expression recognition</article-title><source>Electronics</source><year>2023</year><volume>12</volume><elocation-id>3595</elocation-id><pub-id pub-id-type="doi">10.3390/electronics12173595</pub-id></element-citation></ref><ref id="B42-sensors-24-06748"><label>42.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Hu</surname><given-names>Y.</given-names></name>
<name><surname>He</surname><given-names>X.</given-names></name>
<name><surname>Gao</surname><given-names>J.</given-names></name>
</person-group><article-title>Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</article-title><source>Proceedings of the Computer Vision&#x02013;ECCV 2016: 14th European Conference</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#x02013;14 October 2016</conf-date><comment>Proceedings; Part III 14</comment><publisher-name>Springer International Publishing</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2016</year><fpage>87</fpage><lpage>102</lpage></element-citation></ref><ref id="B43-sensors-24-06748"><label>43.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhong</surname><given-names>Z.</given-names></name>
<name><surname>Zheng</surname><given-names>L.</given-names></name>
<name><surname>Kang</surname><given-names>G.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Random erasing data augmentation</article-title><source>Proceedings of the AAAI Conference on Artificial Intelligence</source><conf-loc>New York, NY, USA</conf-loc><conf-date>7&#x02013;12 February 2020</conf-date><volume>Volume 34</volume><fpage>13001</fpage><lpage>13008</lpage></element-citation></ref><ref id="B44-sensors-24-06748"><label>44.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Selvaraju</surname><given-names>R.R.</given-names></name>
<name><surname>Cogswell</surname><given-names>M.</given-names></name>
<name><surname>Das</surname><given-names>A.</given-names></name>
<name><surname>Vedantam</surname><given-names>R.</given-names></name>
<name><surname>Parikh</surname><given-names>D.</given-names></name>
<name><surname>Batra</surname><given-names>D.</given-names></name>
</person-group><article-title>Grad-cam: Visual explanations from deep networks via gradient-based localization</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#x02013;29 October 2017</conf-date><fpage>618</fpage><lpage>626</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-24-06748-f001"><label>Figure 1</label><caption><p>SFER-MDFAE model framework diagram.</p></caption><graphic xlink:href="sensors-24-06748-g001" position="float"/></fig><fig position="float" id="sensors-24-06748-f002"><label>Figure 2</label><caption><p>Structure of the channel attention mechanism.&#x000a0;<inline-formula>
<mml:math id="mm1049" overflow="scroll"><mml:mrow><mml:mo>&#x02295;</mml:mo></mml:mrow></mml:math>
</inline-formula>&#x000a0;represents addition,&#x000a0;<inline-formula>
<mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mo>&#x02297;</mml:mo></mml:mrow></mml:math>
</inline-formula>&#x000a0;represents multiplication.</p></caption><graphic xlink:href="sensors-24-06748-g002" position="float"/></fig><fig position="float" id="sensors-24-06748-f003"><label>Figure 3</label><caption><p>Structure of the key region-oriented attention mechanism. The blue squares represent patches, the red lines denote the divided regions, and the red italicized sections indicate the key regions.</p></caption><graphic xlink:href="sensors-24-06748-g003" position="float"/></fig><fig position="float" id="sensors-24-06748-f004"><label>Figure 4</label><caption><p>The accuracy curves of the validation sets for each model on five different datasets.</p></caption><graphic xlink:href="sensors-24-06748-g004" position="float"/></fig><fig position="float" id="sensors-24-06748-f005"><label>Figure 5</label><caption><p>Confusion matrices of SFER-MDFAE model on five different datasets.</p></caption><graphic xlink:href="sensors-24-06748-g005a" position="float"/><graphic xlink:href="sensors-24-06748-g005b" position="float"/></fig><fig position="float" id="sensors-24-06748-f006"><label>Figure 6</label><caption><p>Results of ablation experiments of SFER-MDFAE model on five different datasets.</p></caption><graphic xlink:href="sensors-24-06748-g006" position="float"/></fig><fig position="float" id="sensors-24-06748-f007"><label>Figure 7</label><caption><p>Validation set accuracy and test set accuracy of the SFER-MDFAE model at different&#x000a0;<inline-formula>
<mml:math id="mm1059" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math>
</inline-formula>&#x000a0;values.</p></caption><graphic xlink:href="sensors-24-06748-g007" position="float"/></fig><fig position="float" id="sensors-24-06748-f008"><label>Figure 8</label><caption><p>Examples of attention visualization of various facial expression images from the RAF-DB and SCFEDs. Rows (<bold>a</bold>&#x02013;<bold>g</bold>) represent surprise, fear, happy, tired, disgust, doubt, and neutral, respectively. (<bold>I</bold>) Original image; (<bold>II</bold>) attention visualization results for the fourth layer of high-dimensional feature&#x000a0;<inline-formula>
<mml:math id="mm1069" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math>
</inline-formula>; (<bold>III</bold>) attention visualization results output by the baseline model with window-based cross-attention mechanism; (<bold>IV</bold>) attention visualization results output by the KROAFE module.</p></caption><graphic xlink:href="sensors-24-06748-g008" position="float"/></fig><table-wrap position="float" id="sensors-24-06748-t001"><object-id pub-id-type="pii">sensors-24-06748-t001_Table 1</object-id><label>Table 1</label><caption><p>Baseline models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Baseline Models</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model Introduction</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">EAC [<xref rid="B39-sensors-24-06748" ref-type="bibr">39</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">An erasure attention consistency method is proposed to effectively suppress noisy samples during training by randomly erasing images and exploiting flip semantic consistency.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1"><bold>*</bold> EmoNeXt [<xref rid="B40-sensors-24-06748" ref-type="bibr">40</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">An adaptive facial expression recognition model is proposed, which optimizes feature extraction by integrating a spatial transformer network and squeeze-and-excitation blocks. Additionally, a self-attention regularization term is introduced to generate compact feature vectors.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DDAMFN [<xref rid="B41-sensors-24-06748" ref-type="bibr">41</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">Hybrid feature networks and bidirectional attention networks are proposed to effectively extract features and capture remote dependencies.</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER [<xref rid="B16-sensors-24-06748" ref-type="bibr">16</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">A transformer-based dual-stream pyramid cross-fusion method is proposed, which effectively integrates facial landmark features and image features, thereby maximizing attention to prominent facial areas.</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">POSTER++ [<xref rid="B15-sensors-24-06748" ref-type="bibr">15</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A window-based cross-attention mechanism is proposed, and the branch from the image to landmarks is removed in the dual-stream design, achieving effective recognition with lower computational cost.</td></tr></tbody></table><table-wrap-foot><fn><p><bold>*</bold> The baseline model referred to as EmoNext is the EmoNext-base model.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-24-06748-t002"><object-id pub-id-type="pii">sensors-24-06748-t002_Table 2</object-id><label>Table 2</label><caption><p>Experimental environment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Experimental Environment</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Environment Configuration</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Operating systems</td><td align="center" valign="middle" rowspan="1" colspan="1">Linux</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">Intel(R) Core (TM) i5-13490F</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Video Cards</td><td align="center" valign="middle" rowspan="1" colspan="1">GeForce RTX 4060Ti</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RAM</td><td align="center" valign="middle" rowspan="1" colspan="1">16 GB</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ROM</td><td align="center" valign="middle" rowspan="1" colspan="1">2T SSD</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Programming Languages</td><td align="center" valign="middle" rowspan="1" colspan="1">Python 3.8</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Framework</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pytorch</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-06748-t003"><object-id pub-id-type="pii">sensors-24-06748-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of the SFER-MDFAE with baseline models on each dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy (%)</th></tr></thead><tbody><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">FER2013</td><td align="center" valign="middle" rowspan="1" colspan="1">EAC</td><td align="center" valign="middle" rowspan="1" colspan="1">72.78</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EmoNeXt</td><td align="center" valign="middle" rowspan="1" colspan="1">73.78</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DDAMFN</td><td align="center" valign="middle" rowspan="1" colspan="1">72.03</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER</td><td align="center" valign="middle" rowspan="1" colspan="1">74.87</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER++</td><td align="center" valign="middle" rowspan="1" colspan="1">74.73</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SFER-MDFAE</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.18</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">FERPlus</td><td align="center" valign="middle" rowspan="1" colspan="1">EAC</td><td align="center" valign="middle" rowspan="1" colspan="1">89.98</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EmoNeXt</td><td align="center" valign="middle" rowspan="1" colspan="1">90.23</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DDAMFN</td><td align="center" valign="middle" rowspan="1" colspan="1">90.45</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER</td><td align="center" valign="middle" rowspan="1" colspan="1">91.55</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER++</td><td align="center" valign="middle" rowspan="1" colspan="1">91.92</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SFER-MDFAE</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.75</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">RAF-DB</td><td align="center" valign="middle" rowspan="1" colspan="1">EAC</td><td align="center" valign="middle" rowspan="1" colspan="1">90.16</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EmoNeXt</td><td align="center" valign="middle" rowspan="1" colspan="1">87.25</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DDAMFN</td><td align="center" valign="middle" rowspan="1" colspan="1">90.97</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER</td><td align="center" valign="middle" rowspan="1" colspan="1">92.14</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER++</td><td align="center" valign="middle" rowspan="1" colspan="1">92.17</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SFER-MDFAE</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.93</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">AffectNet</td><td align="center" valign="middle" rowspan="1" colspan="1">EAC</td><td align="center" valign="middle" rowspan="1" colspan="1">64.74</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EmoNeXt</td><td align="center" valign="middle" rowspan="1" colspan="1">65.54</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DDAMFN</td><td align="center" valign="middle" rowspan="1" colspan="1">66.72</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER</td><td align="center" valign="middle" rowspan="1" colspan="1">67.23</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER++</td><td align="center" valign="middle" rowspan="1" colspan="1">67.34</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SFER-MDFAE</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67.86</td></tr><tr><td rowspan="6" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">SCFED</td><td align="center" valign="middle" rowspan="1" colspan="1">EAC</td><td align="center" valign="middle" rowspan="1" colspan="1">92.03</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">EmoNeXt</td><td align="center" valign="middle" rowspan="1" colspan="1">92.42</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DDAMFN</td><td align="center" valign="middle" rowspan="1" colspan="1">93.00</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER</td><td align="center" valign="middle" rowspan="1" colspan="1">93.25</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">POSTER++</td><td align="center" valign="middle" rowspan="1" colspan="1">93.11</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SFER-MDFAE</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.74</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-24-06748-t004"><object-id pub-id-type="pii">sensors-24-06748-t004_Table 4</object-id><label>Table 4</label><caption><p>Comparison of FLOPs and Params between the SFER-MDFAE model and baseline models.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Model</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>FLOPs</bold>
</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Params</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EAC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.898 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23.522 M</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">EmoNeXt</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.438 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.708 M</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DDAMFN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.564 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.106 M</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">POSTER</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.895 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.976 M</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">POSTER++</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.482 G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">43.724 M</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>SFER-MDFAE</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>6.362 G</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>31.975 M</bold>
</td></tr></tbody></table></table-wrap></floats-group></article>