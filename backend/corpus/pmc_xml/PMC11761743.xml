<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-id journal-id-type="publisher-id">BIORXIV</journal-id><journal-title-group><journal-title>bioRxiv</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn><publisher><publisher-name>Cold Spring Harbor Laboratory</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39868259</article-id><article-id pub-id-type="pmc">PMC11761743</article-id>
<article-id pub-id-type="doi">10.1101/2025.01.14.633067</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Visuospatial computations vary by category and stream and continue to develop in adolescence</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0091-6153</contrib-id><name><surname>Yao</surname><given-names>Jewelia K.</given-names></name><role>analyzed the data</role><role>wrote the manuscript</role><xref rid="A1" ref-type="aff">1</xref><xref rid="CR1" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Choo</surname><given-names>Justin</given-names></name><role>contributed to data analysis</role><role>contributed to the manuscript</role><xref rid="A3" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Finzi</surname><given-names>Dawn</given-names></name><role>designed the experiment</role><role>collected data</role><role>contributed to the manuscript</role><xref rid="A1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Grill-Spector</surname><given-names>Kalanit</given-names></name><role>designed the experiment</role><role>contributed to data analysis</role><role>wrote the manuscript</role><xref rid="A1" ref-type="aff">1</xref><xref rid="A2" ref-type="aff">2</xref></contrib></contrib-group><aff id="A1"><label>1.</label>Department of Psychology, Stanford University, Stanford, CA 94305</aff><aff id="A2"><label>2.</label>Wu Tsai Neuroscience Institute, Stanford University, Stanford, CA 94305</aff><aff id="A3"><label>3.</label>Department of Symbolic Systems, Stanford University, Stanford, CA, 94305</aff><author-notes><corresp id="CR1"><label>*</label><bold>Correspondence and requests for materials</bold> should be addressed to J.K.Y.</corresp></author-notes><pub-date pub-type="epub"><day>14</day><month>1</month><year>2025</year></pub-date><elocation-id>2025.01.14.633067</elocation-id><permissions><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyndlicense">https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">Creative Commons Attribution-NoDerivatives 4.0 International License</ext-link>, which allows reusers to copy and distribute the material in any medium or format in unadapted form only, and only so long as attribution is given to the creator. The license allows for commercial use.</license-p></license></permissions><self-uri content-type="pdf">nihpp-2025.01.14.633067.pdf</self-uri><abstract id="ABS1"><p id="P1">Reading, face recognition, and navigation are supported by visuospatial computations in category-selective regions across ventral, lateral, and dorsal visual streams. However, the nature of visuospatial computations across streams and their development in adolescence remain unknown. Using fMRI and population receptive field (pRF) modeling in adolescents and adults, we estimate pRFs in high-level visual cortex and determine their development. Results reveal that pRF location, size, and visual field coverage vary across category, stream, and hemisphere in both adolescents and adults. While pRF location is mature by adolescence, pRF size and visual field coverage continue to develop &#x02013; increasing in face-selective and decreasing in place-selective regions &#x02013; alongside similar development of category selectivity. These findings provide a timeline for differential development of visual functions and suggest that visuospatial computations in high-level visual cortex continue to be optimized to accommodate both category and stream demands through adolescence.</p></abstract></article-meta></front><body><sec id="S1"><title>Introduction</title><p id="P2">Across development, the ability to perceive faces, words, bodies, and places is crucial for key behaviors like social interactions, reading, and navigation, and depends on computations by neurons throughout visual cortex<sup><xref rid="R1" ref-type="bibr">1</xref>-<xref rid="R5" ref-type="bibr">5</xref></sup>. In humans, perception of these categories is enabled by computations in category-selective regions across three distinct visual processing streams emerging from early visual cortex (EVC; V1-V3): the ventral stream, which extends ventrally from occipital to temporal cortex and is involved in visual recognition<sup><xref rid="R1" ref-type="bibr">1</xref>,<xref rid="R5" ref-type="bibr">5</xref></sup>, the dorsal stream, which runs through superior occipital-parietal cortex and is engaged in spatial navigation and attention<sup><xref rid="R1" ref-type="bibr">1</xref>,<xref rid="R5" ref-type="bibr">5</xref></sup>, and the lateral stream, which extends from lateral occipitotemporal cortex through the superior temporal sulcus (STS) and is involved in dynamic, action, and social perception<sup><xref rid="R2" ref-type="bibr">2</xref>-<xref rid="R4" ref-type="bibr">4</xref>,<xref rid="R6" ref-type="bibr">6</xref></sup>. While many studies examined functional differences among the three pathways, few studies have investigated how basic visual properties such as receptive fields (RFs) &#x02013; the part of visual space processed by a neuron<sup><xref rid="R7" ref-type="bibr">7</xref></sup> and population receptive fields (pRFs) &#x02013; the portion of visual space processed by the population of neurons in a voxel<sup><xref rid="R8" ref-type="bibr">8</xref></sup>&#x02013; differ within and across streams. Even less is known about how pRFs in category-selective regions develop, particularly in adolescence when visual behaviors like face recognition and reading, and the visual areas supporting them, are still developing<sup><xref rid="R9" ref-type="bibr">9</xref>-<xref rid="R11" ref-type="bibr">11</xref></sup>. Given these gaps in knowledge, we ask: (1) How do pRFs in high-level category-selective regions differ across the ventral, dorsal, and lateral streams? (2) Do pRFs develop during adolescence?</p><p id="P3">In each visual area, pRFs are organized systematically, tiling the visual field; this is referred to as visual field coverage (VFC) of an area<sup><xref rid="R12" ref-type="bibr">12</xref></sup>. While much is known about VFC and pRF properties in early retinotopic areas<sup><xref rid="R13" ref-type="bibr">13</xref>,<xref rid="R14" ref-type="bibr">14</xref></sup>, pRFs in high-level category-selective regions in the ventral, lateral, and dorsal streams have been less studied, partly because traditional pRF mapping experiments used flickering checkerboards designed to activate early and intermediate, rather than high-level visual areas<sup><xref rid="R8" ref-type="bibr">8</xref>,<xref rid="R15" ref-type="bibr">15</xref>-<xref rid="R20" ref-type="bibr">20</xref></sup>. Nonetheless, more recent studies in adults have employed pRF mapping stimuli that include shapes, objects, faces, and colors that drive neurons in high-level regions, consequently enabling the estimation of pRFs in category-selective regions<sup><xref rid="R12" ref-type="bibr">12</xref>,<xref rid="R21" ref-type="bibr">21</xref>-<xref rid="R26" ref-type="bibr">26</xref></sup>. Findings of differential retinotopic biases and pRFs in high-level visual cortex led researchers to hypothesize about the origin of these differences.</p><p id="P4">With respect to category, researchers have hypothesized that fixation patterns on different categories have systematic retinotopic biases that are reflected in retinotopic biases of the respective category-selective region. For example, adults fixate on faces and words in order to recognize faces and read, respectively, and pRFs and VFC in adults&#x02019; ventral face- and word-selective regions are concentrated around the center of gaze (fovea) <sup><xref rid="R22" ref-type="bibr">22</xref>,<xref rid="R27" ref-type="bibr">27</xref>,<xref rid="R28" ref-type="bibr">28</xref>,<xref rid="R28" ref-type="bibr">28</xref>,<xref rid="R29" ref-type="bibr">29</xref></sup>. In general, when people fixate on faces, bodies will be below the face, and this is mirrored in the lower-visual field bias of pRFs of ventral body-selective regions<sup><xref rid="R30" ref-type="bibr">30</xref></sup>. Additionally, in the real world, places encompass the entire visual field, and pRFs and VFC in ventral place-selective areas extend into the periphery<sup><xref rid="R22" ref-type="bibr">22</xref>,<xref rid="R28" ref-type="bibr">28</xref>,<xref rid="R29" ref-type="bibr">29</xref>,<xref rid="R31" ref-type="bibr">31</xref>,<xref rid="R32" ref-type="bibr">32</xref></sup>. The category hypothesis thus suggests that pRF differences across category-selective regions are driven by unique spatial configurations and distinct viewing patterns that are associated with different categories<sup><xref rid="R24" ref-type="bibr">24</xref>,<xref rid="R28" ref-type="bibr">28</xref>,<xref rid="R29" ref-type="bibr">29</xref>,<xref rid="R33" ref-type="bibr">33</xref>,<xref rid="R34" ref-type="bibr">34</xref></sup>.</p><p id="P5">With respect to streams, researchers have hypothesized that different streams have distinct computational objectives or functions<sup><xref rid="R1" ref-type="bibr">1</xref>-<xref rid="R5" ref-type="bibr">5</xref></sup> that utilize visual information from different parts of the visual field. Some studies suggest differences in upper/lower visual field biases across streams with ventral stream pRFs processing the upper visual field and dorsal stream pRFs processing the lower visual field<sup><xref rid="R35" ref-type="bibr">35</xref>,<xref rid="R36" ref-type="bibr">36</xref></sup>. Other studies suggest differences in eccentricity biases across streams whereby pRFs and VFC in ventral face selective regions are centrally biased, but those in lateral face-selective regions extend to the periphery<sup><xref rid="R2" ref-type="bibr">2</xref>,<xref rid="R22" ref-type="bibr">22</xref></sup>. Thus, the stream hypothesis predicts that pRFs will systematically vary across streams. Nonetheless, the category and stream hypotheses are not mutually exclusive, as pRFs and VFC may vary by both category and stream.</p><p id="P6">The category and stream hypotheses offer frameworks for how pRF properties and VFC might differ across visual cortex in adults, but they do not make predictions regarding developmental trajectories. Adolescence, the period between ages 10 to 19 years, presents a unique developmental window as crucial visual behaviors, like face recognition, reading and spatial attention, along with the underlying category selectivity in face- and word- regions, are still developing<sup><xref rid="R10" ref-type="bibr">10</xref>,<xref rid="R37" ref-type="bibr">37</xref>-<xref rid="R40" ref-type="bibr">40</xref></sup>. Currently, we have no knowledge of retinotopic development after age 12<sup><xref rid="R27" ref-type="bibr">27</xref>,<xref rid="R41" ref-type="bibr">41</xref>,<xref rid="R42" ref-type="bibr">42</xref></sup>.</p><p id="P7">Thus, understanding how pRFs may develop during this period will provide important insights into the timeline of development of basic visual functions. We consider two possibilities regarding pRF development: One possibility is that as category selectivity develops into adolescence<sup><xref rid="R10" ref-type="bibr">10</xref>,<xref rid="R37" ref-type="bibr">37</xref>-<xref rid="R39" ref-type="bibr">39</xref></sup>, pRF properties in high-level visual areas will also continue to develop into adolescence. This hypothesis predicts that during adolescence, pRF properties and VFC in category-selective regions will be different from that of adults, and is supported by studies finding that pRFs in pFus-faces and pOTS-word continue to develop from age 5 to adulthood<sup><xref rid="R27" ref-type="bibr">27</xref></sup>. Alternatively, as pRFs perform basic spatial computations on visual inputs, they may mature before higher-level, category computations, and thus may be fully developed by adolescence. This hypothesis predicts no significant difference in the properties of pRFs and VFC in category-selective regions between adolescents and adults and is supported by work demonstrating that pRF properties and VFC of early visual areas (V1-V3) are adultlike as early as 5 to 7 years of age<sup><xref rid="R27" ref-type="bibr">27</xref>,<xref rid="R41" ref-type="bibr">41</xref>,<xref rid="R42" ref-type="bibr">42</xref></sup>. Of course, it is also possible that differential development occurs whereby pRFs/VFC in some streams or category-selective regions mature by adolescence while others continue to develop.</p></sec><sec id="S2"><title>Results</title><sec id="S3"><title>Toonotopy drives high-level category-selective regions in adolescents and adults</title><p id="P8">To test these hypotheses, 15 adolescents (ages 10 - 17; 9 females, 6 males) and 27 adults (ages 22 - 32; 13 females, 14 males) participated in two fMRI experiments, one to map pRFs using sweeping bars with cartoons (Toonotopy, Finzi 2021, <xref rid="F1" ref-type="fig">Fig. 1A</xref>) and another to identify category-selective regions (functional localizer, <xref rid="F2" ref-type="fig">Fig. 2A</xref>). To map pRFs, participants completed four runs of Toonotopy while fixating and performing a color task on fixation (<xref rid="F1" ref-type="fig">Fig. 1A</xref>; Finzi et al., 2021). We model each voxel&#x02019;s pRF with a 2-D Gaussian defined by three main parameters &#x02013; its location (<inline-formula><mml:math id="M3" display="inline"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula>) in the visual field, size (&#x003c3;) or the area of the visual field it processes, and a compressive nonlinearity (<inline-formula><mml:math id="M4" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula>) (<xref rid="F1" ref-type="fig">Fig. 1B</xref>, Kay 2013). Using pRF estimates, we generated polar angle, eccentricity, and size maps in every participant.</p><p id="P9">We first assessed if adolescents have the expected retinotopic maps<sup><xref rid="R8" ref-type="bibr">8</xref>,<xref rid="R13" ref-type="bibr">13</xref>,<xref rid="R15" ref-type="bibr">15</xref>,<xref rid="R31" ref-type="bibr">31</xref></sup>. In every adolescent, we find the expected polar angle map with mirror reversals of the upper and lower visual field representations beginning in the calcarine sulcus (<xref rid="F1" ref-type="fig">Fig. 1C</xref> - Phase, example participant, <xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 1</xref>, all participants). Adolescents also exhibit the expected occipital eccentricity maps showing a gradient of foveal to peripheral representations beginning at the occipital pole and moving anteriorly, as well as a second temporal eccentricity map showing a gradient of foveal to peripheral representations from lateral to medial ventral temporal cortex (VTC, <xref rid="F1" ref-type="fig">Fig. 1C</xref> &#x02013; Eccentricity). Additionally, adolescents have characteristic pRF size maps with pRFs increasing from small to large beginning at the occipital pole and moving anteriorly along the calcarine as well as from the occipital cortex to ventral temporal cortex (<xref rid="F1" ref-type="fig">Fig. 1C</xref> - Size). As expected, pRF x-position, y-position, eccentricity, and size in V1, V2, and V3 do not develop from adolescence to adulthood (<xref rid="SD1" ref-type="supplementary-material">Supplementary Table 1</xref>). For all participants, we quantified the well-established relationship between pRF size and eccentricity across early visual areas<sup><xref rid="R8" ref-type="bibr">8</xref>,<xref rid="R12" ref-type="bibr">12</xref>,<xref rid="R19" ref-type="bibr">19</xref></sup>. In both adolescents and adults, pRF size increases approximately linearly with eccentricity, and the slopes of this relationship increase from V1 to V3 (<xref rid="F1" ref-type="fig">Fig. 1D</xref>, linear mixed model, LMM, <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>E</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:mi>I</mml:mi><mml:mspace width="0.5em"/><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mrow><mml:mo stretchy="false">&#x02215;</mml:mo><mml:mi>V</mml:mi><mml:mstyle mathvariant="italic"><mml:mn>2</mml:mn></mml:mstyle></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">&#x02215;</mml:mo><mml:mi>V</mml:mi><mml:mstyle mathvariant="italic"><mml:mn>3</mml:mn></mml:mstyle></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>), main effect of ROI: p = 7.68*10<sup>&#x02212;25</sup>, F(2,165) = 79.22). We find no significant difference between age groups in pRF size vs. eccentricity slopes (p = 0.89, F(1,33) = 0.02) or intercepts (p = 0.59, F(1,33) = 0.29) or any interactions with age group (p&#x02019;s &#x0003e; 0.18, F&#x02019;s &#x0003c; 1.70; all stats, <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 2</xref>). Together, these results show that Toonotopy can be used to map pRFs in adolescents.</p><p id="P10">Using the category localizer experiment (<xref rid="F2" ref-type="fig">Fig. 2A</xref>), we define face, word, body, and place functional regions of interest (ROIs) in each of these participants (<xref rid="F2" ref-type="fig">Fig. 2B</xref>) and estimated pRFs in each ROI). ROIs were found in both hemispheres and in most participants (<xref rid="SD1" ref-type="supplementary-material">Supplementary Table 3</xref>) except for mOTS-words (mostly left hemisphere), pSTS-faces (mostly adults), and MTG-bodies (mostly adults) As prior research combined the dorsal and lateral stream into a single dorsal stream (e.g. Hasson 2003; Silson 2013; 2016) and because streams differ in the number and type of category-selective regions (<xref rid="F2" ref-type="fig">Fig. 2B</xref>) &#x02013; e.g., dorsal stream contains only one category (places) &#x02013; throughout this study we statistically compare the ventral stream and a combined dorsal-lateral stream.</p><p id="P11">We tested whether Toonotopy drives category-selective regions by evaluating the proportion of voxels in each ROI for which the pRF model explained more than 20% variance during the Toonotopy experiment (<xref rid="F2" ref-type="fig">Fig. 2C</xref> &#x00026; <xref rid="F2" ref-type="fig">2</xref>D). We chose this threshold as it is typically used in the field (e.g., Finzi et al., 2021). In the ventral and dorsal streams, a majority (~80%) of voxels are driven by Toonotopy (<xref rid="F2" ref-type="fig">Fig. 2C</xref>). In the MTG and pSTS MPM ROIs in the lateral stream, fewer voxels (20-60%) are driven by Toonotopy (<xref rid="F2" ref-type="fig">Fig. 2</xref>D). Indeed, the proportion of voxels with greater than 20% variance explained differs significantly across category, stream, and hemisphere <inline-formula><mml:math id="M6" display="inline"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.5em"/><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mstyle mathvariant="italic"><mml:mn>2</mml:mn></mml:mstyle></mml:msup><mml:mo>&#x0003e;</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>0</mml:mn></mml:mstyle><mml:mo>.</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>2</mml:mn></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; category x stream x hemi interaction: p = 0.01, F(2, 681.59) = 4.83; ; all stats, <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 4</xref>). Additionally, adolescents have significantly more voxels driven by the Toonotopy experiment in face-selective regions, but not other category-selective regions compared to adults (significant age group by category interaction (p = 0.01, F(683.89,2) = 4.64, LMM, <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 4</xref>; (post-hoc t-test on faces: p = 0.02, t(252) = 3.12). Overall, while there is regional variability in the proportion of voxels with greater than 20% variance explained, many of the voxels in category selective regions are driven by the Toonotopy experiment.</p></sec><sec id="S4"><title>pRFs vary by stream, category, and hemisphere and develop in size but not location</title><p id="P12">To assess how pRFs properties may vary by processing stream and category, we examined the distribution of pRF centers and their sizes for each ROI in adolescents (<xref rid="F3" ref-type="fig">Fig 3A</xref> - <italic toggle="yes">Top</italic>) and adults (<xref rid="F3" ref-type="fig">Fig. 3A</xref> - <italic toggle="yes">Bottom</italic>). Across all ROIs and groups, pRF centers lie in the contralateral visual field with no significant differences across age groups (Supplementary Fig. 5; LMM: <inline-formula><mml:math id="M7" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mtext>-</mml:mtext><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, main effect of hemisphere: p = 2.73*10-<sup>230</sup>, F(1, 688.85) = 2476.21 all stats, <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 5</xref>). However, pRF distributions appear to differ systematically across both category and stream. For instance, pRFs of ventral face ROIs (IOG/pFus/mFus) are concentrated within the central 5&#x000b0;, those of ventral body ROIs (OTS) span the central 10&#x000b0;, and pRFs in lateral face (pSTS) and body ROIs (LOS, ITG, MTG) extend more peripherally up to 20&#x000b0; (<xref rid="F3" ref-type="fig">Fig 3A</xref>). These qualitative observations align with our hypotheses, which make different predictions about a region&#x02019;s pRF distributions in the center versus periphery or upper versus lower visual field depending on its processing stream or category selectivity. To qualitatively test these predictions, we use linear mixed models (LMMs) to compare pRF parameters across age group (adolescents/adults), streams (ventral/dorsal-lateral), categories (faces, bodies, places), and hemispheres (right/left). To include word-selective ROIs, which were only found in the ventral stream, we used a second, ventral LMM, comparing age group, category, and hemisphere across ventral stream ROIs.</p><sec id="S5"><title>Y position of pRF centers.</title><p id="P13">Building on the observed differences in pRF distributions, we calculated pRF vertical (<inline-formula><mml:math id="M8" display="inline"><mml:mi>y</mml:mi></mml:math></inline-formula>) position in each participant and ROI (<xref rid="F3" ref-type="fig">Fig. 3B</xref>) and evaluated the stream and category hypotheses as well as any development. Our data reveal no significant differences in vertical pRF positions across age groups (F(1, 51.33) = 1.10, p = 0.30, <inline-formula><mml:math id="M9" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mspace width="0.5em"/><mml:mi>y</mml:mi><mml:mtext>-</mml:mtext><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; F(1, 46.43) = 0.27, p = 0.60, <inline-formula><mml:math id="M10" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mspace width="0.5em"/><mml:mi>y</mml:mi><mml:mtext>-</mml:mtext><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>), nor any interactions between age group and other factors (<xref rid="SD1" ref-type="supplementary-material">Supplementary Table 6</xref>). These findings suggest that the vertical location of pRFs is mature by adolescence.</p><p id="P14">Previous studies have largely supported the stream hypothesis, predicting that ventral regions favor the upper visual field and dorsal regions favor the lower visual field<sup><xref rid="R35" ref-type="bibr">35</xref>,<xref rid="R36" ref-type="bibr">36</xref></sup>. However, our results indicate that vertical pRF positions are not determined by stream but instead by category (<inline-formula><mml:math id="M11" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> main effect of category: p = 1.69*10<sup>&#x02212;22</sup>, F(2, 682.79) = 54.0; <inline-formula><mml:math id="M12" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> main effect of category: F(3,354.26)=54.08; p=8.42*10<sup>&#x02212;29</sup>; <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 6</xref>). Across visual cortex, pRFs show category effects as body-selective ROIs display a lower visual field bias (besides left MTG-bodies), place-selective ROIs exhibit an upper visual field bias (besides MOG-places), and face- and word-selective ROIs are located near the horizontal meridian (<xref rid="F3" ref-type="fig">Fig. 3B</xref>). Though category effects are evident, we observed that vertical biases are not always uniform by category but also vary by stream (<inline-formula><mml:math id="M13" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> category x stream effect: p = 6.56*10<sup>&#x02212;6</sup>, F(2, 682.10) = 12.15), particularly in body- and place-selective regions as noted. Specifically, pRFs in ventral OTS-bodies exhibit a lower visual field bias, but pRFs in lateral MTG-bodies are horizontally biased. Likewise, pRFs in ventral CoS-places have an upper visual field bias whereas pRFs in dorsal MOG-places show a lower visual field bias (<xref rid="F3" ref-type="fig">Fig. 3A</xref>, <xref rid="F3" ref-type="fig">B</xref>).</p><p id="P15">Unexpectedly, we find significant hemispheric differences in pRF vertical location (<inline-formula><mml:math id="M14" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 1.30*10<sup>&#x02212;7</sup>, F(1, 686.92) = 28.47; <inline-formula><mml:math id="M15" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 5.66*10<sup>&#x02212;12</sup>, F(1, 357.27) = 50.81) and significant category by hemisphere interactions (<inline-formula><mml:math id="M16" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 0.03, F(2, 681.69)= 3.44; <inline-formula><mml:math id="M17" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 0.02, F(3, 353.77)= 3.33; <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 6</xref>). Overall, left hemisphere pRFs are more superior than right hemisphere pRFs. For example, pRFs in left word-selective ROIs are near the horizontal meridian, but pRFs in right word-selective ROIs have a lower visual field bias. These hemispheric differences are pronounced in place-selective regions whereby CoS- and IPS-place ROIs show upper visual field biases in the left hemisphere but equal distribution of pRFs across the upper and lower visual field in the right hemisphere. Together, these data highlight that vertical biases in high-level visual areas mature by adolescence and vary across categories, with additional modulation by stream and hemisphere.</p></sec><sec id="S6"><title>pRF Eccentricity.</title><p id="P16">With respect to eccentricity, a large body of literature has documented center versus periphery differences in pRF location across category-selective regions, with pRFs in face- and word-selective ROIs exhibiting a central bias &#x02013; which develops in childhood<sup><xref rid="R27" ref-type="bibr">27</xref></sup> &#x02013; and place-selective regions showing a peripheral bias<sup><xref rid="R22" ref-type="bibr">22</xref>,<xref rid="R28" ref-type="bibr">28</xref>,<xref rid="R29" ref-type="bibr">29</xref>,<xref rid="R32" ref-type="bibr">32</xref></sup>. We find no significant development in pRF eccentricity from adolescence to adulthood (no significant age group or age group interaction effects: p&#x02019;s &#x0003e; 0.28, F&#x02019;s &#x0003c; 1.27; <inline-formula><mml:math id="M18" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mspace width="0.5em"/><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; p=0.54, F(1,52)=0.27, <inline-formula><mml:math id="M19" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mspace width="0.5em"/><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; full stats, <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 7</xref>).</p><p id="P17">While pRF eccentricity significantly varies across categories (<inline-formula><mml:math id="M20" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p=2.12*10<sup>&#x02212;3</sup>, F(2,685.49) = 6.21; <inline-formula><mml:math id="M21" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p=2.1*10<sup>&#x02212;41</sup>, F(3,358.5) = 84.59) with face-selective ROIs displaying more centrally-located pRFs compared to other ROIs (<xref rid="F3" ref-type="fig">Fig. 3C</xref>), our data show that pRF eccentricity is more prominently determined by stream (<inline-formula><mml:math id="M22" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 4.19*10<sup>&#x02212;19</sup>, F(1,693.18) = 84.62), with pRFs in the lateral stream located, in general, more peripherally than those in the ventral stream (<xref rid="F3" ref-type="fig">Fig. 3C</xref>). This stream effect is further modulated by category (stream x category interaction: p = 5.38*10<sup>&#x02212;41</sup>, F(2, 684.25)= 106.51; <inline-formula><mml:math id="M23" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>) where in the ventral stream, the most peripheral pRFs are in the CoS-places, but in the dorsal-lateral stream, the most peripheral pRFs are in body-selective ROIs (LOS/ITG/MTG; <xref rid="F3" ref-type="fig">Fig. 3C</xref>).</p><p id="P18">Additionally, there are significant differences in pRF eccentricity across hemispheres (<inline-formula><mml:math id="M24" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> F(1,691.78)=58.60, p=6.5*10<sup>&#x02212;14</sup>, <inline-formula><mml:math id="M25" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p=5.53*10<sup>&#x02212;6</sup>, F(1,364,21)=21.27) whereby pRFs in the left hemisphere are more peripheral than those in the right hemisphere (<xref rid="F3" ref-type="fig">Fig 3C</xref>). Furthermore, we observe significant interactions between stream, category, and hemisphere <inline-formula><mml:math id="M26" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p=2.65*10<sup>&#x02212;5</sup>, F(2,682.72) = 10.7). In the ventral stream, pRFs lie in the central 0&#x000b0; to 5&#x000b0;in right face-selective regions, between 5&#x000b0; to 10&#x000b0; in body- and word-selective ROIs, and extend from 10&#x000b0; to 20&#x000b0; in place-selective regions. But in the dorsal-lateral stream, pRFs in right face- and place-selective regions lie between 5&#x000b0; to 10&#x000b0;, and pRFs in bilateral body- and left face-selective ROIs extend to 10&#x000b0; to 20&#x000b0;. Overall, these data demonstrate that pRF eccentricity is mature by adolescence and that pRF organization in category-selective regions does not segregate simply by category but emerges from the interplay of stream, category, and hemisphere.</p></sec><sec id="S7"><title>pRF size.</title><p id="P19">While previous studies have not explicitly predicted how pRF size might vary across categories and streams, in retinotopic ROIs pRF size increases linearly with eccentricity. This suggests that pRF size in category-selective regions may mirror pRF eccentricity and exhibit differences across streams, categories, and hemispheres. Additionally, prior work<sup><xref rid="R27" ref-type="bibr">27</xref></sup> found that pRF size increases in ventral stream pFus-faces and pOTS-words from childhood to adulthood, but it remains unknown whether pRF size continues to develop during adolescence or in other category-selective ROIs and streams.</p><p id="P20">Unlike pRF location, pRF size continues to develop from adolescence to adulthood, with significant differences across age group (p=0.02, F(1, 64.2)= 5.62, <inline-formula><mml:math id="M27" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mspace width="0.5em"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>), age group and hemisphere (F(1,693.04)=9.83, p=1.79*10<sup>&#x02212;3</sup>), age group, stream, and hemisphere (<inline-formula><mml:math id="M28" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p=0.04, F(1,692.73) = 4.06), and age group, stream, category, and hemisphere (<inline-formula><mml:math id="M29" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p=0.03, F(1,683.16) = 3.62; full stats, <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 8</xref>). Generally, adolescents have larger pRFs than adults, especially in the left hemisphere and in non-face-selective regions (<xref rid="F3" ref-type="fig">Fig. 3D</xref>). Across hemispheres and streams, larger pRF sizes in adolescents compared to adults are more pronounced in the lateral stream than the ventral stream and in body- and place-selective ROIs than face- and word-selective ROIs (besides pSTS-faces, <xref rid="F3" ref-type="fig">Fig. 3D</xref>). In the ventral stream, we do not observe significant development in pRF size (<inline-formula><mml:math id="M30" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> ps&#x0003e;0.05; Fs &#x0003c;3.94; <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 8</xref>).</p><p id="P21">Beyond developmental effects, pRF sizes vary significantly by stream (<inline-formula><mml:math id="M31" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p=3.79*10<sup>&#x02212;9</sup>, F(1,694.48)= 35.64), with larger pRFs in the dorsal-lateral than ventral stream. PRF size also varies by category (<inline-formula><mml:math id="M32" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p=7.33*10<sup>&#x02212;10</sup>, F(2, 686.2)= 21.69; <inline-formula><mml:math id="M33" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p=1.7*10<sup>&#x02212;11</sup>, F(3, 359.16.)= 19.07), as pRFs are larger in body- and place-selective ROIs compared to face- and word-selective ROIs. Like pRF position, pRF size shows an interaction between stream and category (<inline-formula><mml:math id="M34" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p=1.77*10<sup>&#x02212;11</sup>, F(2,684.82)= 25.68; <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 8</xref>). For example, in the ventral stream, pRFs in body-selective ROIs are smaller than those in place-selective ROIs while in the dorsal-lateral stream, pRFs in body-selective ROIs are larger than those in place-selective ROIs (<xref rid="F3" ref-type="fig">Fig. 3D</xref>).</p><p id="P22">Together, these results indicate that pRF size mirrors pRF eccentricity, with regions exhibiting more peripheral pRFs also exhibiting larger pRFs. Furthermore, we find that from adolescence to adulthood, pRF size decreases, particularly in body- and place-selective ROIs and in the dorsal-lateral stream.</p></sec></sec><sec id="S8"><title>Visual field coverage differentially samples the visual field and continues to mature from adolescence to adulthood</title><p id="P23">To summarize the collective effect of pRF location and size, we calculated the visual field coverage (VFC) of each ROI (<xref rid="F4" ref-type="fig">Fig. 4A</xref>, <xref rid="F4" ref-type="fig">B</xref>) integrating pRF location and size. VFC captures how pRFs within a region collectively tile the visual field, indicating the total area of the visual field each region processes information from. Given our findings that pRF size continues to develop, decreasing in some regions, we investigated whether this decrease is coupled with smaller VFC in adults compared to adolescents in the corresponding ROIs. Thus, we quantify the full-width half-max (FWHM; <xref rid="F4" ref-type="fig">Fig. 4A</xref>, <xref rid="F4" ref-type="fig">B</xref> - black dotted line) of the VTC in each participant and ROI and compare across age groups, streams, categories, and hemispheres (<inline-formula><mml:math id="M35" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mi>F</mml:mi><mml:mi>W</mml:mi><mml:mi>H</mml:mi><mml:mi>M</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mspace width="0.5em"/><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mi>F</mml:mi><mml:mi>W</mml:mi><mml:mi>H</mml:mi><mml:mi>M</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>).</p><p id="P24">In parallel with pRF size, VFC develops differentially from adolescence to adulthood, with total FWHM varying significantly (<xref rid="F4" ref-type="fig">Fig. 4C</xref>) by age group, stream, and category (<inline-formula><mml:math id="M36" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 2.62*10<sup>&#x02212;3</sup>, F(2, 685.28) = 6) as well as by age group, stream, and hemisphere (<inline-formula><mml:math id="M37" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 7.68*10<sup>&#x02212;3</sup>, F(1, 693.29) = 7.15; <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 9</xref>). VFC development is more pronounced in the right hemisphere, in the dorsal-lateral stream, and in face ROIs, as we observe significant decreases in overall coverage of the visual field for right pSTS-faces (post-hoc t-test: t(27.06) = 5.20, p = 1.98*10<sup>&#x02212;3</sup>) and right IPS-places (post-hoc t-test: t(37.99) = 2.06, p = 0.05) and significant increases in coverage for right IOG-faces (post-hoc t-test: t(6.04) = &#x02212;3.09, p = 4.61*10<sup>&#x02212;3</sup>) from adolescence to adulthood.</p><p id="P25">In addition, we observe differences in VFC across streams (<inline-formula><mml:math id="M38" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 0.05, F(1,698.78) = 4), with the ventral stream displaying a central bias (~30% of the FWHM in the center 5&#x000b0; of the visual field) and the dorsal-lateral stream showing more peripheral coverage ( just ~15% of the FWHM in the central 5&#x000b0;, <xref rid="F4" ref-type="fig">Fig. 4A</xref>, <xref rid="F4" ref-type="fig">B</xref>; <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 10</xref>). Additionally, the lateral ROIs exhibit more contralateral VFC than both ventral stream ROIs and dorsal place ROIs (<xref rid="F4" ref-type="fig">Fig. 4A</xref>,<xref rid="F4" ref-type="fig">B</xref>; <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 10</xref>). VFC also differed significantly by category (<inline-formula><mml:math id="M39" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 0.02, F(2,689.52) = 3.85; <inline-formula><mml:math id="M40" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 1.55*10<sup>&#x02212;8</sup>, F(3, 359.39) = 13.80; <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 9</xref>). For example, the ipsilateral visual field coverage is larger for face-selective ROIs than place and body ROIs (<xref rid="F4" ref-type="fig">Fig. 4</xref>; <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 10</xref>). Notably, like with pRF location and size, VFC varied by stream and category (<inline-formula><mml:math id="M41" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 9.58 * 10<sup>&#x02212;7</sup>-, F(2,689.01) = 14.14; <xref rid="F4" ref-type="fig">Fig. 4C</xref>), as there is a central bias in ventral face- and body-selective ROIs (~35% of FWHM in central 5&#x000b0;) but a peripheral bias in their lateral counterparts (~17% of FWHM in the central 5&#x000b0;, <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 10</xref>). Place-selective ROIs, however, had similarly peripheral coverage of the visual field across streams (~13%, <xref rid="F4" ref-type="fig">Fig. 4A</xref>, <xref rid="F4" ref-type="fig">B</xref>), even as their upper vs. lower visual coverage varied across streams. Overall, we find the largest central bias in the right ventral face ROIs, in which over 50% of the VFC is concentrated in the central 5&#x000b0;.</p><p id="P26">Collectively, analysis of VFC reveals both decreases and increases in VFC of category-selective ROIs from adolescence to adulthood.</p></sec><sec id="S9"><title>Category selectivity continues to develop during adolescence alongside spatial computations</title><p id="P27">Our findings reveal that while pRF location is mature by adolescence, pRF size and VFC continue to develop into adulthood, raising important questions about how these changes relate to the development of category selectivity. As previous work finds that category selectivity continues to mature into adolescence<sup><xref rid="R10" ref-type="bibr">10</xref>,<xref rid="R37" ref-type="bibr">37</xref>-<xref rid="R39" ref-type="bibr">39</xref></sup>, we examined if category responses develop in these same participants and ROIs and whether this development might be linked with the development of pRFs.</p><p id="P28">To do so, we quantified category selectivity (mean t-value) in 10mm disks ROI centered on each category ROI and tested if selectivity varied across age group, stream, category, and hemisphere (<xref rid="F5" ref-type="fig">Fig. 5A</xref>; <inline-formula><mml:math id="M42" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.5em"/><mml:mi>t</mml:mi><mml:mtext>-</mml:mtext><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mspace width="0.5em"/><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.5em"/><mml:mi>t</mml:mi><mml:mtext>-</mml:mtext><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.5em"/><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>). We find differential development of category selectivity from adolescence to adulthood (significant age group by category interactions, <inline-formula><mml:math id="M43" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 2.36 * 10<sup>&#x02212;7</sup> F(2, 746.92) = 15.58; <inline-formula><mml:math id="M44" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:math></inline-formula> p = 1.81 *10<sup>&#x02212;3</sup>, F(3, 358.77) = 5.1; <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 11</xref>). This development is associated with significant decreases in place selectivity across all place ROIs, bilaterally, (post-hoc t-tests: p&#x02019;s &#x0003c;0.01, t&#x02019;s &#x0003e; 2.39), and significant increases in face selectivity in right hemisphere IOG-faces (post-hoc t-tests: p = 0.02, t(28.27) = &#x02212;2.40; <xref rid="F5" ref-type="fig">Fig. 5A</xref>). Additionally, we find that the size of these regions also develops into adolescence (<xref rid="SD1" ref-type="supplementary-material">Supplementary Table 12</xref>).</p><p id="P29">Given that VFC and category selectivity both develop and are each related to visual behaviors like reading and face recognition<sup><xref rid="R24" ref-type="bibr">24</xref>,<xref rid="R37" ref-type="bibr">37</xref>,<xref rid="R44" ref-type="bibr">44</xref>,<xref rid="R45" ref-type="bibr">45</xref></sup>, we examined if these properties are linked. We find a significant link between category selectivity and VFC (<xref rid="F5" ref-type="fig">Fig. 5B</xref>; p = 0.03, F(1, 628.47) = 4.67, <inline-formula><mml:math id="M45" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mspace width="0.5em"/><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.5em"/><mml:mi>t</mml:mi><mml:mtext>-</mml:mtext><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>F</mml:mi><mml:mi>W</mml:mi><mml:mi>H</mml:mi><mml:mi>M</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; p = 9.45*10<sup>&#x02212;5</sup>, F(1, 323.53) = 15.63, <inline-formula><mml:math id="M46" display="inline"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi><mml:mo>:</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.5em"/><mml:mi>t</mml:mi><mml:mtext>-</mml:mtext><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.5em"/><mml:mi>F</mml:mi><mml:mi>W</mml:mi><mml:mi>H</mml:mi><mml:mi>M</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.5em"/><mml:mi>x</mml:mi><mml:mspace width="0.5em"/><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="italic"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; full stats, <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 13</xref>), which additionally varies by stream and category (significant interaction FWHM x category x stream: p = 6.81*10<sup>&#x02212;3</sup>, F(2, 624.3) = 5.03, <inline-formula><mml:math id="M47" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mi>M</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>). That is, in left pFus-faces, LOS-bodies, and IPS-places, as well as right pOTS-words, ITG-bodies, and MOG-places (but not other ROIs) higher category selectivity is linked with greater VFC (<xref rid="F5" ref-type="fig">Fig. 5B</xref>).</p><p id="P30">Together, these results suggest that category selectivity continues to develop after pRF location has matured and parallels the development of VFC with larger category selectivity associated with larger VFC.</p></sec></sec><sec id="S10"><title>Discussion</title><p id="P31">Here, we examined pRFs in category-selective regions across the ventral, dorsal, and lateral visual streams and chart their development from adolescence to adulthood using a novel experiment that drives high-level regions. Across all ages, we find the location and sizes of individual pRFs, as well as their combined VFC, vary systematically across both category and stream. While the location of pRFs remains stable from adolescence to adulthood, we find that pRF size and VFC continue to develop differentially during adolescence alongside category selectivity. Functionally, our findings suggest that visuospatial processing is not governed by a single principle but by an interplay of category and stream, as well as hemisphere, necessitating a rethinking of visuospatial computations by pRFs in visual cortex. Developmentally, we find that the area of the visual field in which visual information is integrated shows a protracted refinement through adolescence, underscoring the importance of studying the development of the adolescent brain.</p><sec id="S11"><title>Visuospatial computations in high-level visual cortex vary by both stream and category</title><p id="P32">Prior research has implicated either category or stream in how category-selective regions sample the visual field. Eccentricity Bias Theory<sup><xref rid="R29" ref-type="bibr">29</xref></sup> highlights the relation between category selectivity and eccentricity based on evidence that ventral face- and word-selective regions have central biases, whereas place-selective regions have peripheral biases <sup><xref rid="R22" ref-type="bibr">22</xref>,<xref rid="R27" ref-type="bibr">27</xref>-<xref rid="R29" ref-type="bibr">29</xref>,<xref rid="R31" ref-type="bibr">31</xref>,<xref rid="R32" ref-type="bibr">32</xref>,<xref rid="R46" ref-type="bibr">46</xref></sup>. Stream Theory<sup><xref rid="R35" ref-type="bibr">35</xref>,<xref rid="R36" ref-type="bibr">36</xref></sup> highlights the coupling between stream and vertical biases, suggesting that like biases in EVC, ventral stream regions have an upper visual field bias and dorsal stream regions have a lower field bias. However, our results suggest that pRF eccentricity, vertical position, size, and VFC vary across streams, categories, and hemispheres. Surprisingly, pRF eccentricity varies more by stream than by category, with ventral stream pRFs located more centrally than dorsal and lateral streams pRFs which extend to the periphery, while pRF vertical bias varies more by category than by stream, with body-selective regions exhibiting more pRFs in the lower visual field, place-selective regions having more pRFs in the upper visual field, and pRFs in face- and word-selective regions more concentrated along the horizontal meridian. Importantly, we find that the contributions of category and stream for pRF eccentricity and vertical position are not mutually exclusive but rather exhibit significant interactions: pRFs in ventral face and body-selective regions are smaller and more central than those in the place selective regions, but the pattern is reversed in dorso-lateral regions. Finally, pRFs and VFC in right hemisphere category-selective regions extend more into the ipsilateral and lower visual field compared to the left hemisphere. These findings challenge previous hypotheses and suggest that different dimensions than previously posited drive differences in pRF eccentricity and vertical position in high-level visual regions.</p><p id="P33">We hypothesize that retinotopic regularities associated with viewing specific categories in the context of different tasks may shape the differential pRFs properties across category-selective regions, streams, and hemispheres. Prior research has theorized that retinotopic biases in category selective regions are tied to the regularity of different categories in specific locations in the visual field<sup><xref rid="R22" ref-type="bibr">22</xref>,<xref rid="R24" ref-type="bibr">24</xref>,<xref rid="R27" ref-type="bibr">27</xref>-<xref rid="R30" ref-type="bibr">30</xref>,<xref rid="R32" ref-type="bibr">32</xref>,<xref rid="R47" ref-type="bibr">47</xref>-<xref rid="R50" ref-type="bibr">50</xref></sup>. For example, ventral regions associated with categories that observers tend to fixate upon &#x02013; faces, words, and even cultural objects like Pokemon<sup><xref rid="R48" ref-type="bibr">48</xref>,<xref rid="R51" ref-type="bibr">51</xref>-<xref rid="R54" ref-type="bibr">54</xref></sup> &#x02013; have a foveal bias<sup><xref rid="R28" ref-type="bibr">28</xref>,<xref rid="R32" ref-type="bibr">32</xref>,<xref rid="R55" ref-type="bibr">55</xref>,<xref rid="R56" ref-type="bibr">56</xref></sup>. Interestingly, the tendency to fixate on faces produces higher occurrences of bodies and limbs in the lower visual field<sup><xref rid="R30" ref-type="bibr">30</xref>,<xref rid="R51" ref-type="bibr">51</xref></sup>, reflected in the lower field bias of body-selective regions<sup><xref rid="R30" ref-type="bibr">30</xref></sup>. In the real world, places extend to the periphery, reflected in the peripheral bias of place-selective regions<sup><xref rid="R22" ref-type="bibr">22</xref>,<xref rid="R28" ref-type="bibr">28</xref>,<xref rid="R32" ref-type="bibr">32</xref>,<xref rid="R57" ref-type="bibr">57</xref></sup>. We hypothesize that differential pRFs across categories and streams may be a result of the statistical regularity of visuospatial information during specific tasks. That is, pRFs and population codes across pRFs spanning a region may be optimized for processing category-relevant and task-specific information.</p><p id="P34">Indeed, several studies have shown a link between the population code spanned by pRFs tiling a region and behavior<sup><xref rid="R12" ref-type="bibr">12</xref>,<xref rid="R23" ref-type="bibr">23</xref>,<xref rid="R24" ref-type="bibr">24</xref>,<xref rid="R27" ref-type="bibr">27</xref>,<xref rid="R30" ref-type="bibr">30</xref>,<xref rid="R47" ref-type="bibr">47</xref>,<xref rid="R58" ref-type="bibr">58</xref>,<xref rid="R59" ref-type="bibr">59</xref></sup>. For instance, spatial computations in TOS-places predict navigational affordances<sup><xref rid="R47" ref-type="bibr">47</xref></sup> and differences in pRFs for upright versus upside-down faces in ventral face-selective regions predict the face inversion effect<sup><xref rid="R24" ref-type="bibr">24</xref></sup>. Thus, we hypothesize that visual navigational affordances, a task associated with TOS-places in the dorsal stream<sup><xref rid="R60" ref-type="bibr">60</xref>-<xref rid="R62" ref-type="bibr">62</xref></sup>, are more prominent in the lower-visual field and require more central processing than recognition of places<sup><xref rid="R29" ref-type="bibr">29</xref>,<xref rid="R63" ref-type="bibr">63</xref>,<xref rid="R64" ref-type="bibr">64</xref></sup>, but features in the periphery as well as in the upper visual field (e.g., sky, ceiling) may aid scene classification supported by CoS-places. Likewise, processing of visual dynamics and social information associated with body and face regions in the lateral stream<sup><xref rid="R2" ref-type="bibr">2</xref>-<xref rid="R4" ref-type="bibr">4</xref>,<xref rid="R6" ref-type="bibr">6</xref></sup> may require processing of biological motion in the periphery, whereas face recognition<sup><xref rid="R65" ref-type="bibr">65</xref></sup> and learning to read<sup><xref rid="R40" ref-type="bibr">40</xref></sup> (and write<sup><xref rid="R66" ref-type="bibr">66</xref></sup>) tasks associated with the ventral stream, may be associated with fixation on faces, words, and hands, respectively. This hypothesis can be tested with advances in mobile eye tracking that can be used to quantify observers&#x02019; visual diet (occurrence of different categories in different parts of the visual field) under different tasks.</p></sec><sec id="S12"><title>pRF location is mature by adolescence but spatial integration continues to develop</title><p id="P35">Across adolescents and adults, we find no quantitative or qualitative differences in pRF eccentricity or vertical position in category-selective regions in all visual streams. These data suggest that pRF location is mature by adolescence, consistent with prior work showing that pRFs in EVC (V1-V3) are mature by childhood<sup><xref rid="R27" ref-type="bibr">27</xref>,<xref rid="R41" ref-type="bibr">41</xref>,<xref rid="R42" ref-type="bibr">42</xref></sup>.</p><p id="P36">In contrast to pRF location, we find that pRF size and VFC continue to change during adolescence, as does category selectivity in the same individuals. Previous work<sup><xref rid="R27" ref-type="bibr">27</xref></sup> finds that pRFs in ventral face and word-selective regions develop during childhood. Our results suggest that the area of the visual field from which neurons in high-level face- and place-selective regions spatially integrate information also continues to develop during adolescence, and the differential development across hemispheres may be associated with functional specialization<sup><xref rid="R55" ref-type="bibr">55</xref>,<xref rid="R67" ref-type="bibr">67</xref></sup>. Furthermore, in addition to corroborating the hypothesis that category selectivity continues to develop into adolescence<sup><xref rid="R10" ref-type="bibr">10</xref>,<xref rid="R37" ref-type="bibr">37</xref>-<xref rid="R39" ref-type="bibr">39</xref></sup>, our findings reveal a nuanced timeline of visual development, illustrating that pRF location matures first and may support the ongoing maturation of pRF size, VFC, and category selectivity &#x02013; which develop in parallel.</p><p id="P37">The dynamic nature of visual development is exemplified by the differential changes observed in category selectivity and VFC. VFC and category selectivity increase for right hemisphere IOG-faces but decrease for right hemisphere place-selective regions highlighting the coupling between functional specialization and the underlying spatial computations. Critically, this sheds light on the fact that cortical development is not just associated with expansion but also with reduction. This pattern of development has important implications for theories regarding how cortex is recycled to adapt for changing behaviors over development<sup><xref rid="R37" ref-type="bibr">37</xref>,<xref rid="R56" ref-type="bibr">56</xref></sup>. Behaviorally, mature pRF location in adolescents&#x02019; high-level visual areas across streams suggest the possibility that adolescents have similar fixation patterns as adults under different tasks. Nonetheless, developmental changes in pRF size and VFC suggest differential spatial integration across adolescents and adults, which may correlate with differential performance in specific behaviors, e.g., in face recognition, reading, and navigation<sup><xref rid="R12" ref-type="bibr">12</xref>,<xref rid="R24" ref-type="bibr">24</xref>,<xref rid="R27" ref-type="bibr">27</xref>,<xref rid="R47" ref-type="bibr">47</xref>,<xref rid="R58" ref-type="bibr">58</xref></sup>.</p><p id="P38">It is interesting to consider the mechanisms underlying these differential developments. Prior research suggests that eccentricity biases in high-level visual cortex mirror white matter connections<sup><xref rid="R22" ref-type="bibr">22</xref>,<xref rid="R68" ref-type="bibr">68</xref></sup> and functional connectivity<sup><xref rid="R69" ref-type="bibr">69</xref>-<xref rid="R72" ref-type="bibr">72</xref></sup> to eccentricity bands in EVC. For example, ventral face- and word-selective regions have more white matter connections to EVC in the central 5&#x000b0; than in peripheral bands, while place-selective regions have more connections to peripheral EVC bands than the central 5&#x000b0;<sup><xref rid="R22" ref-type="bibr">22</xref>,<xref rid="R68" ref-type="bibr">68</xref></sup>, and these connectivity patterns are present at birth<sup><xref rid="R68" ref-type="bibr">68</xref></sup>. Together, these discoveries suggest that earlier development of pRF location may be established by long range white matter connections that are present in infancy. However, the distribution of white matter connections between category-selective regions in the lateral and dorsal stream to EVC as well as their development is unknown, and can be tested in future research. We hypothesize that the later maturation of pRF size may be related to protracted development of dendritic arborization and synaptic weights in high-level visual cortex<sup><xref rid="R73" ref-type="bibr">73</xref>,<xref rid="R74" ref-type="bibr">74</xref></sup>, which may affect spatial pooling. Another possibility is that changing visual behaviors during adolescence might affect the area over which information is spatially integrated. Future research examining the longitudinal development of adolescents&#x02019; visual diet, together with longitudinal measurements of pRFs and category selectivity, may shed light on how one&#x02019;s visual diet and behaviors impact spatial integration and category selectivity.</p><p id="P39">Overall, our study suggests a rethinking of spatial computations in high-level visual cortex and their development during adolescence. As such, our research sets a new foundation for future investigations into how visual experience and viewing patterns may sculpt visuospatial computations across development and how this processing might diverge in cases of atypical development like autism<sup><xref rid="R75" ref-type="bibr">75</xref></sup>, dyslexia<sup><xref rid="R40" ref-type="bibr">40</xref>,<xref rid="R76" ref-type="bibr">76</xref></sup>, or prosopagnosia<sup><xref rid="R77" ref-type="bibr">77</xref>-<xref rid="R79" ref-type="bibr">79</xref></sup> where altered viewing patterns may also alter visuospatial computations.</p></sec></sec><sec id="S13"><title>Methods</title><sec id="S14"><title>Participants</title><p id="P40">19 neurologically typical adolescents aged 10 to 17 years old (M = 13.74 &#x000b1; 2.13; 11 females, 8 males) and 27 adults (ages 22 - 32; M = 25.52 &#x000b1; 3.00; 13 females, 14 males) participated in this study. 4 of the adolescents participated in two scanning sessions approximately a year apart. All participants had normal or corrected-to-normal vision. Participants, or their parents, gave written informed consent, and all procedures were approved by the Stanford Internal Review Board on Human Subjects Research.</p><p id="P41">Sessions were excluded from the analysis if within scan motion and/or between scan motion was greater than 2 voxels. Of the 19 adolescent sessions, 4 participants were excluded based on these motion criteria. No adult sessions were excluded. After exclusion, we analyzed the data of 15 adolescents (ages 10 - 17; 9 females, 6 males) and 27 adults (ages 22 - 32; 13 females, 14 males).</p></sec><sec id="S15"><title>Data Acquisition</title><sec id="S16"><title>MRI.</title><p id="P42">Participants were scanned using a General Electric Discovery MR750 3T scanner located in the Center for Cognitive and Neurobiological Imaging (CNI) at Stanford University. A phase-array 32-channel head coil was used for the category localizer experiment and to obtain anatomical scans. For the toonotopy experiment, a 16 channel head coil was used.</p></sec><sec id="S17"><title>Anatomical scans.</title><p id="P43">For each participant, we obtained a whole-brain anatomical volume using a T1-weighted pulse sequence (TI = 450ms, 1x1x1mm, flip angle = 12 degrees, FoV = 204 mm). Anatomical images of each brain were used for segmentation of the gray/white matter boundary.</p></sec><sec id="S18"><title>Toonotopy.</title><p id="P44">Participants completed four runs of a wide-field pRF mapping fMRI experiment with cartoon stimuli, which we refer to as Toonotopy<sup><xref rid="R22" ref-type="bibr">22</xref></sup>. In the experiment (<xref rid="F1" ref-type="fig">Fig. 1A</xref>), bars of width 5.7&#x000b0; swept a circular 40&#x000b0;x40&#x000b0; (visual angle) aperture with a fixation dot at center. The bars swept the visual field at four orientations (0&#x000b0;, 45&#x000b0;, 90&#x000b0;, 135&#x000b0;) in eight directions (2 opposite directions orthogonal to each orientation). The cartoon stimuli randomly changed at a rate of 8 Hz with blanks (mean luminance gray background with fixation) appearing at regular intervals. During each run, participants fixated on the central dot and were instructed to press a button whenever the dot changed colors. Each run was 3 minutes and 24 seconds long.</p></sec><sec id="S19"><title>Category localizer.</title><p id="P45">The same participants underwent an fMRI category localizer experiment, which is used to identify voxels whose neural response is stronger to one category vs. many other categories<sup><xref rid="R43" ref-type="bibr">43</xref></sup>. In each run, participants were presented with stimuli from five domains, each with two categories (<xref rid="F2" ref-type="fig">Fig. 2A</xref>; faces: child, adult; bodies: whole, limbs; places: corridors, houses; objects: cars, guitars; characters: words, numbers). Images within the same category were presented in 4s blocks at a rate of 2Hz and were not repeated across blocks or runs. 4s blank trials were also presented throughout a block. During a run, each category was presented eight times in counterbalanced order, with the order differing for each run. Throughout the experiment, participants fixated on a central dot and performed an oddball detection task, pressing a button when phase-scrambled images randomly appeared. Each participant completed 3 runs of the category localizer experiments with different images; each run was 5 minutes and 18 seconds long.</p></sec></sec><sec id="S20"><title>Data Analysis</title><sec id="S21"><title>Anatomical data analysis.</title><p id="P46">T1-weighted images were automatically segmented using FreeSurfer (FreeSurfer 7.0.0:<sup><xref rid="R80" ref-type="bibr">80</xref></sup>) and then manually validated and corrected using ITKGray. Cortical reconstructions were generated from these segmentations using FreeSurfer.</p></sec><sec id="S22"><title>fMRI data analysis.</title><p id="P47">Data were processed and analyzed in MATLAB using mrVista (<ext-link xlink:href="http://github.com/vistalab" ext-link-type="uri">http://github.com/vistalab</ext-link>). All data were analyzed within the individual participant native brain space. Functional data were manually aligned to the T1-weighted volume. The manual alignment was then optimized using robust multiresolution alignment. For participants with more than one session, functional data were aligned to the anatomical scan taken closest to the date of the functional scan. Data were not spatially smoothed and were restricted to the cortical ribbon. Functional data were motion-corrected within and between scans using mrVista motion correction algorithms. Quality assurance was also performed to determine exclusions based on motion.</p></sec><sec id="S23"><title>mrVista to FreeSurfer conversion.</title><p id="P48">To visualize functional maps and draw regions of interest (ROIs), eccentricity and phase maps from Toonotopy and category selectivity maps from the category localizer experiment were converted from mrVista to FreeSurfer coordinates and projected onto the inflated cortical surface reconstruction for each individual participant in Freeview (FreeSurfer 7.2.0).</p></sec><sec id="S24"><title>Defining ROIs.</title><p id="P49">ROIs were drawn using Freeview (FreeSurfer 7.2.0) on the inflated cortical surface reconstruction of each participant&#x02019;s brain then projected back to mrVista for analysis. ROIs were defined by JKY and JOC.</p><sec id="S25"><title>V1-V3:</title><p id="P50">Using polar angle and eccentricity maps from the Toonotopy experiment thresholded at 20% variance explained, we defined early retinotopic visual areas (V1, V2, V3; <xref rid="F1" ref-type="fig">Fig. 1B</xref>) in both hemispheres in each individual. Boundaries between retinotopic areas were defined as the middle of polar angle reversals at the horizontal or vertical meridian representations, and each area included foveal to peripheral representations<sup><xref rid="R14" ref-type="bibr">14</xref>,<xref rid="R81" ref-type="bibr">81</xref></sup>. Dorsal (V1d, V2d, V3d) and ventral (V1v, V2v, V3v) components of each visual area were defined separately and combined in analysis to create representations of the entire visual field (V1, V2, V3).</p></sec><sec id="S26"><title>Category ROIs:</title><p id="P51">From the category localizer experiment, statistical contrast maps of each category domain versus all other category domains (i.e. faces &#x0003e; all other stimuli) were thresholded at a <inline-formula><mml:math id="M48" display="inline"><mml:mrow><mml:mi>t</mml:mi><mml:mtext>-</mml:mtext><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> at the voxel level, as in previous experiments<sup><xref rid="R22" ref-type="bibr">22</xref>,<xref rid="R27" ref-type="bibr">27</xref>,<xref rid="R43" ref-type="bibr">43</xref></sup>. Using these contrast maps, category-selective ROIs in the ventral, lateral, and dorsal streams were defined in each subject as clusters of voxels selective for a category located at a particular anatomical landmark (<xref rid="F2" ref-type="fig">Fig. 2B</xref>).</p><p id="P52">Face-selective voxels (contrast: adult and child faces &#x0003e; all other categories) were defined in the inferior occipital gyrus (IOG-faces), posterior fusiform gyrus (pFus-faces), mid fusiform gyrus (mFus-faces), and posterior superior temporal sulcus (pSTS-faces). Word-selective voxels (contrast: words &#x0003e; all other categories except numbers) were defined in the posterior occipital temporal sulcus (pOTS-words) and mid occipital temporal sulcus (mOTS-words). Because we could identify only in a minority of subjects (~20%) the right mOTS-words, mOTS-words was only defined in the left hemisphere. Bodypart-selective voxels (contrast: bodies and limbs &#x0003e; all other categories) were defined in the occipital temporal sulcus (OTS-bodies), lateral occipital sulcus (LOS-bodies), inferior occipital gyrus (IOG-bodies), and mid temporal gyrus (MTG-bodies). Place-selective voxels (contrast: corridors and houses &#x0003e; all other categories) were defined in the collateral sulcus (CoS-places), intraparietal sulcus (IPS-places), and mid occipital gyrus (MOG-places<sup><xref rid="R82" ref-type="bibr">82</xref>,<xref rid="R83" ref-type="bibr">83</xref></sup>).</p><p id="P53">Because we were only able to identify pSTS-face and MTG-bodies in a minority of adolescents (pSTS-faces: left 0/15; right 3/15; MTG-bodies: left: 5/15, right 4/15) we used group maximum probability maps (MPM) of these ROIs from adults projected to individual cortical surfaces for both adolescents and adults. The MPMs were thresholded at voxels found in 30% or more of the participants. After defining the category ROIs, only regions with ten or more voxels and more than 20% variance explained during the Toonotopy experiment were included in subsequent analyses (see <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 2</xref> for number of subjects per ROI).</p><p id="P54">Overall, we report data from 7 category-selective ROIs in the ventral stream (IOG-faces, pFus-faces, mFus-faces, pOTS-words, mOTS-words, OTS-bodies, CoS-places), 4 ROIs in the lateral stream (pSTS-faces, LOS-bodies, ITG-bodies, MTG-bodies), and 2 ROIs in the dorsal stream (MOG-places, IPS-places). As prior research combined the dorsal and lateral stream into a single dorsal stream<sup><xref rid="R25" ref-type="bibr">25</xref>,<xref rid="R32" ref-type="bibr">32</xref>,<xref rid="R36" ref-type="bibr">36</xref></sup> and because streams differ in the number and type of category-selective regions &#x02013; e.g., dorsal stream contains only one category (places) &#x02013; throughout this study we compare the ventral stream and a combined dorsal-lateral stream.</p></sec></sec><sec id="S27"><title>Estimating pRFs.</title><p id="P55">The time-course data were transformed from functional slices to the T1-weighted whole brain anatomy using trilinear interpolation. The pRF of each voxel was modeled using the compressive spatial summation (CSS) model<sup><xref rid="R19" ref-type="bibr">19</xref></sup> (<xref rid="F2" ref-type="fig">Fig. 2B</xref>) using VISTA lab software (<ext-link xlink:href="http://github.com/vistalab" ext-link-type="uri">http://github.com/vistalab</ext-link>). A pRF is modeled independently for every voxel by fitting a 2D Gaussian with a center (x,y) and a size determined by the standard deviation (&#x003c3;) of the Gaussian in degrees of visual angle. An exponent parameter (0&#x02264;n&#x02264; <italic toggle="yes">1</italic>) is additionally fit for each voxel to capture the compressive nonlinearity of pRFs. pRF size is thus defined as <inline-formula><mml:math id="M49" display="inline"><mml:mfrac><mml:mi>&#x003c3;</mml:mi><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:mfrac></mml:math></inline-formula>. x, y, and &#x003c3; are iteratively optimized to minimize the root mean squared error between the observed and predicted time-series. Eccentricity (<inline-formula><mml:math id="M50" display="inline"><mml:msqrt><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:math></inline-formula>) and phase (<inline-formula><mml:math id="M51" display="inline"><mml:mrow><mml:mi>a</mml:mi><mml:mspace width="0.5em"/><mml:mi>tan</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mi>y</mml:mi><mml:mi>x</mml:mi></mml:mfrac><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) of each voxel were derived from the center (x,y) coordinates and used to generate eccentricity and phase maps, respectively. As in prior studies<sup><xref rid="R22" ref-type="bibr">22</xref>,<xref rid="R24" ref-type="bibr">24</xref></sup> we report pRF parameters for voxels in which the variance explained by the pRF model is greater than 20%.</p><sec id="S28"><title>pRF size vs. eccentricity.</title><p id="P56">To evaluate the relationship between pRF size and eccentricity (<xref rid="F1" ref-type="fig">Fig. 1</xref>), all analyzed voxels in each participant&#x02019;s ROI were entered into a linear regression comparing pRF size to eccentricity. A line of best fit was derived for each participant, and the slope and intercept of the line was averaged across participants of each group (<xref rid="F1" ref-type="fig">Fig. 1D</xref>).</p></sec></sec><sec id="S29"><title>Visual field coverage (VFC).</title><p id="P57">VFC, the region of the visual field processed by the set of pRFs spanning an ROI, was calculated for each ROI and participant. RFs were represented by a binary circular mask centered on their centers (x,y) with size &#x003c3;; coverage was calculated by determining the density of pRFs at each point. To create group VFC maps (<xref rid="F4" ref-type="fig">Fig. 4A</xref>, <xref rid="F4" ref-type="fig">B</xref>), we averaged the individual VFC maps, whereby each visual field location illustrates the average pRF density, for that ROI per each group (adolescents, adults).</p><sec id="S30"><title>Estimating the full-width half-max (FWHM) of the VFC:</title><p id="P58">For each ROI and participant, we calculated the FWHM (<xref rid="F4" ref-type="fig">Fig. 4A</xref>, <xref rid="F4" ref-type="fig">B</xref> - black dashed line), which provides a standardized measure of the spatial extent of the VFC by estimating the diameter, in visual degrees, of the cross section of the VFC in which it reaches half of its maximum amplitude. The FWHM was determined by fitting a circular Gaussian centered at the center of mass, namely, the peak response (<xref rid="F4" ref-type="fig">Fig. 4A</xref>, <xref rid="F4" ref-type="fig">B</xref> -white asterisk), of the VFC, using the equation: <inline-formula><mml:math id="M52" display="inline"><mml:mrow><mml:mi>F</mml:mi><mml:mi>W</mml:mi><mml:mi>H</mml:mi><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x02217;</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt><mml:mo>&#x02217;</mml:mo><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M53" display="inline"><mml:mi>&#x003c3;</mml:mi></mml:math></inline-formula> is the standard deviation of the Gaussian.</p></sec><sec id="S31"><title>Proportion of FWHM measures:</title><p id="P59">We quantified the proportion of the FWHM in (i) each of the four quadrants of the visual field: upper contralateral (UC), lower contralateral (LC), lower ipsilateral (LI), and upper ipsilateral (UI) quadrants of the visual field and (ii) separately in the central 5&#x000b0;. Estimates were done separately for each ROI, hemisphere, and participant. This approach provides a detailed analysis of VFC across different field quadrants and eccentricity (<xref rid="SD1" ref-type="supplementary-material">Supplementary Table 10</xref>).</p></sec></sec><sec id="S32"><title>Category selectivity analysis.</title><p id="P60">To examine the development of category selectivity in our functional category ROIs, we quantified the mean t-value and the ROI size for each ROI in each individual and then compared across age groups.</p><sec id="S33"><title>Mean t-value:</title><p id="P61">To evaluate how the strength of category selectivity develops, we quantified the mean t-value, which indicates how strongly an ROI responds to one visual category versus all other categories. To control for ROI size differences across participants, groups, and ROIs and focus on selectivity, we generated a 10mm radius disk ROI centered on the original functional category ROI. The 10mm disk ROI approximated the average ROI size across all participants and regions. We then calculated the mean t-value (unthresholded) of the 10mm disk ROI for the category and ROI was drawn for (<xref rid="SD1" ref-type="supplementary-material">Supplementary Table 11</xref>).</p></sec><sec id="S34"><title>ROI size:</title><p id="P62">To examine the extent of cortex involved in processing each category and how this develops from adolescence into adulthood, we also measured ROI size. Using our original category ROIs, we quantified the number of category-selective voxels with t-value &#x0003e; 3 in each category ROI for every participant (<xref rid="SD1" ref-type="supplementary-material">Supplementary Table 12</xref>).</p></sec></sec></sec><sec id="S35"><title>Statistical Analysis</title><p id="P63">All statistical analyses were conducted using R version 4.2.2. All error bars in the main and supplementary figures represent the standard error of the mean across participants in a group. Except for analysis of EVC which included all subjects, the number of participants included in each statistical test, based on whether or not they had an ROI, was consistent across all analyses and can be found in <xref rid="SD1" ref-type="supplementary-material">Supplementary Table 3</xref>.</p><p id="P64">We use two sets of linear mixed effect models (LMMs, two-tailed) analyses to quantify significant differences across age group, stream, category, and hemisphere. In the first analysis, referred to as LMM, we compared age groups, streams, categories, and hemispheres for face, body, and place ROIs, which are distributed across multiple streams. As prior research combines the dorsal and lateral stream into a single dorsal stream and because streams differ in the number and type of category-selective regions &#x02013; e.g., dorsal stream contains only one category (places) &#x02013; we compare the ventral stream and a combined dorsal-lateral stream. In the second analysis, referred to as ventral LMM, we assessed differences across age group, category, and hemisphere for ROIs within the ventral stream in order to include word-selective ROIs, which were only found in the ventral stream.</p><sec id="S36"><title>LMMs.</title><p id="P65">were conducted using the lme4<sup><xref rid="R84" ref-type="bibr">84</xref></sup> and emmeans packages in R (<ext-link xlink:href="https://CRAN.R-project.org/package=lme4/" ext-link-type="uri">https://CRAN.R-project.org/package=lme4</ext-link>, <ext-link xlink:href="https://CRAN.R-project.org/package=emmeans" ext-link-type="uri">https://CRAN.R-project.org/package=emmeans</ext-link>). Each dependent variable, including pRF parameters (x-position, y-position, size, eccentricity), visual field coverage (FWHM), and category selectivity (mean t-value, ROI size), was analyzed using a series of LMMs to account for both fixed and random effects, repeated measures, as well as incomplete data (e.g., not all participants had all ROIs). The primary fixed effects included age group (adolescents, adults), stream (ventral, dorsal-lateral), category (faces, words, bodies, places), and hemisphere (left, right), with participant as a random effect to control for inter-subject variability. We additionally conducted analyses with age as a continuous variable for comparison (<xref rid="SD1" ref-type="supplementary-material">Supplementary Tables 3</xref>-<xref rid="SD1" ref-type="supplementary-material">11</xref>). Given the variability in the number and type of category-selective regions found across streams &#x02013; e.g., word-selective ROIs were exclusively found in the ventral stream, and place-selective ROIs dominate the dorsal stream &#x02013; analyses were structured in two complementary approaches:</p><p id="P66">First, we assess differences across streams, categories, groups, and hemispheres across face, body, and place ROIs which are found in multiple streams (1; referred to as LMM). For this analysis, we excluded word ROIs and combined the dorsal and lateral stream into a single dorsal- lateral stream, as prior studies did not separate these streams, and this enables comparisons between categories shared across the dorsal-lateral and ventral streams.</p><disp-formula id="FD1">
<label>(1)</label>
<mml:math id="M1" display="block"><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mrow><mml:mi mathvariant="bold">stream</mml:mi><mml:mrow><mml:mspace width="0.5em"/><mml:mo>&#x02217;</mml:mo><mml:mspace width="0.5em"/><mml:mi mathvariant="bold">category</mml:mi></mml:mrow></mml:mrow><mml:mo>&#x02217;</mml:mo><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</disp-formula><p id="P67">Second, we assess differences across age group, category, and hemisphere for ROIs within the ventral stream (2; referred to as ventral LMM). This analysis allows us to compare word-selective ROIs, which were only found in the ventral stream.</p><disp-formula id="FD2">
<label>(2)</label>
<mml:math id="M2" display="block"><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>F</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi mathvariant="bold">category</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math>
</disp-formula><p id="P68">Full results and statistics from all LMMs and ventral LMMs are included in <xref rid="SD1" ref-type="supplementary-material">Supplementary Tables 1</xref> - <xref rid="SD1" ref-type="supplementary-material">13</xref>. Additional analyses with age as a continuous variable, rather than by group, is also included in the <xref rid="SD1" ref-type="supplementary-material">Supplementary Tables 1</xref> - <xref rid="SD1" ref-type="supplementary-material">13</xref>.</p></sec><sec id="S37"><title>Post-hoc testing.</title><p id="P69">Post-hoc unpaired, independent t-tests (two-tailed) were performed using the t.test function in R to further examine significant main effects of age group and age group interactions from the LMMs. For each ROI, t-tests compared the mean value of the dependent variable between adolescents and adults. Post-hoc t-tests were conducted for pRF size, total FWHM, category selectivity (mean t-value), and ROI size, given main effects of or interactions with age group. Only significant post-hoc t-tests are reported in the main text.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM1"><title>Supplementary Material</title><supplementary-material id="SD1" position="float" content-type="local-data"><label>Supplement 1</label><media xlink:href="media-1.pdf" id="d67e4345" position="anchor"/></supplementary-material></sec></body><back><ack id="S40"><title>Acknowledgements</title><p id="P72">This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. (DGE-2146755) awarded to J.K.Y., NIH grants (grant numbers RO1EY022318, RO1EY023915 to K.G.S.), a William R. and Sara Hart Kimball Stanford Graduate Fellowship awarded to D.F., and a Symbolic Systems Summer Internship Program internship (J.C.).</p></ack><fn-group><fn id="FN1"><p id="P73">Competing Interests</p><p id="P74">The authors declare no competing interests.</p></fn></fn-group><sec sec-type="data-availability" id="S38"><title>Data Availability</title><p id="P70">Raw data available upon request to J.K.Y. Processed data available on Github: <ext-link xlink:href="https://github.com/VPNL/toonCat" ext-link-type="uri">https://github.com/VPNL/toonCat</ext-link></p></sec><sec sec-type="data-availability" id="S39"><title>Code Availability</title><p id="P71">fMRI data were analyzed using the open source mrVista software package (<ext-link xlink:href="https://github.com/vistalab/vistasoft" ext-link-type="uri">https://github.com/vistalab/vistasoft</ext-link>). Custom code for processing the pRF experiment and functional localizer, reproducing figures, and statistics can be found at <ext-link xlink:href="https://github.com/VPNL/toonCat" ext-link-type="uri">https://github.com/VPNL/toonCat</ext-link>.</p></sec><ref-list><title>References</title><ref id="R1"><label>1.</label><mixed-citation publication-type="journal"><name><surname>Mishkin</surname><given-names>M.</given-names></name> &#x00026; <name><surname>Ungerleider</surname><given-names>L. G.</given-names></name>
<source>Object vision and spatial vision: two cortical pathways</source>. (<year>1983</year>).</mixed-citation></ref><ref id="R2"><label>2.</label><mixed-citation publication-type="journal"><name><surname>Pitcher</surname><given-names>D.</given-names></name> &#x00026; <name><surname>Ungerleider</surname><given-names>L. G.</given-names></name>
<article-title>Evidence for a Third Visual Pathway Specialized for Social Perception</article-title>. <source>Trends Cogn. Sci.</source>
<volume>25</volume>, <fpage>100</fpage>&#x02013;<lpage>110</lpage> (<year>2021</year>).<pub-id pub-id-type="pmid">33334693</pub-id>
</mixed-citation></ref><ref id="R3"><label>3.</label><mixed-citation publication-type="journal"><name><surname>Weiner</surname><given-names>K. S.</given-names></name> &#x00026; <name><surname>Grill-Spector</surname><given-names>K.</given-names></name>
<article-title>Neural representations of faces and limbs neighbor in human high-level visual cortex: evidence for a new organization principle</article-title>. <source>Psychol. Res.</source>
<volume>77</volume>, <fpage>74</fpage>&#x02013;<lpage>97</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">22139022</pub-id>
</mixed-citation></ref><ref id="R4"><label>4.</label><mixed-citation publication-type="journal"><name><surname>Wurm</surname><given-names>M. F.</given-names></name> &#x00026; <name><surname>Caramazza</surname><given-names>A.</given-names></name>
<source>Action and object representation in the ventral &#x02018;what&#x02019; stream</source>. (<year>2021</year>).</mixed-citation></ref><ref id="R5"><label>5.</label><mixed-citation publication-type="journal"><name><surname>Goodale</surname><given-names>M.</given-names></name> &#x00026; <name><surname>Milner</surname><given-names>A. D.</given-names></name>
<article-title>Separate visual pathways for perception and action</article-title>. <source>Trends Neurosci</source>. <volume>15</volume>, (<year>1992</year>).</mixed-citation></ref><ref id="R6"><label>6.</label><mixed-citation publication-type="journal"><name><surname>Isik</surname><given-names>L.</given-names></name>, <name><surname>Koldewyn</surname><given-names>K.</given-names></name>, <name><surname>Beeler</surname><given-names>D.</given-names></name> &#x00026; <name><surname>Kanwisher</surname><given-names>N.</given-names></name>
<article-title>Perceiving social interactions in the posterior superior temporal sulcus</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>114</volume>, (<year>2017</year>).</mixed-citation></ref><ref id="R7"><label>7.</label><mixed-citation publication-type="journal"><name><surname>Hubel</surname><given-names>D. H.</given-names></name> &#x00026; <name><surname>Wiesel</surname><given-names>T. N.</given-names></name>
<source>RECEPTIVE FIELDS OF SINGLE NEURONES IN THE CAT&#x02019;S STRIATE CORTEX</source>. (<year>1959</year>).</mixed-citation></ref><ref id="R8"><label>8.</label><mixed-citation publication-type="journal"><name><surname>Dumoulin</surname><given-names>S. O.</given-names></name> &#x00026; <name><surname>Wandell</surname><given-names>B. A.</given-names></name>
<article-title>Population receptive field estimates in human visual cortex</article-title>. <source>NeuroImage</source>
<volume>39</volume>, <fpage>647</fpage>&#x02013;<lpage>660</lpage> (<year>2008</year>).<pub-id pub-id-type="pmid">17977024</pub-id>
</mixed-citation></ref><ref id="R9"><label>9.</label><mixed-citation publication-type="journal"><name><surname>Brem</surname><given-names>S</given-names></name>. <etal/>
<article-title>Evidence for developmental changes in the visual word processing network beyond adolescence</article-title>. <source>NeuroImage</source>
<volume>29</volume>, <fpage>822</fpage>&#x02013;<lpage>837</lpage> (<year>2006</year>).<pub-id pub-id-type="pmid">16257546</pub-id>
</mixed-citation></ref><ref id="R10"><label>10.</label><mixed-citation publication-type="journal"><name><surname>Golarai</surname><given-names>G.</given-names></name>, <name><surname>Liberman</surname><given-names>A.</given-names></name>, <name><surname>Yoon</surname><given-names>J. M. D.</given-names></name> &#x00026; <name><surname>Grill-Spector</surname><given-names>K.</given-names></name>
<article-title>Differential development of the ventral visual cortex extends through adolescence</article-title>. <source>Front. Hum. Neurosci.</source> (<year>2009</year>) doi:<pub-id pub-id-type="doi">10.3389/neuro.09.080.2009</pub-id>.</mixed-citation></ref><ref id="R11"><label>11.</label><mixed-citation publication-type="journal"><name><surname>Schlaggar</surname><given-names>B. L.</given-names></name> &#x00026; <name><surname>McCandliss</surname><given-names>B. D.</given-names></name>
<article-title>Development of Neural Systems for Reading</article-title>. <source>Annu. Rev. Neurosci.</source>
<volume>30</volume>, <fpage>475</fpage>&#x02013;<lpage>503</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17600524</pub-id>
</mixed-citation></ref><ref id="R12"><label>12.</label><mixed-citation publication-type="journal"><name><surname>Kay</surname><given-names>K. N.</given-names></name>, <name><surname>Weiner</surname><given-names>K. S.</given-names></name> &#x00026; <name><surname>Grill-Spector</surname><given-names>K.</given-names></name>
<article-title>Attention Reduces Spatial Uncertainty in Human Ventral Temporal Cortex</article-title>. <source>Curr. Biol.</source>
<volume>25</volume>, <fpage>595</fpage>&#x02013;<lpage>600</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25702580</pub-id>
</mixed-citation></ref><ref id="R13"><label>13.</label><mixed-citation publication-type="journal"><name><surname>Brewer</surname><given-names>A. A.</given-names></name>, <name><surname>Liu</surname><given-names>J.</given-names></name>, <name><surname>Wade</surname><given-names>A. R.</given-names></name> &#x00026; <name><surname>Wandell</surname><given-names>B. A.</given-names></name>
<article-title>Visual field maps and stimulus selectivity in human ventral occipital cortex</article-title>. <source>Nat. Neurosci.</source>
<volume>8</volume>, <fpage>1102</fpage>&#x02013;<lpage>1109</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">16025108</pub-id>
</mixed-citation></ref><ref id="R14"><label>14.</label><mixed-citation publication-type="journal"><name><surname>Wandell</surname><given-names>B. A.</given-names></name> &#x00026; <name><surname>Winawer</surname><given-names>J.</given-names></name>
<article-title>Computational neuroimaging and population receptive fields</article-title>. <source>Trends Cogn. Sci.</source>
<volume>19</volume>, <fpage>349</fpage>&#x02013;<lpage>357</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25850730</pub-id>
</mixed-citation></ref><ref id="R15"><label>15.</label><mixed-citation publication-type="journal"><name><surname>Amano</surname><given-names>K.</given-names></name>, <name><surname>Wandell</surname><given-names>B. A.</given-names></name> &#x00026; <name><surname>Dumoulin</surname><given-names>S. O.</given-names></name>
<article-title>Visual Field Maps, Population Receptive Field Sizes, and Visual Field Coverage in the Human MT+ Complex</article-title>. <source>J. Neurophysiol.</source>
<volume>102</volume>, <fpage>2704</fpage>&#x02013;<lpage>2718</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19587323</pub-id>
</mixed-citation></ref><ref id="R16"><label>16.</label><mixed-citation publication-type="journal"><name><surname>DeYoe</surname><given-names>E. A.</given-names></name>
<etal/>
<article-title>Mapping striate and extrastriate visual areas in human cerebral cortex</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>93</volume>, <fpage>2382</fpage>&#x02013;<lpage>2386</lpage> (<year>1996</year>).<pub-id pub-id-type="pmid">8637882</pub-id>
</mixed-citation></ref><ref id="R17"><label>17.</label><mixed-citation publication-type="journal"><name><surname>Engel</surname><given-names>S.</given-names></name>, <name><surname>Glover</surname><given-names>G. H.</given-names></name> &#x00026; <name><surname>Wandell</surname><given-names>B. A.</given-names></name>
<article-title>Retinotopic organization in human visual cortex and the spatial precision of functional MRI</article-title>. <source>Cereb. Cortex</source>
<volume>7</volume>, <fpage>181</fpage>&#x02013;<lpage>192</lpage> (<year>1997</year>).<pub-id pub-id-type="pmid">9087826</pub-id>
</mixed-citation></ref><ref id="R18"><label>18.</label><mixed-citation publication-type="journal"><name><surname>Engel</surname><given-names>S. A.</given-names></name>
<etal/>
<article-title>fMRI of human visual cortex</article-title>. <source>Nature</source>
<volume>369</volume>, <fpage>525</fpage>&#x02013;<lpage>525</lpage> (<year>1994</year>).<pub-id pub-id-type="pmid">8031403</pub-id>
</mixed-citation></ref><ref id="R19"><label>19.</label><mixed-citation publication-type="journal"><name><surname>Kay</surname><given-names>K. N.</given-names></name>, <name><surname>Winawer</surname><given-names>J.</given-names></name>, <name><surname>Mezer</surname><given-names>A.</given-names></name> &#x00026; <name><surname>Wandell</surname><given-names>B. A.</given-names></name>
<article-title>Compressive spatial summation in human visual cortex</article-title>. <source>J. Neurophysiol.</source>
<volume>110</volume>, <fpage>481</fpage>&#x02013;<lpage>494</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23615546</pub-id>
</mixed-citation></ref><ref id="R20"><label>20.</label><mixed-citation publication-type="journal"><name><surname>Sereno</surname><given-names>M. I.</given-names></name>
<etal/>
<article-title>Borders of Multiple Visual Areas in Humans Revealed by Functional Magnetic Resonance Imaging</article-title>. <source>Science</source>
<volume>268</volume>, <fpage>889</fpage>&#x02013;<lpage>893</lpage> (<year>1995</year>).<pub-id pub-id-type="pmid">7754376</pub-id>
</mixed-citation></ref><ref id="R21"><label>21.</label><mixed-citation publication-type="journal"><name><surname>Benson</surname><given-names>N. C.</given-names></name> &#x00026; <name><surname>Winawer</surname><given-names>J.</given-names></name>
<article-title>Bayesian analysis of retinotopic maps</article-title>. <source>eLife</source>
<volume>7</volume>, <fpage>e40224</fpage> (<year>2018</year>).<pub-id pub-id-type="pmid">30520736</pub-id>
</mixed-citation></ref><ref id="R22"><label>22.</label><mixed-citation publication-type="journal"><name><surname>Finzi</surname><given-names>D</given-names></name>. <etal/>
<article-title>Differential spatial computations in ventral and lateral face-selective regions are scaffolded by structural connections</article-title>. <source>Nat. Commun.</source>
<volume>12</volume>, <fpage>2278</fpage> (<year>2021</year>).<pub-id pub-id-type="pmid">33859195</pub-id>
</mixed-citation></ref><ref id="R23"><label>23.</label><mixed-citation publication-type="journal"><name><surname>Le</surname><given-names>R.</given-names></name>, <name><surname>Witthoft</surname><given-names>N.</given-names></name>, <name><surname>Ben-Shachar</surname><given-names>M.</given-names></name> &#x00026; <name><surname>Wandell</surname><given-names>B.</given-names></name>
<article-title>The field of view available to the ventral occipito-temporal reading circuitry</article-title>. <source>J. Vis.</source>
<volume>17</volume>, <fpage>6</fpage> (<year>2017</year>).</mixed-citation></ref><ref id="R24"><label>24.</label><mixed-citation publication-type="journal"><name><surname>Poltoratski</surname><given-names>S.</given-names></name>, <name><surname>Kay</surname><given-names>K.</given-names></name>, <name><surname>Finzi</surname><given-names>D.</given-names></name> &#x00026; <name><surname>Grill-Spector</surname><given-names>K.</given-names></name>
<article-title>Holistic face recognition is an emergent phenomenon of spatial processing in face-selective regions</article-title>. <source>Nat. Commun.</source>
<volume>12</volume>, <fpage>4745</fpage> (<year>2021</year>).<pub-id pub-id-type="pmid">34362883</pub-id>
</mixed-citation></ref><ref id="R25"><label>25.</label><mixed-citation publication-type="journal"><name><surname>Silson</surname><given-names>E. H.</given-names></name>
<etal/>
<article-title>Specialized and independent processing of orientation and shape in visual field maps LO1 and LO2</article-title>. <source>Nat. Neurosci.</source>
<volume>16</volume>, <fpage>267</fpage>&#x02013;<lpage>269</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23377127</pub-id>
</mixed-citation></ref><ref id="R26"><label>26.</label><mixed-citation publication-type="journal"><name><surname>Steel</surname><given-names>A.</given-names></name>, <name><surname>Silson</surname><given-names>E. H.</given-names></name>, <name><surname>Garcia</surname><given-names>B. D.</given-names></name> &#x00026; <name><surname>Robertson</surname><given-names>C. E.</given-names></name>
<article-title>A retinotopic code structures the interaction between perception and memory systems</article-title>. <source>Nat. Neurosci.</source>
<volume>27</volume>, <fpage>339</fpage>&#x02013;<lpage>347</lpage> (<year>2024</year>).<pub-id pub-id-type="pmid">38168931</pub-id>
</mixed-citation></ref><ref id="R27"><label>27.</label><mixed-citation publication-type="journal"><name><surname>Gomez</surname><given-names>J.</given-names></name>, <name><surname>Natu</surname><given-names>V.</given-names></name>, <name><surname>Jeska</surname><given-names>B.</given-names></name>, <name><surname>Barnett</surname><given-names>M.</given-names></name> &#x00026; <name><surname>Grill-Spector</surname><given-names>K.</given-names></name>
<article-title>Development differentially sculpts receptive fields across early and high-level human visual cortex</article-title>. <source>Nat. Commun.</source>
<volume>9</volume>, <fpage>788</fpage> (<year>2018</year>).<pub-id pub-id-type="pmid">29476135</pub-id>
</mixed-citation></ref><ref id="R28"><label>28.</label><mixed-citation publication-type="journal"><name><surname>Levy</surname><given-names>I.</given-names></name>, <name><surname>Hasson</surname><given-names>U.</given-names></name>, <name><surname>Avidan</surname><given-names>G.</given-names></name>, <name><surname>Hendler</surname><given-names>T.</given-names></name> &#x00026; <name><surname>Malach</surname><given-names>R.</given-names></name>
<article-title>Center&#x02013;periphery organization of human object areas</article-title>. <source>Nat. Neurosci.</source>
<volume>4</volume>, <fpage>533</fpage>&#x02013;<lpage>539</lpage> (<year>2001</year>).<pub-id pub-id-type="pmid">11319563</pub-id>
</mixed-citation></ref><ref id="R29"><label>29.</label><mixed-citation publication-type="journal"><name><surname>Malach</surname><given-names>R.</given-names></name>, <name><surname>Levy</surname><given-names>I.</given-names></name> &#x00026; <name><surname>Hasson</surname><given-names>U.</given-names></name>
<article-title>The topography of high-order human object areas</article-title>. <source>Trends Cogn. Sci.</source>
<volume>6</volume>, <fpage>176</fpage>&#x02013;<lpage>184</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">11912041</pub-id>
</mixed-citation></ref><ref id="R30"><label>30.</label><mixed-citation publication-type="journal"><name><surname>Daniel Hertz</surname><given-names>E.</given-names></name>, <name><surname>Yao</surname><given-names>J. K.</given-names></name>, <name><surname>Gregorek</surname><given-names>S.</given-names></name>, <name><surname>Hoyos</surname><given-names>P. M.</given-names></name> &#x00026; <name><surname>Gomez</surname><given-names>J.</given-names></name>
<source>Spatial Processing of Limbs Reveals the Center-Periphery Bias in High Level Visual Cortex Follows a Nonlinear Topography</source>. <comment><ext-link xlink:href="http://biorxiv.org/lookup/doi/10.1101/2023.10.15.561711" ext-link-type="uri">http://biorxiv.org/lookup/doi/10.1101/2023.10.15.561711</ext-link></comment> (<year>2023</year>) doi:<pub-id pub-id-type="doi">10.1101/2023.10.15.561711</pub-id>.</mixed-citation></ref><ref id="R31"><label>31.</label><mixed-citation publication-type="journal"><name><surname>Arcaro</surname><given-names>M. J.</given-names></name>, <name><surname>McMains</surname><given-names>S. A.</given-names></name>, <name><surname>Singer</surname><given-names>B. D.</given-names></name> &#x00026; <name><surname>Kastner</surname><given-names>S.</given-names></name>
<article-title>Retinotopic Organization of Human Ventral Visual Cortex</article-title>. <source>J. Neurosci.</source>
<volume>29</volume>, <fpage>10638</fpage>&#x02013;<lpage>10652</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19710316</pub-id>
</mixed-citation></ref><ref id="R32"><label>32.</label><mixed-citation publication-type="journal"><name><surname>Hasson</surname><given-names>U.</given-names></name>, <name><surname>Levy</surname><given-names>I.</given-names></name>, <name><surname>Behrmann</surname><given-names>M.</given-names></name>, <name><surname>Hendler</surname><given-names>T.</given-names></name> &#x00026; <name><surname>Malach</surname><given-names>R.</given-names></name>
<source>Eccentricity Bias as an Organizing Principle for Human High-Order Object Areas</source>. <volume>12</volume> (<year>2002</year>).</mixed-citation></ref><ref id="R33"><label>33.</label><mixed-citation publication-type="journal"><name><surname>Gosselin</surname><given-names>F.</given-names></name> &#x00026; <name><surname>Schyns</surname><given-names>P. G.</given-names></name>
<article-title>Bubbles: a technique to reveal the use of information in recognition tasks</article-title>. <source>Vision Res</source>. <volume>41</volume>, <fpage>2261</fpage>&#x02013;<lpage>2271</lpage> (<year>2001</year>).<pub-id pub-id-type="pmid">11448718</pub-id>
</mixed-citation></ref><ref id="R34"><label>34.</label><mixed-citation publication-type="journal"><name><surname>Schyns</surname><given-names>P. G.</given-names></name>, <name><surname>Bonnar</surname><given-names>L.</given-names></name> &#x00026; <name><surname>Gosselin</surname><given-names>F.</given-names></name>
<article-title>Show Me the Features! Understanding Recognition From the Use of Visual Information</article-title>. <source>Psychol. Sci.</source>
<volume>13</volume>, <fpage>402</fpage>&#x02013;<lpage>409</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">12219805</pub-id>
</mixed-citation></ref><ref id="R35"><label>35.</label><mixed-citation publication-type="journal"><name><surname>Silson</surname><given-names>E. H.</given-names></name>, <name><surname>Chan</surname><given-names>A. W.-Y.</given-names></name>, <name><surname>Reynolds</surname><given-names>R. C.</given-names></name>, <name><surname>Kravitz</surname><given-names>D. J.</given-names></name> &#x00026; <name><surname>Baker</surname><given-names>C. I.</given-names></name>
<article-title>A Retinotopic Basis for the Division of High-Level Scene Processing between Lateral and Ventral Human Occipitotemporal Cortex</article-title>. <source>J. Neurosci.</source>
<volume>35</volume>, <fpage>11921</fpage>&#x02013;<lpage>11935</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">26311774</pub-id>
</mixed-citation></ref><ref id="R36"><label>36.</label><mixed-citation publication-type="journal"><name><surname>Silson</surname><given-names>E. H.</given-names></name>, <name><surname>Reynolds</surname><given-names>R. C.</given-names></name>, <name><surname>Kravitz</surname><given-names>D. J.</given-names></name> &#x00026; <name><surname>Baker</surname><given-names>C. I.</given-names></name>
<article-title>Differential Sampling of Visual Space in Ventral and Dorsal Early Visual Cortex</article-title>. <source>J. Neurosci.</source>
<volume>38</volume>, <fpage>2294</fpage>&#x02013;<lpage>2303</lpage> (<year>2018</year>).<pub-id pub-id-type="pmid">29382711</pub-id>
</mixed-citation></ref><ref id="R37"><label>37.</label><mixed-citation publication-type="journal"><name><surname>Nordt</surname><given-names>M</given-names></name>. <etal/>
<article-title>Learning to Read Increases the Informativeness of Distributed Ventral Temporal Responses</article-title>. <source>Cereb. Cortex N. Y. NY</source>
<volume>29</volume>, <fpage>3124</fpage>&#x02013;<lpage>3139</lpage> (<year>2019</year>).</mixed-citation></ref><ref id="R38"><label>38.</label><mixed-citation publication-type="journal"><name><surname>Peelen</surname><given-names>M. V.</given-names></name>, <name><surname>Glaser</surname><given-names>B.</given-names></name>, <name><surname>Vuilleumier</surname><given-names>P.</given-names></name> &#x00026; <name><surname>Eliez</surname><given-names>S.</given-names></name>
<article-title>Differential development of selectivity for faces and bodies in the fusiform gyrus: Development of selectivity for faces and bodies</article-title>. <source>Dev. Sci.</source>
<volume>12</volume>, <fpage>F16</fpage>&#x02013;<lpage>F25</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19840035</pub-id>
</mixed-citation></ref><ref id="R39"><label>39.</label><mixed-citation publication-type="journal"><name><surname>Scherf</surname><given-names>K. S.</given-names></name>, <name><surname>Behrmann</surname><given-names>M.</given-names></name>, <name><surname>Humphreys</surname><given-names>K.</given-names></name> &#x00026; <name><surname>Luna</surname><given-names>B.</given-names></name>
<article-title>Visual category-selectivity for faces, places and objects emerges along different developmental trajectories</article-title>. <source>Dev. Sci.</source>
<volume>10</volume>, <fpage>F15</fpage>&#x02013;<lpage>F30</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17552930</pub-id>
</mixed-citation></ref><ref id="R40"><label>40.</label><mixed-citation publication-type="journal"><name><surname>White</surname><given-names>A. L.</given-names></name>, <name><surname>Boynton</surname><given-names>G. M.</given-names></name> &#x00026; <name><surname>Yeatman</surname><given-names>J. D.</given-names></name>
<article-title>The link between reading ability and visual spatial attention across development</article-title>. <source>Cortex</source>
<volume>121</volume>, <fpage>44</fpage>&#x02013;<lpage>59</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">31542467</pub-id>
</mixed-citation></ref><ref id="R41"><label>41.</label><mixed-citation publication-type="journal"><name><surname>Conner</surname><given-names>I. P.</given-names></name>, <name><surname>Sharma</surname><given-names>S.</given-names></name>, <name><surname>Lemieux</surname><given-names>S. K.</given-names></name> &#x00026; <name><surname>Mendola</surname><given-names>J. D.</given-names></name>
<article-title>Retinotopic organization in children measured with fMRI</article-title>. <source>J. Vis.</source>
<volume>4</volume>, <fpage>10</fpage>&#x02013;<lpage>10</lpage> (<year>2004</year>).</mixed-citation></ref><ref id="R42"><label>42.</label><mixed-citation publication-type="journal"><name><surname>Dekker</surname><given-names>T. M.</given-names></name>, <name><surname>Schwarzkopf</surname><given-names>D. S.</given-names></name>, <name><surname>De Haas</surname><given-names>B.</given-names></name>, <name><surname>Nardini</surname><given-names>M.</given-names></name> &#x00026; <name><surname>Sereno</surname><given-names>M. I.</given-names></name>
<article-title>Population receptive field tuning properties of visual cortex during childhood</article-title>. <source>Dev. Cogn. Neurosci.</source>
<volume>37</volume>, <fpage>100614</fpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30777677</pub-id>
</mixed-citation></ref><ref id="R43"><label>43.</label><mixed-citation publication-type="journal"><name><surname>Stigliani</surname><given-names>A.</given-names></name>, <name><surname>Weiner</surname><given-names>K. S.</given-names></name> &#x00026; <name><surname>Grill-Spector</surname><given-names>K.</given-names></name>
<article-title>Temporal Processing Capacity in High-Level Visual Cortex Is Domain Specific</article-title>. <source>J. Neurosci.</source>
<volume>35</volume>, <fpage>12412</fpage>&#x02013;<lpage>12424</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">26354910</pub-id>
</mixed-citation></ref><ref id="R44"><label>44.</label><mixed-citation publication-type="journal"><name><surname>Golarai</surname><given-names>G.</given-names></name>, <name><surname>Liberman</surname><given-names>A.</given-names></name> &#x00026; <name><surname>Grill-Spector</surname><given-names>K.</given-names></name>
<article-title>Experience Shapes the Development of Neural Substrates of Face Processing in Human Ventral Temporal Cortex</article-title>. <source>Cereb. Cortex N. Y. NY</source>
<volume>27</volume>, <fpage>bhv314</fpage> (<year>2015</year>).</mixed-citation></ref><ref id="R45"><label>45.</label><mixed-citation publication-type="journal"><name><surname>Kubota</surname><given-names>E. C.</given-names></name>, <name><surname>Joo</surname><given-names>S. J.</given-names></name>, <name><surname>Huber</surname><given-names>E.</given-names></name> &#x00026; <name><surname>Yeatman</surname><given-names>J. D.</given-names></name>
<article-title>Word selectivity in high-level visual cortex and reading skill</article-title>. <source>Dev. Cogn. Neurosci.</source>
<volume>36</volume>, <fpage>100593</fpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30318344</pub-id>
</mixed-citation></ref><ref id="R46"><label>46.</label><mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>J.</given-names></name>, <name><surname>Soucy</surname><given-names>E.</given-names></name>, <name><surname>Segawa</surname><given-names>J.</given-names></name>, <name><surname>Mair</surname><given-names>R.</given-names></name> &#x00026; <name><surname>Konkle</surname><given-names>T.</given-names></name>
<source>Immersive scene representation in human visual cortex with ultra-wide angle neuroimaging</source>. (<year>2024</year>).</mixed-citation></ref><ref id="R47"><label>47.</label><mixed-citation publication-type="journal"><name><surname>Bonner</surname><given-names>M. F.</given-names></name> &#x00026; <name><surname>Epstein</surname><given-names>R. A.</given-names></name>
<article-title>Coding of navigational affordances in the human visual system</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>114</volume>, <fpage>4793</fpage>&#x02013;<lpage>4798</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28416669</pub-id>
</mixed-citation></ref><ref id="R48"><label>48.</label><mixed-citation publication-type="journal"><name><surname>Gomez</surname><given-names>J</given-names></name>. <etal/>
<article-title>Development of population receptive fields in the lateral visual stream improves spatial coding amid stable structural-functional coupling</article-title>. <source>NeuroImage</source>
<volume>188</volume>, <fpage>59</fpage>&#x02013;<lpage>69</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30508682</pub-id>
</mixed-citation></ref><ref id="R49"><label>49.</label><mixed-citation publication-type="journal"><name><surname>Kaiser</surname><given-names>D.</given-names></name> &#x00026; <name><surname>Cichy</surname><given-names>R. M.</given-names></name>
<article-title>Typical visual-field locations facilitate access to awareness for everyday objects</article-title>. <source>Cognition</source>
<volume>180</volume>, <fpage>118</fpage>&#x02013;<lpage>122</lpage> (<year>2018</year>).<pub-id pub-id-type="pmid">30029067</pub-id>
</mixed-citation></ref><ref id="R50"><label>50.</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>J.</given-names></name>, <name><surname>Liu</surname><given-names>J.</given-names></name> &#x00026; <name><surname>Xu</surname><given-names>Y.</given-names></name>
<article-title>Neural Decoding Reveals Impaired Face Configural Processing in the Right Fusiform Face Area of Individuals with Developmental Prosopagnosia</article-title>. <source>J. Neurosci.</source>
<volume>35</volume>, <fpage>1539</fpage>&#x02013;<lpage>1548</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25632131</pub-id>
</mixed-citation></ref><ref id="R51"><label>51.</label><mixed-citation publication-type="journal"><name><surname>Broda</surname><given-names>M. D.</given-names></name> &#x00026; <name><surname>De Haas</surname><given-names>B.</given-names></name>
<article-title>Individual differences in human gaze behavior generalize from faces to objects</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>121</volume>, <fpage>e2322149121</fpage> (<year>2024</year>).<pub-id pub-id-type="pmid">38470925</pub-id>
</mixed-citation></ref><ref id="R52"><label>52.</label><mixed-citation publication-type="journal"><name><surname>Nazir</surname><given-names>T. A.</given-names></name>
<article-title>Effects of lateral masking and spatial precueing on gap-resolution in central and peripheral vision</article-title>. <source>Vision Res</source>. <volume>32</volume>, <fpage>771</fpage>&#x02013;<lpage>777</lpage> (<year>1992</year>).<pub-id pub-id-type="pmid">1413560</pub-id>
</mixed-citation></ref><ref id="R53"><label>53.</label><mixed-citation publication-type="journal"><name><surname>Van Belle</surname><given-names>G</given-names></name>. <article-title>Fixation patterns during recognition of personally familiar and unfamiliar faces</article-title>. <source>Front. Psychol.</source> (<year>2010</year>) doi:<pub-id pub-id-type="doi">10.3389/fpsyg.2010.00020</pub-id>.</mixed-citation></ref><ref id="R54"><label>54.</label><mixed-citation publication-type="book"><name><surname>Yarbus</surname><given-names>A. L.</given-names></name>
<source>Eye Movements and Vision</source>. (<publisher-name>Springer US</publisher-name>, <publisher-loc>Boston, MA</publisher-loc>, <year>1967</year>). doi:<pub-id pub-id-type="doi">10.1007/978-1-4899-5379-7</pub-id>.</mixed-citation></ref><ref id="R55"><label>55.</label><mixed-citation publication-type="journal"><name><surname>Behrmann</surname><given-names>M.</given-names></name> &#x00026; <name><surname>Plaut</surname><given-names>D. C.</given-names></name>
<article-title>A vision of graded hemispheric specialization: Graded hemispheric specialization</article-title>. <source>Ann. N. Y. Acad. Sci.</source>
<volume>1359</volume>, <fpage>30</fpage>&#x02013;<lpage>46</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">26199998</pub-id>
</mixed-citation></ref><ref id="R56"><label>56.</label><mixed-citation publication-type="journal"><name><surname>Dehaene</surname><given-names>S.</given-names></name>, <name><surname>Cohen</surname><given-names>L.</given-names></name>, <name><surname>Morais</surname><given-names>J.</given-names></name> &#x00026; <name><surname>Kolinsky</surname><given-names>R.</given-names></name>
<article-title>Illiterate to literate: behavioural and cerebral changes induced by reading acquisition</article-title>. <source>Nat. Rev. Neurosci.</source>
<volume>16</volume>, <fpage>234</fpage>&#x02013;<lpage>244</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25783611</pub-id>
</mixed-citation></ref><ref id="R57"><label>57.</label><mixed-citation publication-type="journal"><name><surname>Arcaro</surname><given-names>M. J.</given-names></name>, <name><surname>Schade</surname><given-names>P. F.</given-names></name> &#x00026; <name><surname>Livingstone</surname><given-names>M. S.</given-names></name>
<article-title>Universal Mechanisms and the Development of the Face Network: What You See Is What You Get</article-title>. <source>Annu. Rev. Vis. Sci.</source>
<volume>5</volume>, <fpage>341</fpage>&#x02013;<lpage>372</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">31226011</pub-id>
</mixed-citation></ref><ref id="R58"><label>58.</label><mixed-citation publication-type="journal"><name><surname>Bonner</surname><given-names>M. F.</given-names></name> &#x00026; <name><surname>Epstein</surname><given-names>R. A.</given-names></name>
<article-title>Computational mechanisms underlying cortical responses to the affordance properties of visual scenes</article-title>. <source>PLOS Comput. Biol.</source>
<volume>14</volume>, <fpage>e1006111</fpage> (<year>2018</year>).<pub-id pub-id-type="pmid">29684011</pub-id>
</mixed-citation></ref><ref id="R59"><label>59.</label><mixed-citation publication-type="journal"><name><surname>Borovska</surname><given-names>P.</given-names></name> &#x00026; <name><surname>De Haas</surname><given-names>B.</given-names></name>
<article-title>Individual gaze shapes diverging neural representations</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>121</volume>, <fpage>e2405602121</fpage> (<year>2024</year>).<pub-id pub-id-type="pmid">39213176</pub-id>
</mixed-citation></ref><ref id="R60"><label>60.</label><mixed-citation publication-type="journal"><name><surname>Dilks</surname><given-names>D. D.</given-names></name>, <name><surname>Kamps</surname><given-names>F. S.</given-names></name> &#x00026; <name><surname>Persichetti</surname><given-names>A. S.</given-names></name>
<article-title>Three cortical scene systems and their development</article-title>. <source>Trends Cogn. Sci.</source>
<volume>26</volume>, <fpage>117</fpage>&#x02013;<lpage>127</lpage> (<year>2022</year>).<pub-id pub-id-type="pmid">34857468</pub-id>
</mixed-citation></ref><ref id="R61"><label>61.</label><mixed-citation publication-type="journal"><name><surname>Epstein</surname><given-names>R. A.</given-names></name>, <name><surname>Patai</surname><given-names>E. Z.</given-names></name>, <name><surname>Julian</surname><given-names>J. B.</given-names></name> &#x00026; <name><surname>Spiers</surname><given-names>H. J.</given-names></name>
<article-title>The cognitive map in humans: spatial navigation and beyond</article-title>. <source>Nat. Neurosci.</source>
<volume>20</volume>, <fpage>1504</fpage>&#x02013;<lpage>1513</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">29073650</pub-id>
</mixed-citation></ref><ref id="R62"><label>62.</label><mixed-citation publication-type="journal"><name><surname>Groen</surname><given-names>I. I. A.</given-names></name>, <name><surname>Dekker</surname><given-names>T. M.</given-names></name>, <name><surname>Knapen</surname><given-names>T.</given-names></name> &#x00026; <name><surname>Silson</surname><given-names>E. H.</given-names></name>
<article-title>Visuospatial coding as ubiquitous scaffolding for human cognition</article-title>. <source>Trends Cogn. Sci.</source>
<volume>26</volume>, <fpage>81</fpage>&#x02013;<lpage>96</lpage> (<year>2022</year>).<pub-id pub-id-type="pmid">34799253</pub-id>
</mixed-citation></ref><ref id="R63"><label>63.</label><mixed-citation publication-type="journal"><name><surname>Aguirre</surname><given-names>G. K.</given-names></name>, <name><surname>Zarahn</surname><given-names>E.</given-names></name> &#x00026; <name><surname>D&#x02019;Esposito</surname><given-names>M.</given-names></name>
<source>An Area within Human Ventral Cortex Sensitive to &#x0201c;Building&#x0201d; Stimuli: Evidence and Implications</source>. (<year>1998</year>).</mixed-citation></ref><ref id="R64"><label>64.</label><mixed-citation publication-type="journal"><name><surname>Epstein</surname><given-names>R.</given-names></name> &#x00026; <name><surname>Kanwisher</surname><given-names>N.</given-names></name>
<article-title>A cortical representation of the local visual environment</article-title>. <source>Nature</source>
<volume>392</volume>, <fpage>598</fpage>&#x02013;<lpage>601</lpage> (<year>1998</year>).<pub-id pub-id-type="pmid">9560155</pub-id>
</mixed-citation></ref><ref id="R65"><label>65.</label><mixed-citation publication-type="journal"><name><surname>Parvizi</surname><given-names>J</given-names></name>. <etal/>
<article-title>Electrical Stimulation of Human Fusiform Face-Selective Regions Distorts Face Perception</article-title>. <source>J. Neurosci.</source>
<volume>32</volume>, <fpage>14915</fpage>&#x02013;<lpage>14920</lpage> (<year>2012</year>).<pub-id pub-id-type="pmid">23100414</pub-id>
</mixed-citation></ref><ref id="R66"><label>66.</label><mixed-citation publication-type="journal"><name><surname>James</surname><given-names>K. H.</given-names></name>
<article-title>The Importance of Handwriting Experience on the Development of the Literate Brain</article-title>. <source>Curr. Dir. Psychol. Sci.</source>
<volume>26</volume>, <fpage>502</fpage>&#x02013;<lpage>508</lpage> (<year>2017</year>).</mixed-citation></ref><ref id="R67"><label>67.</label><mixed-citation publication-type="journal"><name><surname>Hsiao</surname><given-names>J. H.</given-names></name> &#x00026; <name><surname>Cottrell</surname><given-names>G.</given-names></name>
<article-title>Two Fixations Suffice in Face Recognition</article-title>. <source>Psychol. Sci.</source>
<volume>19</volume>, <fpage>9981006</fpage> (<year>2008</year>).</mixed-citation></ref><ref id="R68"><label>68.</label><mixed-citation publication-type="journal"><name><surname>Kubota</surname><given-names>E</given-names></name>. <etal/>
<source>White matter connections of human ventral temporal cortex are organized by cytoarchitecture, eccentricity, and category-selectivity from birth</source>. Preprint at <pub-id pub-id-type="doi">10.1101/2024.07.29.605705</pub-id> (<year>2024</year>).</mixed-citation></ref><ref id="R69"><label>69.</label><mixed-citation publication-type="journal"><name><surname>Arcaro</surname><given-names>M. J.</given-names></name> &#x00026; <name><surname>Livingstone</surname><given-names>M. S.</given-names></name>
<article-title>A hierarchical, retinotopic proto-organization of the primate visual system at birth</article-title>. <source>eLife</source>
<volume>6</volume>, <fpage>e26196</fpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28671063</pub-id>
</mixed-citation></ref><ref id="R70"><label>70.</label><mixed-citation publication-type="journal"><name><surname>Butt</surname><given-names>O. H.</given-names></name>, <name><surname>Benson</surname><given-names>N. C.</given-names></name>, <name><surname>Datta</surname><given-names>R.</given-names></name> &#x00026; <name><surname>Aguirre</surname><given-names>G. K.</given-names></name>
<article-title>The Fine-Scale Functional Correlation of Striate Cortex in Sighted and Blind People</article-title>. <source>J. Neurosci.</source>
<volume>33</volume>, <fpage>16209</fpage>&#x02013;<lpage>16219</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">24107953</pub-id>
</mixed-citation></ref><ref id="R71"><label>71.</label><mixed-citation publication-type="journal"><name><surname>Kamps</surname><given-names>F. S.</given-names></name>, <name><surname>Hendrix</surname><given-names>C. L.</given-names></name>, <name><surname>Brennan</surname><given-names>P. A.</given-names></name> &#x00026; <name><surname>Dilks</surname><given-names>D. D.</given-names></name>
<article-title>Connectivity at the origins of domain specificity in the cortical face and place networks</article-title>. <source>Proc. Natl. Acad. Sci.</source>
<volume>117</volume>, <fpage>6163</fpage>&#x02013;<lpage>6169</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32123077</pub-id>
</mixed-citation></ref><ref id="R72"><label>72.</label><mixed-citation publication-type="journal"><name><surname>Striem-Amit</surname><given-names>E</given-names></name>. <etal/>
<article-title>Functional connectivity of visual cortex in the blind follows retinotopic organization principles</article-title>. <source>Brain</source>
<volume>138</volume>, <fpage>1679</fpage>&#x02013;<lpage>1695</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25869851</pub-id>
</mixed-citation></ref><ref id="R73"><label>73.</label><mixed-citation publication-type="journal"><name><surname>Elston</surname><given-names>G. N.</given-names></name>, <name><surname>Oga</surname><given-names>T.</given-names></name> &#x00026; <name><surname>Fujita</surname><given-names>I.</given-names></name>
<article-title>Spinogenesis and Pruning Scales across Functional Hierarchies</article-title>. <source>J. Neurosci.</source>
<volume>29</volume>, <fpage>3271</fpage>&#x02013;<lpage>3275</lpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19279264</pub-id>
</mixed-citation></ref><ref id="R74"><label>74.</label><mixed-citation publication-type="journal"><name><surname>Elston</surname><given-names>G. N.</given-names></name> &#x00026; <name><surname>Fujita</surname><given-names>I.</given-names></name>
<article-title>Pyramidal cell development: postnatal spinogenesis, dendritic growth, axon growth, and electrophysiology</article-title>. <source>Front. Neuroanat.</source>
<volume>8</volume>, (<year>2014</year>).</mixed-citation></ref><ref id="R75"><label>75.</label><mixed-citation publication-type="journal"><name><surname>Dalton</surname><given-names>K. M.</given-names></name>
<etal/>
<article-title>Gaze fixation and the neural circuitry of face processing in autism</article-title>. <source>Nat. Neurosci.</source>
<volume>8</volume>, <fpage>519</fpage>&#x02013;<lpage>526</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15750588</pub-id>
</mixed-citation></ref><ref id="R76"><label>76.</label><mixed-citation publication-type="journal"><name><surname>Lefton</surname><given-names>L. A.</given-names></name>, <name><surname>Nagle</surname><given-names>R. J.</given-names></name>, <name><surname>Johnson</surname><given-names>G.</given-names></name> &#x00026; <name><surname>Fisher</surname><given-names>D. F.</given-names></name>
<article-title>Eye Movement Dynamics of Good and Poor Readers: Then and Now</article-title>. <source>J. Read. Behav.</source>
<volume>11</volume>, <fpage>319</fpage>&#x02013;<lpage>328</lpage> (<year>1979</year>).</mixed-citation></ref><ref id="R77"><label>77.</label><mixed-citation publication-type="journal"><name><surname>Avidan</surname><given-names>G.</given-names></name>, <name><surname>Hasson</surname><given-names>U.</given-names></name>, <name><surname>Malach</surname><given-names>R.</given-names></name> &#x00026; <name><surname>Behrmann</surname><given-names>M.</given-names></name>
<article-title>Detailed Exploration of Face-related Processing in Congenital Prosopagnosia: 2. Functional Neuroimaging Findings</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>17</volume>, <fpage>1150</fpage>&#x02013;<lpage>1167</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">16102242</pub-id>
</mixed-citation></ref><ref id="R78"><label>78.</label><mixed-citation publication-type="journal"><name><surname>Duchaine</surname><given-names>B.</given-names></name> &#x00026; <name><surname>Nakayama</surname><given-names>K.</given-names></name>
<article-title>The Cambridge Face Memory Test: Results for neurologically intact individuals and an investigation of its validity using inverted face stimuli and prosopagnosic participants</article-title>. <source>Neuropsychologia</source>
<volume>44</volume>, <fpage>576</fpage>&#x02013;<lpage>585</lpage> (<year>2006</year>).<pub-id pub-id-type="pmid">16169565</pub-id>
</mixed-citation></ref><ref id="R79"><label>79.</label><mixed-citation publication-type="journal"><name><surname>Rossion</surname><given-names>B</given-names></name>. <article-title>Constraining the cortical face network by neuroimaging studies of acquired prosopagnosia</article-title>. <source>NeuroImage</source>
<volume>40</volume>, <fpage>423</fpage>&#x02013;<lpage>426</lpage> (<year>2008</year>).<pub-id pub-id-type="pmid">18086537</pub-id>
</mixed-citation></ref><ref id="R80"><label>80.</label><mixed-citation publication-type="journal"><name><surname>Dale</surname><given-names>A. M.</given-names></name>, <name><surname>Fischl</surname><given-names>B.</given-names></name> &#x00026; <name><surname>Sereno</surname><given-names>M. I.</given-names></name>
<source>Cortical Surface-Based Analysis</source>. (<year>1999</year>).</mixed-citation></ref><ref id="R81"><label>81.</label><mixed-citation publication-type="journal"><name><surname>Sereno</surname><given-names>M. I.</given-names></name>
<etal/>
<article-title>Borders of Multiple Visual Areas in Humans Revealed by Functional Magnetic Resonance Imaging</article-title>. <source>Science</source>
<volume>268</volume>, <fpage>889</fpage>&#x02013;<lpage>893</lpage> (<year>1995</year>).<pub-id pub-id-type="pmid">7754376</pub-id>
</mixed-citation></ref><ref id="R82"><label>82.</label><mixed-citation publication-type="journal"><name><surname>Nasr</surname><given-names>S</given-names></name>. <etal/>
<article-title>Scene-Selective Cortical Regions in Human and Nonhuman Primates</article-title>. <source>J. Neurosci.</source>
<volume>31</volume>, <fpage>13771</fpage>&#x02013;<lpage>13785</lpage> (<year>2011</year>).<pub-id pub-id-type="pmid">21957240</pub-id>
</mixed-citation></ref><ref id="R83"><label>83.</label><mixed-citation publication-type="journal"><name><surname>Palejwala</surname><given-names>A. H.</given-names></name>
<etal/>
<article-title>Anatomy and white matter connections of the lateral occipital cortex</article-title>. <source>Surg. Radiol. Anat.</source>
<volume>42</volume>, <fpage>315</fpage>&#x02013;<lpage>328</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">31734739</pub-id>
</mixed-citation></ref><ref id="R84"><label>84.</label><mixed-citation publication-type="journal"><name><surname>Bates</surname><given-names>D.</given-names></name>, <name><surname>Maechler</surname><given-names>M.</given-names></name>, <name><surname>Bolker</surname><given-names>B.</given-names></name> &#x00026; <name><surname>Walker</surname><given-names>S.</given-names></name>
<source>lme4: Linear Mixed-Effects Models using &#x02018;Eigen&#x02019; and S4. 1.1-35.5</source>
<pub-id pub-id-type="doi">10.32614/CRAN.package.lme4</pub-id> (<year>2003</year>).</mixed-citation></ref></ref-list></back><floats-group><fig position="float" id="F1"><label>Figure 1.</label><caption><title>Toonotopy experiment</title><p id="P75">(A) Toonotopy stimuli from Finzi et al. (2021) features a bar with colorful cartoon images of faces, words, bodies, places, and objects that change at 8Hz sweeps across a gray background at 4 angles (0&#x000b0;, 45&#x000b0;, 90&#x000b0;, 135&#x000b0;) each in 2 directions. Participants fixated at the center dot and indicated when the dot changed colors. (B) Population receptive field compressive spatial summation (pRF CSS) model<sup><xref rid="R19" ref-type="bibr">19</xref></sup>. Left two panels show a single pRF with parameters of location (x,y) and size (&#x003c3;) modeled by a 2D Gaussian followed by a compressive nonlinearity, used to model the voxel&#x02019;s response. Middle right panel shows schematic of the pRF distribution within an ROI, and the rightmost panel depicts the visual field coverage of all pRFs in left V1 ROI in an example 11-year-old. (C) Phase, eccentricity, and size maps in an example adolescent (age 11) with V1 (purple), V2 (magenta), and V3 (gold) borders illustrated on the size map. All participants - <xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 2</xref>.(D) pRF size versus eccentricity relationship is similar across adolescents (dotted line) and adults (solid line) in early visual cortex (V1 - V3).</p></caption><graphic xlink:href="nihpp-2025.01.14.633067v1-f0001" position="float"/></fig><fig position="float" id="F2"><label>Figure 2.</label><caption><title>Category-selective regions are modulated by the Toonotopy experiment</title><p id="P76">(A) Functional localizer (fLOC): Example stimuli of faces (children, adults; red), characters (words, numbers; blue), bodies and limbs (yellow), places (corridors, houses; green), and objects (car, guitar; black) from the fLOC experiment). (B) We identified in each participant category-selective functional regions of interest (ROIs) from the fLOC experiment, ROIs are labeled by preferred category and anatomical locations. 7 ROI were in the ventral stream (IOG-faces, pFus-faces, mFus-faces, pOTS-words, mOTS-words, OTS-bodies, CoS-places), 4 ROIs in the lateral stream (pSTS-faces, LOS-bodies, ITG-bodies, MTG-bodies), and 2 ROIs in the dorsal stream (MOG-places, IPS-places). <italic toggle="yes">Left:</italic> bilateral ventral ROIs in an example 17-year-old. <italic toggle="yes">Right:</italic> dorsal and lateral ROIs in the right hemisphere in an example 11-year-old. Left hemisphere dorsal and lateral ROIs are the same as the right hemisphere ROIs. (C) Violin plots of proportion of voxels with greater than 20% variance explained in each pRF in category-selective ROIs in the left hemisphere (light) and right hemisphere (dark) ventral, lateral, and dorsal streams in adolescents (a) and adults (A). Black circle: mean. Error bars: &#x000b1; SE (standard error of the mean). Each dot is a participant.</p></caption><graphic xlink:href="nihpp-2025.01.14.633067v1-f0002" position="float"/></fig><fig position="float" id="F3"><label>Figure 3.</label><caption><title>pRF properties in high-level category selective regions vary across category and stream.</title><p id="P77">(A) <italic toggle="yes">Top:</italic> pRF center polar plots for right hemisphere (dark) and left hemisphere (light) face-selective (reds; IOG, pFus, mFus, pSTS), word-selective (blues; pOTS, mOTS), bodypart-selective (yellows; OTS, LOS, ITG, MTG), and place-selective (greens; CoS, MOG, IPS) regions in the ventral, lateral, and dorsal streams in adolescents ages 10 - 17. <italic toggle="yes">Bottom:</italic> pRF center polar plots same as A but in adults ages 22 - 32. (B) Violin plots of y-position of pRFs in visual degrees for category-selective ROIs in the left hemisphere (light) and right hemisphere (dark) ventral, lateral, and dorsal streams in adolescents (a) and adults (A). ROI colors are the same as in A. Black circle: mean. Error bars: &#x000b1; SE. Each dot is a participant. (C) Same as B but for eccentricity. (D) Same as B but for pRF size.</p></caption><graphic xlink:href="nihpp-2025.01.14.633067v1-f0003" position="float"/></fig><fig position="float" id="F4"><label>Figure 4.</label><caption><title>Visual field coverage (VFC) of category-selective ROIs develops differentially.</title><p id="P78">(A) Average VFC of each ROI in the left hemisphere across adolescents (top row) and adults (bottom row) hemisphere with a gradient ranging from higher coverage in dark red and lower coverage in dark blue. VFC is calculated as the proportion of pRFs covering each point in the visual field for each participant and is then averaged across participants in the group. <italic toggle="yes">White asterisks with coordinates:</italic> average center of mass of the VFC. Black dotted lines: average full-width half max (FWHM) of the coverage. (B) Same as A but in the right hemisphere. (C) Violin plots of total FWHM in visual degrees for category-selective ROIs in the left hemisphere (light) and right hemisphere (dark) ventral, lateral, and dorsal streams in adolescents (a) and adults (A). Black circle: mean. Error bars: &#x000b1; SE</p></caption><graphic xlink:href="nihpp-2025.01.14.633067v1-f0004" position="float"/></fig><fig position="float" id="F5"><label>Figure 5.</label><caption><title>Category-selectivity differentially develops from adolescence to adulthood.</title><p id="P79">(A) Violin plots of average t-values for the category contrast for the preferred category of each ROI (e.g., faces &#x0003e; all other categories for the IOG-face ROI) in the left hemisphere (light) and right hemisphere (dark) ventral, lateral, and dorsal stream ROIs in adolescents (a) and adults (A). Black circle: mean. Error bars: &#x000b1; SE. B. Linear relationships between total FWHM and mean t-value in each ROI in the left hemisphere (top) and right hemisphere (bottom). Each dot is a participant; adolescents are colored in lighter colors.</p></caption><graphic xlink:href="nihpp-2025.01.14.633067v1-f0005" position="float"/></fig></floats-group></article>