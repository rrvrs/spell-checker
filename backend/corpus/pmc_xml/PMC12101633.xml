<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408314</article-id><article-id pub-id-type="pmc">PMC12101633</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0323276</article-id><article-id pub-id-type="publisher-id">PONE-D-24-38627</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Bioassays and Physiological Analysis</subject><subj-group><subject>Electrophysiological Techniques</subject><subj-group><subject>Brain Electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain Electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain Electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain Mapping</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Clinical Medicine</subject><subj-group><subject>Clinical Neurophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability Theory</subject><subj-group><subject>Random Variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>White Noise</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical Noise</subject><subj-group><subject>Gaussian Noise</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Speech Signal Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Neurolinguistics</subject><subj-group><subject>Sentence Processing</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurolinguistics</subject><subj-group><subject>Sentence Processing</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Appropriate data segmentation improves speech encoding models: Analysis and simulation of electrophysiological recordings</article-title><alt-title alt-title-type="running-head">Appropriate data segmentation improves speech encoding models</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4472-7626</contrib-id><name><surname>Bialas</surname><given-names>Ole</given-names></name><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Lalor</surname><given-names>Edmund C.</given-names></name><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Department of Biomedical Engineering, University of Rochester, Rochester, New York, United States of America</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Del Monte Institute for Neuroscience , University of Rochester, Rochester, New York, United States of America</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Center for Visual Science, University of Rochester, Rochester, New York, United States of America</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Yang</surname><given-names>Li</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Sichuan University, CHINA</addr-line>
</aff><author-notes><corresp id="cor001">* E-mail: <email>obialas@ur.rochester.edu</email> (OB); <email>elalor@ur.rochester.edu</email> (CL)</corresp><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0323276</elocation-id><history><date date-type="received"><day>21</day><month>9</month><year>2024</year></date><date date-type="accepted"><day>4</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Bialas, Lalor</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Bialas, Lalor</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0323276.pdf"/><abstract><sec id="sec014"><title>Background:</title><p>In recent decades, studies modeling the neural processing of continuous, naturalistic, speech provided new insights into how speech and language are represented in the brain. However, the linear encoder models commonly used in such studies assume that the underlying data are stationary, varying to a fixed degree around a constant mean. Long, continuous, neural recordings may violate this assumption leading to impaired model performance. We aimed to examine the effect of non-stationary trends in continuous neural recordings on the performance of linear speech encoding models.</p></sec><sec id="sec015"><title>Methods:</title><p>We used temporal response functions (TRFs) to predict continuous neural responses to speech while splitting the data into segments of varying length, prior to model fitting. Our Hypothesis was that if the data were non-stationary, segmentation should improve model performance by making individual segments approximately stationary. We simulated and predicted stationary and non-stationary recordings to test our hypothesis under a known ground truth and predicted the brain activity of participants who listened to a narrated story, to test our hypothesis on actual neural recordings.</p></sec><sec id="sec016"><title>Results:</title><p>Simulations showed that, for stationary data, increasing segmentation steadily decreased model performance. For non-stationary data however, segmentation initially improved model performance. Modeling of neural recordings yielded similar results: segments of intermediate length (5&#x02013;15&#x02009;s) led to improved model performance compared to very short (1&#x02013;2&#x02009;s) and very long (30&#x02013;120&#x02009;s) segments.</p></sec><sec id="sec017"><title>Conclusions:</title><p>We showed that data segmentation improves the performance of encoding models for both simulated and real neural data and that this can be explained by the fact that shorter segments approximate stationarity more closely. Thus, the common practice of applying encoding models to long continuous segments of data is suboptimal and recordings should be segmented prior to modeling.</p></sec></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="4"/><table-count count="0"/><page-count count="11"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All files are available from the OpenNeuro database (doi:10.18112/openneuro.ds004408.v1.0.8).</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All files are available from the OpenNeuro database (doi:10.18112/openneuro.ds004408.v1.0.8).</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Traditionally, studies on the neural processing of speech and language relied on repetitive presentations of prototypical words or sentences that are carefully manipulated along a single dimension of interest [<xref rid="pone.0323276.ref001" ref-type="bibr">1</xref>]. However, while those studies were successful in probing and identifying different modules of speech processing, their artificial and univariate approach makes them incapable of capturing the multi-stream, distributed nature of speech processing [<xref rid="pone.0323276.ref002" ref-type="bibr">2</xref>]. One way to overcome this limitation is to record brain responses to continuous narrative stories that present speech in its natural complexity. However, analyzing these recordings requires statistical models that can dissect activity arising from the different concurrent processes that contribute to the recorded activity.</p><p>In recent decades studies on the neural processing of speech increasingly used encoding models that predict the ongoing brain activity from (a combination of) acoustic, phonetic and semantic speech features [<xref rid="pone.0323276.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0323276.ref002" ref-type="bibr">2</xref>]. Here, we focus on temporal response functions (TRFs) because they are comparatively small models can be easily interpreted and because they are used widely within the speech community.</p><p>The TRF approach treats the brain as a linear, time-invariant (LTI system) and tires to estimate the response characteristics of this system via deconvolution [<xref rid="pone.0323276.ref003" ref-type="bibr">3</xref>]. While, in theory, the TRF method works on indefinitely long continuous data, in practice, the data is usually divided into multiple segments. However, there exists no consensus or standard procedure with respect to the duration of those segments. While many studies analyze responses to segments of narrative stories on the order of minutes [<xref rid="pone.0323276.ref004" ref-type="bibr">4</xref>&#x02013;<xref rid="pone.0323276.ref006" ref-type="bibr">6</xref>], others use shorter segments lasting tens of seconds [<xref rid="pone.0323276.ref007" ref-type="bibr">7</xref>] or even single sentences [<xref rid="pone.0323276.ref008" ref-type="bibr">8</xref>]. The way the data is segmented usually depends on the experimental process (e.g., whether the participants listen to individual sentences or long segments of a story) and how this affects the model&#x02019;s outcome is not considered. This is somewhat negligent because there are several ways in which data segmentation may affect the performance of TRF models.</p><sec id="sec002"><title>Effects of data segmentation</title><p>TRFs are linear models that map specific stimulus features to the ongoing brain response. This mapping is estimated by multiplying the covariance matrix for predictor (e.g. speech envelope) and estimand (e.g. neural recording) with the predictor&#x02019;s autocovariance matrix (see <xref rid="pone.0323276.e009" ref-type="disp-formula">Eq 2</xref>). When the TRF is estimated across multiple segments, the covariance matrices are averaged across all segments and the TRF is obtained from the average matrices. The validity of this procedure hinges on the assumption that this converges on a stable estimate of the average covariance matrix. However, if there are only few segments, a single outlier may substantially alter the average. If the number of segments is larger, the effect of any single one is reduced. Thus, segmentation regulates overfitting to extreme values.</p><p>Segmentation also affects model fitting. Typically, fitting a TRF involves optimizing the regularization parameter <inline-formula id="pone.0323276.e001"><alternatives><graphic xlink:href="pone.0323276.e001.jpg" id="pone.0323276.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> which penalizes large model weights and prevents overfitting. The optimal value for <inline-formula id="pone.0323276.e002"><alternatives><graphic xlink:href="pone.0323276.e002.jpg" id="pone.0323276.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is found by testing multiple candidate values by randomly splitting the data into train and test set, fitting the TRF on the former and evaluating its accuracy by predicting the latter. This requires that both test and train set are representative of the overall trends in the data so that the model can generalize from one set to the other. Representative subsets may be obtained more reliably if many short segments are randomly sampled from the recording.</p><p>Of course, segmenting the data into ever shorter bits may impair model accuracy if, at some point, the segments become too short to reliably estimate the covariance matrices. With too little data, the individual covariance matrices might represent the noise in the respective segment, rather than the overall trend in the data. Finally, a key reason for why segment duration matters is that the TRF method treats the brain as a linear time-invariant (LTI) system which implicitly assumes that the modeled signal is a stationary process whose statistical properties are stable across time such that the particular time we observe it is of no relevance [<xref rid="pone.0323276.ref009" ref-type="bibr">9</xref>].</p></sec><sec id="sec003"><title>Are neural recordings stationary?</title><p>Several studies examined the stationarity of EEG recordings by testing whether their amplitude distribution is Gaussian [<xref rid="pone.0323276.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0323276.ref011" ref-type="bibr">11</xref>] or whether observed sequences of values above or below the mean are longer than would be expected by chance [<xref rid="pone.0323276.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0323276.ref013" ref-type="bibr">13</xref>]. They agreed that, above a certain duration, EEG recordings could not be considered stationary, even though the critical duration varied between 2 and 25 seconds (for a review, see [<xref rid="pone.0323276.ref014" ref-type="bibr">14</xref>]). One study found no difference in the (non-)stationarity between EEG recordings during auditory stimulation, a cognitive task, and resting state, suggesting that these properties are mainly determined by the spontaneous portion of the EEG signal [<xref rid="pone.0323276.ref015" ref-type="bibr">15</xref>]. However, in lack of a ground truth, the origin of non-stationary trends is difficult to assert, and it is of minor importance to the question of appropriate data segmentation.</p><p>One can also reason about the stationarity of neural recordings based on the signals&#x02019; power spectra. It is generally acknowledged that neural recordings exhibit a 1/f-like power spectrum, meaning that there is an inverse linear relationship between log power and log frequency [<xref rid="pone.0323276.ref016" ref-type="bibr">16</xref>&#x02013;<xref rid="pone.0323276.ref018" ref-type="bibr">18</xref>]. This leads to the contradiction that a 1/f-process can not be stationary, since Rayleigh&#x02019;s energy theorem states that a signal&#x02019;s energy is equal to the integral of the power spectrum from negative to positive infinity. For a 1/f-distribution, however, the integral diverges because power approaches infinity as the frequency approaches 0. The alternative is to consider the signal as a non-stationary process where the signal can be integrated over a discrete time and frequency range but a single time-independent energy value can not be obtained [<xref rid="pone.0323276.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0323276.ref020" ref-type="bibr">20</xref>].</p><p>In summary, neural time-series exhibit temporal and spectral characteristics of non-stationary processes. This is intuitively plausible, since the brain is a dynamic system where complex networks interact across multiple temporal and spatial scales and the signals recorded from this system mix with noise from various physiological and artificial sources. Our approach differs from the above cited studies in that we do not test non-stationarity directly but instead estimate the effect of data segmentation on the accuracy of models that assume stationarity. By combining this empirical test with generative simulations, we can show that the observed effect of segmentation can be explained by non-stationary trends in the data.</p></sec><sec id="sec004"><title>Optimal segmentation</title><p>Taken together, the above points suggest that there is an optimal segment duration, where the data can be considered approximately stationary and the effect of outlier segments is suppressed while the individual segments are still long enough to reliably estimate their covariance and autocovariance matrices. Here, we systematically investigate the effect of data segmentation on the performance of TRF models. We use generative simulations to demonstrate that data segmentation improves prediction accuracy in the presence of non-stationary noise. Furthermore, we analyze EEG recordings of neural responses to continuous naturalistic speech and show that segmentation substantially improves prediction accuracy for many participants without negatively affecting the others. We thus recommend that future studies that use TRF models to predict EEG responses to continuous speech use segments of about 10 seconds. We don&#x02019;t necessarily mean to suggest that the experiment needs to be organized in 10 second trials, rather that the data should be restructured prior to model fitting.</p></sec></sec><sec sec-type="materials|methods" id="sec005"><title>Materials and methods</title><p>Our central hypothesis is that dividing continuous neural recordings into shorter segments should improve model accuracy if the data are non-stationary. To test this hypothesis we fit our models on the same data while dividing them into segments of varying length. We use a generative simulation to test our hypothesis under a known ground truth and analyze EEG responses to continuous speech to test our hypothesis on actual neural data.</p><sec id="sec006"><title>0.1. Data and code availability</title><p>The raw data are hosted on <ext-link xlink:href="https://openneuro.org/datasets/ds004408" ext-link-type="uri">OpenNeuro</ext-link> and can be obtained, together the code for all simulations and analyses, from the <ext-link xlink:href="https://github.com/OleBialas/stationarity" ext-link-type="uri">Github</ext-link> repository for this study.</p></sec><sec id="sec007"><title>0.2. Ethics</title><p>This work used publicly available data. The original recordings were acquired in two unrelated studies [<xref rid="pone.0323276.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0323276.ref005" ref-type="bibr">5</xref>] at Trinity College Dublin where they were approved by the Ethics Committee of the School of Psychology. No additional data were recorded for this study.</p></sec><sec id="sec008"><title>Temporal response function</title><p>Under our model, the observed neural response is expressed as the weighted sum of the stimulus feature across multiple time lags, defined by the equation:</p><disp-formula id="pone.0323276.e003"><alternatives><graphic xlink:href="pone.0323276.e003.jpg" id="pone.0323276.e003g" position="anchor"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:munder><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>&#x003f5;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><p>Where <italic toggle="yes">r</italic>(<italic toggle="yes">t</italic>) is the response observed at time <italic toggle="yes">t</italic> and <inline-formula id="pone.0323276.e004"><alternatives><graphic xlink:href="pone.0323276.e004.jpg" id="pone.0323276.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the stimulus feature at time lag <inline-formula id="pone.0323276.e005"><alternatives><graphic xlink:href="pone.0323276.e005.jpg" id="pone.0323276.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. The effect of the feature at time lag <inline-formula id="pone.0323276.e006"><alternatives><graphic xlink:href="pone.0323276.e006.jpg" id="pone.0323276.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is given by the weight <inline-formula id="pone.0323276.e007"><alternatives><graphic xlink:href="pone.0323276.e007.jpg" id="pone.0323276.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and the error term <inline-formula id="pone.0323276.e008"><alternatives><graphic xlink:href="pone.0323276.e008.jpg" id="pone.0323276.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> contains the residual response unexplained by the model. The vector <italic toggle="yes">w</italic> that contains the weights across all time lags is called the temporal response function (TRF). The TRF is obtained via regularized regression, which, in matrix notation, can be written as:</p><disp-formula id="pone.0323276.e009"><alternatives><graphic xlink:href="pone.0323276.e009.jpg" id="pone.0323276.e009g" position="anchor"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>S</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mi>I</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>S</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label></disp-formula><p>Where <inline-formula id="pone.0323276.e010"><alternatives><graphic xlink:href="pone.0323276.e010.jpg" id="pone.0323276.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mtext mathvariant="italic">S</mml:mtext><mml:mtext mathvariant="italic">T</mml:mtext></mml:msup><mml:mtext mathvariant="italic">S</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mrow/></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is the inverted autocovariance matrix of the stimulus, <inline-formula id="pone.0323276.e011"><alternatives><graphic xlink:href="pone.0323276.e011.jpg" id="pone.0323276.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mtext mathvariant="italic">S</mml:mtext><mml:mrow><mml:mtext mathvariant="italic">T</mml:mtext></mml:mrow></mml:msup><mml:mtext mathvariant="italic">r</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula> is the covariance matrix of stimulus and response and <inline-formula id="pone.0323276.e012"><alternatives><graphic xlink:href="pone.0323276.e012.jpg" id="pone.0323276.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is a diagonal regularization matrix. The value of <inline-formula id="pone.0323276.e013"><alternatives><graphic xlink:href="pone.0323276.e013.jpg" id="pone.0323276.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is optimized for model accuracy, defined as the average correlation (Pearson&#x02019;s r) between the observed response <italic toggle="yes">r</italic>(<italic toggle="yes">t</italic>) and its prediction <inline-formula id="pone.0323276.e014"><alternatives><graphic xlink:href="pone.0323276.e014.jpg" id="pone.0323276.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> which is obtained by convolving stimulus features and TRF:</p><disp-formula id="pone.0323276.e015"><alternatives><graphic xlink:href="pone.0323276.e015.jpg" id="pone.0323276.e015g" position="anchor"/><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:munder><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(3)</label></disp-formula><p>A more detailed mathematical description of the TRF, including multi-variable models, can be found in [<xref rid="pone.0323276.ref003" ref-type="bibr">3</xref>].</p></sec><sec id="sec009"><title>Modeling EEG responses to speech</title><p>We analyzed data from 19 healthy, neurotypical adults (age 19-38, 13 male) who listened to a reading of roughly the first hour of "The Old Man and the Sea" by a single American male speaker. The prose in this part of the story is mostly descriptive and focuses on the protagonists environment, actions, and thoughts. It&#x02019;s narrated in the third person and the narrators tone alternates between somber reflection and subtle optimism. The story was divided into 20 segments, each lasting between 170 and 200 seconds. As the participants listened, their brain activity was recorded using a 128-channel ActiveTwo EEG system (BioSemi) at a sampling rate of 512 Hz. The data , which were collected in unrelated studies [<xref rid="pone.0323276.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0323276.ref005" ref-type="bibr">5</xref>], are publicly available and no additional data was collected for the purpose of this study. We chose this data set because it has proven to be well suited for TRF analyses and the individual recordings are long enough to test our models across a wide range of segment durations.</p><p>We cropped all 20 segments to a duration of 120 seconds, resulting in 50 minutes of data per subject We filtered the segments between 1 and 20 Hz using a non-causal hamming-window band pass filter and downsampled to 64 Hz using MNE-Python [<xref rid="pone.0323276.ref021" ref-type="bibr">21</xref>]. Next, we used the random sample consensus algorithm to identify channels as bad if they were poorly predicted by their neighbors [<xref rid="pone.0323276.ref022" ref-type="bibr">22</xref>], interpolated them using spherical splines and re-referenced all channels to the global average.</p><p>We predicted the EEG responses from the spectro-temporal characteristics of the stimulus by band-pass filtering the audio material between 20 and 9000 Hz, dividing it into log-spaced bands [<xref rid="pone.0323276.ref023" ref-type="bibr">23</xref>] and computing the absolute Hilbert envelope for each spectral band, all using the soundlab Python package [<xref rid="pone.0323276.ref024" ref-type="bibr">24</xref>]. To test how the number of parameters and the degree of spectral detail affect model accuracy, we represented the stimulus with 1, 8 and 16 spectral bands and repeated the modeling procedure for each representation.</p><p>The TRF is fit to the data by testing multiple candidate values for <inline-formula id="pone.0323276.e016"><alternatives><graphic xlink:href="pone.0323276.e016.jpg" id="pone.0323276.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and selecting the one that maximizes prediction accuracy. For each of nine log-spaced values between 10<sup>&#x02212;5</sup> and 10<sup>3</sup>, the data segments are repeatedly split into train and test set using 5-fold cross validation. For each split, the TRF is estimated on the train set and evaluated by predicting the test set. Model accuracy for a given value of <inline-formula id="pone.0323276.e017"><alternatives><graphic xlink:href="pone.0323276.e017.jpg" id="pone.0323276.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is obtained by averaging prediction accuracy across all channels and splits. All modeling was done using the mTRFpy toolbox [<xref rid="pone.0323276.ref025" ref-type="bibr">25</xref>].</p><p>To estimate the effect of data segmentation on model accuracy, we repeat the above process while dividing the same 50 minutes of EEG recordings from every participant into segments of length 120, 60, 40, 30, 15, 10, 5, 2 and 1 second(s). The segment durations were chosen so that the 50 minutes of data could be evenly divided into segments for every duration. We excluded the data from further analysis if the accuracy of the best model was below the threshold of <italic toggle="yes">r</italic>&#x02009;=&#x02009;0.01. We deliberately chose a low threshold to only reject participants where the model explained effectively none of the variance in the data. In our sample, this affected one participant for whom prediction accuracy approximated 0 across all models and segment durations (mean <italic toggle="yes">r</italic>&#x02009;=&#x02009;0.001).</p></sec><sec id="sec010"><title>0.3. Simulation</title><p>We use a simple generative model to test the accuracy of TRF models under different conditions. We define the transfer function for simulating the neural response using a Gabor wavelet, obtained by modulating a sinusoidal with a Gaussian function:</p><disp-formula id="pone.0323276.e018"><alternatives><graphic xlink:href="pone.0323276.e018.jpg" id="pone.0323276.e018g" position="anchor"/><mml:math id="M18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>cos</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(4)</label></disp-formula><p>Where <inline-formula id="pone.0323276.e019"><alternatives><graphic xlink:href="pone.0323276.e019.jpg" id="pone.0323276.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0323276.e020"><alternatives><graphic xlink:href="pone.0323276.e020.jpg" id="pone.0323276.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> are the mean and standard deviation of the Gaussian, of is the sinusoidal&#x02019;s frequency and <italic toggle="yes">t</italic> is time. The blue curve in <xref rid="pone.0323276.g001" ref-type="fig">Fig 1a</xref> shows an example of such a wavelet with <inline-formula id="pone.0323276.e021"><alternatives><graphic xlink:href="pone.0323276.e021.jpg" id="pone.0323276.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> s, <inline-formula id="pone.0323276.e022"><alternatives><graphic xlink:href="pone.0323276.e022.jpg" id="pone.0323276.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> s and <italic toggle="yes">f</italic>&#x02009;=&#x02009;4 Hz.</p><fig position="float" id="pone.0323276.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0323276.g001</object-id><label>Fig 1</label><caption><title>Generative simulation framework.</title></caption><graphic xlink:href="pone.0323276.g001" position="float"/></fig><p>We simulate a stimulus by generating a sequence of randomly spaced square pulses, where the free parameters are the range of amplitude and width of the pulses (<xref rid="pone.0323276.g001" ref-type="fig">Fig 1b</xref>, blue line). Then, we convolve this stimulus with the wavelet kernel (<xref rid="pone.0323276.g001" ref-type="fig">Fig 1b</xref>, green line) and add noise (<xref rid="pone.0323276.g001" ref-type="fig">Fig 1c</xref>) to obtain the simulated neural response as defined in <xref rid="pone.0323276.e003" ref-type="disp-formula">Eq 1</xref>. Finally, we fit a TRF to reconstruct the original wavelet (<xref rid="pone.0323276.g001" ref-type="fig">Fig 1a</xref> green line). To estimate the accuracy of the reconstructed TRF, we generate a new stimulus sequence and convolve it with the original and reconstructed TRF and calculate Pearson&#x02019;s correlation coefficient for the two responses (<xref rid="pone.0323276.g001" ref-type="fig">Fig 1d</xref>).</p><p>To determine how violating the assumption of stationarity affects model accuracy, we compare a scenario where the added noise is Gaussian to one where the noise has a 1/f distribution. 1/f noise is generated by computing the Fourier transform of white noise, imposing a 1/f distribution and computing the inverse Fourier transform of the modulated spectrum (for examples of Gaussian and 1/f noise see panels a and b of <xref rid="pone.0323276.g002" ref-type="fig">Fig 2</xref>). Note that the simulation represents the best-case scenario where non-stationarity is only introduced by the added background noise. If non-stationary background noise impairs model performance, this problem would certainly be amplified if the generating process itself would vary over time.</p><fig position="float" id="pone.0323276.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0323276.g002</object-id><label>Fig 2</label><caption><title>Simulation results.</title></caption><graphic xlink:href="pone.0323276.g002" position="float"/></fig><p>For both simulations, we divide the data into segments of 200, 100, 50, 25, 10, 5, 2 and 1 second(s), fit a TRF to 1000 seconds of training data and evaluate it on another 1000 seconds of testing data generated from the same model. Since the model is a stochastic process, we repeated the simulation 1000 times.</p></sec></sec><sec sec-type="results" id="sec011"><title>Results</title><p>The aim of using a generative model to simulate data was to examine the effect of segmentation on model accuracy against a known ground truth. The results show that, when the noise is Gaussian, segmentation into ever shorter bits gradually reduces model accuracy (<xref rid="pone.0323276.g002" ref-type="fig">Fig 2a</xref>). This makes perfect sense since the advantages of segmentation discussed earlier (representative sampling, robustness to outliers, ensuring stationarity), do not apply when dealing with homogeneous data from a stationary process. This trend is independent of the signal-to-noise ratio (SNR), although the model&#x02019;s variability increases at lower SNRs. However, when the noise is non-stationary, segmentation initially improves prediction accuracy if the SNR is sufficiently low (<xref rid="pone.0323276.g002" ref-type="fig">Fig 2b</xref>). Still, further segmentation decreases prediction accuracy, meaning that there is an optimal length where segments can be considered approximately stationary without substantially reducing model accuracy. Note that, since the simulation lacks any physiological plausibility, the absolute values contain no relevant information. For example, the absolute SNRs are very low because a lot of samples in the simulated response are 0, reducing the average power.</p><p>To test how segmentation affects models trained on actual neural data, we predicted EEG recordings of brain responses to continuous naturalistic speech from the spectrogram of the stimulus while dividing the data into ever shorter segments. To test how the effect of segmentation depends on the model&#x02019;s flexibility, we repeated the analysis for three different levels of spectral detail. Because prediction accuracy differed strongly between participants and models, we normalized each model&#x02019;s accuracy, for each participant, by the maximum for that participant, such that the results reflect the relative changes in model accuracy with segment duration. On average, decreasing segment duration from 120&#x02009;s to 10&#x02009;s improved prediction accuracy by 10 to 15 percent (<xref rid="pone.0323276.g003" ref-type="fig">Fig 3a</xref>). The improvement was larger for the 16-band model compared to the 1- and 8-band model, suggesting that models with more more parameters may be more sensitive to non-stationary trends in the data. Decreasing segment duration below 5&#x02009;s rapidly reduced prediction accuracy. The optimal value of the regularization parameter <inline-formula id="pone.0323276.e023"><alternatives><graphic xlink:href="pone.0323276.e023.jpg" id="pone.0323276.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> was decreased with segment duration (<xref rid="pone.0323276.g003" ref-type="fig">Fig 3b</xref>). This shows that averaging across segments regularizes the model such that models fit on a larger number of short segments require less additional regularization via <inline-formula id="pone.0323276.e024"><alternatives><graphic xlink:href="pone.0323276.e024.jpg" id="pone.0323276.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p><fig position="float" id="pone.0323276.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0323276.g003</object-id><label>Fig 3</label><caption><title>Effect of data segmentation on models for EEG responses to speech.</title></caption><graphic xlink:href="pone.0323276.g003" position="float"/></fig><p>Because the effect of segmentation on model accuracy was highly variable across participants, we conducted a post-hoc analysis to investigate the source of this variability. We computed the difference in prediction accuracy between 10&#x02009;s and 120&#x02009;s segments for each participant to obtain an estimate of model improvement due to segmentation. We also took the highest prediction accuracy as an estimate of how well the participant&#x02019;s data could be predicted with the optimal combination of parameters. Linear regression revealed a negative relationship between these two estimates (<italic toggle="yes">r</italic>&#x02009;=&#x02009;&#x02212;0.62, <italic toggle="yes">p</italic>&#x02009;=&#x02009;0.006), suggesting that segmentation was especially effective for participants where model fitting was suboptimal (likely due to non-stationary trends and outliers in the data).</p><p>While segmentation improved model accuracy for most participants, it reduced accuracy by 10 percent for one of them. This may indicate that the recordings from the respective participants were unusually stationary. As our simulations showed, segmentation does not improve model but instead reduce model accuracy when the data are stationary.</p><p>As we elaborated earlier, one way in which segmentation can improve model accuracy is by reducing the effect of outliers - if the number of segments is increased, the effect of any single segment is reduced. To illustrate this point, we selected one of the participants where segmentation had a large effect and estimated the TRF for each segment individually after dividing the data into 120 and 5 (the optimal duration for that participant) second segments, respectively (<xref rid="pone.0323276.g004" ref-type="fig">Fig 4a</xref>, <xref rid="pone.0323276.g004" ref-type="fig">4b</xref>). Here, we used only a single spectral band (i.e. the broadband envelope) to avoid arbitrarily selecting features. We identified outliers by sorting the segments by mean absolute TRF weight and selecting the largest five percent. We then computed the TRF for 120 and 5 second segments on the full data and with the outliers removed.</p><fig position="float" id="pone.0323276.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0323276.g004</object-id><label>Fig 4</label><caption><title>Segmentation regularizes the effect of outliers.</title></caption><graphic xlink:href="pone.0323276.g004" position="float"/></fig><p><xref rid="pone.0323276.g004" ref-type="fig">Fig 4c</xref> shows the respective TRFs for one fronto-central EEG channel with (solid line) and without (dashed line) outliers. While omitting 5% of outlier segments changed the mean absolute weight by 7% for 5&#x02009;s segments, omitting the same amount of data changed the mean absolute weight by 13% for 120&#x02009;s segments. This shows that increasing the number of segments can reduce the effect of outliers on the model&#x02019;s outcome. Note that this is merely an illustration, the optimal segment duration for isolating outliers will vary across recordings.</p></sec><sec sec-type="conclusions" id="sec012"><title>Discussion</title><p>We investigated how speech encoding models are affected by non-stationary trends in neural recordings and how this depends on the length of the individual data segments. Our hypothesis was that, if the data where non-stationary, reshaping the data into smaller segments would improve model accuracy.</p><p>We used a generative simulation to test model accuracy under stationary and non-stationary noise. Under stationary noise, model accuracy decreased monotonically with segment duration. However, when the noise was non-stationary segmentation initially improved accuracy. To test our hypothesis on actual neural data, we used TRFs to predict EEG recordings of participants who listened to a narrated story. The results were qualitatively similar to the non-stationary simulation &#x02014; segmentation improved model accuracy although very short segments were detrimental. The observed improvements were larger for the model that used a 16 band spectrogram, compared to the models using 8- and 1-band versions. Larger models could be affected more strongly because their flexibility could allow them to (over-)fit the non-stationary trends in the data.</p><p>Model improvements were observed mainly for segment durations between 2 and 20 seconds which is the same timescale where prior studies identified the transtion of EEG recordings to non-stationarity [<xref rid="pone.0323276.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0323276.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0323276.ref014" ref-type="bibr">14</xref>]. Taken together, these findings indicate that long continuous neural recordings are indeed non-stationary and that segmentation improves model accuracy by making the data more stationary.</p><p>While we strongly suggest that future studies on speech encoding adopt data segmentation as part of their model fitting process, it should be noted that we analyzed data from a relatively small group of participants who all listened to the same stimulus material and were recorded with the same EEG setup. Thus, other studies should not blindly copy the parameters that yielded the best result in our study but instead test multiple segment durations.</p><p>Naturally, one wonders what may be the source of non-stationary trends in neural recording. A study that tested the non-stationarity of EEG recordings under different experimental conditions found that there was no systematic difference between recordings during auditory stimulation, resting or cognitive tasks [<xref rid="pone.0323276.ref015" ref-type="bibr">15</xref>]. This suggests that non-stationarity is not directly related to perception or cognition but instead results from other, possibly non-physiological processes. This would also explain why we saw the largest improvements due to segmentation for participants where the baseline accuracy was low. However, this is speculative and would require additional studies where parameters of the recording setup are systematically varied.</p><p>Finally, one may object that dividing the data into segments of equal length is an overly simplistic way of dealing with deviations from stationarity in neural recordings. Maybe, there are extended segments where the recordings are perfectly stationary that would be unnecessarily interrupted by segmentation. There are more sophisticated algorithms that use wavelet transform, fractal dimensions or source separation to adaptively segment neural recordings based on their statistical properties [<xref rid="pone.0323276.ref026" ref-type="bibr">26</xref>&#x02013;<xref rid="pone.0323276.ref028" ref-type="bibr">28</xref>]. However, implementation of these procedures requires substantial technical knowledge, while dividing the data into normalized segments of equal length can be done in two lines of code and has very little negative consequences.</p></sec><sec sec-type="conclusions" id="sec013"><title>Conclusion</title><p>We demonstrated that segmentation improves the accuracy of TRF models that predict EEG recordings of brain responses to naturalistic speech. While long, continuous, recordings can provide a more naturalistic situation for speech comprehension, the resulting data should be reshaped into shorter segments prior to modeling. Ideally, one would choose the longest possible duration where the data can be considered (approximately) stationary. However, since this is difficult, the most practical solution may be to test multiple segment durations and choose the one that yields the highest accuracy.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0323276.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Hamilton</surname><given-names>LS</given-names></name>, <name><surname>Huth</surname><given-names>AG</given-names></name>. <article-title>The revolution will not be controlled: natural stimuli in speech neuroscience</article-title>. <source>Lang Cogn Neurosci</source>. <year>2020</year>;<volume>35</volume>(<issue>5</issue>):<fpage>573</fpage>&#x02013;<lpage>82</lpage>.<pub-id pub-id-type="pmid">32656294</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref002"><label>2</label><mixed-citation publication-type="preprint"><name><surname>Bouton</surname><given-names>S</given-names></name>, <name><surname>Chambon</surname><given-names>V</given-names></name>, <name><surname>Golestani</surname><given-names>N</given-names></name>, <name><surname>Formisano</surname><given-names>E</given-names></name>, <name><surname>Proix</surname><given-names>T</given-names></name>, <name><surname>Giraud</surname><given-names>AL</given-names></name>. Interpretability of statistical approaches in speech and language neuroscience. PsyArXiv. <year>2023</year>.</mixed-citation></ref><ref id="pone.0323276.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Crosse</surname><given-names>MJ</given-names></name>, <name><surname>Di Liberto</surname><given-names>GM</given-names></name>, <name><surname>Bednar</surname><given-names>A</given-names></name>, <name><surname>Lalor</surname><given-names>EC</given-names></name>. <article-title>The multivariate temporal response function (mTRF) Toolbox: a MATLAB toolbox for relating neural signals to continuous stimuli</article-title>. <source>Front Hum Neurosci</source>. <year>2016</year>;<volume>10</volume>:<fpage>604</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id>
<pub-id pub-id-type="pmid">27965557</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Di Liberto</surname><given-names>GM</given-names></name>, <name><surname>O&#x02019;Sullivan</surname><given-names>JA</given-names></name>, <name><surname>Lalor</surname><given-names>EC</given-names></name>. <article-title>Low-frequency cortical entrainment to speech reflects phoneme-level processing</article-title>. <source>Curr Biol</source>. <year>2015</year>;<volume>25</volume>(<issue>19</issue>):<fpage>2457</fpage>&#x02013;<lpage>65</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.030</pub-id>
<pub-id pub-id-type="pmid">26412129</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Broderick</surname><given-names>MP</given-names></name>, <name><surname>Anderson</surname><given-names>AJ</given-names></name>, <name><surname>Di Liberto</surname><given-names>GM</given-names></name>, <name><surname>Crosse</surname><given-names>MJ</given-names></name>, <name><surname>Lalor</surname><given-names>EC</given-names></name>. <article-title>Electrophysiological correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech</article-title>. <source>Curr Biol</source>. <year>2018</year>;<volume>28</volume>(<issue>5</issue>):803-809.e3. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cub.2018.01.080</pub-id>
<pub-id pub-id-type="pmid">29478856</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Gwilliams</surname><given-names>L</given-names></name>, <name><surname>King</surname><given-names>J-R</given-names></name>, <name><surname>Marantz</surname><given-names>A</given-names></name>, <name><surname>Poeppel</surname><given-names>D</given-names></name>. <article-title>Neural dynamics of phoneme sequences reveal position-invariant code for content and order</article-title>. <source>Nat Commun</source>. <year>2022</year>;<volume>13</volume>(<issue>1</issue>):<fpage>6606</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41467-022-34326-1</pub-id>
<pub-id pub-id-type="pmid">36329058</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Zion Golumbic</surname><given-names>EM</given-names></name>, <name><surname>Ding</surname><given-names>N</given-names></name>, <name><surname>Bickel</surname><given-names>S</given-names></name>, <name><surname>Lakatos</surname><given-names>P</given-names></name>, <name><surname>Schevon</surname><given-names>CA</given-names></name>, <name><surname>McKhann</surname><given-names>GM</given-names></name>, <etal>et al</etal>. <article-title>Mechanisms underlying selective neuronal tracking of attended speech at a &#x0201c;cocktail party&#x0201d;</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>77</volume>(<issue>5</issue>):<fpage>980</fpage>&#x02013;<lpage>91</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.037</pub-id>
<pub-id pub-id-type="pmid">23473326</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Cheung</surname><given-names>C</given-names></name>, <name><surname>Hamiton</surname><given-names>LS</given-names></name>, <name><surname>Johnson</surname><given-names>K</given-names></name>, <name><surname>Chang</surname><given-names>EF</given-names></name>. <article-title>The auditory representation of speech sounds in human motor cortex</article-title>. <source>Elife</source>. <year>2016</year>;<volume>5</volume>:e12577. <comment>doi: </comment><pub-id pub-id-type="doi">10.7554/eLife.12577</pub-id>
<pub-id pub-id-type="pmid">26943778</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref009"><label>9</label><mixed-citation publication-type="book"><name><surname>Florescu</surname><given-names>I</given-names></name>. <article-title>Probability and stochastic processes</article-title>. <source>Probability and stochastic processes</source>. <publisher-name>Wiley</publisher-name>. <year>2014</year>. p. <fpage>298</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="pone.0323276.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Campbell</surname><given-names>J</given-names></name>, <name><surname>Bower</surname><given-names>E</given-names></name>, <name><surname>Dwyer</surname><given-names>SJ</given-names></name>, <name><surname>Lago</surname><given-names>GV</given-names></name>. <article-title>On the sufficiency of autocorrelation functions as EEG descriptors</article-title>. <source>IEEE Trans Biomed Eng</source>. <year>1967</year>;BME-14(<issue>1</issue>):<fpage>49</fpage>&#x02013;<lpage>52</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tbme.1967.4502461</pub-id></mixed-citation></ref><ref id="pone.0323276.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Elul</surname><given-names>R</given-names></name>. <article-title>Gaussian behavior of the electroencephalogram: changes during performance of mental task</article-title>. <source>Science</source>. <year>1969</year>;<volume>164</volume>(<issue>3877</issue>):<fpage>328</fpage>&#x02013;<lpage>31</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.164.3877.328</pub-id>
<pub-id pub-id-type="pmid">5776646</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Kawabata</surname><given-names>N</given-names></name>. <article-title>Test of statistical stability of the electroencephalogram</article-title>. <source>Biol Cybern</source>. <year>1976</year>;<volume>22</volume>(<issue>4</issue>):<fpage>235</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/BF00365089</pub-id>
<pub-id pub-id-type="pmid">953081</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Cohen</surname><given-names>BA</given-names></name>, <name><surname>Sances</surname><given-names>A</given-names><suffix>Jr</suffix></name>. <article-title>Stationarity of the human electroendephalogram</article-title>. <source>Med Biol Eng Comput</source>. <year>1977</year>;<volume>15</volume>(<issue>5</issue>):<fpage>513</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/BF02442278</pub-id>
<pub-id pub-id-type="pmid">199805</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Gonen</surname><given-names>FF</given-names></name>, <name><surname>Tcheslavski</surname><given-names>GV</given-names></name>. <article-title>Techniques to assess stationarity and gaussianity of EEG: an overview</article-title>. <source>Int J Bioautomation</source>. <year>2012</year>;<volume>16</volume>(<issue>2</issue>):<fpage>135</fpage>.</mixed-citation></ref><ref id="pone.0323276.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Kipi&#x00144;ski</surname><given-names>L</given-names></name>, <name><surname>K&#x000f6;nig</surname><given-names>R</given-names></name>, <name><surname>Sielu&#x0017c;ycki</surname><given-names>C</given-names></name>, <name><surname>Kordecki</surname><given-names>W</given-names></name>. <article-title>Application of modern tests for stationarity to single-trial MEG data: transferring powerful statistical tools from econometrics to neuroscience</article-title>. <source>Biol Cybern</source>. <year>2011</year>;<volume>105</volume>(3&#x02013;4):<fpage>183</fpage>&#x02013;<lpage>95</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00422-011-0456-4</pub-id>
<pub-id pub-id-type="pmid">22095173</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>He</surname><given-names>BJ</given-names></name>. <article-title>Scale-free brain activity: past, present, and future</article-title>. <source>Trends Cogn Sci</source>. <year>2014</year>;<volume>18</volume>(<issue>9</issue>):<fpage>480</fpage>&#x02013;<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.tics.2014.04.003</pub-id>
<pub-id pub-id-type="pmid">24788139</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Voytek</surname><given-names>B</given-names></name>, <name><surname>Kramer</surname><given-names>MA</given-names></name>, <name><surname>Case</surname><given-names>J</given-names></name>, <name><surname>Lepage</surname><given-names>KQ</given-names></name>, <name><surname>Tempesta</surname><given-names>ZR</given-names></name>, <name><surname>Knight</surname><given-names>RT</given-names></name>, <etal>et al</etal>. <article-title>Age-related changes in 1/f neural electrophysiological noise</article-title>. <source>J Neurosci</source>. <year>2015</year>;<volume>35</volume>(<issue>38</issue>):<fpage>13257</fpage>&#x02013;<lpage>65</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2332-14.2015</pub-id>
<pub-id pub-id-type="pmid">26400953</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Meisel</surname><given-names>C</given-names></name>, <name><surname>Bailey</surname><given-names>K</given-names></name>, <name><surname>Achermann</surname><given-names>P</given-names></name>, <name><surname>Plenz</surname><given-names>D</given-names></name>. <article-title>Decline of long-range temporal correlations in the human brain during sustained wakefulness</article-title>. <source>Sci Rep</source>. <year>2017</year>;<volume>7</volume>(<issue>1</issue>):<fpage>11825</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-017-12140-w</pub-id>
<pub-id pub-id-type="pmid">28928479</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Keshner</surname><given-names>MS</given-names></name>. <article-title>1/f noise</article-title>. <source>Proc IEEE</source>. <year>1982</year>;<volume>70</volume>(<issue>3</issue>):<fpage>212</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/proc.1982.12282</pub-id></mixed-citation></ref><ref id="pone.0323276.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>West</surname><given-names>BJ</given-names></name>, <name><surname>Shlesinger</surname><given-names>M</given-names></name>. <article-title>The noise in natural phenomena</article-title>. <source>Am Sci</source>. <year>1990</year>;<volume>78</volume>(<issue>1</issue>):<fpage>40</fpage>&#x02013;<lpage>5</lpage>.</mixed-citation></ref><ref id="pone.0323276.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Luessi</surname><given-names>M</given-names></name>, <name><surname>Larson</surname><given-names>E</given-names></name>, <name><surname>Engemann</surname><given-names>D</given-names></name>, <name><surname>Strohmeier</surname><given-names>D</given-names></name>, <name><surname>Brodbeck</surname><given-names>C</given-names></name>. <article-title>Meg and eeg data analysis with mne-python</article-title>. <source>Front Neurosci</source>. <year>2013</year>;<volume>7</volume>:<fpage>70133</fpage>.</mixed-citation></ref><ref id="pone.0323276.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Bigdely-Shamlo</surname><given-names>N</given-names></name>, <name><surname>Mullen</surname><given-names>T</given-names></name>, <name><surname>Kothe</surname><given-names>C</given-names></name>, <name><surname>Su</surname><given-names>K-M</given-names></name>, <name><surname>Robbins</surname><given-names>KA</given-names></name>. <article-title>The PREP pipeline: standardized preprocessing for large-scale EEG analysis</article-title>. <source>Front Neuroinform</source>. <year>2015</year>;<volume>9</volume>:<fpage>16</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fninf.2015.00016</pub-id>
<pub-id pub-id-type="pmid">26150785</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Glasberg</surname><given-names>BR</given-names></name>, <name><surname>Moore</surname><given-names>BC</given-names></name>. <article-title>Derivation of auditory filter shapes from notched-noise data</article-title>. <source>Hear Res</source>. <year>1990</year>;<volume>47</volume>(1&#x02013;2):<fpage>103</fpage>&#x02013;<lpage>38</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0378-5955(90)90170-t</pub-id>
<pub-id pub-id-type="pmid">2228789</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Sch&#x000f6;nwiesner</surname><given-names>M</given-names></name>, <name><surname>Bialas</surname><given-names>O</given-names></name>. <article-title>s(ound)lab: an easy to learn Python package for designing and running psychoacoustic experiments</article-title>. <source>JOSS</source>. <year>2021</year>;<volume>6</volume>(<issue>62</issue>):<fpage>3284</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.21105/joss.03284</pub-id></mixed-citation></ref><ref id="pone.0323276.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Bialas</surname><given-names>O</given-names></name>, <name><surname>Dou</surname><given-names>J</given-names></name>, <name><surname>Lalor</surname><given-names>EC</given-names></name>. <article-title>mTRFpy: a Python package for temporal response function analysis</article-title>. <source>JOSS</source>. <year>2023</year>;<volume>8</volume>(<issue>89</issue>):<fpage>5657</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.21105/joss.05657</pub-id></mixed-citation></ref><ref id="pone.0323276.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Anisheh</surname><given-names>SM</given-names></name>, <name><surname>Hassanpour</surname><given-names>H</given-names></name>. <article-title>Designing an adaptive approach for segmenting non-stationary signals</article-title>. <source>Int J Electron</source>. <year>2011</year>;<volume>98</volume>(<issue>8</issue>):<fpage>1091</fpage>&#x02013;<lpage>102</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/00207217.2011.560559</pub-id></mixed-citation></ref><ref id="pone.0323276.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Azami</surname><given-names>H</given-names></name>, <name><surname>Hassanpour</surname><given-names>H</given-names></name>, <name><surname>Escudero</surname><given-names>J</given-names></name>, <name><surname>Sanei</surname><given-names>S</given-names></name>. <article-title>An intelligent approach for variable size segmentation of non-stationary signals</article-title>. <source>J Adv Res</source>. <year>2015</year>;<volume>6</volume>(<issue>5</issue>):<fpage>687</fpage>&#x02013;<lpage>98</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jare.2014.03.004</pub-id>
<pub-id pub-id-type="pmid">26425359</pub-id>
</mixed-citation></ref><ref id="pone.0323276.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Haddad</surname><given-names>AE</given-names></name>, <name><surname>Najafizadeh</surname><given-names>L</given-names></name>. <article-title>Source-informed segmentation: a data-driven approach for the temporal segmentation of EEG</article-title>. <source>IEEE Trans Biomed Eng</source>. <year>2019</year>;<volume>66</volume>(<issue>5</issue>):<fpage>1429</fpage>&#x02013;<lpage>46</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2018.2874167</pub-id>
<pub-id pub-id-type="pmid">30295610</pub-id>
</mixed-citation></ref></ref-list></back><sub-article article-type="author-comment" id="pone.0323276.r001" specific-use="rebutted-decision-letter-unavailable"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r001</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">21 Sep 2024</named-content>
</p></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0323276.r002" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r002</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Li</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Li Yang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Li Yang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323276" id="rel-obj002" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">8 Oct 2024</named-content>
</p><p><!--<div>-->PONE-D-24-38627<!--</div>--><!--<div>-->Appropriate data segmentation improves speech encoding models<!--</div>--><!--<div>-->PLOS ONE</p><p>Dear Dr. Bialas,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>==============================</p><p>Editor Comments:<!--</div>--><!--<div>-->Thanks for submitting your work to PLOS ONE. Your manuscript has now been assessed by our editorial team and external peer experts. While they found it interesting, you will see that they have raised many serious problems and are advising that you should revise your manuscript thoroughly. At the same time, please submit the point-by-point responses to reviewers' comments. If you are prepared to undertake the work required, I would be pleased to reconsider my decision. Please note that this revision decision does not assure the acceptance of your work. Thanks for the opportunity to consider your work.<!--</div>--><!--<div>-->&#x000a0;<!--</div>--><!--<div>-->Editor Reviews:<!--</div>--><!--<div>-->1.Please further re-evaluate and clarify the rationality of the methodology, which is linked to the result reliability and scientific rigor.<!--</div>--><!--<div>-->2.Please consider to comprehensively propose the strengths and limitations of your work.<!--</div>--><!--<div>-->3. Please consider to use structured abstract.<!--</div>--><!--<div>-->4. I did not see conclusion section. Please add this part.<!--</div>--><!--<div>-->5. Please consider to add some tables to summarize the core data of your analysis and results. I think it will be clearer for readers to grasp your findings.<!--</div>--><!--<div>-->6. Minor suggestion: improve the quality of your figures (both in resolution and contents).<!--</div>--><!--<div>-->==============================</p><p>Please submit your revised manuscript by Nov 22 2024 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!--</div>--></p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Li Yang, M.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal Requirements:</p><p>When submitting your revision, we need you to address these additional requirements.</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at</p><p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and</p><p>
<ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
</p><p>2. Please update your submission to use the PLOS LaTeX template. The template and more information on our requirements for LaTeX submissions can be found at <ext-link xlink:href="http://journals.plos.org/plosone/s/latex" ext-link-type="uri">http://journals.plos.org/plosone/s/latex</ext-link>.</p><p>3. Please include your full ethics statement in the &#x02018;Methods&#x02019; section of your manuscript file. In your statement, please include the full name of the IRB or ethics committee who approved or waived your study, as well as whether or not you obtained informed written or verbal consent. If consent was waived for your study, please include this information in your statement as well.</p><p>
<bold>Additional Editor Comments:</bold>
</p><p>Please address the concerns proposed by&#x000a0;Editor Reviews.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>
<bold>Reviewers' comments:</bold>
</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #1:&#x000a0;Partly</p><p>Reviewer #2:&#x000a0;Partly</p><p>**********</p><p><!--<font color="black">-->2. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;I Don't Know</p><p>**********</p><p><!--<font color="black">-->3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p><!--<font color="black">-->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p>Reviewer #1:&#x000a0;Thank you for the opportunity to review this manuscript, which presents an important exploration of EEG responses to speech segmentation. I appreciate the authors&#x02019; efforts in conducting this research and believe that with some revisions, the paper has the potential to make a valuable contribution to the field. However, it is important that clear rationales for their methodological decisions are provided, as this will enhance the transparency and reproducibility of the findings. Detailed explanations of participant selection criteria, the choice of EEG data, and the specific segment lengths used would strengthen the overall clarity and impact of the manuscript. Additionally, the figures are almost publication-ready; they just need a few clarifying details.</p><p>I have attached a document with more detailed comments for your consideration. I look forward to reading future installments of this study.</p><p>Reviewer #2:&#x000a0;In this paper the authors have used simulations and EEG recordings to investigate how non-stationarities affect temporal response functions (TRF) models for continuous speech processing. Based on the findings it is suggested that non-stationarities may reduce the performance of TRF models. However, it can be partially compensated by dividing the data into shorter segments that approximate stationary.</p><p>This is a topic of interest. For authors of this paper, I have the following comments.</p><p>1. The expression description of the paper needs improvement. The introduction section should be improved by clarifying the similarities and differences between the related work and the proposed method. In current form the contribution of the paper seems marginal. Authors should emphasize and clearly describe their contribution, preferably in the form of bullets.</p><p>2. Usually, a system used for evaluation purpose has a very careful design about the front-end sensors. However, no physical implementation and parameter detail is provided in this work.</p><p>3. The used materials and methods should be clearly described and clearly specify the implementation details/parameters of all methods.</p><p>4. Provide a URL of the studied dataset or share the studied dataset in csv format for an ease of review process.</p><p>5. Properly support claims in the discussion section by providing quantitative findings. Also present the performance comparison with counterparts, preferably in a tabular form while focusing on the main methods and major findings.</p><p>**********</p><p><!--<font color="black">-->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p><supplementary-material id="pone.0323276.s001" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">PONE-D-24-38627_comments.pdf</named-content></p></caption><media xlink:href="pone.0323276.s001.pdf"/></supplementary-material></body></sub-article><sub-article article-type="author-comment" id="pone.0323276.r003"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r003</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323276" id="rel-obj003" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">12 Jan 2025</named-content>
</p><p>Dear Editor and Reviewers,</p><p>Thank you for providing us with the opportunity to refine and clarify our manuscript. In the following, we will address the points you have raised. Where it is appropriate, we will refer to edited lines in the revised manuscripts (edits are marked red).</p><p>Comments by the Editor</p><p>1.Please further re-evaluate and clarify the rationality of the methodology, which is linked to the result reliability and scientific rigor.</p><p>A: We focus on TRF models because they are the most commonly used kind of encoding model and they are comparatively small and simple (see second paragraph of the introduction). We analyze simulated data because they allow us to test our hypothesis under a known ground truth and we use EEG recordings to test our hypothesis on real data (see &#x0201d;Methods&#x0201d; part of the abstract and the first paragraph in the &#x0201d;Methods&#x0201d; section). We chose this data set because it has proven to be well suited for TRF analyses and the individual recordings are long enough to test our models across a wide range of segment durations (see section &#x0201d;Modeling EEG responses to speech&#x0201d;).</p><p>2.Please consider to comprehensively propose the strengths and limitations of your work.</p><p>A: The strength of our approach lies in the combination of simulations and EEG analysis. This way we can test our hypothesis theoretically, under a known ground truth and empirically, on actual neural data. We added these considerations to the abstract as well as the discussion section (paragraph 4). Our results show that segmentation is an easy method for improving the accuracy of speech encoding models. This is relevant because these models are widely used in the neuroscience community, potentially including medical diagnostics.</p><p>3. Please consider to use structured abstract.</p><p>A: We changed the abstract to use a structured format.</p><p>4. I did not see conclusion section. Please add this part.</p><p>A: We have added a Conclusion section.</p><p>5. Please consider to add some tables to summarize the core data of your analysis and results. I think it will be clearer for readers to grasp your findings.</p><p>A: We could add table that contains the average model accuracies for each model and all segment durations. However, we think that the absolute model accuracy is not relevant here, since the key aspect is the relationship between model accuracy which is depicted in Fig. 3.</p><p>6. Minor suggestion: improve the quality of your figures (both in resolution and contents).</p><p>A: We re-created the figures with higher resolution.</p><p>Comments by Reviewer I</p><p>30-31: While read speech may approximate &#x0201c;real speech,&#x0201d; it still lacks many features of truly spontaneous speech. To substantiate this claim, consider citing the following studies: Howell, P., &#x00026; Kadi-Hanifi (1991). Comparison of prosodic properties betweewn read and sponta- neous speech material. Speech Communication, 10(2), 163-169. <ext-link xlink:href="https://doi.org/10.1016/0167-" ext-link-type="uri">https://doi.org/10.1016/0167-</ext-link> 6393(91)90039-V Mehta, G., &#x00026; Cutler, A. (1988). Detection of target phonemes in sponta- neous and read speech. Language and Speech, 31( Pt 2), 135-156. <ext-link xlink:href="https://doi.org/10.1177/0023830988031002" ext-link-type="uri">https://doi.org/10.1177/0023830988031002</ext-link></p><p>A: Many thanks to the reviewer for pointing us to this interesting literature. We fully agree that read speech may deviate from real conversational speech in important ways. However, in the case of our study, we are not making any argument about how speech is processed in the brain. Rather, we are investigating how the encoding models typically used in the speech literature are affected by the statistical properties of the neural recordings. Nonetheless, we have added a small caveat to the discussion on the nature of the speech materials we used.</p><p>47-49: Please clarify the definition of &#x0201c;estimand&#x0201d; to assist readers who may be unfamiliar with the term.</p><p>A: Rather than providing a definition, we added an example to clarify that predictor refers to speech features and estimand refers to the neural recording</p><p>121-122: Upon reviewing Di Liberto et al. (2015) and Broderick et al. (2018), neither article specifies the actual material read by the speaker. If the original text is unavailable, could details regarding the narrative style or genre be provided? Understanding the type of text used could impact the interpretation of results.</p><p>A: The story is &#x0201d;The Old Man and the Sea&#x0201d;. We added this information to the subsection &#x0201d;Modeling EEG responses to speech&#x0201d;.</p><p>What criteria were used for selecting the 19 participants in this study?</p><p>A: The subjects were all healthy neurotypical adults, we added this information to the subsection &#x0201d;Modeling EEG responses to speech&#x0201d;. There were no other special criteria applied. The data were collected for Di LIberto et al., 2015 and Broderick et al., 2018 and are described in more detail in those studies.</p><p>Why was this particular EEG database chosen for the analysis?</p><p>A: We chose this data set because it has proven to be well suited for TRF analyses and the individual recordings are long enough to test our models across a wide range of segment durations.</p><p>131&#x02013;136: The inclusion of the number of spectral bands as a variable is a valuable addition; however, they are infrequently referenced in the results and discussion sections. It would be beneficial to discuss their impact on the findings.</p><p>A: We found that the accuracy improvement from segmentation was larger for the 16-band model compared to the 8- and 1-band models. This could be because the increased number of parameters increases the extent to which the model (over-)fits the non-stationary trends in the data. We added this to our discussion section.</p><p>143-145: Please clarify the source of the &#x0201c;50 minutes&#x0201d; of data, as the analysis seems to involve only 19 subjects, each contributing between 170 and 200 seconds of data.</p><p>A: Thank you for pointing out the ambiguity here. There are 50 minutes of recordings for every subject, we rephrased the paragraph to make this clearer.</p><p>143-145: What was the rationale for choosing these specific segment lengths? Given that Gonen and Tcheslavski (2012) identified a critical duration of 2 to 25 seconds, why not test segments up to 25 seconds using the current dataset?</p><p>A: The durations were chosen so that the 50 minutes of data could be divided evenly into segments for every duration (we added this expla- 2 nation to the manuscript). The studies that identified critical durations between 2 and 25 seconds tested the randomness or gaussianity of the recording. Because our approach was different we chose to include the widest possible range of values to minimize a priori assumptions about the relevant timescale. However, our results seem to agree with the mentioned studies since there is little difference between using segments that are 20 seconds and longer. We added a sentence to both the Methods and Discussion sections to relate our segment durations to the previous studies</p><p>145-146: An exclusion criterion of 0.01 for model accuracy appears unusually low. Please provide justification for this threshold.</p><p>A: It is important to note that we are attempting to model unaveraged EEG - most of which is unrelated to the speech stimulus. It is quite typical for such models to produce prediction accuracy scores of r &#x000a1; 0.1. The point of the exclusion criterion was to reject participants where the model could not explain any variance in the data. For the one subject that was rejected due to the criterion, accuracy approximated 0 for all models (average r was 0.001). We added this latter information to the paragraph</p><p>191-193: &#x0201c;On average&#x0201d; is stated twice in this sentence.</p><p>A: We corrected the sentence</p><p>198: It might be beyond the scope of this paper, but was any post hoc analysis conducted to determine the factors driving variability across subjects?</p><p>A: This was to motivation for the linear regression that relates the model improvement from segmentation to the baseline prediction accuracy. The fact that the improvement was larger for subjects where the baseline accuracy was low suggests that the non-stationarity is not related to stimulus driven activity. However, since interindividual variability in EEG recordings is another area of research, we do consider further investigations into this beyond the scope of this paper indeed.</p><p>202-204: The phrase, &#x0201c;What&#x02019;s more, there was a negative relationship (linear regression, r = -0.62, p = 0.006) between the relative change in accuracy due to segmentation. . . &#x0201d; needs clarification. Specifically, which segmentation is referenced here: the average of 10 and 120 seconds, the average across all segment durations, or the optimal segmentation for each sub- ject?</p><p>A: This is indeed ambiguously phrased. For every subject we took the relative difference in accuracy between 10 and 120 second segments (i.e. the improvement due to segmentation) and the peak prediction accuracy (i.e. how well the subjects EEG could be predicted with the best combination of parameters). These two estimates were negatively correlated across subjects. We rewrote the paragraph to make it clearer.</p><p>206&#x02013;208: Is there any commentary regarding the identified outlier?</p><p>A: It&#x02019;s possible that the recordings of the outlier were unusually stationary. As our simulations showed, segmentation does not improve but instead reduces model accuracy when the data are stationary. We added this commentary to the paragraph.</p><p>209: Consider using &#x0201c;improve&#x0201d; or &#x0201c;increase&#x0201d; instead of &#x0201c;affect&#x0201d; to more powerfully convey the direction of the change.</p><p>A: We changed the wording of the sentence.</p><p>Figures Although it is common in the field, including a definition for &#x0201c;a.u.&#x0201d; would be helpful for readers.</p><p>A: We added an explanation to the caption of each figure.</p><p>3 Please correct the x-axis label from &#x0201c;segmend&#x0201d; to &#x0201c;segment.&#x0201d;</p><p>A: We fixed the typo.</p><p>Figure 3a &#x00026; c: Although both y-axes are represented in a.u., are they on the same scale? For instance, does a 0.1 change in accuracy in panel c correspond to a 0.1 change in accuracy in panel a? If so, it would be helpful to specify this in the figure legend.</p><p>A: No, they are not the same. Figure 3a shows accuracy normalized to each model&#x02019;s maximum and 3c shows the relative difference between 10s and 120s segments. We modified the caption to make this more explicit.</p><p>Figure 3c: Regarding the &#x0201c;Change in accuracy,&#x0201d; which segmentation level was used to ob- tain these values?</p><p>A: This is the relative difference between 10s and 120s segments. Because this information would have been too long to put it in the axis title, we put it in the figure caption</p><p>While the Ethics statement is appreciated, it is somewhat confusing as neither author is affiliated with Trinity College Dublin. It would be helpful to include a sentence clarifying that the EEG data originated from Di Liberto et al. (2015) or Broderick et al. (2018) at that institution, allowing readers to understand the context without needing to consult those studies.</p><p>A: Personally, we think that an ethics section is not strictly necessary since we only used publicly available data. However, this seems to be a requirement by the journal. Your suggestion is a good way of making the relationship between our study and the data more explicit.</p><p>Comments by Reviewer2</p><p>1. The expression description of the paper needs improvement. The introduction section should be improved by clarifying the similarities and differences between the related work and the proposed method. In current form the contribution of the paper seems marginal. Authors should emphasize and clearly describe their contribution, preferably in the form of bullets.</p><p>A: There are two ways in which the paper contributes to the field: 1. We probe the non- stationarity of EEG recordings by quantifying the improvement of model accuracy due to data segmentation and by using simulations to show that these improvements can be explained by non- stationary trends in the data 2. We show that the common practice of fitting speech encoding models to long, continuous recordings is suboptimal and urge researchers to adopt segmentation as part of their model fitting process. Regarding the differences between ours and related work, see the last paragraph of the subsection &#x0201d;Are neural recordings stationary?&#x0201d;. We also hope that the added structured abstract and conclusion sections add some clarity with respect of the points you raised.</p><p>2. Usually, a system used for evaluation purpose has a very careful design about the front-end sensors. However, no physical implementation and parameter detail is provided in this work.</p><p>A: The data we present in this manuscript came from a publicly available EEG dataset. We note that the data are described in two previously published papers (Di Liberto et al., 2015 Broderick et al., 2018). Those papers describe the data collection hardware in detail.</p><p>3. The used materials and methods should be clearly described and clearly specify the imple- mentation details/parameters of all methods.</p><p>A: We have attempted to be clear about the simulation and EEG analysis parameters throughout the methods and results sections. Also, as mentioned be- low, we have made all of our code available so that others can see precisely what parameters we used for each step. 4</p><p>4. Provide a URL of the studied dataset or share the studied dataset in csv format for an ease of review process.</p><p>A: The data is hosted on OpenNeuro and can be obtained, together with all of our code, from a GitHub repository. Both, repositories are linked in the &#x0201d;Data availability&#x0201d; subsection in &#x0201d;Materials and methods&#x0201d;. The data are shared in the standardized BIDS format &#x02014; csv would be a very inefficient way of sharing a large EEG data set.</p><p>5. Properly support claims in the discussion section by providing quantitative findings. Also present the performance comparison with counterparts, preferably in a tabular form while focusing on the main methods and major findings.</p><p>A: We have made extensive revisions to the results and discussion sections and hope that both are now coherent in terms of the quantitative findings and the discussion of same.</p><p>Sincerely, Ole Bialas and Edmund Lalor</p><supplementary-material id="pone.0323276.s002" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">rebuttal.pdf</named-content></p></caption><media xlink:href="pone.0323276.s002.pdf"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0323276.r004" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r004</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Li</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Li Yang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Li Yang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323276" id="rel-obj004" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">3 Feb 2025</named-content>
</p><p><!--<div>-->PONE-D-24-38627R1<!--</div>--><!--<div>-->Appropriate data segmentation improves speech encoding models<!--</div>--><!--<div>-->PLOS ONE</p><p>Dear Dr. Bialas,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.<!--</div>--><!--<div>--></p><p>Please submit your revised manuscript by Mar 20 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!--</div>--></p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p><!--<div>-->If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Li Yang, M.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>
<bold>Journal Requirements:</bold>
</p><p>Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article&#x02019;s retracted status in the References list and also include a citation and full reference for the retraction notice.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.<!--</font>--></p><p>Reviewer #1:&#x000a0;All comments have been addressed</p><p>**********</p><p><!--<font color="black">-->2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->3. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #1:&#x000a0;N/A</p><p>**********</p><p><!--<font color="black">-->4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p><bold>Reviewer #1:</bold>&#x000a0;Thank you for the thorough and thoughtful revisions. I appreciated the explicit outlining of how this study is distinct and contributes to the field, as well as the clear statement of the hypothesis. The explanation for the low data exclusion threshold and discussion of the outlier were also helpful. The concluding paragraph is a welcome addition and effectively summarized the study. Additionally, I appreciated the clarification regarding the incompatibility for comparing Figure 3 and a.u.s. Thank you for specifying that the reading passage was &#x0201c;The Old Man and the Sea.&#x0201d; For unfamiliar readers, could the authorship or a brief description of the story be added? This is particularly relevant as different emotional states may yield different EEG results. Good luck with this line of research!</p><p>**********</p><p><!--<font color="black">-->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #1:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.<!--</div>--></p></body></sub-article><sub-article article-type="author-comment" id="pone.0323276.r005"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r005</article-id><title-group><article-title>Author response to Decision Letter 2</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323276" id="rel-obj005" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">12 Mar 2025</named-content>
</p><p>Dear Reviewer 1,</p><p>Thank you for your comments. In your response, you requested:</p><p>"For unfamiliar readers, could the authorship or a brief description of the story be added?"</p><p>We have added two sentences to the section "Modeling EEG responses to speech" that describe the content and tone of the narration to allow the reader to assess the emotional impact.</p><p>Sincerely,</p><p>Ole Bialas and Edmund C. Lalor</p><supplementary-material id="pone.0323276.s003" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">rebuttal2.pdf</named-content></p></caption><media xlink:href="pone.0323276.s003.pdf"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0323276.r006" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r006</article-id><title-group><article-title>Decision Letter 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Li</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Li Yang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Li Yang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323276" id="rel-obj006" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">18 Mar 2025</named-content>
</p><p><!--<div>-->PONE-D-24-38627R2<!--</div>--><!--<div>-->Appropriate data segmentation improves speech encoding models<!--</div>--><!--<div>-->PLOS ONE</p><p>Dear Dr. Bialas,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by May 02 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!--</div>--></p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p><!--<div>-->If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Li Yang, M.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>
<bold>Journal Requirements:</bold>
</p><p>Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article&#x02019;s retracted status in the References list and also include a citation and full reference for the retraction notice.</p><p>
<bold>Additional Editor Comments:</bold>
</p><p>Thanks for submitting your revised paper to PLOS ONE, and I am pleased to inform you that your paper has now been approved by the previous peer expert. But before I can recommend the final editorial decision to our journal office, some minor issues need your attention.</p><p>1) Revise your title and specify the detailed study type or brief summary on the study contents.&#x000a0;For example: Appropriate data segmentation improves speech encoding models: A comprehensive analysis of ....., or anything you think suitable.</p><p>2) Please consider to use structured abstract including background, objective, methods, results, and conclusion, and each of them shoule be presented as a separate paragraph, instead of just putting them together.</p><p>3) I note that there is no any table in the paper. Please consider to add some tables to summarize your core data, and it is better for readers to follow your main findings.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.<!--</font>--></p><p>Reviewer #1:&#x000a0;All comments have been addressed</p><p>**********</p><p><!--<font color="black">-->2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->3. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p><bold>Reviewer #1:&#x000a0;</bold>Thank you for the opportunity to review the newly revised manuscript. I appreciate the description of the read speech, and my comments have all been successfully addressed. I have no further questions and wish the authors the best of luck with this project.</p><p>**********</p><p><!--<font color="black">-->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #1:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.<!--</div>--></p></body></sub-article><sub-article article-type="author-comment" id="pone.0323276.r007"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r007</article-id><title-group><article-title>Author response to Decision Letter 3</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323276" id="rel-obj007" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>3</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">25 Mar 2025</named-content>
</p><p>Dear Dr. Yang,</p><p>Thank you for your comments. We have added a summary to the title, it now reads:</p><p>Appropriate data segmentation improves speech encoding models: Analysis and simulation of</p><p>electrophysiological recordings. We also replaced &#x0201d;Introduction&#x0201d; with &#x0201d;Background&#x0201d; in the structured abstract</p><p>and put each point on a separate paragraph. In your last point, you requested that we</p><p>add a table to our publication to make it easier for the reader to follow the main findings.</p><p>We believe that adding a table that compares the accuracy of the different encoding models with</p><p>the different segment durations would likely be confusing and detracting rather than beneficial to</p><p>readers. That is because the key point is the qualitative change in model accuracy that results from</p><p>a change in segment duration, not the absolute values. We believe that the figures, combined with</p><p>the text, illustrate this point well.</p><p>Sincerely,</p><p>Ole Bialas and Edmund Lalor</p><supplementary-material id="pone.0323276.s004" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">rebuttal3.pdf</named-content></p></caption><media xlink:href="pone.0323276.s004.pdf"/></supplementary-material></body></sub-article><sub-article article-type="editor-report" id="pone.0323276.r008" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r008</article-id><title-group><article-title>Decision Letter 3</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Li</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Li Yang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Li Yang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323276" id="rel-obj008" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>3</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">26 Mar 2025</named-content>
</p><p><!--<div>-->PONE-D-24-38627R3<!--</div>--><!--<div>-->Appropriate Data Segmentation Improves Speech Encoding Models: Analysis and Simulation of Electrophysiological Recordings<!--</div>--><!--<div>-->PLOS ONE</p><p>Dear Dr. Bialas,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by May 10 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!--</div>--></p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p><!--<div>-->If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Li Yang, M.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal Requirements:</p><p>Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article&#x02019;s retracted status in the References list and also include a citation and full reference for the retraction notice.</p><p>
<bold>Additional Editor Comments:</bold>
</p><p>Thanks for your response to my concerns. However, the abstract still can not meet the standard for publication. First, each section should be presented as a separate paragraph instead of just putting them together. Second, you should simplify the abstract since there are too many words included, such as background, methods, conclusion. Please just present the core points. Third, you should annotate the abbreviations in the abtract, such as&#x000a0;</p><p>
<bold>Your current abstract in the manuscript:</bold>
</p><p><bold>Background:</bold> In recent decades, studies modeling the neural processing of</p><p>continuous, naturalistic, speech provided new insights into how speech and</p><p>language are represented in the brain. However, the linear encoder models</p><p>commonly used in such studies assume that the underlying data are</p><p>stationary, varying to a fixed degree around a constant mean.</p><p>&#x000a0; &#x000a0; Long, continuous, neural recordings may violate this assumption leading</p><p>to impaired model performance.<bold> Objective:</bold> Our objective was to examine</p><p>the effect of non-stationary trends in continuous neural recordings on the</p><p>performance of linear speech encoding models.</p><p><bold>&#x000a0; &#x000a0; Methods:</bold> We used temporal response functions (TRFs), to predict</p><p>continuous neural responses to speech while splitting the data into segments</p><p>of varying length, prior to model fitting. Our Hypothesis was that, if the</p><p>data were non-stationary, segmentation should improve model performance</p><p>by making each segment approximately stationary.</p><p>&#x000a0; &#x000a0; We used generative models for simulate and predict stationary and</p><p>non-stationary neural data, testing our hypothesis under a known ground</p><p>truth. We then used encoding models to predict the EEG recordings of</p><p>participants who listened to a narrated story, testing our hypothesis on</p><p>actual neural data. <bold>Results:</bold> Simulations showed that, for stationary data,</p><p>increasing segmentation steadily decreased model performance. For</p><p>non-stationary data however, segmentation initially improved model</p><p>performance. Modeling of EEG recordings yielded similar results: segments</p><p>of intermediate length (5-15 s) led to improved model performance</p><p>compared to very short (1-2 s) and very long (30-120 s) segments.</p><p>&#x000a0; &#x000a0; <bold>Conclusions:</bold> We showed that data segmentation improves the</p><p>performance of encoding models in predicting both simulated and real</p><p>neural data and that this can be explained by the fact that shorter segments</p><p>approximate stationarity more closely. We thus conclude that the common</p><p>practice of applying encoding models to long continuous segments of data is</p><p>suboptimal and implore researchers to segment their data prior to model</p><p>fitting.</p><p>
<bold>My recommend abstract format:</bold>
</p><p><bold>Background:&#x000a0;</bold>In recent decades, studies modeling the neural processing of continuous, naturalistic, speech provided new insights into how speech and language are represented in the brain. However, the linear encoder models commonly used in such studies assume that the underlying data are stationary, varying to a fixed degree around a constant mean. Long, continuous, neural recordings may violate this assumption leading to impaired model performance.</p><p><bold>Objective:</bold>&#x000a0;We aimed to examine the effect of non-stationary trends in continuous neural recordings on the performance of linear speech encoding models.&#x000a0;</p><p>(This part can be merged into the introduction as well. Remember to delete the 'Objective:' if&#x000a0;you choose to merge.)</p><p><bold>Methods:&#x000a0;</bold>We used temporal response functions (TRFs), to predict continuous neural responses to speech while splitting the data into segments of varying length, prior to model fitting. Our Hypothesis was that, if the data were non-stationary, segmentation should improve model performance by making each segment approximately stationary. We used generative models for simulate and predict stationary and non-stationary neural data, testing our hypothesis under a known ground truth. We then used encoding models to predict the EEG recordings of participants who listened to a narrated story, testing our hypothesis on actual neural data.</p><p><bold>Results:</bold>&#x000a0;Simulations showed that, for stationary data, increasing segmentation steadily decreased model performance. For non-stationary data however, segmentation initially improved model performance. Modeling of EEG recordings yielded similar results: segments of intermediate length (5-15 s) led to improved model performance compared to very short (1-2 s) and very long (30-120 s) segments.</p><p><bold>Conclusions:</bold> Our analysis&#x000a0;showed that data segmentation improves the performance of encoding models in predicting both simulated and real neural data and that this can be explained by the fact that shorter segments approximate stationarity more closely. Thus, the common practice of applying encoding models to long continuous segments of data is suboptimal and implore researchers to segment their data prior to model fitting.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p></body></sub-article><sub-article article-type="author-comment" id="pone.0323276.r009"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r009</article-id><title-group><article-title>Author response to Decision Letter 4</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323276" id="rel-obj009" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>4</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">1 Apr 2025</named-content>
</p><p>Dear Dr. Yang,</p><p>Thank you for providing an example of the desired formatting. We had inserted new paragraphs in the revised abstract, but the PLOS Latex template does not include a blank line after a paragraph. We changed the formatting of the abstract by manually inserting a blank line after every point. We also followed your suggestion to merge the &#x0201d;Objectives" into the &#x0201d;Background&#x0201d; section. We also made some minor changes for conciseness and hope that this meets the standards for publication.</p><p>Sincerely,</p><p>Ole Bialas and Edmund Lalor</p><supplementary-material id="pone.0323276.s005" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">rebuttal4.pdf</named-content></p></caption><media xlink:href="pone.0323276.s005.pdf"/></supplementary-material></body></sub-article><sub-article article-type="editor-report" id="pone.0323276.r010" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r010</article-id><title-group><article-title>Decision Letter 4</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Li</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Li Yang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Li Yang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323276" id="rel-obj010" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>4</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">6 Apr 2025</named-content>
</p><p>Appropriate Data Segmentation Improves Speech Encoding Models: Analysis and Simulation of Electrophysiological Recordings</p><p>PONE-D-24-38627R4</p><p>Dear Dr. Bialas,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">Editorial Manager&#x000ae;</ext-link>&#x000a0;and clicking the &#x02018;Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at authorbilling@plos.org.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>Kind regards,</p><p>Li Yang, M.D.</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>Thanks for the authors' efforts to comprehensively improve your manuscript according to editor's and reviewers' comments. I am pleased to inform you that your paper can be accepted for publication now. Thanks for the chance to assess your interesting and important work. Additionally, many thanks for all the reviewers' precious inputs.</p><p>Reviewers' comments:</p></body></sub-article><sub-article article-type="editor-report" id="pone.0323276.r011" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323276.r011</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Li</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Li Yang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Li Yang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323276" id="rel-obj011" related-article-type="reviewed-article"/></front-stub><body><p>PONE-D-24-38627R4</p><p>PLOS ONE</p><p>Dear Dr. Bialas,</p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p><p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p><p>* All references, tables, and figures are properly cited</p><p>* All relevant supporting information is included in the manuscript submission,</p><p>* There are no issues that prevent the paper from being properly typeset</p><p>You will receive further&#x000a0;instructions from the production team, including instructions on how to review your proof when it&#x000a0;is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p><p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>If we can help with anything else, please email us at customercare@plos.org.</p><p>Thank you for submitting your work to PLOS ONE and supporting open access.</p><p>Kind regards,</p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Dr. Li Yang</p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>