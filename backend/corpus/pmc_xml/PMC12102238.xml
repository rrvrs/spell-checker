<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40410503</article-id><article-id pub-id-type="pmc">PMC12102238</article-id>
<article-id pub-id-type="publisher-id">95045</article-id><article-id pub-id-type="doi">10.1038/s41598-025-95045-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Reduced listening effort with adaptive binaural beamforming in realistic noisy environments</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Valderrama</surname><given-names>Joaquin T.</given-names></name><address><email>jvalderrama@ugr.es</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><name><surname>Mejia</surname><given-names>Jorge</given-names></name><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><name><surname>Wong</surname><given-names>Angela</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Herbert</surname><given-names>Nicholas C.</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Edwards</surname><given-names>Brent</given-names></name><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04njjy449</institution-id><institution-id institution-id-type="GRID">grid.4489.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0263</institution-id><institution>Department of Signal Theory, Telematics and Communications, </institution><institution>University of Granada, </institution></institution-wrap>18014 Granada, Spain </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04njjy449</institution-id><institution-id institution-id-type="GRID">grid.4489.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0263</institution-id><institution>Research Centre for Information and Communications Technologies, </institution><institution>University of Granada, </institution></institution-wrap>18014 Granada, Spain </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02swxtp23</institution-id><institution-id institution-id-type="GRID">grid.419097.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 0643 6737</institution-id><institution>National Acoustic Laboratories, </institution></institution-wrap>Sydney, NSW 2109 Australia </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01sf06y89</institution-id><institution-id institution-id-type="GRID">grid.1004.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2158 5405</institution-id><institution>Department of Linguistics, </institution><institution>Macquarie University, </institution></institution-wrap>Sydney, NSW 2109 Australia </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01sf06y89</institution-id><institution-id institution-id-type="GRID">grid.1004.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2158 5405</institution-id><institution>School of Computing, </institution><institution>Macquarie University, </institution></institution-wrap>Sydney, NSW 2109 Australia </aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/027rr8795</institution-id><institution-id institution-id-type="GRID">grid.437266.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 0613 8617</institution-id><institution>Research &#x00026; Development, Sonova AG, </institution></institution-wrap>8712 St&#x000e4;fa, Switzerland </aff></contrib-group><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><elocation-id>17998</elocation-id><history><date date-type="received"><day>31</day><month>10</month><year>2024</year></date><date date-type="accepted"><day>18</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">This study evaluates the effectiveness of adaptive binaural beamforming in a realistic cafeteria noise environment. The motivation stems from the common challenge faced by hearing aid users in such environments, where communication often demands significant mental effort. The study employed a combination of behavioural, neurophysiological, and self-reported measures to assess speech intelligibility and listening effort. Results showed that the adaptive binaural beamformer improved speech-in-noise intelligibility at signal-to-noise ratios (SNRs) yielding 80% and 95% intelligibility. Additionally, when this technology was enabled, listening effort was reduced across various metrics: faster reaction times on a dual task, decreased pre-stimulus alpha power (8&#x02013;12&#x000a0;Hz), indicating less inhibition was needed, and increased alpha power during the encoding and retention phases, consistent with greater working memory load due to improved intelligibility. Self-reports indicated lower perceived effort in the more challenging SNR condition. The use of realistic background noise enhances the ecological validity of the findings, contributing to a better understanding of how this hearing aid technology performs in real-world listening environments. Overall, the study demonstrates that adaptive binaural beamforming can ease the cognitive burden on users in noisy, everyday environments, thereby enhancing their overall auditory experience.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Listening effort</kwd><kwd>Directional microphones</kwd><kwd>Dual task</kwd><kwd>Reaction time</kwd><kwd>Alpha power</kwd><kwd>Ecological validity</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Auditory system</kwd><kwd>Diagnostic markers</kwd></kwd-group><funding-group><award-group><funding-source><institution>&#x02018;Ram&#x000f3;n y Cajal&#x02019; Fellowship by the Spanish Ministry of Science and Innovation and the European Social Fund Plus</institution></funding-source><award-id>RYC-2022-037875-I</award-id><principal-award-recipient><name><surname>Valderrama</surname><given-names>Joaquin T.</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003921</institution-id><institution>Department of Health and Aged Care, Australian Government</institution></institution-wrap></funding-source></award-group></funding-group><funding-group><award-group><funding-source><institution>Sonova AG</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Hearing aid users frequently report the substantial mental effort required to communicate in noisy environments<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. Prolonged exertion of listening effort can lead to significant fatigue and may result in disengagement from conversations<sup><xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref></sup>. Regular exposure to these challenges increases the risk of developing anxiety due to fears of being misunderstood and may discourage social interaction<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>, leading to isolation<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, which in turn can accelerate cognitive decline and dementia<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>. The hearing-aid industry has invested considerable effort in developing advanced technologies to facilitate communication in difficult acoustic environments<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>. The present study aims to assess the effectiveness of directional microphones in improving intelligibility and reducing listening effort within a realistic noisy scenario.</p><p id="Par3">Directional microphones are designed to improve the signal-to-noise ratio (SNR) in loud acoustic scenarios with multiple sound sources, such as noisy cafeterias, restaurants and shopping centres<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. Directional microphones enhance speech understanding by focusing on sounds coming from a specific direction, typically from in front of the user, while attenuating sounds from other directions. This is achieved through the use of two or more microphones that capture sound at different locations on the hearing aid. The device then processes the differences in timing and intensity between these signals to create a directional response, thereby helping users focus on conversations by minimising interference from surrounding noise sources<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Extensive literature demonstrates that directional microphones improve speech-in-noise intelligibility, particularly when the noise originates from directions other than the front of the listener<sup><xref ref-type="bibr" rid="CR14">14</xref>&#x02013;<xref ref-type="bibr" rid="CR17">17</xref></sup>.</p><p id="Par4">The Ease of Language Understanding (ELU) model is a theoretical framework that explains listening effort by describing how individuals process spoken language in various conditions<sup><xref ref-type="bibr" rid="CR18">18</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref></sup>. According to the model, when auditory input matches the brain&#x02019;s stored linguistic representations, understanding occurs effortlessly and automatically. However, when the input is degraded or unclear, such as in noisy environments or with hearing loss, the brain must dedicate additional cognitive resources, such as working memory, to reconstruct the message. This increased cognitive demand is perceived as listening effort. The ELU model advocates for a comprehensive assessment of hearing aid benefits, considering not only benefits in terms of speech-in-noise performance but also considering the amount of cognitive resources engaged in speech understanding<sup><xref ref-type="bibr" rid="CR18">18</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref></sup>.</p><p id="Par5">Listening effort can be measured using various methods, each providing unique insights into the cognitive demands of auditory processing<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. <italic>Behavioural</italic> measures, such as the dual-task paradigm, assess listening effort by requiring participants to perform a primary listening task alongside a secondary task. The performance on the secondary task reflects the cognitive resources allocated to listening, with poorer performance indicating greater listening effort<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. <italic>Physiological</italic> measures include techniques such as electroencephalography (EEG), pupillometry, and skin conductance. EEG is used to measure listening effort by analysing brain activity patterns, such as changes in neural oscillations, that reflect the cognitive load associated with processing auditory information in challenging acoustic environments<sup><xref ref-type="bibr" rid="CR23">23</xref>&#x02013;<xref ref-type="bibr" rid="CR25">25</xref></sup>. Pupillometry, which tracks changes in pupil size, is another widely used measure, with larger pupil dilation indicating greater listening effort<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR26">26</xref>&#x02013;<xref ref-type="bibr" rid="CR28">28</xref></sup>. Skin conductance measures changes in sweat gland activity, providing an index of autonomic nervous system activation and is used to gauge stress and cognitive load during listening tasks<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>. <italic>Self-reported</italic> measures involve subjective ratings where individuals assess their own perceived listening effort, typically using scales or questionnaires. These self-assessments provide valuable insights into personal experiences of listening difficulty, complementing objective measures<sup><xref ref-type="bibr" rid="CR31">31</xref>&#x02013;<xref ref-type="bibr" rid="CR33">33</xref></sup>.</p><p id="Par6">Previous studies investigating the effects of directional microphones on listening effort have often examined them in addition to other signal processing algorithms aimed at selectively reducing noise components<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. Recent literature presents mixed findings, highlighting the complexity of these technologies across different listening environments. Hornsby (2013) found no reduction in listening effort or mental fatigue when using directional microphones and noise reduction in hearing-impaired adults at SNRs producing 75% intelligibility in speech-shaped cafeteria babble<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Wu et al. (2014) examined hearing aid amplification and directional technology in two dual-task paradigms, and found that although speech recognition improved, listening effort did not decrease in older adults<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. This contrasts with findings in younger populations, likely due to age-related declines in cognitive processing, making older adults less responsive to hearing aid technology in complex environments, where listening effort remains high despite improved speech recognition. Desjardins (2016) explored the individual and combined effects of noise reduction and directional microphones at SNRs producing 50% intelligibility, showing that listening effort was reduced with directional microphones alone or in combination with noise reduction, but not with noise reduction alone<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. Bernarding et al. (2017) found that, relative to omnidirectional microphones, directional microphones and noise reduction enhanced intelligibility and reduced listening effort in 6-talker babble at SNRs producing 50% intelligibility, measured using self-reports and EEG biomarkers in the 7.68&#x000a0;Hz frequency band (alpha-theta border)<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Winneke et al. (2020) assessed wide versus narrow directional microphones in diffuse cafeteria noise and found that narrow directional microphones reduced both subjective listening effort and alpha power in neurophysiological measures<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. These studies collectively suggest that while directional microphones and noise reduction can lower listening effort, their benefits are context-dependent and require further investigation in real-world listening environments.</p><p id="Par7">This paper investigates the isolated effect of an adaptive binaural beamformer in a commercially available hearing aid on speech-in-noise intelligibility and listening effort. The study was conducted in a realistic Ambisonics cafeteria noise environment and used behavioural measures via a dual task, neurophysiological measures based on alpha power, and self-reported measures.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Ethics</title><p id="Par8">The study was conducted at the National Acoustic Laboratories (NAL, Sydney, Australia) following methodologies in accordance with the Ethical Principles of the World Medical Association (Declaration of Helsinki) for medical research involving human subjects. The study protocols were approved by the Hearing Australia Human Research Ethics Committee (Ref.&#x000a0;AHHREC2019-12). Informed consent was obtained from all subjects of the study.</p></sec><sec id="Sec4"><title>Participants</title><p id="Par9">Potential candidates for the study were recruited from the NAL Research Participants Database (a registry of individuals who consented to be invited to participate in NAL research), and clients from Hearing Australia (a government-funded hearing service provider).</p><p id="Par10">The study involved 20 participants (9 females, 19&#x02013;81 years old, mean &#x000b1; SD = 69.0 &#x000b1; 18.8 years old), who met the five inclusion criteria: (1)&#x000a0;age &#x0003e;18&#x000a0;years, (2)&#x000a0;native English speakers, (3)&#x000a0;absence of cognitive impairment, indicated by scoring above 85% on the Montreal Cognitive Assessment (MoCA)<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, (4)&#x000a0;more than 2 years of bilateral hearing aid use, and (5)&#x000a0;bilateral downward-sloping hearing loss characterised by <inline-formula id="IEq1"><alternatives><tex-math id="d33e426">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ge$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq1.gif"/></alternatives></inline-formula>30&#x000a0;dB hearing loss at 500&#x000a0;Hz, <inline-formula id="IEq2"><alternatives><tex-math id="d33e432">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq2.gif"/></alternatives></inline-formula>100&#x000a0;dB hearing loss at 3000&#x000a0;Hz, steepness <inline-formula id="IEq3"><alternatives><tex-math id="d33e438">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq2.gif"/></alternatives></inline-formula>20&#x000a0;dB/oct, and symmetry differences <inline-formula id="IEq4"><alternatives><tex-math id="d33e444">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\le$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq2.gif"/></alternatives></inline-formula>15&#x000a0;dB between the left and right 4-frequency average hearing loss (500, 1000, 2000, and 4000&#x000a0;Hz). Participants were compensated for their time at the conclusion of the study.</p><p id="Par11">Air-conduction pure-tone audiometry was conducted using an AC40 audiometer (Interacoustics A/S, Middelfart, Denmark). Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> shows the quartile distributions of the participants&#x02019; pure-tone hearing thresholds at different frequencies ranging from 250 to 8000&#x000a0;Hz. Individual participant characteristics, including age, gender, first language, years of hearing aid use, MoCA score, and pure-tone audiometric thresholds at 250, 500, 1000, 2000, 3000, 4000, 6000, and 8000&#x000a0;Hz, are detailed in Section&#x000a0;1 of Appendix A in the online Supplementary Materials.<fig id="Fig1"><label>Fig. 1</label><caption><p>Pure-tone hearing threshold distributions from 250 to 8000&#x000a0;Hz. The central mark represents the median, the box edges are the 25th and 75th percentiles, and the whiskers are the maximum and minimum values.</p></caption><graphic xlink:href="41598_2025_95045_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec5"><title>Dual task paradigm</title><p id="Par12">Listening effort was measured via a dual-task paradigm, in which participants performed two tasks simultaneously. The primary task involved repeating a sentence presented in background noise. This noise consisted of realistic cafeteria sounds obtained from the Ambisonics Recording of Typical Environments (ARTE) database<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, presented at 65&#x000a0;dB sound pressure level (SPL) from an array of 41 speakers arranged spherically in five rows. Additionally, two distractors were positioned at <inline-formula id="IEq5"><alternatives><tex-math id="d33e470">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pm 67^{\circ }$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq5.gif"/></alternatives></inline-formula> azimuth to facilitate the evaluation of the adaptive binaural beamformer&#x02019;s efficacy in suppressing nearby noise sources. These distractors were Australian female speakers delivering speech segments from real conversations, each presented at 65&#x000a0;dB SPL. Therefore, the total background noise level was approximately 70&#x000a0;dB SPL.</p><p id="Par13">The target speech was the Australian version of the Matrix test<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. This test uses a closed set of 10 words from the categories <italic>Name</italic> + <italic>Verb</italic> + <italic>Quantity</italic> + <italic>Adjective</italic> + <italic>Object</italic> to form sentences with identical syntax but unpredictable content (e.g., <italic>Peter likes six red toys</italic>). Words were 500&#x000a0;ms long with a 100&#x000a0;ms interval between them and were delivered from a speaker in front of the participant. The level was adjusted for each participant according to the SNR required to achieve 80% and 95% intelligibility in an aided condition, ensuring consistent speech reception thresholds (SRTs) across participants. The SRT-80 and SRT-95 were estimated in each participant from a psychometric function fitted to intelligibility scores over a range of SNRs from +15&#x000a0; to <inline-formula id="IEq6"><alternatives><tex-math id="d33e501">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-15$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq6.gif"/></alternatives></inline-formula>&#x000a0;dB. Detailed methodologies for estimating these SRTs are provided in Section&#x000a0;2 of Appendix A in the online Supplementary Materials. This appendix also shows the individual SRTs of each participant. The averaged SRT-80 and SRT-95 across participants were +0.1&#x000a0;dB and +4.6&#x000a0;dB, respectively.<fig id="Fig2"><label>Fig. 2</label><caption><p>(<bold>a</bold>) Example of a trial in the dual-task paradigm. (<bold>b</bold>) A black circle is presented in the centre of the left vertical rectangle. Given that &#x0201c;Peter&#x0201d; is a male name, the correct response is to press the left arrow key (pointing <italic>towards</italic> the circle), which is highlighted with a grey background. Reaction time (RT)&#x000a0;is measured from the auditory stimulus onset to the key press.</p></caption><graphic xlink:href="41598_2025_95045_Fig2_HTML" id="MO2"/></fig></p><p id="Par14">The secondary task involved a visual component triggered by the auditory stimulus of the primary task. Two large vertical rectangles were projected on an acoustically transparent screen in front of the participant. When the auditory stimulus began, a black circle appeared randomly in the centre of either the left or right rectangle. Participants were instructed to press the keyboard arrow key pointing <italic>towards</italic> the circle if the first word of the auditory stimulus was a <italic>male</italic> name, or the arrow key pointing <italic>away</italic> from the circle if it was a <italic>female</italic> name. Two seconds after the auditory stimulus ended (retention period), the black circle disappeared from the screen, and the participant repeated the words they had understood. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>a illustrates the structure of an example trial. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>b depicts a black circle in the centre of the left vertical rectangle. Given that the auditory stimulus in this example begins with a male name (Peter), the correct response is to press the left arrow key, highlighted with a grey background, as it points toward the circle.</p><p id="Par15">Speech intelligibility was assessed by manually marking the correctly identified words. Participants&#x02019; listening effort was measured using three methods: <italic>behaviourally</italic>, through reaction time<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> from sentence onset to key press; <italic>neurophysiologically</italic>, through alpha power<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup> recorded via a 64-channel SynAmps-RT NeuroScan electroencephalography (EEG) recording system (Compumedics Limited, Abbotsford, Australia) using a sampling rate of 1&#x000a0;kHz; and <italic>self-reported</italic>, with participants rating their perceived effort on a 7-point scale (i.e., no effort, very little, little, moderate, considerable, much, and extreme effort)<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> after every five sentences. The dual-task paradigm was implemented in MATLAB (The Mathworks Inc., Natick, MA), using functions from the &#x02018;Statistics and Machine Learning&#x02019;, &#x02018;Signal Processing&#x02019; and &#x02018;Optimization&#x02019; toolboxes, along with the &#x02018;Psychophysics Toolbox Version 3&#x02019; extension<sup><xref ref-type="bibr" rid="CR41">41</xref>&#x02013;<xref ref-type="bibr" rid="CR43">43</xref></sup>.</p></sec><sec id="Sec6"><title>Preparatory &#x00026; experimental sessions</title><p id="Par16">The study comprised two preparatory sessions and one experimental session. In the first preparatory session, participants (i)&#x000a0;received an overview of the study&#x02019;s rationale and methods and signed a consent form; (ii)&#x000a0;underwent a hearing assessment with otoscopy and air-conduction audiometry; (iii)&#x000a0;completed the Montreal Cognitive Assessment (MoCA) to screen for cognitive impairment<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>; and (iv)&#x000a0;had two sets of slim-tip earmold impressions taken: one vented appropriate for an acclimatisation period, and one with occluded vents for the experimental session.</p><p id="Par17">Participants were scheduled for a second preparatory session after their earmolds were received at NAL facilities. This session included a practice session of the dual-task methodology, in which participants (i)&#x000a0;read the test instructions (see Section&#x000a0;3 of Appendix A in the online Supplementary Materials), (ii)&#x000a0;practised the test procedure by marking responses on a printed document (also available in the same appendix) without time constraints; and (iii)&#x000a0;practised a simplified version of the dual-task test delivered on a laptop using headphones, first in quiet (only target speech presented), and then with background noise present. This structured approach ensured participants were well-prepared to perform the full dual-task test during the experimental session.</p><p id="Par18">In the second preparatory session, participants were also fitted with Phonak Aud&#x000e9;o M90-312 hearing aids (Sonova AG, Stafa, Switzerland) using Phonak Target 7.0.5 software and vented SlimTip earmolds. The hearing aids were adjusted to meet the NAL-NL2 target at 65 dB, as verified by real-ear measurement using Aurical FreeFit (Natus Medical Inc., Middleton, WI), and individual feedback tests were conducted. Participants were subsequently sent home with these devices for a 4-week acclimatisation period. During this acclimatisation period, the hearing aids defaulted to an automatic program that independently adjusted hearing aid settings based on the listening environment. To emulate a fitting with typical settings, all hearing aid features were left enabled at their default values. Frequency lowering was also permitted but could be disabled at the participant&#x02019;s discretion. In addition to the automatic program, two manually selectable programs were also made available to the listener: (i)&#x000a0;<italic>Quasi-omnidirectional</italic> (Q-Omni) and (ii)&#x000a0;<italic>Directional microphone</italic> (DM). Both manual programs used the same settings as the default automatic program for speech in noise but differed in their microphone modes. Q-Omni employed a quasi-ominidirectional microphone strategy that simulates the ear&#x02019;s natural directionality, whereas DM used an adaptive binaural microphone system, providing a highly directional listening beam. During the 4-weeks acclimatisation, participants were encouraged to try the two manual programs in loud acoustic scenarios to familiarize themselves with the sound. The two manual programs were selected for use in the experimental session. Compared to Q-Omni, acoustic measures showed that DM provided a +5.6&#x000a0;dB improvement in the articulation index-weighted directivity index<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, and a +4.8&#x000a0;dB advantage in the articulation index SNR<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> (details available in Section&#x000a0;4 of Appendix A in the Supplementary Materials online).</p><p id="Par19">Participants attended the experimental session, which took place in the anechoic chamber of the Australian Hearing Hub (Sydney, Australia), after completing their acclimatization period. This session involved: (i)&#x000a0;estimating the SNRs for 80% and 95% speech-in-noise intelligibility (SRT-80 and SRT-95) with hearing aids in Q-Omni mode, as detailed in Section&#x000a0;2 of Appendix A in the online Supplementary Materials; (ii)&#x000a0;practising the dual-task test in quiet (without background noise); and (iii)&#x000a0;conducting the dual-task test under four conditions&#x02013;SRT-80 and SRT-95 in both the Q-Omni and DM hearing aid programs. Each condition was tested four times, resulting in a total of 16 blocks. Each block comprised 20 sentences, amounting to 80 sentences per condition. The order of conditions was pseudo-randomised by randomly varying the sequence of the four conditions. For example: [SRT-95 (Q-Omni) &#x02013; SRT-80 (Q-Omni) &#x02013; SRT-80 (DM) &#x02013; SRT-95 (DM)] &#x02013; [SRT-80 (DM) &#x02013; SRT-95 (Q-Omni) &#x02013; SRT-80 (Q-Omni) &#x02013; SRT-95 (DM)] &#x02013; [SRT-80 (Q-Omni) &#x02013; SRT-95 (Q-Omni) &#x02013; SRT-95 (DM) &#x02013; SRT-80 (DM)] &#x02013; [SRT-80 (DM) &#x02013; SRT-95 (Q-Omni) &#x02013; SRT-95 (DM) &#x02013; SRT-80 (Q-Omni)]. This pseudo-randomisation aimed to balance the effects of learning across the test conditions. During the experimental session, hearing aids were fitted with occluded-vent earmolds to enhance the effectiveness of directional microphones<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, and manual programs were used to select the desired hearing aid settings.</p></sec><sec id="Sec7"><title>Data analysis</title><sec id="Sec8"><title>Speech-in-noise intelligibility, reaction time, and self-reported effort</title><p id="Par20">Data analysis was conducted in MATLAB using functions from the &#x02018;Statistics and Machine Learning&#x02019; toolbox. The DM effect relative to Q-Omni was characterised in the two evaluated SRTs via a series of generalised linear mixed-effects (GLME)<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> models. Speech-in-noise intelligibility, reaction time, or self-reported measures were considered as test variables; the hearing-aid program (Q-Omni or DM) was included as predictor variable; and participants were treated as a random effect. Lilliefors tests indicated that none of the test variables were normally distributed<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, which justified the use of GLME models. GLME models also offer the advantage of accounting for repeated measures and provide robustness against missing and unbalanced data<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. Additional GLME models were considered, incorporating the run order as a predictor variable to model any potential learning effects during the test.</p><p id="Par21">Given that speech-in-noise intelligibility scores and self-reported measures consisted of nonnegative integers, the GLME models employed a Poisson distribution family with a natural logarithmic link function, i.e.,&#x000a0;<inline-formula id="IEq7"><alternatives><tex-math id="d33e631">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g(\cdot ) = \ln (\cdot )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq7.gif"/></alternatives></inline-formula>, and an exponential inverse link function, i.e., <inline-formula id="IEq8"><alternatives><tex-math id="d33e637">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h(\cdot ) = g^{-1} = e^{(\cdot )}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq8.gif"/></alternatives></inline-formula><sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. Reaction times were deemed valid if the first word of the acoustic stimulus was correctly understood, thus excluding unreliable estimates of listening effort due to intelligibility issues. Considering the reaction time distributions across participants, reaction times below 400&#x000a0;ms were considered unreliable and were discarded from analysis. As reaction times consisted of positive numbers, a Gamma distribution family was used with <inline-formula id="IEq9"><alternatives><tex-math id="d33e646">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g(\cdot ) = (\cdot )^{-1}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq9.gif"/></alternatives></inline-formula> as link function, and <inline-formula id="IEq10"><alternatives><tex-math id="d33e652">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h(\cdot ) = g^{-1} = g(\cdot )^{-1}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq10.gif"/></alternatives></inline-formula> as inverse link function<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>.</p><p id="Par22">Appendix B in the online Supplementary Materials provides the raw data for speech-in-noise intelligibility, reaction time, and self-reported measures of effort, along with custom MATLAB scripts for re-generating figures and performing the statistical analyses presented in the Results section.</p></sec><sec id="Sec9"><title>Neurophysiological measures</title><p id="Par23">Recorded EEG signals were processed using MATLAB, employing functions from the FieldTrip<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> and EEGLAB<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> toolboxes. Participant #P06 was excluded due to a technical issue with the triggers, resulting in a final sample size of 19 participants.</p><p id="Par24">Participants&#x02019; EEG signals were processed in each test condition following the steps below: (i)&#x000a0;data loading; (ii)&#x000a0;visual identification of noisy channels; (iii) re-reference to the average across all channels, excluding the identified noisy channels, as well as the horizontal and vertical eye channels; (iv) interpolation of noisy channels by replacing them with the average of its neighbours, weighted by distance; (v)&#x000a0;segmentation of data into 9-seconds trials; (vi)&#x000a0;estimation of independent components via Independent Component Analysis<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>; (vii)&#x000a0;visual identification of components related to eye-blinks and saccades; (viii)&#x000a0;recomposition of data excluding eye-activity components; (ix)&#x000a0;high-pass filtering of EEG signals with a 1&#x000a0;Hz cutoff frequency; (x)&#x000a0;identification of noisy-trials based on absolute values exceeding <inline-formula id="IEq11"><alternatives><tex-math id="d33e682">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$100~\upmu$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq11.gif"/></alternatives></inline-formula>V; (xi) interpolation of noisy trials by averaging neighbouring channels weighted by distance if fewer than 10 noisy channels were present, otherwise rejection of the entire trial; and (xii)&#x000a0;power spectrum estimation through time-frequency analysis using a Morlet wavelet of 5 cycles within the 0&#x02013;30&#x000a0;Hz range. Section&#x000a0;5 of Appendix A in the online Supplementary Materials provides the MATLAB script used for processing the EEG files from a selected participant, detailing each methodological step.</p><p id="Par25">Differences in brain activity between Q-Omni and DM were investigated in the two evaluated SRTs across the time intervals [-1.0 &#x02013; 0.0]&#x000a0;s, [0.0 &#x02013; 2.0]&#x000a0;s, [2.0 &#x02013; 3.5]&#x000a0;s, and [3.5 &#x02013; 5.0]&#x000a0;s. These time intervals were chosen based on the averaged power spectra across participants and electrodes, which follow a related but distinct time structure to the time sections of the dual-task paradigm shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. A cluster-based permutation test was used for statistical analysis to correct for multiple comparisons<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. This test followed a two-stage process: (stage&#x000a0;1) calculation of a cluster-based test statistic, and (stage&#x000a0;2) determination of the significance probability. In stage 1, the procedure was as follows: (i)&#x000a0;The alpha power difference (8&#x02013;12&#x000a0;Hz) between the two hearing aid programs was characterised in each EEG channel using <italic>t</italic>-values; (ii)&#x000a0;EEG channels with <italic>t</italic>-values exceeding the <inline-formula id="IEq12"><alternatives><tex-math id="d33e703">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$97.5^{th}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq12.gif"/></alternatives></inline-formula> percentile (corresponding to a 0.05 significance threshold for a two-sided <italic>t</italic>-test) were identified; (iii)&#x000a0;these selected channels were grouped into clusters; (iv)&#x000a0;cluster-based statistics were computed by summing the <italic>t</italic>-values within each cluster; and (v)&#x000a0;the largest cluster-level statistic was chosen as the cluster-based test statistic. In stage&#x000a0;2, the significance probability was calculated using a Monte Carlo method, as follows: (i)&#x000a0;All trials were pooled into in a single dataset; (ii)&#x000a0;a random partition was obtained by randomly selecting from the pooled dataset a number of trials equal to the size of the identified cluster in stage&#x000a0;1; (iii)&#x000a0;the test statistic (i.e., the maximum cluster-level summed <italic>t</italic>-value) was computed for the random partition; (iv)&#x000a0;steps (ii) and (iii) were repeated many times to generate a histogram of test statistics, using all possible number of randomisations to ensure optimal accuracy; and (v)&#x000a0;the proportion of random partitions with a test statistic exceeding the cluster-based test statistic from stage&#x000a0;1 was calculated, yielding the Monte Carlo significance probability (<italic>p</italic>-value). Clusters were deemed statistically significant if their <italic>p</italic>-values were below the 0.05 threshold. Section&#x000a0;6 of Appendix A in the online Supplementary Materials provides the MATLAB script used for the statistical analysis.</p></sec></sec></sec><sec id="Sec10"><title>Results</title><sec id="Sec11"><title>Speech-in-noise intelligibility</title><p id="Par26">Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> shows the mean speech-in-noise intelligibility scores per participant across the four testing conditions. The GLME models presented in the top section of Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> indicate that at SRT-80, intelligibility improved from 83.6% with Q-Omni (estimated as <inline-formula id="IEq13"><alternatives><tex-math id="d33e737">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e^{\beta _{Intercept}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq13.gif"/></alternatives></inline-formula>) to 88.7% with DM (<inline-formula id="IEq14"><alternatives><tex-math id="d33e743">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 9\cdot 10^{-52}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq14.gif"/></alternatives></inline-formula>, estimated as <inline-formula id="IEq15"><alternatives><tex-math id="d33e749">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e^{\beta _{Intercept}+\beta _{DM}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq15.gif"/></alternatives></inline-formula>); and at SRT-95, from 90.8% with Q-Omni to 93.3% with DM (<inline-formula id="IEq16"><alternatives><tex-math id="d33e756">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 2\cdot 10^{-13}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq16.gif"/></alternatives></inline-formula>). Additionally, the GLME models in the bottom section of Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> demonstrated a statistically significant learning effect in both SRTs. At SRT-80, relative to the first run, intelligibility improved by 2.8% in the second run (<inline-formula id="IEq17"><alternatives><tex-math id="d33e765">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 2\cdot 10^{-10}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq17.gif"/></alternatives></inline-formula>, estimated as <inline-formula id="IEq18"><alternatives><tex-math id="d33e771">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e^{\beta _{Intercept}+\beta _{Run2}} - e^{\beta _{Intercept}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq18.gif"/></alternatives></inline-formula>), 4.6% in the third run (<inline-formula id="IEq19"><alternatives><tex-math id="d33e777">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 2\cdot 10^{-24}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq19.gif"/></alternatives></inline-formula>), and 5.6% in the fourth run (<inline-formula id="IEq20"><alternatives><tex-math id="d33e783">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 1\cdot 10^{-34}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq20.gif"/></alternatives></inline-formula>). At SRT-95, compared to the first run, intelligibility improved by 1.8% in the third run (<inline-formula id="IEq21"><alternatives><tex-math id="d33e790">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 0.0001$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq21.gif"/></alternatives></inline-formula>), and 2.1% in the fourth run (<inline-formula id="IEq22"><alternatives><tex-math id="d33e796">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 9\cdot 10^{-6}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq22.gif"/></alternatives></inline-formula>).<fig id="Fig3"><label>Fig. 3</label><caption><p>Mean intelligibility scores (%-correct) per participant are shown for the two speech reception threshold (SRT) conditions and the two hearing aid programs&#x02014;Quasi-Omnidirectional (Q-Omni) and Directional Microphone (DM). Boxplots represent the quartiles of each distribution. Estimated scores and their associated <italic>p</italic>-values for each scenario are obtained from the generalised linear mixed-effects model in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> (top-section). The colours assigned to each participant are detailed in Section 7 of Appendix A in the online Supplementary Materials. Int: Intercept.</p></caption><graphic xlink:href="41598_2025_95045_Fig3_HTML" id="MO3"/></fig></p><p id="Par27">
<table-wrap id="Tab1"><label>Table 1</label><caption><p>(Top section) Generalised linear mixed-effects (GLME) models for speech-in-noise intelligibility at the 80% and 95% speech reception threshold (SRT-80 and SRT-95), with the hearing aid program (Q-Omni: Quasi-Omnidirectional, DM: Directional Microphone) as a predictor variable and participants as a random effect. The intercept refers to the Q-Omni program. (Bottom section) Equivalent GLME models incorporating the run order as a predictor variable. The intercept corresponds to the Q-Omni program in the first run. GLME models used a Poisson distribution with a natural logarithmic link function. <italic>N</italic> number of observations, <inline-formula id="IEq23"><alternatives><tex-math id="d33e827">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq23.gif"/></alternatives></inline-formula> estimated value of the coefficient, <italic>SE</italic> standard error of the coefficient. <italic>CI</italic> confidence interval.</p></caption><graphic position="anchor" xlink:href="41598_2025_95045_Tab1_HTML" id="MO55"/></table-wrap>
</p></sec><sec id="Sec12"><title>Reaction time</title><p id="Par28">Figure&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref> shows the individuals&#x02019; reaction time data in the four testing conditions. Results from the GLME models presented in the top section of Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> show that the predicted reaction times for SRT-80 [Q-Omni], SRT-80 [DM], SRT-95 [Q-Omni] and SRT-95 [DM] were 1464&#x000a0;ms (estimated as <inline-formula id="IEq36"><alternatives><tex-math id="d33e852">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta _{Intercept}^{-1}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq36.gif"/></alternatives></inline-formula>), 1422&#x000a0;ms (estimated as <inline-formula id="IEq37"><alternatives><tex-math id="d33e858">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[\beta _{Intercept} + \beta _{DM}]^{-1}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq37.gif"/></alternatives></inline-formula>), 1371&#x000a0;ms, and 1342&#x000a0;ms, respectively. The DM effect over Q-Omni was found to be statistically significant, both at SRT-80 (<inline-formula id="IEq38"><alternatives><tex-math id="d33e864">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 0.0055$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq38.gif"/></alternatives></inline-formula>) and SRT-95 (<inline-formula id="IEq39"><alternatives><tex-math id="d33e871">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p = 0.0144$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq39.gif"/></alternatives></inline-formula>). The bottom section of Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> also shows a statistically significant learning effect in the two SRTs. At SRT-80, compared to the first run, reaction times were 255&#x000a0;ms shorter in the second run (<inline-formula id="IEq40"><alternatives><tex-math id="d33e880">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=4\cdot 10^{-31}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq40.gif"/></alternatives></inline-formula>), calculated as <inline-formula id="IEq41"><alternatives><tex-math id="d33e886">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\beta _{Intercept}+\beta _{Run 2})^{-1} - \beta _{Intercept}^{-1}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq41.gif"/></alternatives></inline-formula>, 309&#x000a0;ms shorter in the third run (<inline-formula id="IEq42"><alternatives><tex-math id="d33e892">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=5\cdot 10^{-46}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq42.gif"/></alternatives></inline-formula>), and 289&#x000a0;ms shorter in the fourth run (<inline-formula id="IEq43"><alternatives><tex-math id="d33e898">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=7\cdot 10^{-39}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq43.gif"/></alternatives></inline-formula>). At SRT-95, relative to the first run, reaction times were 143&#x000a0;ms shorter in the second run (<inline-formula id="IEq44"><alternatives><tex-math id="d33e905">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=5\cdot 10^{-18}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq44.gif"/></alternatives></inline-formula>), 208&#x000a0;ms shorter in the third run (<inline-formula id="IEq45"><alternatives><tex-math id="d33e911">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=7\cdot 10^{-37}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq45.gif"/></alternatives></inline-formula>), and 270&#x000a0;ms shorter in the fourth run (<inline-formula id="IEq46"><alternatives><tex-math id="d33e917">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=6\cdot 10^{-59}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq46.gif"/></alternatives></inline-formula>).<fig id="Fig4"><label>Fig. 4</label><caption><p>Individual reaction times (ms) per participant in the two speech reception threshold (SRT) conditions and the two hearing aid programs&#x02014;Quasi-Omnidirectional (Q-Omni) and Directional Microphone (DM). Boxplots represent the quartiles of each distribution. Estimated scores and their associated <italic>p</italic>-values are obtained from the generalised linear mixed-effects model in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> (top section). Participants #P01 to #P20 reaction time distributions are organised from left to right in each scenario. The colours assigned to each participant are detailed in Section 7 of Appendix A in the online Supplementary Materials. Int: Intercept.</p></caption><graphic xlink:href="41598_2025_95045_Fig4_HTML" id="MO4"/></fig></p><p id="Par29">
<table-wrap id="Tab2"><label>Table 2</label><caption><p>(Top section) Generalised linear mixed-effects (GLME) models for reaction time at the 80% and 95% speech reception threshold (SRT-80 and SRT-95), with the hearing aid program (Q-Omni: Quasi-Omnidirectional, DM: Directional Microphone) as a predictor variable and participants as a random effect. The intercept refers to the Q-Omni program. (Bottom section) Equivalent GLME models incorporating the run order as a predictor variable. The intercept corresponds to the Q-Omni program in the first run. GLME models used a Gamma distribution with an inverse function as link function. <italic>N</italic>: Number of observations. <inline-formula id="IEq47"><alternatives><tex-math id="d33e948">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq23.gif"/></alternatives></inline-formula>: Estimated value of the coefficient. SE: Standard error of the coefficient. CI: Confidence interval.</p></caption><graphic position="anchor" xlink:href="41598_2025_95045_Tab2_HTML" id="MO56"/></table-wrap>
</p></sec><sec id="Sec13"><title>Self-reported effort</title><p id="Par30">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows the mean self-reported effort per participant for each test condition, along with the predicted scores and <italic>p</italic>-values resulting from the GLME model presented in the top section of Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>. On a scale from 1 (<italic>no effort</italic>) to 7 (<italic>extreme effort</italic>), participants reported 0.52 units less effort with DM compared to Q-Omni at SRT-80 (<inline-formula id="IEq93"><alternatives><tex-math id="d33e977">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=0.0025$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq93.gif"/></alternatives></inline-formula>). At SRT-95, DM resulted in 0.19 units less effort than Q-Omni, though this was not statistically significant (<inline-formula id="IEq94"><alternatives><tex-math id="d33e983">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=0.2351$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq94.gif"/></alternatives></inline-formula>). No learning effect was observed in this measure.<fig id="Fig5"><label>Fig. 5</label><caption><p>Mean self-reported effort scores of participants in the two speech reception threshold (SRT) conditions and hearing aid programs&#x02014;Quasi-Omnidirectional (Q-Omni) and Directional Microphone (DM). Boxplots represent the quartiles of each distribution. Estimated scores and their associated <italic>p</italic>-values for each scenario are derived from the generalised linear mixed-effects model shown in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> (top section). The colours assigned to each participant are detailed in Section 7 of Appendix A in the online Supplementary Materials. Int: Intercept.</p></caption><graphic xlink:href="41598_2025_95045_Fig5_HTML" id="MO5"/></fig></p><p id="Par31">
<table-wrap id="Tab3"><label>Table 3</label><caption><p>(Top section) Generalised linear mixed-effects (GLME) models for self-reported effort at the 80% and 95% speech reception threshold (SRT-80 and SRT-95), with the hearing aid program (Q-Omni: Quasi-Omnidirectional, DM: Directional Microphone) as a predictor variable and participants as a random effect. The intercept refers to the Q-Omni program. [Bottom section] Equivalent GLME models incorporating the run order as a predictor variable. The intercept corresponds to the Q-Omni program in the first run. GLME models used a Poisson distribution with a natural logarithmic link function. <italic>N</italic> number of observations, <inline-formula id="IEq95"><alternatives><tex-math id="d33e1014">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq23.gif"/></alternatives></inline-formula> estimated value of the coefficient, <italic>SE</italic> standard error of the coefficient, <italic>CI</italic> confidence interval.</p></caption><graphic position="anchor" xlink:href="41598_2025_95045_Tab3_HTML" id="d33e1027"/></table-wrap>
</p></sec><sec id="Sec14"><title>Neurophysiological measures</title><p id="Par32">Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>.a presents the averaged spectrogram across participants and electrodes for the Q-Omni (top) and DM (bottom) hearing aid programs under SRT-80 (left) and SRT-95 (right) conditions. A trial structure diagram is shown at the top. Brain activation patterns are consistent across all scenarios, showing prominent alpha power during the pre-stimulus, the final portion of encoding and in the retention periods. The black rectangles indicate the time intervals where Q-Omni and DM exhibited statistically significant differences in alpha power for each SRT. No significant differences were observed in the [0.0 &#x02013; 2.0]&#x000a0;s or [3.5 &#x02013; 5.0]&#x000a0;s intervals for any SRT condition. Individual spectrograms for each participant are provided in Section&#x000a0;8 of Appendix A in the online Supplementary Materials. These figures show consistent brain activation patterns across participants, and substantial individual variability in alpha power magnitude.</p><p id="Par33">Figure&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>.b shows the averaged power spectrum differences between the two hearing aid programs (Q-Omni &#x02013; DM) across participants and electrodes for both SRTs. These plots show positive alpha power (greater in Q-Omni relative to DM) in the [<inline-formula id="IEq104"><alternatives><tex-math id="d33e1041">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1.0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq104.gif"/></alternatives></inline-formula> , 0.0] ms window, and negative alpha power (greater in DM relative to Q-Omni) in the [2.0 , 3.5] ms window.</p><p id="Par34">Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>.a presents topographic maps of the <italic>t</italic>-statistic distribution across the scalp from the cluster-based permutation analysis on absolute alpha power differences between Q-Omni and DM in the [<inline-formula id="IEq105"><alternatives><tex-math id="d33e1055">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1.0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq104.gif"/></alternatives></inline-formula> &#x02013; 0.0]&#x000a0;s and [2.0 &#x02013; 3.5]&#x000a0;s intervals for SRT-80 and SRT-95. Crosses denote EEG channels within statistically-significant clusters. Results indicate a consistent effect of DM over Q-Omni in both SRTs. In the [<inline-formula id="IEq106"><alternatives><tex-math id="d33e1061">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1.0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq104.gif"/></alternatives></inline-formula> &#x02013; 0.0]&#x000a0;s interval, DM showed decreased alpha power (positive <italic>t</italic>-statistic values) in the right parietal-occipital region for both SRT-80 (cluster <italic>t</italic>-statistic = 20.98, <inline-formula id="IEq107"><alternatives><tex-math id="d33e1074">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=0.0106$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq107.gif"/></alternatives></inline-formula>) and SRT-95 (cluster <italic>t</italic>-statistic = 14.18, <inline-formula id="IEq108"><alternatives><tex-math id="d33e1083">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=0.0389$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq108.gif"/></alternatives></inline-formula>). In the [2.0 &#x02013; 3.5]&#x000a0;s interval, DM resulted in increased alpha power (negative <italic>t</italic>-statistic values) in centro-temporal areas for SRT-80 (cluster <italic>t</italic>-statistic = <inline-formula id="IEq109"><alternatives><tex-math id="d33e1096">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-25.05$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq109.gif"/></alternatives></inline-formula>, <inline-formula id="IEq110"><alternatives><tex-math id="d33e1102">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=0.0219$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq110.gif"/></alternatives></inline-formula>) and in the left centro-parietal region for SRT-95 (cluster <italic>t</italic>-statistic = <inline-formula id="IEq111"><alternatives><tex-math id="d33e1111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-20.09$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq111.gif"/></alternatives></inline-formula>, <inline-formula id="IEq112"><alternatives><tex-math id="d33e1117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p=0.0272$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq112.gif"/></alternatives></inline-formula>).</p><p id="Par35">Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref>.b shows the averaged power spectrum across participants and EEG channels within each cluster for the [<inline-formula id="IEq113"><alternatives><tex-math id="d33e1128">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1.0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq104.gif"/></alternatives></inline-formula> &#x02013; 0.0]&#x000a0;s and [2.0 &#x02013; 3.5]&#x000a0;s intervals in both SRTs. Consistent with the topographic representations shown in Panel a, these plots visually show decreased alpha power with DM in the [<inline-formula id="IEq114"><alternatives><tex-math id="d33e1134">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1.0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq104.gif"/></alternatives></inline-formula> &#x02013; 0.0]&#x000a0;s interval and increased alpha power in the [2.0 &#x02013; 3.5]&#x000a0;interval across both SRTs.<fig id="Fig6"><label>Fig. 6</label><caption><p>(<bold>a</bold>)&#x000a0;Averaged power spectrum across participants and electrodes for the Quasi-Omnidirectional (Q-Omni) and Directional Microphone (DM) hearing aid programs in the two SRT conditions. A trial structure diagram is shown at the top. The black rectangles indicate the time invervals presenting statistically significant differences in alpha power (8&#x02013;12&#x000a0;Hz) between Q-Omni and DM, i.e. [<inline-formula id="IEq115"><alternatives><tex-math id="d33e1149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1.0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq104.gif"/></alternatives></inline-formula> &#x02013; 0.0]&#x000a0;s and [2.0 &#x02013; 3.5]&#x000a0;s. (<bold>b</bold>)&#x000a0;Averaged power spectrum differences between the Q-Omni and DM programs across participants and electrodes for both SRTs.</p></caption><graphic xlink:href="41598_2025_95045_Fig6_HTML" id="MO6"/></fig><fig id="Fig7"><label>Fig. 7</label><caption><p>(<bold>a</bold>)&#x000a0;Alpha power differences between Q-Omni and DM at SRT-80 (left) and SRT-95 (right) represented in terms of <italic>t</italic>-statistics. Crosses indicate EEG channels that form clusters with statistically significant absolute differences between the two hearing aid programs. (<bold>b</bold>)&#x000a0;Averaged power spectrum across participants and EEG channels within each cluster for the two evaluated SRTs at the [<inline-formula id="IEq116"><alternatives><tex-math id="d33e1175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1.0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_95045_Article_IEq104.gif"/></alternatives></inline-formula> &#x02013; 0.0]&#x000a0;s and [2.0 &#x02013; 3.5]&#x000a0;s time intervals. The alpha frequency band is highlighted with a grey background.</p></caption><graphic xlink:href="41598_2025_95045_Fig7_HTML" id="MO7"/></fig></p></sec></sec><sec id="Sec15"><title>Discussion</title><p id="Par36">This study aimed to investigate the effect of an adaptive binaural beamformer in a commercially available hearing aid on speech intelligibility in noise and listening effort. The research was conducted in a realistic Ambisonics-simulated cafeteria noise environment, utilising for the first time simultaneous measures of behavioural dual-task performance, neurophysiological alpha power monitoring, and self-reported data. By integrating these methods, we captured both objective and subjective aspects of auditory processing. Behavioural measures, such as dual-task performance, provided direct evidence of reduced cognitive load, while neurophysiological data (specifically alpha power) offered insights into the underlying brain activity associated with listening effort. Self-reports complemented these by capturing participants&#x02019; perceptions of effort. This multi-faceted approach allowed for a comprehensive evaluation of how hearing aids with directionality alleviate listening challenges in complex auditory environments.</p><p id="Par37">Results show that adaptive binaural beamforming improves speech intelligibility in noisy environments. This aligns with previous research showing that directional microphones enhance speech perception by improving the SNR<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR53">53</xref></sup>. The binaural beamformer directs focus towards frontal speech signals while suppressing noise from other directions, thereby creating a more favourable listening environment. This directional focus enables hearing aid users to converse more effectively in noisy conditions, particularly in diffuse sound environments like a busy cafeteria.</p><p id="Par38">The effect of directional microphones on listening effort were more complex. Behavioural dual-task performance revealed a reduction in listening effort, evidenced by faster reaction times when the hearing aid&#x02019;s directionality was active. This reduction is consistent with the Ease of Language Understanding (ELU) model<sup><xref ref-type="bibr" rid="CR18">18</xref>&#x02013;<xref ref-type="bibr" rid="CR20">20</xref></sup>, suggesting that a lower cognitive load allows for better performance in secondary tasks<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Neurophysiological measures further supported this finding, showing a reduction in pre-stimulus alpha power and an increase during the later portion of encoding when the beamformer was engaged. The reproducibility of this brain activation pattern across the two SRT conditions reinforces its reliability. The observed pattern likely reflects the dual roles of alpha-band oscillations: inhibition and information processing<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. On one hand, pre-stimulus alpha, associated with inhibition<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, was higher in the Q-Omni program, suggesting increased cognitive resource allocation to attenuate the effect of the louder background noise in that condition, thus indicating heightened listening effort in the Q-Omni program relative to DM. On the other hand, the increased alpha power during the later encoding phase in the DM condition likely reflects a greater working memory load due to retaining more words as a result of improved intelligibility<sup><xref ref-type="bibr" rid="CR54">54</xref>&#x02013;<xref ref-type="bibr" rid="CR57">57</xref></sup>. While participants&#x02019; subjective ratings of listening effort were reduced in both SRT conditions, the effect of DM was minimal and statistically insignificant at SRT-95. The combination of behavioural, neurophysiological, and self-reported data supports the positive impact of these technologies on listening effort in challenging acoustic environments.</p><p id="Par39">The findings of this study hold important clinical implications, particularly regarding how clinicians assess and manage listening effort. While self-reported measures are commonly used, this study highlights the potential for more objective measures, such as behavioural measures based on a dual-task, in revealing effort reductions that may not be consciously perceived. In this study, significant effects of directional microphones were observed in behavioural and neurophysiological measures at SRT-95, but these were not reflected in self-reports. Clinicians should be aware that improvements in listening effort due to directional microphones may occur even when patients do not report significant changes. It is therefore essential for audiologists to inform patients about the benefits of these technologies, even if the effects are not immediately obvious, in order to manage expectations and enhance satisfaction, ultimately improving clinical outcomes.</p><p id="Par40">Interestingly, a statistically significant learning effect was observed in both speech intelligibility and reaction time measures over multiple dual-task runs, though not in self-reported listening effort. This discrepancy between objective performance improvements and subjective effort aligns with previous studies, suggesting that repeated exposure to difficult listening conditions can lead to perceptual learning, improving intelligibility and reaction times<sup><xref ref-type="bibr" rid="CR58">58</xref>&#x02013;<xref ref-type="bibr" rid="CR60">60</xref></sup>. However, self-reported effort may remain unchanged, as these assessments are influenced more by cognitive load, emotional state, and individual differences rather than task mastery<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR61">61</xref></sup>. This highlights the complexity of measuring listening effort and the importance of using both objective and subjective assessments to provide a comprehensive evaluation, as subjective reports may not always reflect objective improvements<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par41">We also observed that the effects of DM were more pronounced at the lower SRT. This was reflected in greater improvements in speech-in-noise intelligibility (a 5.1% increase at SRT-80 compared to 2.5% at SRT-95), larger reductions in reaction times (42&#x000a0;ms at SRT-80 compared to 29&#x000a0;ms at SRT-95), greater self-reported benefits (a 0.52-point reduction on a 7-point scale at SRT-80 compared to 0.19 at SRT-95), and more distinct brain activation differences (cluster <italic>t</italic>-statistics of 20.98 and -25.05 during pre-stimulus and encoding at SRT-80, versus 14.18 and -20.09 at SRT-95). This pattern aligns with existing research, which shows that directional microphones and noise reduction systems are most effective in environments with lower SNRs<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR53">53</xref>,<xref ref-type="bibr" rid="CR62">62</xref></sup>. These findings suggest that DM systems offer greater benefit in more challenging listening conditions, particularly where auditory processing demands are higher. By selecting SRTs of 80% and 95%, we also ensured that intelligibility was high enough for participants to engage meaningfully with the dual-task paradigm, minimising potential confounds related to motivation. Excessively low SNRs could lead to a lack of participant engagement, diminishing the perceived benefits of the hearing aid features<sup><xref ref-type="bibr" rid="CR63">63</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>. Therefore, it is possible that at extremely low SNRs, the benefits of directionality may plateau, as the cognitive demand may surpass the technology&#x02019;s capacity to assist.</p><p id="Par42">A key strength of this study lies in the use of a realistic Ambisonics-simulated cafeteria noise environment at relatively high SNRs, which closely mimics the complex auditory landscapes that hearing aid users encounter in daily life. Previous studies have often relied on simplified laboratory settings that do not fully capture the spatial and temporal dynamics of real-world listening situations<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR53">53</xref>,<xref ref-type="bibr" rid="CR62">62</xref></sup>. By simulating a realistic environment, this study provides insights that are more applicable to everyday listening conditions, thus enhancing the ecological validity of the findings. However, the effectiveness of hearing aid technologies may vary across different environments and user populations. While this study focused on cafeteria noise, further research should explore the generalisability of these findings to other challenging listening contexts, such as fluctuating outdoor noise or environments with significant reverberation. Methodological improvements to increase ecological validity could include using more realistic speech stimuli in the primary task, such as the Everyday Conversational Sentences in Noise (ECO-SiN) test<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>, which presents natural conversation in real-world background noise, and employing Ecological Momentary Assessment (EMA) methods to assess self-reported DM benefits in a broad range of everyday listening environments<sup><xref ref-type="bibr" rid="CR66">66</xref>,<xref ref-type="bibr" rid="CR67">67</xref></sup>.</p></sec><sec id="Sec16"><title>Conclusion</title><p id="Par43">This study provides robust evidence that hearing aids incorporating adaptive binaural beamforming significantly enhance speech intelligibility and reduce listening effort in noisy environments, particularly under challenging conditions with lower signal-to-noise ratios. The combination of behavioural, neurophysiological, and self-reported data offers a comprehensive assessment of these benefits. Objective measures, such as faster reaction times and changes in alpha power, indicate a clear reduction in cognitive load when these hearing aid features are activated. However, subjective ratings of listening effort did not fully reflect the extent of these improvements, underscoring the need for both objective and subjective measures in assessing listening effort. Self-reports alone may not capture the nuanced reductions in cognitive load observed through behavioural and neurophysiological data. The use of a realistic Ambisonics-simulated cafeteria environment further strengthens the ecological validity of the findings, offering insights applicable to everyday listening challenges faced by hearing aid users. Clinically, this study highlights the value of directional microphones in improving listening outcomes, while emphasising that patient expectations should be carefully managed, given that subjective perceptions of benefit may not always align with measurable improvements in performance. Ultimately, these findings support the adoption of advanced hearing aid technologies to alleviate listening effort, particularly in complex auditory environments.</p><p>Portions of this research were presented at the (i) 5th International Conference on Cognitive Hearing Science for Communication(CHS-COM), Link&#x000f6;ping, Sweden (2019)<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>, (ii) 45th Association for Research in Otolaryngology (ARO) Annual MidwinterMeeting, San Jose, CA (2022)<sup><xref ref-type="bibr" rid="CR69">69</xref></sup>, and (iii) 7th International Conference on Cognitive Hearing Science for Communication(CHS-COM), Link&#x000f6;ping, Sweden (2024)<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>.</p></sec><sec id="Sec17" sec-type="supplementary-material"><title>Supplementary Information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2025_95045_MOESM1_ESM.zip"><caption><p>Supplementary Information.</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-025-95045-3.</p></sec><ack><title>Acknowledgements</title><p>The authors gratefully acknowledge the following contributions: Dr.&#x000a0;Elizabeth F. Beach and Dr.&#x000a0;Bram Van Dun (National Acoustic Laboratories, Sydney, Australia) for their supervision during early stages of this research; Mr.&#x000a0;James Galloway and Mr.&#x000a0;Greg Stewart (NAL) for their work in collecting and analysing acoustic measures; Dr.&#x000a0;Ronny Ibrahim (NAL) for technical support with EEG data analysis; Mr.&#x000a0;Mark Seeto (NAL) for general statistical guidance; Dr.&#x000a0;Kelly Miles (NAL) for assistance with implementing dual-task methodologies; and Mr.&#x000a0;Paul Jevelle (NAL) for data collection during initial pilot studies. The authors also appreciate the valuable contributions from collaborators from Sonova AG (St&#x000e4;fa, Switzerland), including Dr.&#x000a0;Stefan Launer, Dr.&#x000a0;Mathias Latzel, Dr.&#x000a0;Matthias Keller, Dr.&#x000a0;Charlotte Vercammen, Ms.&#x000a0;Juliane Raether, and Dr.&#x000a0;Peter Derleth. This research was supported by the Ram&#x000f3;n y Cajal Fellowship (RYC-2022-037875-I) awarded to Dr.&#x000a0;Joaqu&#x000ed;n T. Valderrama by the Spanish Ministry of Science and Innovation (MCIU/AEI/10.13039/501100011033) and the European Social Fund Plus (FSE+); by Sonova AG (St&#x000e4;fa, Switzerland); and by the Australian Government Department of Health.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>J.T.V., J.M., N.C.H., and B.E. conceptualised and designed the experiments. J.T.V. and A.W. implemented the methodologies. A.W. recruited participants and collected data. J.T.V. and J.M. analysed data. J.T.V., J.M., and B.E. managed the administrative elements of the project. J.T.V. and B.E. secured the funding. J.T.V. wrote the original draft of the manuscript. All authors reviewed and approved the final version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>All raw data, processed data, programming scripts, and other research materials supporting the findings of this study are available upon reasonable request. Correspondence and requests for materials should be addressed to J.T.V.</p></notes><notes><title>Declaration</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par47">N.C.H. is employed by Sonova AG. He declares that this affiliation did not influence the study design, data collection and analysis, decision to publish, or preparation of the manuscript. Sonova AG provided funding for this work but had no additional role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">McGarrigle, R. et al. Listening effort and fatigue: What exactly are we measuring? A British Society of Audiology Cognition in Hearing Special Interest Group &#x0201c;white paper&#x0201d;. <italic>Int. J. Audiol.</italic><bold>53</bold>, 433&#x02013;445. 10.3109/14992027.2014.890296 (2014).</mixed-citation></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Eckert</surname><given-names>MA</given-names></name><name><surname>Teubner-Rhodes</surname><given-names>S</given-names></name><name><surname>Vaden</surname><given-names>KI</given-names><suffix>Jr</suffix></name></person-group><article-title>Is listening in noise worth it? The neurobiology of speech perception in challenging listening conditions</article-title><source>Ear Hear.</source><year>2017</year><volume>38</volume><fpage>725</fpage><lpage>732</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000300</pub-id></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Eckert, M. A., Teubner-Rhodes, S. &#x00026; Vaden, K. I. Jr. Is listening in noise worth it? The neurobiology of speech perception in challenging listening conditions. <italic>Ear Hear.</italic><bold>38</bold>, 725&#x02013;732. 10.1097/AUD.0000000000000300 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>MK</given-names></name><etal/></person-group><article-title>Hearing impairment and cognitive energy: The framework for understanding effortful listening (FUEL)</article-title><source>Ear Hear.</source><year>2016</year><volume>37</volume><fpage>5S</fpage><lpage>27S</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000312</pub-id><pub-id pub-id-type="pmid">27355771</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Pichora-Fuller, M. K. et al. Hearing impairment and cognitive energy: The framework for understanding effortful listening (FUEL). <italic>Ear Hear.</italic><bold>37</bold>, 5S-27S. 10.1097/AUD.0000000000000312 (2016).<pub-id pub-id-type="pmid">27355771</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Hornsby</surname><given-names>BWY</given-names></name></person-group><article-title>The effects of hearing aid use on listening effort and mental fatigue associated with sustained speech processing demands</article-title><source>Ear Hear.</source><year>2013</year><volume>34</volume><fpage>523</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e31828003d8</pub-id><pub-id pub-id-type="pmid">23426091</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Hornsby, B. W. Y. The effects of hearing aid use on listening effort and mental fatigue associated with sustained speech processing demands. <italic>Ear Hear.</italic><bold>34</bold>, 523&#x02013;534. 10.1097/AUD.0b013e31828003d8 (2013).<pub-id pub-id-type="pmid">23426091</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Hornsby</surname><given-names>BWY</given-names></name><name><surname>Naylor</surname><given-names>G</given-names></name><name><surname>Bess</surname><given-names>FH</given-names></name></person-group><article-title>A taxonomy of fatigue concepts and their relation to hearing loss</article-title><source>Ear Hear.</source><year>2016</year><volume>37</volume><fpage>136S</fpage><lpage>144S</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000289</pub-id><pub-id pub-id-type="pmid">27355763</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Hornsby, B. W. Y., Naylor, G. &#x00026; Bess, F. H. A taxonomy of fatigue concepts and their relation to hearing loss. <italic>Ear Hear.</italic><bold>37</bold>, 136S-144S. 10.1097/AUD.0000000000000289 (2016).<pub-id pub-id-type="pmid">27355763</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Contrera</surname><given-names>KJ</given-names></name><etal/></person-group><article-title>Association of hearing impairment and anxiety in older adults</article-title><source>J. Aging Health</source><year>2017</year><volume>29</volume><fpage>172</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1177/0898264316634571</pub-id><pub-id pub-id-type="pmid">26916793</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Contrera, K. J. et al. Association of hearing impairment and anxiety in older adults. <italic>J. Aging Health</italic><bold>29</bold>, 172&#x02013;184. 10.1177/0898264316634571 (2017).<pub-id pub-id-type="pmid">26916793</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Mealings</surname><given-names>K</given-names></name><etal/></person-group><article-title>Discovering the unmet needs of people with difficulties understanding speech in noise and a normal or near-normal audiogram</article-title><source>Am. J. Audiol.</source><year>2020</year><volume>29</volume><fpage>329</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1044/2020_AJA-19-00093</pub-id><pub-id pub-id-type="pmid">32463705</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Mealings, K. et al. Discovering the unmet needs of people with difficulties understanding speech in noise and a normal or near-normal audiogram. <italic>Am. J. Audiol.</italic><bold>29</bold>, 329&#x02013;355. 10.1044/2020_AJA-19-00093 (2020).<pub-id pub-id-type="pmid">32463705</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Mick</surname><given-names>P</given-names></name><name><surname>Kawachi</surname><given-names>I</given-names></name><name><surname>Lin</surname><given-names>FR</given-names></name></person-group><article-title>The association between hearing loss and social isolation in older adults</article-title><source>Otolaryngol.-Head Neck Surg.</source><year>2014</year><volume>150</volume><fpage>378</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1177/0194599813518021</pub-id><pub-id pub-id-type="pmid">24384545</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Mick, P., Kawachi, I. &#x00026; Lin, F. R. The association between hearing loss and social isolation in older adults. <italic>Otolaryngol.-Head Neck Surg.</italic><bold>150</bold>, 378&#x02013;384. 10.1177/0194599813518021 (2014).<pub-id pub-id-type="pmid">24384545</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Livingston</surname><given-names>G</given-names></name><etal/></person-group><article-title>Dementia prevention, intervention, and care: 2020 report of the Lancet Commission</article-title><source>Lancet</source><year>2020</year><volume>396</volume><fpage>413</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(20)30367-6</pub-id><pub-id pub-id-type="pmid">32738937</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Livingston, G. et al. Dementia prevention, intervention, and care: 2020 report of the Lancet Commission. <italic>Lancet</italic><bold>396</bold>, 413&#x02013;446. 10.1016/S0140-6736(20)30367-6 (2020).<pub-id pub-id-type="pmid">32738937</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>FR</given-names></name><etal/></person-group><article-title>Hearing loss and cognitive decline in older adults</article-title><source>JAMA Intern. Med.</source><year>2013</year><volume>173</volume><fpage>293</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1001/jamainternmed.2013.1868</pub-id><pub-id pub-id-type="pmid">23337978</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Lin, F. R. et al. Hearing loss and cognitive decline in older adults. <italic>JAMA Intern. Med.</italic><bold>173</bold>, 293&#x02013;299. 10.1001/jamainternmed.2013.1868 (2013).<pub-id pub-id-type="pmid">23337978</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>JM</given-names></name></person-group><article-title>Hearing aid technology to improve speech intelligibility in noise</article-title><source>Semin. Hear.</source><year>2021</year><volume>42</volume><fpage>175</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1055/s-0041-1735174</pub-id><pub-id pub-id-type="pmid">34594083</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Alexander, J. M. Hearing aid technology to improve speech intelligibility in noise. <italic>Semin. Hear.</italic><bold>42</bold>, 175&#x02013;185. 10.1055/s-0041-1735174 (2021).<pub-id pub-id-type="pmid">34594083</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Edwards</surname><given-names>B</given-names></name></person-group><article-title>Emerging technologies, market segments, and MarkeTrak 10 insights in hearing health technology</article-title><source>Semin. Hear.</source><year>2020</year><volume>41</volume><fpage>37</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1055/s-0040-1701244</pub-id><pub-id pub-id-type="pmid">32047347</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Edwards, B. Emerging technologies, market segments, and MarkeTrak 10 insights in hearing health technology. <italic>Semin. Hear.</italic><bold>41</bold>, 37&#x02013;54. 10.1055/s-0040-1701244 (2020).<pub-id pub-id-type="pmid">32047347</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Edwards</surname><given-names>B</given-names></name></person-group><article-title>Beyond amplification: Signal processing techniques for improving speech intelligibility in noise with hearing aids</article-title><source>Semin. Hear.</source><year>2000</year><volume>21</volume><fpage>137</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1055/s-2000-7312</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Edwards, B. Beyond amplification: Signal processing techniques for improving speech intelligibility in noise with hearing aids. <italic>Semin. Hear.</italic><bold>21</bold>, 137&#x02013;156. 10.1055/s-2000-7312 (2000).</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Ricketts</surname><given-names>TA</given-names></name></person-group><article-title>Directional hearing aids</article-title><source>Trends Amplif.</source><year>2001</year><volume>5</volume><fpage>139</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1177/108471380100500401</pub-id><pub-id pub-id-type="pmid">25425906</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Ricketts, T. A. Directional hearing aids. <italic>Trends Amplif.</italic><bold>5</bold>, 139&#x02013;176. 10.1177/108471380100500401 (2001).<pub-id pub-id-type="pmid">25425906</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Bentler</surname><given-names>RA</given-names></name></person-group><article-title>Effectiveness of directional microphones and noise reduction schemes in hearing aids: A systematic review of the evidence</article-title><source>J. Am. Acad. Audiol.</source><year>2005</year><volume>16</volume><fpage>473</fpage><lpage>484</lpage><pub-id pub-id-type="doi">10.3766/jaaa.16.7.7</pub-id><pub-id pub-id-type="pmid">16295234</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Bentler, R. A. Effectiveness of directional microphones and noise reduction schemes in hearing aids: A systematic review of the evidence. <italic>J. Am. Acad. Audiol.</italic><bold>16</bold>, 473&#x02013;484. 10.3766/jaaa.16.7.7 (2005).<pub-id pub-id-type="pmid">16295234</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Desjardins</surname><given-names>JL</given-names></name></person-group><article-title>The effects of hearing aid directional microphone and noise reduction processing on listening effort in older adults with hearing loss</article-title><source>J. Am. Acad. Audiol.</source><year>2016</year><volume>27</volume><fpage>29</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.3766/jaaa.15030</pub-id><pub-id pub-id-type="pmid">26809324</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Desjardins, J. L. The effects of hearing aid directional microphone and noise reduction processing on listening effort in older adults with hearing loss. <italic>J. Am. Acad. Audiol.</italic><bold>27</bold>, 29&#x02013;41. 10.3766/jaaa.15030 (2016).<pub-id pub-id-type="pmid">26809324</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Valente</surname><given-names>M</given-names></name><name><surname>Fabry</surname><given-names>DA</given-names></name><name><surname>Potts</surname><given-names>LG</given-names></name></person-group><article-title>Recognition of speech in noise with hearing aids using dual microphones</article-title><source>J. Am. Acad. Audiol.</source><year>1995</year><volume>6</volume><fpage>440</fpage><lpage>449</lpage><pub-id pub-id-type="pmid">8580504</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Valente, M., Fabry, D. A. &#x00026; Potts, L. G. Recognition of speech in noise with hearing aids using dual microphones. <italic>J. Am. Acad. Audiol.</italic><bold>6</bold>, 440&#x02013;449 (1995).<pub-id pub-id-type="pmid">8580504</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000f6;nnberg</surname><given-names>J</given-names></name><etal/></person-group><article-title>The ease of language understanding (ELU) model: Theoretical, empirical, and clinical advances</article-title><source>Front. Syst. Neurosci.</source><year>2013</year><volume>7</volume><fpage>31</fpage><pub-id pub-id-type="doi">10.3389/fnsys.2013.00031</pub-id><pub-id pub-id-type="pmid">23874273</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">R&#x000f6;nnberg, J. et al. The ease of language understanding (ELU) model: Theoretical, empirical, and clinical advances. <italic>Front. Syst. Neurosci.</italic><bold>7</bold>, 31. 10.3389/fnsys.2013.00031 (2013).<pub-id pub-id-type="pmid">23874273</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000f6;nnberg</surname><given-names>J</given-names></name><name><surname>Holmer</surname><given-names>E</given-names></name><name><surname>Rudner</surname><given-names>M</given-names></name></person-group><article-title>Cognitive hearing science and ease of language understanding</article-title><source>Int. J. Audiol.</source><year>2019</year><volume>58</volume><fpage>247</fpage><lpage>261</lpage><pub-id pub-id-type="doi">10.1080/14992027.2018.1551631</pub-id><pub-id pub-id-type="pmid">30714435</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">R&#x000f6;nnberg, J., Holmer, E. &#x00026; Rudner, M. Cognitive hearing science and ease of language understanding. <italic>Int. J. Audiol.</italic><bold>58</bold>, 247&#x02013;261. 10.1080/14992027.2018.1551631 (2019).<pub-id pub-id-type="pmid">30714435</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000f6;nnberg</surname><given-names>J</given-names></name><name><surname>Signoret</surname><given-names>C</given-names></name><name><surname>Andin</surname><given-names>J</given-names></name><name><surname>Holmer</surname><given-names>E</given-names></name></person-group><article-title>The cognitive hearing science perspective on perceiving, understanding, and remembering language: The ELU model</article-title><source>Front. Psychol.</source><year>2022</year><volume>13</volume><fpage>967260</fpage><pub-id pub-id-type="doi">10.3389/fpsyg.2022.967260</pub-id><pub-id pub-id-type="pmid">36118435</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">R&#x000f6;nnberg, J., Signoret, C., Andin, J. &#x00026; Holmer, E. The cognitive hearing science perspective on perceiving, understanding, and remembering language: The ELU model. <italic>Front. Psychol.</italic><bold>13</bold>, 967260. 10.3389/fpsyg.2022.967260 (2022).<pub-id pub-id-type="pmid">36118435</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Gagn&#x000e9;</surname><given-names>J-P</given-names></name><name><surname>Besser</surname><given-names>J</given-names></name><name><surname>Lemke</surname><given-names>U</given-names></name></person-group><article-title>Behavioral assessment of listening effort using a dual-task paradigm: A review</article-title><source>Trends Hear.</source><year>2017</year><volume>21</volume><fpage>2331216516687287</fpage><pub-id pub-id-type="doi">10.1177/2331216516687287</pub-id><pub-id pub-id-type="pmid">28091178</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Gagn&#x000e9;, J.-P., Besser, J. &#x00026; Lemke, U. Behavioral assessment of listening effort using a dual-task paradigm: A review. <italic>Trends Hear.</italic><bold>21</bold>, 2331216516687287. 10.1177/2331216516687287 (2017).<pub-id pub-id-type="pmid">28091178</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Schiller</surname><given-names>IS</given-names></name><etal/></person-group><article-title>A lecturer&#x02019;s voice quality and its effect on memory, listening effort, and perception in a VR environment</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>12407</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-63097-6</pub-id><pub-id pub-id-type="pmid">38811832</pub-id>
</element-citation><mixed-citation id="mc-CR22" publication-type="journal">Schiller, I. S. et al. A lecturer&#x02019;s voice quality and its effect on memory, listening effort, and perception in a VR environment. <italic>Sci. Rep.</italic><bold>14</bold>, 12407. 10.1038/s41598-024-63097-6 (2024).<pub-id pub-id-type="pmid">38811832</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Dimitrijevic</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>ML</given-names></name><name><surname>Kadis</surname><given-names>DS</given-names></name><name><surname>Moore</surname><given-names>DR</given-names></name></person-group><article-title>Neural indices of listening effort in noisy environments</article-title><source>Sci. Rep.</source><year>2019</year><volume>9</volume><fpage>11278</fpage><pub-id pub-id-type="doi">10.1038/s41598-019-47643-1</pub-id><pub-id pub-id-type="pmid">31375712</pub-id>
</element-citation><mixed-citation id="mc-CR23" publication-type="journal">Dimitrijevic, A., Smith, M. L., Kadis, D. S. &#x00026; Moore, D. R. Neural indices of listening effort in noisy environments. <italic>Sci. Rep.</italic><bold>9</bold>, 11278. 10.1038/s41598-019-47643-1 (2019).<pub-id pub-id-type="pmid">31375712</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Ala</surname><given-names>TS</given-names></name><etal/></person-group><article-title>An exploratory study of EEG alpha oscillation and pupil dilation in hearing-aid users during effortful listening to continuous speech</article-title><source>PLoS One</source><year>2020</year><volume>15</volume><fpage>e0235782</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0235782</pub-id><pub-id pub-id-type="pmid">32649733</pub-id>
</element-citation><mixed-citation id="mc-CR24" publication-type="journal">Ala, T. S. et al. An exploratory study of EEG alpha oscillation and pupil dilation in hearing-aid users during effortful listening to continuous speech. <italic>PLoS One</italic><bold>15</bold>, e0235782. 10.1371/journal.pone.0235782 (2020).<pub-id pub-id-type="pmid">32649733</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Klimesch</surname><given-names>W</given-names></name></person-group><article-title>Alpha-band oscillations, attention, and controlled access to stored information</article-title><source>Trends Cognit. Sci.</source><year>2012</year><volume>16</volume><fpage>606</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.10.007</pub-id><pub-id pub-id-type="pmid">23141428</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Klimesch, W. Alpha-band oscillations, attention, and controlled access to stored information. <italic>Trends Cognit. Sci.</italic><bold>16</bold>, 606&#x02013;617. 10.1016/j.tics.2012.10.007 (2012).<pub-id pub-id-type="pmid">23141428</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Wendt</surname><given-names>D</given-names></name><name><surname>Hietkamp</surname><given-names>RK</given-names></name><name><surname>Lunner</surname><given-names>T</given-names></name></person-group><article-title>Impact of noise and noise reduction on processing effort: A pupillometry study</article-title><source>Ear Hear.</source><year>2017</year><volume>38</volume><fpage>690</fpage><lpage>700</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000454</pub-id><pub-id pub-id-type="pmid">28640038</pub-id>
</element-citation><mixed-citation id="mc-CR26" publication-type="journal">Wendt, D., Hietkamp, R. K. &#x00026; Lunner, T. Impact of noise and noise reduction on processing effort: A pupillometry study. <italic>Ear Hear.</italic><bold>38</bold>, 690&#x02013;700. 10.1097/AUD.0000000000000454 (2017).<pub-id pub-id-type="pmid">28640038</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Ohlenforst</surname><given-names>B</given-names></name><etal/></person-group><article-title>Impact of stimulus-related factors and hearing impairment on listening effort as indicated by pupil dilation</article-title><source>Hear. Res.</source><year>2017</year><volume>351</volume><fpage>68</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2017.05.012</pub-id><pub-id pub-id-type="pmid">28622894</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Ohlenforst, B. et al. Impact of stimulus-related factors and hearing impairment on listening effort as indicated by pupil dilation. <italic>Hear. Res.</italic><bold>351</bold>, 68&#x02013;79. 10.1016/j.heares.2017.05.012 (2017).<pub-id pub-id-type="pmid">28622894</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Impact of SNR, peripheral auditory sensitivity, and central cognitive profile on the psychometric relation between pupillary response and speech performance in CI users</article-title><source>Front. Neurosci.</source><year>2023</year><volume>17</volume><fpage>1307777</fpage><pub-id pub-id-type="doi">10.3389/fnins.2023.1307777</pub-id><pub-id pub-id-type="pmid">38188029</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Zhang, Y. et al. Impact of SNR, peripheral auditory sensitivity, and central cognitive profile on the psychometric relation between pupillary response and speech performance in CI users. <italic>Front. Neurosci.</italic><bold>17</bold>, 1307777. 10.3389/fnins.2023.1307777 (2023).<pub-id pub-id-type="pmid">38188029</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Zekveld</surname><given-names>AA</given-names></name><name><surname>Kramer</surname><given-names>SE</given-names></name><name><surname>Festen</surname><given-names>JM</given-names></name></person-group><article-title>Pupil response as an indication of effortful listening: The influence of sentence intelligibility</article-title><source>Ear Hear.</source><year>2010</year><volume>31</volume><fpage>480</fpage><lpage>490</lpage><pub-id pub-id-type="doi">10.1097/AUD.0b013e3181d4f251</pub-id><pub-id pub-id-type="pmid">20588118</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Zekveld, A. A., Kramer, S. E. &#x00026; Festen, J. M. Pupil response as an indication of effortful listening: The influence of sentence intelligibility. <italic>Ear Hear.</italic><bold>31</bold>, 480&#x02013;490. 10.1097/AUD.0b013e3181d4f251 (2010).<pub-id pub-id-type="pmid">20588118</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Mackersie</surname><given-names>CL</given-names></name><name><surname>Cones</surname><given-names>H</given-names></name></person-group><article-title>Subjective and psychophysiological indexes of listening effort in a competing-talker task</article-title><source>J. Am. Acad. Audiol.</source><year>2011</year><volume>22</volume><fpage>113</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.3766/jaaa.22.2.6</pub-id><pub-id pub-id-type="pmid">21463566</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">Mackersie, C. L. &#x00026; Cones, H. Subjective and psychophysiological indexes of listening effort in a competing-talker task. <italic>J. Am. Acad. Audiol.</italic><bold>22</bold>, 113&#x02013;122. 10.3766/jaaa.22.2.6 (2011).<pub-id pub-id-type="pmid">21463566</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Schulte, M. <italic>Listening effort scaling and preference rating for hearing aid evaluation</italic> In <italic>Workshop Hearing Screening and Technology</italic> (Brussels, 2009).</mixed-citation></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Desjardins</surname><given-names>JL</given-names></name><name><surname>Doherty</surname><given-names>KA</given-names></name></person-group><article-title>The effect of hearing aid noise reduction on listening effort in hearing-impaired adults</article-title><source>Ear Hear.</source><year>2014</year><volume>35</volume><fpage>600</fpage><lpage>610</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000028</pub-id><pub-id pub-id-type="pmid">24622352</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">Desjardins, J. L. &#x00026; Doherty, K. A. The effect of hearing aid noise reduction on listening effort in hearing-impaired adults. <italic>Ear Hear.</italic><bold>35</bold>, 600&#x02013;610. 10.1097/AUD.0000000000000028 (2014).<pub-id pub-id-type="pmid">24622352</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Bernarding</surname><given-names>C</given-names></name><name><surname>Strauss</surname><given-names>DJ</given-names></name><name><surname>Hannemann</surname><given-names>R</given-names></name><name><surname>Seidler</surname><given-names>H</given-names></name><name><surname>Corona-Strauss</surname><given-names>FI</given-names></name></person-group><article-title>Neurodynamic evaluation of hearing aid features using EEG correlates of listening effort</article-title><source>Cognit. Neurodyn.</source><year>2017</year><volume>11</volume><fpage>203</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1007/s11571-017-9425-5</pub-id><pub-id pub-id-type="pmid">28559951</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">Bernarding, C., Strauss, D. J., Hannemann, R., Seidler, H. &#x00026; Corona-Strauss, F. I. Neurodynamic evaluation of hearing aid features using EEG correlates of listening effort. <italic>Cognit. Neurodyn.</italic><bold>11</bold>, 203&#x02013;215. 10.1007/s11571-017-9425-5 (2017).<pub-id pub-id-type="pmid">28559951</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Bentler</surname><given-names>RA</given-names></name><name><surname>Chiou</surname><given-names>L-K</given-names></name></person-group><article-title>Digital noise reduction: An overview</article-title><source>Trends Amplif.</source><year>2006</year><volume>10</volume><fpage>67</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1177/1084713806289514</pub-id><pub-id pub-id-type="pmid">16959731</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">Bentler, R. A. &#x00026; Chiou, L.-K. Digital noise reduction: An overview. <italic>Trends Amplif.</italic><bold>10</bold>, 67&#x02013;82. 10.1177/1084713806289514 (2006).<pub-id pub-id-type="pmid">16959731</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y-H</given-names></name><etal/></person-group><article-title>Measuring listening effort: Driving simulator versus simple dual-task paradigm</article-title><source>Ear Hear.</source><year>2014</year><volume>35</volume><fpage>623</fpage><lpage>632</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000000079</pub-id><pub-id pub-id-type="pmid">25083599</pub-id>
</element-citation><mixed-citation id="mc-CR35" publication-type="journal">Wu, Y.-H. et al. Measuring listening effort: Driving simulator versus simple dual-task paradigm. <italic>Ear Hear.</italic><bold>35</bold>, 623&#x02013;632. 10.1097/AUD.0000000000000079 (2014).<pub-id pub-id-type="pmid">25083599</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name><surname>Winneke</surname><given-names>AH</given-names></name><name><surname>Schulte</surname><given-names>M</given-names></name><name><surname>Vormann</surname><given-names>M</given-names></name><name><surname>Latzel</surname><given-names>M</given-names></name></person-group><article-title>Effect of directional microphone technology in hearing aids on neural correlates of listening and memory effort: An electroencephalographic study</article-title><source>Trends Hear.</source><year>2020</year><volume>24</volume><fpage>2331216520948410</fpage><pub-id pub-id-type="doi">10.1177/2331216520948410</pub-id><pub-id pub-id-type="pmid">32833586</pub-id>
</element-citation><mixed-citation id="mc-CR36" publication-type="journal">Winneke, A. H., Schulte, M., Vormann, M. &#x00026; Latzel, M. Effect of directional microphone technology in hearing aids on neural correlates of listening and memory effort: An electroencephalographic study. <italic>Trends Hear.</italic><bold>24</bold>, 2331216520948410. 10.1177/2331216520948410 (2020).<pub-id pub-id-type="pmid">32833586</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>Nasreddine</surname><given-names>ZS</given-names></name><etal/></person-group><article-title>The Montreal Cognitive Assessment, MoCA: A brief screening tool for mild cognitive impairment</article-title><source>J. Am. Geriatr. Soc.</source><year>2005</year><volume>53</volume><fpage>695</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1111/j.1532-5415.2005.53221.x</pub-id><pub-id pub-id-type="pmid">15817019</pub-id>
</element-citation><mixed-citation id="mc-CR37" publication-type="journal">Nasreddine, Z. S. et al. The Montreal Cognitive Assessment, MoCA: A brief screening tool for mild cognitive impairment. <italic>J. Am. Geriatr. Soc.</italic><bold>53</bold>, 695&#x02013;699. 10.1111/j.1532-5415.2005.53221.x (2005).<pub-id pub-id-type="pmid">15817019</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Weisser</surname><given-names>A</given-names></name><etal/></person-group><article-title>The Ambisonic Recordings of Typical Environments (ARTE) database</article-title><source>Acta Acust. United Acust.</source><year>2019</year><volume>105</volume><fpage>695</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.3813/AAA.919349</pub-id></element-citation><mixed-citation id="mc-CR38" publication-type="journal">Weisser, A. et al. The Ambisonic Recordings of Typical Environments (ARTE) database. <italic>Acta Acust. United Acust.</italic><bold>105</bold>, 695&#x02013;713. 10.3813/AAA.919349 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>H</given-names></name><etal/></person-group><article-title>Development and evaluation of a mixed gender, multi-talker matrix sentence test in Australian English</article-title><source>Int. J. Audiol.</source><year>2017</year><volume>56</volume><fpage>85</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1080/14992027.2016.1236415</pub-id><pub-id pub-id-type="pmid">27758153</pub-id>
</element-citation><mixed-citation id="mc-CR39" publication-type="journal">Kelly, H. et al. Development and evaluation of a mixed gender, multi-talker matrix sentence test in Australian English. <italic>Int. J. Audiol.</italic><bold>56</bold>, 85&#x02013;91. 10.1080/14992027.2016.1236415 (2017).<pub-id pub-id-type="pmid">27758153</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Alhanbali</surname><given-names>S</given-names></name><name><surname>Munro</surname><given-names>KJ</given-names></name><name><surname>Dawes</surname><given-names>P</given-names></name><name><surname>Perugia</surname><given-names>E</given-names></name><name><surname>Millman</surname><given-names>RE</given-names></name></person-group><article-title>Associations between pre-stimulus alpha power, hearing level and performance in a digits-in-noise task</article-title><source>Int. J. Audiol.</source><year>2022</year><volume>61</volume><fpage>197</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1080/14992027.2021.1899314</pub-id><pub-id pub-id-type="pmid">33794733</pub-id>
</element-citation><mixed-citation id="mc-CR40" publication-type="journal">Alhanbali, S., Munro, K. J., Dawes, P., Perugia, E. &#x00026; Millman, R. E. Associations between pre-stimulus alpha power, hearing level and performance in a digits-in-noise task. <italic>Int. J. Audiol.</italic><bold>61</bold>, 197&#x02013;204. 10.1080/14992027.2021.1899314 (2022).<pub-id pub-id-type="pmid">33794733</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The psychophysics toolbox</article-title><source>Spatial Vis.</source><year>1997</year><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></element-citation><mixed-citation id="mc-CR41" publication-type="journal">Brainard, D. H. The psychophysics toolbox. <italic>Spatial Vis.</italic><bold>10</bold>, 433&#x02013;436. 10.1163/156856897X00357 (1997).</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title><source>Spatial Vis.</source><year>1997</year><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1163/156856897X00366</pub-id></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Pelli, D. G. The VideoToolbox software for visual psychophysics: Transforming numbers into movies. <italic>Spatial Vis.</italic><bold>10</bold>, 437&#x02013;442. 10.1163/156856897X00366 (1997).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Kleiner, M., Brainard, D. &#x00026; Pelli, D. What&#x02019;s new in psychtoolbox-3. <italic>Percept. ECVP &#x02018;07 Abstr. Suppl.</italic><bold>36</bold>, 14. 10.1177/03010066070360S101 (2007).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Killion, M. et al. Real-world performance of an ITE directional microphone. <italic> Hear. J.</italic><bold>51</bold>, 24&#x02013;39. <ext-link ext-link-type="uri" xlink:href="https://www.etymotic.com/wp-content/uploads/2021/05/erl-0038-1998.pdf">https://www.etymotic.com/wp-content/uploads/2021/05/erl-0038-1998.pdf</ext-link> (1998).</mixed-citation></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname><given-names>A</given-names></name><name><surname>Latzel</surname><given-names>M</given-names></name><name><surname>Holube</surname><given-names>I</given-names></name></person-group><article-title>Open versus closed hearing-aid fittings: A literature review of both fitting approaches</article-title><source>Trends Hear.</source><year>2016</year><volume>20</volume><fpage>2331216516631741</fpage><pub-id pub-id-type="doi">10.1177/2331216516631741</pub-id><pub-id pub-id-type="pmid">26879562</pub-id>
</element-citation><mixed-citation id="mc-CR45" publication-type="journal">Winkler, A., Latzel, M. &#x00026; Holube, I. Open versus closed hearing-aid fittings: A literature review of both fitting approaches. <italic>Trends Hear.</italic><bold>20</bold>, 2331216516631741. 10.1177/2331216516631741 (2016).<pub-id pub-id-type="pmid">26879562</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name><surname>Bolker</surname><given-names>BM</given-names></name><etal/></person-group><article-title>Generalized linear mixed models: A practical guide for ecology and evolution</article-title><source>Trends Ecol. Evolut.</source><year>2009</year><volume>24</volume><fpage>127</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2008.10.008</pub-id></element-citation><mixed-citation id="mc-CR46" publication-type="journal">Bolker, B. M. et al. Generalized linear mixed models: A practical guide for ecology and evolution. <italic>Trends Ecol. Evolut.</italic><bold>24</bold>, 127&#x02013;135. 10.1016/j.tree.2008.10.008 (2009).</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name><surname>Lilliefors</surname><given-names>HW</given-names></name></person-group><article-title>On the Kolmogorov-Smirnov test for normality with mean and variance unknown</article-title><source>J. Am. Stat. Assoc.</source><year>1967</year><volume>62</volume><fpage>399</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.2307/2283970</pub-id></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Lilliefors, H. W. On the Kolmogorov-Smirnov test for normality with mean and variance unknown. <italic>J. Am. Stat. Assoc.</italic><bold>62</bold>, 399&#x02013;402. 10.2307/2283970 (1967).</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="book"><person-group person-group-type="author"><name><surname>McCullagh</surname><given-names>P</given-names></name><name><surname>Nelder</surname><given-names>JA</given-names></name></person-group><source>Generalized Linear Models</source><year>1989</year><edition>2</edition><publisher-name>Chapman &#x00026; Hall / CRC Press</publisher-name></element-citation><mixed-citation id="mc-CR48" publication-type="book">McCullagh, P. &#x00026; Nelder, J. A. <italic>Generalized Linear Models</italic> 2nd edn. (Chapman &#x00026; Hall / CRC Press, 1989).</mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Comput. Intell. Neurosci.</source><year>2011</year><volume>2011</volume><fpage>156869</fpage><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id><pub-id pub-id-type="pmid">21253357</pub-id>
</element-citation><mixed-citation id="mc-CR49" publication-type="journal">Oostenveld, R., Fries, P., Maris, E. &#x00026; Schoffelen, J. M. FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data. <italic>Comput. Intell. Neurosci.</italic><bold>2011</bold>, 156869. 10.1155/2011/156869 (2011).<pub-id pub-id-type="pmid">21253357</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><article-title>EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>J. Neurosci. Methods</source><year>2004</year><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id>
</element-citation><mixed-citation id="mc-CR50" publication-type="journal">Delorme, A. &#x00026; Makeig, S. EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis. <italic>J. Neurosci. Methods</italic><bold>134</bold>, 9&#x02013;21. 10.1016/j.jneumeth.2003.10.009 (2004).<pub-id pub-id-type="pmid">15102499</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>AJ</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><article-title>An information-maximization approach to blind separation and blind deconvolution</article-title><source>Neural Comput.</source><year>2019</year><volume>7</volume><fpage>1129</fpage><lpage>1159</lpage><pub-id pub-id-type="doi">10.1162/neco.1995.7.6.1129</pub-id></element-citation><mixed-citation id="mc-CR51" publication-type="journal">Bell, A. J. &#x00026; Sejnowski, T. J. An information-maximization approach to blind separation and blind deconvolution. <italic>Neural Comput.</italic><bold>7</bold>, 1129&#x02013;1159. 10.1162/neco.1995.7.6.1129 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>J. Neurosci. Methods</source><year>2007</year><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id><pub-id pub-id-type="pmid">17517438</pub-id>
</element-citation><mixed-citation id="mc-CR52" publication-type="journal">Maris, E. &#x00026; Oostenveld, R. Nonparametric statistical testing of EEG- and MEG-data. <italic>J. Neurosci. Methods</italic><bold>164</bold>, 177&#x02013;190. 10.1016/j.jneumeth.2007.03.024 (2007).<pub-id pub-id-type="pmid">17517438</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR53"><label>53.</label><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name><surname>Ohlenforst</surname><given-names>B</given-names></name><etal/></person-group><article-title>Impact of SNR, masker type and noise reduction processing on sentence recognition performance and listening effort as indicated by the pupil dilation response</article-title><source>Hear. Res.</source><year>2018</year><volume>365</volume><fpage>90</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2018.05.003</pub-id><pub-id pub-id-type="pmid">29779607</pub-id>
</element-citation><mixed-citation id="mc-CR53" publication-type="journal">Ohlenforst, B. et al. Impact of SNR, masker type and noise reduction processing on sentence recognition performance and listening effort as indicated by the pupil dilation response. <italic>Hear. Res.</italic><bold>365</bold>, 90&#x02013;99. 10.1016/j.heares.2018.05.003 (2018).<pub-id pub-id-type="pmid">29779607</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR54"><label>54.</label><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Gelfand</surname><given-names>J</given-names></name><name><surname>Kounios</surname><given-names>J</given-names></name><name><surname>Lisman</surname><given-names>JE</given-names></name></person-group><article-title>Oscillations in the alpha band (9&#x02013;12 Hz) increase with memory load during retention in a short-term memory task</article-title><source>Cereb. Cortex</source><year>2002</year><volume>12</volume><fpage>877</fpage><lpage>882</lpage><pub-id pub-id-type="doi">10.1093/cercor/12.8.877</pub-id><pub-id pub-id-type="pmid">12122036</pub-id>
</element-citation><mixed-citation id="mc-CR54" publication-type="journal">Jensen, O., Gelfand, J., Kounios, J. &#x00026; Lisman, J. E. Oscillations in the alpha band (9&#x02013;12 Hz) increase with memory load during retention in a short-term memory task. <italic>Cereb. Cortex</italic><bold>12</bold>, 877&#x02013;882. 10.1093/cercor/12.8.877 (2002).<pub-id pub-id-type="pmid">12122036</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR55"><label>55.</label><citation-alternatives><element-citation id="ec-CR55" publication-type="journal"><person-group person-group-type="author"><name><surname>Tulhadar</surname><given-names>AM</given-names></name><etal/></person-group><article-title>Parieto-occipital sources account for the increase in alpha activity with working memory load</article-title><source>Hum. Brain Mapp.</source><year>2007</year><volume>28</volume><fpage>785</fpage><lpage>792</lpage><pub-id pub-id-type="doi">10.1002/hbm.20306</pub-id><pub-id pub-id-type="pmid">17266103</pub-id>
</element-citation><mixed-citation id="mc-CR55" publication-type="journal">Tulhadar, A. M. et al. Parieto-occipital sources account for the increase in alpha activity with working memory load. <italic>Hum. Brain Mapp.</italic><bold>28</bold>, 785&#x02013;792. 10.1002/hbm.20306 (2007).<pub-id pub-id-type="pmid">17266103</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR56"><label>56.</label><citation-alternatives><element-citation id="ec-CR56" publication-type="journal"><person-group person-group-type="author"><name><surname>Meltzer</surname><given-names>JA</given-names></name><etal/></person-group><article-title>Effects of working memory load on oscillatory power in human intracranial EEG</article-title><source>Cereb. Cortex</source><year>2008</year><volume>18</volume><fpage>1843</fpage><lpage>1855</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm213</pub-id><pub-id pub-id-type="pmid">18056698</pub-id>
</element-citation><mixed-citation id="mc-CR56" publication-type="journal">Meltzer, J. A. et al. Effects of working memory load on oscillatory power in human intracranial EEG. <italic>Cereb. Cortex</italic><bold>18</bold>, 1843&#x02013;1855. 10.1093/cercor/bhm213 (2008).<pub-id pub-id-type="pmid">18056698</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR57"><label>57.</label><citation-alternatives><element-citation id="ec-CR57" publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>W&#x000f6;stmann</surname><given-names>M</given-names></name><name><surname>Hellbernd</surname><given-names>N</given-names></name><name><surname>Wilsch</surname><given-names>A</given-names></name><name><surname>Maess</surname><given-names>B</given-names></name></person-group><article-title>Adverse listening conditions and memory load drive a common alpha oscillatory network</article-title><source>J. Neurosci.</source><year>2012</year><volume>32</volume><fpage>12376</fpage><lpage>12383</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4908-11.2012</pub-id><pub-id pub-id-type="pmid">22956828</pub-id>
</element-citation><mixed-citation id="mc-CR57" publication-type="journal">Obleser, J., W&#x000f6;stmann, M., Hellbernd, N., Wilsch, A. &#x00026; Maess, B. Adverse listening conditions and memory load drive a common alpha oscillatory network. <italic>J. Neurosci.</italic><bold>32</bold>, 12376&#x02013;12383. 10.1523/JNEUROSCI.4908-11.2012 (2012).<pub-id pub-id-type="pmid">22956828</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR58"><label>58.</label><citation-alternatives><element-citation id="ec-CR58" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Samuel</surname><given-names>AG</given-names></name></person-group><article-title>Perceptual learning of speech under optimal and adverse conditions</article-title><source>J. Exp. Psychol. Hum. Percept. Perform.</source><year>2014</year><volume>40</volume><fpage>200</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1037/a0033182</pub-id><pub-id pub-id-type="pmid">23815478</pub-id>
</element-citation><mixed-citation id="mc-CR58" publication-type="journal">Zhang, X. &#x00026; Samuel, A. G. Perceptual learning of speech under optimal and adverse conditions. <italic>J. Exp. Psychol. Hum. Percept. Perform.</italic><bold>40</bold>, 200&#x02013;217. 10.1037/a0033182 (2014).<pub-id pub-id-type="pmid">23815478</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR59"><label>59.</label><citation-alternatives><element-citation id="ec-CR59" publication-type="journal"><person-group person-group-type="author"><name><surname>Hervais-Adelman</surname><given-names>A</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name><name><surname>Carlyon</surname><given-names>RP</given-names></name></person-group><article-title>Perceptual learning of noise vocoded words: Effects of feedback and lexicality</article-title><source>J. Exp. Psychol. Hum. Percept. Perform.</source><year>2008</year><volume>34</volume><fpage>460</fpage><lpage>474</lpage><pub-id pub-id-type="doi">10.1037/0096-1523.34.2.460</pub-id><pub-id pub-id-type="pmid">18377182</pub-id>
</element-citation><mixed-citation id="mc-CR59" publication-type="journal">Hervais-Adelman, A., Davis, M. H., Johnsrude, I. S. &#x00026; Carlyon, R. P. Perceptual learning of noise vocoded words: Effects of feedback and lexicality. <italic>J. Exp. Psychol. Hum. Percept. Perform.</italic><bold>34</bold>, 460&#x02013;474. 10.1037/0096-1523.34.2.460 (2008).<pub-id pub-id-type="pmid">18377182</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Dosher, B.&#x000a0;A. &#x00026; Lu, Z.-L. Perceptual learning: Learning, memory, and models. In <italic>The Oxford Handbook of Human Memory, Two Volume Pack: Foundations and Applications</italic> (Kahana, M.&#x000a0;J. &#x00026; Wagner, A.&#x000a0;D. eds.) . 10.1093/oxfordhb/9780190917982.013.11 (Oxford University Press, 2024).</mixed-citation></ref><ref id="CR61"><label>61.</label><citation-alternatives><element-citation id="ec-CR61" publication-type="journal"><person-group person-group-type="author"><name><surname>Picou</surname><given-names>EM</given-names></name><name><surname>Ricketts</surname><given-names>TA</given-names></name><name><surname>Hornsby</surname><given-names>BWY</given-names></name></person-group><article-title>Visual cues and listening effort: Individual variability</article-title><source>J. Speech Lang. Hear. Res.</source><year>2011</year><volume>54</volume><fpage>1416</fpage><lpage>1430</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2011/10-0154)</pub-id><pub-id pub-id-type="pmid">21498576</pub-id>
</element-citation><mixed-citation id="mc-CR61" publication-type="journal">Picou, E. M., Ricketts, T. A. &#x00026; Hornsby, B. W. Y. Visual cues and listening effort: Individual variability. <italic>J. Speech Lang. Hear. Res.</italic><bold>54</bold>, 1416&#x02013;1430. 10.1044/1092-4388(2011/10-0154) (2011).<pub-id pub-id-type="pmid">21498576</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR62"><label>62.</label><citation-alternatives><element-citation id="ec-CR62" publication-type="journal"><person-group person-group-type="author"><name><surname>Sarampalis</surname><given-names>A</given-names></name><name><surname>Kalluri</surname><given-names>S</given-names></name><name><surname>Edwards</surname><given-names>B</given-names></name><name><surname>Hafter</surname><given-names>E</given-names></name></person-group><article-title>Objective measures of listening effort: Effects of background noise and noise reduction</article-title><source>J. Speech Lang. Hear. Res.</source><year>2009</year><volume>52</volume><fpage>1230</fpage><lpage>1240</lpage><pub-id pub-id-type="doi">10.1044/1092-4388(2009/08-0111)</pub-id><pub-id pub-id-type="pmid">19380604</pub-id>
</element-citation><mixed-citation id="mc-CR62" publication-type="journal">Sarampalis, A., Kalluri, S., Edwards, B. &#x00026; Hafter, E. Objective measures of listening effort: Effects of background noise and noise reduction. <italic>J. Speech Lang. Hear. Res.</italic><bold>52</bold>, 1230&#x02013;1240. 10.1044/1092-4388(2009/08-0111) (2009).<pub-id pub-id-type="pmid">19380604</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR63"><label>63.</label><citation-alternatives><element-citation id="ec-CR63" publication-type="journal"><person-group person-group-type="author"><name><surname>Picou</surname><given-names>EM</given-names></name><name><surname>Ricketts</surname><given-names>TA</given-names></name></person-group><article-title>Increasing motivation changes subjective reports of listening effort and choice of coping strategy</article-title><source>Int. J. Audiol.</source><year>2014</year><volume>53</volume><fpage>418</fpage><lpage>426</lpage><pub-id pub-id-type="doi">10.3109/14992027.2014.880814</pub-id><pub-id pub-id-type="pmid">24597604</pub-id>
</element-citation><mixed-citation id="mc-CR63" publication-type="journal">Picou, E. M. &#x00026; Ricketts, T. A. Increasing motivation changes subjective reports of listening effort and choice of coping strategy. <italic>Int. J. Audiol.</italic><bold>53</bold>, 418&#x02013;426. 10.3109/14992027.2014.880814 (2014).<pub-id pub-id-type="pmid">24597604</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR64"><label>64.</label><citation-alternatives><element-citation id="ec-CR64" publication-type="journal"><person-group person-group-type="author"><name><surname>Carolan</surname><given-names>PJ</given-names></name><name><surname>Heinrich</surname><given-names>A</given-names></name><name><surname>Munro</surname><given-names>KJ</given-names></name><name><surname>Millman</surname><given-names>RE</given-names></name></person-group><article-title>Quantifying the effects of motivation on listening effort: A systematic review and meta-analysis</article-title><source>Trends Hear.</source><year>2022</year><volume>26</volume><fpage>23312165211059982</fpage><pub-id pub-id-type="doi">10.1177/23312165211059982</pub-id><pub-id pub-id-type="pmid">35077257</pub-id>
</element-citation><mixed-citation id="mc-CR64" publication-type="journal">Carolan, P. J., Heinrich, A., Munro, K. J. &#x00026; Millman, R. E. Quantifying the effects of motivation on listening effort: A systematic review and meta-analysis. <italic>Trends Hear.</italic><bold>26</bold>, 23312165211059984. 10.1177/23312165211059982 (2022).<pub-id pub-id-type="pmid">35077257</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR65"><label>65.</label><citation-alternatives><element-citation id="ec-CR65" publication-type="journal"><person-group person-group-type="author"><name><surname>Miles</surname><given-names>KM</given-names></name><etal/></person-group><article-title>Development of the everyday conversational sentences in noise test</article-title><source>J. Acoust. Soc. Am.</source><year>2020</year><volume>147</volume><fpage>1562</fpage><lpage>1576</lpage><pub-id pub-id-type="doi">10.1121/10.0000780</pub-id><pub-id pub-id-type="pmid">32237858</pub-id>
</element-citation><mixed-citation id="mc-CR65" publication-type="journal">Miles, K. M. et al. Development of the everyday conversational sentences in noise test. <italic>J. Acoust. Soc. Am.</italic><bold>147</bold>, 1562&#x02013;1576. 10.1121/10.0000780 (2020).<pub-id pub-id-type="pmid">32237858</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR66"><label>66.</label><citation-alternatives><element-citation id="ec-CR66" publication-type="journal"><person-group person-group-type="author"><name><surname>Mealings</surname><given-names>K</given-names></name><etal/></person-group><article-title>Hearing aids reduce self-perceived difficulties in noise for listeners with normal audiograms</article-title><source>Ear Hear.</source><year>2024</year><volume>45</volume><fpage>151</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1097/AUD.0000000000001412</pub-id><pub-id pub-id-type="pmid">37553897</pub-id>
</element-citation><mixed-citation id="mc-CR66" publication-type="journal">Mealings, K. et al. Hearing aids reduce self-perceived difficulties in noise for listeners with normal audiograms. <italic>Ear Hear.</italic><bold>45</bold>, 151&#x02013;163. 10.1097/AUD.0000000000001412 (2024).<pub-id pub-id-type="pmid">37553897</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR67"><label>67.</label><citation-alternatives><element-citation id="ec-CR67" publication-type="journal"><person-group person-group-type="author"><name><surname>Valderrama</surname><given-names>JT</given-names></name><name><surname>Mejia</surname><given-names>J</given-names></name><name><surname>Wong</surname><given-names>A</given-names></name><name><surname>Chong-White</surname><given-names>N</given-names></name><name><surname>Edwards</surname><given-names>B</given-names></name></person-group><article-title>The value of headphone accommodations in Apple Airpods Pro for managing speech-in-noise hearing difficulties of individuals with normal audiograms</article-title><source>Int. J. Audiol.</source><year>2024</year><volume>63</volume><fpage>447</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1080/14992027.2023.2199442</pub-id><pub-id pub-id-type="pmid">37105144</pub-id>
</element-citation><mixed-citation id="mc-CR67" publication-type="journal">Valderrama, J. T., Mejia, J., Wong, A., Chong-White, N. &#x00026; Edwards, B. The value of headphone accommodations in Apple Airpods Pro for managing speech-in-noise hearing difficulties of individuals with normal audiograms. <italic>Int. J. Audiol.</italic><bold>63</bold>, 447&#x02013;457. 10.1080/14992027.2023.2199442 (2024).<pub-id pub-id-type="pmid">37105144</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">Valderrama, J.&#x000a0;T., Jevelle, P., Beechey, T., Miles, K. &#x00026; Bardy, F. Towards a combined behavioural and physiological measure of listening effort. In <italic>5th International Conference on Cognitive Hearing Science for Communication (CHS-COM)</italic> (Link&#x000f6;ping, 2019).</mixed-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Valderrama, J.&#x000a0;T. et al. The use of binaural beamforming to reduce listening effort. In <italic>45th Association for Research in Otolaryngology (ARO) Annual Midwinter Meeting 2022</italic> (2022).</mixed-citation></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="other">Valderrama, J.&#x000a0;T., Mejia, J., Wong, A., Herbert, N. &#x00026; Edwards, B. Reducing listening effort in a realistic sound environment using directional microphones: Insights from behavioural, neurophysiological and self-reported data. In <italic>7th International Conference on Cognitive Hearing Science for Communication (CHS-COM)</italic> (2024).</mixed-citation></ref></ref-list></back></article>