<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id><journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id><journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id><journal-title-group><journal-title>Frontiers in Neuroscience</journal-title></journal-title-group><issn pub-type="ppub">1662-4548</issn><issn pub-type="epub">1662-453X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC12098520</article-id><article-id pub-id-type="doi">10.3389/fnins.2025.1501511</article-id><article-categories><subj-group subj-group-type="heading"><subject>Neuroscience</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>A sleep staging model based on adversarial domain generalized residual attention network</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Pengwei</given-names></name><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib><contrib contrib-type="author"><name><surname>Xiang</surname><given-names>Sijia</given-names></name><uri xlink:href="https://loop.frontiersin.org/people/2847131/overview"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/></contrib><contrib contrib-type="author"><name><surname>Hu</surname><given-names>Kailei</given-names></name><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name><surname>He</surname><given-names>Jialing</given-names></name><role content-type="https://credit.niso.org/contributor-roles/formal-analysis/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Chen</surname><given-names>Jingxia</given-names></name><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="https://loop.frontiersin.org/people/969853/overview"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/resources/"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff><institution>School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology</institution>, <addr-line>Xi&#x02019;an</addr-line>, <country>China</country></aff><author-notes><fn fn-type="edited-by" id="fn0001"><p>Edited by: Mark Stephen Kindy, United States Department of Veterans Affairs, United States</p></fn><fn fn-type="edited-by" id="fn0002"><p>Reviewed by: Siamak Aram, Harrisburg University of Science and Technology, United States</p><p>Zhuo Huang, The University of Sydney, Australia</p></fn><corresp id="c001">*Correspondence: Jingxia Chen, <email>chenjingxia@sust.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>09</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>19</volume><elocation-id>1501511</elocation-id><history><date date-type="received"><day>25</day><month>9</month><year>2024</year></date><date date-type="accepted"><day>24</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025 Zhang, Xiang, Hu, He and Chen.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Zhang, Xiang, Hu, He and Chen</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>To solve the problem of poor generalization ability of the model on unknown data and the difference of physiological signals between different subjects. A sleep staging model based on Adversarial Domain Generalized Residual Attention Network (ADG-RANet) is designed. The model is divided into three parts: feature extractor, domain discriminator and label classifier. In the feature extractor part, the channel attention network is combined with the residual block to selectively enhance the important features and the correlation between multi-channel physiological signals. Inspired by the idea of U-shaped network, the details and context information in the input data are effectively captured through up-sampling and skip connection operations. The Bi-GRU network is used to further extract the deep temporal features. A Gradient Reversal Layer (GRL) is introduced between the domain discriminator and the feature extractor to promote the feature extractor to obtain the invariant features between different subjects through the adversarial training process. The label classifier uses the deep features learned by the feature extractor to perform sleep staging. According to the AASM sleep staging criterion, the five-classification accuracy of the model on the ISRUC-S3 dataset was 82.51%, the m-F1 score was 0.8100, and the Kappa coefficient was 0.7748. By observing the test results of each fold and comparing with the benchmark model, it is verified that the proposed model has better generalization on unknown data.</p></abstract><kwd-group><kwd>adversarial domain generalization</kwd><kwd>sleep staging</kwd><kwd>residual attention network</kwd><kwd>GRL</kwd><kwd>bi-GRU</kwd></kwd-group><funding-group><award-group><funding-source id="cn1"><institution-wrap><institution>National Natural Science Foundation of China</institution><institution-id institution-id-type="doi">10.13039/501100001809</institution-id></institution-wrap></funding-source></award-group><funding-statement>The author(s) declare that financial support was received for the research and/or publication of this article. This work was supported by the National Natural Science Foundation of China (grant no. 61806118).</funding-statement></funding-group><counts><fig-count count="4"/><table-count count="6"/><equation-count count="14"/><ref-count count="23"/><page-count count="10"/><word-count count="7020"/></counts><custom-meta-group><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Sleep and Circadian Rhythms</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="sec1"><label>1</label><title>Introduction</title><p>Sleep, driven by the circadian rhythm, is a crucial means to improve bodily functions and alleviate fatigue (<xref rid="ref5" ref-type="bibr">Foster, 2020</xref>). Both the duration and quality of sleep play pivotal roles in physical and mental health. Assessing sleep stages and conducting research related to sleep staging is of significant importance for human health and clinical disease diagnosis. Polysomnography (PSG) is a technique widely used in sleep medicine research and sleep disorder diagnosis, which continuously synchronizes the recording of biological electrical changes and physiological activities during sleep. It mainly records various physiological indicators including Electroencephalogram (EEG), Electrooculogram (EOG), Electrocardiogram (ECG), Electromyograp-hy (EMG), Blood Oxygen Saturation (SpO2 Saturation), Pulse, Nasal-oral Air Flow, and Thoracic Abdominal Effort (<xref rid="ref2" ref-type="bibr">Chinese Medical Association Neurology Physicians Branch Sleep Disorders Specialty Committee, Chinese Sleep Research Society Sleep Disorders Specialty Committee, Chinese Medical Association Neurology Branch Sleep Disorders Study Group, 2018</xref>). The early classification standards for sleep stages divided sleep into three stages: Wake (W), Non-Rapid Eye Movement sleep (NREM sleep), and Rapid Eye Movement sleep (REM sleep). In 1968, Rechtschaffen and Kales from the United States proposed and formulated the &#x0201c;Manual of Standardized Terminology, Techniques and Scoring System for Sleep Stages in Human Subjects,&#x0201d; abbreviated as the R&#x00026;K criteria. This criterion divides sleep stages into six stages: W stage, Stage 1(S1), Stage 2(S2), Stage 3(S3), Stage 4 (S4), and REM stage. Based on this, the American Academy of Sleep Medicine published the &#x0201c;AASM Manual for the Scoring of Sleep and Associated Events: Rules, Terminology and Technical Specifications,&#x0201d; abbreviated as the AASM criteria. The AASM criteria merge stages S3 and S4 from the R&#x00026;K criteria, dividing sleep stages into W stage, Non-Rapid Eye Movement 1(NREM 1, N1) stage, Non-Rapid Eye Movement 2(NREM 2, N2) stage, Non-Rapid Eye Movement 3(NREM 3, N3) stage, and REM stage, totaling five stages (<xref rid="ref20" ref-type="bibr">Moser et al., 2009</xref>).</p><p>The early staging of sleep relied on manual assessment of sleep data by sleep experts, a process that was not only time-consuming and labor-intensive but also prone to errors due to subjective factors. With the continuous development of computer technology, automatic sleep staging techniques have gradually become prevalent. Machine learning based automatic sleep staging methods require manual feature extraction and use classifiers such as Random Forest (RF) for sleep staging tasks (<xref rid="ref19" ref-type="bibr">Memar and Faradji, 2018</xref>). With the ongoing advancement of deep learning, deep learning techniques rely on deep neural networks for end-to-end feature extraction, avoiding the subjectivity of manual feature extraction.</p><p>Traditional deep learning methods mostly conduct experiments on specific subject data (<xref rid="ref17" ref-type="bibr">Li et al., 2022</xref>). However, due to the non-stationarity of EEG signals and significant differences between biological signals of different individuals, the generalization performance of models on test sets decreases. To address this issue, some studies employ contrastive learning methods to capture the correlation information between data of the same category. For example, attention mechanisms are incorporated into bidirectional Recurrent Neural Network (RNN), and feature extractors are trained using triplet loss to learn the similarity between the same sleep stages and the differences between different stages (<xref rid="ref16" ref-type="bibr">Kumar, 2023</xref>). The model achieved a five-class classification accuracy of 94.11% on the public dataset Sleep-EDF. Some researchers further obtain common features between the source domain and the target domain through Domain Adaptation (DA) methods. Suppose we take several research efforts as an example. They used a conditional adversarial domain generalization method, feeding the classifier&#x02019;s output back to the discriminator as a condition to learn domain-invariant features (<xref rid="ref22" ref-type="bibr">Zhao et al., 2021</xref>). The proposed model was validated on the migration between different channels of the Sleep-EDF dataset and between different datasets, demonstrating the superiority of unsupervised model migration to the sleep staging problem. Although existing domain adaptation methods can address domain shift problems, they rely on training with target samples. Domain Generalization (DG) methods aim to improve model generalization by leveraging the diversity of the source domain. Models assume access only to the source domain during training, and improve model generalization performance by leveraging the diversity of the source domain, which is more realistic. For example, some studies reinforced spatiotemporal features learned by the feature extractor using adversarial domain generalization methods, enhancing the globality of features and further strengthening the robustness of the model (<xref rid="ref13" ref-type="bibr">Jia et al., 2021</xref>).</p><p>In addition, some studies have further refined adversarial training methods from a theoretical perspective. MADG designs a novel discrepancy metric based on margin loss, which has been theoretically proven to be more optimizable and robust compared to the conventional 0&#x02013;1 loss, thereby significantly enhancing domain generalization performance (<xref rid="ref3" ref-type="bibr">Dayal et al., 2023</xref>). Other studies focus on the generalization ability of internal neural network mechanisms. EVIL identifies stable and variant parameters at the parameter level to extract a robust subnetwork, improving the model&#x02019;s adaptability under distribution shifts (<xref rid="ref11" ref-type="bibr">Huang et al., 2025</xref>). In a seemingly opposite direction, H-NTL addresses non-transferable learning by disentangling content and style through a causal model and proposes a controllable feature modeling framework, which inspires us to design structured constraints for feature learning (<xref rid="ref10" ref-type="bibr">Hong et al., 2024</xref>).</p><p>Inspired by previous studies, this study proposes a sleep staging model&#x02014;Adversarial Domain Generalized Residual Attention Network (ADG-RANet)&#x02014;by integrating domain generalization strategies into the DANN (<xref rid="ref1" ref-type="bibr">Ajakan et al., 2014</xref>) framework. The model consists of three main modules: a feature extractor <italic>G<sub>f</sub></italic>, a domain discriminator <italic>G<sub>d</sub></italic>, and a label predictor <italic>G<sub>cla</sub></italic>. During training, the dataset is divided into multi-source domains containing data from multiple subjects and a target domain containing data from one subject. The feature extractor is responsible for extracting useful information from the multi-source domain data, while the domain discriminator attempts to distinguish the input data&#x02019;s domain based on the extracted features. The label predictor performs sleep stage classification based on the extracted features. A Gradient Reversal Layer (GRL) is deployed between the feature extractor and the domain discriminator to ensure that the feature extractor learns more generalized shared features through adversarial learning strategies.</p></sec><sec sec-type="methods" id="sec2"><label>2</label><title>Methods</title><p>The main purpose of proposed ADG-RANet is to acquire shared features with better generalization performance. Since domain generalization methods require the model to be agnostic to the test set during training, each subject&#x02019;s data is treated as a sub-domain, with the training set composed of multiple sub-domains called multi-source domains, and the test set composed of one sub-domain called the target domain. The overall architecture of the network is depicted in <xref rid="fig1" ref-type="fig">Figure 1</xref>, where <xref rid="fig1" ref-type="fig">Figure 1a</xref> illustrates the training process of the model using multi-source domain data, and <xref rid="fig1" ref-type="fig">Figure 1b</xref> illustrates the process of testing the model on the target domain.</p><fig position="float" id="fig1"><label>Figure 1</label><caption><p>Overall structure of the ADG-RANet model.</p></caption><graphic xlink:href="fnins-19-1501511-g001" position="float"/></fig><p>During the training phase: Firstly, the original multimodal signals for each sub-domain are divided into several segments of 30&#x0202f;s each, referred to as epoch, and each epoch is transformed into a time-frequency graph representing the time-frequency features via Short-Time Fourier Transform. Next, useful information is extracted from the spectrograms by the feature extractor. Then, the label predictor is utilized to further learn deep feature representations and predict sleep stages. Meanwhile, the domain discriminator uses the features obtained by the feature extractor to distinguish which domain the input data belongs to. By deploying the GRL between the feature extractor and the domain discriminator, an adversarial game is conducted between them to learn features that are relevant to sleep stages but domain-independent (<xref rid="ref18" ref-type="bibr">Liu et al., 2023</xref>). During the testing phase, the trained model is evaluated using unknown target domain data.</p><sec id="sec3"><label>2.1</label><title>Feature extractors</title><p>During the process of constructing deep models, researchers have observed that the deeper the abstraction level of semantic feature information, the stronger the expression capability. However, a problem arises as the model deepens, where gradient issues occur. ResNet addresses this problem to some extent by introducing residual connections, which enable the model to preserve the original features while learning deep features (<xref rid="ref9" ref-type="bibr">He et al., 2016</xref>). However, for image data, the output is represented by the sum of matrices across all channels. Therefore, it is necessary to embed the dependency relationships between channels into the features, enhancing useful feature channels and reducing redundant feature channels, thus exploring the information contained in multi-channel spectrogram data (<xref rid="ref7" ref-type="bibr">Gai, 2020</xref>). Inspired by this, this study introduces a channel attention module into residual blocks, forming Residual Channel Attention Blocks (Res_CAB). The two structures of Res_CAB are shown in <xref rid="fig2" ref-type="fig">Figure 2</xref>. By stacking multiple Res_CABs, the encoder part of the feature extractor is constructed.</p><fig position="float" id="fig2"><label>Figure 2</label><caption><p>Two model structures of Res_CAB.</p></caption><graphic xlink:href="fnins-19-1501511-g002" position="float"/></fig><p>The implementation process of Res_CAB is as follows:</p><p>Define the input data as <italic>X<sup>TF</sup></italic>. First, perform a convolution operation on <italic>X<sup>TF</sup></italic> to obtain the feature map <italic>o<sub>c</sub></italic> &#x02208; &#x0211d;<italic>
<sup>H&#x0202f;&#x000d7;&#x0202f;W&#x0202f;&#x000d7;&#x0202f;C</sup>
</italic>, as shown in <xref rid="EQ1" ref-type="disp-formula">Equation 1</xref>:</p><disp-formula id="EQ1">
<label>(1)</label>
<mml:math id="M1" overflow="scroll"><mml:msub><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x02217;</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:munderover><mml:msubsup><mml:mi>v</mml:mi><mml:mi>c</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>&#x02217;</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:math>
</disp-formula><p>Where <italic>v<sub>c</sub></italic> is the <italic>c</italic>-th convolution kernel, and <italic>x<sup>s</sup></italic> is the <italic>s</italic>-th input.</p><p>Next, the obtained feature <italic>o<sub>c</sub></italic> is input into the squeeze module <italic>f<sub>sq</sub></italic>, where global average pooling is applied to each channel, resulting in the feature vector <italic>z<sub>c</sub></italic> &#x02208; &#x0211d;<italic>
<sup>1&#x0202f;&#x000d7;&#x0202f;1&#x0202f;&#x000d7;&#x0202f;C</sup>
</italic>, where <italic>C</italic> is 10, as shown in <xref rid="EQ2" ref-type="disp-formula">Equation 2</xref>. Through this operation, information between channels is retained, further determining the global features of the c-channel feature maps.</p><disp-formula id="EQ2">
<label>(2)</label>
<mml:math id="M2" overflow="scroll"><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")"><mml:msub><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>W</mml:mi></mml:munderover><mml:msub><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mfenced></mml:math>
</disp-formula><p>Next, the obtained channel information <italic>z<sub>c</sub></italic> is input into the excitation module <italic>f<sub>ex</sub></italic> to obtain interchannel correlation information. Firstly, it passes through the first fully connected layer, reducing the <italic>C</italic> channels to <italic>C/r</italic>, where <italic>r</italic> represents the dimension reduction ratio. Then, it undergoes a non-linear activation function Relu and is fed into the second fully connected layer, where the number of feature channels is restored to <italic>C</italic>. Through this operation, each channel feature obtains weight parameters <italic>s</italic> with attention weights, as shown in <xref rid="EQ3" ref-type="disp-formula">Equation 3</xref>:</p><disp-formula id="EQ3">
<label>(3)</label>
<mml:math id="M3" overflow="scroll"><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="italic">ex</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>z</mml:mi><mml:mi>W</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>g</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:mi>z</mml:mi><mml:mi>W</mml:mi></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>z</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>
</disp-formula><p>Where <italic>W, W1, W2</italic> are learnable parameters, <italic>&#x003b4;</italic> represents the ReLU activation function, and <italic>&#x003c3;</italic> represents the Sigmoid activation function.</p><p>Next, the obtained attention weights are multiplied channel-wise with the feature map <italic>o<sub>c</sub></italic>, completing the re-calibration of the input features in the channel dimension. This is shown in <xref rid="EQ4" ref-type="disp-formula">Equation 4</xref>:</p><disp-formula id="EQ4">
<label>(4)</label>
<mml:math id="M4" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="italic">scale</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:msub><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math>
</disp-formula><p>Finally, the recalibrated features are added to the input data and passed through the ReLU activation function. For the standard Res-CAB, its implementation process is shown in <xref rid="EQ5" ref-type="disp-formula">Equation 5</xref>:</p><disp-formula id="EQ5">
<label>(5)</label>
<mml:math id="M5" overflow="scroll"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>
</disp-formula><p>For the Res-CAB with 1&#x0202f;&#x000d7;&#x0202f;1 convolution, its implementation process is shown in <xref rid="EQ6" ref-type="disp-formula">Equation 6</xref>:</p><disp-formula id="EQ6">
<label>(6)</label>
<mml:math id="M6" overflow="scroll"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>o</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:msup><mml:mi>X</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mi>F</mml:mi></mml:mrow></mml:msup><mml:mi>W</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:math>
</disp-formula><p>The Res-CA-FE model structure is illustrated in <xref rid="fig3" ref-type="fig">Figure 3</xref>. To further learn the features contained in the spectrogram data and obtain a more robust model, the design philosophy of the U-Net is employed in this study, dividing the feature extractor into encoder and decoder parts (<xref rid="ref21" ref-type="bibr">Yang et al., 2022</xref>). In the encoder part, the feature extractor utilizes stacked Res-CAB for feature extraction. The residual blocks effectively mitigate the problem of performance degradation in deep learning networks, while the channel attention mechanism adaptively adjusts the weights of different channels. By integrating the SE-block into the shortcut connection before the Res-block, feature recalibration on the branch&#x02019;s features is achieved. This allows the network to better fit the correlations between channels while obtaining local information for each channel, thereby enhancing the model&#x02019;s expressive power and generalization ability. In the decoder part, upsampling operations are used to increase the feature maps to a higher resolution, enhancing the feature extractor&#x02019;s ability to extract detailed information from the spectrogram. By concatenating the contextual information from the encoder with the local information from the decoder, feature fusion is achieved. Subsequently, temporal features related to sleep stages are obtained through a Bi-GRU network.</p><fig position="float" id="fig3"><label>Figure 3</label><caption><p>Overall Res_CA_FE model structure.</p></caption><graphic xlink:href="fnins-19-1501511-g003" position="float"/></fig><p>Define the feature extractor module, which maps the input data to the feature space, as shown in <xref rid="EQ7" ref-type="disp-formula">Equation 7</xref>:</p><disp-formula id="EQ7">
<label>(7)</label>
<mml:math id="M7" overflow="scroll"><mml:msub><mml:mi>G</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=";"><mml:mi mathvariant="bold">F</mml:mi><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mi>&#x02131;</mml:mi></mml:math>
</disp-formula><p>Where <italic>F</italic> is the input feature matrix, <italic>&#x1d703;<sub>f</sub></italic> represents the learnable parameters, and <italic>&#x02131;</italic> is the obtained feature matrix.</p></sec><sec id="sec4"><label>2.2</label><title>Label predictor</title><p>Label predictor <italic>G<sub>cla</sub></italic> passes the features learned by the feature extractor through three fully connected layers to fully mine the abstract semantic features in the input data. And after the last fully connected layer, the softmax activation function is used for the five-class sleep stage classification task, as shown in <xref rid="EQ8" ref-type="disp-formula">Equation 8</xref>:</p><disp-formula id="EQ8">
<label>(8)</label>
<mml:math id="M8" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>exp</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>exp</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math>
</disp-formula><p>Where <italic>&#x02131;<sub>i</sub></italic> is the features obtained by sample <italic>i</italic> through the feature extractor, is the prediction result obtained by the label predictor, <italic>W<sub>cla</sub></italic> is the learnable parameter, and <italic>b<sub>cla</sub></italic> is the bias term.</p><p>In this study, sleep stages are categorized into five categories based on the AASM criterion, therefore, the difference between the true category labels and the model predictions is evaluated using the cross-entropy loss function, and the loss <italic>&#x02112;<sub>cla</sub></italic> of the labeling predictor is shown in <xref rid="EQ9" ref-type="disp-formula">Equation 9</xref>:</p><disp-formula id="EQ9">
<label>(9)</label>
<mml:math id="M9" overflow="scroll"><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>log</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>
</disp-formula><p>Where <italic>M</italic> is the number of samples, <italic>J</italic> is the number of labels, <italic>y<sub>i,j</sub></italic> is the true sleep stage <italic>j</italic> for the <italic>i</italic>-th sample, and <inline-formula>
<mml:math id="M10" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>
</inline-formula> is the predicted sleep stage for the <italic>i</italic>-th sample.</p></sec><sec id="sec5"><label>2.3</label><title>Domain discriminator</title><p>Similar to the label predictor, the domain discriminator <italic>G<sub>d</sub></italic> inputs the features learned by the feature extractor into a network consisting of three fully connected layers stacked on top of each other, fuses the input features and extracts higher-level feature representations, and introduces a softmax activation function after the last fully connected layer to compute the probability value that the input data belongs to a certain domain, as shown in <xref rid="EQ10" ref-type="disp-formula">Equation 10</xref>:</p><disp-formula id="EQ10">
<label>(10)</label>
<mml:math id="M11" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>exp</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>exp</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math>
</disp-formula><p>Where <inline-formula>
<mml:math id="M12" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo stretchy="true">&#x002dc;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math>
</inline-formula> is the prediction obtained by the domain discriminator, <italic>W<sub>d</sub></italic> is the learnable parameter and <italic>b<sub>d</sub></italic> is the bias term.</p><p>In this study, data from different subjects can be considered as candidates for multi-source or target domains, Define <inline-formula>
<mml:math id="M13" overflow="scroll"><mml:msubsup><mml:mi>D</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mn>2</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math>
</inline-formula> and <inline-formula>
<mml:math id="M14" overflow="scroll"><mml:msup><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:math>
</inline-formula> denote the multi-source domain (training set) containing data from n subjects and the target domain (test set) containing data from one subject, respectively. The multi-source <inline-formula>
<mml:math id="M15" overflow="scroll"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:math>
</inline-formula> and target domains <inline-formula>
<mml:math id="M16" overflow="scroll"><mml:msup><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:math>
</inline-formula>contain the corresponding features as well as labels, with the difference that the source domain data contains its domain label, as shown in <xref rid="EQ11" ref-type="disp-formula">Equations 11</xref>, <xref rid="EQ12" ref-type="disp-formula">12</xref>:</p><disp-formula id="EQ11">
<label>(11)</label>
<mml:math id="M17" overflow="scroll"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mfenced open="{" close="}" separators=",,"><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:msubsup></mml:math>
</disp-formula><disp-formula id="EQ12">
<label>(12)</label>
<mml:math id="M18" overflow="scroll"><mml:msup><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mfenced open="{" close="}" separators=","><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msubsup></mml:math>
</disp-formula><p>Where, <italic>m<sub>s</sub></italic> is the sample size of the source domain, <italic>m<sub>t</sub></italic> is the sample size of the target domain, <italic>F<sup>s</sup></italic> is the features of the source domain, <italic>F<sup>t</sup></italic> is the features of the target domain, <italic>y<sup>s</sup></italic> is the category labels of the source domain, <italic>y<sup>t</sup></italic> is the category labels of the target domain, and <italic>d<sub>i</sub></italic> is the domain labeling, and in this study, we set the domain labels as 1&#x0202f;~&#x0202f;9, which denote each subject in the training set, respectively.</p><p>Similar to the label predictor, this study uses the cross-entropy loss function to evaluate the difference between the real domain labels and the model predictions. The loss <italic>&#x02112;<sub>d</sub></italic> of the domain discriminator is shown in <xref rid="EQ13" ref-type="disp-formula">Equation 13</xref>:</p><disp-formula id="EQ13">
<label>(13)</label>
<mml:math id="M19" overflow="scroll"><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:munderover><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>log</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>
</disp-formula><p>Where <italic>d<sub>i,j</sub></italic> is the true domain label <italic>j</italic> for the <italic>i</italic>-th sample and <inline-formula>
<mml:math id="M20" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo stretchy="true">&#x002dc;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>
</inline-formula> is the predictive domain label for the <italic>i</italic>-th sample.</p></sec><sec id="sec6"><label>2.4</label><title>Overall loss</title><p>To address the variability between different subjects and construct a model structure more suitable for actual clinical diagnosis, this study introduces the domain generalization theory, motivated by the fact that learning is a migration invariant feature to multi-source domain data, and the model should be robust to the migration of any unknown target domain (<xref rid="ref23" ref-type="bibr">Zhou et al., 2023</xref>). Therefore, the model in the backpropagation process, the domain discriminator will domain classification loss of the gradient through the GRL layer for automatic inversion, so that the model to achieve in the maximization of the loss of the domain discriminator at the same time, minimize the loss of the label predictor (<xref rid="ref4" ref-type="bibr">Fan et al., 2022</xref>). Through this operation, the feature extractor learns domain-invariant feature representations that can deceive the domain discriminator, which is unable to distinguish which domain the data comes from, but the label predictor is able to discern the labels of the input data.</p><p>By combining the DANN network with the domain generalization theory and introducing an adversarial learning strategy, feature learning and domain generalization are integrated into a unified framework (<xref rid="ref13" ref-type="bibr">Jia et al., 2021</xref>), which enables adversarial training between the domain discriminator and the feature extractor, and motivates the model to acquire subject-independent common features to improve the model&#x02019;s generalization performance on unknown data.</p><p>Therefore, the overall loss of the model is shown in <xref rid="EQ14" ref-type="disp-formula">Equation 14</xref>, where <italic>&#x003bb;</italic> is the weight parameter:</p><disp-formula id="EQ14">
<label>(14)</label>
<mml:math id="M21" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi mathvariant="italic">all</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>log</mml:mo><mml:msub><mml:mover><mml:mi>y</mml:mi><mml:mo>&#x0223c;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2.5em"/><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo stretchy="true">&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:munderover><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>log</mml:mo><mml:msub><mml:mover><mml:mi>d</mml:mi><mml:mo>&#x0223c;</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math>
</disp-formula></sec></sec><sec id="sec7"><label>3</label><title>Experiments and results</title><sec id="sec8"><label>3.1</label><title>Datasets and preprocessing</title><p>In this study, the proposed model is validated using the publicly available dataset ISRUC-S3, which contains data from 10 subjects, and each subject&#x02019;s data contains physiological signals from 6 EEG channels, 3 EMG channels, 1 ECG channel, and 2 EOG channels, as well as physiological information on snoring, abdominal breathing, oxygen saturation, and body position (<xref rid="ref15" ref-type="bibr">Khalighi et al., 2016</xref>). In this study, the data from a total of 10 channels collected by EEG, EMG, EOG, and ECG were selected, following the international sleep medicine standards (AASM) and field-specific conventions.</p><p>First, we downsample the data to 128&#x0202f;Hz to reduce the computational load while preserving the key features of the signals. Then, we apply the Short-Time Fourier Transform (STFT) to convert the data into 2D time-frequency spectrograms, which serve as the model&#x02019;s input (<xref rid="ref12" ref-type="bibr">Jadhav and Mukhopadhyay, 2022</xref>). The STFT is implemented using the scipy.signal.stft function, with a Hamming window, a window length of 128 samples, an overlap rate of 50%, and the number of FFT points set to None (default is the window length). The output magnitude spectrum is used for subsequent feature extraction.</p><p>To evaluate the generalization ability of the proposed model, we additionally employed the ISRUC-S1 dataset, which contains PSG recordings from 100 subjects. Although ISRUC-S1 follows the same sleep staging protocol, signal configuration, and preprocessing pipeline as ISRUC-S3, it comprises an entirely different subject population and exhibits independent data distributions. To conduct the fine-tuning experiments, we randomly selected data from 20 subjects to form a representative and diverse subset, enabling a meaningful evaluation of the model&#x02019;s adaptability under distributional shift.</p></sec><sec id="sec9"><label>3.2</label><title>Experimental environment and evaluation metrics</title><p>To evaluate the performance and cross-subject generalization ability of the proposed model, each subject&#x02019;s data is treated as an independent domain. Since the dataset used in this study contains data from 10 subjects, a 10-fold cross-validation approach is employed, where all the data from one subject is selected as the test set for each fold, and the remaining data is used as the training set. No subject overlap occurs between training and test sets, and the test set remains unseen during training. The performance on the test set of each fold is averaged as the final performance of the model.</p><p>To further evaluate the model&#x02019;s adaptability to new subjects, the model pre-trained on the ISRUC-S3 dataset was fine-tuned using data randomly selected from 20 subjects in the ISRUC-S1 dataset. During fine-tuning, the same hyperparameters used in ISRUC-S3 were retained, and 10-fold cross-validation was applied to assess the model&#x02019;s performance, ensuring the reliability and robustness of the experiment. Additionally, most of the convolutional layers were frozen, and only the final layers were fine-tuned to preserve the general features learned from the ISRUC-S3 dataset and adjust the high-level features to fit the ISRUC-S1 dataset.</p><p>All experiments were conducted on a workstation equipped with an NVIDIA RTX 3090 GPU and 128&#x0202f;GB of RAM. The experiments were implemented using Tensorflow-gpu2.5.0, with the experimental parameter settings listed in <xref rid="tab1" ref-type="table">Table 1</xref>.</p><table-wrap position="float" id="tab1"><label>Table 1</label><caption><p>Experimental parameter setting.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1">Hyperparameter</th><th align="center" valign="top" rowspan="1" colspan="1">Value</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Number of training sessions</td><td align="center" valign="middle" rowspan="1" colspan="1">150</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Learning rate</td><td align="center" valign="middle" rowspan="1" colspan="1">2e-5</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">Adam</td></tr></tbody></table></table-wrap><p>In this study, we evaluate the overall performance of the model using Accuracy (Acc), Cohen&#x02019;s Kappa coefficient (Kappa), and the macro-average F1 score. Additionally, Precision (Pre), Recall (Rec), and F1 score are employed as subcategory evaluation metrics.</p></sec><sec id="sec10"><label>3.3</label><title>Results</title><sec id="sec11"><label>3.3.1</label><title>Original results</title><p>The classification performance of the proposed model ADG-RANet on the ISRUC-S3 dataset pairs is shown in <xref rid="tab2" ref-type="table">Table 2</xref>.</p><table-wrap position="float" id="tab2"><label>Table 2</label><caption><p>Overall classification performance of the ADG-RANet model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2" colspan="1">Category</th><th align="center" valign="top" colspan="3" rowspan="1">Subcategory performance</th><th align="center" valign="top" colspan="3" rowspan="1">Overall classification performance</th></tr><tr><th align="center" valign="top" rowspan="1" colspan="1">Pre</th><th align="center" valign="top" rowspan="1" colspan="1">Rec</th><th align="center" valign="top" rowspan="1" colspan="1">F1 score</th><th align="center" valign="top" rowspan="1" colspan="1">Acc</th><th align="center" valign="top" rowspan="1" colspan="1">m-F1</th><th align="center" valign="top" rowspan="1" colspan="1">Kappa</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">W</td><td align="center" valign="top" rowspan="1" colspan="1">0.8988</td><td align="center" valign="top" rowspan="1" colspan="1">0.9091</td><td align="center" valign="top" rowspan="1" colspan="1">0.9039</td><td align="center" valign="top" rowspan="5" colspan="1">0.8251</td><td align="center" valign="top" rowspan="5" colspan="1">0.8100</td><td align="center" valign="top" rowspan="5" colspan="1">0.7748</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">N1</td><td align="center" valign="top" rowspan="1" colspan="1">0.6217</td><td align="center" valign="top" rowspan="1" colspan="1">0.6033</td><td align="center" valign="top" rowspan="1" colspan="1">0.6124</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">N2</td><td align="center" valign="top" rowspan="1" colspan="1">0.7994</td><td align="center" valign="top" rowspan="1" colspan="1">0.8551</td><td align="center" valign="top" rowspan="1" colspan="1">0.8263</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">N3</td><td align="center" valign="top" rowspan="1" colspan="1">0.9254</td><td align="center" valign="top" rowspan="1" colspan="1">0.8620</td><td align="center" valign="top" rowspan="1" colspan="1">0.8925</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">REM</td><td align="center" valign="top" rowspan="1" colspan="1">0.8258</td><td align="center" valign="top" rowspan="1" colspan="1">0.8047</td><td align="center" valign="top" rowspan="1" colspan="1">0.8151</td></tr></tbody></table></table-wrap><p>Observing <xref rid="tab2" ref-type="table">Table 2</xref>, it can be seen that the F1 score of the proposed model is above 0.8 in most of the sleep stages, and the F1 score in the most difficult to classify N1 stage is 0.6124.</p><p><xref rid="fig4" ref-type="fig">Figure 4</xref> demonstrates the classification accuracy per fold of the proposed model using Res-CA-FE as the feature extractor of ADG-RANet.</p><fig position="float" id="fig4"><label>Figure 4</label><caption><p>Test results per fold.</p></caption><graphic xlink:href="fnins-19-1501511-g004" position="float"/></fig><p>Since the data of each subject is treated as a separate domain in this study during the experimental process, all the data of one subject is selected each time to test the model. Therefore, the test results of each fold can indicate the classification accuracy of each subject&#x02019;s data on the model. As can be seen in <xref rid="fig4" ref-type="fig">Figure 4</xref>, the model achieved above 80% classification accuracy on most of the subjects&#x02019; data.</p></sec><sec id="sec12"><label>3.3.2</label><title>Fine-tuning results</title><p>After fine-tuning on the ISRUC-S1 dataset, the model achieved an accuracy of 77.38%, an F1 score of 0.7478, and a Cohen&#x02019;s Kappa of 0.7075 on previously unseen subject data. These results indicate that the model retains a certain level of generalization capability on new data.</p><p>The differences between ISRUC-S1 and ISRUC-S3, including variations in data distribution, signal quality, and annotation consistency, present additional challenges for cross-dataset generalization. Nevertheless, the results demonstrate that the model retains transferable features, providing a solid foundation for future research on broader domain generalization and its clinical applications.</p></sec><sec id="sec13"><label>3.3.3</label><title>Confidence intervals</title><p>In the experiments, bootstrap sampling was used to calculate the confidence intervals for each model with a 95% confidence level to assess their performance stability. The results show that the RF model has an accuracy confidence interval of (0.648, 0.729), indicating relatively high variability; the GraphSleepNet model has an accuracy confidence interval of (0.741, 0.799), demonstrating more stable performance; and the ADG-RANet model has an accuracy confidence interval of (0.775, 0.825), outperforming MSTGCN with stable performance. These confidence intervals reveal the superior and stable performance of ADG-RANet across different data subsets.</p></sec></sec><sec id="sec14"><label>3.4</label><title>Model ablation experiments</title><p>To verify the ability of the feature extractor Res-CA-FE in the proposed model, the following two models are designed in the encoder part of the feature extractor:</p><p>Model 1: The SE-block is integrated with the residual block of Res2Net (<xref rid="ref8" ref-type="bibr">Gao et al., 2021</xref>) to form a residual channel attention block based on Res2Net. This block is designed to extract multi-scale features and is used to replace the residual blocks in the encoder of the Res-CA-FE model.</p><p>Model 2: First, the time-frequency map is input into the ResNet18 network to learn local information. After downsampling in the last layer, the parallel Position Attention Module (PAM) and Channel Attention Module (CAM) proposed in literature (<xref rid="ref6" ref-type="bibr">Fu et al., 2019</xref>) are used to further extract spectral and channel features from multi-channel time-frequency data. The obtained features are then fused.</p><p>The comparison results are shown in <xref rid="tab3" ref-type="table">Table 3</xref>. From the table, it can be seen that the classification performance using the Res-CA-FE model is optimal; the overall accuracy of Model 1 is 1.99% lower than that of the method proposed in this study. Analyzing the reason may be that the feature extractor constructed in this study obtains rich multi-scale information through jump-join operation. When the residual block proposed by Res2Net is introduced, the model fails to mine more useful information and increases the parameters required for model training; the overall accuracy of Model 2 is reduced by 0.75% compared with the method proposed in this study. Analyzing the reason may be that Model 2 first uses ResNet18 to learn the spectral information of multi-channel time-frequency map data, and then enhances the feature representation by combining the parallel channel and spectral self-attention mechanism, but does not pay attention to the correlation between the channels in the pre-feature extraction, which leads to the loss of important information.</p><table-wrap position="float" id="tab3"><label>Table 3</label><caption><p>Performance comparison of different feature extraction networks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2" colspan="1">Method</th><th align="center" valign="top" colspan="3" rowspan="1">Overall performance</th><th align="center" valign="top" colspan="5" rowspan="1">F1 score for each category</th></tr><tr><th align="center" valign="top" rowspan="1" colspan="1">Acc</th><th align="center" valign="top" rowspan="1" colspan="1">m-F1</th><th align="center" valign="top" rowspan="1" colspan="1">Kappa</th><th align="center" valign="top" rowspan="1" colspan="1">W</th><th align="center" valign="top" rowspan="1" colspan="1">N1</th><th align="center" valign="top" rowspan="1" colspan="1">N2</th><th align="center" valign="top" rowspan="1" colspan="1">N3</th><th align="center" valign="top" rowspan="1" colspan="1">REM</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Model 1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8052</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7902</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7498</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8835</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5853</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7917</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8900</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8004</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Model 2</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.8176</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8010</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.768</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.8861</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5831</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8078</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8912</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.8368</underline>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Model 3</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.8176</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.8068</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7653</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8766</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.6188</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.8105</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8906</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.8374</bold>
</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Model 4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8115</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7965</td><td align="center" valign="middle" rowspan="1" colspan="1">0.7574</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8843</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5865</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8063</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.8932</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8121</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">Ours</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.8251</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.8100</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.7748</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.9039</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.6124</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.8263</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<underline>0.8925</underline>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8151</td></tr></tbody></table><table-wrap-foot><p>Bold values represent the optimal results, and underlined numbers represent the second-best results.</p></table-wrap-foot></table-wrap><p>To verify the validity of the modules of the model proposed in this study, the following two models were designed to carry out ablation experiments:</p><p>Model 3: Remove the SE-block introduced in Res_CAB in the Res-CA-FE model to verify the effectiveness of the channel attention mechanism in capturing feature correlations between multi-channel feature maps.</p><p>Model 4: Remove the domain discriminator, perform feature extraction only through a feature extractor, and use a label predictor to classify the sleep stages and verify the effectiveness of using the adversarial domain generalization approach.</p><p>The results of the ablation experiment are shown in <xref rid="tab3" ref-type="table">Table 3</xref>. From the table, it can be seen that model 3 removes the channel attention module, and its accuracy rate is 81.76%, compared with the model proposed in this study, the accuracy rate is reduced by 0.75%, which proves that, for multi-channel time-frequency map data, using the channel attention mechanism can further explore the intrinsic correlation between the channels of the multi-channel physiological signals, which helps to increase the accuracy rate of sleep staging; model 4removes the domain discriminator module is removed, and the model becomes a conventional deep learning structure with an accuracy of 81.15%, which is 1.36% lower than that of the model proposed in this study, which proves that the use of the adversarial domain generalization method helps the model to learn domain invariant features by training the adversarial training between the domain discriminator and the feature extractor, which then improves the model&#x02019;s generalization performance on the unknown data.</p></sec><sec id="sec15"><label>3.5</label><title>Comparison with benchmark models</title><p>In order to verify the superiority of the proposed model, the classification results of the model on the ISRUC-S3 dataset were compared with other benchmark models, and an overview of the benchmark models is shown below:</p><list list-type="simple"><list-item><p>(1) RF (<xref rid="ref19" ref-type="bibr">Memar and Faradji, 2018</xref>): using unimodal EEG signals, eight sub-bands of each epoch EEG signal are obtained, feature selection is performed by Kruskal-Wallis test and minimum redundancy-maximum correlation, and finally sleep staging is performed using RF.</p></list-item><list-item><p>(2) GraphSleepNet (<xref rid="ref13" ref-type="bibr">Jia et al., 2021</xref>): using EEG, EOG, EMG, and ECG signals, a functional connectivity map of the brain was constructed, features were acquired through a deep learning model, and sleep staging was performed using a label classifier to train and validate the model.</p></list-item><list-item><p>(3) MSTGCN (<xref rid="ref13" ref-type="bibr">Jia et al., 2021</xref>): using EEG, EOG, EMG, and ECG signals, two views, based on brain function and based on physical distance, were constructed, and features capable of generalizing to unknown domains were obtained through domain generalization methods.</p></list-item></list><p>The comparison results are shown in <xref rid="tab4" ref-type="table">Table 4</xref>. From the table, it can be seen that the RF uses machine learning methods for sleep staging, and its accuracy rate is relatively low. The reason for this is analyzed because the machine learning method requires manual feature extraction, which is unable to obtain the potential deep information in the data, which makes it easy to lose important information related to sleep in the process of feature extraction. The model proposed in the GraphSleepNet uses a traditional deep learning approach to obtain sleep-related temporal features by constructing a functional brain connectivity map using a spatio-temporal attention graph convolutional network with an accuracy of 79.9%. However, the model did not take into account the domain bias due to the variability among different subjects. MSTGCN constructed two views based on brain functional connectivity and physical distance, which effectively solved the domain bias problem by introducing the domain generalization method and accuracy rate reached 82.1%. This proves that the introduction of the idea of domain generalization can improve the model&#x02019;s generalization performance on unknown data. In this study, by constructing multimodal physiological signals into grid-structured data suitable for CNN, RNN, and other models, we constructed a residual attention network based on antagonistic domain generalization to mine the salient features of each stage of sleep, and the overall classification accuracy reached 82.5%. Although the overall accuracy is not much improved over the literature (<xref rid="ref13" ref-type="bibr">Jia et al., 2021</xref>), the classification performance in N1 stage which is the most difficult to classify is improved by 0.016, and all other sleep stages are significantly improved.</p><table-wrap position="float" id="tab4"><label>Table 4</label><caption><p>Performance comparison with benchmark models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2" colspan="1">Method</th><th align="center" valign="top" colspan="3" rowspan="1">Overall performance</th><th align="center" valign="top" colspan="5" rowspan="1">F1 score for each category</th></tr><tr><th align="center" valign="top" rowspan="1" colspan="1">Acc</th><th align="center" valign="top" rowspan="1" colspan="1">m-F1</th><th align="center" valign="top" rowspan="1" colspan="1">Kappa</th><th align="center" valign="top" rowspan="1" colspan="1">W</th><th align="center" valign="top" rowspan="1" colspan="1">N1</th><th align="center" valign="top" rowspan="1" colspan="1">N2</th><th align="center" valign="top" rowspan="1" colspan="1">N3</th><th align="center" valign="top" rowspan="1" colspan="1">REM</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">RF</td><td align="center" valign="top" rowspan="1" colspan="1">0.729</td><td align="center" valign="top" rowspan="1" colspan="1">0.708</td><td align="center" valign="top" rowspan="1" colspan="1">0.648</td><td align="center" valign="top" rowspan="1" colspan="1">0.858</td><td align="center" valign="top" rowspan="1" colspan="1">0.473</td><td align="center" valign="top" rowspan="1" colspan="1">0.704</td><td align="center" valign="top" rowspan="1" colspan="1">0.809</td><td align="center" valign="top" rowspan="1" colspan="1">0.699</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">GraphSleepNet</td><td align="center" valign="top" rowspan="1" colspan="1">0.799</td><td align="center" valign="top" rowspan="1" colspan="1">0.787</td><td align="center" valign="top" rowspan="1" colspan="1">0.741</td><td align="center" valign="top" rowspan="1" colspan="1">0.878</td><td align="center" valign="top" rowspan="1" colspan="1">0.574</td><td align="center" valign="top" rowspan="1" colspan="1">0.776</td><td align="center" valign="top" rowspan="1" colspan="1">0.864</td><td align="center" valign="top" rowspan="1" colspan="1">
<underline>0.841</underline>
</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">MSTGCN</td><td align="center" valign="top" rowspan="1" colspan="1">
<underline>0.821</underline>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<underline>0.808</underline>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<underline>0.769</underline>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<underline>0.894</underline>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<underline>0.596</underline>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<underline>0.806</underline>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<underline>0.890</underline>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>0.856</bold>
</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">ADG-RANet</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>0.825</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>0.810</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>0.775</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>0.904</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>0.612</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>0.826</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">
<bold>0.893</bold>
</td><td align="center" valign="top" rowspan="1" colspan="1">0.815</td></tr></tbody></table><table-wrap-foot><p>Bold values represent the optimal results, and underlined numbers represent the second-best results.</p></table-wrap-foot></table-wrap></sec><sec id="sec16"><label>3.6</label><title>Computational efficiency analysis</title><p>In this experiment, we evaluated the computational efficiency of the model by measuring its total inference time, per-sample latency, throughput, and the configuration of each module. The results for each experimental condition are summarized in <xref rid="tab5" ref-type="table">Table 5</xref>.</p><table-wrap position="float" id="tab5"><label>Table 5</label><caption><p>Computational efficiency evaluation of the model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1">Fold</th><th align="center" valign="top" rowspan="1" colspan="1">Total inference time (s)</th><th align="center" valign="top" rowspan="1" colspan="1">Latency per sample (s)</th><th align="center" valign="top" rowspan="1" colspan="1">Throughput (samples/s)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">3.3367</td><td align="center" valign="top" rowspan="1" colspan="1">0.003611</td><td align="center" valign="top" rowspan="1" colspan="1">276.92</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">2.7115</td><td align="center" valign="top" rowspan="1" colspan="1">0.002976</td><td align="center" valign="top" rowspan="1" colspan="1">335.98</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">2.5239</td><td align="center" valign="top" rowspan="1" colspan="1">0.003179</td><td align="center" valign="top" rowspan="1" colspan="1">314.59</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">2.3775</td><td align="center" valign="top" rowspan="1" colspan="1">0.003112</td><td align="center" valign="top" rowspan="1" colspan="1">321.34</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">2.4109</td><td align="center" valign="top" rowspan="1" colspan="1">0.002638</td><td align="center" valign="top" rowspan="1" colspan="1">379.11</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">2.4919</td><td align="center" valign="top" rowspan="1" colspan="1">0.003028</td><td align="center" valign="top" rowspan="1" colspan="1">330.27</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">2.4526</td><td align="center" valign="top" rowspan="1" colspan="1">0.003128</td><td align="center" valign="top" rowspan="1" colspan="1">319.66</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">2.7848</td><td align="center" valign="top" rowspan="1" colspan="1">0.002871</td><td align="center" valign="top" rowspan="1" colspan="1">348.32</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">2.7849</td><td align="center" valign="top" rowspan="1" colspan="1">0.002966</td><td align="center" valign="top" rowspan="1" colspan="1">337.18</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">2.3000</td><td align="center" valign="top" rowspan="1" colspan="1">0.003003</td><td align="center" valign="top" rowspan="1" colspan="1">333.04</td></tr></tbody></table></table-wrap><p>On average, the model&#x02019;s total inference time was 2.6175&#x0202f;s, while the per-sample latency averaged 0.003051&#x0202f;s. The throughput was 329.64 samples per second across all experimental conditions.</p><p>Furthermore, the input and output dimensions, as well as the number of parameters for each module, are provided in <xref rid="tab6" ref-type="table">Table 6</xref>. The model has a total of 15.12 M parameters, making it lightweight and efficient for deployment in resource-constrained environments.</p><table-wrap position="float" id="tab6"><label>Table 6</label><caption><p>The structure configuration of the ADG-RANet model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="1" colspan="1">Module</th><th align="left" valign="top" rowspan="1" colspan="1">Network layer</th><th align="center" valign="top" rowspan="1" colspan="1">Inputs</th><th align="center" valign="top" rowspan="1" colspan="1">Outputs</th><th align="center" valign="top" rowspan="1" colspan="1">Params (Million)</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="2" colspan="1">Res_CA_FE</td><td align="left" valign="top" rowspan="1" colspan="1">Encoder</td><td align="center" valign="top" rowspan="1" colspan="1">(128,128,10)</td><td align="center" valign="top" rowspan="1" colspan="1">(8,8,256)</td><td align="center" valign="top" rowspan="1" colspan="1">11.25</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Decoder</td><td align="center" valign="top" rowspan="1" colspan="1">(8,8,256)</td><td align="center" valign="top" rowspan="1" colspan="1">(32,256)</td><td align="center" valign="top" rowspan="1" colspan="1">3.83</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Label Classifier</td><td rowspan="1" colspan="1"/><td align="center" valign="top" rowspan="1" colspan="1">(32,256)</td><td align="center" valign="top" rowspan="1" colspan="1">(5,)</td><td align="center" valign="top" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Domain Discriminator</td><td rowspan="1" colspan="1"/><td align="center" valign="top" rowspan="1" colspan="1">(32,256)</td><td align="center" valign="top" rowspan="1" colspan="1">(9,)</td><td align="center" valign="top" rowspan="1" colspan="1">0.02</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Total</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="center" valign="top" rowspan="1" colspan="1">15.12</td></tr></tbody></table></table-wrap></sec></sec><sec id="sec17"><label>4</label><title>Summarize</title><p>In this study, based on the idea of adversarial domain generalization, the original dataset is divided into a training set containing data from multiple subjects, and a test set containing data from another subject. First, the original multimodality of each epoch is represented with 2D time-frequency graph of 10 channels, and a residual network with an attention mechanism is designed, in which the feature extractor is to extract the key information from the multi-source domain data for the label predictor and the domain discriminator is to perform sleep-stage 5 classification task and 9 domain discriminative classification task, respectively. During the backpropagation process, a GRL reverses the gradient of the domain discriminator, which prompts the feature extractor to further optimize the acquired features and improve the generalization ability of the model over the unknown domain. Through ablation experiments conducted on the ISRUC-S3 dataset and comparisons with the baseline model, the proposed domain generalization method, combined with the residual attention-based feature extraction network, was shown to effectively improve the model&#x02019;s generalization ability to unseen data. Furthermore, after fine-tuning on the ISRUC-S1 dataset, the model maintained a high classification accuracy on previously unseen subjects, indicating a certain level of cross-dataset transferability and generalization performance.</p><p>The experimental results demonstrate that the proposed sleep staging model achieves strong classification performance on the ISRUC-S3 multi-channel PSG dataset, providing a solid foundation for practical applications in sleep disorder diagnosis. With efficient inference, the model supports preliminary screening and personalized interventions, thereby improving diagnostic accuracy and reducing clinical workload. Its computational efficiency further enables broad deployment across medical institutions, home monitoring systems, and primary care terminals. The model also facilitates automated generation of structured sleep reports, enhancing the accessibility and reach of sleep health services. Future integration with multimodal data (e.g., fMRI, PET) may enhance the model&#x02019;s capacity to identify complex sleep disorders and distinguish between their subtypes.</p><p>While the model demonstrates promising results in classifying most sleep stages, further improvements are needed in handling fine-grained transitions and boundary samples. Future research will explore advanced techniques such as contrastive learning to build a more robust and discriminative feature space, improving sensitivity to subtle sleep state changes. Given its effectiveness in scenarios with limited labels, class imbalance, and noisy data, contrastive learning is particularly suitable for complex, high-dimensional sleep signals. Incorporating transfer learning, attention mechanisms, and few-shot learning strategies will also help the model adapt to diverse patient groups and device settings, advancing its applicability in intelligent sleep medicine.</p></sec></body><back><sec sec-type="data-availability" id="sec18"><title>Data availability statement</title><p>Publicly available datasets were analyzed in this study. This data can be found at: <ext-link xlink:href="https://sleeptight.isr.uc.pt/" ext-link-type="uri">https://sleeptight.isr.uc.pt/</ext-link>.</p></sec><sec sec-type="ethics-statement" id="sec19"><title>Ethics statement</title><p>Ethical approval was not required for the study involving humans in accordance with the local legislation and institutional requirements. Written informed consent to participate in this study was not required from the participants or the participants&#x02019; legal guardians/next of kin in accordance with the national legislation and the institutional requirements.</p></sec><sec sec-type="author-contributions" id="sec20"><title>Author contributions</title><p>PZ: Methodology, Writing &#x02013; review &#x00026; editing. SX: Data curation, Writing &#x02013; original draft, Validation. KH: Visualization, Validation, Writing &#x02013; original draft. JH: Formal analysis, Investigation, Software, Writing &#x02013; original draft. JC: Conceptualization, Resources, Supervision, Writing &#x02013; review &#x00026; editing.</p></sec><sec sec-type="COI-statement" id="sec22"><title>Conflict of interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="ai-statement" id="sec23"><title>Generative AI statement</title><p>The author(s) declare that no Gen AI was used in the creation of this manuscript.</p></sec><sec sec-type="disclaimer" id="sec24"><title>Publisher&#x02019;s note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><ref-list><title>References</title><ref id="ref1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ajakan</surname><given-names>H.</given-names></name><name><surname>Germain</surname><given-names>P.</given-names></name><name><surname>Larochelle</surname><given-names>H.</given-names></name><name><surname>Marchand</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Domain-adversarial neural networks</article-title>. <source>arXiv preprint</source>. <volume>arXiv</volume>:<fpage>1412.4446</fpage>.</mixed-citation></ref><ref id="ref2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><collab id="coll1">Chinese Medical Association Neurology Physicians Branch Sleep Disorders Specialty Committee, Chinese Sleep Research Society Sleep Disorders Specialty Committee, Chinese Medical Association Neurology Branch Sleep Disorders Study Group</collab></person-group> (<year>2018</year>). <article-title>Expert consensus on technical specifications and clinical applications of multiple sleep latency tests for adults in China</article-title>. <source>Chin. Med. J.</source>
<volume>98</volume>, <fpage>3825</fpage>&#x02013;<lpage>3831</lpage>. doi: <pub-id pub-id-type="doi">10.3760/cma.j.issn.0376-2491.2018.47.004</pub-id></mixed-citation></ref><ref id="ref3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayal</surname><given-names>A.</given-names></name><name><surname>Cenkeramaddi</surname><given-names>L. R.</given-names></name><name><surname>Mohan</surname><given-names>C.</given-names></name></person-group> (<year>2023</year>). <article-title>MADG: margin-based adversarial learning for domain generalization</article-title>. <source>Adv. Neural Inf. Proces. Syst.</source>
<volume>36</volume>, <fpage>58938</fpage>&#x02013;<lpage>58952</lpage>.</mixed-citation></ref><ref id="ref4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>J. H.</given-names></name><name><surname>Zhu</surname><given-names>H. Y.</given-names></name><name><surname>Jiang</surname><given-names>X. Y.</given-names></name></person-group> (<year>2022</year>). <article-title>Unsupervised domain adaptation by statistics alignment for deep sleep staging networks</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng.</source>
<volume>30</volume>, <fpage>205</fpage>&#x02013;<lpage>216</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TNSRE.2022.3144169</pub-id>, PMID: <pub-id pub-id-type="pmid">35041607</pub-id>
</mixed-citation></ref><ref id="ref5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>R. G.</given-names></name></person-group> (<year>2020</year>). <article-title>Sleep, circadian rhythms and health</article-title>. <source>Interface Focus</source>
<volume>10</volume>:<fpage>20190098</fpage>. doi: <pub-id pub-id-type="doi">10.1098/rsfs.2019.0098</pub-id>, PMID: <pub-id pub-id-type="pmid">32382406</pub-id>
</mixed-citation></ref><ref id="ref6"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Tian</surname><given-names>H</given-names></name></person-group>, &#x0201c;Dual attention network for scene segmentation[C]//Long Beach, USA, 2019&#x0201d;. <italic>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</italic>, (<year>2019</year>): 3141&#x02013;3149.</mixed-citation></ref><ref id="ref7"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gai</surname><given-names>C. Y.</given-names></name></person-group> (<year>2020</year>). <source>Research on object detection algorithm based on feature pyramid structure</source>. <publisher-loc>Qinhuangdao</publisher-loc>: <publisher-name>Yanshan University</publisher-name>.</mixed-citation></ref><ref id="ref8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>S. H.</given-names></name><name><surname>Cheng</surname><given-names>M. M.</given-names></name><name><surname>Zhao</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X. Y.</given-names></name><name><surname>Yang</surname><given-names>M. H.</given-names></name><name><surname>Torr</surname><given-names>P.</given-names></name></person-group> (<year>2021</year>). <article-title>Res2net: a new multi-scale backbone architecture</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>43</volume>, <fpage>652</fpage>&#x02013;<lpage>662</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2019.2938758</pub-id>, PMID: <pub-id pub-id-type="pmid">31484108</pub-id>
</mixed-citation></ref><ref id="ref9"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name></person-group>, &#x0201c;Deep residual learning for image recognition[C]// Las Vegas, USA, 2016&#x0201d;. <italic>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>, (<year>2016</year>): 770&#x02013;778.</mixed-citation></ref><ref id="ref10"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Shen</surname><given-names>L</given-names></name></person-group>. &#x0201c;Improving non-transferable representation learning by harnessing content and style&#x0201d;. <italic>The twelfth international conference on learning representations</italic>. (<year>2024</year>).</mixed-citation></ref><ref id="ref11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>M.</given-names></name><name><surname>Shen</surname><given-names>L.</given-names></name><name><surname>Yu</surname><given-names>J.</given-names></name><name><surname>Gong</surname><given-names>C.</given-names></name><name><surname>Han</surname><given-names>B.</given-names></name><etal/></person-group>. (<year>2025</year>). <article-title>Winning prize comes from losing tickets: improve invariant learning by exploring variant parameters for out-of-distribution generalization</article-title>. <source>Int. J. Comput. Vis.</source>
<volume>133</volume>, <fpage>456</fpage>&#x02013;<lpage>474</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s11263-024-02075-x</pub-id></mixed-citation></ref><ref id="ref12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jadhav</surname><given-names>P.</given-names></name><name><surname>Mukhopadhyay</surname><given-names>S.</given-names></name></person-group> (<year>2022</year>). <article-title>Automated sleep stage scoring using time-frequency spectra convolution neural network</article-title>. <source>IEEE Trans. Instrum. Meas.</source>
<volume>71</volume>, <fpage>1</fpage>&#x02013;<lpage>9</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TIM.2022.3177747</pub-id></mixed-citation></ref><ref id="ref13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>Z. Y.</given-names></name><name><surname>Lin</surname><given-names>Y. F.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>). <article-title>Multi-view spatial-temporal graph convolutional networks with domain generalization for sleep stage classification</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng.</source>
<volume>29</volume>, <fpage>1977</fpage>&#x02013;<lpage>1986</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TNSRE.2021.3110665</pub-id>, PMID: <pub-id pub-id-type="pmid">34487495</pub-id>
</mixed-citation></ref><ref id="ref14"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>Z Y</given-names></name><name><surname>Lin</surname><given-names>Y F</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name></person-group>, &#x0201c;GraphSleepNet: adaptive spatial-temporal graph convolutional networks for sleep stage classification//Yokohama, Japan,&#x0201d; In <italic>Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</italic>, (<year>2021</year>): 1324&#x02013;1330.</mixed-citation></ref><ref id="ref15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khalighi</surname><given-names>S.</given-names></name><name><surname>Sousa</surname><given-names>T.</given-names></name><name><surname>Santos</surname><given-names>J. M.</given-names></name><name><surname>Nunes</surname><given-names>U.</given-names></name></person-group> (<year>2016</year>). <article-title>ISRUC-sleep: a comprehensive public dataset for sleep researchers</article-title>. <source>Comput. Methods Prog. Biomed.</source>
<volume>124</volume>, <fpage>180</fpage>&#x02013;<lpage>192</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.cmpb.2015.10.013</pub-id>, PMID: <pub-id pub-id-type="pmid">26589468</pub-id>
</mixed-citation></ref><ref id="ref16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>C. B.</given-names></name></person-group> (<year>2023</year>). <source>SCL-SSC: Supervised contrastive learning for sleep stage classification</source>. <volume>TechRxiv</volume>. doi: <pub-id pub-id-type="doi">10.36227/techrxiv.17711369.v1</pub-id></mixed-citation></ref><ref id="ref17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>G.</given-names></name><name><surname>Ji</surname><given-names>Z.</given-names></name><name><surname>Qu</surname><given-names>X.</given-names></name><name><surname>Zhou</surname><given-names>R.</given-names></name><name><surname>Cao</surname><given-names>D.</given-names></name></person-group> (<year>2022</year>). <article-title>Cross-domain object detection for autonomous driving: a stepwise domain adaptative YOLO approach</article-title>. <source>IEEE Transactions on Intelligent Vehicles</source>
<volume>7</volume>, <fpage>603</fpage>&#x02013;<lpage>615</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TIV.2022.3165353</pub-id></mixed-citation></ref><ref id="ref18"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y L</given-names></name><name><surname>Xu</surname><given-names>Y W</given-names></name><name><surname>Zou</surname><given-names>Z F</given-names></name></person-group>, &#x0201c;Adversarial domain generalization for surveillance face anti-spoofing [C]//Vancouver, Canada&#x0201d;. <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</italic>, (<year>2023</year>): 6352&#x02013;6360.</mixed-citation></ref><ref id="ref19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Memar</surname><given-names>P.</given-names></name><name><surname>Faradji</surname><given-names>F.</given-names></name></person-group> (<year>2018</year>). <article-title>A novel multi-class EEG-based sleep stage classification system</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng.</source>
<volume>26</volume>, <fpage>84</fpage>&#x02013;<lpage>95</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TNSRE.2017.2776149</pub-id>, PMID: <pub-id pub-id-type="pmid">29324406</pub-id>
</mixed-citation></ref><ref id="ref20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moser</surname><given-names>D.</given-names></name><name><surname>Anderer</surname><given-names>P.</given-names></name><name><surname>Gruber</surname><given-names>G.</given-names></name><name><surname>Parapatics</surname><given-names>S.</given-names></name><name><surname>Loretz</surname><given-names>E.</given-names></name><name><surname>Boeck</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2009</year>). <article-title>Sleep classification according to AASM and Rechtschaffen &#x00026; Kales: effects on sleep scoring parameters</article-title>. <source>Sleep</source>
<volume>32</volume>, <fpage>139</fpage>&#x02013;<lpage>149</lpage>. doi: <pub-id pub-id-type="doi">10.1093/sleep/32.2.139</pub-id>, PMID: <pub-id pub-id-type="pmid">19238800</pub-id>
</mixed-citation></ref><ref id="ref21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>M.</given-names></name><name><surname>Yuan</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>G.</given-names></name></person-group> (<year>2022</year>). <article-title>SDUNet: road extraction via spatial enhanced and densely connected UNet</article-title>. <source>Pattern Recogn.</source>
<volume>126</volume>:<fpage>108549</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.patcog.2022.108549</pub-id></mixed-citation></ref><ref id="ref22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>R. Q.</given-names></name><name><surname>Xia</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>Y. L.</given-names></name></person-group> (<year>2021</year>). <article-title>Unsupervised sleep staging system based on domain adaptation</article-title>. <source>Biomed. Signal Process. Control</source>
<volume>69</volume>:<fpage>102937</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.bspc.2021.102937</pub-id></mixed-citation></ref><ref id="ref23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>K. Y.</given-names></name><name><surname>Liu</surname><given-names>Z. W.</given-names></name><name><surname>Qiao</surname><given-names>Y.</given-names></name></person-group> (<year>2023</year>). <article-title>Domain generalization: a survey</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>45</volume>, <fpage>4396</fpage>&#x02013;<lpage>4415</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2022.3195549</pub-id>, PMID: <pub-id pub-id-type="pmid">35914036</pub-id>
</mixed-citation></ref></ref-list></back></article>