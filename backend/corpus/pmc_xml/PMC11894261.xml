<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" id="cre270115" xml:lang="en" article-type="review-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Clin Exp Dent Res</journal-id><journal-id journal-id-type="iso-abbrev">Clin Exp Dent Res</journal-id><journal-id journal-id-type="doi">10.1002/(ISSN)2057-4347</journal-id><journal-id journal-id-type="publisher-id">CRE2</journal-id><journal-title-group><journal-title>Clinical and Experimental Dental Research</journal-title></journal-title-group><issn pub-type="epub">2057-4347</issn><publisher><publisher-name>John Wiley and Sons Inc.</publisher-name><publisher-loc>Hoboken</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40066511</article-id><article-id pub-id-type="pmc">PMC11894261</article-id>
<article-id pub-id-type="doi">10.1002/cre2.70115</article-id><article-id pub-id-type="publisher-id">CRE270115</article-id><article-categories><subj-group subj-group-type="overline"><subject>Review Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Review Article</subject></subj-group></article-categories><title-group><article-title>Artificial Intelligence in Temporomandibular Joint Disorders: An Umbrella Review</article-title></title-group><contrib-group><contrib id="cre270115-cr-0001" contrib-type="author" corresp="yes"><name><surname>Mehta</surname><given-names>Vini</given-names></name><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-4174-907X</contrib-id><xref rid="cre270115-aff-0001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cre270115-aff-0002" ref-type="aff">
<sup>2</sup>
</xref><address><email>vini.mehta@statsense.in</email></address></contrib><contrib id="cre270115-cr-0002" contrib-type="author"><name><surname>Tripathy</surname><given-names>Snehasish</given-names></name><xref rid="cre270115-aff-0002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib id="cre270115-cr-0003" contrib-type="author"><name><surname>Noor</surname><given-names>Toufiq</given-names></name><xref rid="cre270115-aff-0002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib id="cre270115-cr-0004" contrib-type="author"><name><surname>Mathur</surname><given-names>Ankita</given-names></name><xref rid="cre270115-aff-0002" ref-type="aff">
<sup>2</sup>
</xref></contrib></contrib-group><aff id="cre270115-aff-0001">
<label>
<sup>1</sup>
</label>
<named-content content-type="organisation-division">Faculty of Dentistry</named-content>
<institution>University of Ibn al&#x02010;Nafis for Medical Sciences</institution>
<city>San'a</city>
<country country="YE">Yemen</country>
</aff><aff id="cre270115-aff-0002">
<label>
<sup>2</sup>
</label>
<named-content content-type="organisation-division">Department of Dental Research Cell</named-content>
<institution>Dr. D. Y. Patil Dental College and Hospital, Dr. D. Y. Patil Vidyapeeth</institution>
<city>Pune</city>
<country country="IN">India</country>
</aff><author-notes><corresp id="correspondenceTo">
<label>*</label>
<bold>Correspondence:</bold> Vini Mehta (<email>vini.mehta@statsense.in</email>)<break/>
</corresp></author-notes><pub-date pub-type="epub"><day>11</day><month>3</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>11</volume><issue seq="79">1</issue><issue-id pub-id-type="doi">10.1002/cre2.v11.1</issue-id><elocation-id>e70115</elocation-id><history>
<date date-type="rev-recd"><day>18</day><month>2</month><year>2025</year></date>
<date date-type="received"><day>10</day><month>12</month><year>2024</year></date>
<date date-type="accepted"><day>01</day><month>3</month><year>2025</year></date>
</history><permissions><!--&#x000a9; 2025 The Author(s). Clinical and Experimental Dental Research published by John Wiley & Sons Ltd.--><copyright-statement content-type="article-copyright">&#x000a9; 2025 The Author(s). <italic toggle="yes">Clinical and Experimental Dental Research</italic> published by John Wiley &#x00026; Sons Ltd.</copyright-statement><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="file:CRE2-11-e70115.pdf"/><abstract><title>ABSTRACT</title><sec id="cre270115-sec-0010"><title>Objectives</title><p>Given the complexity of temporomandibular joint disorders (TMDs) and their overlapping symptoms with other conditions, an accurate diagnosis necessitates a thorough examination, which can be time&#x02010;consuming and resource&#x02010;intensive. Consequently, innovative diagnostic tools are required to increase TMD diagnosis efficiency and precision. Therefore, the purpose of this umbrella review was to examine the existing evidence about the usefulness of artificial intelligence (AI) in TMD diagnosis.</p></sec><sec id="cre270115-sec-0020"><title>Material and Methods</title><p>A comprehensive search of the literature was performed from inception to November 30, 2024, in PubMed&#x02010;MEDLINE, Embase, and Scopus databases. This review evaluated systematic reviews (SRs) and meta&#x02010;analyses (MAs) that reported TMD patients/datasets, any AI model as intervention, no treatment, placebo as comparator and accuracy, sensitivity, specificity, or predictive value of AI models as outcome. The extracted data were complemented with narrative synthesis.</p></sec><sec id="cre270115-sec-0030"><title>Results</title><p>Out of 1497 search results, this umbrella review included five studies. One of the five articles was an SR while the other four were SRMAs. Three studies focused on patients with temporomandibular joint (TMJ) problems as a group, whereas two were specific to temporomandibular joint osteoarthritis (TMJOA). The included studies reported the use of imaging datasets as samples, including cone&#x02010;beam computed tomography (CBCT), magnetic resonance imaging (MRI), and panoramic radiography. The studies reported an accuracy level ranging from 0.59 to 1. Four studies reported sensitivity levels ranging from 0.76 to 0.80. Four studies reported specificity values ranging from 0.63 to 0.95 for TMJ conditions. However, only one study provided the area under the curve (AUC) in the diagnosis of TMDs.</p></sec><sec id="cre270115-sec-0040"><title>Conclusions</title><p>AI has the ability to provide faster, more accurate, sensitive, and objective diagnosis of TMJ condition. However, the performance is determined on the AI models and datasets used. Therefore, before implementing AI models in clinical practice, it is essential for researchers to extensively refine and evaluate the AI application.</p></sec></abstract><kwd-group><kwd id="cre270115-kwd-0001">artificial Intelligence</kwd><kwd id="cre270115-kwd-0002">machine learning</kwd><kwd id="cre270115-kwd-0003">temporomandibular joint disorders</kwd></kwd-group><funding-group><award-group id="funding-0001"><funding-source>The authors received no specific funding for this work.</funding-source></award-group></funding-group><counts><fig-count count="1"/><table-count count="4"/><page-count count="10"/><word-count count="5938"/></counts><custom-meta-group><custom-meta><meta-name>source-schema-version-number</meta-name><meta-value>2.0</meta-value></custom-meta><custom-meta><meta-name>cover-date</meta-name><meta-value>February 2025</meta-value></custom-meta><custom-meta><meta-name>details-of-publishers-convertor</meta-name><meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.5.4 mode:remove_FC converted:07.04.2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="cre270115-sec-0050"><label>1</label><title>Introduction</title><p>Temporomandibular disorders (TMDs) are painful and debilitating craniofacial conditions that affect the temporomandibular joints (TMJs) and masticatory muscles, leading to restricted range of motion, joint noise, and distorted mouth opening due to joint dysregulation. Worldwide, 34% of individuals, the majority of whom are between the ages of 18 and 60, have reported experiencing symptoms of TMJ (Zieli&#x00144;ski et al.&#x000a0;<xref rid="cre270115-bib-0034" ref-type="bibr">2024</xref>). TMJ disorders have a complex and multifaceted etiology that has been linked to comorbidities such as heart disease, osteoarthritis, hearing loss, sinusitis, and thyroid dysfunction. TMJ issues have long&#x02010;term consequences for both individuals and society. TMD can lead to lasting complications, such as malocclusion or facial deformities. Additionally, it can contribute to mental health challenges. A recent systematic study found that 43.0% of TMD patients had depression and 60.0% had somatization (Felin et al.&#x000a0;<xref rid="cre270115-bib-0008" ref-type="bibr">2024</xref>). TMJ dysfunction also causes productivity loss along with substantial medical expenses, negatively affecting the economy (Yost et al.&#x000a0;<xref rid="cre270115-bib-0030" ref-type="bibr">2020</xref>). Therefore, early diagnosis, classification, and management of TMJ are essential for enhancing treatment effectiveness, alleviating symptoms, implementing preventive strategies, maintaining joint mobility, and optimizing healthcare resource utilization. Early detection and intervention may drastically enhance patient outcomes and quality of life, potentially decreasing the need for more invasive and expensive therapies (Xu, Chen, et al.&#x000a0;<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>).</p><p>The current criteria for diagnosing TMJ disorders include a comprehensive medical history to rule out an underlying cause of the disorder and a TMJ joint assessment by a dentist or oral maxillofacial specialist to determine the extent of movement, joint sounds, tenderness, and any signs of inflammation or swelling. Furthermore, diagnostic imaging techniques such as X&#x02010;rays, computed tomography (CT), and medical resonance imaging are used to get comprehensive views of the TMJ and its neighboring tissues (Xu, Chen, et al.&#x000a0;<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>; Ozsari et al.&#x000a0;<xref rid="cre270115-bib-0016" ref-type="bibr">2023</xref>). In addition, the diagnostic criteria for TMDs (DC&#x02010;TMD), developed in the 1990s, remains the most extensively used diagnostic criteria to date. However, it has drawbacks such as limited diagnostic accuracy, sensitivity, and specificity (Jha et al.&#x000a0;<xref rid="cre270115-bib-0013" ref-type="bibr">2022</xref>). As a result, given the complexity of TMDs and their overlapping symptoms with other conditions, an accurate diagnosis necessitates a thorough examination, which can be time&#x02010;consuming and resource&#x02010;intensive. Consequently, innovative diagnostic tools are required to increase TMD diagnosis efficiency and precision.</p><p>Artificial intelligence (AI) has advanced rapidly in recent years. Its subfields, including machine learning (ML) and deep learning, have garnered significant attention for their ability to autonomously learn from diverse datasets, such as images, texts, videos, and more (Xu, Chen, et al.&#x000a0;<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>). AI models have also demonstrated superior predicted accuracy, speed, and efficiency in medical diagnostic procedures as compared to traditional approaches (Abd&#x02010;alrazaq et al.&#x000a0;<xref rid="cre270115-bib-0001" ref-type="bibr">2022</xref>; Zha et al.&#x000a0;<xref rid="cre270115-bib-0032" ref-type="bibr">2024</xref>; Huang et al.&#x000a0;<xref rid="cre270115-bib-0011" ref-type="bibr">2024</xref>). These AI algorithms are capable of analyzing medical images such as traditional X&#x02010;rays, CT scans, magnetic resonance imaging (MRI), and ultrasound, allowing physicians to identify and diagnose diseases more precisely and faster. Its application has been observed across various health domains, including dentistry, neuropathies (Yeti&#x0015f; et al.&#x000a0;<xref rid="cre270115-bib-0028" ref-type="bibr">2024</xref>), and so on. Several systematic reviews (SRs) have documented the use of AI in diagnosing TMD. The accuracy of AI models varies depending on factors such as the model used, the participant population, data inputs, and performance metrics. Therefore, an umbrella review (UR) was needed to synthesize the existing body of knowledge, assess the quality of current studies, and identify gaps in the research. The aim of this UR was to evaluate the evidence on the effectiveness of AI in diagnosing TMD. The results will inform future research and support the integration of AI into standard TMD diagnostic practices.</p></sec><sec sec-type="methods" id="cre270115-sec-0060"><label>2</label><title>Methods</title><sec id="cre270115-sec-0070"><label>2.1</label><title>Review Registration</title><p>This UR was carried out in full compliance with the predefined protocol registered in the international prospective register of SRs, PROSPERO database. We followed the methodology outlined by Aromataris et al. (<xref rid="cre270115-bib-0004" ref-type="bibr">2020</xref>) and PRIOR guidelines (Pollock et al.&#x000a0;<xref rid="cre270115-bib-0018" ref-type="bibr">2019</xref>) for conducting the UR, which includes key components such as a systematic search, eligibility criteria, screening process, data extraction, critical assessment, and the presentation of both quantitative and qualitative systematic review findings in a clear and comprehensible manner. The UR checklist has been provided in Supporting Information S1: Table&#x000a0;<xref rid="cre270115-suppl-0001" ref-type="supplementary-material">S1</xref>.</p><sec id="cre270115-sec-0080"><label>2.1.1</label><title>Search Strategy</title><p>A comprehensive literature search strategy was developed to identify SRs that examined AI models in TMJ problem diagnosis and treatment. The search was completed online on November 30, 2024, which covered systematic reviews (SRs) with or without meta&#x02010;analysis (MA), regardless of language or date restrictions. The search utilized three databases: PubMed&#x02010;MEDLINE, Embase, and Scopus. The search approach was developed initially for MEDLINE (PubMed). Each search concept entailed Medical Subject Headings (MeSH) and synonymous key terms, which were joined using Boolean operators. The search was then customized for each database to follow the particular database search standards. The supplementary material (Supporting Information S1: Table&#x000a0;<xref rid="cre270115-suppl-0001" ref-type="supplementary-material">S2</xref>) includes detailed search algorithms for all databases.</p></sec></sec><sec id="cre270115-sec-0090"><label>2.2</label><title>Eligibility Criteria</title><p>The inclusion and exclusion criteria for this UR were formed based on the PICOS (P&#x02009;=&#x02009;Population, I&#x02009;=&#x02009;Intervention, C&#x02009;=&#x02009;Comparator, O&#x02009;=&#x02009;Outcome, S&#x02009;=&#x02009;Study) framework where</p><p>P<italic toggle="yes">opulation (P)</italic>&#x02009;=&#x02009;temporomandibular joint disorders patients/datasets of TMJ patients,</p><p>I<italic toggle="yes">ntervention (I)</italic>&#x02009;=&#x02009;any artificial intelligence model,</p><p>C<italic toggle="yes">omparators (C)</italic>&#x02009;=&#x02009;no treatment, placebo, or other conventional interventions,</p><p>O<italic toggle="yes">utcome (O)</italic>&#x02009;=&#x02009;accuracy, sensitivity, specificity, or predictive value of AI models in the diagnosis, treatment, or prediction of TMJ disorders/treatments,</p><p>The <italic toggle="yes">study design (S)</italic> included only SRs or SRMAs that reported the outcomes of interest.</p><p>We excluded the studies, which were literature reviews, narrative reviews, rapid reviews, URs, or other primary research designs.</p></sec><sec id="cre270115-sec-0100"><label>2.3</label><title>Selection Process</title><p>The studies were selected in two key stages: title/abstract screening and full&#x02010;text screening. First, the search results for each database were obtained in the research information systems (RIS) format. These RIS files were subsequently imported into Rayyan, an online systematic review application, to aid in the initial filtering of the search results. Two authors independently and blindly reviewed the titles and abstracts. Any discrepancies were resolved through discussion with a third reviewer to reach a unanimous decision. The full text of all potentially eligible studies was then downloaded for further screening. The same two authors assessed the full&#x02010;text articles according to the inclusion criteria, with any disagreements addressed through consultation with the third author.</p></sec><sec id="cre270115-sec-0110"><label>2.4</label><title>Data Extraction</title><p>To ensure uniformity in extraction, the authors developed and evaluated a data extraction form on two randomly selected papers for this UR. A pilot test was used to modify the template. Any disagreement in data extraction was resolved after consultation with a third author. The extracted information included general study characteristics (authors, year of publication, and country of origin), protocol registration, language, time limit, PICOS structure, search details (databases, number of studies detected and included), critical appraisal, and review of key findings (AI model, type of data, sample size, key outcomes). Overlapping studies were identified in the included reviews using a citation matrix (Supporting Information S1: Figure&#x000a0;<xref rid="cre270115-suppl-0001" ref-type="supplementary-material">S1</xref>).</p></sec><sec id="cre270115-sec-0120"><label>2.5</label><title>Risk of Bias Assessment of the Included SRs</title><p>Two reviewers independently assessed the risk of bias in the included reviews using the AMSTAR&#x02010;2 critical appraisal checklist (Shea et al.&#x000a0;<xref rid="cre270115-bib-0022" ref-type="bibr">2017</xref>). The checklist included a total of 16 questions divided into critical and noncritical domains. Each question on the checklist can be assigned responses such as Yes, No, or Partial Yes. Based on the fulfillment of important or noncritical domains, the total confidence in the included review is divided into four main categories: high confidence (no or one noncritical weakness), medium confidence (&#x0003e;&#x02009;1 weakness but no critical weakness), low confidence (one critical weakness with or without noncritical weakness), and critically low confidence (&#x0003e;&#x02009;1 critical weakness with or without noncritical weakness). Any differences were resolved through conversation with a third author.</p></sec><sec id="cre270115-sec-0130"><label>2.6</label><title>Data Curation and Synthesis</title><p>The first author evaluated and summarized the evidence and data, which was then validated by the second author. This information was then compiled in a tabular format and presented descriptively.</p></sec></sec><sec sec-type="results" id="cre270115-sec-0140"><label>3</label><title>Results</title><p>The database search identified 1497 studies, of which 342 were duplicates and removed. This left 1155 studies for eligibility assessment. Following title and abstract screening, 1146 studies were deemed ineligible, leaving nine studies for full&#x02010;text retrieval. After a thorough evaluation of the full&#x02010;text articles, three studies were excluded as they were narrative reviews. Consequently, this UR included five studies (Figure&#x000a0;<xref rid="cre270115-fig-0001" ref-type="fig">1</xref>).</p><fig position="float" fig-type="Figure" id="cre270115-fig-0001"><label>Figure 1</label><caption><p>PRISMA flowchart depicting the study selection process (PRISMA 2020).</p></caption><graphic xlink:href="CRE2-11-e70115-g001" position="anchor" id="jats-graphic-1"/></fig><sec id="cre270115-sec-0150"><label>3.1</label><title>Study Characteristics</title><p>The included reviews were published in the last 2 years (2022&#x02013;2024). One of the five articles considered was an SR (Farook and Dudley&#x000a0;<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>), while the other four were SRMAs (Xu, Chen, et al.&#x000a0;<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>; Jha et al.&#x000a0;<xref rid="cre270115-bib-0013" ref-type="bibr">2022</xref>; Alm&#x00103;&#x00219;an et al.&#x000a0;<xref rid="cre270115-bib-0003" ref-type="bibr">2023</xref>; Zhang et al.&#x000a0;<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>). Three studies focused on patients with TMJ problems as a group, whereas two were specific to temporomandibular joint osteoarthritis (TMJOA). Four research used an AI model, while one used deep learning and automation for TMJ diagnosis. The comparator in the studies ranges from none to clinical diagnosis based on medical diagnostic images/patient data. The outcomes comprised AI performance indicators such as accuracy, sensitivity, specificity, and area under the curve for TMJ subtype diagnosis. The detailed PICO format of the included studies is provided in Table&#x000a0;<xref rid="cre270115-tbl-0001" ref-type="table">1</xref>.</p><table-wrap position="float" id="cre270115-tbl-0001" content-type="Table"><label>Table 1</label><caption><p>PICO elements of included studies.</p></caption><table frame="hsides" rules="groups"><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><thead valign="bottom"><tr style="border-bottom:solid 1px #000000" valign="bottom"><th align="left" valign="bottom" rowspan="1" colspan="1">Author and year of publication</th><th align="center" valign="bottom" rowspan="1" colspan="1">Population</th><th align="center" valign="bottom" rowspan="1" colspan="1">Intervention</th><th align="center" valign="bottom" rowspan="1" colspan="1">Comparator</th><th align="center" valign="bottom" rowspan="1" colspan="1">Outcome</th></tr></thead><tbody valign="top"><tr><td align="left" valign="top" rowspan="1" colspan="1">Alm&#x00103;&#x00219;an et al. (<xref rid="cre270115-bib-0003" ref-type="bibr">2023</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">Patients with TMJ osteoarthritis</td><td align="center" valign="top" rowspan="1" colspan="1">AI as a diagnosis method</td><td align="center" valign="top" rowspan="1" colspan="1">None/Human</td><td align="center" valign="top" rowspan="1" colspan="1">Sensitivity and specificity</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Jha et al. (<xref rid="cre270115-bib-0013" ref-type="bibr">2022</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">Patients with TMDs</td><td align="center" valign="top" rowspan="1" colspan="1">Type of data and algorithm for an AI&#x02010;based automated diagnostic model</td><td align="center" valign="top" rowspan="1" colspan="1">Use of medical diagnostic images (CBCT, MRI, panoramic radiographs) and patient records</td><td align="center" valign="top" rowspan="1" colspan="1">Performance of AI algorithms assessed using diagnostic accuracy</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Xu, Chen, et al. (<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">Patients with TMJOA</td><td align="center" valign="top" rowspan="1" colspan="1">AI models applied to radiographic imaging for TMJOA detection</td><td align="center" valign="top" rowspan="1" colspan="1">NA</td><td align="center" valign="top" rowspan="1" colspan="1">Sensitivity, specificity, area&#x02010;under&#x02010;the&#x02010;curve value</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Zhang et al. (<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">TMD patients</td><td align="center" valign="top" rowspan="1" colspan="1">Any diagnostic test based on machine learning (including deep learning)</td><td align="center" valign="top" rowspan="1" colspan="1">Clinical diagnosis by physicians</td><td align="center" valign="top" rowspan="1" colspan="1">Sensitivity and specificity</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Farook and Dudley&#x000a0;(<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">Patients with TMJ disorders, including TMJ osteoarthritis, disc disorders, and trauma</td><td align="center" valign="top" rowspan="1" colspan="1">Automation and deep learning applied to TMJ radiomics using MRI, CBCT, panoramic radiographs, and thermographic imaging</td><td align="center" valign="top" rowspan="1" colspan="1">Practitioners' diagnostic assessments or manually annotated datasets</td><td align="center" valign="top" rowspan="1" colspan="1">Disc disorders, trauma, and temporomandibular joint pathologies.</td></tr></tbody></table><table-wrap-foot><fn id="cre270115-tbl1-note-0001"><p>Abbreviations: AI, artificial intelligence; CBCT, cone beam computed tomography; MRI, magnetic resonance imaging; TMD, temporomandibular disorders; TMJ, temporomandibular joint; TMJOA, temporomandibular joint osteoarthritis.</p></fn></table-wrap-foot><permissions><copyright-holder>John Wiley &#x00026; Sons, Ltd.</copyright-holder></permissions></table-wrap><p>The reviewers of the included studies were affiliated with institutions from South Korea (<italic toggle="yes">n</italic>&#x02009;=&#x02009;1), Australia (<italic toggle="yes">n</italic>&#x02009;=&#x02009;1), Romania (<italic toggle="yes">n</italic>&#x02009;=&#x02009;1), and China (<italic toggle="yes">n</italic>&#x02009;=&#x02009;2). Four research indicated registering their study procedures in the open science framework (OSF) [14] and PROSPERO (<italic toggle="yes">N</italic>&#x02009;=&#x02009;3) databases (Xu, Chen, et al.&#x000a0;<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>; Farook and Dudley&#x000a0;<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>; Zhang et al.&#x000a0;<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>), whereas one study did not mention of a priori protocol registration (Jha et al). Two studies (Farook and Dudley&#x000a0;<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>; Zhang et al.&#x000a0;<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>) used PRISMA&#x02010;DTA guidelines for their reviews, while three followed PRISMA guidelines. Three of the included publications utilized the QUDAS&#x02010;2 tool to assess the quality of the primary studies (Xu, Chen, et al.&#x000a0;<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>; Jha et al.&#x000a0;<xref rid="cre270115-bib-0013" ref-type="bibr">2022</xref>; Zhang et al.&#x000a0;<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>), one used QUADAS 2 and the MI&#x02010;CLAIM checklist (Alm&#x00103;&#x00219;an et al.&#x000a0;<xref rid="cre270115-bib-0003" ref-type="bibr">2023</xref>), and one used the MI&#x02010;CLAIM checklist and the Cochrane GRADE tool (Farook and Dudley&#x000a0;<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>). In terms of the search approach, one study placed a time limit beginning in 2010, but the other four studies searched from the database's inception. Furthermore, three of the studies applied an English&#x02010;language filter to their search (Xu, Chen, et al.&#x000a0;<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>; Jha et al.&#x000a0;<xref rid="cre270115-bib-0013" ref-type="bibr">2022</xref>; Farook and Dudley&#x000a0;<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>) but the other two did not (Table&#x000a0;<xref rid="cre270115-tbl-0002" ref-type="table">2</xref>).</p><table-wrap position="float" id="cre270115-tbl-0002" content-type="Table"><label>Table 2</label><caption><p>Study characteristics.</p></caption><table frame="hsides" rules="groups"><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><thead valign="bottom"><tr style="border-bottom:solid 1px #000000" valign="bottom"><th align="left" valign="bottom" rowspan="1" colspan="1">Author and year of publication</th><th align="center" valign="bottom" rowspan="1" colspan="1">Country of origin</th><th align="center" valign="bottom" rowspan="1" colspan="1">Protocol registration</th><th align="center" valign="bottom" rowspan="1" colspan="1">Guidelines followed</th><th align="center" valign="bottom" rowspan="1" colspan="1">Search duration</th><th align="center" valign="bottom" rowspan="1" colspan="1">Language restriction</th><th align="center" valign="bottom" rowspan="1" colspan="1">Critical appraisal tool</th></tr></thead><tbody valign="top"><tr><td align="left" valign="top" rowspan="1" colspan="1">Alm&#x00103;&#x00219;an et al. (<xref rid="cre270115-bib-0003" ref-type="bibr">2023</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">Romania</td><td align="center" valign="top" rowspan="1" colspan="1">Open Science Framework</td><td align="center" valign="top" rowspan="1" colspan="1">PRISMA</td><td align="center" valign="top" rowspan="1" colspan="1">Inception to May 28, 2022</td><td align="center" valign="top" rowspan="1" colspan="1">None specified</td><td align="center" valign="top" rowspan="1" colspan="1">QUADAS&#x02010;2 and MI&#x02010;CLAIM checklist</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Jha et al. (<xref rid="cre270115-bib-0013" ref-type="bibr">2022</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">South Korea</td><td align="center" valign="top" rowspan="1" colspan="1">Not registered</td><td align="center" valign="top" rowspan="1" colspan="1">PRISMA</td><td align="center" valign="top" rowspan="1" colspan="1">Inception to June 30, 2022</td><td align="center" valign="top" rowspan="1" colspan="1">English</td><td align="center" valign="top" rowspan="1" colspan="1">QUADAS&#x02010;2</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Xu, Chen, et al. (<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">China</td><td align="center" valign="top" rowspan="1" colspan="1">PROSPERO</td><td align="center" valign="top" rowspan="1" colspan="1">PRISMA</td><td align="center" valign="top" rowspan="1" colspan="1">January 2010 to January 2023</td><td align="center" valign="top" rowspan="1" colspan="1">English</td><td align="center" valign="top" rowspan="1" colspan="1">QUADAS&#x02010;2</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Zhang et al. (<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">China</td><td align="center" valign="top" rowspan="1" colspan="1">PROSPERO</td><td align="center" valign="top" rowspan="1" colspan="1">PRISMA&#x02010;DTA</td><td align="center" valign="top" rowspan="1" colspan="1">Inception to up to July 19, 2023</td><td align="center" valign="top" rowspan="1" colspan="1">None specified</td><td align="center" valign="top" rowspan="1" colspan="1">QUADAS&#x02010;2</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Farook and Dudley (<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">Australia</td><td align="center" valign="top" rowspan="1" colspan="1">PROSPERO</td><td align="center" valign="top" rowspan="1" colspan="1">PRISMA&#x02010;DTA</td><td align="center" valign="top" rowspan="1" colspan="1">Inception to October 01, 2022</td><td align="center" valign="top" rowspan="1" colspan="1">English</td><td align="center" valign="top" rowspan="1" colspan="1">MI&#x02010;CLAIM checklist and Cochrane's GRADE approach</td></tr></tbody></table><table-wrap-foot><fn id="cre270115-tbl2-note-0001"><p>Abbreviations: GRADE, grading of recommendations, assessment, development, and evaluation; MI&#x02010;CLAIM, minimal information for complex low&#x02010;risk artificial intelligence models; PRISMA, preferred reporting items for systematic reviews and meta&#x02010;analyses; PRISMA&#x02010;DTA, preferred reporting items for systematic reviews and meta&#x02010;analyses of diagnostic test accuracy studies; PROSPERO, prospective register of systematic reviews; QUADAS&#x02010;2, quality assessment of diagnostic accuracy studies&#x02010;2.</p></fn></table-wrap-foot><permissions><copyright-holder>John Wiley &#x00026; Sons, Ltd.</copyright-holder></permissions></table-wrap><p>The included studies conducted a thorough and rigorous search across many databases, with PubMed, Web of Science, Embase, and Scopus being the most commonly used. The number of databases searched varied from three to twelve. Zhang et al. (<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>) conducted a more comprehensive search, including searches in databases, preprint servers, and trial platforms. The number of results retrieved varied from 203 to 1923, with an average of approximately 901 studies. The total number of studies included ranged from 6 to 28, with an average of 16 studies. The included studies reported using a variety of imaging datasets as samples, including CBCT, MRI, and panoramic radiography. Only three research mentioned the data sets' sample sizes, which ranged from 28 to 10,077 TMJ images (Xu, Chen, et al.&#x000a0;<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>; Zhang et al.&#x000a0;<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>) (Table&#x000a0;<xref rid="cre270115-tbl-0003" ref-type="table">3</xref>).</p><table-wrap position="float" id="cre270115-tbl-0003" content-type="Table"><label>Table 3</label><caption><p>Database search and key findings of included studies.</p></caption><table frame="hsides" rules="groups"><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><thead valign="bottom"><tr style="border-bottom:solid 1px #000000" valign="bottom"><th align="left" valign="bottom" rowspan="1" colspan="1">Author and year of publication</th><th align="center" valign="bottom" rowspan="1" colspan="1">Database searched</th><th align="center" valign="bottom" rowspan="1" colspan="1">Total studies found</th><th align="center" valign="bottom" rowspan="1" colspan="1">Total included studies</th><th align="center" valign="bottom" rowspan="1" colspan="1">Sample population</th><th align="center" valign="bottom" rowspan="1" colspan="1">Sample size</th><th align="center" valign="bottom" rowspan="1" colspan="1">Key findings</th></tr></thead><tbody valign="top"><tr><td align="left" valign="top" rowspan="1" colspan="1">Alm&#x00103;&#x00219;an et al. (<xref rid="cre270115-bib-0003" ref-type="bibr">2023</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">Pubmed, Embase, Scopus, Web of Science, LILACS, ProQuest, SpringerLink</td><td align="center" valign="top" rowspan="1" colspan="1">203</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">Structured (clinical and biomolecular) and unstructured (imaging) TMJ images</td><td align="center" valign="top" rowspan="1" colspan="1">10,077 TMJ images analyzed, with 5520 images in meta&#x02010;analysis</td><td align="center" valign="top" rowspan="1" colspan="1">
<p>Accuracy: higher accuracy noted for fine&#x02010;tuned models (e.g., XGBoost&#x02009;+&#x02009;LightGBM achieved 82.3% accuracy)</p>
<p>Pooled sensitivity of 0.76 (95% CI 0.35&#x02013;0.95) ResNet classifications</p>
<p>Pooled specificity of 0.79 (95% CI 0.75&#x02013;0.83)</p>
</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Jha et al. (<xref rid="cre270115-bib-0013" ref-type="bibr">2022</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">PubMed, Embase, Web of Science</td><td align="center" valign="top" rowspan="1" colspan="1">1923</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">Medical imaging data (CBCT, MRI, radiographs)</td><td align="center" valign="top" rowspan="1" colspan="1">Not mentioned</td><td align="center" valign="top" rowspan="1" colspan="1">The diagnostic accuracy was 0.69&#x02013;1.00, and the pooled accuracy was 0.91</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Xu, Chen, et al. (<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">Pubmed, Web of Science, Scopus, Embase</td><td align="center" valign="top" rowspan="1" colspan="1">513</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">Imaging data (MRI, CBCT, and OPG)</td><td align="center" valign="top" rowspan="1" colspan="1">523 images with TMJOA and 734 images from controls</td><td align="center" valign="top" rowspan="1" colspan="1">
<p>Pooled accuracy of 0.92</p>
<p>Pooled sensitivity of 0.80 (95% CI: 0.67&#x02013;0.89)</p>
<p>Pooled specificity of 0.90 (95% CI: 0.87&#x02013;0.92);</p>
<p>AUC was 0.92 (95% CI: 0.89&#x02013;0.94)</p>
</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Zhang et al. (<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">Europe PMC; Embase via Ovid, EBM Reviews via Ovid, Scopus, Web of Science, Information Service in Physics, Electro&#x02010;Technology and Computer and Control (Inspec), Korea Citation Index (KCI), (SciELO), WHO Global Index Medicus (GIM), <ext-link xlink:href="http://arXiv.org" ext-link-type="uri">arXiv.org</ext-link>, OSF Preprints, and IEEE Xplore. Two trail platforms: WHO ICTRP, and <ext-link xlink:href="http://ClinicalTrials.org" ext-link-type="uri">ClinicalTrials.org</ext-link>.</td><td align="center" valign="top" rowspan="1" colspan="1">1660</td><td align="center" valign="top" rowspan="1" colspan="1">28 studies (29 reports)</td><td align="center" valign="top" rowspan="1" colspan="1">MRI (<italic toggle="yes">n&#x02009;</italic>=&#x02009;8), panoramic radiographs (<italic toggle="yes">n</italic>&#x02009;=&#x02009;4), cone&#x02010;beam computed tomography (CBCT, <italic toggle="yes">n</italic>&#x02009;=&#x02009;11), and other image modalities (<italic toggle="yes">n</italic>&#x02009;=&#x02009;5).</td><td align="center" valign="top" rowspan="1" colspan="1">28</td><td align="center" valign="top" rowspan="1" colspan="1">
<p>Diagnosis of DJD with CBCT using random forest Sensitivity: 0.745</p>
<p>Specificity 0.770:</p>
<p>Diagnosis of DJD with CBCT using XGBoost: Sensitivity: 0.765</p>
<p>Specificity 0.766</p>
<p>Diagnosis of DJD with CBCT using LightGBM: Sensitivity: 0.781</p>
<p>Specificity: 0.781</p>
</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Farook and Dudley&#x000a0;(<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>)</td><td align="center" valign="top" rowspan="1" colspan="1">MEDLINE, EBSCOHost, Scopus, PubMed, Web of Science</td><td align="center" valign="top" rowspan="1" colspan="1">208</td><td align="center" valign="top" rowspan="1" colspan="1">20</td><td align="center" valign="top" rowspan="1" colspan="1">MRI, CBCT, panoramic radiographs, and thermographic imaging</td><td align="center" valign="top" rowspan="1" colspan="1">Not mentioned</td><td align="center" valign="top" rowspan="1" colspan="1">
<p>Accuracy: 0.59&#x02013;0.92</p>
<p>Sensitivity: 0.54&#x02013;0.99</p>
<p>Specificity: 0.63&#x02013;0.95</p>
</td></tr></tbody></table><table-wrap-foot><fn id="cre270115-tbl3-note-0001"><p>Abbreviations: CBCT, cone beam computed tomography; CI, confidence interval; DJD: degenerative joint disease; MRI, magnetic resonance imaging; OPG, orthopantomogram; TMJ, temporomandibular joint; TMJOA: temporomandibular joint osteoarthritis.</p></fn></table-wrap-foot><permissions><copyright-holder>John Wiley &#x00026; Sons, Ltd.</copyright-holder></permissions></table-wrap><sec id="cre270115-sec-0160"><label>3.1.1</label><title>Summary of AI Classification Models and Their Performance Metrics</title><p>The included studies revealed the application of a variety of AI models for TMJ diagnosis such as convoluted neural networks (<italic toggle="yes">n</italic>&#x02009;=&#x02009;3), artificial neural networks (<italic toggle="yes">n</italic>&#x02009;=&#x02009;2), deep neural networks (<italic toggle="yes">n</italic>&#x02009;=&#x02009;3), decision trees (<italic toggle="yes">n</italic>&#x02009;=&#x02009;2), support vector machines (<italic toggle="yes">n</italic>&#x02009;=&#x02009;2), K&#x02010;nearest neighbors (KNNs) (<italic toggle="yes">n</italic>&#x02009;=&#x02009;3), and deep learning models such as ResNet, Inception V3, and so on.</p><p>Four studies reported the diagnostic accuracy of AI models. The studies reported an accuracy level ranging from 0.59 (Shea et al.&#x000a0;<xref rid="cre270115-bib-0022" ref-type="bibr">2017</xref>) to 1 (Alm&#x00103;&#x00219;an et al.&#x000a0;<xref rid="cre270115-bib-0003" ref-type="bibr">2023</xref>; Talaat et al.&#x000a0;<xref rid="cre270115-bib-0024" ref-type="bibr">2023</xref>). Notably, Almasan et al. noted a high accuracy level with fine&#x02010;tuned ML models (e.g., XGBoost&#x02009;+&#x02009;LightGBM) up to 82.3% accuracy. Farook and Dudley (<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>) found a sensitivity of 0.63&#x02013;0.95 utilizing deep learning models.</p><p>Four studies reported sensitivity levels ranging from 0.76 to 0.80. Alm&#x00103;&#x00219;an et al. (<xref rid="cre270115-bib-0003" ref-type="bibr">2023</xref>) found a pooled sensitivity of 0.76 for ResNet classifications in TMJ Joint Osteoarthritis Diagnosis E, while Jha et al. (<xref rid="cre270115-bib-0013" ref-type="bibr">2022</xref>) discovered a pooled sensitivity of 0.80 for CNNs and KNNs. Zhang et al. (<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>) exhibited 100% sensitivity in identifying degenerative joint conditions using SVM, random forest, logistic regression, and Yolov5. In contrast, Inception V3 exhibited 100% sensitivity in diagnosing disc displacement. Zhang et al. (<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>) observed that LightGBM (0.79) had a higher sensitivity in diagnosing degenerative joint conditions using CBCT than XGBoost (0.76) and random forest (0.76). Farook and Dudley (<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>) reported a specificity of 0.54&#x02013;0.99 for TMJ radiomics.</p><p>Four studies (Xu, Chen, et al.&#x000a0;<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>; Farook and Dudley&#x000a0;<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>; Alm&#x00103;&#x00219;an et al.&#x000a0;<xref rid="cre270115-bib-0003" ref-type="bibr">2023</xref>; Zhang et al.&#x000a0;<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>) found specificity values ranging from 0.63 to 0.95 for TMJ radiomics. Farook and Dudley (<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>) reported a pooled specificity of 0.79 for deep learning models in TMJ Osteoarthritis Diagnosis E, while Farook and Dudley (<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>) reported a specificity of 0.54&#x02013;0.99. Xu et al. found a pooled specificity of 0.80 using CNNs. Zhang et al. (<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>) revealed that the ANN model's specificity in disc displacement is 91.8%. On the other hand, LightGBM (0.781) outperformed XGBoost (0.7650) and random forests (0.745) in detecting DJD using CBCT. Only one study reported an area under the curve, with AUC values ranging from 0.89 to 0.54 to 0.89 of CNN and KNN in the diagnosis of TMJ osteoarthritis (Jha et al.&#x000a0;<xref rid="cre270115-bib-0013" ref-type="bibr">2022</xref>).</p></sec><sec id="cre270115-sec-0170"><label>3.1.2</label><title>Quality Appraisal Findings</title><p>One study was deemed to be of poor quality, one was of severely low quality, and the remaining three were medium quality. The tool's least met domain was reporting funding for included studies and the list of excluded studies (Table&#x000a0;<xref rid="cre270115-tbl-0004" ref-type="table">4</xref>).</p><table-wrap position="float" id="cre270115-tbl-0004" content-type="Table"><label>Table 4</label><caption><p>Quality appraisal using the AMSTAR 2 (assessment of multiple systematic reviews&#x02010;2) checklist of critical and noncritical domains.</p></caption><table frame="hsides" rules="groups"><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="center" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><thead valign="bottom"><tr style="border-bottom:solid 1px #000000" valign="bottom"><th align="left" valign="bottom" rowspan="1" colspan="1">Author and year of publication</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q1</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q2</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q3</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q4</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q5</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q6</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q7</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q8</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q9</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q10</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q11</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q12</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q13</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q14</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q15</th><th align="center" valign="bottom" rowspan="1" colspan="1">Q16</th><th align="center" valign="bottom" rowspan="1" colspan="1">Overall rating</th></tr></thead><tbody valign="top"><tr><td align="left" valign="top" rowspan="1" colspan="1">Alm&#x00103;&#x00219;an et al. (<xref rid="cre270115-bib-0003" ref-type="bibr">2023</xref>)</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FFFF00" align="center" valign="top" rowspan="1" colspan="1">Partial Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FF0000" align="center" valign="top" rowspan="1" colspan="1">No</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FF0000" align="center" valign="top" rowspan="1" colspan="1">No</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td align="center" valign="top" rowspan="1" colspan="1">Low confidence</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Jha et al. (<xref rid="cre270115-bib-0013" ref-type="bibr">2022</xref>)</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FF0000" align="center" valign="top" rowspan="1" colspan="1">No</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FF0000" align="center" valign="top" rowspan="1" colspan="1">No</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FF0000" align="center" valign="top" rowspan="1" colspan="1">No</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td align="center" valign="top" rowspan="1" colspan="1">Critically low confidence</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Xu, Chen, et al. (<xref rid="cre270115-bib-0025" ref-type="bibr">2023</xref>)</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FFFF00" align="center" valign="top" rowspan="1" colspan="1">Partial Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FF0000" align="center" valign="top" rowspan="1" colspan="1">No</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td align="center" valign="top" rowspan="1" colspan="1">Medium confidence</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Zhang et al. (<xref rid="cre270115-bib-0033" ref-type="bibr">2024</xref>)</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FFFF00" align="center" valign="top" rowspan="1" colspan="1">Partial Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td align="center" valign="top" rowspan="1" colspan="1">Medium confidence</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Farook and Dudley (<xref rid="cre270115-bib-0007" ref-type="bibr">2023</xref>)</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FF0000" align="center" valign="top" rowspan="1" colspan="1">No</td><td style="background-color:#FFFF00" align="center" valign="top" rowspan="1" colspan="1">Partial Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#FF0000" align="center" valign="top" rowspan="1" colspan="1">No</td><td style="background-color:#808080" align="center" valign="top" rowspan="1" colspan="1">NA</td><td style="background-color:#808080" align="center" valign="top" rowspan="1" colspan="1">NA</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td style="background-color:#808080" align="center" valign="top" rowspan="1" colspan="1">NA</td><td style="background-color:#00B050" align="center" valign="top" rowspan="1" colspan="1">Yes</td><td align="center" valign="top" rowspan="1" colspan="1">Medium confidence</td></tr></tbody></table><table-wrap-foot><fn id="cre270115-note-tbl4-0001"><p>
<italic toggle="yes">Note:</italic> The colour is highlighting the adherence of the systematic reviews to quality parameters.</p></fn></table-wrap-foot><permissions><copyright-holder>John Wiley &#x00026; Sons, Ltd.</copyright-holder></permissions></table-wrap></sec></sec></sec><sec sec-type="discussion" id="cre270115-sec-0180"><label>4</label><title>Discussion</title><p>TMJ disorder patients often suffer due to inaccurate or delayed diagnosis due to the complex and multifactorial etiology of the disorders. Moreover, the pain associated with TMJ is obscure because it often spreads to other locations distant from joints or related muscles, thus further increasing the diagnosis complexity. Although the Research Diagnostic Criteria for TMDs (RDC/TMD) is a reliable diagnostic tool, the interpretation of TMD signs is largely subjective. Therefore, there is an urgent need to eliminate this subjectivity to improve the diagnosis of TMJ disorders (Talaat et al.&#x000a0;<xref rid="cre270115-bib-0024" ref-type="bibr">2023</xref>). Thus, this UR presents evidence on the performance of AI models in detecting TMJ diseases by combining data on measures such as accuracy, sensitivity, specificity, and area under the curve, so providing a valuable resource to inform future research. In our comprehensive assessment of AI models for TMJ diagnosis, we included evidence from five SRs with and without MA encompassing 78 primary studies.</p><p>The findings demonstrate that AI models have excellent sensitivity, specificity, and accuracy for diagnosing TMJ conditions; nevertheless, their efficiency is significantly dependent on the different AI algorithms and the data set type. Fine&#x02010;tuned ML models, such as XGBoost and LightGBM, have higher diagnosis accuracy levels than other AI models. XGBoost has demonstrated great accuracy in the detection of other diseases, such as lung cancer (Guan et al.&#x000a0;<xref rid="cre270115-bib-0010" ref-type="bibr">2023</xref>), Alzheimer's disease (Yi et al.&#x000a0;<xref rid="cre270115-bib-0029" ref-type="bibr">2023</xref>), chronic renal disease (Ogunleye and Wang&#x000a0;<xref rid="cre270115-bib-0014" ref-type="bibr">2018</xref>), depression (Sharma and Verbeke&#x000a0;<xref rid="cre270115-bib-0021" ref-type="bibr">2020</xref>), and so on. XGBoost, also called extreme gradient boosting (XGBoost), is a gradient boosting decision tree&#x02010;based algorithm that is well&#x02010;known for its great efficiency, adaptability, and portability. It is utilized in data extraction, recommendation systems, and other domains (Song et al.&#x000a0;<xref rid="cre270115-bib-0023" ref-type="bibr">2022</xref>). In addition, it is relatively cheaper, simpler, and easier to interpret than neural networks. Furthermore, it outperforms single ML models such as logistic regression, support vector machine, and decision tree (Guan et al.&#x000a0;<xref rid="cre270115-bib-0010" ref-type="bibr">2023</xref>). Similarly, LightGBM is an efficient gradient boosting method&#x02010;based algorithm. The key advantage is that it significantly accelerates the training process, leading to better&#x02010;performing models. It is based on decision tree algorithms and performs well in classification and regression tasks, surpassing other predictive models. It can optimize decision support systems by determining the number, depth, and leaf nodes of decision trees. LightGBM has been extensively used to diagnose illnesses such as diabetes (Rufo et al.&#x000a0;<xref rid="cre270115-bib-0020" ref-type="bibr">2021</xref>), coronary artery disease (Omotehinwa et al.&#x000a0;<xref rid="cre270115-bib-0015" ref-type="bibr">2024</xref>; Yang et al.&#x000a0;<xref rid="cre270115-bib-0027" ref-type="bibr">2023</xref>), and Parkinson's disease (Dhruva Kumar et al.&#x000a0;<xref rid="cre270115-bib-0006" ref-type="bibr">2022</xref>). Overall, both models&#x02014;XGBoost and LightGBM&#x02014;perform better than neural networks in various structured data tasks because of their strong feature selection, lower prediction time, integrated over&#x02010;fitting preventive features, small&#x02010;to&#x02010;medium data set compatibility, and ability to handle tabular data such as effectively such as patient records (Huang and Chen&#x000a0;<xref rid="cre270115-bib-0012" ref-type="bibr">2021</xref>). While neural networks are excellent at tasks involving unstructured data, they frequently fall short of gradient boosting models in structured datasets.</p><p>ResNet, Inception V3, and machine LightGBM have shown improved sensitivity and specificity in diagnosing specific TMJ problems. This can be attributed to their distinct advantage in feature extraction and categorization. ResNet (Residual networks) established layer&#x02010;level linkage, which eliminates the gradient disappearance problem in deep networks, allowing it to effectively and rapidly assimilate detailed patterns in complex datasets such as MRI or CBCT scans. Its deep architecture allows it to collect fine and broad structural information of images, improving its ability to detect small anomalies (Xu, Fu, et al.&#x000a0;<xref rid="cre270115-bib-0026" ref-type="bibr">2023</xref>; Roy et al.&#x000a0;<xref rid="cre270115-bib-0019" ref-type="bibr">2021</xref>; Al&#x02010;Haija and Adebanjo&#x000a0;<xref rid="cre270115-bib-0002" ref-type="bibr">2020</xref>). Similarly, Inception V3 is made up of multilayer convolutional neural networks with varying filter sizes, allowing it to process intricate features at various levels of detail and thus identify invisible trends or small anomalies in TMJ imaging data that the human eye may miss (Guan et al.&#x000a0;<xref rid="cre270115-bib-0009" ref-type="bibr">2019</xref>). As a result, both deep learning models have demonstrated great sensitivity and specificity in a variety of conditions, including breast cancer, fundus disease (Pan et al.&#x000a0;<xref rid="cre270115-bib-0017" ref-type="bibr">2023</xref>), and Alzheimer's disease (Roy et al.&#x000a0;<xref rid="cre270115-bib-0019" ref-type="bibr">2021</xref>). Collectively, these models offer a major improvement in the accuracy and efficiency of AI&#x02010;powered healthcare solutions.</p><p>While accuracy, sensitivity, and specificity were reported in all investigations, only one study reported area under curve (AUC) values (0.54&#x02013;0.89). AUC is a statistic that compares the true positive rate to false positives at various threshold values, with greater AUC indicating better performance (Bradley et al.&#x000a0;<xref rid="cre270115-bib-0005" ref-type="bibr">2019</xref>). While the findings of a single study indicate greater performance, the small number of studies on the topic implies an inadequate assessment of predictive performance. This indicates that although AI models show potential, further research and validation are needed to ensure their reliability, generalizability, and successful integration into clinical practice.</p><sec id="cre270115-sec-0190"><label>4.1</label><title>Strengths and Limitations</title><p>This UR has several strengths. We adhered to PRISMA and PRIOR guidelines for conducting a systematic search and rigorous analysis of the included studies. Additionally, all assessment phases were carried out by two reviewers, with consistent results. However, there are some limitations to this study. The main limitation is the inability to statistically synthesize the data due to variations in AI models, datasets, and analysis methods across the included studies. Consequently, statistical tests could not be applied to evaluate publication bias within the selected studies. Furthermore, many of the included reviews did not meet SRs guidelines, such as failure to report protocol registration, the funding sources of included papers, or a list of excluded studies. This reduces transparency and raises concerns about potential selection bias. Moreover, most of the reviews did not provide the necessary data to assess classifier performance, such as true positives, true negatives, and false positives. Another limitation is that, although we identified overlapping research, we did not exclude these studies. As a result, the performance ranges presented in our review may include some repetitions. We also did not exclude reviews based on quality, as the majority of the collected reviews were rated as low to moderate. Additionally, although we identified overlapping studies across the included SRs and meta&#x02010;analyses using a citation matrix, we did not adjust the findings to account for potential duplication of primary studies. As a result, some outcomes may reflect redundant data, which could influence the interpretation of the pooled evidence. Future analyses could address these issues by prioritizing the most comprehensive or recent reviews or by weighting outcomes to account for overlap. Moreover, all the included studies utilized data from diagnostic imaging modalities such as MRI, CBCT, and radiography. A recent observational study demonstrated that ML algorithms based on clinical measurement parameters&#x02014;such as mouth opening, pain levels, and oral parafunctions&#x02014;achieved high accuracy, with the Bagging algorithm emerging as the most effective ML model. These findings indicate that ML&#x02010;based predictive models relying on relatively simple and easily accessible clinical data could assist less experienced general dentists in the early detection of TMD. Further research into the use of clinical parameters for TMD diagnosis could enhance early detection while reducing dependence on costly imaging techniques (Y&#x00131;ld&#x00131;z et al.&#x000a0;<xref rid="cre270115-bib-0031" ref-type="bibr">2024</xref>).</p></sec><sec id="cre270115-sec-0200"><label>4.2</label><title>Future Implications for Research and Practice</title><p>Although AI has the potential to detect and diagnose TMJ disorders, the clinical significance of the results may be limited. The small number of studies indicates that AI models for TMJ diagnosis require further validation. Moreover, before AI models are implemented in clinical settings, the data used by these models must be thoroughly refined and evaluated by researchers and healthcare professionals to ensure their applicability and prevent unnecessary healthcare costs and potential adverse outcomes.</p></sec></sec><sec sec-type="conclusions" id="cre270115-sec-0210"><label>5</label><title>Conclusion</title><p>AI has the potential to deliver faster, more accurate, sensitive, and objective diagnoses of TMJ conditions, as evidenced by superior performance metrics. However, the effectiveness of AI depends on the models and datasets utilized. Therefore, before integrating AI&#x02010;based technologies into clinical practice, medical professionals must carefully assess their potential benefits for routine tasks. Additionally, researchers must thoroughly refine and evaluate the AI data before applying these models in clinical settings.</p></sec><sec id="cre270115-sec-0220"><title>Author Contributions</title><p>
<bold>Vini Mehta:</bold> conceptualization, writing &#x02013; review and editing, resources and supervision. <bold>Snehasish Tripathy:</bold> data acquisition, analysis and original manuscript writing. <bold>Toufiq Noor:</bold> writing &#x02013; review and editing, resources and supervision. <bold>Ankita Mathur:</bold> writing &#x02013; review and editing, resources and supervision.</p></sec><sec id="cre270115-sec-0240"><title>Ethics Statement</title><p>The authors have nothing to report.</p></sec><sec id="cre270115-sec-0250"><title>Consent</title><p>The authors have nothing to report.</p></sec><sec sec-type="COI-statement" id="cre270115-sec-0260"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></sec><sec sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="cre270115-suppl-0001" position="float" content-type="local-data"><caption><p>Supporting information.</p></caption><media xlink:href="CRE2-11-e70115-s001.docx"/></supplementary-material></sec></body><back><ack id="cre270115-sec-0230"><title>Acknowledgments</title><p>The authors have nothing to report.</p></ack><sec sec-type="data-availability" id="cre270115-sec-0280"><title>Data Availability Statement</title><p>The data that supports the findings of this study are available in the Supporting material of this article.</p></sec><ref-list content-type="cited-references" id="cre270115-bibl-0001"><title>References</title><ref id="cre270115-bib-0001"><mixed-citation publication-type="journal" id="cre270115-cit-0001">
<string-name>
<surname>Abd&#x02010;alrazaq</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<given-names>D.</given-names>
<surname>Alhuwail</surname>
</string-name>, <string-name>
<given-names>J.</given-names>
<surname>Schneider</surname>
</string-name>, et al. <year>2022</year>. &#x0201c;<article-title>The Performance of Artificial Intelligence&#x02010;Driven Technologies in Diagnosing Mental Disorders: An Umbrella Review</article-title>.&#x0201d; <source>Npj Digital Medicine</source>
<volume>5</volume>, no. <issue>1</issue>: <fpage>1</fpage>&#x02013;<lpage>12</lpage>.<pub-id pub-id-type="pmid">35013539</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0002"><mixed-citation publication-type="book" id="cre270115-cit-0002">
<string-name>
<surname>Al&#x02010;Haija</surname>, <given-names>Q. A.</given-names>
</string-name>, and <string-name>
<given-names>A.</given-names>
<surname>Adebanjo</surname>
</string-name>. <year>2020</year>. &#x0201c;<part-title>Breast Cancer Diagnosis in Histopathological Images Using ResNet&#x02010;50 Convolutional Neural Network</part-title>.&#x0201d; In <source>2020 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS)</source>, <publisher-loc>Vancouver, BC, Canada</publisher-loc>, <fpage>1</fpage>&#x02013;<lpage>7</lpage>. <publisher-name>IEEE</publisher-name>.</mixed-citation></ref><ref id="cre270115-bib-0003"><mixed-citation publication-type="journal" id="cre270115-cit-0003">
<string-name>
<surname>Alm&#x00103;&#x00219;an</surname>, <given-names>O.</given-names>
</string-name>, <string-name>
<given-names>D.&#x02010;C.</given-names>
<surname>Leucu&#x0021b;a</surname>
</string-name>, <string-name>
<given-names>M.</given-names>
<surname>Hede&#x00219;iu</surname>
</string-name>, <string-name>
<given-names>S.</given-names>
<surname>Mure&#x00219;anu</surname>
</string-name>, and <string-name>
<given-names>L.</given-names>
<surname>Popa</surname>
</string-name>. <year>2023</year>. &#x0201c;<article-title>Temporomandibular Joint Osteoarthritis Diagnosis Employing Artificial Intelligence: Systematic Review and Meta&#x02010;Analysis</article-title>.&#x0201d; <source>Journal of Clinical Medicine</source>
<volume>12</volume>, no. <issue>3</issue>: <fpage>942</fpage>.<pub-id pub-id-type="pmid">36769590</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0004"><mixed-citation publication-type="book" id="cre270115-cit-0004">
<string-name>
<surname>Aromataris</surname>, <given-names>E.</given-names>
</string-name>, <string-name>
<given-names>R.</given-names>
<surname>Fernandez</surname>
</string-name>, <string-name>
<given-names>C.</given-names>
<surname>Godfrey</surname>
</string-name>, <string-name>
<given-names>H.</given-names>
<surname>Khalil</surname>
</string-name>, and <string-name>
<given-names>P.</given-names>
<surname>Bhatarasakoon</surname>
</string-name>. <year>2020</year>. &#x0201c;<part-title>Chapter 10: Umbrella Reviews</part-title>.&#x0201d; In <source>JBI Manual for Evidence Synthesis</source>. <publisher-name>JBI</publisher-name>.</mixed-citation></ref><ref id="cre270115-bib-0005"><mixed-citation publication-type="journal" id="cre270115-cit-0005">
<string-name>
<surname>Bradley</surname>, <given-names>E.</given-names>
</string-name>, <string-name>
<given-names>K.</given-names>
<surname>Forsberg</surname>
</string-name>, <string-name>
<given-names>J. E.</given-names>
<surname>Betts</surname>
</string-name>, et al. <year>2019</year>. &#x0201c;<article-title>Factors Affecting Pre&#x02010;Exposure Prophylaxis Implementation for Women in the United States: A Systematic Review</article-title>.&#x0201d; <source>Journal of Women's Health</source>
<volume>28</volume>, no. <issue>9</issue>: <fpage>1272</fpage>&#x02013;<lpage>1285</lpage>.</mixed-citation></ref><ref id="cre270115-bib-0006"><mixed-citation publication-type="book" id="cre270115-cit-0006">
<string-name>
<surname>Dhruva Kumar</surname>, <given-names>G. V.</given-names>
</string-name>, <string-name>
<given-names>V.</given-names>
<surname>Deepa</surname>
</string-name>, <string-name>
<given-names>N.</given-names>
<surname>Vineela</surname>
</string-name>, and <string-name>
<given-names>G.</given-names>
<surname>Emmanuel</surname>
</string-name>, <year>2022</year>. &#x0201c;<part-title>LightGBM Model Based Parkinson's Disease Detection by Using Spiral Drawings</part-title>.&#x0201d; In <source>2022 Sixth International Conference on I&#x02010;SMAC (IoT in Social, Mobile, Analytics and Cloud) (I&#x02010;SMAC)</source>, <publisher-loc>Dharan, Nepal</publisher-loc>, <fpage>1</fpage>&#x02013;<lpage>5</lpage>. <publisher-name>IEEE</publisher-name>.</mixed-citation></ref><ref id="cre270115-bib-0007"><mixed-citation publication-type="journal" id="cre270115-cit-0007">
<string-name>
<surname>Farook</surname>, <given-names>T. H.</given-names>
</string-name>, and <string-name>
<given-names>J.</given-names>
<surname>Dudley</surname>
</string-name>. <year>2023</year>. &#x0201c;<article-title>Automation and Deep (Machine) Learning in Temporomandibular Joint Disorder Radiomics: A Systematic Review</article-title>.&#x0201d; <source>Journal of Oral Rehabilitation</source>
<volume>50</volume>, no. <issue>6</issue>: <fpage>501</fpage>&#x02013;<lpage>521</lpage>.<pub-id pub-id-type="pmid">36843391</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0008"><mixed-citation publication-type="journal" id="cre270115-cit-0008">
<string-name>
<surname>Felin</surname>, <given-names>G. C.</given-names>
</string-name>, <string-name>
<given-names>C. V. C.</given-names>
<surname>Tagliari</surname>
</string-name>, <string-name>
<given-names>B. A.</given-names>
<surname>Agostini</surname>
</string-name>, and <string-name>
<given-names>K.</given-names>
<surname>Collares</surname>
</string-name>. <year>2024</year>. &#x0201c;<article-title>Prevalence of Psychological Disorders in Patients With Temporomandibular Disorders: A Systematic Review and Meta&#x02010;Analysis</article-title>.&#x0201d; <source>Journal of Prosthetic Dentistry</source>
<volume>132</volume>, no. <issue>2</issue>: <fpage>392</fpage>&#x02013;<lpage>401</lpage>.<pub-id pub-id-type="pmid">36114016</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0009"><mixed-citation publication-type="journal" id="cre270115-cit-0009">
<string-name>
<surname>Guan</surname>, <given-names>Q.</given-names>
</string-name>, <string-name>
<given-names>X.</given-names>
<surname>Wan</surname>
</string-name>, <string-name>
<given-names>H.</given-names>
<surname>Lu</surname>
</string-name>, et al. <year>2019</year>. &#x0201c;<article-title>Deep Convolutional Neural Network Inception&#x02010;v3 Model for Differential Diagnosing of Lymph Node in Cytological Images: A Pilot Study</article-title>.&#x0201d; <source>Annals of Translational Medicine</source>
<volume>7</volume>, no. <issue>14</issue>: <fpage>307</fpage>.<pub-id pub-id-type="pmid">31475177</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0010"><mixed-citation publication-type="journal" id="cre270115-cit-0010">
<string-name>
<surname>Guan</surname>, <given-names>X.</given-names>
</string-name>, <string-name>
<given-names>Y.</given-names>
<surname>Du</surname>
</string-name>, <string-name>
<given-names>R.</given-names>
<surname>Ma</surname>
</string-name>, et al. <year>2023</year>. &#x0201c;<article-title>Construction of the Xgboost Model for Early Lung Cancer Prediction Based on Metabolic Indices</article-title>.&#x0201d; <source>BMC Medical Informatics and Decision Making</source>
<volume>23</volume>, no. <issue>1</issue>: <fpage>107</fpage>.<pub-id pub-id-type="pmid">37312179</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0011"><mixed-citation publication-type="miscellaneous" id="cre270115-cit-0011">
<string-name>
<surname>Huang</surname>, <given-names>C.</given-names>
</string-name>, <string-name>
<given-names>Y.</given-names>
<surname>Song</surname>
</string-name>, <string-name>
<given-names>J. Z.</given-names>
<surname>Dong</surname>
</string-name>, et al. <year>2024</year>. <italic toggle="yes">Current and Potential Applications of Artificial Intelligence in Endoscopic Imaging of Gastrointestinal Tumor: An Umbrella Review. SSRN Scholarly Paper</italic>. Social Science Research Network.</mixed-citation></ref><ref id="cre270115-bib-0012"><mixed-citation publication-type="journal" id="cre270115-cit-0012">
<string-name>
<surname>Huang</surname>, <given-names>Z.</given-names>
</string-name>, and <string-name>
<given-names>Z.</given-names>
<surname>Chen</surname>
</string-name>. <year>2021</year>. &#x0201c;<article-title>Comparison of Different Machine Learning Algorithms for Predicting the SAGD Production Performance</article-title>.&#x0201d; <source>Journal of Petroleum Science and Engineering</source>
<volume>202</volume>: <elocation-id>108559</elocation-id>.</mixed-citation></ref><ref id="cre270115-bib-0013"><mixed-citation publication-type="journal" id="cre270115-cit-0013">
<string-name>
<surname>Jha</surname>, <given-names>N.</given-names>
</string-name>, <string-name>
<given-names>K.</given-names>
<surname>Lee</surname>
</string-name>, and <string-name>
<given-names>Y.&#x02010;J.</given-names>
<surname>Kim</surname>
</string-name>. <year>2022</year>. &#x0201c;<article-title>Diagnosis of Temporomandibular Disorders Using Artificial Intelligence Technologies: A Systematic Review and Meta&#x02010;Analysis</article-title>.&#x0201d; <source>PLoS One</source>
<volume>17</volume>, no. <issue>8</issue>: <elocation-id>e0272715</elocation-id>.<pub-id pub-id-type="pmid">35980894</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0014"><mixed-citation publication-type="book" id="cre270115-cit-0014">
<string-name>
<surname>Ogunleye</surname>, <given-names>A.</given-names>
</string-name>, and <string-name>
<given-names>Q.&#x02010;G.</given-names>
<surname>Wang</surname>
</string-name>. <year>2018</year>. &#x0201c;<part-title>Enhanced XGBoost&#x02010;Based Automatic Diagnosis System for Chronic Kidney Disease</part-title>.&#x0201d; In <source>2018 IEEE 14th International Conference on Control and Automation (ICCA)</source>, <publisher-loc>Anchorage, AK, USA</publisher-loc>, <fpage>805</fpage>&#x02013;<lpage>810</lpage>. <publisher-name>IEEE</publisher-name>.</mixed-citation></ref><ref id="cre270115-bib-0015"><mixed-citation publication-type="journal" id="cre270115-cit-0015">
<string-name>
<surname>Omotehinwa</surname>, <given-names>T. O.</given-names>
</string-name>, <string-name>
<given-names>D. O.</given-names>
<surname>Oyewola</surname>
</string-name>, and <string-name>
<given-names>E. G.</given-names>
<surname>Moung</surname>
</string-name>. <year>2024</year>. &#x0201c;<article-title>Optimizing the Light Gradient&#x02010;Boosting Machine Algorithm for an Efficient Early Detection of Coronary Heart Disease</article-title>.&#x0201d; <source>Informatics and Health</source>
<volume>1</volume>, no. <issue>2</issue>: <fpage>70</fpage>&#x02013;<lpage>81</lpage>.</mixed-citation></ref><ref id="cre270115-bib-0016"><mixed-citation publication-type="journal" id="cre270115-cit-0016">
<string-name>
<surname>Ozsari</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<given-names>M. S.</given-names>
<surname>G&#x000fc;zel</surname>
</string-name>, <string-name>
<given-names>D.</given-names>
<surname>Y&#x00131;lmaz</surname>
</string-name>, and <string-name>
<given-names>K.</given-names>
<surname>Kamburo&#x0011f;lu</surname>
</string-name>. <year>2023</year>. &#x0201c;<article-title>A Comprehensive Review of Artificial Intelligence Based Algorithms Regarding Temporomandibular Joint Related Diseases</article-title>.&#x0201d; <source>Diagnostics</source>
<volume>13</volume>, no. <issue>16</issue>: <fpage>2700</fpage>.<pub-id pub-id-type="pmid">37627959</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0017"><mixed-citation publication-type="journal" id="cre270115-cit-0017">
<string-name>
<surname>Pan</surname>, <given-names>Y.</given-names>
</string-name>, <string-name>
<given-names>J.</given-names>
<surname>Liu</surname>
</string-name>, <string-name>
<given-names>Y.</given-names>
<surname>Cai</surname>
</string-name>, et al. <year>2023</year>. &#x0201c;<article-title>Fundus Image Classification Using Inception V3 and ResNet&#x02010;50 for the Early Diagnostics of Fundus Diseases</article-title>.&#x0201d; <source>Frontiers in Physiology</source>
<volume>14</volume>: <fpage>14</fpage>.</mixed-citation></ref><ref id="cre270115-bib-0018"><mixed-citation publication-type="journal" id="cre270115-cit-0018">
<string-name>
<surname>Pollock</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<given-names>R. M.</given-names>
<surname>Fernandes</surname>
</string-name>, <string-name>
<given-names>D.</given-names>
<surname>Pieper</surname>
</string-name>, et al. <year>2019</year>. &#x0201c;<article-title>Preferred Reporting Items for Overviews of Reviews (PRIOR): A Protocol for Development of a Reporting Guideline for Overviews of Reviews of Healthcare Interventions</article-title>.&#x0201d; <source>Systematic Reviews</source>
<volume>8</volume>, no. <issue>1</issue>: <fpage>335</fpage>.<pub-id pub-id-type="pmid">31870434</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0019"><mixed-citation publication-type="book" id="cre270115-cit-0019">
<string-name>
<surname>Roy</surname>, <given-names>P.</given-names>
</string-name>, <string-name>
<given-names>M. M.</given-names>
<surname>Oddin Chisty</surname>
</string-name>, and <string-name>
<given-names>H. M.</given-names>
<surname>Abdul Fattah</surname>
</string-name>. <year>2021</year>. &#x0201c;<part-title>Alzheimer's Disease Diagnosis From MRI Images Using ResNet&#x02010;152 Neural Network Architecture</part-title>.&#x0201d; In <source>2021 5th International Conference on Electrical Information and Communication Technology (EICT)</source>, <publisher-loc>Khulna, Bangladesh</publisher-loc>, <fpage>1</fpage>&#x02013;<lpage>6</lpage>. <publisher-name>IEEE</publisher-name>.</mixed-citation></ref><ref id="cre270115-bib-0020"><mixed-citation publication-type="journal" id="cre270115-cit-0020">
<string-name>
<surname>Rufo</surname>, <given-names>D. D.</given-names>
</string-name>, <string-name>
<given-names>T. G.</given-names>
<surname>Debelee</surname>
</string-name>, <string-name>
<given-names>A.</given-names>
<surname>Ibenthal</surname>
</string-name>, and <string-name>
<given-names>W. G.</given-names>
<surname>Negera</surname>
</string-name>. <year>2021</year>. &#x0201c;<article-title>Diagnosis of Diabetes Mellitus Using Gradient Boosting Machine (LightGBM)</article-title>.&#x0201d; <source>Diagnostics</source>
<volume>11</volume>, no. <issue>9</issue>: <fpage>1714</fpage>.<pub-id pub-id-type="pmid">34574055</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0021"><mixed-citation publication-type="journal" id="cre270115-cit-0021">
<string-name>
<surname>Sharma</surname>, <given-names>A.</given-names>
</string-name>, and <string-name>
<given-names>W. J. M. I.</given-names>
<surname>Verbeke</surname>
</string-name>. <year>2020</year>. &#x0201c;<article-title>Improving Diagnosis of Depression With XGBOOST Machine Learning Model and a Large Biomarkers Dutch Dataset (n = 11,081)</article-title>.&#x0201d; <source>Frontiers in Big Data</source>
<volume>3</volume>: <fpage>3</fpage>.<pub-id pub-id-type="pmid">33693378</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0022"><mixed-citation publication-type="journal" id="cre270115-cit-0022">
<string-name>
<surname>Shea</surname>, <given-names>B. J.</given-names>
</string-name>, <string-name>
<given-names>B. C.</given-names>
<surname>Reeves</surname>
</string-name>, <string-name>
<given-names>G.</given-names>
<surname>Wells</surname>
</string-name>, et al. <year>2017</year>. &#x0201c;<article-title>AMSTAR 2: A Critical Appraisal Tool for Systematic Reviews That Include Randomised or Non&#x02010;Randomised Studies of Healthcare Interventions, or Both</article-title>.&#x0201d; <source>BMJ (London)</source>
<volume>358</volume>: <fpage>j4008</fpage>.</mixed-citation></ref><ref id="cre270115-bib-0023"><mixed-citation publication-type="journal" id="cre270115-cit-0023">
<string-name>
<surname>Song</surname>, <given-names>X.</given-names>
</string-name>, <string-name>
<given-names>J.</given-names>
<surname>Zhu</surname>
</string-name>, <string-name>
<given-names>X.</given-names>
<surname>Tan</surname>
</string-name>, et al. <year>2022</year>. &#x0201c;<article-title>XGBoost&#x02010;Based Feature Learning Method for Mining COVID&#x02010;19 Novel Diagnostic Markers</article-title>.&#x0201d; <source>Frontiers in Public Health</source>
<volume>10</volume>: <fpage>10</fpage>.</mixed-citation></ref><ref id="cre270115-bib-0024"><mixed-citation publication-type="journal" id="cre270115-cit-0024">
<string-name>
<surname>Talaat</surname>, <given-names>W. M.</given-names>
</string-name>, <string-name>
<given-names>S.</given-names>
<surname>Shetty</surname>
</string-name>, <string-name>
<given-names>S.</given-names>
<surname>Al Bayatti</surname>
</string-name>, et al. <year>2023</year>. &#x0201c;<article-title>An Artificial Intelligence Model for the Radiographic Diagnosis of Osteoarthritis of the Temporomandibular Joint</article-title>.&#x0201d; <source>Scientific Reports</source>
<volume>13</volume>, no. <issue>1</issue>: <elocation-id>15972</elocation-id>.<pub-id pub-id-type="pmid">37749161</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0025"><mixed-citation publication-type="journal" id="cre270115-cit-0025">
<string-name>
<surname>Xu</surname>, <given-names>L.</given-names>
</string-name>, <string-name>
<given-names>J.</given-names>
<surname>Chen</surname>
</string-name>, <string-name>
<given-names>K.</given-names>
<surname>Qiu</surname>
</string-name>, <string-name>
<given-names>F.</given-names>
<surname>Yang</surname>
</string-name>, and <string-name>
<given-names>W.</given-names>
<surname>Wu</surname>
</string-name>. <year>2023</year>. &#x0201c;<article-title>Artificial Intelligence for Detecting Temporomandibular Joint Osteoarthritis Using Radiographic Image Data: A Systematic Review and Meta&#x02010;Analysis of Diagnostic Test Accuracy</article-title>.&#x0201d; <source>PLoS One</source>
<volume>18</volume>, no. <issue>7</issue>: <elocation-id>e0288631</elocation-id>.<pub-id pub-id-type="pmid">37450501</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0026"><mixed-citation publication-type="journal" id="cre270115-cit-0026">
<string-name>
<surname>Xu</surname>, <given-names>W.</given-names>
</string-name>, <string-name>
<given-names>Y.&#x02010;L.</given-names>
<surname>Fu</surname>
</string-name>, and <string-name>
<given-names>D.</given-names>
<surname>Zhu</surname>
</string-name>. <year>2023</year>. &#x0201c;<article-title>ResNet and Its Application to Medical Image Processing: Research Progress and Challenges</article-title>.&#x0201d; <source>Computer Methods and Programs in Biomedicine</source>
<volume>240</volume>: <elocation-id>107660</elocation-id>.<pub-id pub-id-type="pmid">37320940</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0027"><mixed-citation publication-type="journal" id="cre270115-cit-0027">
<string-name>
<surname>Yang</surname>, <given-names>H.</given-names>
</string-name>, <string-name>
<given-names>Z.</given-names>
<surname>Chen</surname>
</string-name>, <string-name>
<given-names>H.</given-names>
<surname>Yang</surname>
</string-name>, and <string-name>
<given-names>M.</given-names>
<surname>Tian</surname>
</string-name>. <year>2023</year>. &#x0201c;<article-title>Predicting Coronary Heart Disease Using an Improved LightGBM Model: Performance Analysis and Comparison</article-title>.&#x0201d; <source>IEEE Access</source>
<volume>11</volume>: <fpage>23366</fpage>&#x02013;<lpage>23380</lpage>.</mixed-citation></ref><ref id="cre270115-bib-0028"><mixed-citation publication-type="journal" id="cre270115-cit-0028">
<string-name>
<surname>Yeti&#x0015f;</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<given-names>H.</given-names>
<surname>Kocaman</surname>
</string-name>, <string-name>
<given-names>M.</given-names>
<surname>Canl&#x00131;</surname>
</string-name>, <string-name>
<given-names>H.</given-names>
<surname>Y&#x00131;ld&#x00131;r&#x00131;m</surname>
</string-name>, <string-name>
<given-names>A.</given-names>
<surname>Yeti&#x0015f;</surname>
</string-name>, and <string-name>
<given-names>&#x00130;.</given-names>
<surname>Ceylan</surname>
</string-name>. <year>2024</year>. &#x0201c;<article-title>Carpal Tunnel Syndrome Prediction With Machine Learning Algorithms Using Anthropometric and Strength&#x02010;Based Measurement</article-title>.&#x0201d; <source>PLoS One</source>
<volume>19</volume>, no. <issue>4</issue>: <elocation-id>e0300044</elocation-id>.<pub-id pub-id-type="pmid">38630703</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0029"><mixed-citation publication-type="journal" id="cre270115-cit-0029">
<string-name>
<surname>Yi</surname>, <given-names>F.</given-names>
</string-name>, <string-name>
<given-names>H.</given-names>
<surname>Yang</surname>
</string-name>, <string-name>
<given-names>D.</given-names>
<surname>Chen</surname>
</string-name>, et al. <year>2023</year>. &#x0201c;<article-title>XGBoost&#x02010;SHAP&#x02010;Based Interpretable Diagnostic Framework for Alzheimer's Disease</article-title>.&#x0201d; <source>BMC Medical Informatics and Decision Making</source>
<volume>23</volume>, no. <issue>1</issue>: <fpage>137</fpage>.<pub-id pub-id-type="pmid">37491248</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0030"><mixed-citation publication-type="book" id="cre270115-cit-0030">
<string-name>
<surname>Yost</surname>, <given-names>O.</given-names>
</string-name>, <string-name>
<given-names>C. T.</given-names>
<surname>Liverman</surname>
</string-name>, <string-name>
<given-names>R.</given-names>
<surname>English</surname>
</string-name>, <string-name>
<given-names>S.</given-names>
<surname>Mackey</surname>
</string-name>, and <string-name>
<given-names>E. C.</given-names>
<surname>Bond</surname>
</string-name>. <year>2020</year>. <source>Individual and Societal Burden of TMDs, in Temporomandibular Disorders: Priorities for Research and Care</source>. <publisher-name>National Academies Press</publisher-name>.</mixed-citation></ref><ref id="cre270115-bib-0031"><mixed-citation publication-type="journal" id="cre270115-cit-0031">
<string-name>
<surname>Y&#x00131;ld&#x00131;z</surname>, <given-names>N. T.</given-names>
</string-name>, <string-name>
<given-names>H.</given-names>
<surname>Kocaman</surname>
</string-name>, <string-name>
<given-names>H.</given-names>
<surname>Y&#x00131;ld&#x00131;r&#x00131;m</surname>
</string-name>, and <string-name>
<given-names>M.</given-names>
<surname>Canl&#x00131;</surname>
</string-name>. <year>2024</year>. &#x0201c;<article-title>An Investigation of Machine Learning Algorithms for Prediction of Temporomandibular Disorders by Using Clinical Parameters</article-title>.&#x0201d; <source>Medicine</source>
<volume>103</volume>, no. <issue>41</issue>: <elocation-id>e39912</elocation-id>.<pub-id pub-id-type="pmid">39465879</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0032"><mixed-citation publication-type="journal" id="cre270115-cit-0032">
<string-name>
<surname>Zha</surname>, <given-names>B.</given-names>
</string-name>, <string-name>
<given-names>A.</given-names>
<surname>Cai</surname>
</string-name>, and <string-name>
<given-names>G.</given-names>
<surname>Wang</surname>
</string-name>. <year>2024</year>. &#x0201c;<article-title>Diagnostic Accuracy of Artificial Intelligence in Endoscopy: Umbrella Review</article-title>.&#x0201d; <source>JMIR Medical Informatics</source>
<volume>12</volume>, no. <issue>1</issue>: <elocation-id>e56361</elocation-id>.<pub-id pub-id-type="pmid">39093715</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0033"><mixed-citation publication-type="journal" id="cre270115-cit-0033">
<string-name>
<surname>Zhang</surname>, <given-names>Y.</given-names>
</string-name>, <string-name>
<given-names>T.</given-names>
<surname>Zhu</surname>
</string-name>, <string-name>
<given-names>Y.</given-names>
<surname>Zheng</surname>
</string-name>, et al. <year>2024</year>. &#x0201c;<article-title>Machine Learning&#x02010;Based Medical Imaging Diagnosis in Patients With Temporomandibular Disorders: A Diagnostic Test Accuracy Systematic Review and Meta&#x02010;Analysis</article-title>.&#x0201d; <source>Clinical Oral Investigations</source>
<volume>28</volume>, no. <issue>3</issue>: <fpage>186</fpage>.<pub-id pub-id-type="pmid">38430334</pub-id>
</mixed-citation></ref><ref id="cre270115-bib-0034"><mixed-citation publication-type="journal" id="cre270115-cit-0034">
<string-name>
<surname>Zieli&#x00144;ski</surname>, <given-names>G.</given-names>
</string-name>, <string-name>
<given-names>B.</given-names>
<surname>Paj&#x00105;k&#x02010;Zieli&#x00144;ska</surname>
</string-name>, and <string-name>
<given-names>M.</given-names>
<surname>Ginszt</surname>
</string-name>. <year>2024</year>. &#x0201c;<article-title>A Meta&#x02010;Analysis of the Global Prevalence of Temporomandibular Disorders</article-title>.&#x0201d; <source>Journal of Clinical Medicine</source>
<volume>13</volume>, no. <issue>5</issue>: <fpage>1365</fpage>.<pub-id pub-id-type="pmid">38592227</pub-id>
</mixed-citation></ref></ref-list></back></article>