<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40394017</article-id><article-id pub-id-type="pmc">PMC12092676</article-id>
<article-id pub-id-type="publisher-id">1578</article-id><article-id pub-id-type="doi">10.1038/s41598-025-01578-y</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A metaverse laboratory setup for interactive atom visualization and manipulation with scanning probe microscopy</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5358-444X</contrib-id><name><surname>Diao</surname><given-names>Zhuo</given-names></name><address><email>diao.zhuo.es@osaka-u.ac.jp</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4473-5773</contrib-id><name><surname>Yamashita</surname><given-names>Hayato</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5619-3911</contrib-id><name><surname>Abe</surname><given-names>Masayuki</given-names></name><address><email>abe.masayuki.es@osaka-u.ac.jp</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/035t8zc32</institution-id><institution-id institution-id-type="GRID">grid.136593.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 0373 3971</institution-id><institution>Graduate School of Engineering Science, </institution><institution>Osaka University, </institution></institution-wrap>1-3 Machikaneyama, Toyonaka, Osaka, 560-8531 Japan </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><elocation-id>17490</elocation-id><history><date date-type="received"><day>4</day><month>3</month><year>2025</year></date><date date-type="accepted"><day>7</day><month>5</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">We present a metaverse laboratory system that integrates mixed reality (MR) technologies with scanning probe microscopy (SPM) for interactive atomic-scale visualization and manipulation. In order to accommodate both the visualization and input of SPM data in a virtual environment and the physical interaction with SPM-related equipment in the laboratory, the system incorporates a virtual reality (VR) and augmented reality (AR) framework to enable seamless switching between these two environments. Utilizing the pose-tracking capabilities in AR, users can intuitively interact with virtual interface elements and three-dimensional objects through physical hand gesture input to control SPM parameters and probe positioning. The system provides real-time visualization of scanned surfaces at the atomic scale in the virtual environment, enabling immediate feedback during experiments. To demonstrate the system&#x02019;s capabilities, we performed atomic manipulation experiments using hand gestures for lateral probe positioning, showing how MR-enhanced SPM can simplify nanoscale operations and improve experimental efficiency. Our integrated MR&#x02013;SPM system allows users to conduct experiments via the metaverse platform while enhancing the human-instrument interaction experience. It extends the practical utility required for both real-time physical and virtual environment SPM operations in the laboratory, making nanoscale research more accessible and intuitive.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Characterization and analytical techniques</kwd><kwd>Microscopy</kwd><kwd>Information technology</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Metaverse technologies, such as virtual reality (VR), augmented reality (AR), and mixed reality (MR), have gained significant attention as emerging platforms that transcend physical constraints and accelerate the digitalization of society.<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> These immersive technologies provide intuitive and interactive interfaces that enhance efficiency and adaptability in business<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, education<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, and healthcare<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. The applications also span multiple scientific domains, including data visualization<sup><xref ref-type="bibr" rid="CR5">5</xref>&#x02013;<xref ref-type="bibr" rid="CR7">7</xref></sup>, scientific computing<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>, and experimental sciences<sup><xref ref-type="bibr" rid="CR10">10</xref>&#x02013;<xref ref-type="bibr" rid="CR13">13</xref></sup>. In laboratory settings, metaverse technologies enable enhanced experimental operations and data exploration, potentially leading to more efficient research workflows. For training purposes, these platforms allow users to practice experimental procedures in a virtual environment before using actual equipment. While the implementation of metaverse technologies in laboratories is still in its early stages, their integration shows promising potential for advancing scientific research methodologies.</p><p id="Par3">One area of scientific research where metaverse technologies can be particularly beneficial is nanoscience and nanotechnology. In these fields, scanning probe microscopy (SPM) is widely used to study nanoscale properties across materials science and biological systems. SPM is a powerful technique for measuring surface structures and properties with resolution ranging from nanoscale to atomic scale. When operating at its highest performance, it enables atomic-resolution imaging, providing information about surface topography as well as electrical, magnetic, and mechanical properties. These capabilities, combined with its ability to operate in various environments including liquids and controlled atmospheres, make SPM an essential tool for investigating both materials and biological systems. Despite its capabilities, SPM operation requires significant expertise, from sample preparation to optimization of measurement parameters and data analysis. Achieving atomic-resolution imaging and spectroscopy demands particularly advanced skills, which can present a substantial challenge for new users.</p><p id="Par4">To address these operational challenges, the integration of metaverse technologies with SPM offers a potential solution. By virtually recreating SPM, it provides a simplified and intuitive operating environment<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>, making beginners easier to experience SPM. Furthermore, this integration facilitates remote experiment operation and promotes collaborative research across institutions<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. The synergy between metaverse and SPM not only enhances education by providing accessible training platforms but also advances surface science research. Simulating and optimizing experimental conditions in metaverse also enables efficient experimental design<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Recent advancements in microscopy systems have focused on integrating head-mounted displays (HMDs) with VR computer graphics and implementing hand-controlled manipulation<sup><xref ref-type="bibr" rid="CR15">15</xref>&#x02013;<xref ref-type="bibr" rid="CR17">17</xref></sup> to gain a deeper intuition of scientific phenomena in experimental instruments. Through these developments, we anticipate the emergence of innovative SPM experimental systems that combine virtual and physical capabilities. However, pure VR environments isolate users from the physical laboratory space, potentially limiting their ability to maintain awareness of crucial experimental conditions and safety considerations.</p><p id="Par5">In response to these challenges, the MR technique combining VR with AR offers a more comprehensive solution by overlaying virtual objects onto the real world. The scene understanding capability enables interaction with digital SPM data while maintaining direct visual contact with the physical experimental setup. Additionally, the pose-tracking technique provides a natural hand input interface for engaging with three-dimensional data, thereby improving the efficiency of spatial relationship-based experiments. This physical hand-based interaction method unifies both the physical and virtual inputs within the laboratory environment. After integrating real-space tracking functionalities from AR into SPM, the system creates a more practical and efficient user experience compared to VR-only solutions.</p><p id="Par6">In this study, we have developed an MR&#x02013;SPM system that introduces gesture-based probe manipulation and imaging control, enabling intuitive operation analogous to robotic arm control. MR seamlessly blends virtual elements with the real laboratory environment, enabling the simultaneous monitoring of both virtual and physical aspects of experiments. The SPM operations represent a significant advancement over traditional SPM control methods, combining the accessibility of MR interfaces with the precision required for atomic-scale operations. Our MR&#x02013;SPM system complements traditional atomic resolution imaging and manipulation capabilities by providing an enhanced spatial perspective that helps operators better conceptualize three-dimensional atomic arrangements, particularly useful for complex manipulation sequences at room temperature.</p></sec><sec id="Sec2"><title>MR&#x02013;SPM system</title><sec id="Sec3"><title>The concept of the metaverse laboratory setup</title><p id="Par7">
<fig id="Fig1"><label>Fig. 1</label><caption><p>The concept of the mixed reality scanning probe microscopy (MR&#x02013;SPM) system, including several components for the metaverse laboratory setup.</p></caption><graphic xlink:href="41598_2025_1578_Fig1_HTML" id="MO1"/></fig>
</p><p id="Par8">Figure <xref rid="Fig1" ref-type="fig">1</xref> illustrates our MR&#x02013;SPM framework, which integrates VR for collaborative experimentation and AR for real-world environment tracking. The system consists of three key components: <list list-type="order"><list-item><p id="Par9">Control Interface: An FPGA device digitalizes the experimental setup, enabling precise control of instruments via TCP commands over a local network. When designing such a system, it is important to provide customizable APIs that allow user modding to implement additional SPM functionalities<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>.</p></list-item><list-item><p id="Par10">Console and Server: Implemented in Unity, this component serves as the VR workspace client and executes user commands to control the instruments. The administrator&#x02019;s view is shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, which allows monitoring of the user&#x02019;s movements (red rim effect and hands rendering) and managing the virtual environment&#x02019;s parameters and objects through the sidebar menu.</p></list-item><list-item><p id="Par11">MR Interface: An adaptive transition system between VR and physical visuals, a pose tracking input system, and a hand-gesture UI allow users to simultaneously interact with virtual and real environments.</p></list-item></list>The VR workspace enables researchers worldwide to conduct experiments unconstrained by physical labs<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, while AR enhances the system with body feedback mechanisms like pose tracking<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> and simultaneous localization and mapping<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. Merging VR and AR facilitates seamless data integration between real and virtual spaces. AR-assisted manual interfaces guide users through instrument operation by overlaying visual instructions<sup><xref ref-type="bibr" rid="CR23">23</xref>&#x02013;<xref ref-type="bibr" rid="CR25">25</xref></sup>. The MR system accommodates diverse laboratory instruments requiring physical interaction by enabling transitions between VR and traditional visuals, enhancing the experimental workflow and user experience.</p></sec><sec id="Sec4"><title>Implementation of MR interface for SPM</title><p id="Par12">Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the view seen by the experimental operator while wearing a headset during the MR&#x02013;SPM experiment. The headset provides the user with automatic switching between (a) a real space view and (b) a virtual space view, enabling conventional SPM control on PC. In the real space view (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>a), the SPM control interface display is automatically highlighted with a transparent green overlay, while the workspace on the desk is highlighted with a transparent blue overlay, as shown in the magnified callouts above the figure. These regions are pre-registered as marked objects to be detected by the MR system. Hand gesture detection enables interaction with the screen and various menu options in the real-world view, as illustrated in the callout on the left side of the figure.</p><p id="Par13">The virtual space view (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b) shows the virtual PC display that the user sees, containing the SPM control software user interface. The virtual UI is aligned with the real workspace using the desk as a reference marker. A virtual user interface has been implemented to monitor scanning signals and provide buttons for operating the equipment. The user can interact with these controls by performing specific hand gestures. Furthermore, by using a virtual PC monitor, standard computer operations can be reproduced even while wearing the headset. Typically, scientific experimental systems require multiple PC monitors to monitor complex operation windows and multi-channel signals. However, in the MR system, these physical monitors can be replaced with corresponding virtual UIs, significantly reducing equipment and space costs. This demonstrates the advanced capabilities of MR&#x02013;SPM in bridging the physical and digital domains, enhancing user experience and functionality.<fig id="Fig2"><label>Fig. 2</label><caption><p>View of the mixed reality scanning probe microscopy (MR&#x02013;SPM) experiment being conducted through the headset, with automatic switching between (<bold>a</bold>) real space view and (<bold>b</bold>) virtual space view to enable the conventional SPM control on a personal computer. The multimedia file [<xref rid="MOESM2" ref-type="media">Movie S1.mp4</xref>] illustrates the object tracking of real space.</p></caption><graphic xlink:href="41598_2025_1578_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec5"><title>Adaptive switch of virtual and real views</title><p id="Par14">
<fig id="Fig3"><label>Fig. 3</label><caption><p>Seamless integration of virtual and real spaces in the mixed reality experimental system. (<bold>a</bold>) Automatic view switching based on the distance between the marked object (desktop PC display at this time) and the user&#x02019;s face position. A fade area ensures a gradual transition between views. (<bold>b</bold>) Real-time switching between physical and virtual views during an SPM experiment. (<bold>c</bold>) The layer rendering framework implemented for smooth view transitions. The multimedia file [<xref rid="MOESM3" ref-type="media">Movie S2.mp4</xref>] illustrates the transition between real and virtual views.</p></caption><graphic xlink:href="41598_2025_1578_Fig3_HTML" id="MO3"/></fig>
</p><p id="Par15">In our mixed reality system, precise probe positioning and surface imaging can be performed in the virtual reality (VR) environment. However, VR alone does not allow for direct interaction with physical instruments, which is crucial during the experiment. To overcome this limitation and enable the operator to control multiple instruments simultaneously, a real-world view must be incorporated. Seamlessly switching between the virtual and physical workspaces is essential for maintaining the efficiency and accuracy of the experiment. To address this requirement, we have developed a framework that balances both virtual and real-world views, as depicted in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>. Our system allows for smooth transitions between VR and the real-world view, ensuring that the operator can effectively interact with the physical instruments while benefiting from the precision and visualization capabilities provided by the VR environment.</p><p id="Par16">The virtual and real views are designed to switch dynamically based on the current task requirements, which can be detected from the relative world position of the user&#x02019;s head anchor and the marked object in the real world. Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>a illustrates an example where we set the PC display as the &#x0201c;marked&#x0201d; object. When the experimenter is near the marked object, the transition between the real and virtual views is achieved through a fade zone, defined by the distances <italic>D</italic> from the marked object. When the user is located beyond <inline-formula id="IEq1"><alternatives><tex-math id="d33e378">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq1.gif"/></alternatives></inline-formula>, the system displays a fully virtual view. As the user moves closer to the marked object and enters the fade zone <inline-formula id="IEq2"><alternatives><tex-math id="d33e384">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_f$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq2.gif"/></alternatives></inline-formula>, the system gradually changes the contrast from real to virtual. This transition is completed when the user reaches <inline-formula id="IEq3"><alternatives><tex-math id="d33e390">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq1.gif"/></alternatives></inline-formula>, and the view becomes fully real. The virtual view persists until the user moves beyond <inline-formula id="IEq4"><alternatives><tex-math id="d33e397">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D=d_1+d_0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq4.gif"/></alternatives></inline-formula>, at which point the system switches back to the real view. This approach ensures a smooth transition between the real and virtual environments, enhancing the user&#x02019;s immersion in the experimental experience while minimizing distractions from real-world elements. The transparency <italic>T</italic> dynamically changes in real-time depending on the distance <italic>D</italic> between the user&#x02019;s head anchor and the target real object, according to the following equation:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e409">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} T = {\left\{ \begin{array}{ll} 1 &#x00026; (0 \le D \le d_1 - d_f) \\ \frac{d_1 - D}{d_f} &#x00026; (d_1 - d_f \le D &#x0003c; d_1) \\ 0 &#x00026; (d_1 \le D \le d_1 + d_0) \\ 1 &#x00026; (d_1 + d_0 \le D) \end{array}\right. } \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_1578_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>As the user approaches the target, the system is designed to fade the view to the real world gradually. This transition eventually reaches a state where the user can interact with the real object while still maintaining the visibility of relevant virtual elements [Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>b]. To correspond with the real view, the VR scene background consists of six high-resolution (8192<inline-formula id="IEq5"><alternatives><tex-math id="d33e419">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq5.gif"/></alternatives></inline-formula>8192) textures with distinct scenery for each direction, arranged in a hexagonal tiling. By aligning the real and VR backgrounds, users can smoothly transition between the two environments, preventing abrupt changes that might cause disorientation and ensuring a seamless and intuitive experience. During the gradual transition, users can maintain their sense of direction by corresponding to the real-world and VR backgrounds, which helps prevent abrupt changes that might cause disorientation, ensuring a smooth and intuitive experience. To achieve this implementation, we implemented a 3-layer rendering pipeline, as illustrated in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>c. This pipeline consists of a CG layer, a camera layer, and a UI layer. These layers are rendered sequentially to produce the final composite image displayed on the VR screen. When <inline-formula id="IEq6"><alternatives><tex-math id="d33e429">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T = 1,$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq6.gif"/></alternatives></inline-formula> the camera layer completely overwrites the CG layer, causing the game engine to display real-world imagery. Conversely, when <inline-formula id="IEq7"><alternatives><tex-math id="d33e435">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T = 0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq7.gif"/></alternatives></inline-formula>, the camera layer is fully transparent, allowing the virtual CG layer to be fully visible. For values of <italic>T</italic> between 0 and 1, a blend of virtual and real imagery is presented, with the proportion determined by the value of <italic>T</italic>.</p></sec><sec id="Sec6"><title>Hand gestures</title><p id="Par17">Hand gestures play a fundamental role in handling SPM experiments operation, serving as an alternative to hand-held VR controllers. For MR systems designed for scientific experiments, hand gesture control provides a unified interaction method that enables seamless manipulation of both physical experimental instruments and virtual objects. This interface offers an alternative approach to operating SPM probes that leverages natural hand gestures and spatial awareness. The immersive 3D environment makes atomic manipulation more intuitive for novice operators and provides experienced users with clearer contextual information during complex tasks. Hand gestures are particularly beneficial for precise tasks in SPM experiments, such as atom manipulation, as they can replace traditional input devices like keyboards and mice. This allows users to input tree-dimensional spatial data, including coordinates and trajectories, more intuitively.<fig id="Fig4"><label>Fig. 4</label><caption><p>Hand gesture interface to interact with virtual UI and objects. The multimedia files [<xref rid="MOESM4" ref-type="media">Movie S3a.mp4</xref>, <xref rid="MOESM5" ref-type="media">Movie S3b.mp4</xref>, and <xref rid="MOESM6" ref-type="media">Movie S3c.mp4</xref>] respectively illustrate the usage scenarios of the three interactive systems.</p></caption><graphic xlink:href="41598_2025_1578_Fig4_HTML" id="MO4"/></fig></p><p id="Par18">Figure <xref rid="Fig4" ref-type="fig">4</xref> illustrates three primary hand gesture interaction methods for interacting with virtual user interfaces (UI) and objects in a mixed reality (MR) environment. (a) depicts the poke interaction, where the user directly touches a UI panel on a virtual window to configure options. (b) shows the raycast interaction, which allows users to operate virtual objects from a distance by aiming at a floating virtual display. (c) demonstrates the grab interaction, enabling users to directly grasp and manipulate virtual objects that display data graphs and numerical values, providing a more tangible and intuitive interaction experience. The grab interaction allows users to adjust object transformations such as scale, rotation, and position using hand gestures. These three interaction methods provide users with intuitive and effective ways to interact with virtual objects and information in an MR environment. Each technique is suited for different situations and levels of precision, offering a versatile interaction framework that can adapt to various user requirements and scenarios.</p></sec></sec><sec id="Sec7"><title>Experimental results</title><sec id="Sec8"><title>SPM imaging operations in VR workspace</title><p id="Par19">
<fig id="Fig5"><label>Fig. 5</label><caption><p>Rendering in the SPM imaging in the VR workspace. The users can use gestures to display windows to be displayed in the virtual space. Here, as an example, the normal scanning screen, line profile display, tip shaping, moving the probe, and changing the scanning range are shown. The multimedia files [<xref rid="MOESM7" ref-type="media">Movie S4a.mp4</xref>] and [<xref rid="MOESM8" ref-type="media">Movie S4b.mp4</xref>] respectively illustrate the topographic scan scene and the atomic scan scene. The multimedia file [<xref rid="MOESM9" ref-type="media">Movie S4c.mp4</xref>] demonstrates the adjustment of the scan range using hand gestures.</p></caption><graphic xlink:href="41598_2025_1578_Fig5_HTML" id="MO5"/></fig>
</p><p id="Par20">Figure <xref rid="Fig5" ref-type="fig">5</xref> illustrates performances of typical SPM operations performed in the VR workspace. These results are obtained as the user interacts with the virtual environment using gestures while conducting experiments. The operations can be selected from the menu using hand gestures, as shown in the central panel of the figure. Panel A showcases the &#x02019;scan &#x00026; line profile&#x02019; feature. In normal imaging mode, it is possible to acquire images in real-time and display them in both 2D and 3D. Additionally, line profiles can also be displayed to provide cross-sectional information about the surface topography. In the B panel, the &#x02019;tip shaping&#x02019; functionality is showcased, enabling researchers to modify and optimize the probe tip for improved experimental performance by adjusting the distance between the probe and the sample and the applied voltage<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. The C demonstrates the &#x0201c;tip position change&#x0201d; capability, where users can precisely adjust the probe&#x02019;s location on the sample surface. The D panel illustrates the &#x02018;scan area change&#x02019; function, which enables users to dynamically alter the scanning range to focus on specific regions of interest, allowing for a flexible selection of areas for detailed analysis.</p></sec><sec id="Sec9"><title>Mechanical vertical manipulation of selected single atoms</title><p id="Par21">In addition to atomic-scale imaging, we demonstrated mechanical vertical atomic manipulation<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> in VR space using this system at room temperature. Atomic manipulation technology represents a significant advancement in the field of nanoscience. This technique enables precise control and manipulation of individual atoms at the atomic scale<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, dramatically deepening our understanding of material structures and properties at the atomic level. By controlling the arrangement of atoms and molecules at will, various applications are anticipated, including not only the construction of novel atomic structures but also the fabrication of atomic-scale logic circuits<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> and memory devices<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, and the control of chemical reactions at the single-atom level<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup> . MR&#x02013;SPM has the potential to become a powerful platform that enables real-time visualization of these atomic manipulation processes and allows for intuitive operation.<fig id="Fig6"><label>Fig. 6</label><caption><p>Mechanical vertical atom manipulation using MR&#x02013;SPM at room temperature. This experiment demonstrates the selective extraction of individual silicon atoms from a Si(111)-(7&#x000d7;7) surface. (<bold>a</bold>) and (<bold>b</bold>) show the VR workplace views from different perspectives before and after the atomic extraction process, respectively. The STM probe was positioned over the targeted Si atom to be manipulated using lateral (X-Y) gesture controls in the VR environment.</p></caption><graphic xlink:href="41598_2025_1578_Fig6_HTML" id="MO6"/></fig></p><p id="Par22">As shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>, we conducted vertical atomic manipulation experiments in the VR environment. The STM probe was moved using hand gestures in VR space to extract atoms mechanically. In this experiment, we utilized the gesture functionality of the VR system to manually position the STM tip above the target silicon atom on the sample surface. In this experiment, the operator&#x02019;s tip movement was restricted to directions parallel to the sample surface. Since vertical atomic manipulation requires precise control of the tip-sample distance, we decided not to use gestures for vertical tip control to prevent unintended tip-sample crash interactions. Figure <xref rid="Fig6" ref-type="fig">6</xref>a,b show STM images before and after the experiment as displayed in VR space, depicting the same 7.50 <inline-formula id="IEq9"><alternatives><tex-math id="d33e556">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq5.gif"/></alternatives></inline-formula> 7.50 nm scan area from different perspectives. The red circles in both images indicate the location of the extracted silicon atom. The experimental results demonstrate successful selective extraction of the targeted silicon atom in VR space, with the change clearly visible in the STM images. Notably, the post-extraction STM image revealed a characteristic vacancy where the target atom had been removed, while maintaining the surrounding atomic structure intact.</p></sec></sec><sec id="Sec10"><title>Discussion</title><p id="Par23">The atomic manipulation experiments demonstrated that VR-based gesture control enables more intuitive and rapid probe positioning compared to conventional interfaces. While traditional two-dimensional displays certainly provide continuous feedback, three-dimensional visualization offers complementary spatial context that can be particularly valuable when planning and executing multi-step atomic manipulations that require an understanding of complex three-dimensional relationships. Future improvements should focus on system stability and precision through automated control functions. Machine learning for optimal probe path calculation and haptic feedback implementation could enhance atomic-level manipulation capabilities. Currently, probe movement is limited to X and Y directions to prevent equipment damage. Incorporating Z-axis control with an integrated haptic feedback system<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup> could provide real-time tactile perception of probe-surface interactions, enabling safer three-dimensional manipulation. A key technical limitation is scanning speed, with our system requiring 30 seconds per area scan. To match human visual perception, the surface structure rendering should update within 15 ms<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Recent advances in hardware<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> and software<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> implementations have demonstrated video-rate scanning capabilities. In the system architecture of the SPM and MR application client, multi-processing needs to be implemented to reduce CPU load and increase the update rate for data handling. Achieving instantaneous surface rendering would significantly enhance atomic manipulation, spectroscopy measurements, and other precise positioning applications.</p><p id="Par24">The MR&#x02013;SPM system demonstrates the potential integration of metaverse technology with scientific instrumentation. The MR system utilizes a hand gesture interface, allowing users to intuitively understand the instrument&#x02019;s operation through complex workflows. The scanning results are rendered by computer graphics, making atomic-scale phenomena more vivid and tangible. This advancement not only makes the system more accessible for education but also enables remote experimentation and efficient research collaboration. Further optimization of user interaction can be achieved by introducing a language model-driven SPM system<sup><xref ref-type="bibr" rid="CR38">38</xref>&#x02013;<xref ref-type="bibr" rid="CR40">40</xref></sup> to simplify device control and by integrating autonomous experimentation algorithms<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup> to replace manual operations in executing specific experimental workflows. By implementing these capabilities, future metaverse laboratory platforms could enable users to focus on analyzing data rendered more intuitively within a 3D virtual space, while the experiment could be solely conducted via general decision-making through direct interaction, such as hand gestures. Our system completes the SPM instrument digitalization, allowing researchers to conduct experiments from any location with minimal on-site administration. This system makes multi-user participation in experiments on the MR platform possible, thus manifesting an SPM metaverse laboratory.</p><p id="Par25">Despite the promising capabilities of our mixed reality interface for SPM, several important challenges and limitations should be acknowledged before deploying as a public application. The laboratory&#x02019;s real-world mapping relies on pre-calibration, which limits its ability to detect dynamic objects in laboratory environments. This makes MR headset users less environmentally aware and creates potential safety hazards during experiments. Mapping algorithms suited for indoor scenes<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> with lower computational costs may be introduced to ensure a safer user experience. Current commercial hardware faces significant limitations, such as excessive weight (<inline-formula id="IEq10"><alternatives><tex-math id="d33e607">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\approx$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq10.gif"/></alternatives></inline-formula>500g) and short battery life (<inline-formula id="IEq11"><alternatives><tex-math id="d33e613">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\approx$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq10.gif"/></alternatives></inline-formula>2 hours), which contribute to user fatigue during extended sessions. Although VR displays typically project images at a distance of around 2&#x000a0;m, potentially reducing certain types of focus strain for eyes compared to conventional monitors, MR systems with binocular displays still suffer from the persistent vergence-accommodation conflict<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, which exacerbates visual fatigue and limits comfortable headset use. Addressing these ergonomic challenges is essential for enhancing long-term usability and user adoption. Additionally, in a cross-border metaverse laboratory, user regulation and security concerns may raise a problem<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. In MR applications, user data streams are presented in more diverse ways, making real-time communication management more challenging compared to conventional social media platforms. Addressing these limitations can expand the applicability of metaverse laboratories.</p><p id="Par26">In conclusion, we developed an MR interface that seamlessly integrates real and virtual laboratory environments for SPM experiments. While wearing an HMD, operators can efficiently transition between physical equipment control and virtual system interaction. The system&#x02019;s tree-dimensional visualization enhances spatial awareness and enables intuitive configuration of measurement parameters based on tip-sample interactions. We demonstrated the system&#x02019;s capabilities through successful atomic manipulation experiments on Si(111)-(7&#x000d7;7) using hand gesture controls. This integration of SPM with metaverse technology represents a significant advancement in materials science experimentation.</p></sec><sec id="Sec11"><title>Methods</title><sec id="Sec12"><title>MR&#x02013;SPM system</title><p id="Par27">The experiments were conducted using a home-built SPM operated at room temperature in an ultra-high vacuum environment (<inline-formula id="IEq13"><alternatives><tex-math id="d33e635">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&#x0003c; 1 \times 10^{-8}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1578_Article_IEq13.gif"/></alternatives></inline-formula> Pa). An Si(111)-(7&#x000d7;7) surface was prepared using the standard flashing-annealing method. STM images were acquired using a Pt/Ir probe. To integrate SPM instrument and MR, a custom system was developed using LabVIEW and Python for SPM data acquisition. The measurement board employed was a PXIe-7857R from National Instruments. The MR headset used was a Meta Quest 3 from Meta. The MR component of the program was developed via the Unity platform, while the SPM remote program was implemented in Python.</p></sec><sec id="Sec13"><title>Mixed-reality environment setup</title><p id="Par28">The Meta Quest 3 serves as the head-mounted display (HMD), while Unity functions as the computer graphics renderer and game engine to host the MR system. This setup runs on a PC equipped with an Intel i9-9900K CPU and NVIDIA RTX 4090 GPU. Data communication between the Meta Quest 3 and Unity is facilitated via Oculus Link. To implement MR features in Unity, we utilize Meta XR SDKs. Within the Unity component of the Meta XR SDKs, the behavior of virtual UI and virtual game objects is constructed through Unity components based on Hand Poke Interactive, Ray Interactive, and Hand Grab Interactive components. Additionally, tip interaction is facilitated by the Hand Pose Detection component. The spatial data of real objects is referenced from the Meta Quest 3 scene, which can be preliminarily set up to include object labels and detected surfaces. In Unity, the &#x0201c;HTTP GET&#x0201d; method is employed to acquire application cache data via the HTTP server module on the SPM application, while the &#x0201c;HTTP POST&#x0201d; method is utilized for data transfer and remote scanning. Given that we run the HTTP server on localhost, the latency between the MR environment and the SPM system is less than 10 ms. Considering the latency in the instrument data processing framework and the load on the CPU, the update rate of the thread monitoring data communication in the MR interface is set to 60 Hz, while the update rate for the thread acquiring and updating measurement data in the SPM system is set to 20 Hz.</p></sec><sec id="Sec14"><title>Atom manipulation method</title><p id="Par29">This research involves the manipulation of a Si dopant atom on a Si(111)-(7&#x000d7;7) surface using the &#x0201c;atomic pen technique&#x0201d;<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> at room temperature. Initially, we perform an automatic drift compensation<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> method to ensure that the distance between SPM probe and sample can be located precisely. Next, the system maintains the Z feedback and positions the SPM probe precisely over the target atom&#x02019;s X and Y coordinates. Then, the atomic scan is activated, periodically pausing the Z feedback and gradually lowering the probe to a predetermined distance from the surface. This process continues until a specific and significant change is observed in the atomic scan signal which can indicate the successful removal of the target Si adatom.</p></sec></sec><sec id="Sec15" sec-type="supplementary-material"><title>Supplementary Information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2025_1578_MOESM1_ESM.pdf"><caption><p>Supplementary Information.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="41598_2025_1578_MOESM2_ESM.mp4"><caption><p>Supplementary Movie S1.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="41598_2025_1578_MOESM3_ESM.mp4"><caption><p>Supplementary Movie S2.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM4"><media xlink:href="41598_2025_1578_MOESM4_ESM.mp4"><caption><p>Supplementary Movie S3a.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM5"><media xlink:href="41598_2025_1578_MOESM5_ESM.mp4"><caption><p>Supplementary Movie S3b.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM6"><media xlink:href="41598_2025_1578_MOESM6_ESM.mp4"><caption><p>Supplementary Movie S3c.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM7"><media xlink:href="41598_2025_1578_MOESM7_ESM.mp4"><caption><p>Supplementary Movie S4a.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM8"><media xlink:href="41598_2025_1578_MOESM8_ESM.mp4"><caption><p>Supplementary ViMoviedeo S4b.</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM9"><media xlink:href="41598_2025_1578_MOESM9_ESM.mp4"><caption><p>Supplementary ViMoviedeo S4c.</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-025-01578-y.</p></sec><ack><title>Acknowledgements</title><p>This work was supported in part by a Grant-in-Aid for Scientific Research (22K18945, 24K21716,&#x000a0;25K17654) from the Ministry of Education, Culture, Sports, Science and Technology of Japan (MEXT).&#x000a0;A part of MA work is supported by JKA and its promotion funds from KEIRIN RACE.&#x000a0;This work is also partially supported by the Kyoto Technoscience Center.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>The concept for this research was developed by all of the authors. M.A. and H.Y. were mainly responsible for maintaining the SPM equipment. Z.D. was in charge of the hardware and set up the system. The experiments were mainly carried out by Z.D. and M.A.. M.A. was in charge of sample preparation. M.A. and H.Y. were in charge of acquiring the budget for the research. All authors reviewed the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets used and/or analysed during the current study are available from the corresponding author on reasonable request.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par33">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Mystakidis</surname><given-names>S</given-names></name></person-group><article-title>Metaverse</article-title><source>Encyclopedia</source><year>2022</year><volume>2</volume><fpage>486</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.3390/encyclopedia2010031</pub-id></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Mystakidis, S. Metaverse. <italic>Encyclopedia</italic><bold>2</bold>, 486&#x02013;497 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Xi</surname><given-names>N</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Gama</surname><given-names>F</given-names></name><name><surname>Korkeila</surname><given-names>H</given-names></name><name><surname>Hamari</surname><given-names>J</given-names></name></person-group><article-title>Acceptance of the metaverse: A laboratory experiment on augmented and virtual reality shopping</article-title><source>Internet Res.</source><year>2024</year><volume>34</volume><fpage>82</fpage><lpage>117</lpage><pub-id pub-id-type="doi">10.1108/INTR-05-2022-0334</pub-id></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Xi, N., Chen, J., Gama, F., Korkeila, H. &#x00026; Hamari, J. Acceptance of the metaverse: A laboratory experiment on augmented and virtual reality shopping. <italic>Internet Res.</italic><bold>34</bold>, 82&#x02013;117. 10.1108/INTR-05-2022-0334 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Al-Muqbil</surname><given-names>N</given-names></name></person-group><article-title>Impact of metaverse technology on academic achievement and motivation in middle school science</article-title><source>Multimodal Technol. Interact.</source><year>2024</year><volume>8</volume><issue>10</issue><fpage>91</fpage><pub-id pub-id-type="doi">10.3390/mti8100091</pub-id></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Al-Muqbil, N. Impact of metaverse technology on academic achievement and motivation in middle school science. <italic>Multimodal Technol. Interact.</italic><bold>8</bold>(10), 91 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Ullah</surname><given-names>H</given-names></name><name><surname>Manickam</surname><given-names>S</given-names></name><name><surname>Obaidat</surname><given-names>M</given-names></name><name><surname>Laghari</surname><given-names>S</given-names></name><name><surname>Uddin</surname><given-names>M</given-names></name></person-group><article-title>Exploring the potential of metaverse technology in healthcare: Applications, challenges, and future directions</article-title><source>IEEE Access</source><year>2023</year><volume>11</volume><fpage>69686</fpage><lpage>69707</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2023.3286696</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Ullah, H., Manickam, S., Obaidat, M., Laghari, S. &#x00026; Uddin, M. Exploring the potential of metaverse technology in healthcare: Applications, challenges, and future directions. <italic>IEEE Access</italic><bold>11</bold>, 69686&#x02013;69707 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>P</given-names></name><etal/></person-group><article-title>Virtual reality for understanding artificial-intelligence-driven scientific discovery with an application in quantum optics</article-title><source>Mach. Learn.: Sci. Technol.</source><year>2024</year><volume>5</volume><fpage>035045</fpage><pub-id pub-id-type="doi">10.1088/2632-2153/ad5fdb</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Schmidt, P. et al. Virtual reality for understanding artificial-intelligence-driven scientific discovery with an application in quantum optics. <italic>Mach. Learn.: Sci. Technol.</italic><bold>5</bold>, 035045. 10.1088/2632-2153/ad5fdb (2024).</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Spark</surname><given-names>A</given-names></name><etal/></person-group><article-title>vlume: 3d virtual reality for single-molecule localization microscopy</article-title><source>Nat. Methods</source><year>2020</year><volume>17</volume><fpage>1097</fpage><lpage>1099</lpage><pub-id pub-id-type="doi">10.1038/s41592-020-0962-1</pub-id><pub-id pub-id-type="pmid">33046895</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Spark, A. et al. vlume: 3d virtual reality for single-molecule localization microscopy. <italic>Nat. Methods</italic><bold>17</bold>, 1097&#x02013;1099. 10.1038/s41592-020-0962-1 (2020).<pub-id pub-id-type="pmid">33046895</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Pirch</surname><given-names>S</given-names></name><etal/></person-group><article-title>The vrnetzer platform enables interactive network analysis in virtual reality</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><fpage>2432</fpage><pub-id pub-id-type="doi">10.1038/s41467-021-22570-w</pub-id><pub-id pub-id-type="pmid">33893283</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Pirch, S. et al. The vrnetzer platform enables interactive network analysis in virtual reality. <italic>Nat. Commun.</italic><bold>12</bold>, 2432. 10.1038/s41467-021-22570-w (2021).<pub-id pub-id-type="pmid">33893283</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Lu</surname><given-names>Y</given-names></name><name><surname>Zhu</surname><given-names>X</given-names></name></person-group><article-title>Mateverse, the future materials science computation platform based on metaverse</article-title><source>J. Phys. Chem. Lett.</source><year>2023</year><volume>14</volume><fpage>148</fpage><lpage>157</lpage><pub-id pub-id-type="doi">10.1021/acs.jpclett.2c03459</pub-id><pub-id pub-id-type="pmid">36579474</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Gao, Y., Lu, Y. &#x00026; Zhu, X. Mateverse, the future materials science computation platform based on metaverse. <italic>J. Phys. Chem. Lett.</italic><bold>14</bold>, 148&#x02013;157. 10.1021/acs.jpclett.2c03459 (2023).<pub-id pub-id-type="pmid">36579474</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>O&#x02019;Connor</surname><given-names>MB</given-names></name><etal/></person-group><article-title>Interactive molecular dynamics in virtual reality from quantum chemistry to drug binding: An open-source multi-person framework</article-title><source>J. Chem. Phys.</source><year>2019</year><volume>150</volume><fpage>220901</fpage><pub-id pub-id-type="doi">10.1063/1.5092590</pub-id><pub-id pub-id-type="pmid">31202243</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">O&#x02019;Connor, M. B. et al. Interactive molecular dynamics in virtual reality from quantum chemistry to drug binding: An open-source multi-person framework. <italic>J. Chem. Phys.</italic><bold>150</bold>, 220901. 10.1063/1.5092590 (2019).<pub-id pub-id-type="pmid">31202243</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Jacopin</surname><given-names>E</given-names></name><name><surname>Sakamoto</surname><given-names>Y</given-names></name><name><surname>Nishida</surname><given-names>K</given-names></name><name><surname>Kaizu</surname><given-names>K</given-names></name><name><surname>Takahashi</surname><given-names>K</given-names></name></person-group><article-title>An architecture for collaboration in systems biology at the age of the metaverse</article-title><source>npj Syst. Biol. Appl.</source><year>2024</year><volume>10</volume><fpage>12</fpage><pub-id pub-id-type="doi">10.1038/s41540-024-00334-8</pub-id><pub-id pub-id-type="pmid">38280851</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Jacopin, E., Sakamoto, Y., Nishida, K., Kaizu, K. &#x00026; Takahashi, K. An architecture for collaboration in systems biology at the age of the metaverse. <italic>npj Syst. Biol. Appl.</italic><bold>10</bold>, 12. 10.1038/s41540-024-00334-8 (2024).<pub-id pub-id-type="pmid">38280851</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Deeks</surname><given-names>HM</given-names></name><etal/></person-group><article-title>Free energy along drug-protein binding pathways interactively sampled in virtual reality</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><fpage>16665</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-43523-x</pub-id><pub-id pub-id-type="pmid">37794083</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Deeks, H. M. et al. Free energy along drug-protein binding pathways interactively sampled in virtual reality. <italic>Sci. Rep.</italic><bold>13</bold>, 16665. 10.1038/s41598-023-43523-x (2023).<pub-id pub-id-type="pmid">37794083</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>X</given-names></name></person-group><article-title>Toward the uniform of chemical theory, simulation, and experiments in metaverse technology</article-title><source>Precis. Chem.</source><year>2023</year><volume>1</volume><fpage>192</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1021/prechem.3c00045</pub-id></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Zhu, X. Toward the uniform of chemical theory, simulation, and experiments in metaverse technology. <italic>Precis. Chem.</italic><bold>1</bold>, 192&#x02013;198. 10.1021/prechem.3c00045 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Zhu</surname><given-names>X</given-names></name></person-group><article-title>Human-guided metaverse synthesis for quantum dots: Advancing nanomaterial research through augmented artificial intelligence</article-title><source>ACS Appl. Mater. &#x00026; Interfaces</source><year>2024</year><volume>16</volume><fpage>45207</fpage><lpage>45213</lpage><pub-id pub-id-type="doi">10.1021/acsami.4c09842</pub-id><pub-id pub-id-type="pmid">39138122</pub-id>
</element-citation><mixed-citation id="mc-CR13" publication-type="journal">Xu, Y., Gao, Y., Wang, M. &#x00026; Zhu, X. Human-guided metaverse synthesis for quantum dots: Advancing nanomaterial research through augmented artificial intelligence. <italic>ACS Appl. Mater. &#x00026; Interfaces</italic><bold>16</bold>, 45207&#x02013;45213. 10.1021/acsami.4c09842 (2024).<pub-id pub-id-type="pmid">39138122</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>M</given-names></name><etal/></person-group><article-title>Patterning a hydrogen-bonded molecular monolayer with a hand-controlled scanning probe microscope</article-title><source>Beilstein J. Nanotechnol.</source><year>2014</year><volume>5</volume><fpage>1926</fpage><lpage>1932</lpage><pub-id pub-id-type="doi">10.3762/bjnano.5.203</pub-id><pub-id pub-id-type="pmid">25383304</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Green, M. et al. Patterning a hydrogen-bonded molecular monolayer with a hand-controlled scanning probe microscope. <italic>Beilstein J. Nanotechnol.</italic><bold>5</bold>, 1926&#x02013;1932. 10.3762/bjnano.5.203 (2014).<pub-id pub-id-type="pmid">25383304</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Leinen</surname><given-names>P</given-names></name><etal/></person-group><article-title>Virtual reality visual feedback for hand-controlled scanning probe microscopy manipulation of single molecules</article-title><source>Beilstein J. Nanotechnol.</source><year>2015</year><volume>6</volume><fpage>2148</fpage><lpage>2153</lpage><pub-id pub-id-type="doi">10.3762/bjnano.6.220</pub-id><pub-id pub-id-type="pmid">26665087</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Leinen, P. et al. Virtual reality visual feedback for hand-controlled scanning probe microscopy manipulation of single molecules. <italic>Beilstein J. Nanotechnol.</italic><bold>6</bold>, 2148&#x02013;2153. 10.3762/bjnano.6.220 (2015).<pub-id pub-id-type="pmid">26665087</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Tewari</surname><given-names>S</given-names></name><name><surname>Bakermans</surname><given-names>J</given-names></name><name><surname>Wagner</surname><given-names>C</given-names></name><name><surname>Galli</surname><given-names>F</given-names></name><name><surname>van Ruitenbeek</surname><given-names>JM</given-names></name></person-group><article-title>Intuitive human interface to a scanning tunnelling microscope: Observation of parity oscillations for a single atomic chain</article-title><source>Beilstein J. Nanotechnol.</source><year>2019</year><volume>10</volume><fpage>337</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.3762/bjnano.10.33</pub-id><pub-id pub-id-type="pmid">30800573</pub-id>
</element-citation><mixed-citation id="mc-CR16" publication-type="journal">Tewari, S., Bakermans, J., Wagner, C., Galli, F. &#x00026; van Ruitenbeek, J. M. Intuitive human interface to a scanning tunnelling microscope: Observation of parity oscillations for a single atomic chain. <italic>Beilstein J. Nanotechnol.</italic><bold>10</bold>, 337&#x02013;348. 10.3762/bjnano.10.33 (2019).<pub-id pub-id-type="pmid">30800573</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Ferretti</surname><given-names>S</given-names></name><name><surname>Bianchi</surname><given-names>S</given-names></name><name><surname>Frangipane</surname><given-names>G</given-names></name><name><surname>Di Leonardo</surname><given-names>R</given-names></name></person-group><article-title>A virtual reality interface for the immersive manipulation of live microscopic systems</article-title><source>Sci. Rep.</source><year>2021</year><volume>11</volume><fpage>7610</fpage><pub-id pub-id-type="doi">10.1038/s41598-021-87004-5</pub-id><pub-id pub-id-type="pmid">33828325</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Ferretti, S., Bianchi, S., Frangipane, G. &#x00026; Di Leonardo, R. A virtual reality interface for the immersive manipulation of live microscopic systems. <italic>Sci. Rep.</italic><bold>11</bold>, 7610. 10.1038/s41598-021-87004-5 (2021).<pub-id pub-id-type="pmid">33828325</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Aecroscopy: A software-hardware framework empowering microscopy toward automated and autonomous experimentation</article-title><source>Small Methods</source><year>2024</year><volume>8</volume><fpage>2301740</fpage><pub-id pub-id-type="doi">10.1002/smtd.202301740</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Liu, Y. et al. Aecroscopy: A software-hardware framework empowering microscopy toward automated and autonomous experimentation. <italic>Small Methods</italic><bold>8</bold>, 2301740. 10.1002/smtd.202301740 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Diao</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Ai-equipped scanning probe microscopy for autonomous site-specific atomic-level characterization at room temperature</article-title><source>Small Methods</source><year>2025</year><volume>9</volume><fpage>2400813</fpage><pub-id pub-id-type="doi">10.1002/smtd.202400813</pub-id><pub-id pub-id-type="pmid">39240014</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Diao, Z. et al. Ai-equipped scanning probe microscopy for autonomous site-specific atomic-level characterization at room temperature. <italic>Small Methods</italic><bold>9</bold>, 2400813. 10.1002/smtd.202400813 (2025).<pub-id pub-id-type="pmid">39240014</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>G&#x000f3;mez-Zar&#x000e1;</surname><given-names>D</given-names></name><name><surname>Schiffer</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>D</given-names></name></person-group><article-title>The promise and pitfalls of the metaverse for science</article-title><source>Nat. Hum. Behav.</source><year>2023</year><volume>7</volume><fpage>1237</fpage><lpage>1240</lpage><pub-id pub-id-type="doi">10.1038/s41562-023-01599-5</pub-id><pub-id pub-id-type="pmid">37202534</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">G&#x000f3;mez-Zar&#x000e1;, D., Schiffer, P. &#x00026; Wang, D. The promise and pitfalls of the metaverse for science. <italic>Nat. Hum. Behav.</italic><bold>7</bold>, 1237&#x02013;1240. 10.1038/s41562-023-01599-5 (2023).<pub-id pub-id-type="pmid">37202534</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Toshev, A. &#x00026; Szegedy, C. Deeppose: Human pose estimation via deep neural networks. In <italic>2014 IEEE Conference on Computer Vision and Pattern Recognition</italic>, 1653&#x02013;1660 (2014).</mixed-citation></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Mur-Artal</surname><given-names>R</given-names></name><name><surname>Montiel</surname><given-names>J</given-names></name><name><surname>Tard&#x000f3;s</surname><given-names>JD</given-names></name></person-group><article-title>Orb-slam: A versatile and accurate monocular slam system</article-title><source>IEEE Trans. Rob.</source><year>2015</year><volume>31</volume><fpage>1147</fpage><lpage>1163</lpage><pub-id pub-id-type="doi">10.1109/TRO.2015.2463671</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Mur-Artal, R., Montiel, J. &#x00026; Tard&#x000f3;s, J. D. Orb-slam: A versatile and accurate monocular slam system. <italic>IEEE Trans. Rob.</italic><bold>31</bold>, 1147&#x02013;1163 (2015).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Van Gestel</surname><given-names>F</given-names></name><etal/></person-group><article-title>Augmented reality guidance improves accuracy of orthopedic drilling procedures</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>25269</fpage><pub-id pub-id-type="doi">10.1038/s41598-024-76132-3</pub-id><pub-id pub-id-type="pmid">39448659</pub-id>
</element-citation><mixed-citation id="mc-CR23" publication-type="journal">Van Gestel, F. et al. Augmented reality guidance improves accuracy of orthopedic drilling procedures. <italic>Sci. Rep.</italic><bold>14</bold>, 25269. 10.1038/s41598-024-76132-3 (2024).<pub-id pub-id-type="pmid">39448659</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>C-K</given-names></name><name><surname>Chen</surname><given-names>Y-H</given-names></name><name><surname>Chuang</surname><given-names>T-J</given-names></name><name><surname>Shankhwar</surname><given-names>K</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>An augmented reality-based training system with a natural user interface for manual milling operations</article-title><source>Virtual Reality</source><year>2020</year><volume>24</volume><fpage>527</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1007/s10055-019-00415-8</pub-id></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Yang, C.-K., Chen, Y.-H., Chuang, T.-J., Shankhwar, K. &#x00026; Smith, S. An augmented reality-based training system with a natural user interface for manual milling operations. <italic>Virtual Reality</italic><bold>24</bold>, 527&#x02013;539. 10.1007/s10055-019-00415-8 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Valentini</surname><given-names>PP</given-names></name></person-group><article-title>Interactive virtual assembling in augmented reality</article-title><source>Int. J. Interact. Des. Manuf. (IJIDeM)</source><year>2009</year><volume>3</volume><fpage>109</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1007/s12008-009-0064-x</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Valentini, P. P. Interactive virtual assembling in augmented reality. <italic>Int. J. Interact. Des. Manuf. (IJIDeM)</italic><bold>3</bold>, 109&#x02013;119. 10.1007/s12008-009-0064-x (2009).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Diao</surname><given-names>Z</given-names></name><name><surname>Hou</surname><given-names>L</given-names></name><name><surname>Abe</surname><given-names>M</given-names></name></person-group><article-title>Probe conditioning via convolution neural network for scanning probe microscopy automation</article-title><source>Appl. Phys. Express</source><year>2023</year><volume>16</volume><fpage>085002</fpage><pub-id pub-id-type="doi">10.35848/1882-0786/acecd6</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Diao, Z., Hou, L. &#x00026; Abe, M. Probe conditioning via convolution neural network for scanning probe microscopy automation. <italic>Appl. Phys. Express</italic><bold>16</bold>, 085002. 10.35848/1882-0786/acecd6 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Oyabu</surname><given-names>N</given-names></name><name><surname>Custance</surname><given-names>O</given-names></name><name><surname>Yi</surname><given-names>I</given-names></name><name><surname>Sugawara</surname><given-names>Y</given-names></name><name><surname>Morita</surname><given-names>S</given-names></name></person-group><article-title>Mechanical vertical manipulation of selected single atoms by soft nanoindentation using near contact atomic force microscopy</article-title><source>Phys. Rev. Lett.</source><year>2003</year><volume>90</volume><fpage>176102</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.90.176102</pub-id><pub-id pub-id-type="pmid">12786084</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Oyabu, N., Custance, O., Yi, I., Sugawara, Y. &#x00026; Morita, S. Mechanical vertical manipulation of selected single atoms by soft nanoindentation using near contact atomic force microscopy. <italic>Phys. Rev. Lett.</italic><bold>90</bold>, 176102. 10.1103/PhysRevLett.90.176102 (2003).<pub-id pub-id-type="pmid">12786084</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Eigler</surname><given-names>DM</given-names></name><name><surname>Schweizer</surname><given-names>EK</given-names></name></person-group><article-title>Positioning single atoms with a scanning tunnelling microscope</article-title><source>Nature</source><year>1990</year><volume>344</volume><fpage>524</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1038/344524a0</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Eigler, D. M. &#x00026; Schweizer, E. K. Positioning single atoms with a scanning tunnelling microscope. <italic>Nature</italic><bold>344</bold>, 524&#x02013;526. 10.1038/344524a0 (1990).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Heinrich</surname><given-names>AJ</given-names></name><name><surname>Lutz</surname><given-names>CP</given-names></name><name><surname>Gupta</surname><given-names>JA</given-names></name><name><surname>Eigler</surname><given-names>DM</given-names></name></person-group><article-title>Molecule cascades</article-title><source>Science</source><year>2002</year><volume>298</volume><fpage>1381</fpage><lpage>1387</lpage><pub-id pub-id-type="doi">10.1126/science.1076768</pub-id><pub-id pub-id-type="pmid">12399543</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Heinrich, A. J., Lutz, C. P., Gupta, J. A. &#x00026; Eigler, D. M. Molecule cascades. <italic>Science</italic><bold>298</bold>, 1381&#x02013;1387 (2002).<pub-id pub-id-type="pmid">12399543</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Kalff</surname><given-names>FE</given-names></name><etal/></person-group><article-title>A kilobyte rewritable atomic memory</article-title><source>Nat. Nanotechnol.</source><year>2016</year><volume>11</volume><fpage>926</fpage><lpage>929</lpage><pub-id pub-id-type="doi">10.1038/nnano.2016.131</pub-id><pub-id pub-id-type="pmid">27428273</pub-id>
</element-citation><mixed-citation id="mc-CR30" publication-type="journal">Kalff, F. E. et al. A kilobyte rewritable atomic memory. <italic>Nat. Nanotechnol.</italic><bold>11</bold>, 926&#x02013;929. 10.1038/nnano.2016.131 (2016).<pub-id pub-id-type="pmid">27428273</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Hla</surname><given-names>S-W</given-names></name><name><surname>Bartels</surname><given-names>L</given-names></name><name><surname>Meyer</surname><given-names>G</given-names></name><name><surname>Rieder</surname><given-names>K-H</given-names></name></person-group><article-title>Inducing all steps of a chemical reaction with the scanning tunneling microscope tip: Towards single molecule engineering</article-title><source>Phys. Rev. Lett.</source><year>2000</year><volume>85</volume><fpage>2777</fpage><lpage>2780</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.85.2777</pub-id><pub-id pub-id-type="pmid">10991231</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">Hla, S.-W., Bartels, L., Meyer, G. &#x00026; Rieder, K.-H. Inducing all steps of a chemical reaction with the scanning tunneling microscope tip: Towards single molecule engineering. <italic>Phys. Rev. Lett.</italic><bold>85</bold>, 2777&#x02013;2780 (2000).<pub-id pub-id-type="pmid">10991231</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Sugimoto</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Chemical identification of individual surface atoms by atomic force microscopy</article-title><source>Nature</source><year>2007</year><volume>446</volume><fpage>64</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1038/nature05530</pub-id><pub-id pub-id-type="pmid">17330040</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">Sugimoto, Y. et al. Chemical identification of individual surface atoms by atomic force microscopy. <italic>Nature</italic><bold>446</bold>, 64&#x02013;67. 10.1038/nature05530 (2007).<pub-id pub-id-type="pmid">17330040</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Leroy</surname><given-names>E</given-names></name><name><surname>Hinchet</surname><given-names>R</given-names></name><name><surname>Shea</surname><given-names>H</given-names></name></person-group><article-title>Multimode hydraulically amplified electrostatic actuators for wearable haptics</article-title><source>Adv. Mater.</source><year>2020</year><volume>32</volume><fpage>2002564</fpage><pub-id pub-id-type="doi">10.1002/adma.202002564</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Leroy, E., Hinchet, R. &#x00026; Shea, H. Multimode hydraulically amplified electrostatic actuators for wearable haptics. <italic>Adv. Mater.</italic><bold>32</bold>, 2002564. 10.1002/adma.202002564 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Z</given-names></name><name><surname>Zhu</surname><given-names>M</given-names></name><name><surname>Shan</surname><given-names>X</given-names></name><name><surname>Lee</surname><given-names>C</given-names></name></person-group><article-title>Augmented tactile-perception and haptic-feedback rings as human-machine interfaces aiming for immersive interactions</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><fpage>5224</fpage><pub-id pub-id-type="doi">10.1038/s41467-022-32745-8</pub-id><pub-id pub-id-type="pmid">36064838</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">Sun, Z., Zhu, M., Shan, X. &#x00026; Lee, C. Augmented tactile-perception and haptic-feedback rings as human-machine interfaces aiming for immersive interactions. <italic>Nat. Commun.</italic><bold>13</bold>, 5224. 10.1038/s41467-022-32745-8 (2022).<pub-id pub-id-type="pmid">36064838</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><etal/></person-group><article-title>Effect of frame rate on user experience, performance, and simulator sickness in virtual reality</article-title><source>IEEE Trans. Visual Comput. Graphics</source><year>2023</year><volume>29</volume><fpage>2478</fpage><lpage>2488</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2023.3247057</pub-id></element-citation><mixed-citation id="mc-CR35" publication-type="journal">Wang, J. et al. Effect of frame rate on user experience, performance, and simulator sickness in virtual reality. <italic>IEEE Trans. Visual Comput. Graphics</italic><bold>29</bold>, 2478&#x02013;2488 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name><surname>Kodera</surname><given-names>N</given-names></name><name><surname>Yamamoto</surname><given-names>D</given-names></name><name><surname>Ishikawa</surname><given-names>R</given-names></name><name><surname>Ando</surname><given-names>T</given-names></name></person-group><article-title>Video imaging of walking myosin v by high-speed atomic force microscopy</article-title><source>Nature</source><year>2010</year><volume>468</volume><fpage>72</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1038/nature09450</pub-id><pub-id pub-id-type="pmid">20935627</pub-id>
</element-citation><mixed-citation id="mc-CR36" publication-type="journal">Kodera, N., Yamamoto, D., Ishikawa, R. &#x00026; Ando, T. Video imaging of walking myosin v by high-speed atomic force microscopy. <italic>Nature</italic><bold>468</bold>, 72&#x02013;76. 10.1038/nature09450 (2010).<pub-id pub-id-type="pmid">20935627</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>Gura</surname><given-names>L</given-names></name><etal/></person-group><article-title>Spiral high-speed scanning tunneling microscopy: Tracking atomic diffusion on the millisecond timescale</article-title><source>Appl. Phys. Lett.</source><year>2021</year><volume>119</volume><fpage>251601</fpage><pub-id pub-id-type="doi">10.1063/5.0071340</pub-id></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Gura, L. et al. Spiral high-speed scanning tunneling microscopy: Tracking atomic diffusion on the millisecond timescale. <italic>Appl. Phys. Lett.</italic><bold>119</bold>, 251601. 10.1063/5.0071340 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Checa</surname><given-names>M</given-names></name><name><surname>Vasudevan</surname><given-names>RK</given-names></name></person-group><article-title>Synergizing human expertise and ai efficiency with language model for microscopy operation and automated experiment design</article-title><source>Mach. Learn.: Sci. Technol.</source><year>2024</year><volume>5</volume><fpage>02LT01</fpage><pub-id pub-id-type="doi">10.1088/2632-2153/ad52e9</pub-id></element-citation><mixed-citation id="mc-CR38" publication-type="journal">Liu, Y., Checa, M. &#x00026; Vasudevan, R. K. Synergizing human expertise and ai efficiency with language model for microscopy operation and automated experiment design. <italic>Mach. Learn.: Sci. Technol.</italic><bold>5</bold>, 02LT01. 10.1088/2632-2153/ad52e9 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Mandal, I. et al. Autonomous microscopy experiments through large language model agents (2024). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2501.10385">arXiv:2501.10385</ext-link>.</mixed-citation></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Diao</surname><given-names>Z</given-names></name><name><surname>Yamashita</surname><given-names>H</given-names></name><name><surname>Abe</surname><given-names>M</given-names></name></person-group><article-title>Leveraging large language model and social network service for automation in scanning probe microscopy</article-title><source>Meas. Sci. Technol.</source><year>2025</year><volume>36</volume><fpage>047001</fpage><pub-id pub-id-type="doi">10.1088/1361-6501/adbf3a</pub-id></element-citation><mixed-citation id="mc-CR40" publication-type="journal">Diao, Z., Yamashita, H. &#x00026; Abe, M. Leveraging large language model and social network service for automation in scanning probe microscopy. <italic>Meas. Sci. Technol.</italic><bold>36</bold>, 047001. 10.1088/1361-6501/adbf3a (2025).</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>SB</given-names></name><name><surname>Vasudevan</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name></person-group><article-title>Active oversight and quality control in standard bayesian optimization for autonomous experiments</article-title><source>npj Comput. Mater.</source><year>2025</year><volume>11</volume><fpage>23</fpage><pub-id pub-id-type="doi">10.1038/s41524-024-01485-2</pub-id></element-citation><mixed-citation id="mc-CR41" publication-type="journal">Harris, S. B., Vasudevan, R. &#x00026; Liu, Y. Active oversight and quality control in standard bayesian optimization for autonomous experiments. <italic>npj Comput. Mater.</italic><bold>11</bold>, 23. 10.1038/s41524-024-01485-2 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>El-Sheimy</surname><given-names>N</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name></person-group><article-title>Indoor navigation: State of the art and future trends</article-title><source>Satell. Navig.</source><year>2021</year><volume>2</volume><fpage>7</fpage><pub-id pub-id-type="doi">10.1186/s43020-021-00041-3</pub-id></element-citation><mixed-citation id="mc-CR42" publication-type="journal">El-Sheimy, N. &#x00026; Li, Y. Indoor navigation: State of the art and future trends. <italic>Satell. Navig.</italic><bold>2</bold>, 7. 10.1186/s43020-021-00041-3 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Fang</surname><given-names>F</given-names></name></person-group><article-title>Vergence-accommodation conflict in optical see-through display: Review and prospect</article-title><source>Results Opt.</source><year>2021</year><volume>5</volume><fpage>100160</fpage><pub-id pub-id-type="doi">10.1016/j.rio.2021.100160</pub-id></element-citation><mixed-citation id="mc-CR43" publication-type="journal">Zhou, Y., Zhang, J. &#x00026; Fang, F. Vergence-accommodation conflict in optical see-through display: Review and prospect. <italic>Results Opt.</italic><bold>5</bold>, 100160 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name><surname>Sugimoto</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Complex patterning by vertical interchange atom manipulation using atomic force microscopy</article-title><source>Science</source><year>2008</year><volume>322</volume><fpage>413</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1126/science.1160601</pub-id><pub-id pub-id-type="pmid">18927388</pub-id>
</element-citation><mixed-citation id="mc-CR44" publication-type="journal">Sugimoto, Y. et al. Complex patterning by vertical interchange atom manipulation using atomic force microscopy. <italic>Science</italic><bold>322</bold>, 413&#x02013;417 (2008).<pub-id pub-id-type="pmid">18927388</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Diao</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Automatic drift compensation for nanoscale imaging using feature point matching</article-title><source>Appl. Phys. Lett.</source><year>2023</year><volume>122</volume><fpage>121601</fpage><pub-id pub-id-type="doi">10.1063/5.0139330</pub-id></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Diao, Z. et al. Automatic drift compensation for nanoscale imaging using feature point matching. <italic>Appl. Phys. Lett.</italic><bold>122</bold>, 121601. 10.1063/5.0139330 (2023).</mixed-citation></citation-alternatives></ref></ref-list></back></article>