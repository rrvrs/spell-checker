<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Tomography</journal-id><journal-id journal-id-type="iso-abbrev">Tomography</journal-id><journal-id journal-id-type="publisher-id">tomography</journal-id><journal-title-group><journal-title>Tomography</journal-title></journal-title-group><issn pub-type="ppub">2379-1381</issn><issn pub-type="epub">2379-139X</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39997998</article-id><article-id pub-id-type="pmc">PMC11860310</article-id>
<article-id pub-id-type="doi">10.3390/tomography11020015</article-id><article-id pub-id-type="publisher-id">tomography-11-00015</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Utilizing Vision Transformers for Predicting Early Response of Brain Metastasis to Magnetic Resonance Imaging-Guided Stage Gamma Knife Radiosurgery Treatment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Volov&#x00103;&#x0021b;</surname><given-names>Simona Ruxandra</given-names></name><xref rid="af1-tomography-11-00015" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-7650-7798</contrib-id><name><surname>Boboc</surname><given-names>Diana-Ioana</given-names></name><xref rid="af1-tomography-11-00015" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Ostafe</surname><given-names>M&#x00103;d&#x00103;lina-Raluca</given-names></name><xref rid="af1-tomography-11-00015" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2791-3400</contrib-id><name><surname>Buzea</surname><given-names>C&#x00103;lin Gheorghe</given-names></name><xref rid="af2-tomography-11-00015" ref-type="aff">2</xref><xref rid="af3-tomography-11-00015" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Agop</surname><given-names>Maricel</given-names></name><xref rid="af4-tomography-11-00015" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6447-0958</contrib-id><name><surname>Ochiuz</surname><given-names>L&#x00103;cr&#x00103;mioara</given-names></name><xref rid="af5-tomography-11-00015" ref-type="aff">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5485-0620</contrib-id><name><surname>Rusu</surname><given-names>Drago&#x00219; Ioan</given-names></name><xref rid="af6-tomography-11-00015" ref-type="aff">6</xref></contrib><contrib contrib-type="author"><name><surname>Vasincu</surname><given-names>Decebal</given-names></name><xref rid="af7-tomography-11-00015" ref-type="aff">7</xref></contrib><contrib contrib-type="author"><name><surname>Ungureanu</surname><given-names>Monica Iuliana</given-names></name><xref rid="af8-tomography-11-00015" ref-type="aff">8</xref><xref rid="c1-tomography-11-00015" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Volov&#x00103;&#x0021b;</surname><given-names>Cristian Constantin</given-names></name><xref rid="af9-tomography-11-00015" ref-type="aff">9</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Quaia</surname><given-names>Emilio</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Sigmund</surname><given-names>Eric</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-tomography-11-00015"><label>1</label>Medical Oncology-Radiotherapy Department, &#x0201c;Grigore T. Popa&#x0201d; University of Medicine and Pharmacy Ia&#x00219;i, 700115 Ia&#x00219;i, Romania; <email>simonavolovat@gmail.com</email> (S.R.V.); <email>dianaiboboc@gmail.com</email> (D.-I.B.); <email>madalina.ostafe@gmail.com</email> (M.-R.O.)</aff><aff id="af2-tomography-11-00015"><label>2</label>&#x0201c;Prof. Dr. Nicolae Oblu&#x0201d; Clinical Emergency Hospital Ia&#x00219;i, 700309 Ia&#x00219;i, Romania; <email>calinb2003@yahoo.com</email></aff><aff id="af3-tomography-11-00015"><label>3</label>National Institute of Research and Development for Technical Physics, IFT Ia&#x00219;i, 700050 Ia&#x00219;i, Romania</aff><aff id="af4-tomography-11-00015"><label>4</label>Physics Department, &#x0201c;Gheorghe Asachi&#x0201d; Technical University Ia&#x00219;i, 700050 Ia&#x00219;i, Romania; <email>m.agop@yahoo.com</email></aff><aff id="af5-tomography-11-00015"><label>5</label>Faculty of Pharmacy, &#x0201c;Grigore T. Popa&#x0201d; University of Medicine and Pharmacy Ia&#x00219;i, 700115 Ia&#x00219;i, Romania; <email>lacramioara.ochiuz@umfiasi.ro</email></aff><aff id="af6-tomography-11-00015"><label>6</label>Faculty of Science, &#x0201c;V. Alecsandri&#x0201d; University of Bac&#x00103;u, 600115 Bac&#x00103;u, Romania; <email>drusu@ub.ro</email></aff><aff id="af7-tomography-11-00015"><label>7</label>Surgery Department, &#x0201c;Grigore T. Popa&#x0201d; University of Medicine and Pharmacy Ia&#x00219;i, 700115 Ia&#x00219;i, Romania; <email>decebal.vasincu@umfiasi.ro</email></aff><aff id="af8-tomography-11-00015"><label>8</label>Preventive Medicine and Interdisciplinarity Department, &#x0201c;Grigore T. Popa&#x0201d; University of Medicine and Pharmacy Ia&#x00219;i, 700115 Ia&#x00219;i, Romania</aff><aff id="af9-tomography-11-00015"><label>9</label>Radiology Department, &#x0201c;Grigore T. Popa&#x0201d; University of Medicine and Pharmacy Ia&#x00219;i, 700115 Ia&#x00219;i, Romania; <email>cristian.volovat@yahoo.com</email></aff><author-notes><corresp id="c1-tomography-11-00015"><label>*</label>Correspondence: <email>ungureanu_monica73@yahoo.com</email></corresp></author-notes><pub-date pub-type="epub"><day>07</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>11</volume><issue>2</issue><elocation-id>15</elocation-id><history><date date-type="received"><day>21</day><month>11</month><year>2024</year></date><date date-type="rev-recd"><day>11</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>01</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Background/Objectives: This study explores the application of vision transformers to predict early responses to stereotactic radiosurgery in patients with brain metastases using minimally pre-processed magnetic resonance imaging scans. The objective is to assess the potential of vision transformers as a predictive tool for clinical decision-making, particularly in the context of imbalanced datasets. Methods: We analyzed magnetic resonance imaging scans from 19 brain metastases patients, focusing on axial fluid-attenuated inversion recovery and high-resolution contrast-enhanced T1-weighted sequences. Patients were categorized into responders (complete or partial response) and non-responders (stable or progressive disease). Results: Despite the imbalanced nature of the dataset, our results demonstrate that vision transformers can predict early treatment responses with an overall accuracy of 99%. The model exhibited high precision (99% for progression and 100% for regression) and recall (99% for progression and 100% for regression). The use of the attention mechanism in the vision transformers allowed the model to focus on relevant features in the magnetic resonance imaging images, ensuring an unbiased performance even with the imbalanced data. Confusion matrix analysis further confirmed the model&#x02019;s reliability, with minimal misclassifications. Additionally, the model achieved a perfect area under the receiver operator characteristic curve (AUC = 1.00), effectively distinguishing between responders and non-responders. Conclusions: These findings highlight the potential of vision transformers, aided by the attention mechanism, as a non-invasive, predictive tool for early response assessment in clinical oncology. The vision transformer (ViT) model employed in this study processes MRIs as sequences of patches, enabling the capture of localized tumor features critical for early response prediction. By leveraging patch-based feature learning, this approach enhances robustness, interpretability, and clinical applicability, addressing key challenges in tumor progression prediction following stereotactic radiosurgery (SRS). The model&#x02019;s robust performance, despite the dataset imbalance, underscores its ability to provide unbiased predictions. This approach could significantly enhance clinical decision-making and support personalized treatment strategies for brain metastases. Future research should validate these findings in larger, more diverse cohorts and explore the integration of additional data types to further optimize the model&#x02019;s clinical utility.</p></abstract><kwd-group><kwd>brain metastases</kwd><kwd>stereotactic radiosurgery</kwd><kwd>vision transformers</kwd><kwd>magnetic resonance imaging</kwd><kwd>response prediction</kwd><kwd>machine learning</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-tomography-11-00015"><title>1. Introduction</title><p>Brain metastases represent a significant challenge in oncology, affecting a substantial proportion of cancer patients, particularly those with advanced-stage disease. According to recent statistics, approximately 20&#x02013;40% of patients with systemic cancer will develop brain metastases, leading to increased morbidity and mortality [<xref rid="B1-tomography-11-00015" ref-type="bibr">1</xref>,<xref rid="B2-tomography-11-00015" ref-type="bibr">2</xref>]. Stereotactic radiosurgery (SRS) has emerged as a common treatment modality for patients with brain metastases (BM), offering a minimally invasive option that can effectively control tumor growth while preserving the surrounding healthy brain tissue [<xref rid="B3-tomography-11-00015" ref-type="bibr">3</xref>]. However, the variability in patient response to SRS necessitates the development of predictive models that can identify responders and non-responders early in the treatment process [<xref rid="B4-tomography-11-00015" ref-type="bibr">4</xref>]. The emergence of targeted treatment methods, particularly Gamma Knife radiosurgery (GKRS), is increasingly recognized as a preferred strategy for managing BMs due to its enhanced ability to locally target tumors while minimizing side effects, compared to traditional whole-brain radiation therapy. Its effectiveness is comparable to that of surgical removal [<xref rid="B5-tomography-11-00015" ref-type="bibr">5</xref>,<xref rid="B6-tomography-11-00015" ref-type="bibr">6</xref>,<xref rid="B7-tomography-11-00015" ref-type="bibr">7</xref>,<xref rid="B8-tomography-11-00015" ref-type="bibr">8</xref>]. However, GKRS faces limitations when it comes to treating larger BMs (those exceeding 3 cm in diameter or 10 cc in volume) due to the risks of radiation-induced complications, such as radiation toxicity [<xref rid="B9-tomography-11-00015" ref-type="bibr">9</xref>,<xref rid="B10-tomography-11-00015" ref-type="bibr">10</xref>,<xref rid="B11-tomography-11-00015" ref-type="bibr">11</xref>,<xref rid="B12-tomography-11-00015" ref-type="bibr">12</xref>]. The Gamma Knife ICON, which utilizes mask fixation for patient positioning, offers an advanced solution for the hypo-fractionated treatment of larger BMs. Despite these advancements, the median survival rate for patients with BMs remains around one year [<xref rid="B13-tomography-11-00015" ref-type="bibr">13</xref>,<xref rid="B14-tomography-11-00015" ref-type="bibr">14</xref>]. In light of the complexity and variability of tumor dynamics following GKRS, there is a growing interest in employing advanced data analysis techniques to improve predictions of patient outcomes. Recent studies have explored various machine learning (ML) and deep learning (DL) approaches for predicting treatment responses in brain metastases, including convolutional neural networks (CNNs) and traditional statistical methods. However, these approaches often require extensive pre-processing and are limited by their inability to capture long-range dependencies within imaging data. Vision transformers (ViTs), leveraging self-attention mechanisms, present a promising alternative by directly modeling images as sequences of patches, thereby improving performance in medical imaging tasks.</p><p>Recent advancements in deep learning, particularly vision transformers, have shown promise in medical imaging analysis due to their ability to model complex data distributions and capture intricate patterns within imaging data [<xref rid="B15-tomography-11-00015" ref-type="bibr">15</xref>]. ViTs leverage self-attention mechanisms, enabling them to focus on relevant image regions, which is essential for tasks such as tumor classification and response prediction [<xref rid="B16-tomography-11-00015" ref-type="bibr">16</xref>].</p><p>While ViTs have been widely adopted in generic image analysis tasks, their application in specific clinical contexts, such as predicting early treatment responses in brain metastases, presents unique challenges. These include handling small, imbalanced datasets, ensuring model robustness, and addressing clinical interpretability. This study leverages the ViT architecture to address these challenges, demonstrating its potential in a translational oncology setting.</p><p>We aim to evaluate the effectiveness of ViTs in predicting the early response of BM to GKRS using a minimal pre-processing strategy on magnetic resonance imaging (MRI) images. Also, we evaluate the effectiveness of ViTs as a non-invasive predictive tool, aiming to bridge the gap in early response prediction and to enhance clinical applicability.</p><p>Unlike previous studies [<xref rid="B15-tomography-11-00015" ref-type="bibr">15</xref>,<xref rid="B16-tomography-11-00015" ref-type="bibr">16</xref>] that primarily focused on traditional CNN-based architectures for radiological applications, our work leverages the attention mechanisms in vision transformers to achieve interpretable predictions with minimal preprocessing. This study is novel in its focus on early response prediction in brain metastases treatment, which remains underexplored. Additionally, by addressing challenges such as dataset imbalance and demonstrating robustness in a clinical oncology setting, this work highlights the translational potential of ViTs.</p><p>This paper expands upon our previous work [<xref rid="B17-tomography-11-00015" ref-type="bibr">17</xref>], which focused on binary classification and feature extraction in MRI data. Here, we explore the novel application of vision transformers for early response prediction, emphasizing the interpretability provided by attention mechanisms. Here we demonstrate improved generalizability by addressing dataset imbalance and validating the model&#x02019;s performance on challenging clinical tasks.</p><p>Our study leverages the capabilities of vision transformers (ViTs) in the challenging context of brain metastases response prediction, introducing the use of minimally pre-processed MRI images. Unlike prior work predominantly using convolutional neural networks (CNNs), our approach benefits from ViTs&#x02019; ability to model global dependencies, making it particularly suited for the small, imbalanced datasets often encountered in clinical oncology. This methodology not only advances early response prediction in stereotactic radiosurgery but also demonstrates interpretability through attention mechanisms, making it a novel contribution to translational oncology.</p><p>This paper is structured as follows: In <xref rid="sec2-tomography-11-00015" ref-type="sec">Section 2</xref>, we outline the materials and methods employed in our study, including patient characteristics, imaging protocols, and the deep learning framework utilized. <xref rid="sec3-tomography-11-00015" ref-type="sec">Section 3</xref> presents the results of our analyses, detailing patient demographics, model performance metrics, and qualitative evaluations of model predictions. In <xref rid="sec4-tomography-11-00015" ref-type="sec">Section 4</xref>, we discuss the implications of our findings, highlighting the potential integration of ViTs into clinical practice and the importance of minimal pre-processing in enhancing the applicability of artificial intelligence (AI) in oncology. Finally, <xref rid="sec5-tomography-11-00015" ref-type="sec">Section 5</xref> concludes the paper by summarizing our contributions and proposing directions for future research.</p></sec><sec id="sec2-tomography-11-00015"><title>2. Materials and Methods</title><p>All experiments were conducted in compliance with applicable guidelines and regulations. The study utilized only pre-existing medical data, which eliminated the need for patient consent. Additionally, as this was a retrospective study, approval from the Ethics Committee of the Prof. Dr. Nicolae Oblu Emergency Clinical Hospital in Iasi was not necessary.</p><p>For details on the patient cohort, including demographic and tumor characteristics, please refer to the next <xref rid="sec2dot1-tomography-11-00015" ref-type="sec">Section 2.1</xref> Study Population.</p><sec id="sec2dot1-tomography-11-00015"><title>2.1. Study Population</title><p>This retrospective study analyzed MRI data from 19 patients diagnosed with brain metastases who underwent stereotactic radiosurgery (SRS) within three months of diagnosis. Patient demographic information, including age, sex, primary cancer type, and Karnofsky performance status (KPS), was collected from medical records. Among the 19 patients, there were 5 females and 14 males, aged between 43 and 80 years. All patients had a KPS of at least 70. The initial tumor volumes before the first treatment session ranged from 2 to 81 cm<sup>3</sup>, with an average volume of 16 cm<sup>3</sup>. The primary sites of the metastases included bronchopulmonary neoplasms in 14 cases, breast neoplasms in 3 cases, and 1 case each of laryngeal and prostate neoplasms.</p></sec><sec id="sec2dot2-tomography-11-00015"><title>2.2. Strategy for Gamma Knife Radiosurgery Implementation</title><p>The study was conducted from July 2022 to February 2023 at the Gamma Knife Stereotactic Radiosurgery Laboratory of the Prof. Dr. N. Oblu Emergency Clinical Hospital in Iasi. All patients underwent GKRS using the Leksell Gamma Knife ICON (Elekta AB, Stockholm, Sweden). All magnetic resonance images were registered with Leksell Gamma Plan (LGP, Version11.3.2, TMR algorithm) and any images with motion artifacts were excluded. The tumor volumes were calculated by LGP without margin. The treatment protocol involved administering a total dose of 30 Gy in three sessions (S1, S2, S3) of 10 Gy each, delivered at two-week intervals based on the linear quadratic model [<xref rid="B18-tomography-11-00015" ref-type="bibr">18</xref>,<xref rid="B19-tomography-11-00015" ref-type="bibr">19</xref>] and the work of Higuchi et al. from 2009 [<xref rid="B20-tomography-11-00015" ref-type="bibr">20</xref>]. The GKRS planning was determined through a consensus between the neurosurgeon, radiation oncologist, and medical physicist. Treatment response was assessed based on the response assessment in neuro-oncology brain metastases (RANO-BM) criteria, categorizing patients as responders (complete or partial response) or non-responders (stable or progressive disease) [<xref rid="B21-tomography-11-00015" ref-type="bibr">21</xref>]. Following treatment, only one patient exhibited a clear progression of the lesion, while three others demonstrated fluctuating patterns of progression and regression.</p></sec><sec id="sec2dot3-tomography-11-00015"><title>2.3. Medical Imaging Protocol</title><p>All MRI examinations were performed using a 1.5 Tesla whole-body scanner (GE SIGNA EXPLORER, Cincinnati, OH, USA) equipped with a standard 16-channel head coil. The MRI study protocol included both conventional and advanced imaging techniques for the clinical routine diagnosis of brain tumors.</p><p>The conventional anatomical MRI (cMRI) protocol encompassed an axial fluid-attenuated inversion recovery (FLAIR) sequence and a high-resolution contrast-enhanced T1-weighted (CE T1w) sequence. The FLAIR sequence was utilized to visualize edema and non-enhancing tumor components, providing insights into the tumor microenvironment, while the CE T1w sequence was employed to delineate tumor boundaries and assess the degree of enhancement indicative of tumor activity. The selected sequences, FLAIR and CE T1w, are crucial for visualizing edema and tumor boundaries, respectively, making them particularly relevant for assessing treatment responses in brain metastases.</p><p>In addition to the conventional protocol, the advanced MRI (advMRI) protocol included axial diffusion-weighted imaging (DWI) with b values of 0 and 1000 s/mm<sup>2</sup>, as well as a gradient echo dynamic susceptibility contrast (GE-DSC) perfusion MRI sequence. The GE-DSC sequence involved 60 dynamic measurements during the administration of 0.1 mmol/kg body weight gadoterate meglumine, enhancing the evaluation of tumor perfusion characteristics.</p><p>All imaging data were extracted from the tumor center, ensuring a standardized region of interest that minimized variability in tumor localization. This comprehensive imaging approach aimed to facilitate accurate assessment of tumor characteristics and treatment response in patients with brain metastases.</p></sec><sec id="sec2dot4-tomography-11-00015"><title>2.4. Image Dataset</title><p>The <bold>brain_met_1</bold> image dataset was stored on the cloud (using Google Drive), consisting of two subfolders: progression and regression. For training, the <bold>train_ds</bold> was used and for validation, the <bold>valid_ds</bold> was used. The image dataset contains 3194 MRI brain metastasis images, with 2320 images of regression class &#x02018;1&#x02019; and 874 images of progression class &#x02018;0&#x02019;. See example of images from the dataset in <xref rid="tomography-11-00015-f001" ref-type="fig">Figure 1</xref>.</p></sec><sec sec-type="methods" id="sec2dot5-tomography-11-00015"><title>2.5. Project Workflow and Methodology Overview</title><p><xref rid="tomography-11-00015-f002" ref-type="fig">Figure 2</xref> illustrates the comprehensive workflow employed in this study for predicting early responses of brain metastases to stereotactic radiosurgery (SRS) using vision transformers. The process begins with data preprocessing, which includes normalizing MRI images and extracting slices for model input. The architecture of the vision transformer is then defined, incorporating essential components such as patch embedding, transformer blocks, and a classification head. During the training process, the model undergoes multiple epochs to optimize its performance, tracking metrics like loss and accuracy. Following training, response classification distinguishes between responders and non-responders based on the model&#x02019;s predictions. Finally, evaluation metrics, including classification reports, ROC AUC, and confusion matrices, provide a thorough assessment of the model&#x02019;s effectiveness in clinical applications. This structured approach underscores the integration of advanced machine learning techniques in oncology research, aimed at enhancing patient outcomes.</p></sec><sec id="sec2dot6-tomography-11-00015"><title>2.6. Mathematical Framework</title><p>This subsection outlines the mathematical principles underpinning the proposed methodology, including the vision transformer (ViT) architecture, the binary cross-entropy loss function, and the evaluation metrics employed for assessing model performance.</p><sec id="sec2dot6dot1-tomography-11-00015"><title>2.6.1. Vision Transformer Architecture</title><p>The vision transformer (ViT) model leverages a transformer-based framework for image classification by dividing an image into patches and processing them as sequences. The main steps of the ViT process can be summarized mathematically as follows:</p><p><bold>Image Patch Embedding</bold>: The input image <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (height H, width W, channels C) is divided into N patches of size P &#x000d7; P. Each patch is flattened and linearly projected to an embedding space of dimension D, forming the patch embeddings <bold>E</bold>:<disp-formula id="FD1-tomography-11-00015"><label>(1)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mi>F</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>&#x000b7;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are learnable parameters.</p><p><bold>Positional Encoding</bold>: To preserve spatial information, learnable positional embeddings <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are added to the patch embeddings:<disp-formula id="FD2-tomography-11-00015"><label>(2)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">E</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><bold>Self-Attention Mechanism</bold>: The core of the transformer lies in the self-attention mechanism, which computes a weighted representation of patches. For a query <italic toggle="yes">Q</italic>, key <italic toggle="yes">K</italic>, and value <italic toggle="yes">V</italic>, the attention mechanism is defined as:<disp-formula id="FD3-tomography-11-00015"><label>(3)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>&#x0221a;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula>where d<sub>k</sub> is the dimension of the key vectors.</p><p><bold>Multi-Head Attention</bold>: The multi-head attention mechanism enhances the model&#x02019;s ability to focus on multiple aspects of the image. Given h attention heads, the outputs are concatenated and projected:<disp-formula id="FD4-tomography-11-00015"><label>(4)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <bold><italic toggle="yes">W</italic></bold><sub>0</sub> is a learnable weight matrix.</p><p><bold>Transformer Encoder Layers</bold>: The output of the attention mechanism is passed through a feedforward network (<italic toggle="yes">FFN</italic>) with residual connections and layer normalization:<disp-formula id="FD5-tomography-11-00015"><label>(5)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">l</italic> denotes the encoder layer index.</p><p><bold>Classification Token</bold>: A learnable class token T<sub>cls</sub> is appended to the patch embeddings and serves as the input to the final classification head.</p></sec><sec id="sec2dot6dot2-tomography-11-00015"><title>2.6.2. Loss Function</title><p>The model is trained using the binary cross-entropy loss function, as the task involves binary classification (response vs. non-response). For each sample <italic toggle="yes">i</italic>, the loss is computed as:<disp-formula id="FD6-tomography-11-00015"><label>(6)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where:</p><list list-type="bullet"><list-item><p><inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the ground truth label (1 for responder, 0 for non-responder),</p></list-item><list-item><p><inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the predicted probability for the positive class.</p></list-item></list><p>The overall loss across N samples is averaged:<disp-formula id="FD7-tomography-11-00015"><label>(7)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This choice of loss function is appropriate for imbalanced datasets when used in conjunction with weighted class sampling, ensuring that the model does not bias predictions toward the majority class.</p></sec><sec id="sec2dot6dot3-tomography-11-00015"><title>2.6.3. Evaluation Metrics</title><p>Model performance is evaluated using several metrics, including precision, recall, F1-score, confusion matrix analysis, and the area under the receiver operating characteristic curve (AUC).</p><p><bold>Precision</bold>: Precision quantifies the proportion of true positive predictions among all positive predictions:<disp-formula id="FD8-tomography-11-00015"><label>(8)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where TP and FP are true positives and false positives, respectively.</p><p><bold>Recall (Sensitivity)</bold>: Recall measures the proportion of true positives identified correctly:<disp-formula id="FD9-tomography-11-00015"><label>(9)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where FN denotes false negatives.</p><p><bold>F1-Score</bold>: The harmonic mean of precision and recall is computed as:<disp-formula id="FD10-tomography-11-00015"><label>(10)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x000b7;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><bold>AUC</bold>: The area under the receiver operating characteristic curve (AUC) measures the model&#x02019;s ability to discriminate between responders and non-responders. AUC is calculated as:<disp-formula id="FD11-tomography-11-00015"><label>(11)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>U</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x0222b;</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where TPR is the true positive rate, and FPR is the false positive rate.</p><p><bold>Confusion Matrix Analysis</bold>: The confusion matrix provides a detailed breakdown of classification outcomes (true positives, true negatives, false positives, false negatives), enabling an assessment of misclassification rates.</p><p><bold>Sensitivity (Recall for the Positive Class&#x02014;Progression)</bold>: Sensitivity measures the model&#x02019;s ability to correctly identify true positive cases (progression). The sensitivity is calculated as:<disp-formula id="FD12-tomography-11-00015"><label>(12)</label><mml:math id="mm16" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><bold>Specificity (Recall for the Negative Class&#x02014;Regression)</bold>: Specificity measures the model&#x02019;s ability to correctly identify true negative cases (regression). The specificity is calculated as:<disp-formula id="FD13-tomography-11-00015"><label>(13)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><bold>In summary,</bold> the vision transformer leverages self-attention to extract meaningful representations from medical images. The use of binary cross-entropy loss ensures robust training even with imbalanced datasets, while evaluation metrics like precision, recall, F1-score, and AUC provide comprehensive insights into model performance.</p></sec></sec><sec id="sec2dot7-tomography-11-00015"><title>2.7. Our Model&#x02019;s Architecture</title><sec id="sec2dot7dot1-tomography-11-00015"><title>2.7.1. Overview of the Used Vision Transformer Model</title><p>The vision transformer (ViT) model represents a significant advancement in computer vision, employing a transformer architecture originally designed for natural language processing. The key innovation of ViT is its ability to process images as sequences of patches, enabling the model to capture long-range dependencies effectively. This section outlines the ViT architecture tailored for the prediction of early responses in brain metastases (BM) following stereotactic radiosurgery (SRS) (<xref rid="tomography-11-00015-f003" ref-type="fig">Figure 3</xref>).</p></sec><sec id="sec2dot7dot2-tomography-11-00015"><title>2.7.2. Input Preprocessing</title><p>Before feeding images into the ViT model, the input data undergo a series of preprocessing steps:</p><p><bold>Image Acquisition:</bold> The MRI images utilized in this study were obtained from 19 patients, comprising axial fluid-attenuated inversion recovery (FLAIR) sequences and high-resolution contrast-enhanced T1-weighted (CE T1w) sequences.</p><p><bold>Patch Extraction:</bold> Each image is divided into non-overlapping patches of size 25 &#x000d7; 25 pixels. This division converts the images into a sequence format, suitable for transformer processing. The total number of patches is determined by the formula:<disp-formula id="FD14-tomography-11-00015"><label>(14)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where image_size = 200 pixels.</p><p><bold>Patch-Based Feature Learning</bold>: By dividing MRI images into patches, the model captures spatially localized tumor features that may not be prominent in global image representations. This approach ensures that the model can focus on morphologically distinct tumor regions, enhancing its ability to predict early responses to stereotactic radiosurgery (SRS). Such localized learning is crucial in medical imaging, where clinically relevant patterns are often limited to specific areas within the image.</p><p><bold>Normalization:</bold> The pixel values of the images are normalized to the range [0, 1] to facilitate effective training and convergence of the model.</p></sec><sec id="sec2dot7dot3-tomography-11-00015"><title>2.7.3. Architecture</title><p>The vision transformer model architecture comprises several key components:
<list list-type="order"><list-item><p>Input Layer: The model accepts input of shape</p></list-item></list></p><p>(num_patches, patch_size &#x000d7; patch_size &#x000d7; num_channels), where num_channels is 3 for RGB images.</p><list list-type="simple"><list-item><label>2.</label><p>Patch Embedding: Each patch is linearly projected into a higher-dimensional space (hidden dimension), which allows the model to learn richer representations. This is achieved through a dense layer defined as:
<disp-formula>patch_embeded = Dense (hidden_dim)(inputs)<label>(15)</label></disp-formula></p></list-item><list-item><label>3.</label><p>Positional Encoding: To retain the spatial information of the patches, positional embeddings are added to the patch embeddings. The positional embeddings are computed using an embedding layer that maps patch indices to dense vectors of the same dimensionality as the hidden representations.</p></list-item><list-item><label>4.</label><p>Patch-Based Aggregation: The use of patch embeddings, combined with the class token, allows the model to aggregate localized features into a single global representation. This mechanism ensures that the model attends to clinically relevant spatial patterns, even when they are confined to small regions of the MRI. The patch-based aggregation strategy provides a robust framework for addressing the challenges of heterogeneous tumor morphologies and small datasets in medical imaging applications.</p></list-item><list-item><label>5.</label><p>Class Token: A learnable class token is pre-pended to the sequence of patch embeddings. This token aggregates information from all patches and is used for the final classification task.</p></list-item><list-item><label>6.</label><p>Transformer Encoder Blocks: The core of the model consists of a stack of transformer encoder blocks, each comprising:
<list list-type="bullet"><list-item><p>Multi-Head Self-Attention Mechanism: This mechanism allows the model to attend to different parts of the input sequence simultaneously, capturing long-range dependencies.</p></list-item><list-item><p>Multilayer Perceptron (MLP): After self-attention, the output is passed through a neural network with a non-linear activation function (GELU).</p></list-item><list-item><p>Residual Connections and Layer Normalization: Each block includes residual connections that facilitate gradient flow during training and layer normalization that stabilizes the learning process.</p></list-item></list></p></list-item></list><p>The number of encoder layers in this architecture is set to 12, allowing for extensive learning capacity.</p><list list-type="simple"><list-item><label>7.</label><p>Classification Head: The output corresponding to the class token is passed through a final dense layer with a softmax activation function to predict the class probabilities (responders or non-responders).
<disp-formula>output = Dense (num_classes, activation = &#x02032;softmax&#x02032;)(x[:,0,:])<label>(16)</label></disp-formula></p></list-item></list></sec><sec id="sec2dot7dot4-tomography-11-00015"><title>2.7.4. Summary of Hyperparameters</title><p>The following hyperparameters were employed in the model architecture:<list list-type="bullet"><list-item><p>Image Size: 200 pixels</p></list-item><list-item><p>Patch Size: 25 pixels</p></list-item><list-item><p>Hidden Dimension: 768</p></list-item><list-item><p>MLP Dimension: 3072</p></list-item><list-item><p>Number of Heads: 12</p></list-item><list-item><p>Number of Layers: 12</p></list-item><list-item><p>Dropout Rate: 0.1.</p></list-item></list></p><p>Notably, the patch-based processing mechanism, in combination with the transformer architecture, enables the model to focus on spatially distinct tumor regions. This novel application to early response prediction in brain metastases following SRS demonstrates the model&#x02019;s ability to leverage localized learning for improved clinical decision-making.</p><p>The vision transformer architecture, with its innovative approach to image processing through patches and attention mechanisms, provides a robust framework for predicting early responses in brain metastases following radiosurgery. The subsequent chapters will discuss the training procedure, evaluation metrics, and results obtained from this model.</p></sec><sec id="sec2dot7dot5-tomography-11-00015"><title>2.7.5. Patch-Based Feature Learning</title><p>The vision transformer (ViT) processes images by dividing them into non-overlapping patches, each treated as a token for subsequent processing. In this study, each input MRI was divided into fixed-size patches of 16 &#x000d7; 16, resulting in 196 patches per image. These patches were independently embedded into feature vectors through a linear projection layer and processed using multi-head self-attention.</p><p>The ViT uses a class token to aggregate information from all patches into a single global representation, enabling it to focus on morphologically distinct tumor regions. This patch-based feature learning mechanism enhances the model&#x02019;s robustness and interpretability, as it captures localized patterns associated with tumor progression, even in small datasets.</p></sec></sec><sec id="sec2dot8-tomography-11-00015"><title>2.8. Training Evaluation</title><p>Unlike many classification tasks in medical imaging that rely on pre-trained models, the vision transformer (ViT) in this study was trained from scratch using our dataset. This approach ensures that the model is specifically adapted to the unique characteristics of early response prediction in brain metastases following stereotactic radiosurgery (SRS). Training the model from scratch avoids biases introduced by unrelated pre-training datasets and enables a more tailored learning process for this clinically significant application.</p><p>Training the vision transformer model requires substantial computational resources. In this study, we utilized a high-performance GPU (Google Colab) to facilitate efficient training over the 20 epochs. The batch size was set to 16, which balanced memory usage and training speed. The model was trained for 20 epochs with an Adam optimizer and used gradient clipping (clip value = 1.0) to stabilize the training process. A learning rate of 1 &#x000d7; 10<sup>&#x02212;4</sup> reduced the rate upon plateau, enhancing convergence. The binary cross-entropy loss function was used for optimization. This loss function is particularly suited for binary classification tasks and was chosen for its ability to handle imbalanced datasets effectively when combined with weighted class sampling. These configurations provided a stable and effective training setup, as shown in <xref rid="tomography-11-00015-f004" ref-type="fig">Figure 4</xref>, which illustrates the model&#x02019;s accuracy and loss curves.</p><sec id="sec2dot8dot1-tomography-11-00015"><title>2.8.1. Handling Class Imbalance with Class Weights</title><p>This implementation accounts for the imbalance in the dataset by using <bold>class weights</bold> during model training. Class weights are calculated based on the frequency of each class in the training dataset. The class_weight parameter in the fit function ensures that the loss function penalizes misclassifications of the minority class more heavily than the majority class, encouraging the model to learn balanced representations.</p><p>The compute_class_weight function from sklearn.utils.class_weight is used to compute the weights dynamically. The weights are passed as a dictionary to the fit function:<list list-type="bullet"><list-item><p><bold>Majority Class</bold>: Receives a lower weight, as it has a higher number of samples.</p></list-item><list-item><p><bold>Minority Class</bold>: Receives a higher weight, as it has fewer samples.</p></list-item></list></p><p>This approach helps mitigate the bias toward the majority class and improves the model&#x02019;s performance, especially for underrepresented classes.</p></sec><sec id="sec2dot8dot2-tomography-11-00015"><title>2.8.2. Summary of Model Performance Curves</title><p>The performance curves for the vision transformer model indicate effective learning in predicting early responses of brain metastases to radiosurgery (<xref rid="tomography-11-00015-f004" ref-type="fig">Figure 4</xref>).</p><fig position="anchor" id="tomography-11-00015-f004"><label>Figure 4</label><caption><p>ViT&#x02019;s loss using binary cross-entropy and accuracy over epochs for training and validation curves.</p></caption><graphic xlink:href="tomography-11-00015-g004" position="float"/></fig><p>Loss Over Epochs: The training loss declines significantly from about 1.0 to below 0.2, indicating effective weight adjustment, while the validation loss decreases more slowly, stabilizing around epoch 10 and remaining higher than training loss, suggesting mild overfitting without significant upward trends.</p><p>Accuracy Over Epochs: Training accuracy increases from approximately 70% to nearly 100%, reflecting strong performance on training data, whereas validation accuracy peaks near 98% by epoch 18, indicating good generalization to unseen data.</p><p>Overall, the model displays robust learning and generalization, making it a promising clinical tool.</p></sec></sec></sec><sec sec-type="results" id="sec3-tomography-11-00015"><title>3. Results</title><sec id="sec3dot1-tomography-11-00015"><title>3.1. Patient Characteristics</title><p>The study population comprised 19 patients (5 females and 14 males), with ages from 43 to 80 years, with a median of 63. Tumor types included lung (n = 14), breast (n = 3), laryngeal (n = 1), and prostate (n = 1), reflecting the common primary cancers associated with BM. The distribution of treatment responses was characterized by 15 responders (78.9%) and 4 non-responders (21.1%).</p></sec><sec id="sec3dot2-tomography-11-00015"><title>3.2. Model Performance Evaluation</title><p>Evaluation metrics included accuracy, precision, recall, and F1-score (<xref rid="tomography-11-00015-t001" ref-type="table">Table 1</xref>), with a focus on the area under the receiver operating characteristic curve (AUC-ROC) for overall performance assessment (<xref rid="tomography-11-00015-f005" ref-type="fig">Figure 5</xref>). Additionally, confusion matrices were generated to visualize the true positive, false positive, true negative, and false negative rates (<xref rid="tomography-11-00015-f006" ref-type="fig">Figure 6</xref>).</p><p>The classification report (<xref rid="tomography-11-00015-t001" ref-type="table">Table 1</xref>) summarizes the performance of the vision transformer model in predicting early responses of brain metastases, providing detailed metrics for both classes: progression and regression.</p><list list-type="simple"><list-item><p>
<bold>Precision</bold>
<list list-type="bullet"><list-item><p><bold>Progression:</bold> The precision for progression is <bold>0.99</bold>, indicating that 99% of the instances predicted as progression were indeed correct. This high precision suggests the model is reliable in identifying true progression cases.</p></list-item><list-item><p><bold>Regression:</bold> The precision for regression is <bold>1.00</bold>, indicating that 100% of the instances predicted as regression were correct. This exceptionally high precision reflects the model&#x02019;s strong performance in classifying regression cases.</p></list-item></list>
</p></list-item><list-item><p>
<bold>Recall</bold>
<list list-type="bullet"><list-item><p><bold>Progression:</bold> The recall for progression is <bold>0.99</bold>, meaning the model correctly identifies 99% of actual progression cases. This high recall indicates the model&#x02019;s effectiveness in capturing most of the true positive cases for progression.</p></list-item><list-item><p><bold>Regression:</bold> The recall for regression is <bold>1.00</bold>, suggesting the model successfully identifies 100% of actual regression cases, demonstrating strong performance in recognizing positive instances.</p></list-item></list>
</p></list-item><list-item><p>
<bold>F1-Score</bold>
<list list-type="bullet"><list-item><p><bold>Progression:</bold> The F1-score for progression is <bold>0.99</bold>, reflecting a good balance between precision and recall, indicating the model&#x02019;s overall accuracy in identifying progression cases.</p></list-item><list-item><p><bold>Regression:</bold> The F1-score for regression is <bold>1.00</bold>, showing excellent performance, emphasizing both precision and recall for this class.</p></list-item></list>
</p></list-item><list-item><p>
<bold>Overall Accuracy</bold>
<list list-type="bullet"><list-item><p>The overall accuracy of the model is <bold>0.99</bold>, demonstrating that the model correctly classified 99% of the total cases. This high accuracy, combined with the strong metrics for both classes, underscores the model&#x02019;s robustness and effectiveness in clinical predictions.</p></list-item></list>
</p></list-item><list-item><p>
<bold>Averages</bold>
<list list-type="bullet"><list-item><p><bold>Macro Average:</bold> The macro average precision and recall are <bold>0.99</bold> and <bold>0.99</bold>, respectively, reflecting the model&#x02019;s balanced performance across both classes without being biased towards the majority class.</p></list-item><list-item><p><bold>Weighted Average:</bold> The weighted average precision and recall are <bold>0.99</bold> and <bold>0.99</bold>, showing that the model maintains high performance even when accounting for the unequal class sizes.</p></list-item></list>
</p></list-item></list><p>Here are the key points from the curve in <xref rid="tomography-11-00015-f005" ref-type="fig">Figure 5</xref>:
<list list-type="simple"><list-item><p><bold>True Positive Rate (Sensitivity)</bold>: The y-axis represents the true positive rate (TPR), which indicates the proportion of actual positive cases correctly identified by the model. In this case, a TPR close to 1.00 signifies that the model is highly effective in identifying responders.</p></list-item><list-item><p><bold>False Positive Rate</bold>: The x-axis represents the false positive rate (FPR), indicating the proportion of negative cases that are incorrectly classified as positive. A lower FPR is desirable, as it means the model is making fewer mistakes in identifying non-responders.</p></list-item><list-item><p><bold>Area Under the Curve (AUC)</bold>: The area under the ROC curve (AUC) is reported as 1.00, indicating perfect classification performance. This suggests that the model correctly distinguishes between responders and non-responders without any overlap, effectively making accurate predictions across all thresholds.</p></list-item><list-item><p><bold>Curve Shape</bold>: The ROC curve rises sharply and approaches the top left corner of the plot, reflecting that the model achieves high sensitivity with very few false positives. This shape indicates strong performance, particularly in clinical contexts where the cost of false negatives (missing a responder) can be high.</p></list-item></list></p><p>The confusion matrix displayed (<xref rid="tomography-11-00015-f006" ref-type="fig">Figure 6</xref>) provides a clear overview of the classification performance of the vision transformer model in predicting the early response of brain metastases to radiosurgery. Here are the key observations:
<list list-type="simple"><list-item><p><bold>True Positives (TP):</bold> The model correctly identified 85 cases of progression (top left cell), indicating a strong ability to classify patients who experienced disease progression.</p></list-item><list-item><p><bold>True Negatives (TN):</bold> The model accurately classified 232 cases of regression (bottom right cell), demonstrating effective recognition of patients whose condition improved following treatment.</p></list-item><list-item><p><bold>False Positives (FP):</bold> There is one case where the model incorrectly predicted progression when the actual outcome was regression (top right cell).</p></list-item><list-item><p><bold>False Negatives (FN):</bold> The model misclassified one case of progression as regression (bottom left cell).</p></list-item></list></p><p>These results demonstrate that the model is highly effective at classifying both progression and regression, with only a small number of misclassifications.</p><p>Using Equations (12) and (13) and the results of the confusion matrix in <xref rid="tomography-11-00015-f006" ref-type="fig">Figure 6</xref>, we can further calculate the sensitivity and the <bold>specificity</bold>. Its <bold>sensitivity</bold> of <bold>98.84%</bold> and <bold>specificity</bold> of <bold>99.57%</bold> reflect its strong predictive capability.</p><p>An initial review of the confusion matrix revealed trends in misclassification, particularly in patients with larger tumor volumes or borderline responses to treatment. However, due to the small dataset size, further subgroup analysis was not feasible. Future research should investigate misclassification patterns in larger datasets to identify patient-specific factors influencing model performance.</p><p>To assess the robustness of the ViT model&#x02019;s predictive performance, we used <bold>bootstrap resampling to compute 95% confidence intervals for both accuracy and AUC scores</bold> based on test set predictions. Using 1000 bootstrap samples, we obtained confidence intervals for <bold>accuracy</bold>, with lower and upper bounds at <bold>98.43%</bold> and <bold>100.00%</bold>, respectively, and for <bold>AUC</bold>, at <bold>99.83%</bold> and <bold>100.00%</bold>. These intervals provide a statistically grounded measure of the model&#x02019;s reliability in classifying treatment responses.</p></sec><sec id="sec3dot3-tomography-11-00015"><title>3.3. Qualitative Analysis</title><p>The vision transformer (ViT) model demonstrated the ability to focus on key tumor-related features, as shown by the attention maps in <xref rid="tomography-11-00015-f007" ref-type="fig">Figure 7</xref>. In the MRI images, distinct tumor regions are visible, characterized by altered enhancement patterns and edema surrounding the lesion. The corresponding attention maps reveal that the model consistently highlights patches associated with these abnormal regions.</p><p>The attention maps indicate that the model assigns higher attention (marked by warmer colors like red and yellow) to areas likely containing tumor tissue, suggesting it recognizes significant pathological features. This interpretability reinforces the clinical relevance of the ViT model, as it not only distinguishes tumor areas effectively but also aligns its focus with known tumor characteristics, aiding in understanding the basis of its predictions. These examples illustrate the model&#x02019;s capacity to differentiate response patterns in brain metastasis, potentially contributing to improved diagnostic confidence in clinical settings.</p><p><xref rid="tomography-11-00015-f008" ref-type="fig">Figure 8</xref> displays ten randomly selected MRI cases from the patient image database, showcasing the model&#x02019;s performance in distinguishing between progression and regression patterns. Each image includes the ground truth label, the model&#x02019;s prediction, and the predicted probability. Across these cases, the model accurately identifies both regression and progression with high confidence (probabilities close to or equal to 1.00).</p><p>These examples emphasize the model&#x02019;s robust capability to differentiate between treatment response types, reinforcing its clinical relevance. Consistently high confidence levels indicate that the vision transformer model effectively recognizes patterns associated with each response type, thereby supporting its potential utility in real-world clinical settings for monitoring brain metastasis.</p></sec></sec><sec sec-type="discussion" id="sec4-tomography-11-00015"><title>4. Discussion</title><p>This study demonstrates the potential of vision transformers in predicting early treatment responses in brain metastases after SRS using minimal MRI pre-processing [<xref rid="B22-tomography-11-00015" ref-type="bibr">22</xref>]. The promising results indicate that ViTs can be integrated into clinical workflows to aid in personalized treatment planning and improve patient outcomes. By leveraging minimal pre-processing, this approach addresses the need for efficient, scalable solutions in clinical practice, especially in resource-limited settings.</p><p>The present work differentiates itself from existing research by focusing on the novel clinical application of predicting early treatment responses in brain metastases using vision transformers (ViTs). Unlike most studies that apply pretrained models for general classification tasks, our model was trained from scratch to address the specific challenge of early response prediction following stereotactic radiosurgery (SRS). Additionally, the use of minimal MRI preprocessing demonstrates the potential for integrating this approach into clinical workflows, where simplicity and efficiency are critical.</p><p>The retrospective dataset used in this study is described in detail in <xref rid="sec2dot1-tomography-11-00015" ref-type="sec">Section 2.1</xref>, including patient demographics, tumor volumes, primary cancer types, and imaging protocols. The dataset&#x02019;s diversity provides a foundation for assessing the model&#x02019;s robustness, though its small size remains a limitation.</p><p>Recent studies have highlighted the effectiveness of vision transformers in medical imaging, particularly in capturing global contextual information and outperforming traditional convolutional neural networks in complex tasks [<xref rid="B23-tomography-11-00015" ref-type="bibr">23</xref>,<xref rid="B24-tomography-11-00015" ref-type="bibr">24</xref>].</p><p>The findings align with previous studies highlighting the efficacy of deep learning models in radiological applications [<xref rid="B25-tomography-11-00015" ref-type="bibr">25</xref>,<xref rid="B26-tomography-11-00015" ref-type="bibr">26</xref>,<xref rid="B27-tomography-11-00015" ref-type="bibr">27</xref>,<xref rid="B28-tomography-11-00015" ref-type="bibr">28</xref>]. The high accuracy and AUC-ROC underscore the robustness of ViTs in handling the complexity of medical images. Moreover, the use of minimal pre-processing simplifies the pipeline, making it feasible for implementation in busy clinical environments where rapid decision-making is essential.</p><p>The robustness of the vision transformer model is further validated through bootstrap analysis, with confidence intervals for accuracy ranging from 98.43% to 100.00% and for AUC from 99.83% to 100.00%. These metrics underscore the reliability of the model&#x02019;s predictions, even with a small dataset, and highlight its potential for clinical application in early response prediction.</p><p>The interpretability of the model, demonstrated through attention maps, presents an additional advantage, allowing clinicians to understand the rationale behind predictions and potentially enhance the trustworthiness of AI applications in medical settings [<xref rid="B29-tomography-11-00015" ref-type="bibr">29</xref>,<xref rid="B30-tomography-11-00015" ref-type="bibr">30</xref>,<xref rid="B31-tomography-11-00015" ref-type="bibr">31</xref>]. Future research should explore the integration of additional clinical and radiological features to further enhance predictive accuracy and generalizability across diverse patient populations.</p><sec id="sec4dot1-tomography-11-00015"><title>4.1. Related Studies</title><p>Recent advancements in vision transformers (ViTs) for medical imaging have highlighted their potential in a range of applications, including segmentation, classification, and anomaly detection [<xref rid="B23-tomography-11-00015" ref-type="bibr">23</xref>,<xref rid="B24-tomography-11-00015" ref-type="bibr">24</xref>]. These studies have demonstrated ViTs&#x02019; ability to outperform traditional CNNs by capturing global contextual information through their attention mechanisms. However, many existing studies rely on generalized imaging datasets or require extensive preprocessing (see <xref rid="tomography-11-00015-t002" ref-type="table">Table 2</xref>). Reddy et al. (2024) developed fine-tuned vision transformer models for multi-class brain tumor classification, achieving an accuracy of 98.70% with the FTVT-l16 model [<xref rid="B32-tomography-11-00015" ref-type="bibr">32</xref>]. Labbaf Khaniki et al. (2024) proposed a vision transformer with a novel cross-attention mechanism for brain tumor classification, achieving an accuracy of 98.93% [<xref rid="B33-tomography-11-00015" ref-type="bibr">33</xref>]. Lyu et al. (2021) introduced a transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole-brain MRI, reporting an AUC of 0.878 [<xref rid="B34-tomography-11-00015" ref-type="bibr">34</xref>]. Krishnan et al. (2024) developed a rotation invariant vision transformer (RViT) for enhancing brain tumor detection in MRI, achieving an accuracy of 98.6% [<xref rid="B35-tomography-11-00015" ref-type="bibr">35</xref>].</p><p>In contrast, our study emphasizes minimal preprocessing and focuses specifically on early response prediction in brain metastases, addressing a critical gap in the application of ViTs for oncology-specific tasks. By achieving high accuracy (99%) and AUC (1.00), this work demonstrates the viability of ViTs in real-world clinical workflows.</p></sec><sec id="sec4dot2-tomography-11-00015"><title>4.2. Limitations</title><p>Sample Size: The study is based on a relatively small cohort of 19 patients. While the initial results are promising, a larger and more diverse dataset is necessary to validate the model&#x02019;s performance across different populations and to ensure its generalizability. A limited sample size may also increase the risk of overfitting, making it crucial to confirm these findings with additional data. It may limit also the generalizability of our findings, particularly in diverse populations. A larger cohort with varied demographics and tumor characteristics will be crucial for validating the model&#x02019;s robustness. Furthermore, the retrospective nature of this study could introduce biases, necessitating future prospective studies to mitigate these concerns.</p><p>The small dataset size (19 patients) is a limitation of this study, as it restricts the generalizability of our findings. To address this, future studies should validate the model on larger, multi-center datasets with diverse patient populations. Additionally, we performed a sensitivity analysis to evaluate the model&#x02019;s robustness under varying class distribution scenarios. The model maintained a high AUC (&#x0003e;0.95) even in these conditions, suggesting its robustness despite the dataset&#x02019;s imbalance. However, larger datasets would enable further refinement and validation of these findings.</p><p>While bootstrap analysis demonstrates the model&#x02019;s robustness, the small sample size (19 patients) remains a limitation. Future work will validate the model on larger, multi-center datasets with diverse patient populations.</p><p><bold>Imaging Modalities:</bold> The model&#x02019;s effectiveness was evaluated using specific MRI sequences (FLAIR and CE T1w). While these modalities are common in clinical practice, recent studies have demonstrated the potential of integrating additional imaging modalities, such as functional MRI or diffusion-weighted imaging, to enhance predictive accuracy [<xref rid="B36-tomography-11-00015" ref-type="bibr">36</xref>]. Future research should explore multi-modality imaging approaches to leverage the complementary information available from various sequences.</p><p><bold>Retrospective Nature:</bold> The study utilized a retrospective design, relying on pre-existing medical data. This approach may introduce biases related to patient selection and data quality. Prospective studies would help address these biases and provide a clearer understanding of the model&#x02019;s performance in real-time clinical settings.</p><p><bold>Class Imbalance:</bold> Although the model demonstrated good performance metrics, there is a slight imbalance in the number of cases between progression and regression classes. Imbalanced datasets are a recognized challenge in deep learning, and advanced approaches, such as label-distribution-aware margin loss or synthetic data generation, have been proposed to mitigate their effects [<xref rid="B37-tomography-11-00015" ref-type="bibr">37</xref>,<xref rid="B38-tomography-11-00015" ref-type="bibr">38</xref>]. Future studies could incorporate these techniques to further improve the model&#x02019;s robustness.</p><p><bold>Clinical Context:</bold> The model&#x02019;s predictions are based solely on imaging data and do not incorporate other potentially influential clinical factors, such as genetic markers, histopathological features, or patient comorbidities. Integrating these additional variables could enhance the model&#x02019;s predictive power and its applicability in clinical decision-making. Recent advancements in interpretability frameworks for vision transformers could also aid in integrating multi-modal data to improve prediction confidence and clinical utility [<xref rid="B29-tomography-11-00015" ref-type="bibr">29</xref>].</p></sec><sec id="sec4dot3-tomography-11-00015"><title>4.3. Technical Contributions and Clinical Implications</title><p>The integration of patch-based feature learning within the vision transformer (ViT) architecture represents a novel and significant advancement in early tumor progression prediction for stereotactic radiosurgery (SRS). Traditional convolutional neural networks (CNNs) often emphasize global image features, which may dilute critical localized tumor patterns. In contrast, the patch-based approach employed by ViTs captures spatially distinct morphological features by processing non-overlapping image patches.</p><p>This localized learning mechanism addresses several key challenges in medical imaging:<list list-type="order"><list-item><p><bold>Tumor Heterogeneity</bold>: Tumors exhibit heterogeneous morphology, and clinically relevant changes often occur in small regions. The patch-based approach enables the model to focus on these localized patterns, making it particularly effective for identifying early progression or regression.</p></list-item><list-item><p><bold>Small Dataset Robustness</bold>: By leveraging patch-level representations, the model mitigates overfitting, which is a common limitation when training on small medical datasets.</p></list-item><list-item><p><bold>Interpretability</bold>: The attention mechanism within the ViT allows for visualization of the regions that contribute most to the model&#x02019;s predictions. This enhances clinical trust and facilitates integration into workflows by providing explainable outputs.</p></list-item></list></p><p>To our knowledge, this study is the first to apply a patch-based ViT model for early tumor progression prediction in brain metastases following SRS. This approach not only demonstrates strong predictive performance but also offers insights into the spatial patterns associated with tumor progression, contributing to a deeper understanding of the biological processes involved.</p></sec><sec id="sec4dot4-tomography-11-00015"><title>4.4. Future Directions</title><p><bold>Larger Multi-Center Studies:</bold> Future research should focus on conducting larger, multi-center studies to validate the model&#x02019;s performance across diverse patient populations. This would help establish robustness and generalizability, ensuring that the model is applicable in various clinical settings.</p><p><bold>Integration of Additional Data Types:</bold> Expanding the model to incorporate various data types, including genomic information, histopathological data, and clinical features, could improve predictive accuracy. Developing a multimodal approach would allow for a more comprehensive assessment of patient responses to treatment. Recent studies on vision transformers in medical imaging suggest that such approaches are increasingly feasible and effective [<xref rid="B23-tomography-11-00015" ref-type="bibr">23</xref>,<xref rid="B24-tomography-11-00015" ref-type="bibr">24</xref>].</p><p><bold>Real-Time Predictive Systems:</bold> Efforts should be made to develop real-time predictive systems that can be integrated into clinical workflows. Such systems would assist clinicians in making timely decisions about patient management based on live imaging data and model predictions. For instance, Lin and Heckel (2022) [<xref rid="B36-tomography-11-00015" ref-type="bibr">36</xref>] demonstrated the capability of vision transformers in accelerating MRI workflows, which could be a key step toward real-time applications in SRS.</p><p><bold>Model Refinement:</bold> Continuous refinement of the vision transformer architecture and training methodologies should be pursued to enhance model performance. Experimentation with hyperparameter tuning, different model architectures, and advanced training techniques (such as transfer learning) could lead to improved outcomes. Techniques for handling imbalanced datasets, such as those proposed by Cao et al. (2019) [<xref rid="B38-tomography-11-00015" ref-type="bibr">38</xref>], may also enhance robustness.</p><p><bold>Future studies</bold> could compare the performance of vision transformers with other machine learning (ML) and deep learning (DL) models, such as convolutional neural networks (CNNs) or ensemble methods, to better understand their relative strengths and weaknesses in this specific clinical application.</p><p><bold>Subgroup-level analysis</bold> of misclassifications, including factors such as tumor size, primary cancer type, and response variability, could provide valuable insights into model performance and reliability. This would enhance the model&#x02019;s clinical interpretability and applicability.</p><p><bold>Longitudinal Studies:</bold> Future studies should consider a longitudinal approach to assess how the model&#x02019;s predictions correlate with long-term patient outcomes. This could provide valuable insights into the model&#x02019;s utility in guiding treatment and monitoring disease progression over time.</p><p><bold>User-Friendly Interfaces</bold>: Developing user-friendly interfaces that facilitate the use of the model in clinical practice is essential. These tools should be designed to allow clinicians to input MRI data and obtain predictions with minimal complexity, ultimately aiding in the decision-making process. Moreover, interpretability frameworks, such as those evaluated by Komorowski et al. (2023) [<xref rid="B29-tomography-11-00015" ref-type="bibr">29</xref>], could enhance clinician trust and adoption of these systems.</p><p>In addition to oncology, recent deep learning innovations have shown success in other domains, such as automated ultrasonic-based diagnosis of concrete compressive damage and compressive strength evaluation of materials under environmental stress [<xref rid="B39-tomography-11-00015" ref-type="bibr">39</xref>]. These advancements underscore the transformative potential of deep learning in solving diverse real-world challenges.</p><p>In conclusion, while the vision transformer model shows great promise in predicting the early responses of brain metastases to radiosurgery, addressing its limitations and pursuing these future directions will be critical for enhancing its utility in clinical practice and improving patient outcomes.</p></sec></sec><sec sec-type="conclusions" id="sec5-tomography-11-00015"><title>5. Conclusions</title><p>The vision transformer model has demonstrated exceptional performance in predicting the early responses of brain metastases to radiosurgery, achieving an overall accuracy of 99%. With high precision (99% for progression and 100% for regression) and strong recall rates (99% for progression and 100% for regression), the model effectively distinguishes between treatment outcomes. The confusion matrix analysis further supports its reliability, showing minimal misclassifications. The perfect area under the ROC curve (AUC = 1.00) indicates that the model can accurately differentiate between responders and non-responders across various thresholds. These findings suggest that the vision transformer model is not only robust but also holds significant promise for clinical applications in oncology, enhancing decision-making processes and ultimately improving patient outcomes. By improving early response predictions for brain metastases, this model not only has the potential to refine clinical decision-making but also to contribute to the development of personalized treatment strategies in oncology.</p><p>To address the &#x02018;black box&#x02019; nature of AI models, attention maps were employed to visualize the areas of MRI images that the vision transformer model prioritized in making its predictions. These maps enhance interpretability by providing clinicians with insights into the decision-making process, thereby fostering trust and facilitating the integration of AI tools into clinical decision-making. The use of explainable AI (XAI) aligns with current trends in medical AI research, emphasizing transparency and accountability.</p><p>Future research should focus on validating these results in larger, diverse cohorts, integrating additional data types, and refining the model to further enhance its utility in clinical practice.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization: S.R.V., D.-I.B. and L.O.; data curation: M.-R.O., C.C.V. and D.I.R.; investigation: D.-I.B., M.A., M.I.U. and D.V.; software: C.G.B.; supervision: S.R.V., D.-I.B. and D.I.R.; validation: S.R.V., M.A. and D.V.; visualization: C.G.B., C.C.V. and L.O.; writing&#x02014;original draft: S.R.V., D.-I.B., M.-R.O. and D.I.R.; writing&#x02014;review and editing: M.A., M.I.U., C.C.V. and D.V.; funding acquisition: S.R.V., M.I.U. and D.V. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Since the study was retrospective, there was no need for approval from the Ethics Committee.</p></notes><notes><title>Informed Consent Statement</title><p>The study used only pre-existing medical data, therefore, patient consent was not required.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The dataset used in this study is the property of the Clinical Emergency Hospital &#x0201c;Prof. Dr. Nicolae Oblu&#x0201d; in Iasi and is hosted on Google Cloud. Access to the data is restricted due to privacy regulations and ethical considerations. Researchers interested in accessing the dataset may submit a formal request to Clinical Emergency Hospital &#x0201c;Prof. Dr. Nicolae Oblu&#x0201d; in Iasi at the Gamma Knife Department (<email>gamma.oblu@gmail.com</email>). Approval is subject to compliance with the hospital&#x02019;s data-sharing policies and applicable regulations.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-tomography-11-00015"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kuksis</surname><given-names>M.</given-names></name>
<name><surname>Gao</surname><given-names>Y.</given-names></name>
<name><surname>Tran</surname><given-names>W.</given-names></name>
<name><surname>Hoey</surname><given-names>C.</given-names></name>
<name><surname>Kiss</surname><given-names>A.</given-names></name>
<name><surname>Komorowski</surname><given-names>A.S.</given-names></name>
<name><surname>Dhaliwal</surname><given-names>A.J.</given-names></name>
<name><surname>Sahgal</surname><given-names>A.</given-names></name>
<name><surname>Das</surname><given-names>S.</given-names></name>
<name><surname>Chan</surname><given-names>K.K.</given-names></name>
<etal/>
</person-group><article-title>The incidence of brain metastases among patients with metastatic breast cancer: A systematic review and meta-analysis</article-title><source>Neuro Oncol.</source><year>2021</year><volume>23</volume><fpage>894</fpage><lpage>904</lpage><!--<pub-id pub-id-type="pmcid">PMC8168821</pub-id>--><pub-id pub-id-type="doi">10.1093/neuonc/noaa285</pub-id><pub-id pub-id-type="pmid">33367836</pub-id>
</element-citation></ref><ref id="B2-tomography-11-00015"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gavrilovic</surname><given-names>I.T.</given-names></name>
<name><surname>Posner</surname><given-names>J.B.</given-names></name>
</person-group><article-title>Brain metastases: Epidemiology and pathophysiology</article-title><source>J. Neurooncol.</source><year>2005</year><volume>75</volume><fpage>5</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1007/s11060-004-8093-6</pub-id><pub-id pub-id-type="pmid">16215811</pub-id>
</element-citation></ref><ref id="B3-tomography-11-00015"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lehrer</surname><given-names>E.J.</given-names></name>
<name><surname>Jones</surname><given-names>B.M.</given-names></name>
<name><surname>Sindhu</surname><given-names>K.K.</given-names></name>
<name><surname>Dickstein</surname><given-names>D.R.</given-names></name>
<name><surname>Cohen</surname><given-names>M.</given-names></name>
<name><surname>Lazarev</surname><given-names>S.</given-names></name>
<name><surname>Qui&#x000f1;ones-Hinojosa</surname><given-names>A.</given-names></name>
<name><surname>Green</surname><given-names>S.</given-names></name>
<name><surname>Trifiletti</surname><given-names>D.M.</given-names></name>
</person-group><article-title>A Review of the Role of Stereotactic Radiosurgery and Immunotherapy in the Management of Primary Central Nervous System Tumors</article-title><source>Biomedicines</source><year>2022</year><volume>10</volume><elocation-id>2977</elocation-id><!--<pub-id pub-id-type="pmcid">PMC9687865</pub-id>--><pub-id pub-id-type="doi">10.3390/biomedicines10112977</pub-id><pub-id pub-id-type="pmid">36428546</pub-id>
</element-citation></ref><ref id="B4-tomography-11-00015"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mangesius</surname><given-names>J.</given-names></name>
<name><surname>Seppi</surname><given-names>T.</given-names></name>
<name><surname>Arnold</surname><given-names>C.R.</given-names></name>
<name><surname>Mangesius</surname><given-names>S.</given-names></name>
<name><surname>Kerschbaumer</surname><given-names>J.</given-names></name>
<name><surname>Demetz</surname><given-names>M.</given-names></name>
<name><surname>Minasch</surname><given-names>D.</given-names></name>
<name><surname>Vorbach</surname><given-names>S.M.</given-names></name>
<name><surname>Sarcletti</surname><given-names>M.</given-names></name>
<name><surname>Lukas</surname><given-names>P.</given-names></name>
<etal/>
</person-group><article-title>Prognosis versus Actual Outcomes in Stereotactic Radiosurgery of Brain Metastases: Reliability of Common Prognostic Parameters and Indices</article-title><source>Curr. Oncol.</source><year>2024</year><volume>31</volume><fpage>1739</fpage><lpage>1751</lpage><!--<pub-id pub-id-type="pmcid">PMC11049204</pub-id>--><pub-id pub-id-type="doi">10.3390/curroncol31040132</pub-id><pub-id pub-id-type="pmid">38668035</pub-id>
</element-citation></ref><ref id="B5-tomography-11-00015"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kondziolka</surname><given-names>D.</given-names></name>
<name><surname>Patel</surname><given-names>A.</given-names></name>
<name><surname>Lunsford</surname><given-names>L.</given-names></name>
<name><surname>Kassam</surname><given-names>A.</given-names></name>
<name><surname>Flickinger</surname><given-names>J.C.</given-names></name>
</person-group><article-title>Stereotactic radiosurgery plus whole brain radiotherapy versus radiotherapy alone for patients with multiple brain metastases</article-title><source>Int. J. Radiat. Oncol. Biol. Phys.</source><year>1999</year><volume>45</volume><fpage>427</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/S0360-3016(99)00198-4</pub-id><pub-id pub-id-type="pmid">10487566</pub-id>
</element-citation></ref><ref id="B6-tomography-11-00015"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Patchell</surname><given-names>R.A.</given-names></name>
<name><surname>Tibbs</surname><given-names>P.A.</given-names></name>
<name><surname>Walsh</surname><given-names>J.W.</given-names></name>
<name><surname>Dempsey</surname><given-names>R.J.</given-names></name>
<name><surname>Maruyama</surname><given-names>Y.</given-names></name>
<name><surname>Kryscio</surname><given-names>R.J.</given-names></name>
<name><surname>Markesbery</surname><given-names>W.R.</given-names></name>
<name><surname>Macdonald</surname><given-names>J.S.</given-names></name>
<name><surname>Young</surname><given-names>B.</given-names></name>
</person-group><article-title>A Randomized trial of surgery in the treatment of single metastases to the brain</article-title><source>N. Engl. J. Med.</source><year>1990</year><volume>322</volume><fpage>494</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1056/NEJM199002223220802</pub-id><pub-id pub-id-type="pmid">2405271</pub-id>
</element-citation></ref><ref id="B7-tomography-11-00015"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Park</surname><given-names>Y.G.</given-names></name>
<name><surname>Choi</surname><given-names>J.Y.</given-names></name>
<name><surname>Chang</surname><given-names>J.W.</given-names></name>
<name><surname>Chung</surname><given-names>S.S.</given-names></name>
</person-group><article-title>Gamma knife radiosurgery for metastatic brain tumors</article-title><source>Ster. Funct. Neurosurg.</source><year>2001</year><volume>76</volume><fpage>201</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1159/000066718</pub-id></element-citation></ref><ref id="B8-tomography-11-00015"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kocher</surname><given-names>M.</given-names></name>
<name><surname>Soffietti</surname><given-names>R.</given-names></name>
<name><surname>Abacioglu</surname><given-names>U.</given-names></name>
<name><surname>Vill&#x000e0;</surname><given-names>S.</given-names></name>
<name><surname>Fauchon</surname><given-names>F.</given-names></name>
<name><surname>Baumert</surname><given-names>B.G.</given-names></name>
<name><surname>Fariselli</surname><given-names>L.</given-names></name>
<name><surname>Tzuk-Shina</surname><given-names>T.</given-names></name>
<name><surname>Kortmann</surname><given-names>R.-D.</given-names></name>
<name><surname>Carrie</surname><given-names>C.</given-names></name>
<etal/>
</person-group><article-title>Adjuvant whole-brain radiotherapy versus observation after radiosurgery or surgical resection of one to three cerebral metastases: Results of the EORTC 22952-26001 study</article-title><source>J. Clin. Oncol.</source><year>2011</year><volume>29</volume><fpage>134</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1200/JCO.2010.30.1655</pub-id><pub-id pub-id-type="pmid">21041710</pub-id>
</element-citation></ref><ref id="B9-tomography-11-00015"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>Y.-J.</given-names></name>
<name><surname>Cho</surname><given-names>K.H.</given-names></name>
<name><surname>Kim</surname><given-names>J.-Y.</given-names></name>
<name><surname>Lim</surname><given-names>Y.K.</given-names></name>
<name><surname>Min</surname><given-names>H.S.</given-names></name>
<name><surname>Lee</surname><given-names>S.H.</given-names></name>
<name><surname>Kim</surname><given-names>H.J.</given-names></name>
<name><surname>Gwak</surname><given-names>H.S.</given-names></name>
<name><surname>Yoo</surname><given-names>H.</given-names></name>
<name><surname>Lee</surname><given-names>S.H.</given-names></name>
</person-group><article-title>Single-dose versus fractionated stereotactic radiotherapy for brain metastases</article-title><source>Int. J. Radiat. Oncol. Biol. Phys.</source><year>2011</year><volume>81</volume><fpage>483</fpage><lpage>489</lpage><pub-id pub-id-type="doi">10.1016/j.ijrobp.2010.05.033</pub-id><pub-id pub-id-type="pmid">20800386</pub-id>
</element-citation></ref><ref id="B10-tomography-11-00015"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jee</surname><given-names>T.K.</given-names></name>
<name><surname>Seol</surname><given-names>H.J.</given-names></name>
<name><surname>Im</surname><given-names>Y.-S.</given-names></name>
<name><surname>Kong</surname><given-names>D.-S.</given-names></name>
<name><surname>Nam</surname><given-names>D.-H.</given-names></name>
<name><surname>Park</surname><given-names>K.</given-names></name>
<name><surname>Shin</surname><given-names>H.J.</given-names></name>
<name><surname>Lee</surname><given-names>J.-I.</given-names></name>
</person-group><article-title>Fractionated gamma knife radiosurgery for benign perioptic tumors: Outcomes of 38 patients in a single institute</article-title><source>Brain Tumor Res. Treat.</source><year>2014</year><volume>2</volume><fpage>56</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.14791/btrt.2014.2.2.56</pub-id><pub-id pub-id-type="pmid">25408926</pub-id>
</element-citation></ref><ref id="B11-tomography-11-00015"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ernst-Stecken</surname><given-names>A.</given-names></name>
<name><surname>Ganslandt</surname><given-names>O.</given-names></name>
<name><surname>Lambrecht</surname><given-names>U.</given-names></name>
<name><surname>Sauer</surname><given-names>R.</given-names></name>
<name><surname>Grabenbauer</surname><given-names>G.</given-names></name>
</person-group><article-title>Phase II trial of hypofractionated stereotactic radiotherapy for brain metastases: Results and toxicity</article-title><source>Radiother. Oncol.</source><year>2006</year><volume>81</volume><fpage>18</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.radonc.2006.08.024</pub-id><pub-id pub-id-type="pmid">16978720</pub-id>
</element-citation></ref><ref id="B12-tomography-11-00015"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>J.W.</given-names></name>
<name><surname>Park</surname><given-names>H.R.</given-names></name>
<name><surname>Lee</surname><given-names>J.M.</given-names></name>
<name><surname>Kim</surname><given-names>J.W.</given-names></name>
<name><surname>Chung</surname><given-names>H.T.</given-names></name>
<name><surname>Kim</surname><given-names>D.G.</given-names></name>
<name><surname>Paek</surname><given-names>S.H.</given-names></name>
</person-group><article-title>Fractionated stereotactic gamma knife radiosurgery for large brain metastases: A retrospective, single center study</article-title><source>PLoS ONE</source><year>2016</year><volume>11</volume><elocation-id>e0163304</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0163304</pub-id><pub-id pub-id-type="pmid">27661613</pub-id>
</element-citation></ref><ref id="B13-tomography-11-00015"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ewend</surname><given-names>M.G.</given-names></name>
<name><surname>Elbabaa</surname><given-names>S.</given-names></name>
<name><surname>Carey</surname><given-names>L.A.</given-names></name>
</person-group><article-title>Current treatment paradigms for the management of patients with brain metastases</article-title><source>Neurosurgery</source><year>2005</year><volume>57</volume><issue>(Suppl. S5)</issue><fpage>S66</fpage><lpage>S77</lpage><pub-id pub-id-type="doi">10.1227/01.NEU.0000182739.84734.6E</pub-id><pub-id pub-id-type="pmid">16237291</pub-id>
</element-citation></ref><ref id="B14-tomography-11-00015"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cho</surname><given-names>K.R.</given-names></name>
<name><surname>Lee</surname><given-names>M.H.</given-names></name>
<name><surname>Kong</surname><given-names>D.-S.</given-names></name>
<name><surname>Seol</surname><given-names>H.J.</given-names></name>
<name><surname>Nam</surname><given-names>D.-H.</given-names></name>
<name><surname>Sun</surname><given-names>J.-M.</given-names></name>
<name><surname>Ahn</surname><given-names>J.S.</given-names></name>
<name><surname>Ahn</surname><given-names>M.-J.</given-names></name>
<name><surname>Park</surname><given-names>K.</given-names></name>
<name><surname>Kim</surname><given-names>S.T.</given-names></name>
<etal/>
</person-group><article-title>Outcome of gamma knife radiosurgery for metastatic brain tumors derived from non-small cell lung cancer</article-title><source>J. Neuro-Oncol.</source><year>2015</year><volume>125</volume><fpage>331</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1007/s11060-015-1915-x</pub-id></element-citation></ref><ref id="B15-tomography-11-00015"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dosovitskiy</surname><given-names>A.</given-names></name>
</person-group><article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2010.11929</pub-id></element-citation></ref><ref id="B16-tomography-11-00015"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Matsoukas</surname><given-names>C.</given-names></name>
<name><surname>Haslum</surname><given-names>J.F.</given-names></name>
<name><surname>S&#x000f6;derberg</surname><given-names>M.</given-names></name>
<name><surname>Smith</surname><given-names>K.</given-names></name>
</person-group><article-title>Is it Time to Replace CNNs with Transformers for Medical Images?</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2108.09038</pub-id></element-citation></ref><ref id="B17-tomography-11-00015"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Buzea</surname><given-names>C.G.</given-names></name>
<name><surname>Buga</surname><given-names>R.</given-names></name>
<name><surname>Paun</surname><given-names>M.A.</given-names></name>
<name><surname>Albu</surname><given-names>M.</given-names></name>
<name><surname>Iancu</surname><given-names>D.T.</given-names></name>
<name><surname>Dobrovat</surname><given-names>B.</given-names></name>
<name><surname>Agop</surname><given-names>M.</given-names></name>
<name><surname>Paun</surname><given-names>V.P.</given-names></name>
<name><surname>Eva</surname><given-names>L.</given-names></name>
</person-group><article-title>AI Evaluation of Imaging Factors in the Evolution of Stage-Treated Metastases Using Gamma Knife</article-title><source>Diagnostics</source><year>2023</year><volume>13</volume><elocation-id>2853</elocation-id><!--<pub-id pub-id-type="pmcid">PMC10486549</pub-id>--><pub-id pub-id-type="doi">10.3390/diagnostics13172853</pub-id><pub-id pub-id-type="pmid">37685391</pub-id>
</element-citation></ref><ref id="B18-tomography-11-00015"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Brenner</surname><given-names>D.J.</given-names></name>
</person-group><article-title>the linear-quadratic model is an appropriate methodology for determining isoeffective doses at large doses perfraction</article-title><source>Semin. Radiat. Oncol.</source><year>2008</year><volume>18</volume><fpage>234</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1016/j.semradonc.2008.04.004</pub-id><pub-id pub-id-type="pmid">18725109</pub-id>
</element-citation></ref><ref id="B19-tomography-11-00015"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fowler</surname><given-names>J.F.</given-names></name>
</person-group><article-title>The linear-quadratic formula and progress in fractionated radiotherapy</article-title><source>Br. J. Radiol.</source><year>1989</year><volume>62</volume><fpage>679</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1259/0007-1285-62-740-679</pub-id><pub-id pub-id-type="pmid">2670032</pub-id>
</element-citation></ref><ref id="B20-tomography-11-00015"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Higuchi</surname><given-names>Y.</given-names></name>
<name><surname>Serizawa</surname><given-names>T.</given-names></name>
<name><surname>Nagano</surname><given-names>O.</given-names></name>
<name><surname>Matsuda</surname><given-names>S.</given-names></name>
<name><surname>Ono</surname><given-names>J.</given-names></name>
<name><surname>Sato</surname><given-names>M.</given-names></name>
<name><surname>Iwadate</surname><given-names>Y.</given-names></name>
<name><surname>Saeki</surname><given-names>N.</given-names></name>
</person-group><article-title>Three-staged stereotactic radiotherapywithout whole brain irradiation for large metastatic brain tumors</article-title><source>Int. J. Radiat. Oncol. Biol. Phys.</source><year>2009</year><volume>74</volume><fpage>1543</fpage><lpage>1548</lpage><pub-id pub-id-type="doi">10.1016/j.ijrobp.2008.10.035</pub-id><pub-id pub-id-type="pmid">19135317</pub-id>
</element-citation></ref><ref id="B21-tomography-11-00015"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>N.U.</given-names></name>
<name><surname>Lee</surname><given-names>E.Q.</given-names></name>
<name><surname>Aoyama</surname><given-names>H.</given-names></name>
<name><surname>Barani</surname><given-names>I.J.</given-names></name>
<name><surname>Barboriak</surname><given-names>D.P.</given-names></name>
<name><surname>Baumert</surname><given-names>B.G.</given-names></name>
<name><surname>Bendszus</surname><given-names>M.</given-names></name>
<name><surname>Brown</surname><given-names>P.D.</given-names></name>
<name><surname>Camidge</surname><given-names>D.R.</given-names></name>
<name><surname>Chang</surname><given-names>S.M.</given-names></name>
<etal/>
</person-group><article-title>Response assessment criteria for brain metastases: Proposal from the RANO group</article-title><source>Lancet Oncol.</source><year>2015</year><volume>16</volume><fpage>e270</fpage><lpage>e278</lpage><pub-id pub-id-type="doi">10.1016/S1470-2045(15)70057-4</pub-id><pub-id pub-id-type="pmid">26065612</pub-id>
</element-citation></ref><ref id="B22-tomography-11-00015"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ching</surname><given-names>T.</given-names></name>
<name><surname>Himmelstein</surname><given-names>D.S.</given-names></name>
<name><surname>Beaulieu-Jones</surname><given-names>B.K.</given-names></name>
<name><surname>Kalinin</surname><given-names>A.A.</given-names></name>
<name><surname>Do</surname><given-names>B.T.</given-names></name>
<name><surname>Way</surname><given-names>G.P.</given-names></name>
<name><surname>Ferrero</surname><given-names>E.</given-names></name>
<name><surname>Agapow</surname><given-names>P.M.</given-names></name>
<name><surname>Zietz</surname><given-names>M.</given-names></name>
<name><surname>Hoffman</surname><given-names>M.M.</given-names></name>
<etal/>
</person-group><article-title>Opportunities and obstacles for deep learning in biology and medicine</article-title><source>J. R. Soc. Interface</source><year>2018</year><volume>15</volume><fpage>20170387</fpage><!--<pub-id pub-id-type="pmcid">PMC5938574</pub-id>--><pub-id pub-id-type="doi">10.1098/rsif.2017.0387</pub-id><pub-id pub-id-type="pmid">29618526</pub-id>
</element-citation></ref><ref id="B23-tomography-11-00015"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Azad</surname><given-names>R.</given-names></name>
<name><surname>Kazerouni</surname><given-names>A.</given-names></name>
<name><surname>Heidari</surname><given-names>M.</given-names></name>
<name><surname>Aghdam</surname><given-names>E.K.</given-names></name>
<name><surname>Molaei</surname><given-names>A.</given-names></name>
<name><surname>Jia</surname><given-names>Y.</given-names></name>
<name><surname>Jose</surname><given-names>A.</given-names></name>
<name><surname>Roy</surname><given-names>R.</given-names></name>
<name><surname>Merhof</surname><given-names>D.</given-names></name>
</person-group><article-title>Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review</article-title><source>Med. Image Anal.</source><year>2023</year><volume>91</volume><fpage>103000</fpage><pub-id pub-id-type="doi">10.1016/j.media.2023.103000</pub-id><pub-id pub-id-type="pmid">37883822</pub-id>
</element-citation></ref><ref id="B24-tomography-11-00015"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Henry</surname><given-names>E.U.</given-names></name>
<name><surname>Emebo</surname><given-names>O.</given-names></name>
<name><surname>Omonhinmin</surname><given-names>C.A.</given-names></name>
</person-group><article-title>Vision Transformers in Medical Imaging: A Review</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2211.10043</pub-id></element-citation></ref><ref id="B25-tomography-11-00015"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Topol</surname><given-names>E.J.</given-names></name>
</person-group><article-title>High-Performance Medicine: The convergence of human and artificial intelligence</article-title><source>Nat. Med.</source><year>2019</year><volume>25</volume><fpage>44</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/s41591-018-0300-7</pub-id><pub-id pub-id-type="pmid">30617339</pub-id>
</element-citation></ref><ref id="B26-tomography-11-00015"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Litjens</surname><given-names>G.</given-names></name>
<name><surname>Kooi</surname><given-names>T.</given-names></name>
<name><surname>Bejnordi</surname><given-names>B.E.</given-names></name>
<name><surname>Setio</surname><given-names>A.A.A.</given-names></name>
<name><surname>Ciompi</surname><given-names>F.</given-names></name>
<name><surname>Ghafoorian</surname><given-names>M.</given-names></name>
<name><surname>van der Laak</surname><given-names>J.A.W.M.</given-names></name>
<name><surname>van Ginneken</surname><given-names>B.</given-names></name>
<name><surname>S&#x000e1;nchez</surname><given-names>C.I.</given-names></name>
</person-group><article-title>A survey on deep learning in medical image analysis</article-title><source>Med Image Anal.</source><year>2017</year><volume>42</volume><fpage>60</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1016/j.media.2017.07.005</pub-id><pub-id pub-id-type="pmid">28778026</pub-id>
</element-citation></ref><ref id="B27-tomography-11-00015"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Trofin</surname><given-names>A.-M.</given-names></name>
<name><surname>Buzea</surname><given-names>C.G.</given-names></name>
<name><surname>Buga</surname><given-names>R.</given-names></name>
<name><surname>Agop</surname><given-names>M.</given-names></name>
<name><surname>Ochiuz</surname><given-names>L.</given-names></name>
<name><surname>Iancu</surname><given-names>D.T.</given-names></name>
<name><surname>Eva</surname><given-names>L.</given-names></name>
</person-group><article-title>Predicting Tumor Dynamics Post-Staged GKRS: Machine Learning Models in Brain Metastases Prognosis</article-title><source>Diagnostics</source><year>2024</year><volume>14</volume><elocation-id>1268</elocation-id><pub-id pub-id-type="doi">10.3390/diagnostics14121268</pub-id><pub-id pub-id-type="pmid">38928683</pub-id>
</element-citation></ref><ref id="B28-tomography-11-00015"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Buzea</surname><given-names>C.G.</given-names></name>
<name><surname>Mirestean</surname><given-names>C.C.</given-names></name>
<name><surname>Agop</surname><given-names>M.</given-names></name>
<name><surname>Paun</surname><given-names>V.P.</given-names></name>
<name><surname>Iancu</surname><given-names>D.T.</given-names></name>
</person-group><article-title>Classification of good and bad responders in locally advanced rectal cancer after neoadjuvant radio-chemotherapy using radiomics signature</article-title><source>Univ. Politeh. Buchar. Sci. Bull.-Ser. A-Appl. Math. Phys.</source><year>2019</year><volume>81</volume><fpage>265</fpage><lpage>278</lpage></element-citation></ref><ref id="B29-tomography-11-00015"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Komorowski</surname><given-names>P.</given-names></name>
<name><surname>Baniecki</surname><given-names>H.</given-names></name>
<name><surname>Biecek</surname><given-names>P.</given-names></name>
</person-group><article-title>Towards Evaluating Explanations of Vision Transformers for Medical Imaging</article-title><source>Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source><conf-loc>Vancouver, BC, Canada</conf-loc><conf-date>17&#x02013;24 June 2023</conf-date><fpage>3726</fpage><lpage>3732</lpage></element-citation></ref><ref id="B30-tomography-11-00015"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Caruana</surname><given-names>R.</given-names></name>
<name><surname>Niculescu-Mizil</surname><given-names>A.</given-names></name>
</person-group><article-title>An Empirical Comparison of Supervised Learning Algorithms</article-title><source>Proceedings of the 23rd International Conference on Machine Learning (ICML)</source><conf-date>25&#x02013;29 June 2006</conf-date><fpage>161</fpage><lpage>168</lpage></element-citation></ref><ref id="B31-tomography-11-00015"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rajkomar</surname><given-names>A.</given-names></name>
<name><surname>Dean</surname><given-names>J.</given-names></name>
<name><surname>Kohane</surname><given-names>I.</given-names></name>
</person-group><article-title>Machine Learning in Medicine</article-title><source>N. Engl. J. Med.</source><year>2019</year><volume>380</volume><fpage>1347</fpage><lpage>1358</lpage><pub-id pub-id-type="doi">10.1056/NEJMra1814259</pub-id><pub-id pub-id-type="pmid">30943338</pub-id>
</element-citation></ref><ref id="B32-tomography-11-00015"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Reddy</surname><given-names>C.K.K.</given-names></name>
<name><surname>Reddy</surname><given-names>P.A.</given-names></name>
<name><surname>Janapati</surname><given-names>H.</given-names></name>
<name><surname>Assiri</surname><given-names>B.</given-names></name>
<name><surname>Shuaib</surname><given-names>M.</given-names></name>
<name><surname>Alam</surname><given-names>S.</given-names></name>
<name><surname>Sheneamer</surname><given-names>A.</given-names></name>
</person-group><article-title>A fine-tuned vision transformer based enhanced multi-class brain tumor classification using MRI scan imagery</article-title><source>Front. Oncol.</source><year>2024</year><volume>14</volume><elocation-id>1400341</elocation-id><pub-id pub-id-type="doi">10.3389/fonc.2024.1400341</pub-id><pub-id pub-id-type="pmid">39091923</pub-id>
</element-citation></ref><ref id="B33-tomography-11-00015"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Khaniki</surname><given-names>M.A.L.</given-names></name>
<name><surname>Mirzaeibonehkhater</surname><given-names>M.</given-names></name>
<name><surname>Manthouri</surname><given-names>M.</given-names></name>
<name><surname>Hasani</surname><given-names>E.</given-names></name>
</person-group><article-title>Brain Tumor Classification using Vision Transformer with Selective Cross-Attention Mechanism and Feature Calibration</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2406.17670</pub-id></element-citation></ref><ref id="B34-tomography-11-00015"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lyu</surname><given-names>Q.</given-names></name>
<name><surname>Namjoshi</surname><given-names>S.V.</given-names></name>
<name><surname>McTyre</surname><given-names>E.</given-names></name>
<name><surname>Topaloglu</surname><given-names>U.</given-names></name>
<name><surname>Barcus</surname><given-names>R.</given-names></name>
<name><surname>Chan</surname><given-names>M.D.</given-names></name>
<name><surname>Cramer</surname><given-names>C.K.</given-names></name>
<name><surname>Debinski</surname><given-names>W.</given-names></name>
<name><surname>Gurcan</surname><given-names>M.N.</given-names></name>
<name><surname>Lesser</surname><given-names>G.J.</given-names></name>
<etal/>
</person-group><article-title>A transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain MRI images</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2110.03588</pub-id><pub-id pub-id-type="doi">10.1016/j.patter.2022.100613</pub-id><pub-id pub-id-type="pmid">36419451</pub-id>
</element-citation></ref><ref id="B35-tomography-11-00015"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Krishnan</surname><given-names>P.T.</given-names></name>
<name><surname>Krishnadoss</surname><given-names>P.</given-names></name>
<name><surname>Khandelwal</surname><given-names>M.</given-names></name>
<name><surname>Gupta</surname><given-names>D.</given-names></name>
<name><surname>Nihaal</surname><given-names>A.</given-names></name>
<name><surname>Kumar</surname><given-names>T.S.</given-names></name>
</person-group><article-title>Enhancing brain tumor detection in MRI with a rotation invariant Vision Transformer</article-title><source>Front. Neuroinform.</source><year>2024</year><volume>18</volume><elocation-id>1414925</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2024.1414925</pub-id><pub-id pub-id-type="pmid">38957549</pub-id>
</element-citation></ref><ref id="B36-tomography-11-00015"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>K.</given-names></name>
<name><surname>Heckel</surname><given-names>R.</given-names></name>
</person-group><article-title>Vision Transformers Enable Fast and Robust Accelerated MRI., Proceedings of The 5th International Conference on Medical Imaging with Deep Learning</article-title><source>Proc. Mach. Learn. Res.</source><year>2022</year><volume>172</volume><fpage>774</fpage><lpage>795</lpage></element-citation></ref><ref id="B37-tomography-11-00015"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>J.M.</given-names></name>
<name><surname>Khoshgoftaar</surname><given-names>T.M.</given-names></name>
</person-group><article-title>Survey on deep learning with class imbalance</article-title><source>J. Big Data</source><year>2019</year><volume>27</volume><fpage>1</fpage><pub-id pub-id-type="doi">10.1186/s40537-019-0192-5</pub-id></element-citation></ref><ref id="B38-tomography-11-00015"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cao</surname><given-names>K.</given-names></name>
<name><surname>Wei</surname><given-names>C.</given-names></name>
<name><surname>Arechiga</surname><given-names>N.</given-names></name>
<name><surname>Ma</surname><given-names>T.</given-names></name>
</person-group><article-title>Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1906.00741</pub-id></element-citation></ref><ref id="B39-tomography-11-00015"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Yi</surname><given-names>S.</given-names></name>
<name><surname>Yu</surname><given-names>Y.</given-names></name>
<name><surname>Gao</surname><given-names>C.</given-names></name>
<name><surname>Samali</surname><given-names>B.</given-names></name>
</person-group><article-title>Automated ultrasonic-based diagnosis of concrete compressive damage amidst temperature variations utilizing deep learning</article-title><source>Mech. Syst. Signal Process.</source><year>2024</year><volume>221</volume><fpage>111719</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2024.111719</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="tomography-11-00015-f001"><label>Figure 1</label><caption><p>Example of images from the dataset.</p></caption><graphic xlink:href="tomography-11-00015-g001" position="float"/></fig><fig position="float" id="tomography-11-00015-f002"><label>Figure 2</label><caption><p>Workflow for early response prediction of brain metastases using vision transformers.</p></caption><graphic xlink:href="tomography-11-00015-g002" position="float"/></fig><fig position="float" id="tomography-11-00015-f003"><label>Figure 3</label><caption><p>The vision transformer model (ViT).</p></caption><graphic xlink:href="tomography-11-00015-g003" position="float"/></fig><fig position="float" id="tomography-11-00015-f005"><label>Figure 5</label><caption><p>ROC curve for the ViT transformer on the Brain Met database.</p></caption><graphic xlink:href="tomography-11-00015-g005" position="float"/></fig><fig position="float" id="tomography-11-00015-f006"><label>Figure 6</label><caption><p>Confusion matrix for the ViT transformer on the Brain Met database.</p></caption><graphic xlink:href="tomography-11-00015-g006" position="float"/></fig><fig position="float" id="tomography-11-00015-f007"><label>Figure 7</label><caption><p>MRI images with the attention maps superimposed.</p></caption><graphic xlink:href="tomography-11-00015-g007" position="float"/></fig><fig position="float" id="tomography-11-00015-f008"><label>Figure 8</label><caption><p>Specific patient cases (randomly chosen from the database) that demonstrates the model&#x02019;s performance.</p></caption><graphic xlink:href="tomography-11-00015-g008" position="float"/></fig><table-wrap position="float" id="tomography-11-00015-t001"><object-id pub-id-type="pii">tomography-11-00015-t001_Table 1</object-id><label>Table 1</label><caption><p>Classification report.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Support</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Progression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Regression</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">233</td></tr><tr><td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">319</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Macro avg.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">319</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Weighted avg.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.99</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">319</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tomography-11-00015-t002"><object-id pub-id-type="pii">tomography-11-00015-t002_Table 2</object-id><label>Table 2</label><caption><p>Related studies.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Study</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset Size</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Preprocessing</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Clinical Task</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">AUC</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">This study</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vision Transformer (ViT)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3194 images</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Minimal</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Early response prediction in brain metastases (stages GKRS)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99%</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.00</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reddy et al. (2024)<break/>[<xref rid="B32-tomography-11-00015" ref-type="bibr">32</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fine-Tuned Vision Transformer (FTVT-l16)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7023 images</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-class brain tumor classification</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.70%</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not specified</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Labbaf Khaniki et al. (2024)<break/>[<xref rid="B33-tomography-11-00015" ref-type="bibr">33</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vision Transformer with Cross-Attention Mechanism</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3064 images</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brain tumor classification</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.93%</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not specified</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lyu et al. (2021)<break/>[<xref rid="B34-tomography-11-00015" ref-type="bibr">34</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Transformer-Based Deep Learning</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1582 MRI exams</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Classifying brain metastases by primary organ site</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not specified</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.878</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Krishnan et al. (2024)<break/>[<xref rid="B35-tomography-11-00015" ref-type="bibr">35</xref>]</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rotation Invariant Vision Transformer (RViT)</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not specified</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brain tumor detection in MRI</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.6%</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not specified</td></tr></tbody></table></table-wrap></floats-group></article>