<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">EJNMMI Phys</journal-id><journal-id journal-id-type="iso-abbrev">EJNMMI Phys</journal-id><journal-title-group><journal-title>EJNMMI Physics</journal-title></journal-title-group><issn pub-type="epub">2197-7364</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40392417</article-id><article-id pub-id-type="pmc">PMC12092854</article-id>
<article-id pub-id-type="publisher-id">762</article-id><article-id pub-id-type="doi">10.1186/s40658-025-00762-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Research</subject></subj-group></article-categories><title-group><article-title>Shorter SPECT scans using self-supervised coordinate learning to synthesize skipped projection views</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Li</surname><given-names>Zongyu</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3651-9387</contrib-id><name><surname>Jia</surname><given-names>Yixuan</given-names></name><address><email>jiayx@umich.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Xiaojian</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hu</surname><given-names>Jason</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Fessler</surname><given-names>Jeffrey A.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Dewaraja</surname><given-names>Yuni K.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00jmfr291</institution-id><institution-id institution-id-type="GRID">grid.214458.e</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 7347</institution-id><institution>Department of Electrical Engineering and Computer Science, </institution><institution>University of Michigan, </institution></institution-wrap>Ann Arbor, MI 48109-2122 USA </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00jmfr291</institution-id><institution-id institution-id-type="GRID">grid.214458.e</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 7347</institution-id><institution>Department of Radiology, </institution><institution>University of Michigan, </institution></institution-wrap>Ann Arbor, MI USA </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2025</year></pub-date><volume>12</volume><elocation-id>47</elocation-id><history><date date-type="received"><day>10</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>30</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><sec><title>Purpose</title><p id="Par1">This study addresses the challenge of extended SPECT imaging duration under low-count conditions, as encountered in Lu-177 SPECT imaging, by developing a self-supervised learning approach to synthesize skipped SPECT projection views, thus shortening scan times in clinical settings.</p></sec><sec><title>Methods</title><p id="Par2">We developed <bold>SpeRF</bold>, a <bold>SPE</bold>CT reconstruction pipeline that integrates synthesized and measured projections, using a self-supervised coordinate-based learning framework inspired by Neural Radiance Fields (Ne<bold>RF</bold>). For each single scan, SpeRF independently trains a multi-layer perceptron (MLP) to estimate skipped SPECT projection views. SpeRF was tested with various down-sampling factors (DFs&#x02009;=&#x02009;2, 4, 8) on both Lu-177 phantom SPECT/CT measurements and clinical SPECT/CT datasets, from 11 patients undergoing [177Lu]Lu-DOTATATE and 6 patients undergoing [177Lu]Lu-PSMA-617 radiopharmaceutical therapy. Performance was evaluated both in projection space and by comparing reconstructed images using (1) all measured views (&#x0201c;Full&#x0201d;), (2) down-sampled measured views only (&#x0201c;Partial&#x0201d;), and partially measured views combined with skipped views (3) generated by linear interpolation (&#x0201c;LinInt&#x0201d;) and (4) synthesized by our method (&#x0201c;SpeRF&#x0201d;).</p></sec><sec><title>Results</title><p id="Par3">SpeRF projections demonstrated lower Normalized Root Mean Squared Difference (NRMSD) compared to the measured projections, than LinInt projections. Across various DFs, the NRMSD for SpeRF projections averaged 7% vs. 10% in phantom studies, 18% vs. 26% in DOTATATE patient studies, and 20% vs. 21% in PSMA-617 patient studies, compared to LinInt projections. For SPECT reconstructions, DF&#x02009;=&#x02009;4 is recommended as the best trade-off between acquisition time and image quality. At DF&#x02009;=&#x02009;4, in terms of Contrast-to-Noise Ratio relative to Full, SpeRF outperformed LinInt and Partial: (1) DOTATATE: 88% vs. 69% vs. 69% for lesions and 88% vs. 73% vs. 67% for kidney, (2) PSMA-617: 78% vs. 71% vs. 69% for lesions and 78% vs. 57% vs. 67% for organs, including kidneys, lacrimal glands, parotid glands, and submandibular glands. SpeRF slightly underestimated count recovery relative to Full, compared to Partial but still outperformed LinInt: (1) DOTATATE: 98% vs. 100% vs. 88% for lesions and 98% vs. 100% vs. 94% for kidney, (2) PSMA-617: 98% vs. 101% vs. 94% for lesions and 96% vs. 101% vs. 78% for organs.</p></sec><sec><title>Conclusion</title><p id="Par4">The proposed method, SpeRF, shows potential for significant reduction in acquisition time (up to a factor of 4) while maintaining quantitative accuracy in clinical SPECT protocols by allowing for the collection of fewer projections. The self-supervised nature of SpeRF, with data processed independently on each patient&#x02019;s projection data, eliminates the need for extensive training datasets. The reduction in acquisition time is particularly relevant for imaging under low-count conditions and for protocols that require multiple-bed positions such as whole-body imaging.</p></sec><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1186/s40658-025-00762-3.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Self-supervised learning</kwd><kwd>SPECT</kwd><kwd>Sparse projection views</kwd><kwd>Lu-177 imaging</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000070</institution-id><institution>National Institute of Biomedical Imaging and Bioengineering</institution></institution-wrap></funding-source><award-id>R01 EB022075</award-id><principal-award-recipient><name><surname>Dewaraja</surname><given-names>Yuni K.</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000054</institution-id><institution>National Cancer Institute</institution></institution-wrap></funding-source><award-id>R01 CA240706</award-id><principal-award-recipient><name><surname>Dewaraja</surname><given-names>Yuni K.</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Switzerland AG 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par5">SPECT/CT imaging has had many advances [<xref ref-type="bibr" rid="CR1">1</xref>]; however, one continuing limitation is that SPECT acquisition is slow, especially under the low-count conditions encountered when imaging therapy radionuclides, such as Y-90, Ac-225, Ra-223, and Lu-177. These radionuclides are chosen for the therapeutic properties of their alpha and beta emissions, hence do not have ideal properties for gamma-camera imaging. For example, the photon/gamma-ray yield is relatively low, leading to low count conditions. Nevertheless, it is very desirable to perform both therapy and imaging with the same radionuclide, even in very low-count applications.</p><p id="Par6">With Lu-177 where the 208&#x000a0;keV gamma-ray emission probability is only 10%, it can take 15&#x02013;30&#x000a0;min per bed (~&#x02009;40&#x000a0;cm axial) for SPECT on standard gamma-camera systems following radiopharmaceutical therapies (RPTs) such as [177Lu]Lu-DOTATATE and [177Lu]Lu-prostate-specific membrane antigen (PSMA-617) [<xref ref-type="bibr" rid="CR2">2</xref>]. For RPTs involving alpha-emitters, such as [225Ac]Ac-PSMA-I&#x00026;T, acquisition times of up to 1&#x000a0;h have been proposed [<xref ref-type="bibr" rid="CR3">3</xref>]. This is because both the administered activities and the gamma-ray yields are very low. SPECT under low-count conditions is particularly challenging when multiple beds are needed to encompass critical organs and metastases throughout the body. For example, in [177Lu]Lu-PSMA-617 therapy for metastatic castration-resistant prostate cancer (mCRPC), SPECT imaging may require up to 3 bed positions to include all critical organs such as lacrimal glands, salivary glands, bone marrow, and kidneys, as well as lesions that can be found throughout the body [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. Multi-bed position SPECT imaging demands a significant amount of camera time, which can not only lead to patient discomfort, but can also increase motion artifacts. Additionally, in many facilities, camera availability can be limited.</p><p id="Par7">To overcome these challenges, a shorter acquisition time is preferable by taking either fewer projection views or shorter acquisition time per view. These strategies pose additional challenges due to either the missing (skipped) view angles or the increased image noise [<xref ref-type="bibr" rid="CR6">6</xref>]. Numerous algorithms have been proposed with a focus on enhancing the image quality of the reconstructed images from noisy projections [<xref ref-type="bibr" rid="CR7">7</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref>]. As an example, Pan et al. introduced a content-attention image restoration approach to recover high-quality images from low-dose planar bone scans obtained during fast acquisitions [<xref ref-type="bibr" rid="CR15">15</xref>]. In contrast, the approach of synthesizing the missing projections [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR16">16</xref>] has been relatively unexplored. Most prior studies have employed deep learning techniques to learn the relationship between one projection and its neighboring views, often relying on ground truth data for training purposes. For instance, Ryd&#x000e9;n et al. trained a deep convolutional U-Net [<xref ref-type="bibr" rid="CR17">17</xref>] to generate synthetic intermediate projections [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. Meanwhile, Li et al. introduced a network architecture called LU-Net that integrates Long Short-Term Memory network [<xref ref-type="bibr" rid="CR19">19</xref>] and U-Net to understand the transformation from sparse-view projection data to full-view data [<xref ref-type="bibr" rid="CR20">20</xref>]. Chen et al. presented a cross-domain method using SPECT images predicted in the image domain as reference for synthesizing full-view projections in the sinogram domain [<xref ref-type="bibr" rid="CR21">21</xref>]. These approaches are reported to be effective, but they are all based on supervised learning methods that require hundreds of paired data for training. However, in many cases, obtaining enough paired ground truth data for training is challenging. This difficulty is especially true in the case of post-therapy imaging for verifying uptake or dosimetry following RPT because such imaging is typically not part of routine clinical practice in some countries. On the other hand, self-supervised learning, which does not require separate training labels and instead learns from each scan itself, has the potential to overcome the limitations of supervised learning in such scenarios.</p><p id="Par8">The aim of this research was to reduce SPECT acquisition time by reducing the required number of measured projection views while maintaining image quality by incorporating synthetic projections generated by deep neural networks. We implemented a multi-layer perceptron (MLP) and trained it to generate skipped SPECT projection views through self-supervised coordinate learning [<xref ref-type="bibr" rid="CR22">22</xref>]. We evaluated the performance of the proposed method both qualitatively and quantitatively in phantom studies and in patients imaged after [177Lu]Lu-DOTATATE therapy and [177Lu]Lu-PSMA-617 therapy.</p></sec><sec id="Sec2"><title>Materials and methods</title><sec id="Sec3"><title>Phantom study</title><p id="Par9">We used an elliptical phantom with six hot sphere inserts of volumes 2,4,8,16,30,114mL. These &#x02018;hot&#x02019; spheres (having the same Lu-177 activity concentration of 0.22 MBq/mL) were placed in a &#x02018;warm&#x02019; background (0.035 MBq/mL) to achieve a sphere-to-background ratio of 6.3:1, which is representative of tumor-to-background ratios encountered in patient imaging [<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. The total activity in the phantom at scan time was 356 MBq. The sphere volumes of interest (VOIs), corresponding to the physical filling volume, were defined on the CT images.</p></sec><sec id="Sec4"><title>Patient studies</title><p id="Par10">We used retrospective SPECT/CT data from patients who had volunteered for imaging under a University of Michigan Institutional Review Board (IRB) approved protocol. This included 11 patients imaged after [177Lu]Lu-DOTATATE therapy for neuroendocrine tumor (NET) and 6 patients imaged after [177Lu]Lu-PSMA-617 therapy for mCRPC. We defined organs of interest (kidneys for DOTATATE therapy, and kidneys, lacrimal glands, parotid glands, and submandibular glands for [177Lu]Lu-PSMA-617 therapy) using deep learning-based segmentation methods available within MIM Software<sup>&#x000ae;</sup>. A radiologist manually defined the lesions (78 in total, volume ranging from 2 to 250 mL) as described previously [<xref ref-type="bibr" rid="CR24">24</xref>].</p></sec><sec id="Sec5"><title>SPECT/CT acquisition</title><p id="Par11">All scans were acquired on a Siemens Intevo Bold SPECT/CT with a 5/8&#x02019;&#x02019; crystal equipped with medium-energy low penetration (MELP) collimators. Acquisition parameters included 120 views, with 60 views per head, a 20% photopeak window centered at 208&#x000a0;keV, and two adjacent scatter windows of 10% width each. The phantom study used a prolonged acquisition of 196&#x000a0;s/view to achieve a count level which is representative of patient imaging after the administration of [177Lu]Lu-DOTATATE. The patient images were acquired under the standard protocols used in our clinic. [177Lu]Lu-DOTATATE SPECT images were acquired for a single bed position at day 2 or day 4 after the cycle 1 administration of 7.4 GBq using an acquisition time of 25&#x000a0;s per view (total scan time of 25&#x000a0;min). The [177Lu]Lu-PSMA-617 SPECT images were acquired with two bed positions at day 2 or day 3 after the cycle 1 administration of 7.4 GBq with an acquisition time of 17&#x000a0;s per view per bed (total scan time of 34&#x000a0;min). The projection view matrix size was 128&#x02009;&#x000d7;&#x02009;128, with a pixel size of 4.8&#x02009;&#x000d7;&#x02009;4.8&#x000a0;mm. The CT images, with a matrix size of 512&#x02009;&#x000d7;&#x02009;512 and a pixel size of 0.98&#x02009;&#x000d7;&#x02009;0.98&#x000a0;mm, were acquired in low-dose mode (120 kVp; 15&#x02013;80 mAs) under free-breathing conditions. The slice thickness was 3&#x000a0;mm for [177Lu]Lu-DOTATATE patients and 1.5&#x000a0;mm for [177Lu]Lu-PSMA-617 patients.</p></sec><sec id="Sec6"><title>Self-supervised coordinate learning</title><p id="Par12">Given the limited amount of data, we focused on a self-supervised learning approach, namely SpeRF, rather than supervised methods for this study. Our method is inspired by the neural radiance field (NeRF) approach that maps 3D spatial coordinates to radiance values using a neural network [<xref ref-type="bibr" rid="CR25">25</xref>]. Similarly, we developed a coordinate-based MLP with 12 hidden layers and 256 neurons per layer to synthesize skipped projection views in SPECT imaging.</p><sec id="Sec7"><title>Network framework and workflow</title><p id="Par13">The core of our method is a coordinate-based MLP that operates on 5-dimensional input coordinates for each pixel in the SPECT projection views (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). These coordinates include: (1) pixel positions <inline-formula id="IEq1"><alternatives><tex-math id="d33e321">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:(x,\:\:y)$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq1.gif"/></alternatives></inline-formula>, (2) the sine and cosine of the view angle <inline-formula id="IEq2"><alternatives><tex-math id="d33e327">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:(\text{s}\text{i}\text{n}\theta\:,\:\:\text{c}\text{o}\text{s}\theta\:)$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq2.gif"/></alternatives></inline-formula>, and (3) the radial position <inline-formula id="IEq3"><alternatives><tex-math id="d33e333">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:r$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq3.gif"/></alternatives></inline-formula>, which accounts for noncircular orbits. The MLP processes each 5D coordinate independently, predicting a single scalar projection count for that pixel. While the input coordinate set for all projection views are conceptually of size <inline-formula id="IEq4"><alternatives><tex-math id="d33e339">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{n}_{x}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq4.gif"/></alternatives></inline-formula>&#x000d7;<inline-formula id="IEq5"><alternatives><tex-math id="d33e346">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{n}_{y}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq5.gif"/></alternatives></inline-formula>&#x000d7;<inline-formula id="IEq6"><alternatives><tex-math id="d33e352">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{n}_{\theta\:}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq6.gif"/></alternatives></inline-formula>&#x000d7;5, the network operates efficiently by iterating over individual 5D inputs, producing outputs of size <inline-formula id="IEq7"><alternatives><tex-math id="d33e358">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{n}_{x}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq4.gif"/></alternatives></inline-formula>&#x000d7;<inline-formula id="IEq8"><alternatives><tex-math id="d33e364">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{n}_{y}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq5.gif"/></alternatives></inline-formula>&#x000d7;<inline-formula id="IEq9"><alternatives><tex-math id="d33e370">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{n}_{\theta\:}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq6.gif"/></alternatives></inline-formula>&#x000d7;1. Here, <inline-formula id="IEq10"><alternatives><tex-math id="d33e376">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{n}_{x}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq4.gif"/></alternatives></inline-formula> and<inline-formula id="IEq11"><alternatives><tex-math id="d33e383">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\:{n}_{y}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq11.gif"/></alternatives></inline-formula> represent the projection matrix dimensions, and <inline-formula id="IEq12"><alternatives><tex-math id="d33e389">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{n}_{\theta\:}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq6.gif"/></alternatives></inline-formula> denotes number of projection view angles, which correspond to the measured angles during training and the skipped angles during inference.</p><p id="Par27">
<fig id="Fig1"><label>Fig. 1</label><caption><p>Overview of SpeRF pipeline. The top panel (light purple) illustrates the training phase, where a coordinate-based MLP processes training coordinate sets consisting of <inline-formula id="IEq13"><alternatives><tex-math id="d33e403">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:x,\:y$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq13.gif"/></alternatives></inline-formula> coordinates, radial position <inline-formula id="IEq14"><alternatives><tex-math id="d33e409">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:r$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq3.gif"/></alternatives></inline-formula>, and trigonometric features <inline-formula id="IEq15"><alternatives><tex-math id="d33e415">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:sin\theta\:\:$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq15.gif"/></alternatives></inline-formula> and <inline-formula id="IEq16"><alternatives><tex-math id="d33e421">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:cos\theta\:$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq16.gif"/></alternatives></inline-formula> derived from the view angle <inline-formula id="IEq17"><alternatives><tex-math id="d33e427">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\theta\:$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq17.gif"/></alternatives></inline-formula>. The MLP predicts projection counts at the corresponding coordinates, which are compared to true projection counts using the Huber Loss function for backpropagation. The bottom panel (blue) shows the inference phase, where the trained MLP processes query coordinate sets corresponding to skipped projection angles. This allows the model to interpolate or predict projection counts at unseen angles that were not part of the training data. SpeRF is a patient-specific method, where a separate MLP is trained and used for each patient&#x02019;s dataset to account for individual imaging characteristics and variations. The same MLP architecture is used in both phases, but training is performed on known projections, while inference generates skipped projections</p></caption><graphic xlink:href="40658_2025_762_Fig1_HTML" id="d33e434"/></fig>
</p><p id="Par14">During training, the MLP learns to map the input training coordinate sets to the measured projection counts. To ensure patient-specific learning, separate MLPs are trained for each patient using their available measured projections. Additionally, for each patient, we train two independent MLPs: one for the main acquisition window and the other for the sum of the two scatter windows.</p><p id="Par15">In the inference phase, the trained MLP is provided with the query coordinate sets corresponding to the skipped projection angles. The network predicts the projection counts for each pixel at these missing angles, enabling the synthesis of skipped projections. This coordinate-based design provides the flexibility to accommodate various down-sampling factors (DFs). For example, with 30 measured views and 90 synthesized views, SpeRF achieves a 75% reduction in scan time (DF&#x02009;=&#x02009;4) while maintaining the same network architecture and hyperparameters across different DFs.</p></sec><sec id="Sec8"><title>Rescale trick</title><p id="Par16">To enhance the representation of the continuous measurement field, we applied the &#x02018;rescale trick&#x02019;, i.e., upscaling the original 128&#x02009;&#x000d7;&#x02009;128 projection images to 256&#x02009;&#x000d7;&#x02009;256 using nearest neighbor interpolation during the input stage. During training, the measured projection counts at 256&#x02009;&#x000d7;&#x02009;256 resolution serve as the target. After inference, the synthesized 256&#x02009;&#x000d7;&#x02009;256 projections are downscaled back to 128&#x02009;&#x000d7;&#x02009;128 using the same nearest neighbor interpolation method to align with the original resolution for reconstruction. Nearest neighbor interpolation duplicates the closest pixel value into the upscaled grid, preserving the original pixel intensities. After inference, the results are downscaled back to 128&#x02009;&#x000d7;&#x02009;128 by selecting (adhered to a pre-defined rule) one pixel per 2&#x02009;&#x000d7;&#x02009;2 block. This strategy maintains the pixel intensity distribution while allowing the network to operate on a finer spatial grid. Although no new information is introduced, the higher resolution helps the network approximate a smoother and more continuous measurement field, capturing essential sharp edges and variations of projection images.</p></sec></sec><sec id="Sec9"><title>Training and optimization</title><p id="Par17">For each scan, we optimized the MLP weights by minimizing the Huber loss function, defined as<disp-formula id="Equa"><alternatives><tex-math id="d33e448">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{L}_{\delta\:}\left(a\right)=\left\{\begin{array}{c}\frac{1}{2}{a}^{2}\\\:\delta\:\bullet\:(\left|a\right|-\frac{1}{2}\delta\:)\end{array}\:\right.\begin{array}{c}\:\text{\:for\:}\left|a\right|&#x0003c;\delta\:\\\:\text{\:otherwise}\end{array}$$\end{document}</tex-math><graphic xlink:href="40658_2025_762_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par18">Here, <inline-formula id="IEq18"><alternatives><tex-math id="d33e455">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:a$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq18.gif"/></alternatives></inline-formula> represents the difference between the MLP-predicted projection count and the measured projection count at a corresponding coordinate, and <inline-formula id="IEq19"><alternatives><tex-math id="d33e461">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\delta\:$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq19.gif"/></alternatives></inline-formula> is a hyperparameter that controls the transition between the squared loss and the absolute loss. The Huber loss is particularly suited for this task because it is robust to outliers, which are common in noisy measurements. It combines aspects of two common loss functions: for smaller errors (<inline-formula id="IEq20"><alternatives><tex-math id="d33e467">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\left|a\right|&#x0003c;\delta\:$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq20.gif"/></alternatives></inline-formula>), it behaves like Mean Squared Error (MSE) providing strong learning signals due to its differentiability; for larger discrepancies (<inline-formula id="IEq21"><alternatives><tex-math id="d33e473">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\left|a\right|\ge\:\delta\:$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq21.gif"/></alternatives></inline-formula>), it behaves like Mean Absolute Error (MAE), making it less sensitive to outliers, as demonstrated in [<xref ref-type="bibr" rid="CR26">26</xref>]. We empirically set <inline-formula id="IEq22"><alternatives><tex-math id="d33e483">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\delta\:=1$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq22.gif"/></alternatives></inline-formula> in our implementation.</p><p id="Par19">To optimize the MLP, we minimized the Huber loss function using the Adam optimizer [<xref ref-type="bibr" rid="CR27">27</xref>] with an initial learning rate of 0.001. A reduce-on-plateau scheduler was applied to dynamically lower the learning rate when the validation loss plateaued. For each scan, we randomly selected 20% of the pixel coordinates and their corresponding projection counts from the available measured projection views as validation data. The final patient-specific model was selected based on the lowest validation loss over 200 training epochs. We used a batch size of 10,000 coordinates sampled from the <inline-formula id="IEq23"><alternatives><tex-math id="d33e494">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:256\times\:256\times\:{n}_{\text{bed}}\times\:{n}_{\theta\:}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq23.gif"/></alternatives></inline-formula> input space. During inference, skipped projections were synthesized in approximately 40&#x000a0;s per view on a system equipped with a single RTX 4090 GPU, 64 GB of DDR5 memory, and a 24-core Intel i9-13900KF CPU. The implementation of our method, including training and testing virtual patient phantom images, is publicly available in PyTorch at: <ext-link ext-link-type="uri" xlink:href="https://github.com/ZongyuLi-umich/">https://github.com/ZongyuLi-umich/</ext-link>.</p></sec><sec id="Sec10"><title>SPECT reconstruction</title><p id="Par20">The ordered-subset expectation-maximization (OS-EM) algorithm [<xref ref-type="bibr" rid="CR28">28</xref>] is commonly used to reconstruct 3D SPECT images. In this study we performed OS-EM SPECT reconstructions ([177Lu]Lu-DOTATATE matrix size: 128&#x02009;&#x000d7;&#x02009;128&#x02009;&#x000d7;&#x02009;81 and 2-bed [177Lu]Lu-PSMA-617 matrix size: 128&#x02009;&#x000d7;&#x02009;128&#x02009;&#x000d7;&#x02009;158, both with voxel size in mm: 4.8&#x02009;&#x000d7;&#x02009;4.8&#x02009;&#x000d7;&#x02009;4.8) with 6 subsets and 16 iterations using an in-house open-sourced toolbox, benchmarked on CPU with multi-threading and verified by Monte Carlo simulation [<xref ref-type="bibr" rid="CR29">29</xref>]. No post-processing filter was applied. Scatter correction was applied using a triple energy window method, and attenuation correction was based on the standard CT-to-density calibration curve. The point spread function for depth-dependent collimator-detector response modeling was simulated with Monte Carlo [<xref ref-type="bibr" rid="CR30">30</xref>] using a point source in air and fitted with Gaussian curves.</p></sec><sec id="Sec11"><title>Evaluation</title><p id="Par21">SPECT image quality was evaluated for four distinct data processing pipelines: (1) &#x0201c;Full&#x0201d;: OS-EM reconstruction using all 120 measured projections. (2) &#x0201c;Partial&#x0201d;: OS-EM reconstruction using a certain DF of the measured projections. (3) &#x0201c;LinInt&#x0201d;: a certain DF of projections were measured, and the remaining projections were generated through linear interpolation, followed by OS-EM. (4) &#x0201c;SpeRF&#x0201d;: a certain DF of projections were measured, and the remaining were MLP-predicted synthetic projections, followed by OS-EM.</p><p id="Par22">Our evaluation was structured into three aspects: Synthesized Projections, Phantom Reconstructions, and Patient Reconstructions, each with different metrics. When comparing synthesized projections (from SpeRF and LinInt) against measured projections for both phantom and patient studies, we used the Normalized Root Mean Squared Difference (NRMSD) defined as:<disp-formula id="Equb"><alternatives><tex-math id="d33e523">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{N}\text{R}\text{M}\text{S}\text{D}=\:\frac{\sqrt{\frac{1}{{n}_{p}}\sum\:_{j=1}^{{n}_{p}}{({\widehat{x}}_{j}-{x}_{j})}^{2}}}{\sqrt{\frac{1}{{n}_{p}}\sum\:_{j=1}^{{n}_{p}}{x}_{j}^{2}}},$$\end{document}</tex-math><graphic xlink:href="40658_2025_762_Article_Equb.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par23">where <inline-formula id="IEq24"><alternatives><tex-math id="d33e530">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{n}_{p}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq24.gif"/></alternatives></inline-formula> is the total number of voxels within the VOI, including lesions and relevant organs. Subscript <inline-formula id="IEq25"><alternatives><tex-math id="d33e536">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:j$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq25.gif"/></alternatives></inline-formula>, i.e., <inline-formula id="IEq26"><alternatives><tex-math id="d33e542">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{x}_{j}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq26.gif"/></alternatives></inline-formula>, denotes the <inline-formula id="IEq27"><alternatives><tex-math id="d33e548">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:j$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq25.gif"/></alternatives></inline-formula>th voxel in the image. The reference image and the reconstructed image are denoted by <inline-formula id="IEq28"><alternatives><tex-math id="d33e554">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:x$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq28.gif"/></alternatives></inline-formula> and <inline-formula id="IEq29"><alternatives><tex-math id="d33e561">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\widehat{x}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq29.gif"/></alternatives></inline-formula>, respectively.</p><p id="Par24">When evaluating reconstructions in phantom studies, we calculated the noise level and the Activity Recovery (AR) to assess how well the reconstruction matched the true activity map. The background (BKG) was defined as the union of six uniform &#x0201c;warm&#x0201d; regions, ensuring no overlap with the hot sphere VOIs. The noise level was computed as the standard deviation of voxel activity within this BKG, denoted as <inline-formula id="IEq30"><alternatives><tex-math id="d33e569">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\text{S}\text{T}\text{D}}_{\text{B}\text{K}\text{G}}$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq30.gif"/></alternatives></inline-formula>. The AR is defined as:<disp-formula id="Equc"><alternatives><tex-math id="d33e575">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{A}\text{R}=\:\frac{{\text{m}\text{e}\text{a}\text{n}(\text{r}\text{e}\text{c}\text{o}\text{n}\_\text{a}\text{c}\text{t}\text{i}\text{v}\text{i}\text{t}\text{y}}_{\text{V}\text{O}\text{I}})}{{\text{m}\text{e}\text{a}\text{n}(\text{t}\text{r}\text{u}\text{e}\_\text{a}\text{c}\text{t}\text{i}\text{v}\text{i}\text{t}\text{y}}_{\text{V}\text{O}\text{I}})}.$$\end{document}</tex-math><graphic xlink:href="40658_2025_762_Article_Equc.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par25">When evaluating reconstructions in patient studies, we defined relative measures, including Relative Count Recovery (RCR) and Relative Contrast-to-Noise Ratio (RCNR), in comparison to the Full recon. Here, the BKG was chosen as a homogeneous region within the lung. The RCNR and RCR are defined as:<disp-formula id="Equd"><alternatives><tex-math id="d33e582">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{C}\text{N}\text{R}=\frac{{\text{m}\text{e}\text{a}\text{n}(\text{r}\text{e}\text{c}\text{o}\text{n}\_\text{c}\text{o}\text{u}\text{n}\text{t}}_{\text{V}\text{O}\text{I}})-{\text{m}\text{e}\text{a}\text{n}(\text{r}\text{e}\text{c}\text{o}\text{n}\_\text{c}\text{o}\text{u}\text{n}\text{t}}_{\text{B}\text{K}\text{G}})}{{\text{S}\text{T}\text{D}}_{\text{B}\text{K}\text{G}}}$$\end{document}</tex-math><graphic xlink:href="40658_2025_762_Article_Equd.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Eque"><alternatives><tex-math id="d33e587">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{R}\text{C}\text{N}\text{R}=\frac{{\text{C}\text{N}\text{R}}_{\text{s}\text{p}\text{a}\text{r}\text{s}\text{e}\_\text{v}\text{i}\text{e}\text{w}\:\text{r}\text{e}\text{c}\text{o}\text{n}}}{{\text{C}\text{N}\text{R}}_{\text{F}\text{u}\text{l}\text{l}\_\text{r}\text{e}\text{c}\text{o}\text{n}}\:}\times\:100\%$$\end{document}</tex-math><graphic xlink:href="40658_2025_762_Article_Eque.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equf"><alternatives><tex-math id="d33e592">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\:\:\text{R}\text{C}\text{R}=\frac{{\text{m}\text{e}\text{a}\text{n}(\text{s}\text{p}\text{a}\text{r}\text{s}\text{e}\_\text{v}\text{i}\text{e}\text{w}\_\text{r}\text{e}\text{c}\text{o}\text{n}\_\text{c}\text{o}\text{u}\text{n}\text{t}}_{\text{V}\text{O}\text{I}})}{{\text{m}\text{e}\text{a}\text{n}(\text{F}\text{u}\text{l}\text{l}\_\text{r}\text{e}\text{c}\text{o}\text{n}\_\text{c}\text{o}\text{u}\text{n}\text{t}}_{\text{V}\text{O}\text{I}})}\times\:100\%,\:\:$$\end{document}</tex-math><graphic xlink:href="40658_2025_762_Article_Equf.gif" position="anchor"/></alternatives></disp-formula></p></sec></sec><sec id="Sec12"><title>Results</title><sec id="Sec13"><title>Synthesized projections</title><p id="Par26">Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> compares the performance of linearly interpolated projections against SpeRF projections, summarizing the NRMSD values across various DFs for phantom studies and patient studies. The results consistently demonstrate that the SpeRF projections outperform LinInt projections, exhibiting lower NRMSD values in both phantom and patient studies.</p><p id="Par28">Visually, SpeRF projections appear smoother than their measured counterparts. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> displays the measured (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>a) and synthesized projections (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>b and c) for a representative [177Lu]Lu-PSMA-617 patient. Close examination of the intensity profiles across the lacrimals reveals notable differences: the SpeRF projection exhibits two peaks (corresponding to high uptake in left and right lacrimals as expected with [177Lu]Lu-PSMA-617), more closely aligning with the pattern observed in the measured projection, while the LinInt projection presents four peaks due to angular interpolation.</p><p id="Par29">
<table-wrap id="Tab1"><label>Table 1</label><caption><p>NRMSD comparisons between sperf projections and LinInt projections, relative to measured projections, across different down-sampling factors (DFs) for Phantom study and patient studies (values are average across 11 [177Lu]Lu-DOTATATE studies and 6 [177Lu]Lu-PSMA-617 studies)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="3"/><th align="left" rowspan="2" colspan="2">Phantom Study</th><th align="left" colspan="4">Patient Study</th></tr><tr><th align="left" colspan="2">DOTATATE</th><th align="left" colspan="2">PSMA-617</th></tr><tr><th align="left">SpeRF</th><th align="left">LinInt</th><th align="left">SpeRF</th><th align="left">LinInt</th><th align="left">SpeRF</th><th align="left">LinInt</th></tr></thead><tbody><tr><td align="left">
<bold>DF&#x02009;=&#x02009;2</bold>
</td><td char="." align="char">
<bold>5.9%</bold>
</td><td char="." align="char">9.0%</td><td char="." align="char">
<bold>16.9%</bold>
</td><td char="." align="char">23.4%</td><td char="." align="char">
<bold>17.5%</bold>
</td><td char="." align="char">24.6%</td></tr><tr><td align="left">
<bold>DF&#x02009;=&#x02009;4</bold>
</td><td char="." align="char">
<bold>6.2%</bold>
</td><td char="." align="char">9.5%</td><td char="." align="char">
<bold>17.5%</bold>
</td><td char="." align="char">25.5%</td><td char="." align="char">
<bold>18.4%</bold>
</td><td char="." align="char">27.4%</td></tr><tr><td align="left">
<bold>DF&#x02009;=&#x02009;8</bold>
</td><td char="." align="char">
<bold>7.5%</bold>
</td><td char="." align="char">11.1%</td><td char="." align="char">
<bold>18.8%</bold>
</td><td char="." align="char">30.4%</td><td char="." align="char">
<bold>23.7%</bold>
</td><td char="." align="char">34.1%</td></tr></tbody></table></table-wrap>
</p><p id="Par30">
<fig id="Fig2"><label>Fig. 2</label><caption><p>Comparison of measured and synthesized projections for a patient after [177Lu]Lu-PSMA-617 therapy. Fig. (<bold>a</bold>), (<bold>b</bold>), and (<bold>c</bold>) show measured projection, LinInt projection (generated through linear interpolation), and SpeRF projection, respectively. The images and profile comparison across lacrimal glands show two hot spots/peaks in the SpeRF projection (green line) corresponding to left and right lacrimals, closely resembling the profile of the measured projection (red line), whereas the corresponding LinInt projection profile shows 4 peaks due to distortions caused by angular interpolation</p></caption><graphic xlink:href="40658_2025_762_Fig2_HTML" id="d33e770"/></fig>
</p></sec><sec id="Sec14"><title>Phantom reconstruction results</title><p id="Par31">Consider the DF&#x02009;=&#x02009;4 scenario as an illustrative case. Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> compares four data processing pipelines (Full, SpeRF, LinInt, Partial) with the true activity map. Although each pipeline exhibits structural similarities with the true activity, the Partial recon is noticeably noisier than its counterparts. Quantitative comparisons, presented in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, plot noise to mean activity recovery (average across all six hot spheres) curves at DF&#x02009;=&#x02009;2, 4 and 8, where the SpeRF recon outperforms both the Partial recon and LinInt recon by most closely paralleling the Full recon through OS-EM iterations. Note that even for the Full recon, AR is degraded (AR&#x02009;&#x0003c;&#x02009;1) due to the partial volume effects [<xref ref-type="bibr" rid="CR31">31</xref>]. Supplementary Fig.&#x000a0;1 provides individual noise to activity recovery curves for each hot sphere.</p><p id="Par32">Moreover, the noise level in all sparse-view reconstructions increases as the DF becomes larger. But the SpeRF recon consistently achieves highest activity recoveries for all six lesions at the same noise level. When DF&#x02009;=&#x02009;8, as evident in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>(c), the Partial recon attained higher activity recovery for small lesions, at the expense of substantially increased noise level, while the SpeRF recon remains superior for larger lesions. For all sizes of lesions and DFs, the SpeRF recon matched the activity recovery of the LinInt recon while maintaining a significantly lower noise level.</p><p id="Par33">
<fig id="Fig3"><label>Fig. 3</label><caption><p>Visual comparison of Full recon, SpeRF recon, LinInt recon and Partial recon, against phantom true activity, for DF&#x02009;=&#x02009;4. All reconstructed images and true activity maps are in the same color scale. Error maps present pixel value differences between reconstructed images and true activity</p></caption><graphic xlink:href="40658_2025_762_Fig3_HTML" id="d33e798"/></fig>
</p><p id="Par34">
<fig id="Fig4"><label>Fig. 4</label><caption><p>Noise to mean activity recovery (AR) curves averaged across six sphere volumes for DFs&#x02009;=&#x02009;2, 4, and 8 are shown in subplots (<bold>a</bold>), (<bold>b</bold>), and (<bold>c</bold>), respectively. When DF&#x02009;=&#x02009;2 and 4, SpeRF recon consistently outperforms both LinInt and Partial recon, achieving noise-to-AR performance that most closely aligns with Full recon across OS-EM iterations. When DF&#x02009;=&#x02009;8, Partial recon exhibits significantly increased noise level, while both SpeRF and LinInt suffer from reduced sphere AR</p></caption><graphic xlink:href="40658_2025_762_Fig4_HTML" id="d33e818"/></fig>
</p></sec><sec id="Sec15"><title>Patient reconstruction results</title><p id="Par35">Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows the coronal Maximum Intensity Projections (MIPs) of an example patient image after [177Lu]Lu-DOTATATE (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> left) and [177Lu]Lu-PSMA-617 (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> right) therapy, respectively, derived from four different data processing pipelines at various DFs. In both the [177Lu]Lu-DOTATATE and [177Lu]Lu-PSMA-617 studies, the LinInt recons exhibit noticeable artifacts due to distortions caused by angular interpolation, more pronounced at higher DFs. This effect is particularly evident in the [177Lu]Lu-PSMA-617 study for organs like the lacrimal, parotid, and submandibular glands at DF&#x02009;=&#x02009;4 and 8, substantially affecting the structural clarity of the SPECT images. Partial recons became noisier with increasing DFs, making it challenging to distinguish small hot spots from the background. However, SpeRF recon maintains an image quality closer to Full recon.</p><p id="Par36">The RCR and RCNR results are presented in Tables&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref> and visualized in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>. For RCR, Partial recon consistently achieves the highest values, closely matching the Full recon (100%). SpeRF recon slightly underestimates RCR compared to Partial but remains close to Full recon and consistently outperforms LinInt recon. For example, in the [177Lu]Lu-DOTATATE study at DF&#x02009;=&#x02009;4 (Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>), SpeRF recon achieves RCR values of ~&#x02009;98.3% for lesions, compared to ~&#x02009;100.3% for Partial recon and ~&#x02009;87.5% for LinInt recon. Similarly, in the [177Lu]Lu-PSMA-617 study at DF&#x02009;=&#x02009;4 (Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>), SpeRF recon achieves&#x02009;~&#x02009;98.4% RCR for lesions, slightly lower than Partial recon (~&#x02009;100.5%) but significantly better than LinInt recon (~&#x02009;93.7%). For RCNR, SpeRF recon demonstrates a consistent advantage across all DFs, particularly at DF&#x02009;=&#x02009;4, where it achieves the best balance of RCR and RCNR. In the [177Lu]Lu-DOTATATE study at DF&#x02009;=&#x02009;4 (Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>), SpeRF recon achieves&#x02009;~&#x02009;87.9% RCNR for lesions, outperforming both LinInt recon (~&#x02009;68.7%) and Partial recon (~&#x02009;68.7%). A similar trend is observed in the [177Lu]Lu-PSMA-617 study (Table&#x000a0;<xref rid="Tab5" ref-type="table">5</xref>), where SpeRF recon achieves&#x02009;~&#x02009;78.4% RCNR for lesions at DF&#x02009;=&#x02009;4, compared to ~&#x02009;70.7% for LinInt recon and ~&#x02009;68.5% for Partial recon. At higher DFs, such as DF&#x02009;=&#x02009;8, SpeRF recon continues to outperform the other methods in RCNR, although the RCR decreases more significantly. These results indicate that SpeRF recon maintains high activity recovery while providing additional advantages in RCNR.</p><p id="Par37">Overall, DF&#x02009;=&#x02009;4 provides the best trade-off for SpeRF recon, offering high RCR (within ~&#x02009;2-3% of Full recon) and the highest RCNR across all VOIs. While Partial recon achieves slightly higher RCR values, SpeRF recon delivers superior RCNR, particularly in small or challenging regions such as the lacrimal glands (~&#x02009;0.4 mL), where LinInt recon often fails. These results highlight the potential of SpeRF recon as a robust method for reducing scan time while preserving image quality and clinical usability.</p><p id="Par38">
<fig id="Fig5"><label>Fig. 5</label><caption><p>Coronal Maximum Intensity Projections of SPECT reconstructions for [177Lu]Lu-DOTATATE (left panel) and [177Lu]Lu-PSMA-617 (right panel) patient studies are presented side by side, using four data processing pipelines (columns) and three DFs (rows). Colored boxes highlighted regions with apparent distinctions, which are zoomed in for closer inspection. Gamma correction, a non-linear adjustment of displayed pixel intensities, is applied to all images to enhance contrast and emphasize reconstruction artifacts. In both panels, blurring artifacts in the LinInt recon and noise in the Partial recon become increasingly prominent, especially obvious at higher DFs. SpeRF recon, however, maintains image quality closer to Full recon</p></caption><graphic xlink:href="40658_2025_762_Fig5_HTML" id="d33e873"/></fig>
</p><p id="Par39">
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Average relative count recovery (RCR) values of the SpeRF recon, the LinInt recon, and the Partial recon across all eleven [177Lu]Lu-DOTATATE patient studies, benchmarked against the Full recon, whose RCR is standardized at 100%</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="3">DF&#x02009;=&#x02009;2</th><th align="left" colspan="3">DF&#x02009;=&#x02009;4</th><th align="left" colspan="3">DF&#x02009;=&#x02009;8</th></tr><tr><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th></tr></thead><tbody><tr><td align="left">
<bold>Lesion</bold>
</td><td char="." align="char">100.6%</td><td char="." align="char">97.7%</td><td char="." align="char">
<bold>100.1%</bold>
</td><td char="." align="char">98.3%</td><td char="." align="char">87.5%</td><td char="." align="char">
<bold>100.3%</bold>
</td><td char="." align="char">90.3%</td><td char="." align="char">71.6%</td><td char="." align="char">
<bold>98.8%</bold>
</td></tr><tr><td align="left">
<bold>Kidney</bold>
</td><td char="." align="char">103.9%</td><td char="." align="char">100.5%</td><td char="." align="char">
<bold>100.8%</bold>
</td><td char="." align="char">97.7%</td><td char="." align="char">94.2%</td><td char="." align="char">
<bold>99.5%</bold>
</td><td char="." align="char">93.0%</td><td char="." align="char">83.7%</td><td char="." align="char">
<bold>99.0%</bold>
</td></tr></tbody></table></table-wrap>
</p><p id="Par40">
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Average RCR values of the SpeRF recon, the LinInt recon, and the Partial recon across all six [177Lu]Lu-PSMA-617 patient studies, benchmarked against the Full recon, whose RCR is standardized at 100%</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="3">DF&#x02009;=&#x02009;2</th><th align="left" colspan="3">DF&#x02009;=&#x02009;4</th><th align="left" colspan="3">DF&#x02009;=&#x02009;8</th></tr><tr><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th></tr></thead><tbody><tr><td align="left">Lesion</td><td char="." align="char">101.1%</td><td char="." align="char">
<bold>99.9%</bold>
</td><td char="." align="char">100.9%</td><td char="." align="char">98.4%</td><td char="." align="char">93.7%</td><td char="." align="char">
<bold>100.5%</bold>
</td><td char="." align="char">95.6%</td><td char="." align="char">92.2%</td><td char="." align="char">
<bold>104.5%</bold>
</td></tr><tr><td align="left">All Organ ROIs</td><td char="." align="char">
<bold>99.9%</bold>
</td><td char="." align="char">93.8%</td><td char="." align="char">
<bold>99.9%</bold>
</td><td char="." align="char">96.1%</td><td char="." align="char">78.1%</td><td char="." align="char">
<bold>101.1%</bold>
</td><td char="." align="char">87.3%</td><td char="." align="char">58.8%</td><td char="." align="char">
<bold>97.8%</bold>
</td></tr><tr><td align="left">Kidney</td><td char="." align="char">100.5%</td><td char="." align="char">99.1%</td><td char="." align="char">
<bold>99.6%</bold>
</td><td char="." align="char">
<bold>98.3%</bold>
</td><td char="." align="char">95.9%</td><td char="." align="char">102.1%</td><td char="." align="char">92.4%</td><td char="." align="char">83.0%</td><td char="." align="char">
<bold>98.1%</bold>
</td></tr><tr><td align="left">Lacrimal</td><td char="." align="char">99.4%</td><td char="." align="char">79.6%</td><td char="." align="char">
<bold>99.8%</bold>
</td><td char="." align="char">95.5%</td><td char="." align="char">41.8%</td><td char="." align="char">
<bold>103.5%</bold>
</td><td char="." align="char">79.6%</td><td char="." align="char">21.4%</td><td char="." align="char">
<bold>94.9%</bold>
</td></tr><tr><td align="left">Parotid</td><td char="." align="char">
<bold>100.2%</bold>
</td><td char="." align="char">98.4%</td><td char="." align="char">
<bold>100.2%</bold>
</td><td char="." align="char">97.1%</td><td char="." align="char">87.5%</td><td char="." align="char">
<bold>99.7%</bold>
</td><td char="." align="char">93.9%</td><td char="." align="char">65.6%</td><td char="." align="char">
<bold>100.0%</bold>
</td></tr><tr><td align="left">Submandibular</td><td char="." align="char">99.6%</td><td char="." align="char">98.1%</td><td char="." align="char">
<bold>100.2%</bold>
</td><td char="." align="char">93.6%</td><td char="." align="char">87.3%</td><td char="." align="char">
<bold>99.1%</bold>
</td><td char="." align="char">83.3%</td><td char="." align="char">65.4%</td><td char="." align="char">
<bold>98.2%</bold>
</td></tr></tbody></table></table-wrap>
</p><p id="Par41">
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Average relative contrast-to-noise ratio (RCNR) values of the SpeRF recon, the LinInt recon, and the Partial recon across all eleven [177Lu]Lu-DOTATATE patient studies, benchmarked against the Full recon, whose RCNR is standardized at 100%</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="3">DF&#x02009;=&#x02009;2</th><th align="left" colspan="3">DF&#x02009;=&#x02009;4</th><th align="left" colspan="3">DF&#x02009;=&#x02009;8</th></tr><tr><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th></tr></thead><tbody><tr><td align="left">
<bold>Lesion</bold>
</td><td char="." align="char">
<bold>88.6%</bold>
</td><td char="." align="char">82.5%</td><td char="." align="char">82.7%</td><td char="." align="char">
<bold>87.9%</bold>
</td><td char="." align="char">68.7%</td><td char="." align="char">68.7%</td><td char="." align="char">
<bold>73.5%</bold>
</td><td char="." align="char">43.9%</td><td char="." align="char">48.2%</td></tr><tr><td align="left">
<bold>Kidney</bold>
</td><td char="." align="char">
<bold>92.6%</bold>
</td><td char="." align="char">85.8%</td><td char="." align="char">84.5%</td><td char="." align="char">
<bold>88.0%</bold>
</td><td char="." align="char">73.1%</td><td char="." align="char">67.0%</td><td char="." align="char">
<bold>76.5%</bold>
</td><td char="." align="char">51.3%</td><td char="." align="char">48.8%</td></tr></tbody></table></table-wrap>
</p><p id="Par42">
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Average RCNR values of the SpeRF recon, the LinInt recon, and the Partial recon across all six [177Lu]Lu-PSMA-617 patient studies, benchmarked against the Full recon, whose RCNR is standardized at 100%</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="3">DF&#x02009;=&#x02009;2</th><th align="left" colspan="3">DF&#x02009;=&#x02009;4</th><th align="left" colspan="3">DF&#x02009;=&#x02009;8</th></tr><tr><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th><th align="left">SpeRF Recon</th><th align="left">LinInt Recon</th><th align="left">Partial Recon</th></tr></thead><tbody><tr><td align="left">
<bold>Lesion</bold>
</td><td char="." align="char">
<bold>83.8%</bold>
</td><td char="." align="char">79.8%</td><td char="." align="char">80.7%</td><td char="." align="char">
<bold>78.4%</bold>
</td><td char="." align="char">70.7%</td><td char="." align="char">68.5%</td><td char="." align="char">
<bold>65.7%</bold>
</td><td char="." align="char">55.7%</td><td char="." align="char">54.9%</td></tr><tr><td align="left">
<bold>All Organ ROIs</bold>
</td><td char="." align="char">
<bold>84.7%</bold>
</td><td char="." align="char">75.7%</td><td char="." align="char">80.9%</td><td char="." align="char">
<bold>78.4%</bold>
</td><td char="." align="char">56.9%</td><td char="." align="char">67.3%</td><td char="." align="char">
<bold>63.2%</bold>
</td><td char="." align="char">31.0%</td><td char="." align="char">50.8%</td></tr><tr><td align="left">
<bold>Kidney</bold>
</td><td char="." align="char">
<bold>84.8%</bold>
</td><td char="." align="char">79.9%</td><td char="." align="char">80.3%</td><td char="." align="char">
<bold>80.1%</bold>
</td><td char="." align="char">69.6%</td><td char="." align="char">67.6%</td><td char="." align="char">
<bold>65.8%</bold>
</td><td char="." align="char">44.2%</td><td char="." align="char">51.3%</td></tr><tr><td align="left">
<bold>Lacrimal</bold>
</td><td char="." align="char">
<bold>83.6%</bold>
</td><td char="." align="char">63.6%</td><td char="." align="char">80.4%</td><td char="." align="char">
<bold>77.5%</bold>
</td><td char="." align="char">29.9%</td><td char="." align="char">68.6%</td><td char="." align="char">
<bold>57.2%</bold>
</td><td char="." align="char">10.2%</td><td char="." align="char">47.9%</td></tr><tr><td align="left">
<bold>Parotid</bold>
</td><td char="." align="char">
<bold>84.5%</bold>
</td><td char="." align="char">79.3%</td><td char="." align="char">80.9%</td><td char="." align="char">
<bold>79.1%</bold>
</td><td char="." align="char">63.4%</td><td char="." align="char">66.0%</td><td char="." align="char">
<bold>67.6%</bold>
</td><td char="." align="char">34.7%</td><td char="." align="char">52.0%</td></tr><tr><td align="left">
<bold>Submandibular</bold>
</td><td char="." align="char">
<bold>85.6%</bold>
</td><td char="." align="char">79.7%</td><td char="." align="char">81.8%</td><td char="." align="char">
<bold>77.1%</bold>
</td><td char="." align="char">64.0%</td><td char="." align="char">66.9%</td><td char="." align="char">
<bold>62.2%</bold>
</td><td char="." align="char">34.6%</td><td char="." align="char">51.7%</td></tr></tbody></table></table-wrap>
</p><p id="Par43">
<fig id="Fig6"><label>Fig. 6</label><caption><p>Box plot visualization of results presented in Tables&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>, showing RCR, i.e., relative count recovery (upper panel) and relative CNR (lower panel) for three sparse-view reconstructions: SpeRF recon, Partial recon, and LinInt recon. Results are reported for [177Lu]Lu-DOTATATE (left panel) and [177Lu]Lu-PSMA-617 (right panel) patient studies across different down-sampling factors (DF&#x02009;=&#x02009;2, 4, 8). SpeRF recon maintains a balance between RCR and RCNR (especially at DF&#x02009;=&#x02009;4), outperforming LinInt in both metrics and Partial in RCNR. Note that for the RCR of lacrimal glands in LinInt recon, a few data points are not visible because they fall below the plotted range</p></caption><graphic xlink:href="40658_2025_762_Fig6_HTML" id="d33e1668"/></fig>
</p></sec></sec><sec id="Sec16"><title>Discussion</title><p id="Par44">The field of machine learning, particularly deep learning (DL), is rapidly advancing. However, DL applications in SPECT imaging remain limited, partly due to challenges such as limited availability of training data. Supervised DL methods, like 3D U-Net, have shown promise in predicting missing SPECT projection views, but their reliance on large datasets poses a barrier. Although simulated SPECT projection data can be useful for training, there is a domain gap between simulated data and real patient data, with differences in activity distributions and noise characteristics. This gap can limit the applicability of supervised models trained solely on simulations. In our RPT application, with only tens of patient datasets available, obtaining the hundreds or thousands of datasets required for supervised methods was impractical. Additionally, variations in camera-specific parameters, such as gamma-camera crystal thickness and body contour orbits, can hinder generalizability. In contrast, self-supervised learning methods derive insights directly from the available data, without the need for extensive labeled datasets, making them inherently adaptable. This motivated our focus on a self-supervised approach in this study.</p><p id="Par45">To evaluate the performance of supervised methods with our limited dataset, we implemented a supervised learning approach similar to that introduced in [<xref ref-type="bibr" rid="CR2">2</xref>]. Specifically, we used three separate 3D U-Nets to predict the skipped 3/4 of total projections given the 1/4 measured projections. Each U-Net was trained to predict a subset of skipped projections: the first U-Net targeted projections 2, 6, 10,&#x02026;, 118; the second targeted projections 3, 7, 11,&#x02026;, 119; and the third targeted projections 4, 8, 12,&#x02026;, 120. We split our dataset of 11 [177Lu]Lu-DOTATATE patients into 4 for training, 2 for validation, and 5 for testing. The results, shown in Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>, indicate that this supervised approach achieved a substantially lower RCR and RCNR compared to SpeRF (Tables&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>), emphasizing the challenges of applying supervised methods to scenarios with restricted data availability.</p><p id="Par46">
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Average RCR and RCNR values for lesions and kidneys achieved by a supervised learning method on our [177Lu]Lu-DOTATATE patient dataset, using three 3D U-Nets. The skipped 3/4 projections were predicted based on the 1/4 measured projections. Results are evaluated on 5 test patients, with 4 patients used for training and 2 for validation. SpeRF recon results are repeated for comparison with supervised 3D U-Net method</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="2">SpeRF Recon</th><th align="left" colspan="2">Supervised 3D U-Net</th></tr><tr><th align="left">RCR</th><th align="left">RCNR</th><th align="left">RCR</th><th align="left">RCNR</th></tr></thead><tbody><tr><td align="left">Lesion</td><td char="." align="char">
<bold>98.3%</bold>
</td><td char="." align="char">
<bold>87.9%</bold>
</td><td char="." align="char">80.4%</td><td char="." align="char">57.4%</td></tr><tr><td align="left">Kidney</td><td char="." align="char">
<bold>97.7%</bold>
</td><td char="." align="char">
<bold>88.0%</bold>
</td><td char="." align="char">81.9%</td><td char="." align="char">63.5%</td></tr></tbody></table></table-wrap>
</p><p id="Par47">The extension from NeRF to SpeRF is natural. NeRF was originally designed to render photorealistic novel views of scenes with complex geometries and appearances by representing a scene as a continuous function that outputs radiance in the coordinate space. To learn this continuous representation, an MLP is trained with scene coordinates as inputs and three-channel RGB colors as the training targets. Similarly, in this work, we employed an MLP to learn a continuous representation, but the training targets were defined as single-channel SPECT projection counts. This coordinate-based learning approach operates directly in the projection domain, making it agnostic to the choice of image reconstruction method. It is compatible with a wide range of reconstruction techniques, including model-based image reconstruction (MBIR) and plug-and-play [<xref ref-type="bibr" rid="CR32">32</xref>] approaches. MBIR methods typically process a complete set of projection views with fewer counts per view, improving image quality and reducing noise by incorporating regularizers and priors. However, these methods often require careful tuning of regularization parameters, which can be challenging. In contrast, SpeRF is tuning-free, as demonstrated by its robust performance across two distinct therapies with significantly different activity distributions in the body.</p><p id="Par48">While SpeRF recon effectively compensates for image quality degradation in sparse view acquisitions, several limitations remain. At a DF of 4, SpeRF recon achieved RCNRs of ~&#x02009;80% or higher for all organs and lesions in patient studies, outperforming other sparse view methods (~&#x02009;60&#x02013;70%, Tables&#x000a0;<xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>). However, at higher DFs, such as DF&#x02009;=&#x02009;8, we observed reduced activity recovery (Tables&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref>). This reduction likely stems from the neural network&#x02019;s smoothing tendency in high-noise scenarios, where voxel values are averaged due to noise variances. Additionally, the limited training data at high DFs impacts the MLP&#x02019;s ability to capture finer textures in measurement projections, further reducing activity recovery for small lesions. A similar reduction in activity/count recovery has also been reported in previous studies [<xref ref-type="bibr" rid="CR2">2</xref>]. Future research could explore the integration of variational inference or generative models to mitigate this smoothing effect and enhance the fidelity of fine details. Another limitation is the computational efficiency of SpeRF, as it currently takes&#x02009;~&#x02009;40&#x000a0;s to synthesize a single projection image on our machine (RTX 4090 GPU&#x02009;+&#x02009;24-core Intel i9-13900KF CPU&#x02009;+&#x02009;64 GB of DDR5 memory), making real-time synthesis challenging. Optimizing the implementations, such as through a customized CUDA kernel, could significantly accelerate processing and address this issue.</p><p id="Par49">Shiri et al. proposed a deep convolutional residual neural network-based approach to reduce SPECT acquisition time by predicting full-time and full-view projection images from half-time and half-view projection images, respectively, to maintain reconstruction quality [<xref ref-type="bibr" rid="CR16">16</xref>]. Similar to [<xref ref-type="bibr" rid="CR2">2</xref>], training of [<xref ref-type="bibr" rid="CR16">16</xref>] was conducted in a supervised manner. However, as shown in Table&#x000a0;<xref rid="Tab6" ref-type="table">6</xref>, supervised learning methods are less effective when only limited data is available. We investigated the effects of reducing the acquisition time per projection view by applying retrospective Bernoulli Thinning (BerTin) to full-time projection views. Specifically, let <inline-formula id="IEq31"><alternatives><tex-math id="d33e1800">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:Y\sim\text{P}\text{o}\text{i}\text{s}\text{s}\text{o}\text{n}\left(\mu\:\right)$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq31.gif"/></alternatives></inline-formula> and define <inline-formula id="IEq32"><alternatives><tex-math id="d33e1807">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:Z$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq32.gif"/></alternatives></inline-formula> such that <inline-formula id="IEq33"><alternatives><tex-math id="d33e1813">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:Z|Y=k\sim\text{B}\text{e}\text{r}\text{n}\text{o}\text{u}\text{l}\text{l}\text{i}(1/\text{D}\text{F},\:k)$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq33.gif"/></alternatives></inline-formula>. It follows that <inline-formula id="IEq34"><alternatives><tex-math id="d33e1819">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:Z\sim\text{P}\text{o}\text{i}\text{s}\text{s}\text{o}\text{n}(\mu\:/\text{D}\text{F})$$\end{document}</tex-math><inline-graphic xlink:href="40658_2025_762_Article_IEq34.gif"/></alternatives></inline-formula>. In BerTin, each projection event is independently retained with a probability of 1/DF, resulting in temporally subsampled projection images by a DF [<xref ref-type="bibr" rid="CR33">33</xref>]. These subsampled projection images are then reconstructed using OS-EM. Table&#x000a0;<xref rid="Tab7" ref-type="table">7</xref> presents metrics, including RA and RCNR for BerTin recons across all [177Lu]Lu-DOTATATE patients. BerTin achieves results comparable to Partial. Both methods maintain high RCR values across all DFs, with BerTin achieving ~&#x02009;99% or higher for lesions and kidneys even at DF=8. However, RCNR values for BerTin decline significantly at higher DFs, similar to Partial. For example, at DF=8, BerTin achieves RCNR values of 45.2% for lesions and 49.6% for kidneys, indicating substantial CNR degradation.</p><p id="Par50">
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Average RCR and RCNR values for lesions and kidneys for BerTin recons, which reduces acquisition time per projection by retaining projection events with a probability of 1/DF using Bernoulli thinning. Results are based on experiments conducted with our [177Lu]Lu-DOTATATE dataset across various DFs. SpeRF recon results are repeated for comparison with BerTin recon</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" rowspan="2" colspan="2"/><td align="left" colspan="2">DF&#x02009;=&#x02009;2</td><td align="left" colspan="2">DF&#x02009;=&#x02009;4</td><td align="left" colspan="2">DF&#x02009;=&#x02009;8</td></tr><tr><td align="left">SpeRF</td><td align="left">BerTin</td><td align="left">SpeRF</td><td align="left">BerTin</td><td align="left">SpeRF</td><td align="left">BerTin</td></tr><tr><td align="left" rowspan="2">RCR</td><td align="left">Lesion</td><td align="left">100.6%</td><td align="left">
<bold>99.6%</bold>
</td><td align="left">98.3%</td><td align="left">
<bold>99.2%</bold>
</td><td align="left">90.3%</td><td align="left">
<bold>99.1%</bold>
</td></tr><tr><td align="left">Kidney</td><td align="left">103.9%</td><td align="left">
<bold>100.0%</bold>
</td><td align="left">97.7%</td><td align="left">
<bold>101.4%</bold>
</td><td align="left">93.0%</td><td align="left">
<bold>104.8%</bold>
</td></tr><tr><td align="left" rowspan="2">RCNR</td><td align="left">Lesion</td><td align="left">
<bold>88.6%</bold>
</td><td align="left">80.1%</td><td align="left">
<bold>87.9%</bold>
</td><td align="left">60.2%</td><td align="left">
<bold>73.5%</bold>
</td><td align="left">45.2%</td></tr><tr><td align="left">Kidney</td><td align="left">
<bold>92.6%</bold>
</td><td align="left">80.5%</td><td align="left">
<bold>88.0%</bold>
</td><td align="left">63.2%</td><td align="left">
<bold>76.5%</bold>
</td><td align="left">49.6%</td></tr></tbody></table></table-wrap>
</p><p id="Par51">Although our research was initially focused on Lu-177 SPECT imaging, we expect that our coordinates learning-based self-supervised method could be adapted for use in other low-count applications, where the imaging acquisition could vary between scans and a single pre-trained model may not generalize well. This includes pure <italic>&#x003b2;</italic>--emitters, like Y-90, characterized by a low yield of bremsstrahlung photons for SPECT imaging [<xref ref-type="bibr" rid="CR34">34</xref>], and therapies with &#x003b1;-emitters, like Ac-225 that use very low activities [<xref ref-type="bibr" rid="CR3">3</xref>]. Both present inherent low-count imaging challenges that could potentially benefit from our approach. Furthermore, SpeRF could benefit diagnostic SPECT imaging by enabling administration of lower activities, therefore supporting low-dose SPECT protocols that reduce radiation exposure to patients with minimal compromise to image quality.</p></sec><sec id="Sec17"><title>Conclusion</title><p id="Par52">This study addresses the challenge of extended SPECT imaging durations under low-count conditions, as encountered in Lu-177 SPECT imaging, by developing a self-supervised coordinate learning approach, namely SpeRF, that efficiently synthesizes skipped SPECT projection views without separate training data. SpeRF enables a significant reduction in SPECT acquisition time by allowing for skipping projection views and using an MLP to synthesize skipped projections, while preserving image quality, as indicated by improved NRMSD in projections and relative CNR in reconstructions compared with other methods for sparse acquisitions, though minor underestimation of count recovery in patient studies is observed. Unlike supervised deep learning-based approaches, this self-supervised method addresses the challenge of limited training data availability commonly encountered in clinical settings. The feasibility for reduction in acquisition time demonstrated in this work is particularly relevant for imaging under low-count conditions and for protocols that require multiple-bed positions.</p></sec><sec id="Sec18" sec-type="supplementary-material"><title>Electronic supplementary material</title><p>Below is the link to the electronic supplementary material.</p><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="40658_2025_762_MOESM1_ESM.docx"><caption><p>Supplementary Material 1</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>Zongyu Li and Yixuan Jia contributed equally to this work.</p></fn></fn-group><ack><title>Acknowledgements</title><p>Not applicable.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Each author has contributed to the submitted work as follows: ZL, XJ and YD designed the study. YD collected the clinical data. ZL, YJ and JH processed and analyzed the data and conducted the study. ZL and YJ drafted the manuscript. YD and JF revised the manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This research was supported by R01CA240706 and R01CA289631 awarded by the National Cancer Institute (NCI), National Institute of Health (NIH).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets generated during and/or analyzed during the current study are available from the corresponding author on reasonable request. Code for reproducing the results will be available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ZongyuLi-umich/">https://github.com/ZongyuLi-umich/</ext-link> after the paper is accepted.</p></notes><notes><title>Declarations</title><notes id="FPar1"><title>Ethics approval and consent to participate</title><p id="Par53">This retrospective study did not involve direct participation of human subjects. The study was conducted in accordance with the ethical principles outlined in the Declaration of Helsinki. Procedures were used to protect the privacy and confidentiality of the data.</p></notes><notes id="FPar2"><title>Consent for publication</title><p id="Par54">Not applicable.</p></notes><notes id="FPar3" notes-type="COI-statement"><title>Competing interests</title><p id="Par55">Yuni Dewaraja serves as a consultant for GE Healthcare&#x02019;s MIM Software and for Novartis. The authors declare no other competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Ritt</surname><given-names>P</given-names></name></person-group><article-title>Recent developments in SPECT/CT</article-title><source>Semin Nucl Med</source><year>2022</year><volume>52</volume><issue>3</issue><fpage>276</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1053/j.semnuclmed.2022.01.004</pub-id><pub-id pub-id-type="pmid">35210065</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Ritt P. Recent developments in SPECT/CT. Semin Nucl Med. 2022;52(3):276&#x02013;85.<pub-id pub-id-type="pmid">35210065</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Ryd&#x000e9;n</surname><given-names>T</given-names></name><name><surname>Essen</surname><given-names>MV</given-names></name><name><surname>Marin</surname><given-names>I</given-names></name><name><surname>Svensson</surname><given-names>J</given-names></name><name><surname>Bernhardt</surname><given-names>P</given-names></name></person-group><article-title>Deep-Learning generation of synthetic intermediate projections improves 177Lu SPECT images reconstructed with sparsely acquired projections</article-title><source>J Nucl Med</source><year>2021</year><volume>62</volume><issue>4</issue><fpage>528</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.2967/jnumed.120.245548</pub-id><pub-id pub-id-type="pmid">32859710</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Ryd&#x000e9;n T, Essen MV, Marin I, Svensson J, Bernhardt P. Deep-Learning generation of synthetic intermediate projections improves 177Lu SPECT images reconstructed with sparsely acquired projections. J Nucl Med. 2021;62(4):528&#x02013;35.<pub-id pub-id-type="pmid">32859710</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Delker</surname><given-names>A</given-names></name><name><surname>Schleske</surname><given-names>M</given-names></name><name><surname>Liubchenko</surname><given-names>G</given-names></name><name><surname>Berg</surname><given-names>I</given-names></name><name><surname>Zacherl</surname><given-names>MJ</given-names></name><name><surname>Brendel</surname><given-names>M</given-names></name><etal/></person-group><article-title>Biodistribution and dosimetry for combined [177Lu]Lu-PSMA-I&#x00026;T/[225Ac]Ac-PSMA-I&#x00026;T therapy using multi-isotope quantitative SPECT imaging</article-title><source>Eur J Nucl Med Mol Imaging</source><year>2023</year><volume>50</volume><issue>5</issue><fpage>1280</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1007/s00259-022-06092-1</pub-id><pub-id pub-id-type="pmid">36629878</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Delker A, Schleske M, Liubchenko G, Berg I, Zacherl MJ, Brendel M, et al. Biodistribution and dosimetry for combined [177Lu]Lu-PSMA-I&#x00026;T/[225Ac]Ac-PSMA-I&#x00026;T therapy using multi-isotope quantitative SPECT imaging. Eur J Nucl Med Mol Imaging. 2023;50(5):1280&#x02013;90.<pub-id pub-id-type="pmid">36629878</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Kabasakal</surname><given-names>L</given-names></name><name><surname>Toklu</surname><given-names>T</given-names></name><name><surname>Yeyin</surname><given-names>N</given-names></name><name><surname>Demirci</surname><given-names>E</given-names></name><name><surname>Abuqbeitah</surname><given-names>M</given-names></name><name><surname>Ocak</surname><given-names>M</given-names></name><etal/></person-group><article-title>Lu-177-PSMA-617 prostate-Specific membrane antigen inhibitor therapy in patients with Castration-Resistant prostate cancer: stability, Bio-distribution and dosimetry</article-title><source>Mol Imaging Radionucl Ther</source><year>2017</year><volume>26</volume><issue>2</issue><fpage>62</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.4274/mirt.08760</pub-id><pub-id pub-id-type="pmid">28613198</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Kabasakal L, Toklu T, Yeyin N, Demirci E, Abuqbeitah M, Ocak M, et al. Lu-177-PSMA-617 prostate-Specific membrane antigen inhibitor therapy in patients with Castration-Resistant prostate cancer: stability, Bio-distribution and dosimetry. Mol Imaging Radionucl Ther. 2017;26(2):62&#x02013;8.<pub-id pub-id-type="pmid">28613198</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Violet</surname><given-names>J</given-names></name><name><surname>Jackson</surname><given-names>P</given-names></name><name><surname>Ferdinandus</surname><given-names>J</given-names></name><name><surname>Sandhu</surname><given-names>S</given-names></name><name><surname>Akhurst</surname><given-names>T</given-names></name><name><surname>Iravani</surname><given-names>A</given-names></name><etal/></person-group><article-title>Dosimetry of<sup>177</sup> Lu-PSMA-617 in metastatic Castration-Resistant prostate cancer: correlations between pretherapeutic imaging and Whole-Body tumor dosimetry with treatment outcomes</article-title><source>J Nucl Med</source><year>2019</year><volume>60</volume><issue>4</issue><fpage>517</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.2967/jnumed.118.219352</pub-id><pub-id pub-id-type="pmid">30291192</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Violet J, Jackson P, Ferdinandus J, Sandhu S, Akhurst T, Iravani A, et al. Dosimetry of<sup>177</sup> Lu-PSMA-617 in metastatic Castration-Resistant prostate cancer: correlations between pretherapeutic imaging and Whole-Body tumor dosimetry with treatment outcomes. J Nucl Med. 2019;60(4):517&#x02013;23.<pub-id pub-id-type="pmid">30291192</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Picone V, Makris N, Boutevin F, Roy S, Playe M, Soussan M. Clinical validation of time reduction strategy in continuous step-and-shoot mode during SPECT acquisition. EJNMMI Phys. 2021;8(1).</mixed-citation></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Sohlberg</surname><given-names>A</given-names></name><name><surname>Kangasmaa</surname><given-names>T</given-names></name><name><surname>Constable</surname><given-names>C</given-names></name><name><surname>Tikkakoski</surname><given-names>A</given-names></name></person-group><article-title>Comparison of deep learning-based denoising methods in cardiac SPECT</article-title><source>EJNMMI Phys</source><year>2023</year><volume>10</volume><issue>1</issue><fpage>9</fpage><pub-id pub-id-type="doi">10.1186/s40658-023-00531-0</pub-id><pub-id pub-id-type="pmid">36752847</pub-id>
</element-citation><mixed-citation id="mc-CR7" publication-type="journal">Sohlberg A, Kangasmaa T, Constable C, Tikkakoski A. Comparison of deep learning-based denoising methods in cardiac SPECT. EJNMMI Phys. 2023;10(1):9.<pub-id pub-id-type="pmid">36752847</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Aghakhan Olia</surname><given-names>N</given-names></name><name><surname>Kamali-Asl</surname><given-names>A</given-names></name><name><surname>Hariri Tabrizi</surname><given-names>S</given-names></name><name><surname>Geramifar</surname><given-names>P</given-names></name><name><surname>Sheikhzadeh</surname><given-names>P</given-names></name><name><surname>Farzanefar</surname><given-names>S</given-names></name><etal/></person-group><article-title>Deep learning&#x02013;based denoising of low-dose SPECT myocardial perfusion images: quantitative assessment and clinical performance</article-title><source>Eur J Nucl Med Mol Imaging</source><year>2022</year><volume>49</volume><issue>5</issue><fpage>1508</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1007/s00259-021-05614-7</pub-id><pub-id pub-id-type="pmid">34778929</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Aghakhan Olia N, Kamali-Asl A, Hariri Tabrizi S, Geramifar P, Sheikhzadeh P, Farzanefar S, et al. Deep learning&#x02013;based denoising of low-dose SPECT myocardial perfusion images: quantitative assessment and clinical performance. Eur J Nucl Med Mol Imaging. 2022;49(5):1508&#x02013;22.<pub-id pub-id-type="pmid">34778929</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Wernick</surname><given-names>MN</given-names></name><name><surname>Pretorius</surname><given-names>PH</given-names></name><name><surname>King</surname><given-names>MA</given-names></name></person-group><article-title>Deep learning with noise-to-noise training for denoising in SPECT myocardial perfusion imaging</article-title><source>Med Phys</source><year>2021</year><volume>48</volume><issue>1</issue><fpage>156</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1002/mp.14577</pub-id><pub-id pub-id-type="pmid">33145782</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Liu J, Yang Y, Wernick MN, Pretorius PH, King MA. Deep learning with noise-to-noise training for denoising in SPECT myocardial perfusion imaging. Med Phys. 2021;48(1):156&#x02013;68.<pub-id pub-id-type="pmid">33145782</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>MA</given-names></name><name><surname>Yu</surname><given-names>Z</given-names></name><name><surname>Siegel</surname><given-names>BA</given-names></name><name><surname>Jha</surname><given-names>AK</given-names></name></person-group><article-title>A task-specific deep-learning-based denoising approach for myocardial perfusion SPECT</article-title><source>Proc SPIE Int Soc Opt Eng</source><year>2023</year><volume>12467</volume><fpage>1246719</fpage><pub-id pub-id-type="pmid">37990706</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Rahman MA, Yu Z, Siegel BA, Jha AK. A task-specific deep-learning-based denoising approach for myocardial perfusion SPECT. Proc SPIE Int Soc Opt Eng. 2023;12467:1246719.<pub-id pub-id-type="pmid">37990706</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>J</given-names></name><name><surname>Du</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Wu</surname><given-names>TH</given-names></name><name><surname>Yang</surname><given-names>B</given-names></name><name><surname>Mok</surname><given-names>GSP</given-names></name></person-group><article-title>Pix2Pix generative adversarial network for low dose myocardial perfusion SPECT denoising</article-title><source>Quant Imaging Med Surg</source><year>2022</year><volume>12</volume><issue>7</issue><fpage>3539</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.21037/qims-21-1042</pub-id><pub-id pub-id-type="pmid">35782241</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Sun J, Du Y, Li C, Wu TH, Yang B, Mok GSP. Pix2Pix generative adversarial network for low dose myocardial perfusion SPECT denoising. Quant Imaging Med Surg. 2022;12(7):3539&#x02013;55.<pub-id pub-id-type="pmid">35782241</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>J</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Du</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>CY</given-names></name><name><surname>Wu</surname><given-names>TH</given-names></name><name><surname>Liu</surname><given-names>YH</given-names></name><etal/></person-group><article-title>Deep learning-based denoising in projection-domain and reconstruction-domain for low-dose myocardial perfusion SPECT</article-title><source>J Nucl Cardiol</source><year>2023</year><volume>30</volume><issue>3</issue><fpage>970</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1007/s12350-022-03045-x</pub-id><pub-id pub-id-type="pmid">35982208</pub-id>
</element-citation><mixed-citation id="mc-CR12" publication-type="journal">Sun J, Jiang H, Du Y, Li CY, Wu TH, Liu YH, et al. Deep learning-based denoising in projection-domain and reconstruction-domain for low-dose myocardial perfusion SPECT. J Nucl Cardiol. 2023;30(3):970&#x02013;85.<pub-id pub-id-type="pmid">35982208</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Akhavanallaf</surname><given-names>A</given-names></name><name><surname>Fessler</surname><given-names>JA</given-names></name><name><surname>Dewaraja</surname><given-names>YK</given-names></name></person-group><article-title>90Y SPECT scatter Estimation and voxel dosimetry in radioembolization using a unified deep learning framework</article-title><source>EJNMMI Phys</source><year>2023</year><volume>10</volume><issue>1</issue><fpage>82</fpage><pub-id pub-id-type="doi">10.1186/s40658-023-00598-9</pub-id><pub-id pub-id-type="pmid">38091168</pub-id>
</element-citation><mixed-citation id="mc-CR13" publication-type="journal">Jia Y, Li Z, Akhavanallaf A, Fessler JA, Dewaraja YK. 90Y SPECT scatter Estimation and voxel dosimetry in radioembolization using a unified deep learning framework. EJNMMI Phys. 2023;10(1):82.<pub-id pub-id-type="pmid">38091168</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Balaji</surname><given-names>V</given-names></name><name><surname>Song</surname><given-names>TA</given-names></name><name><surname>Malekzadeh</surname><given-names>M</given-names></name><name><surname>Heidari</surname><given-names>P</given-names></name><name><surname>Dutta</surname><given-names>J</given-names></name></person-group><article-title>Artificial intelligence for PET and SPECT image enhancement</article-title><source>J Nucl Med</source><year>2024</year><volume>65</volume><issue>1</issue><fpage>4</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.2967/jnumed.122.265000</pub-id><pub-id pub-id-type="pmid">37945384</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Balaji V, Song TA, Malekzadeh M, Heidari P, Dutta J. Artificial intelligence for PET and SPECT image enhancement. J Nucl Med. 2024;65(1):4&#x02013;12.<pub-id pub-id-type="pmid">37945384</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Z</given-names></name><name><surname>Qi</surname><given-names>N</given-names></name><name><surname>Meng</surname><given-names>Q</given-names></name><name><surname>Pan</surname><given-names>B</given-names></name><name><surname>Feng</surname><given-names>T</given-names></name><name><surname>Zhao</surname><given-names>J</given-names></name><etal/></person-group><article-title>Fast SPECT/CT planar bone imaging enabled by deep learning enhancement</article-title><source>Med Phys</source><year>2024</year><volume>51</volume><issue>8</issue><fpage>5414</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1002/mp.17094</pub-id><pub-id pub-id-type="pmid">38652084</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Pan Z, Qi N, Meng Q, Pan B, Feng T, Zhao J, et al. Fast SPECT/CT planar bone imaging enabled by deep learning enhancement. Med Phys. 2024;51(8):5414&#x02013;26.<pub-id pub-id-type="pmid">38652084</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Shiri</surname><given-names>I</given-names></name><name><surname>AmirMozafari Sabet</surname><given-names>K</given-names></name><name><surname>Arabi</surname><given-names>H</given-names></name><name><surname>Pourkeshavarz</surname><given-names>M</given-names></name><name><surname>Teimourian</surname><given-names>B</given-names></name><name><surname>Ay</surname><given-names>MR</given-names></name><etal/></person-group><article-title>Standard SPECT myocardial perfusion Estimation from half-time acquisitions using deep convolutional residual neural networks</article-title><source>J Nuclear Cardiol</source><year>2021</year><volume>28</volume><issue>6</issue><fpage>2761</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1007/s12350-020-02119-y</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Shiri I, AmirMozafari Sabet K, Arabi H, Pourkeshavarz M, Teimourian B, Ay MR, et al. Standard SPECT myocardial perfusion Estimation from half-time acquisitions using deep convolutional residual neural networks. J Nuclear Cardiol. 2021;28(6):2761&#x02013;79.</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Hornegger</surname><given-names>J</given-names></name><name><surname>Wells</surname><given-names>WM</given-names></name><name><surname>Frangi</surname><given-names>AF</given-names></name></person-group><article-title>U-Net: convolutional networks for biomedical image segmentation</article-title><source>Medical image computing and Computer-Assisted Intervention&#x02013; MICCAI 2015</source><year>2015</year><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>234</fpage><lpage>41</lpage></element-citation><mixed-citation id="mc-CR17" publication-type="book">Ronneberger O, Fischer P, Brox T. U-Net: convolutional networks for biomedical image segmentation. In: Navab N, Hornegger J, Wells WM, Frangi AF, editors. Medical image computing and Computer-Assisted Intervention&#x02013; MICCAI 2015. Cham: Springer International Publishing; 2015. pp. 234&#x02013;41.</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Wikberg</surname><given-names>E</given-names></name><name><surname>van Essen</surname><given-names>M</given-names></name><name><surname>Ryd&#x000e9;n</surname><given-names>T</given-names></name><name><surname>Svensson</surname><given-names>J</given-names></name><name><surname>Gjertsson</surname><given-names>P</given-names></name><name><surname>Bernhardt</surname><given-names>P</given-names></name></person-group><article-title>Improvements of 177Lu SPECT images from sparsely acquired projections by reconstruction with deep-learning-generated synthetic projections</article-title><source>EJNMMI Phys</source><year>2024</year><volume>11</volume><issue>1</issue><fpage>53</fpage><pub-id pub-id-type="doi">10.1186/s40658-024-00655-x</pub-id><pub-id pub-id-type="pmid">38941040</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Wikberg E, van Essen M, Ryd&#x000e9;n T, Svensson J, Gjertsson P, Bernhardt P. Improvements of 177Lu SPECT images from sparsely acquired projections by reconstruction with deep-learning-generated synthetic projections. EJNMMI Phys. 2024;11(1):53.<pub-id pub-id-type="pmid">38941040</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><article-title>Long Short-Term memory</article-title><source>Neural Comput</source><year>1997</year><volume>9</volume><issue>8</issue><fpage>1735</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Hochreiter S, Schmidhuber J. Long Short-Term memory. Neural Comput. 1997;9(8):1735&#x02013;80.<pub-id pub-id-type="pmid">9377276</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Ye</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>F</given-names></name></person-group><article-title>School of computer science and technology, Guangdong university of technology, Guangzhou 510006, China. LU-Net: combining LSTM and U-Net for sinogram synthesis in sparse-view SPECT reconstruction</article-title><source>MBE</source><year>2022</year><volume>19</volume><issue>4</issue><fpage>4320</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.3934/mbe.2022200</pub-id><pub-id pub-id-type="pmid">35341300</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Li S, Ye W, Li F. School of computer science and technology, Guangdong university of technology, Guangzhou 510006, China. LU-Net: combining LSTM and U-Net for sinogram synthesis in sparse-view SPECT reconstruction. MBE. 2022;19(4):4320&#x02013;40.<pub-id pub-id-type="pmid">35341300</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Xie</surname><given-names>H</given-names></name><name><surname>Miao</surname><given-names>T</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Holler</surname><given-names>W</given-names></name><etal/></person-group><article-title>DuDoSS: Deep-learning-based dual-domain sinogram synthesis from sparsely sampled projections of cardiac SPECT</article-title><source>Med Phys</source><year>2023</year><volume>50</volume><issue>1</issue><fpage>89</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1002/mp.15958</pub-id><pub-id pub-id-type="pmid">36048541</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Chen X, Zhou B, Xie H, Miao T, Liu H, Holler W, et al. DuDoSS: Deep-learning-based dual-domain sinogram synthesis from sparsely sampled projections of cardiac SPECT. Med Phys. 2023;50(1):89&#x02013;103.<pub-id pub-id-type="pmid">36048541</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Xie</surname><given-names>M</given-names></name><name><surname>Wohlberg</surname><given-names>B</given-names></name><name><surname>Kamilov</surname><given-names>US</given-names></name></person-group><article-title>CoIL: Coordinate-Based internal learning for tomographic imaging</article-title><source>IEEE Trans Comput Imaging</source><year>2021</year><volume>7</volume><fpage>1400</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1109/TCI.2021.3125564</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Sun Y, Liu J, Xie M, Wohlberg B, Kamilov US. CoIL: Coordinate-Based internal learning for tomographic imaging. IEEE Trans Comput Imaging. 2021;7:1400&#x02013;12.</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>SMB</given-names></name><name><surname>Meyer Viol</surname><given-names>SL</given-names></name><name><surname>van der Werf</surname><given-names>NR</given-names></name><name><surname>de Jong</surname><given-names>N</given-names></name><name><surname>van Velden</surname><given-names>FHP</given-names></name><name><surname>Meeuwis</surname><given-names>A</given-names></name><etal/></person-group><article-title>Variability in lutetium-177 SPECT quantification between different state-of-the-art SPECT/CT systems</article-title><source>EJNMMI Phys</source><year>2020</year><volume>7</volume><issue>1</issue><fpage>9</fpage><pub-id pub-id-type="doi">10.1186/s40658-020-0278-3</pub-id><pub-id pub-id-type="pmid">32048097</pub-id>
</element-citation><mixed-citation id="mc-CR23" publication-type="journal">Peters SMB, Meyer Viol SL, van der Werf NR, de Jong N, van Velden FHP, Meeuwis A, et al. Variability in lutetium-177 SPECT quantification between different state-of-the-art SPECT/CT systems. EJNMMI Phys. 2020;7(1):9.<pub-id pub-id-type="pmid">32048097</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Dewaraja</surname><given-names>YK</given-names></name><name><surname>Mirando</surname><given-names>DM</given-names></name><name><surname>Peterson</surname><given-names>AB</given-names></name><name><surname>Niedbala</surname><given-names>J</given-names></name><name><surname>Millet</surname><given-names>JD</given-names></name><name><surname>Mikell</surname><given-names>JK</given-names></name><etal/></person-group><article-title>A pipeline for automated voxel dosimetry: application in patients with Multi-SPECT/CT imaging after 177Lu-Peptide receptor radionuclide therapy</article-title><source>J Nucl Med</source><year>2022</year><volume>63</volume><issue>11</issue><fpage>1665</fpage><lpage>72</lpage><pub-id pub-id-type="pmid">35422445</pub-id>
</element-citation><mixed-citation id="mc-CR24" publication-type="journal">Dewaraja YK, Mirando DM, Peterson AB, Niedbala J, Millet JD, Mikell JK, et al. A pipeline for automated voxel dosimetry: application in patients with Multi-SPECT/CT imaging after 177Lu-Peptide receptor radionuclide therapy. J Nucl Med. 2022;63(11):1665&#x02013;72.<pub-id pub-id-type="pmid">35422445</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Mildenhall</surname><given-names>B</given-names></name><name><surname>Srinivasan</surname><given-names>PP</given-names></name><name><surname>Tancik</surname><given-names>M</given-names></name><name><surname>Barron</surname><given-names>JT</given-names></name><name><surname>Ramamoorthi</surname><given-names>R</given-names></name><name><surname>Ng</surname><given-names>R</given-names></name></person-group><article-title>NeRF: representing scenes as neural radiance fields for view synthesis</article-title><source>Commun ACM</source><year>2021</year><volume>65</volume><issue>1</issue><fpage>99</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1145/3503250</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Mildenhall B, Srinivasan PP, Tancik M, Barron JT, Ramamoorthi R, Ng R. NeRF: representing scenes as neural radiance fields for view synthesis. Commun ACM. 2021;65(1):99&#x02013;106.</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Huber PJ. Robust Estimation of a Location Parameter. In: Kotz S, Johnson NL, editors. Breakthroughs in Statistics: Methodology and Distribution [Internet]. New York, NY: Springer; 1992 [cited 2024 Dec 10]. pp. 492&#x02013;518. Available from: 10.1007/978-1-4612-4380-9_35</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Kingma DP, Ba J, Adam. A Method for Stochastic Optimization [Internet]. arXiv; 2017 [cited 2024 Dec 10]. Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link></mixed-citation></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Hudson</surname><given-names>HM</given-names></name><name><surname>Larkin</surname><given-names>RS</given-names></name></person-group><article-title>Accelerated image reconstruction using ordered subsets of projection data</article-title><source>IEEE Trans Med Imaging</source><year>1994</year><volume>13</volume><issue>4</issue><fpage>601</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1109/42.363108</pub-id><pub-id pub-id-type="pmid">18218538</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Hudson HM, Larkin RS. Accelerated image reconstruction using ordered subsets of projection data. IEEE Trans Med Imaging. 1994;13(4):601&#x02013;9.<pub-id pub-id-type="pmid">18218538</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Dewaraja</surname><given-names>YK</given-names></name><name><surname>Fessler</surname><given-names>JA</given-names></name></person-group><article-title>Training End-to-End unrolled iterative neural networks for SPECT image reconstruction</article-title><source>IEEE Trans Radiation Plasma Med Sci</source><year>2023</year><volume>7</volume><issue>4</issue><fpage>410</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/TRPMS.2023.3240934</pub-id></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Li Z, Dewaraja YK, Fessler JA. Training End-to-End unrolled iterative neural networks for SPECT image reconstruction. IEEE Trans Radiation Plasma Med Sci. 2023;7(4):410&#x02013;20.</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Ljungberg M, Strand SE, King MA. Monte Carlo calculations in nuclear medicine, second edition: applications in diagnostic imaging. CRC; 2012. p. 361.</mixed-citation></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Ritt</surname><given-names>P</given-names></name><name><surname>Vija</surname><given-names>H</given-names></name><name><surname>Hornegger</surname><given-names>J</given-names></name><name><surname>Kuwert</surname><given-names>T</given-names></name></person-group><article-title>Absolute quantification in SPECT</article-title><source>Eur J Nucl Med Mol Imaging</source><year>2011</year><volume>38</volume><issue>1</issue><fpage>69</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1007/s00259-011-1770-8</pub-id></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Ritt P, Vija H, Hornegger J, Kuwert T. Absolute quantification in SPECT. Eur J Nucl Med Mol Imaging. 2011;38(1):69&#x02013;77.</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Wohlberg</surname><given-names>B</given-names></name><name><surname>Kamilov</surname><given-names>US</given-names></name></person-group><article-title>An online Plug-and-Play algorithm for regularized image reconstruction</article-title><source>IEEE Trans Comput Imaging</source><year>2019</year><volume>5</volume><issue>3</issue><fpage>395</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1109/TCI.2019.2893568</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Sun Y, Wohlberg B, Kamilov US. An online Plug-and-Play algorithm for regularized image reconstruction. IEEE Trans Comput Imaging. 2019;5(3):395&#x02013;408.</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Ross SM. Introduction to probability models. Academic; 2014.</mixed-citation></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Minarik</surname><given-names>D</given-names></name><name><surname>Gleisner</surname><given-names>KS</given-names></name><name><surname>Ljungberg</surname><given-names>M</given-names></name></person-group><article-title>Evaluation of quantitative 90Y SPECT based on experimental Phantom studies</article-title><source>Phys Med Biol</source><year>2008</year><volume>53</volume><issue>20</issue><fpage>5689</fpage><pub-id pub-id-type="doi">10.1088/0031-9155/53/20/008</pub-id><pub-id pub-id-type="pmid">18812648</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">Minarik D, Gleisner KS, Ljungberg M. Evaluation of quantitative 90Y SPECT based on experimental Phantom studies. Phys Med Biol. 2008;53(20):5689.<pub-id pub-id-type="pmid">18812648</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>