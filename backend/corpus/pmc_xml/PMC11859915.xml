<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006345</article-id><article-id pub-id-type="pmc">PMC11859915</article-id><article-id pub-id-type="doi">10.3390/s25041117</article-id><article-id pub-id-type="publisher-id">sensors-25-01117</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>CAML-PSPNet: A Medical Image Segmentation Network Based on Coordinate Attention and a Mixed Loss Function</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-6669-7510</contrib-id><name><surname>Li</surname><given-names>Yuxia</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01117" ref-type="aff">1</xref><xref rid="fn1-sensors-25-01117" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Peng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-25-01117" ref-type="aff">2</xref><xref rid="fn1-sensors-25-01117" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Hailing</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-01117" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Gong</surname><given-names>Xiaomei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af2-sensors-25-01117" ref-type="aff">2</xref><xref rid="c1-sensors-25-01117" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Zhijun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af1-sensors-25-01117" ref-type="aff">1</xref><xref rid="c1-sensors-25-01117" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Bevilacqua</surname><given-names>Alessandro</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Corsi</surname><given-names>Cristiana</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01117"><label>1</label>School of Electronic and Electrical Engineering, Shanghai University of Engineering Science, Shanghai 201620, China; <email>sueslyx@gmail.com</email> (Y.L.); <email>02200009@sues.edu.cn</email> (H.W.)</aff><aff id="af2-sensors-25-01117"><label>2</label>Department of Radiation Oncology, Shanghai Pulmonary Hospital, Tongji University School of Medicine, Shanghai 200433, China; <email>18701837650@126.com</email></aff><author-notes><corresp id="c1-sensors-25-01117"><label>*</label>Correspondence: <email>gongxiaomei1981@163.com</email> (X.G.); <email>zjfang@dhu.edu.cn</email> (Z.F.)</corresp><fn id="fn1-sensors-25-01117"><label>&#x02020;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1117</elocation-id><history><date date-type="received"><day>30</day><month>11</month><year>2024</year></date><date date-type="rev-recd"><day>01</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>08</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>The problems of missed segmentation with fuzzy boundaries of segmented regions and small regions are common in segmentation tasks, and greatly decrease the accuracy of clinicians&#x02019; diagnosis. For this, a new network based on PSPNet, using a coordinate attention mechanism and a mixed loss function for segmentation (CAML-PSPNet), is proposed. Firstly, the coordinate attention module splits the input feature map into horizontal and vertical directions to locate the edge position of the segmentation target. Then, a Mixed Loss function (MLF) is introduced in the model training stage to solve the problem of the low accuracy of small-target tumor segmentation. Finally, the lightweight MobilenetV2 is utilized in backbone feature extraction, which largely reduces the model&#x02019;s parameter count and enhances computation speed. Three datasets&#x02014;PrivateLT, Kvasir-SEG and ISIC 2017&#x02014;are selected for the experimental part, and the experimental results demonstrate significant enhancements in both visual effects and evaluation metrics for the segmentation achieved by CAML-PSPNet. Compared with Deeplabv3, HrNet, U-Net and PSPNet networks, the average intersection rates of CAML-PSPNet are increased by 2.84%, 3.1%, 5.4% and 3.08% on lung cancer data, 7.54%, 3.1%, 5.91% and 8.78% on Kvasir-SEG data, and 1.97%, 0.71%, 3.83% and 0.78% on ISIC 2017 data, respectively. When compared to other methods, CAML-PSPNet has the greatest similarity with the gold standard in boundary segmentation, and effectively enhances the segmentation accuracy for small targets.</p></abstract><kwd-group><kwd>lung tumor</kwd><kwd>PSPNet</kwd><kwd>attention mechanism</kwd><kwd>mixed loss</kwd><kwd>image segmentation</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62001284</award-id></award-group><award-group><funding-source>Shanghai Science and Technology Innovation Action Plan</funding-source><award-id>20Y11913600</award-id></award-group><award-group><funding-source>Clinical Research Foundation of Shanghai Pulmonary Hospital</funding-source><award-id>SKPY20211006</award-id></award-group><funding-statement>This work was supported in part by a grant from the National Natural Science Foundation of China (grant No. 62001284) and Shanghai Science and Technology Innovation Action Plan (20Y11913600) Clinical Research Foundation of Shanghai Pulmonary Hospital (SKPY20211006).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01117"><title>1. Introduction</title><p>Medical images are generally characterized by low contrast and difficulties in identify the boundaries between different tissues; these medical images can be presented across multiple modalities. The intrinsic complexity of medical images, often marked by low contrast and the challenging task of distinguishing tissue boundaries, necessitates advanced analytical approaches. Bridging this gap, the evolution of deep learning has provided a transformative motivation for the field. Recently, deep learning has been rapidly developed and different modalities of medical image segmentation tasks have shown good results by deep learning methods. Empirical evidence from an array of studies suggests that deep learning techniques surpass traditional machine learning techniques in yielding more dependable outcomes for medical image segmentation tasks [<xref rid="B1-sensors-25-01117" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01117" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01117" ref-type="bibr">3</xref>].</p><p>Currently, several network architectures have been at the forefront of the field. The Fully Convolutional Network (FCN) [<xref rid="B4-sensors-25-01117" ref-type="bibr">4</xref>] stands out for its unique adaptation in medical imaging, where it employs fully convolutional layers in lieu of fully connected ones. This design is capable of processing images of varying sizes as input and producing segmentation results with matching dimensions. FCN leverages convolution and multi-scale information fusion techniques to enhance segmentation precision while preserving detailed image features. Another notable architecture is SegNet [<xref rid="B5-sensors-25-01117" ref-type="bibr">5</xref>], which operates on an encoder&#x02013;decoder framework and performs feature extraction and pixel-level classification. SegNet introduces a novel approach to pooling by using maximal pooling indices, which serves to decrease the model&#x02019;s computational burden. Parallel to these developments is U-Net [<xref rid="B6-sensors-25-01117" ref-type="bibr">6</xref>], a model specifically tailored for the segmentation of biomedical images. It boasts a symmetric U-shaped structure that adeptly combines high-resolution pathways with a contracting path to deliver highly accurate segmentation results. Building on the FCN framework, the DeepLab network [<xref rid="B7-sensors-25-01117" ref-type="bibr">7</xref>] also caters to biomedical image segmentation. It enhances the basic FCN structure by incorporating atrous convolution, which allows it to manage the segmentation of images at varying scales effectively. Although CNN architectures have advanced considerably in segmentation, there remains the challenge of detail loss during the pooling and convolution operations, which can compromise the quality of the segmentation outcomes. The continuous down-sampling process inherent in these networks tends to obscure finer details, which is a critical limitation when high precision is required, such as in medical image analysis. Thus, ongoing research is directed toward refining these networks to maintain detail integrity while achieving the overarching goal of accurate and high-resolution segmentation.</p><p>The Pyramid Scene Parsing Network (PSPNet) [<xref rid="B8-sensors-25-01117" ref-type="bibr">8</xref>] has garnered attention for its novel approach to semantic segmentation. It integrates a pyramid pooling module that aggregates contextual information across varying scales and synthesizes multi-scale features to enhance segmentation accuracy. Initially applied to scene parsing, PSPNet assigns category labels to each pixel while capturing information on category, location, and shape. Its capability to handle complex issues such as contextual relationship mismatches, category confusion, and obscurity is notable. Furthermore, the network&#x02019;s proficiency in fusing global and local feature sets via pyramid pooling operations significantly mitigates the issue of feature detail loss. This makes PSPNet a formidable contender in semantic segmentation tasks. Wu et al. [<xref rid="B9-sensors-25-01117" ref-type="bibr">9</xref>] introduced a depth-guided fusion module on the basis of PSPNet and used a channel attention module during the network&#x02019;s feature extraction stage, and obtained a strong advantage in object edge recognition and segmentation and obtained effective results. Zhu et al. [<xref rid="B10-sensors-25-01117" ref-type="bibr">10</xref>] introduced an improved network based on a PSPNet network to segment coronary angiography images, achieving a segmentation accuracy of 0.957.</p><p>Connecting these advancements with the broader scope of attention mechanisms [<xref rid="B11-sensors-25-01117" ref-type="bibr">11</xref>] in neural networks, it becomes clear that the ability to concentrate on pertinent segments of input data is integral to the improvement of network performance and generalization. A traditional attention mechanism is based on content, and the attention weight is computed based on the similarity, which cannot obtain the target position and object edge information well. The Coordinate Attention (CA) mechanism [<xref rid="B12-sensors-25-01117" ref-type="bibr">12</xref>] is used in sequence modeling; it emphasizes the positional information within the sequence, enabling the model to capture relationships between various positions.</p><p>Transitioning to the topic of model training [<xref rid="B13-sensors-25-01117" ref-type="bibr">13</xref>], selecting an appropriate loss function is crucial. The chosen loss function quantifies the deviation between the model&#x02019;s predictions and the ground truth values, moving toward greater predictive accuracy through learning and optimization. Usually, loss functions have the role of model evaluation, back propagation and goal optimization in the process of model training. However, in some cases, due to data imbalance and small objectives, a mixed loss function combining different loss functions is needed to improve the model training, which optimizes the model training effect to a large extent.</p><p>Addressing the issues related to small targets&#x02014;missed segmentation and wrong segmentation due to blurred tissue boundaries in the task of medical image segmentation in actual clinics&#x02014;this study introduces PSPNet for natural scene parsing in the task of medical image segmentation and proposes a novel network architecture, CA mechanism and mixed loss function based on the PSPNet network. The approach first employs the CA mechanism to pinpoint the tumor region, followed by the introduction of the mixed loss function to expedite model convergence, which can effectively segment tumors. This paper makes the three contributions:<list list-type="simple"><list-item><label>(1)</label><p>A novel network architecture, CAML-PSPNet, utilizes the coordinate attention module during network feature extraction to accurately locate the feature region and effectively assist the network in accurate segmentation.</p></list-item><list-item><label>(2)</label><p>Addressing the limitations of existing hybrid loss functions in segmenting small-target tumors, a new Mixed Loss is proposed; this expedites the model&#x02019;s convergence rate while effectively improving the accuracy of segmentation by defining new scale coefficients.</p></list-item><list-item><label>(3)</label><p>MobileNetV2 serves as the backbone network in the PSPNet network, which greatly reduces the computation time and parameter count.</p></list-item></list></p></sec><sec id="sec2-sensors-25-01117"><title>2. Related Works</title><sec id="sec2dot1-sensors-25-01117"><title>2.1. CNN-Based Segmentation</title><p>Deep learning-powered segmentation algorithms, particularly CNNs, have become the vanguard, obviating the need for manual feature design by automating feature extraction and task-specific segmentation [<xref rid="B14-sensors-25-01117" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01117" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-01117" ref-type="bibr">16</xref>]. The Fully Convolutional Network (FCN) revolutionizes CNNs by introducing convolutional layers in place of fully connected ones and employing transposed convolution for pixel-wise classification, while U-Net further refines this by integrating more up-sampling and skip connections, significantly enhancing segmentation precision and achieving wide success in medical applications.</p><p>As deep learning progresses steadily, with Zhang et al. [<xref rid="B17-sensors-25-01117" ref-type="bibr">17</xref>] having introduced SAU-Net, excellent experimental results have been obtained in brain tumor datasets. Wu et al. [<xref rid="B18-sensors-25-01117" ref-type="bibr">18</xref>] introduced a segment-anything network (MSA) adapted to medical images, which is able to perform outstandingly in 19 medical image segmentation tasks, surpassing the current leading medical image segmentation networks. Zhao et al. [<xref rid="B19-sensors-25-01117" ref-type="bibr">19</xref>] introduced DSU-Net, which clearly considers the information of the fuzzy region and applies the proposed distraction module to the process of local information extraction, which is more effective than other segmentation networks in lung tumor segmentation. Furthermore, Yang et al. [<xref rid="B20-sensors-25-01117" ref-type="bibr">20</xref>] introduced an efficient 3D U-Net network featuring the ResNet architecture, which is capable of learning richer global and local lung tumor representations, and obtains a segmentation Dice accuracy of 0.691 on the TCIA dataset.</p><p>Transformer architecture has been applied to computer vision tasks and combined with CNN networks, and has been introduced to medical image segmentation tasks, achieving outstanding results [<xref rid="B21-sensors-25-01117" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01117" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01117" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-01117" ref-type="bibr">24</xref>]. In particular, Wang et al. [<xref rid="B25-sensors-25-01117" ref-type="bibr">25</xref>] combined CNN and a transformer structure, with experiments on the BraTS 2020 and 2019 test sets achieving the best segmentation results.</p></sec><sec id="sec2dot2-sensors-25-01117"><title>2.2. Segmentation Based on the Attention Mechanism</title><p>The attention mechanism draws on the thinking mode of human visual signal processing [<xref rid="B26-sensors-25-01117" ref-type="bibr">26</xref>]. Attentional mechanisms capture dependencies of remote information and are widely used in computer vision, which allows models to selectively concentrate on various sections of the input sequence when performing sequence modeling or processing.</p><p>In recent years, researchers have persistently integrated the attention mechanism into medical image segmentation networks. Oktay et al. [<xref rid="B27-sensors-25-01117" ref-type="bibr">27</xref>] introduced Attention Gate, which filters the output of the U-Net down-sampling process through the AG and then connects it to the up-sampling result in order to eliminate noise and irrelevant information in the hopping process, and to decrease the computational overhead of the model. Li et al. [<xref rid="B28-sensors-25-01117" ref-type="bibr">28</xref>], inspired by Attention U-Net, proposed a connection-sensitive attention network for the task of accurate segmentation of retinal blood vessels. Qi et al. [<xref rid="B29-sensors-25-01117" ref-type="bibr">29</xref>] included a self-attention module in the coding and decoding network and added residual connectivity to the regular convolution and segmented a stroke dataset. Cheng et al. [<xref rid="B30-sensors-25-01117" ref-type="bibr">30</xref>] used masked attention in the transformer decoder, which was able to converge faster and improve performance, outperforming the current network in different segmentation datasets. The use of different attention mechanisms on different medical image segmentation tasks gives better segmentation results, especially in how the coordinate attention mechanism locates the target object position by dividing the feature map into horizontal and vertical directions, which allows the network to take into account inter-channel relationships as well as long-distance positional information in the image feature extraction stage, and by using pixel positional information, it allows the model to be more sensitive to the different location pixels, which is a great advantage in image segmentation tasks.</p></sec><sec id="sec2dot3-sensors-25-01117"><title>2.3. Segmentation Based on Hybrid Loss Function</title><p>Choosing the right loss function is very important for training the model and enhancing performance. And the design of a mixed loss function can combine the advantages of two or more loss functions to achieve better model performance and generalization ability. Mixed loss functions can come in the form of simple linear weighted combinations or more complex forms such as in applying different loss functions according to different stages or conditions.</p><p>Zulfiqar et al. [<xref rid="B31-sensors-25-01117" ref-type="bibr">31</xref>] introduced a segmentation network, DRU-Net; the performance of the network is improved by the implemented hybrid loss function, which works well for the segmentation of pulmonary arteries. Zhang et al. [<xref rid="B32-sensors-25-01117" ref-type="bibr">32</xref>] introduced a hybrid loss function in a task on the segmentation of veins in magnetically sensitive weighted imaging (SWI) which contains accurate classification and a hybrid loss function with a global region overlap term, and the best Dice coefficient value of 0.756 was obtained experimentally. Tan et al. [<xref rid="B33-sensors-25-01117" ref-type="bibr">33</xref>] introduced a hybrid loss function to solve the fuzzy boundaries and heterogeneous pathological sensitivity problem in the task of segmenting the liver, and completed the Sliver07 challenge with a best score of 82.55. Zhang et al. [<xref rid="B34-sensors-25-01117" ref-type="bibr">34</xref>] introduced a DFL &#x02212; UNet + CBAM-based method for spot segmentation and disease identification, and the outcomes demonstrated superior segmentation results compared to other networks. Yousefirizi et al. [<xref rid="B35-sensors-25-01117" ref-type="bibr">35</xref>] obtained the best performance in a lymphoma lesion segmentation task with a hybrid loss function incorporating distribution, region, and boundary loss components.</p><p>From the above research, it can be seen that deep learning segmentation models enable the automated segmentation of medical images. However, due to the problem that medical images exhibit multimodality, unclear organizational boundaries and different sizes of segmentation regions, a basic segmentation network cannot obtain effective segmentation results. In this paper, we chose the PSPNet network with multi-scale feature fusion, a pyramid pooling operation and a fusion of local and global features, and employ a coordinate attention mechanism instead of the conventional global attention mechanism; the number of parameters used is smaller, decreasing computational overhead. This reduction facilitates faster model inference times, which is crucial for clinical applications where rapid diagnosis is often required. The hybrid loss function is utilized to effectively address issues such as category imbalance and the inaccurate segmentation of small targets.</p></sec></sec><sec sec-type="methods" id="sec3-sensors-25-01117"><title>3. Methods</title><sec id="sec3dot1-sensors-25-01117"><title>3.1. Architecture Overview</title><p>Semantic segmentation networks such as U-Net, Deeplab, and HrNet are commonly used. However, these networks lose the detailed information of the image after continuous pooling and convolutional operations, which affects the final segmentation accuracy. Initially, PSPNet was employed in the task of scene parsing for semantic segmentation, and, by using the pyramid pooling module, enhances the semantic information contained within the shallow features, so that the segmentation in the shallow layer has enough contextual information and the detailed information of the target&#x02019;s. Meanwhile, PSPNet also adopts the atrous convolution module, which uses different atrous rates to expand the sensory field, capturing multi-scale contextual information, so that spatial accuracy can be effectively improved.</p><p>Although PSPNet uses a pyramid pooling operation to alleviate the problem of context information loss to some extent, the results of segmentation for PSPNet still fail to achieve the expected results due to the problems of small-target omission and organizing the fuzzy boundaries of target in the task of medical image segmentation. Based on this problem, this paper improves on the basis of a PSPNet network. Firstly, coordinate attention is introduced before and after the backbone feature extraction network to locate the position of the features, so that the model can more accurately focus on the significance of various positions, thus enhancing the model&#x02019;s performance at critical locations, such as regions of interest and object edges. Secondly, a mixed loss function, Mixed Loss, is employed in the network to address the issue of data imbalance. Lastly, the ResNet backbone feature extraction network is substituted with MobilenetV2, to improve the model segmentation accuracy while decreasing the number of model parameters and saving computational resources. An overview of CAML-PSPNet is shown in <xref rid="sensors-25-01117-f001" ref-type="fig">Figure 1</xref>.</p><p>The network is segmented into four components: the input image, feature extraction, pyramid pooling and the segmentation result output. The input image enters into the backbone network MobilenetV2 to obtain a shallow feature map, and a coordinate attention mechanism is incorporated at both the beginning and end of this feature map, which calculates the weight of each feature point by introducing the position information, thus enabling enhanced focus on the local region of the entire image. Subsequently, the feature maps undergo pooling operations of various sizes within the pyramid pooling module, i.e., the input incoming feature layer is divided into 1 &#x000d7; 1, 2 &#x000d7; 2, 3 &#x000d7; 3, and 6 &#x000d7; 6 sub-regions, respectively, and then each sub-region is pooled on average. Finally, to ensure uniform size, the fused feature maps from the 1 &#x000d7; 1, 2 &#x000d7; 2, 3 &#x000d7; 3, and 6 &#x000d7; 6 layers are resized using up-sampling, and fused with the shallow features extracted from the backbone features, and the final segmentation results are derived following the convolution operation.</p></sec><sec id="sec3dot2-sensors-25-01117"><title>3.2. Coordinate Attention</title><p>Coordinate attention effectively incorporates positional information into channel attention, rendering it a novel and potent attention mechanism. The maps capture long-range dependencies along spatial directions, maintaining location details. By introducing the coordinate encoding and attention mechanism, coordinate attention enables the model to more effectively concentrate on various locations within the image and weight the features according to the importance of the location, thus improving the model&#x02019;s capacity to perceive and represent specific locations, to achieve better performance in image processing tasks, as shown in <xref rid="sensors-25-01117-f002" ref-type="fig">Figure 2</xref>.</p><p>The coordinate attention mechanism involves two steps: coordinate attention generation and coordinate information embedding.</p><p>Coordinate information embedding: to effectively capture distant spatial interactions with accurate location details, the global average pooling is decomposed, and given the following input: F &#x02208; <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The feature maps are pooled in both the X- and Y-directions using two pooling kernels&#x02014;(H, 1) and (1, W)&#x02014;to generate the feature maps of <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi mathvariant="normal">W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The feature maps are as follows:<disp-formula id="FD1-sensors-25-01117"><label>(1)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>X</mml:mi><mml:mo>:</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msubsup><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msub><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-01117"><label>(2)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>Y</mml:mi><mml:mo>:</mml:mo><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:msub><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>&#x02264;</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mtext>&#x000a0;</mml:mtext><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Coordinate attention generation: the embedded feature maps <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> are spliced along the spatial dimension after a 1 &#x000d7; 1 convolutional transformation and activated, as in Equation (3), followed by the separation of the two feature maps along the spatial dimensions, respectively, of <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>r</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext>&#x000a0;</mml:mtext><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>r</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The resulting attention vector is obtained by combination with the sigmoid activation functions, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mtext>&#x000a0;</mml:mtext><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. The final output formula of coordinate attention combines the input with the results from the X-direction and Y-direction operations, expressed as follows:<disp-formula id="FD3-sensors-25-01117"><label>(3)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-01117"><label>(4)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msup><mml:mfenced separators="|"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formula, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003b4;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denote the activation functions, respectively, and r is the reduction factor.</p></sec><sec id="sec3dot3-sensors-25-01117"><title>3.3. Mixed Loss Function</title><p>A mixed loss function is a way to use several different types of loss functions to comprehensively evaluate a model. Combining the Dice coefficient and Focal Loss enables the creation of a comprehensive hybrid loss function. The Dice coefficient quantifies the similarity of overlapping regions. The Dice coefficient loss function as follows:<disp-formula id="FD5-sensors-25-01117"><label>(5)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (5):<inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the result of the <italic toggle="yes">i</italic>th pixel in the CT image undergoing manual segmentation by the expert;<inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the network&#x02019;s predicted result for the <italic toggle="yes">i</italic>th pixel to belong to the target class; <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of pixels in the segmented CT image. However, the Dice coefficient loss function is prone to bias for data with unbalanced categories, which tends to cause the model to focus excessively on large categories and ignore small categories. To solve this problem, Focal Loss is introduced:<disp-formula id="FD6-sensors-25-01117"><label>(6)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>F</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Equation (6): <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the balancing factor; <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">&#x003b3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the adjustment factor, <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability value predicted by the model. Focal Loss solves the problem of category imbalance with an adjustable hyperparameter, thereby enhancing the model&#x02019;s categorization effect for a few categories.</p><p>Taking into full consideration the advantages of the two loss functions, the experimental process of this paper employs Mixed Loss:<disp-formula id="FD7-sensors-25-01117"><label>(7)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:mi>F</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mtext>&#x000a0;</mml:mtext><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The trade-off factor &#x003bb; is used to balance the weights. By adjusting the value range of &#x003bb;, the trade-off relationship between similarity and category imbalance can be adjusted. Because the target regions in medical images constitute a smaller portion of the overall image, in using the cross-entropy loss function, it is easy to be influenced by the background region, which reduces training efficiency. The mixed loss function is used to optimize the difficult-to-learn samples in the process of network backpropagation in a stable and targeted way, and the weight is reduced by Focal Loss, thus enabling the model to allocate greater attention to challenging samples and preserve intricate boundary details, and at the same time, the imbalance of pixel categories is solved by Dice Loss, which alleviates the noise caused by Focal Loss to a certain degree.</p></sec></sec><sec id="sec4-sensors-25-01117"><title>4. Experimental Results and Discussion</title><sec id="sec4dot1-sensors-25-01117"><title>4.1. Datasets</title><p>The CT data for lung tumor segmentation used in the experiments are from Shanghai Pulmonary Hospital. To validate the capability of CAML-PSPNet in segmentation tasks, the segmentation performance is evaluated. Two publicly available datasets, the polyp segmentation data from colonoscopy cases, Kvasir-SEG [<xref rid="B36-sensors-25-01117" ref-type="bibr">36</xref>], and the skin lesion segmentation data from dermatoscopy cases, ISIC 2017 [<xref rid="B37-sensors-25-01117" ref-type="bibr">37</xref>], are selected for the comparison experiments. <xref rid="sensors-25-01117-t001" ref-type="table">Table 1</xref> shows the data details:</p><p>In the experimental data processing, firstly, to adapt the network requirements, the original medical image data were processed in different formats, the value of the background pixel point of the label of the original data, 0, and the value of the target pixel point, 255, were modified to 0 and 1, respectively. Secondly, the original sizes of the images of each dataset were different, and before inputting the data into the model, the image size was uniformly resized to 473 &#x000d7; 473 to fit the model training. Finally, data augmentation on the training dataset took place to improve the effect of training and improve the model generalization, which is mainly divided into the random flipping and rotating of the images, and, at the same time, the dataset is divided into the test set, validation set and training set, with a ratio of 1:1:8.</p></sec><sec id="sec4dot2-sensors-25-01117"><title>4.2. Experimental Environment and Parameter Settings</title><p>In this paper, the algorithm is implemented in the Python programming language and based on PyTorch 1.6.0. Meanwhile, the experiments are carried out under the Ubuntu 18.04.6 LTS operating system. The hardware for the experiments is the NVIDIA GeForce RTX 2080, which has 32 G of RAM. To optimize the model training process, the Stochastic Gradient Descent (SGD) algorithm is chosen. Moreover, the epoch of the experiment is set to 150 and the batch-size is set to 8.</p></sec><sec id="sec4dot3-sensors-25-01117"><title>4.3. Evaluation Indicators</title><p>In the image segmentation task, evaluation metrics are employed to quantify the similarity and accuracy. In order to quantitatively evaluate the segmentation result of the network, four common evaluation metrics are chosen: mean intersection over union (mIoU), mean pixel accuracy (mPA), accuracy and Recall.</p><p>Here, mIoU is used to evaluate the performance of semantic segmentation models:<disp-formula id="FD8-sensors-25-01117"><label>(8)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>I</mml:mi><mml:mi>O</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>IoU calculates the degree of overlap between the predicted results and the real labels, which is used to measure the model&#x02019;s localization accuracy for the target region.</p><p>The mPA is used for semantic segmentation:<disp-formula id="FD9-sensors-25-01117"><label>(9)</label><mml:math id="mm24" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The mPA indicates the number of pixels correctly predicted. In simple terms, it measures the overall accuracy for all pixel classifications.</p><p>Precision is a metric used to evaluate classification models:<disp-formula id="FD10-sensors-25-01117"><label>(10)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Precision measures the proportion of samples predicted by the model to be in the positive category that are actually in the positive category.</p><p>Recall is a metric used to evaluate the performance of classification models and can also be used for semantic segmentation tasks:<disp-formula id="FD11-sensors-25-01117"><label>(11)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Recall represents the number of samples correctly predicted by the model as positive cases as a proportion of the total number of actual positive class samples. It measures the model&#x02019;s ability to detect the target class.</p><p>Here, N<sub>TP</sub> denotes pixels predicted to be in the positive category and labeled in the positive category; N<sub>TN</sub> denotes pixels predicted to be in the negative category and labeled in the negative category; N<sub>FP</sub> denotes pixels predicted to be in the positive category and labeled in the negative category; N<sub>FN</sub> denotes pixels predicted to be in the negative category but labeled in the positive category.</p></sec><sec id="sec4dot4-sensors-25-01117"><title>4.4. Results and Analysis</title><sec id="sec4dot4dot1-sensors-25-01117"><title>4.4.1. PrivateLT Segmentation Experiments</title><p>The PrivateLT dataset contains 1008 CT images with different original image sizes, which are uniformly resized to 473 &#x000d7; 473. The proposed network CAML-PSPNet is experimentally compared with the currently popular Deeplabv3, HrNet, UNet and PSPNet networks. In <xref rid="sensors-25-01117-t002" ref-type="table">Table 2</xref>, it can be seen that the proposed algorithm surpasses the other algorithms in four evaluation metrics. Its mIOU, mPA, Precision and Recall reach 73.2%, 85.19%, 83.87% and 85.19%, respectively, and the mIOU metrics are improved by 2.84%, 3.1%, 5.4% and 3.08%, respectively, compared to the Deeplabv3, HrNet, Unet and PSPNet networks. Comprehensive comparative analysis shows that the segmentation performance is better than other algorithms.</p><p>In <xref rid="sensors-25-01117-f003" ref-type="fig">Figure 3</xref>, the viewable area of the segmentation effect of PrivateLT is shown, and it can be seen that in the segmentation results of groups (a) and (b), the Deeplabv3, HrNet, UNet and PSPNet networks have the problems of unclear boundaries and large differences when compared with the labels, whereas the proposed CAML-PSPNet is able to more accurately segment the target, and it can obtain more consistent results in the segmentation of the tumor boundary, which proves the method is effective in solving the problem of unclear tumor boundaries. In the (c) group of segmentation visualization graphs, the Deeplabv3 network fails to segment the tumor location due to the small tumor region, the HrNet, UNet and PSPNet networks obviously have under-segmentation problems and the tumor edge segmentation is rougher, while CAML-PSPNet is able to segment the tumor region to a larger extent, indicating the effectiveness of the method for segmenting small targets. Compared with the existing popular segmentation models, the proposed algorithm has certain advantages.</p></sec><sec id="sec4dot4dot2-sensors-25-01117"><title>4.4.2. Kvasir-SEG Polyp Segmentation Experiments</title><p>Kvasir-SEG is a polyp dataset for medical image segmentation; 1001 colonoscopy images were selected and their size was processed to 473 &#x000d7; 473 dimensions for the experiment. <xref rid="sensors-25-01117-t003" ref-type="table">Table 3</xref> shows that the proposed algorithm has effective segmentation results when compared with other commonly used segmentation networks. In the visualization in <xref rid="sensors-25-01117-f004" ref-type="fig">Figure 4</xref>, polyps of different sizes, shapes and complexities, such as with blurred boundaries, can be accurately localized to obtain more precise segmentation results.</p></sec><sec id="sec4dot4dot3-sensors-25-01117"><title>4.4.3. ISIC 2017 Dermatology Segmentation Experiment</title><p>ISIC 2017 is a dermatologic dataset, and 960 dermoscopic images were selected for comparison experiments. <xref rid="sensors-25-01117-t004" ref-type="table">Table 4</xref> shows that the proposed algorithm surpasses other algorithms in the evaluation metrics of mIoU, mPA, Precision and Recall. From the segmentation visualization in <xref rid="sensors-25-01117-f005" ref-type="fig">Figure 5</xref>, the segmentation of the CAML-PSPNet network is closest to the labeled graph, especially at the edge of the tumor, where the segmentation effect is more accurate than other networks.</p></sec><sec id="sec4dot4dot4-sensors-25-01117"><title>4.4.4. Comparative Networks</title><p>In this research, the compared networks predominantly consist of fundamental networks such as Deeplabv3, HrNet, UNet and PSPNet. These networks have long exerted a substantial influence within the domain of segmentation. To further validate the effectiveness of the proposed network in the specialized field of medical image segmentation, a novel round of comparison is meticulously executed. Specifically, the networks Swin-UNETR and TransUNet, which have incorporated the transformer architecture in recent years, are selected for in-depth comparative experiments.</p><p>During these experiments, the mean intersection over mIOU and Precision are chosen as the key verification indicators. As presented in <xref rid="sensors-25-01117-t005" ref-type="table">Table 5</xref>, it is evident that the proposed CAML-PSPNet network not only outperforms but also attains the optimal results in both the mIOU and Precision evaluation metrics. In the visual representation of the segmentation results, depicted in <xref rid="sensors-25-01117-f006" ref-type="fig">Figure 6</xref>, the segmentation effects of the two compared networks, in terms of mIOU and Precision, clearly exhibit discernible disparities when compared with those of the proposed network.</p></sec><sec id="sec4dot4dot5-sensors-25-01117"><title>4.4.5. Ablation Experiments</title><p>The proposed CAML-PSPNet algorithm network model is an improvement of the PSPNet network. To verify the effectiveness of using the coordinate attention mechanism and Mixed Loss for the segmentation task, ablation experiments were performed on PrivateLT. The PSPNet network was used as the baseline network to which no other modules were added. First, Dice Loss + Focal Loss (Mixed Loss) was tested in the PSPNet network, i.e., PSPNet+Mixed Loss; second, coordinate attention was introduced into the PSPNet for the experiments, i.e., PSPNet+Att; and lastly, the Mixed Loss function and coordinate attention were simultaneously applied to the PSPNet network for experimentation, i.e., CAML-PSPNet, the model proposed.</p><p><xref rid="sensors-25-01117-t006" ref-type="table">Table 6</xref> shows that the Mixed Loss and coordinate attention mechanism provide significant improvement in all evaluation metrics of the PrivateLT segmentation task. Among them, compared with the benchmark model PSPNet, the network with the Mixed Loss improved its mIoU, mPA, Precision and Recall by 1.29%, 0.23%, 2.7% and 0.23%. Incorporating the coordinate attention module into the baseline model, the evaluation metric mIoU, mPA, Precision and Recall are measured, and improved by 2.65%, 0.36%, 3.15% and 0.36%, indicating that the adopted coordinate attention module enables the model to acquire the location of the tumor in order to enhance the segmentation accuracy. Finally, applying the above two strategies to PSPNet, all the metrics have been greatly improved, with mIoU, mPA, Precision and Recall improved by 3.08%, 0.32%, 3.74% and 0.32%, respectively.</p><p>In conclusion, compared to the original PSPNet network, the segmentation results with both the Mixed Loss and coordinate attention are somewhat improved, making the final segmentation results closer to the labeled graph. The effectiveness of adding a hybrid loss function and coordinate attention on segmentation in a PSPNet network is proved.</p></sec><sec id="sec4dot4dot6-sensors-25-01117"><title>4.4.6. Effect of Loss Function on Segmentation Accuracy</title><p>To test the degree of influence of the adopted Mixed Loss on the segmentation accuracy, the magnitude of the trade-off factor &#x003bb; is discussed in this paper. First, a step size of 0.1 is used to obtain the value of &#x003bb;, and the tested mIOU results are obtained as shown in <xref rid="sensors-25-01117-f007" ref-type="fig">Figure 7</xref>, where it can be seen that the trade-off factor &#x003bb; takes on different values, and the obtained values of the mIOU are also different. When &#x003bb; takes the value of 0, the result of mIOU is only 69.6%, and with the gradual increase in &#x003bb; value, the value of mIOU rises rapidly, and the segmentation result reaches the maximum value of 71.40% when the value of &#x003bb; is selected as 0.6. By examining the impact of the &#x003bb; value on the segmentation results, we can assess its influence. Finally, the &#x003bb; value is set to 0.6.</p></sec><sec id="sec4dot4dot7-sensors-25-01117"><title>4.4.7. Comparison of Number of Parameters in Different Networks</title><p>In order to verify that the proposed network has the advantages of saving computational resources, the parametric counting for the model is carried out, and the parametric counting experiments were carried out on CAML-PSPNet and other networks, with the statistical results shown in <xref rid="sensors-25-01117-f008" ref-type="fig">Figure 8</xref>. In comparison with the Deeplabv3, HrNet, UNet and PSPNet networks, the proposed network CAML-PSPNet has only 2.39 M parameters, without this affecting the segmentation accuracy. To further verify that using MobileNetV2 as the backbone network can significantly reduce the number of parameters of the proposed model, <xref rid="sensors-25-01117-f009" ref-type="fig">Figure 9</xref> shows the number of parameters of the proposed model when the backbone network is replaced with ResNet-34, ResNet-50, and MobileNetV2, respectively. The results indicate that the use of MobilenetV2 as the backbone network of the model is able to reduce the count of parameters of the model effectively and save a lot of computational resources.</p></sec><sec id="sec4dot4dot8-sensors-25-01117"><title>4.4.8. Limitations and Future Prospects</title><p>During our research, we had limited access to high-performance computing resources. This restricted the scale of experiments we could conduct, especially when training more complex models. As a result, our model may not be as optimized as it could be. We have recently secured access to a more powerful computing cluster. This will enable us to train more advanced models, perform more extensive hyperparameter tuning and conduct in-depth sensitivity analyses.</p></sec><sec id="sec4dot4dot9-sensors-25-01117"><title>4.4.9. Discussion</title><p>In the research of medical image segmentation, errors caused by the model structure occurred. We found that for some image regions with complex textures and blurred boundaries, the model&#x02019;s segmentation performance was not satisfactory. Through analysis, this was due to the limited receptive field of the convolutional layers in the model, which was unable to fully capture the global information of these regions. Meanwhile, we tested the model on three different medical image datasets. The PrivateLT dataset contains a large number of high-resolution lung CT images, the Kvasir-SEG dataset consists of low-resolution Colonoscopy images and the ISIC 2017 dataset is composed of dermatoscope images that are susceptible to external influences. The test results showed that on the PrivateLT dataset, the average error rate of the model was 5%, and the main errors were concentrated in the missed detection of small lung lesions. On the Kvasir-SEG dataset, the error rate increased to 7%. Due to the low image resolution, the model inaccurately judged the size and location of rectal lesion areas. On the ISIC 2017 dataset, the error rate was as high as 11%. This was because the surface of dermatoscope images was easily affected by factors such as hair and oil, and the lesion shapes were diverse, resulting in more errors during the model&#x02019;s segmentation. Through such comparisons, we can more clearly understand the impact of factors such as data resolution, noise level and image type on the model error.</p><p>In the comparative experiments, we selected two networks, Swin-UNETR and TransUNet, which have emerged in recent years, for comparative experiments. The results show that the mIOU of these two networks on the datasets PrivateLT, Kvasir-SEG and ISIC 2017 is 63.72%, 71.60%, and 80.56% and 65.22%, 73.55%, and 81.20%, respectively, which is significantly lower than the segmentation results of the proposed network. The segmentation Precision is also much lower than that of the proposed network. Meanwhile, it can be seen from the visual diagrams of the segmentation results of different networks that there are still certain differences between the segmentation of the target by the Swin-UNETR and TransUNet networks and the label values. We believe that the computational resource limitations have affected the experimental results to some extent. The transformer frameworks adopted by the Swin-UNETR and TransUNet networks often require a sufficient amount of data and a large number of training epochs. However, this will increase the training time and there is a problem of training interruption. This requires higher-performance GPU resources and larger-capacity memory resources.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01117"><title>5. Conclusions</title><p>This paper investigates the application of CAML-PSPNet in medical image segmentation tasks. Aiming at the problems that usually exist for medical images, such as the problem of missed segmentation caused by a small target region and the low segmentation accuracy caused by the fuzzy boundary of the target, which is difficult to distinguish, this study proposes a method based on a CAML-PSPNet network. This model stands to significantly enhance segmentation performance, offering a robust tool for doctors in the interpretation of complex imaging data and ultimately contributing to improved diagnostic accuracy.</p><p>CAML-PSPNet solves the medical image segmentation task by introducing a coordinate attention mechanism and Mixed Loss, a hybrid loss function. Firstly, in the mixed loss function, Dice Loss serves as a metric to quantify the resemblance between the predicted outcomes and the actual labels to make the model more convergent. Secondly, the coordinate attention mechanism makes the network place greater emphasis on the characteristics of the target region. By fusing the position coordinate information with the input features, the model has the capability to assign varying attention weights to pixels located in different positions. In addition, to minimize computational expenses and conserve computational resources, the experiments utilize MobilenetV2 as the backbone, which has the characteristics of being lightweight and a high computational efficiency, which further boosts the network&#x02019;s performance.</p><p>To validate the efficacy of CAML-PSPNet, three sets of experiments were carried out. First, CAML-PSPNet was compared with the existing Deeplabv3, HrNet, UNet and PSPNet methods on the private dataset of PrivateLT; the results show that the method is able to effectively solve the problems of missed segmentation caused by the small tumor region and inaccurate segmentation caused by the blurring of tumor boundaries. Then, comparison experiments were performed on two publicly accessible datasets, Kvasir-SEG rectal polyps and ISIC 2017 dermatology, and the results demonstrate that the method still achieves effective segmentation results on these two datasets and is significantly higher than the segmentation accuracies of other networks, which proves the proposed method is valid.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Methodology, H.W.; Software, Y.L.; Data curation, P.L.; Writing&#x02014;original draft, Y.L.; Writing&#x02014;review &#x00026; editing, X.G. and Z.F.; Supervision, H.W. and Z.F. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>PrivateLT data are available from the corresponding authors on reasonable request, and other data underlying the results presented in this paper are available in Refs. [<xref rid="B36-sensors-25-01117" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-01117" ref-type="bibr">37</xref>].</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01117"><label>1.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Cardenas</surname><given-names>C.E.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Anderson</surname><given-names>B.M.</given-names></name>
<name><surname>Court</surname><given-names>L.E.</given-names></name>
<name><surname>Brock</surname><given-names>K.B.</given-names></name>
</person-group><article-title>Advances in auto-segmentation</article-title><source>Seminars in Radiation Oncology</source><publisher-name>WB Saunders</publisher-name><publisher-loc>Philadelphia, PA, USA</publisher-loc><year>2019</year><volume>Volume 29</volume><fpage>185</fpage><lpage>197</lpage></element-citation></ref><ref id="B2-sensors-25-01117"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hesamian</surname><given-names>M.H.</given-names></name>
<name><surname>Jia</surname><given-names>W.</given-names></name>
<name><surname>He</surname><given-names>X.</given-names></name>
<name><surname>Kennedy</surname><given-names>P.</given-names></name>
</person-group><article-title>Deep learning techniques for medical image segmentation: Achievements and challenges</article-title><source>J. Digit. Imaging</source><year>2019</year><volume>32</volume><fpage>582</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1007/s10278-019-00227-x</pub-id><pub-id pub-id-type="pmid">31144149</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01117"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>R.</given-names></name>
<name><surname>Lei</surname><given-names>T.</given-names></name>
<name><surname>Cui</surname><given-names>R.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>Meng</surname><given-names>H.</given-names></name>
<name><surname>Nandi</surname><given-names>A.K.</given-names></name>
</person-group><article-title>Medical image segmentation using deep learning: A survey</article-title><source>IET Image Process.</source><year>2022</year><volume>16</volume><fpage>1243</fpage><lpage>1267</lpage><pub-id pub-id-type="doi">10.1049/ipr2.12419</pub-id></element-citation></ref><ref id="B4-sensors-25-01117"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Long</surname><given-names>J.</given-names></name>
<name><surname>Shelhamer</surname><given-names>E.</given-names></name>
<name><surname>Darrell</surname><given-names>T.</given-names></name>
</person-group><article-title>Fully convolutional networks for semantic segmentation</article-title><source>Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Boston, MA, USA</conf-loc><conf-date>7&#x02013;12 June 2015</conf-date><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref><ref id="B5-sensors-25-01117"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Badrinarayanan</surname><given-names>V.</given-names></name>
<name><surname>Kendall</surname><given-names>A.</given-names></name>
<name><surname>Cipolla</surname><given-names>R.</given-names></name>
</person-group><article-title>Segnet: A deep convolutional encoder-decoder architecture for image segmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>39</volume><fpage>2481</fpage><lpage>2495</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id><pub-id pub-id-type="pmid">28060704</pub-id>
</element-citation></ref><ref id="B6-sensors-25-01117"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ronneberger</surname><given-names>O.</given-names></name>
<name><surname>Fischer</surname><given-names>P.</given-names></name>
<name><surname>Brox</surname><given-names>T.</given-names></name>
</person-group><article-title>U-net: Convolutional networks for biomedical image segmentation</article-title><source>Proceedings of the Medical Image Computing and Computer-Assisted Intervention&#x02013;MICCAI 2015: 18th international Conference</source><conf-loc>Munich, Germany</conf-loc><conf-date>5&#x02013;9 October 2015</conf-date><comment>Proceedings, Part III 18</comment><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><year>2015</year><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="B7-sensors-25-01117"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>L.C.</given-names></name>
<name><surname>Papandreou</surname><given-names>G.</given-names></name>
<name><surname>Kokkinos</surname><given-names>I.</given-names></name>
<name><surname>Murphy</surname><given-names>K.</given-names></name>
<name><surname>Yuille</surname><given-names>A.L.</given-names></name>
</person-group><article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2017</year><volume>40</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><pub-id pub-id-type="pmid">28463186</pub-id>
</element-citation></ref><ref id="B8-sensors-25-01117"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Shi</surname><given-names>J.</given-names></name>
<name><surname>Qi</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Jia</surname><given-names>J.</given-names></name>
</person-group><article-title>Pyramid scene parsing network</article-title><source>Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Honolulu, HI, USA</conf-loc><conf-date>21&#x02013;26 July 2017</conf-date><fpage>2881</fpage><lpage>2890</lpage></element-citation></ref><ref id="B9-sensors-25-01117"><label>9.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>D.</given-names></name>
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
</person-group><source>AM-PSPNet: Pyramid Scene Parsing Network Based on Attentional Mechanism for Image Semantic Segmentation</source><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2022</year><pub-id pub-id-type="doi">10.1007/978-981-19-5194-7_32</pub-id></element-citation></ref><ref id="B10-sensors-25-01117"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Cheng</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Lu</surname><given-names>G.</given-names></name>
</person-group><article-title>Coronary angiography image segmentation based on PSPNet</article-title><source>Comput. Methods Programs Biomed.</source><year>2021</year><volume>200</volume><elocation-id>105897</elocation-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105897</pub-id><pub-id pub-id-type="pmid">33317873</pub-id>
</element-citation></ref><ref id="B11-sensors-25-01117"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Waswani</surname><given-names>A.</given-names></name>
<name><surname>Shazeer</surname><given-names>N.</given-names></name>
<name><surname>Parmar</surname><given-names>N.</given-names></name>
<name><surname>Uszkoreit</surname><given-names>J.</given-names></name>
<name><surname>Jones</surname><given-names>L.</given-names></name>
<name><surname>Gomez</surname><given-names>A.</given-names></name>
<name><surname>Kaiser</surname><given-names>L.</given-names></name>
<name><surname>Polosukhin</surname><given-names>I.</given-names></name>
</person-group><article-title>Attention is all you need</article-title><source>Proceedings of the Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>4&#x02013;9 December 2017</conf-date></element-citation></ref><ref id="B12-sensors-25-01117"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hou</surname><given-names>Q.</given-names></name>
<name><surname>Zhou</surname><given-names>D.</given-names></name>
<name><surname>Feng</surname><given-names>J.</given-names></name>
</person-group><article-title>Coordinate attention for efficient mobile network design</article-title><source>Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#x02013;25 June 2021</conf-date><fpage>13713</fpage><lpage>13722</lpage></element-citation></ref><ref id="B13-sensors-25-01117"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Barron</surname><given-names>J.T.</given-names></name>
</person-group><article-title>A general and adaptive robust loss function</article-title><source>Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#x02013;20 June 2019</conf-date><fpage>4331</fpage><lpage>4339</lpage></element-citation></ref><ref id="B14-sensors-25-01117"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Abbani</surname><given-names>N.</given-names></name>
<name><surname>Baudier</surname><given-names>T.</given-names></name>
<name><surname>Rit</surname><given-names>S.</given-names></name>
<name><surname>di Franco</surname><given-names>F.</given-names></name>
<name><surname>Okoli</surname><given-names>F.</given-names></name>
<name><surname>Jaouen</surname><given-names>V.</given-names></name>
<name><surname>Tilquin</surname><given-names>F.</given-names></name>
<name><surname>Barateau</surname><given-names>A.</given-names></name>
<name><surname>Simon</surname><given-names>A.</given-names></name>
<name><surname>de Crevoisier</surname><given-names>R.</given-names></name>
<etal/>
</person-group><article-title>Deep learning-based segmentation in prostate radiation therapy using Monte Carlo simulated cone-beam computed tomography</article-title><source>Med. Phys.</source><year>2022</year><volume>49</volume><fpage>6930</fpage><lpage>6944</lpage><pub-id pub-id-type="doi">10.1002/mp.15946</pub-id><pub-id pub-id-type="pmid">36000762</pub-id>
</element-citation></ref><ref id="B15-sensors-25-01117"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sahiner</surname><given-names>B.</given-names></name>
<name><surname>Pezeshk</surname><given-names>A.</given-names></name>
<name><surname>Hadjiiski</surname><given-names>L.M.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Drukker</surname><given-names>K.</given-names></name>
<name><surname>Cha</surname><given-names>K.H.</given-names></name>
<name><surname>Summers</surname><given-names>R.M.</given-names></name>
<name><surname>Giger</surname><given-names>M.L.</given-names></name>
</person-group><article-title>Deep learning in medical imaging and radiation therapy</article-title><source>Med. Phys.</source><year>2019</year><volume>46</volume><fpage>e1</fpage><lpage>e36</lpage><pub-id pub-id-type="doi">10.1002/mp.13264</pub-id><pub-id pub-id-type="pmid">30367497</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01117"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Maier</surname><given-names>A.</given-names></name>
<name><surname>Syben</surname><given-names>C.</given-names></name>
<name><surname>Lasser</surname><given-names>T.</given-names></name>
<name><surname>Riess</surname><given-names>C.</given-names></name>
</person-group><article-title>A gentle introduction to deep learning in medical image processing</article-title><source>Z. F&#x000fc;r Med. Phys.</source><year>2019</year><volume>29</volume><fpage>86</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.zemedi.2018.12.003</pub-id></element-citation></ref><ref id="B17-sensors-25-01117"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Peng</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
</person-group><article-title>SAU-Net: Medical image segmentation method based on u-net and self-attention</article-title><source>Acta Electron. Sin.</source><year>2022</year><volume>50</volume><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="B18-sensors-25-01117"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Ji</surname><given-names>W.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Fu</surname><given-names>H.</given-names></name>
<name><surname>Xu</surname><given-names>M.</given-names></name>
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Jin</surname><given-names>Y.</given-names></name>
</person-group><article-title>Medical sam adapter: Adapting segment anything model for medical image segmentation</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2304.12620</pub-id></element-citation></ref><ref id="B19-sensors-25-01117"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Dang</surname><given-names>M.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Wan</surname><given-names>L.</given-names></name>
</person-group><article-title>DSU-Net: Distraction-Sensitive U-Net for 3D lung tumor segmentation</article-title><source>Eng. Appl. Artif. Intell.</source><year>2022</year><volume>109</volume><elocation-id>104649</elocation-id><pub-id pub-id-type="doi">10.1016/j.engappai.2021.104649</pub-id></element-citation></ref><ref id="B20-sensors-25-01117"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>B.</given-names></name>
<name><surname>Li</surname><given-names>L.</given-names></name>
<name><surname>Cao</surname><given-names>P.</given-names></name>
<name><surname>Zaiane</surname><given-names>O.</given-names></name>
</person-group><article-title>MSDS-UNet: A multi-scale deeply supervised 3D U-Net for automatic segmentation of lung tumor in CT</article-title><source>Comput. Med. Imaging Graph.</source><year>2021</year><volume>92</volume><elocation-id>101957</elocation-id><pub-id pub-id-type="doi">10.1016/j.compmedimag.2021.101957</pub-id><pub-id pub-id-type="pmid">34325225</pub-id>
</element-citation></ref><ref id="B21-sensors-25-01117"><label>21.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Jia</surname><given-names>Q.</given-names></name>
<name><surname>Shu</surname><given-names>H.</given-names></name>
</person-group><article-title>Bitr-unet: A cnn-transformer combined network for mri brain tumor segmentation</article-title><source>International MICCAI Brainlesion Workshop</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><fpage>3</fpage><lpage>14</lpage></element-citation></ref><ref id="B22-sensors-25-01117"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Lu</surname><given-names>Y.</given-names></name>
<name><surname>Yu</surname><given-names>Q.</given-names></name>
<name><surname>Luo</surname><given-names>X.</given-names></name>
<name><surname>Adeli</surname><given-names>E.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Lu</surname><given-names>L.</given-names></name>
<name><surname>Yuille</surname><given-names>A.L.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
</person-group><article-title>Transunet: Transformers make strong encoders for medical image segmentation</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2102.04306</pub-id></element-citation></ref><ref id="B23-sensors-25-01117"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Lin</surname><given-names>X.</given-names></name>
<name><surname>Dong</surname><given-names>J.</given-names></name>
<name><surname>Cheng</surname><given-names>T.</given-names></name>
<name><surname>Liang</surname><given-names>J.</given-names></name>
</person-group><article-title>SwinBTS: A method for 3D multimodal brain tumor segmentation using swin transformer</article-title><source>Brain Sci.</source><year>2022</year><volume>12</volume><elocation-id>797</elocation-id><pub-id pub-id-type="doi">10.3390/brainsci12060797</pub-id><pub-id pub-id-type="pmid">35741682</pub-id>
</element-citation></ref><ref id="B24-sensors-25-01117"><label>24.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Hatamizadeh</surname><given-names>A.</given-names></name>
<name><surname>Nath</surname><given-names>V.</given-names></name>
<name><surname>Tang</surname><given-names>Y.</given-names></name>
<name><surname>Yang</surname><given-names>D.</given-names></name>
<name><surname>Roth</surname><given-names>H.R.</given-names></name>
<name><surname>Xu</surname><given-names>D.</given-names></name>
</person-group><article-title>Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images</article-title><source>International MICCAI Brainlesion Workshop</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><fpage>272</fpage><lpage>284</lpage></element-citation></ref><ref id="B25-sensors-25-01117"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wenxuan</surname><given-names>W.</given-names></name>
<name><surname>Chen</surname><given-names>C.</given-names></name>
<name><surname>Meng</surname><given-names>D.</given-names></name>
<name><surname>Hong</surname><given-names>Y.</given-names></name>
<name><surname>Sen</surname><given-names>Z.</given-names></name>
<name><surname>Jiangyun</surname><given-names>L.</given-names></name>
</person-group><article-title>Transbts: Multimodal brain tumor segmentation using transformer</article-title><source>Proceedings of the Medical Image Computing and Computer Assisted Intervention&#x02013;MICCAI 2021: 24th International Conference</source><conf-loc>Strasbourg, France</conf-loc><conf-date>27 September&#x02013;1 October 2021</conf-date><comment>Proceedings, Part I 24</comment><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><year>2021</year><fpage>109</fpage><lpage>119</lpage></element-citation></ref><ref id="B26-sensors-25-01117"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Itti</surname><given-names>L.</given-names></name>
<name><surname>Koch</surname><given-names>C.</given-names></name>
<name><surname>Niebur</surname><given-names>E.</given-names></name>
</person-group><article-title>A model of saliency-based visual attention for rapid scene analysis</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>1998</year><volume>20</volume><fpage>1254</fpage><lpage>1259</lpage><pub-id pub-id-type="doi">10.1109/34.730558</pub-id></element-citation></ref><ref id="B27-sensors-25-01117"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Oktay</surname><given-names>O.</given-names></name>
<name><surname>Schlemper</surname><given-names>J.</given-names></name>
<name><surname>Le Folgoc</surname><given-names>L.</given-names></name>
<name><surname>Lee</surname><given-names>M.</given-names></name>
<name><surname>Heinrich</surname><given-names>M.</given-names></name>
<name><surname>Misawa</surname><given-names>K.</given-names></name>
<name><surname>Mori</surname><given-names>K.</given-names></name>
<name><surname>McDonagh</surname><given-names>S.</given-names></name>
<name><surname>Hammerla</surname><given-names>N.Y.</given-names></name>
<name><surname>Kainz</surname><given-names>B.</given-names></name>
<etal/>
</person-group><article-title>Attention u-net: Learning where to look for the pancreas</article-title><source>arXiv</source><year>2018</year><pub-id pub-id-type="arxiv">1804.03999</pub-id></element-citation></ref><ref id="B28-sensors-25-01117"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>R.</given-names></name>
<name><surname>Li</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
</person-group><article-title>Connection sensitive attention U-NET for accurate retinal vessel segmentation</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1903.05558</pub-id></element-citation></ref><ref id="B29-sensors-25-01117"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Qi</surname><given-names>K.</given-names></name>
<name><surname>Yang</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>M.</given-names></name>
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
</person-group><article-title>X-net: Brain stroke lesion segmentation based on depthwise separable convolution and long-range dependencies</article-title><source>Proceedings of the Medical Image Computing and Computer Assisted Intervention&#x02013;MICCAI 2019: 22nd International Conference</source><conf-loc>Shenzhen, China</conf-loc><conf-date>13&#x02013;17 October 2019</conf-date><comment>Proceedings, Part III 22</comment><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><year>2019</year><fpage>247</fpage><lpage>255</lpage></element-citation></ref><ref id="B30-sensors-25-01117"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>B.</given-names></name>
<name><surname>Misra</surname><given-names>I.</given-names></name>
<name><surname>Schwing</surname><given-names>A.G.</given-names></name>
<name><surname>Kirillov</surname><given-names>A.</given-names></name>
<name><surname>Girdhar</surname><given-names>R.</given-names></name>
</person-group><article-title>Masked-attention mask transformer for universal image segmentation</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#x02013;24 June 2022</conf-date><fpage>1290</fpage><lpage>1299</lpage></element-citation></ref><ref id="B31-sensors-25-01117"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zulfiqar</surname><given-names>M.</given-names></name>
<name><surname>Stanuch</surname><given-names>M.</given-names></name>
<name><surname>Wodzinski</surname><given-names>M.</given-names></name>
<name><surname>Skalski</surname><given-names>A.</given-names></name>
</person-group><article-title>DRU-Net: Pulmonary Artery Segmentation via Dense Residual U-Network with Hybrid Loss Function</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>5427</elocation-id><pub-id pub-id-type="doi">10.3390/s23125427</pub-id><pub-id pub-id-type="pmid">37420595</pub-id>
</element-citation></ref><ref id="B32-sensors-25-01117"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Hu</surname><given-names>Q.</given-names></name>
</person-group><article-title>Deep learning-based vein segmentation from susceptibility-weighted images</article-title><source>Computing</source><year>2019</year><volume>101</volume><fpage>637</fpage><lpage>652</lpage><pub-id pub-id-type="doi">10.1007/s00607-018-0677-7</pub-id></element-citation></ref><ref id="B33-sensors-25-01117"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>M.</given-names></name>
<name><surname>Wu</surname><given-names>F.</given-names></name>
<name><surname>Kong</surname><given-names>D.</given-names></name>
<name><surname>Mao</surname><given-names>X.</given-names></name>
</person-group><article-title>Automatic Liver Segmentation Using 3D Convolutional Neural Networks with A Hybrid Loss Function</article-title><source>Med. Phys.</source><year>2021</year><volume>48</volume><fpage>1707</fpage><lpage>1719</lpage><pub-id pub-id-type="doi">10.1002/mp.14732</pub-id><pub-id pub-id-type="pmid">33496971</pub-id>
</element-citation></ref><ref id="B34-sensors-25-01117"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>D.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Sun</surname><given-names>T.</given-names></name>
<name><surname>Lin</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>Z.</given-names></name>
</person-group><article-title>Research of segmentation recognition of small disease spots on apple leaves based on hybrid loss function and CBAM</article-title><source>Front. Plant Sci.</source><year>2023</year><volume>14</volume><elocation-id>1175027</elocation-id><pub-id pub-id-type="doi">10.3389/fpls.2023.1175027</pub-id><pub-id pub-id-type="pmid">37346136</pub-id>
</element-citation></ref><ref id="B35-sensors-25-01117"><label>35.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Yousefirizi</surname><given-names>F.</given-names></name>
<name><surname>Dubljevic</surname><given-names>N.</given-names></name>
<name><surname>Ahamed</surname><given-names>S.</given-names></name>
<name><surname>Bloise</surname><given-names>I.</given-names></name>
<name><surname>Gowdy</surname><given-names>C.</given-names></name>
<name><surname>Hyun</surname><given-names>O.J.</given-names></name>
<name><surname>Farag</surname><given-names>Y.</given-names></name>
<name><surname>de Schaetzen</surname><given-names>R.</given-names></name>
<name><surname>Martineau</surname><given-names>P.</given-names></name>
<name><surname>Wilson</surname><given-names>D.</given-names></name>
<etal/>
</person-group><article-title>Convolutional neural network with a hybrid loss function for fully automated segmentation of lymphoma lesions in FDG PET images</article-title><source>Medical Imaging 2022: Image Processing</source><publisher-name>SPIE</publisher-name><publisher-loc>Bellingham, WA, USA</publisher-loc><year>2022</year><volume>Volume 12032</volume><fpage>214</fpage><lpage>220</lpage></element-citation></ref><ref id="B36-sensors-25-01117"><label>36.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Jha</surname><given-names>D.</given-names></name>
<name><surname>Smedsrud</surname><given-names>P.H.</given-names></name>
<name><surname>Riegler</surname><given-names>M.A.</given-names></name>
<name><surname>Halvorsen</surname><given-names>P.</given-names></name>
<name><surname>De Lange</surname><given-names>T.</given-names></name>
<name><surname>Johansen</surname><given-names>D.</given-names></name>
<name><surname>Johansen</surname><given-names>H.D.</given-names></name>
</person-group><article-title>Kvasir-seg: A segmented polyp dataset</article-title><source>Proceedings of the MultiMedia Modeling: 26th International Conference, MMM 2020</source><conf-loc>Daejeon, Republic of Korea</conf-loc><conf-date>5&#x02013;8 January 2020</conf-date><comment>Proceedings, Part II 26</comment><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Berlin, Germany</publisher-loc><year>2020</year><fpage>451</fpage><lpage>462</lpage></element-citation></ref><ref id="B37-sensors-25-01117"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Berseth</surname><given-names>M.</given-names></name>
</person-group><article-title>SIC 2017: Skin Lesion Analysis Towards Melanoma Detection</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1703.00523</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01117-f001"><label>Figure 1</label><caption><p>An overview of CAML-PSPNet.</p></caption><graphic xlink:href="sensors-25-01117-g001" position="float"/></fig><fig position="float" id="sensors-25-01117-f002"><label>Figure 2</label><caption><p>The module of coordinate attention.</p></caption><graphic xlink:href="sensors-25-01117-g002" position="float"/></fig><fig position="float" id="sensors-25-01117-f003"><label>Figure 3</label><caption><p>(<bold>a</bold>&#x02013;<bold>c</bold>) Qualitative comparison of different methods on PrivateLT datasets.</p></caption><graphic xlink:href="sensors-25-01117-g003" position="float"/></fig><fig position="float" id="sensors-25-01117-f004"><label>Figure 4</label><caption><p>Qualitative comparison of different methods on Kvasir-SEG datasets.</p></caption><graphic xlink:href="sensors-25-01117-g004" position="float"/></fig><fig position="float" id="sensors-25-01117-f005"><label>Figure 5</label><caption><p>Qualitative comparison of different methods on ISIC 2017 datasets.</p></caption><graphic xlink:href="sensors-25-01117-g005" position="float"/></fig><fig position="float" id="sensors-25-01117-f006"><label>Figure 6</label><caption><p>Qualitative comparison of comparative methods on three datasets.</p></caption><graphic xlink:href="sensors-25-01117-g006" position="float"/></fig><fig position="float" id="sensors-25-01117-f007"><label>Figure 7</label><caption><p>The trend in mIOU values as a function of the trade-off factor &#x003bb;.</p></caption><graphic xlink:href="sensors-25-01117-g007" position="float"/></fig><fig position="float" id="sensors-25-01117-f008"><label>Figure 8</label><caption><p>Comparison of different network parameters.</p></caption><graphic xlink:href="sensors-25-01117-g008" position="float"/></fig><fig position="float" id="sensors-25-01117-f009"><label>Figure 9</label><caption><p>Comparison of different backbone network parameters.</p></caption><graphic xlink:href="sensors-25-01117-g009" position="float"/></fig><table-wrap position="float" id="sensors-25-01117-t001"><object-id pub-id-type="pii">sensors-25-01117-t001_Table 1</object-id><label>Table 1</label><caption><p>Dataset details.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Datasets</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Modality</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Number of Images</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Image Size</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">PrivateLT</td><td align="center" valign="middle" rowspan="1" colspan="1">CT</td><td align="center" valign="middle" rowspan="1" colspan="1">1008</td><td align="center" valign="middle" rowspan="1" colspan="1">512 &#x000d7; 512</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Kvasir-SEG</td><td align="center" valign="middle" rowspan="1" colspan="1">Colonoscopy image</td><td align="center" valign="middle" rowspan="1" colspan="1">1001</td><td align="center" valign="middle" rowspan="1" colspan="1">Different sizes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ISIC 2017</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Skin lesions</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">960</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">256 &#x000d7; 256</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01117-t002"><object-id pub-id-type="pii">sensors-25-01117-t002_Table 2</object-id><label>Table 2</label><caption><p>Segmentation results of different networks in PrivateLT datasets (unit: %).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIOU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mPA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Deeplabv3</td><td align="center" valign="middle" rowspan="1" colspan="1">70.36</td><td align="center" valign="middle" rowspan="1" colspan="1">80.40</td><td align="center" valign="middle" rowspan="1" colspan="1">84.67</td><td align="center" valign="middle" rowspan="1" colspan="1">80.40</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HrNet</td><td align="center" valign="middle" rowspan="1" colspan="1">70.10</td><td align="center" valign="middle" rowspan="1" colspan="1">80.27</td><td align="center" valign="middle" rowspan="1" colspan="1">84.88</td><td align="center" valign="middle" rowspan="1" colspan="1">80.27</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UNet</td><td align="center" valign="middle" rowspan="1" colspan="1">67.8</td><td align="center" valign="middle" rowspan="1" colspan="1">79.83</td><td align="center" valign="middle" rowspan="1" colspan="1">81.30</td><td align="center" valign="middle" rowspan="1" colspan="1">79.83</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PSPNet</td><td align="center" valign="middle" rowspan="1" colspan="1">70.12</td><td align="center" valign="middle" rowspan="1" colspan="1">84.87</td><td align="center" valign="middle" rowspan="1" colspan="1">80.13</td><td align="center" valign="middle" rowspan="1" colspan="1">84.87</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CAML-PSPNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.19</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01117-t003"><object-id pub-id-type="pii">sensors-25-01117-t003_Table 3</object-id><label>Table 3</label><caption><p>Segmentation results of different networks in Kvasir-SEG datasets (unit: %).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIOU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mPA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Deeplabv3</td><td align="center" valign="middle" rowspan="1" colspan="1">75.77</td><td align="center" valign="middle" rowspan="1" colspan="1">84.77</td><td align="center" valign="middle" rowspan="1" colspan="1">87.71</td><td align="center" valign="middle" rowspan="1" colspan="1">84.77</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HrNet</td><td align="center" valign="middle" rowspan="1" colspan="1">80.01</td><td align="center" valign="middle" rowspan="1" colspan="1">86.07</td><td align="center" valign="middle" rowspan="1" colspan="1">91.91</td><td align="center" valign="middle" rowspan="1" colspan="1">86.07</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UNet</td><td align="center" valign="middle" rowspan="1" colspan="1">77.40</td><td align="center" valign="middle" rowspan="1" colspan="1">83.24</td><td align="center" valign="middle" rowspan="1" colspan="1">90.66</td><td align="center" valign="middle" rowspan="1" colspan="1">83.24</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PSPNet</td><td align="center" valign="middle" rowspan="1" colspan="1">74.53</td><td align="center" valign="middle" rowspan="1" colspan="1">81.18</td><td align="center" valign="middle" rowspan="1" colspan="1">90.10</td><td align="center" valign="middle" rowspan="1" colspan="1">81.18</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CAML-PSPNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.14</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.14</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01117-t004"><object-id pub-id-type="pii">sensors-25-01117-t004_Table 4</object-id><label>Table 4</label><caption><p>Segmentation results of different networks in ISIC 2017 datasets (unit: %).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIOU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mPA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Deeplabv3</td><td align="center" valign="middle" rowspan="1" colspan="1">86.16</td><td align="center" valign="middle" rowspan="1" colspan="1">93.51</td><td align="center" valign="middle" rowspan="1" colspan="1">91.64</td><td align="center" valign="middle" rowspan="1" colspan="1">93.51</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">HrNet</td><td align="center" valign="middle" rowspan="1" colspan="1">87.42</td><td align="center" valign="middle" rowspan="1" colspan="1">92.70</td><td align="center" valign="middle" rowspan="1" colspan="1">93.88</td><td align="center" valign="middle" rowspan="1" colspan="1">92.70</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UNet</td><td align="center" valign="middle" rowspan="1" colspan="1">84.30</td><td align="center" valign="middle" rowspan="1" colspan="1">90.66</td><td align="center" valign="middle" rowspan="1" colspan="1">91.00</td><td align="center" valign="middle" rowspan="1" colspan="1">90.66</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PSPNet</td><td align="center" valign="middle" rowspan="1" colspan="1">87.35</td><td align="center" valign="middle" rowspan="1" colspan="1">93.39</td><td align="center" valign="middle" rowspan="1" colspan="1">93.11</td><td align="center" valign="middle" rowspan="1" colspan="1">93.39</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CAML-PSPNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.89</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.05</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01117-t005"><object-id pub-id-type="pii">sensors-25-01117-t005_Table 5</object-id><label>Table 5</label><caption><p>Segmentation results of three datasets for compared networks Swin-UNETR and TransUNet (unit: %).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">&#x000a0;</th><th align="right" valign="top" style="border-top:solid thin" rowspan="1" colspan="1">Datasets</th><th rowspan="2" colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">PrivateLT</th><th rowspan="2" colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">Kvasir-SEG</th><th rowspan="2" colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin">ISIC 2017</th></tr><tr><th align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000a0;</th><th align="left" valign="top" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000a0;</th></tr><tr><th colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Methods</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mIOU</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mIOU</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">mIOU</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</th></tr></thead><tbody><tr><td colspan="2" align="center" valign="middle" rowspan="1">Swin-UNETR</td><td align="center" valign="middle" rowspan="1" colspan="1">63.72</td><td align="center" valign="middle" rowspan="1" colspan="1">70.10</td><td align="center" valign="middle" rowspan="1" colspan="1">71.60</td><td align="center" valign="middle" rowspan="1" colspan="1">82.71</td><td align="center" valign="middle" rowspan="1" colspan="1">80.56</td><td align="center" valign="middle" rowspan="1" colspan="1">85.44</td></tr><tr><td colspan="2" align="center" valign="middle" rowspan="1">TransUNet</td><td align="center" valign="middle" rowspan="1" colspan="1">65.22</td><td align="center" valign="middle" rowspan="1" colspan="1">76.35</td><td align="center" valign="middle" rowspan="1" colspan="1">73.55</td><td align="center" valign="middle" rowspan="1" colspan="1">85.90</td><td align="center" valign="middle" rowspan="1" colspan="1">81.20</td><td align="center" valign="middle" rowspan="1" colspan="1">87.30</td></tr><tr><td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">CAML-PSPNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.31</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.67</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.13</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93.89</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01117-t006"><object-id pub-id-type="pii">sensors-25-01117-t006_Table 6</object-id><label>Table 6</label><caption><p>Result of ablation experiments (unit: %).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mIOU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mPA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">PSPNet</td><td align="center" valign="middle" rowspan="1" colspan="1">70.12</td><td align="center" valign="middle" rowspan="1" colspan="1">84.87</td><td align="center" valign="middle" rowspan="1" colspan="1">80.13</td><td align="center" valign="middle" rowspan="1" colspan="1">84.87</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PSPNet+Mixed Loss</td><td align="center" valign="middle" rowspan="1" colspan="1">71.41</td><td align="center" valign="middle" rowspan="1" colspan="1">85.10</td><td align="center" valign="middle" rowspan="1" colspan="1">82.83</td><td align="center" valign="middle" rowspan="1" colspan="1">85.10</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">PSPNet+Att</td><td align="center" valign="middle" rowspan="1" colspan="1">72.77</td><td align="center" valign="middle" rowspan="1" colspan="1">85.23</td><td align="center" valign="middle" rowspan="1" colspan="1">83.28</td><td align="center" valign="middle" rowspan="1" colspan="1">85.23</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CAML-PSPNet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">73.20</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.19</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.87</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85.19</td></tr></tbody></table></table-wrap></floats-group></article>