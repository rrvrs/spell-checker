<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40397868</article-id><article-id pub-id-type="pmc">PMC12094762</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0323631</article-id><article-id pub-id-type="publisher-id">PONE-D-25-03307</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Equipment</subject><subj-group><subject>Optical Equipment</subject><subj-group><subject>Lasers</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Semantics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning Curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning Curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning Curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning Curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Remote Sensing</subject><subj-group><subject>Lidar</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Remote Sensing</subject><subj-group><subject>Radar</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Earth Sciences</subject><subj-group><subject>Geomorphology</subject><subj-group><subject>Topography</subject><subj-group><subject>Landforms</subject><subj-group><subject>Mountains</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Ecology and Environmental Sciences</subject><subj-group><subject>Terrestrial Environments</subject><subj-group><subject>Mountains</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Research on autonomous obstacle avoidance of mountainous tractors based on semantic neural network and laser SLAM</article-title><alt-title alt-title-type="running-head">Research on tractor obstacle avoidance based on semantic neural network and laser SLAM</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chang</surname><given-names>Ningjie</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Yan</surname><given-names>Xianghai</given-names></name><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Bingxin</given-names></name><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0866-1228</contrib-id><name><surname>Wu</surname><given-names>Yiwei</given-names></name><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9014-3643</contrib-id><name><surname>Xu</surname><given-names>Liyou</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>College of Vehicle and Traffic Engineering, Henan University of Science and Technology, Luoyang, Henan, China</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>State Key Laboratory of Intelligent Agricultural Power Equipment, Luoyang, Henan, China</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Zhang</surname><given-names>Lei</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Beijing Institute of Technology, CHINA</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>xlyou@haust.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>21</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0323631</elocation-id><history><date date-type="received"><day>13</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Chang et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Chang et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0323631.pdf">
</self-uri><abstract><p>The accuracy and consistency of obstacle avoidance map construction are poor in complex and changeable dynamic environment. In order to improve the driving safety of mountain tractors in complex mountain environment, an autonomous obstacle avoidance method for mountain tractors based on semantic neural network and laser SLAM was studied. Firstly, the RPLIDAR-A1 lidar sensor is used to realize high-precision scanning of mountain environment and construction of observation model. Then, the observed data is input into the lightweight convolutional neural network, and obstacles in the mountain environment are detected and semantic information is extracted through layer pruning and channel pruning strategies, and key information such as the type, size, and location of obstacles is output. Finally, the 3D map of mountain environment containing semantic information is further constructed by combining laser SLAM technology. A* algorithm is used on the map for global path planning to realize the autonomous obstacle avoidance function of mountain tractors. Experimental results show that this method can accurately detect obstacles in mountain environment, identify steep hillsides, rock piles and densely vegetated areas in mountain environment, plan the shortest and optimal driving path, and flexibly avoid diverse obstacles.</p></abstract><funding-group><award-group id="award001"><funding-source><institution>Key Research and Development Project of Henan Province</institution>
</funding-source><award-id>231111112600</award-id><principal-award-recipient>
<name><surname>Yan</surname><given-names>Xianghai</given-names></name>
</principal-award-recipient></award-group><award-group id="award002"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100006407</institution-id><institution>Natural Science Foundation of Henan Province</institution></institution-wrap>
</funding-source><award-id>242300420370</award-id><principal-award-recipient>
<name><surname>Yan</surname><given-names>Xianghai</given-names></name>
</principal-award-recipient></award-group><award-group id="award003"><funding-source><institution>Training Program for Young Backbone Teachers in Undergraduate Universities in Henan Province</institution>
</funding-source><award-id>2024GGJS051</award-id><principal-award-recipient>
<name><surname>Yan</surname><given-names>Xianghai</given-names></name>
</principal-award-recipient></award-group><award-group id="award004"><funding-source><institution>Heluo Youth Talent Support Project</institution>
</funding-source><award-id>2024 HLTJ03</award-id><principal-award-recipient>
<name><surname>Yan</surname><given-names>Xianghai</given-names></name>
</principal-award-recipient></award-group><award-group id="award005"><funding-source><institution>Henan Province University Science and Technology Innovation Team Support Program Project</institution>
</funding-source><award-id>24IRTSTHN029</award-id><principal-award-recipient>
<name><surname>Yan</surname><given-names>Xianghai</given-names></name>
</principal-award-recipient></award-group><award-group id="award006"><funding-source><institution>Key Research Project Plan for Higher Education Institutions in Henan Province</institution>
</funding-source><award-id>25B460004</award-id><principal-award-recipient>
<name><surname>Yan</surname><given-names>Xianghai</given-names></name>
</principal-award-recipient></award-group><funding-statement>Key Research and Development Project of Henan Province, 231111112600; Natural Science Foundation of Henan Province, 242300420370; Training Program for Young Backbone Teachers in Undergraduate Universities in Henan Province, 2024GGJS051; Heluo Youth Talent Support Project, 2024 HLTJ03; Henan Province University Science and Technology Innovation Team Support Program Project, 24IRTSTHN029; Key Research Project Plan for Higher Education Institutions in Henan Province, 25B460004.</funding-statement></funding-group><counts><fig-count count="8"/><table-count count="5"/><page-count count="20"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data are within the manuscript and its <xref rid="sec013" ref-type="sec">Supporting Information</xref> files.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data are within the manuscript and its <xref rid="sec013" ref-type="sec">Supporting Information</xref> files.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>In complex terrain such as mountainous areas, as the main force of agricultural production in mountainous areas [<xref rid="pone.0323631.ref001" ref-type="bibr">1</xref>], tractors often face various natural and man-made obstacles when operating, such as steep hillsides, gullies, rocks, trees, and sudden pedestrians or animals. If these obstacles are not identified in time and effectively avoided, it is very easy to cause accidents such as collision or rollover of the tractor, which seriously threatens the safety of the driver and the machine [<xref rid="pone.0323631.ref002" ref-type="bibr">2</xref>]. The autonomous obstacle avoidance method can perceive the surrounding environment in real time, make autonomous decisions and control the tractor to avoid obstacles, thus significantly reducing accident risk and improving operation safety [<xref rid="pone.0323631.ref003" ref-type="bibr">3</xref>]. Therefore, it is of great significance to study the autonomous obstacle avoidance method of mountain tractors and realize their autonomous navigation, obstacle detection and obstacle avoidance decision-making in complex environments for promoting the development of agricultural mechanization and intelligence in mountain areas [<xref rid="pone.0323631.ref004" ref-type="bibr">4</xref>].</p><p>Some shortcomings of autonomous obstacle avoidance methods for mountain tractors not only limit the working efficiency of tractors in complex mountain environments, but also highlight the urgency and necessity of autonomous obstacle avoidance methods. For example, Ozdemir A et al. proposed a GET algorithm for tractor obstacle avoidance [<xref rid="pone.0323631.ref005" ref-type="bibr">5</xref>], introduced the gap based elastic tree (GET) algorithm, obtained environmental data and extracted the position, shape, size and other information of obstacles, built a local environmental model, identified the gaps that the tractor can safely pass through, selected the optimal smooth path, and converted it into a specific driving track, the tractor can avoid obstacles independently. However, this obstacle avoidance method is limited by the ability of its data processing algorithm, which makes it impossible to conduct accurate analysis and judgment quickly after receiving sensor data, thus delaying the generation and implementation of obstacle avoidance decisions. Adamiec Wojcik I et al. proposed a lumped mass autonomous obstacle avoidance model for mountain tractors [<xref rid="pone.0323631.ref006" ref-type="bibr">6</xref>], combined with the dynamic characteristics of bending and longitudinal flexibility, extracted effective environmental information, identified obstacles, classified the identified obstacles, evaluated their impact on tractor driving, and formulated specific driving decisions based on the current state of the tractor and obstacle information. However, this obstacle avoidance method relies on simple infrared sensors. In the complex and changeable mountain environment, its perception ability is often limited, and it is unable to accurately and comprehensively identify the surrounding obstacles. Sezer V et al. proposed a tractor obstacle avoidance method based on the FGM algorithm [<xref rid="pone.0323631.ref007" ref-type="bibr">7</xref>]. Referring to the concept of looking ahead distance (LAD) in the geometric path tracking method, they integrated it into the local planner, defined a dynamic and optimized LAD, which can be automatically adjusted according to the tractor speed to optimize the tracking, obstacle avoidance and driving comfort, and combined the following clearance method (FGM) with the global planner. It is applied to mountain tractors as part of a complete autonomous system. However, this obstacle avoidance method may not be able to accurately identify and effectively deal with obstacles of different heights, shapes and materials, resulting in poor obstacle avoidance effect. Conte D et al. proposed a Bayesian autonomous obstacle avoidance method for mountain tractors [<xref rid="pone.0323631.ref008" ref-type="bibr">8</xref>]. Using Bayesian technology, their intentions were inferred by observing the head posture of people walking with the tractor. The Gaussian fusion method was used to synthesize two independent prediction models to predict the future position of people. When determining the target posture of the tractor, adjustments were made according to the predicted movement of people, With the help of unoccupied distance graph, obstacle avoidance is realized. But this obstacle avoidance method not only increases the workload of the driver, but also reduces the working efficiency and intelligent level of the tractor.Irshayyid A et al. first reviewed the different traffic scenarios discussed in the literature [<xref rid="pone.0323631.ref009" ref-type="bibr">9</xref>], and then conducted a comprehensive review of DRL technology, such as the state representation method for capturing the interactive dynamics required for safe and efficient merging, and the reward formula for managing key indicators such as safety, efficiency, comfort, and adaptability. The insights from this review can guide future research towards realizing the potential of DRL in complex traffic automation under uncertainty. A dynamic adaptive trajectory planning method based on Bezier curve is proposed. Li H et al. first established a mathematical model of the Bezier curve [<xref rid="pone.0323631.ref010" ref-type="bibr">10</xref>] and analyzed its curve characteristics, which helps to establish the correlation between trajectory control points and vehicles and surrounding obstacles. Secondly, a mathematical function representing the Bezier curve was established, with control points as inputs and lane change control curves as outputs, to achieve lane change trajectory planning for automobiles.</p><p>Semantic neural network is a kind of neural network that uses deep learning technology to extract high-level semantic information from data. In the autonomous obstacle avoidance of mountain tractors, semantic neural networks can be used to identify obstacles in the surrounding environment, such as trees, rocks, gullies, etc., and understand their attributes and spatial relationships, which will help tractors make more accurate obstacle avoidance decisions in complex mountain environments. The laser SLAM method is a technology that uses laser radar to sense and locate the environment. In the autonomous obstacle avoidance of mountain tractors, the laser SLAM can build a 3D map of the surrounding environment in real time, and accurately estimate the position and posture of the tractor, providing a wealth of environmental information for the tractor and helping to achieve accurate obstacle avoidance control. Therefore, the research of autonomous obstacle avoidance method for mountain tractors based on semantic neural network and laser SLAM is of great significance for improving the efficiency of mountain operations and ensuring the safety of operations.</p></sec><sec id="sec002"><title>Autonomous obstacle avoidance methods for mountain tractors</title><sec id="sec003"><title>LIDAR observation model</title><p>In the actual movement process of the tractor, due to the observation noise and error of the sensor, it is difficult to accurately describe the actual state of the tractor. The sensor data is used to establish the tractor observation model, so as to more accurately simulate the perception process of the tractor to the environment. The RPLIDAR-A1 single line laser radar is used as the main sensor to scan the mountain environment, obtain the surrounding environment data, and accurately understand the mountain environment information through the coordinate conversion process and the establishment of observation model.</p><list list-type="simple"><list-item><p>(1) Coordinate transformation. The conversion of laser measurement data from the radar coordinate system to the tractor coordinate system in a mountainous environment is shown in <xref rid="pone.0323631.g001" ref-type="fig">Fig 1</xref>, where, <inline-formula id="pone.0323631.e001"><alternatives><graphic xlink:href="pone.0323631.e001.jpg" id="pone.0323631.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the coordinates of the tractor in the mountain environment at the current moment; <inline-formula id="pone.0323631.e002"><alternatives><graphic xlink:href="pone.0323631.e002.jpg" id="pone.0323631.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the original coordinates of the laser irradiation point. <inline-formula id="pone.0323631.e003"><alternatives><graphic xlink:href="pone.0323631.e003.jpg" id="pone.0323631.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the tractor&#x02019;s facing angle, which is the angle between tractor&#x02019;s current facing with the positive direction of the <inline-formula id="pone.0323631.e004"><alternatives><graphic xlink:href="pone.0323631.e004.jpg" id="pone.0323631.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> axis; <inline-formula id="pone.0323631.e005"><alternatives><graphic xlink:href="pone.0323631.e005.jpg" id="pone.0323631.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the angle between the laser irradiation point and the radar direction, measured directly by the lidar. <inline-formula id="pone.0323631.e006"><alternatives><graphic xlink:href="pone.0323631.e006.jpg" id="pone.0323631.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> represents the included angle between the laser irradiation point and the line of origin and the positive direction of the x-axis after coordinate conversion; <inline-formula id="pone.0323631.e007"><alternatives><graphic xlink:href="pone.0323631.e007.jpg" id="pone.0323631.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> represents the straight line distance between the center of the tractor and the laser irradiation point, which is also measured directly by the LiDAR.</p></list-item></list><fig position="float" id="pone.0323631.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.g001</object-id><label>Fig 1</label><caption><title>Coordinate Conversion Process.</title></caption><graphic xlink:href="pone.0323631.g001" position="float"/></fig><p>Using the trigonometric relationship, the coordinates of the laser irradiation point in the coordinate system of the tractor are calculated and expressed by the formula:</p><disp-formula id="pone.0323631.e008">
<alternatives><graphic xlink:href="pone.0323631.e008.jpg" id="pone.0323631.e008g" position="anchor"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003c7;</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>[1]</label>
</disp-formula><disp-formula id="pone.0323631.e009">
<alternatives><graphic xlink:href="pone.0323631.e009.jpg" id="pone.0323631.e009g" position="anchor"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>sin</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003c7;</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>[2]</label>
</disp-formula><disp-formula id="pone.0323631.e010">
<alternatives><graphic xlink:href="pone.0323631.e010.jpg" id="pone.0323631.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:mi>arctan</mml:mi><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>[3]</label>
</disp-formula><list list-type="simple"><list-item><p>(2) Observational model. During the movement of a tractor in a mountainous environment [<xref rid="pone.0323631.ref011" ref-type="bibr">11</xref>], LIDAR observation will be affected by a variety of factors, including the measurement error and the tractor&#x02019;s movement state. Therefore, it is necessary to establish an observation model that integrates these factors. The observation model is usually expressed as follows:</p></list-item></list><disp-formula id="pone.0323631.e011">
<alternatives><graphic xlink:href="pone.0323631.e011.jpg" id="pone.0323631.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#x003b4;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:msqrt><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mi>arctan</mml:mi><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></alternatives>
<label>[4]</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e012"><alternatives><graphic xlink:href="pone.0323631.e012.jpg" id="pone.0323631.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> indicates the number of observations for the radar at time <inline-formula id="pone.0323631.e013"><alternatives><graphic xlink:href="pone.0323631.e013.jpg" id="pone.0323631.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, usually including distance and angle information. <inline-formula id="pone.0323631.e014"><alternatives><graphic xlink:href="pone.0323631.e014.jpg" id="pone.0323631.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the systematic measurement function, it maps the system state <inline-formula id="pone.0323631.e015"><alternatives><graphic xlink:href="pone.0323631.e015.jpg" id="pone.0323631.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> of the tractor and the observation noise <inline-formula id="pone.0323631.e016"><alternatives><graphic xlink:href="pone.0323631.e016.jpg" id="pone.0323631.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003d5;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> at time <inline-formula id="pone.0323631.e017"><alternatives><graphic xlink:href="pone.0323631.e017.jpg" id="pone.0323631.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> into the observation space. <inline-formula id="pone.0323631.e018"><alternatives><graphic xlink:href="pone.0323631.e018.jpg" id="pone.0323631.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> denotes the coordinates of the tractor at the moment; <inline-formula id="pone.0323631.e019"><alternatives><graphic xlink:href="pone.0323631.e019.jpg" id="pone.0323631.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the angle of orientation of the tractor at the moment; <inline-formula id="pone.0323631.e020"><alternatives><graphic xlink:href="pone.0323631.e020.jpg" id="pone.0323631.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> indicates the coordinates of an obstacle in a mountainous environment.</p></sec><sec id="sec004"><title>Neural network-based semantic information extraction for obstacles</title><p>Taking the observations <inline-formula id="pone.0323631.e021"><alternatives><graphic xlink:href="pone.0323631.e021.jpg" id="pone.0323631.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> obtained from the observational model described above as the input data of neural network, semantic information of obstacles is extracted. With the pruned YOLOv3 lightweight convolutional neural network and the addition of semantic tags, the accurate semantic information extraction and segmentation of roads, trees, rocks, buildings and other obstacles in mountain environment are realized.</p></sec><sec id="sec005"><title>Design of a lightweight convolutional neural network</title><p>Some convolution layers and convolution cores in YOLOv3 network have little impact, even have no impact on the whole network [<xref rid="pone.0323631.ref012" ref-type="bibr">12</xref>]. Therefore, YOLOv3 after pruning is used to detect obstacles in mountain environment observed in LIDAR observation model. Prune the small and ineffective parts of the network. After pruning, YOLOv3&#x02019;s model size and parameters are greatly reduced while maintaining a certain precision.</p><p>In the convolutional neural network pruning method [<xref rid="pone.0323631.ref013" ref-type="bibr">13</xref>], layer pruning is based on the role of each convolutional layer as an evaluation criterion, the less influential convolutional layer is cut off, so as to achieve the effect of model compression; channel pruning is to operate on the internal channels or channel weights of the convolutional kernel, and after the channel pruning, the complete convolutional structure will still exist in the network.</p><p>During the channel pruning process, the mountain environment feature map matrices and convolution kernel sizes observed in the <inline-formula id="pone.0323631.e022"><alternatives><graphic xlink:href="pone.0323631.e022.jpg" id="pone.0323631.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> convolutional layers are respectively <inline-formula id="pone.0323631.e023"><alternatives><graphic xlink:href="pone.0323631.e023.jpg" id="pone.0323631.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0323631.e024"><alternatives><graphic xlink:href="pone.0323631.e024.jpg" id="pone.0323631.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, from which the convolution kernel matrix for this convolutional layer can be computed as:</p><disp-formula id="pone.0323631.e025">
<alternatives><graphic xlink:href="pone.0323631.e025.jpg" id="pone.0323631.e025g" position="anchor"/><mml:math id="M25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mi>z</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>t</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives>
<label>[5]</label>
</disp-formula><p>In this convolutional layer, if the weight of a channel plays a smaller role in the convolutional calculation, the channel is chosen to be pruned. During convolutional computation, the number of convolutional layers is equal to the number of input feature map channels, so after channel pruning, to adjust the number of channels of the input feature maps of the <inline-formula id="pone.0323631.e026"><alternatives><graphic xlink:href="pone.0323631.e026.jpg" id="pone.0323631.e026g" position="anchor"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> convolutional layers. In the previous convolutional layer, a single convolutional kernel can get the corresponding feature map channel in the next convolutional layer after convolution operation, so the corresponding convolutional kernel in the previous layer needs to be clipped.</p><p>In the pruning process of lightweight neural network [<xref rid="pone.0323631.ref014" ref-type="bibr">14</xref>], in order to get a large model with high accuracy, normal basic training is needed, and then through sparse training, the unimportant part of the convolutional neural network structure is highlighted and cut out this part, because the accuracy of the previous model will be degraded to varying degrees after the pruning, fine-tuning is needed to restore the model accuracy.</p><p>Layer pruning and channel pruning are two methods that have a common feature, they are both structured pruning methods; after pruning, they do not need additional methods to accelerate them. After layer pruning, the depth of the network structure will become shallow. After channel pruning, the width of the network structure becomes narrower. By combining these two methods and pruning the network model together, the depth and width of the network structure will be reduced, and the model can be further compressed by combining the two methods compared to a single pruning method. Therefore, by combining layer pruning and channel pruning methods, a more compact model can be obtained after pruning.</p><p>Sparse training is the first step in the pruning process. Through training, some parameters in the model become less important, which makes it easier to remove them when pruning. In YOLOv3, since each convolution layer is followed by a BN (batch normalization) layer, the trainable scale factor <inline-formula id="pone.0323631.e027"><alternatives><graphic xlink:href="pone.0323631.e027.jpg" id="pone.0323631.e027g" position="anchor"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in the BN layer is used as an indicator of channel importance. BN layer uses small batch statistics to normalize convolution characteristics, and the formula is as follows:</p><disp-formula id="pone.0323631.e028">
<alternatives><graphic xlink:href="pone.0323631.e028.jpg" id="pone.0323631.e028g" position="anchor"/><mml:math id="M28" display="block" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003c6;</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>[6]</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e029"><alternatives><graphic xlink:href="pone.0323631.e029.jpg" id="pone.0323631.e029g" position="anchor"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the feature map of the output of the convolutional layer; <inline-formula id="pone.0323631.e030"><alternatives><graphic xlink:href="pone.0323631.e030.jpg" id="pone.0323631.e030g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the mean of the feature map; <inline-formula id="pone.0323631.e031"><alternatives><graphic xlink:href="pone.0323631.e031.jpg" id="pone.0323631.e031g" position="anchor"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the variance of the feature map; <inline-formula id="pone.0323631.e032"><alternatives><graphic xlink:href="pone.0323631.e032.jpg" id="pone.0323631.e032g" position="anchor"/><mml:math id="M32" display="inline" overflow="scroll"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes a positive number in order to prevent the divisor from being zero. <inline-formula id="pone.0323631.e033"><alternatives><graphic xlink:href="pone.0323631.e033.jpg" id="pone.0323631.e033g" position="anchor"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes deviation.</p><p>In order to effectively differentiate between important and unimportant channels, by giving the scale factor <inline-formula id="pone.0323631.e034"><alternatives><graphic xlink:href="pone.0323631.e034.jpg" id="pone.0323631.e034g" position="anchor"/><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> apply L1 regularization to perform channel sparse training. The training objective formula of sparse training can be expressed as:</p><disp-formula id="pone.0323631.e035">
<alternatives><graphic xlink:href="pone.0323631.e035.jpg" id="pone.0323631.e035g" position="anchor"/><mml:math id="M35" display="block" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>&#x003b9;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x00393;</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b9;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>[7]</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e036"><alternatives><graphic xlink:href="pone.0323631.e036.jpg" id="pone.0323631.e036g" position="anchor"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b9;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> represents L1 norm; <inline-formula id="pone.0323631.e037"><alternatives><graphic xlink:href="pone.0323631.e037.jpg" id="pone.0323631.e037g" position="anchor"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x00393;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> represents the set of the scale factor <inline-formula id="pone.0323631.e038"><alternatives><graphic xlink:href="pone.0323631.e038.jpg" id="pone.0323631.e038g" position="anchor"/><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c6;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in all BN layers; <inline-formula id="pone.0323631.e039"><alternatives><graphic xlink:href="pone.0323631.e039.jpg" id="pone.0323631.e039g" position="anchor"/><mml:math id="M39" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the penalty factor that balances the two loss terms. The non-smooth penalty term is optimized using the sub gradient method.</p><p>After sparse training, a global threshold <inline-formula id="pone.0323631.e040"><alternatives><graphic xlink:href="pone.0323631.e040.jpg" id="pone.0323631.e040g" position="anchor"/><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is introduced, based on a global threshold <inline-formula id="pone.0323631.e041"><alternatives><graphic xlink:href="pone.0323631.e041.jpg" id="pone.0323631.e041g" position="anchor"/><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> determine whether a channel should be trimmed. In order to control the trimming ratio of a channel, the global threshold <inline-formula id="pone.0323631.e042"><alternatives><graphic xlink:href="pone.0323631.e042.jpg" id="pone.0323631.e042g" position="anchor"/><mml:math id="M42" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is set as the <inline-formula id="pone.0323631.e043"><alternatives><graphic xlink:href="pone.0323631.e043.jpg" id="pone.0323631.e043g" position="anchor"/><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> th percentile to all <inline-formula id="pone.0323631.e044"><alternatives><graphic xlink:href="pone.0323631.e044.jpg" id="pone.0323631.e044g" position="anchor"/><mml:math id="M44" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">|</mml:mo><mml:mi>&#x003b9;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. In order to prevent over-pruning on the convolutional layer and to maintain the integrity of the network connections, in addition to the global threshold, a local safety threshold <inline-formula id="pone.0323631.e045"><alternatives><graphic xlink:href="pone.0323631.e045.jpg" id="pone.0323631.e045g" position="anchor"/><mml:math id="M45" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is introduced. Local security thresholds are set as the <inline-formula id="pone.0323631.e046"><alternatives><graphic xlink:href="pone.0323631.e046.jpg" id="pone.0323631.e046g" position="anchor"/><mml:math id="M46" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> th percentile in a hierarchical manner to the specific layer in <inline-formula id="pone.0323631.e047"><alternatives><graphic xlink:href="pone.0323631.e047.jpg" id="pone.0323631.e047g" position="anchor"/><mml:math id="M47" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">|</mml:mo><mml:mi>&#x003b9;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. In pruning, the feature channel whose pruning scaling factor is smaller than the global threshold <inline-formula id="pone.0323631.e048"><alternatives><graphic xlink:href="pone.0323631.e048.jpg" id="pone.0323631.e048g" position="anchor"/><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and the safety threshold <inline-formula id="pone.0323631.e049"><alternatives><graphic xlink:href="pone.0323631.e049.jpg" id="pone.0323631.e049g" position="anchor"/><mml:math id="M49" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> minimum. In addition, during the pruning process, the largest pooling and up-sampling layers, which have no relationship with the number of channels, are directly discarded.</p><p>Based on the global thresholds <inline-formula id="pone.0323631.e050"><alternatives><graphic xlink:href="pone.0323631.e050.jpg" id="pone.0323631.e050g" position="anchor"/><mml:math id="M50" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and local safety thresholds <inline-formula id="pone.0323631.e051"><alternatives><graphic xlink:href="pone.0323631.e051.jpg" id="pone.0323631.e051g" position="anchor"/><mml:math id="M51" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003d1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, find the mask (convolution kernel) of all convolution layers. For the routing layer, connect the pruning mask of its incoming layer in order, and use the connected mask as its pruning mask. Traverse the pruning masks of all connected layers, and perform the &#x0201c;OR&#x0201d; operation on these pruning masks to generate the final pruning masks of these connected layers. After pruning, the accuracy of the model will decline to some extent, so it is necessary to train the model again to restore the accuracy of the model.</p></sec><sec id="sec006"><title>Adding semantic tags</title><p>In the driving environment of mountain tractors [<xref rid="pone.0323631.ref015" ref-type="bibr">15</xref>], semantic segmentation using the modified YOLOv3 network is a key step to extract semantic information of obstacles. The goal of semantic segmentation is to assign a category label to each pixel in the observed data, including roads, trees, rocks, buildings, etc. in the mountain environment. The model can accurately recognize and segment the obstacles in the mountain environment by precisely labeling the obstacles in the image with the category and boundary information, and inputting them into the lightweight convolutional neural network for training. After the model training is completed, it is applied to the real-time mountain environment image. The model will output the category label of each pixel to achieve the semantic segmentation of obstacles.</p><p>The process of adding semantic tags is shown in <xref rid="pone.0323631.g002" ref-type="fig">Fig 2</xref>.</p><fig position="float" id="pone.0323631.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.g002</object-id><label>Fig 2</label><caption><title>Semantic Label Addition Process.</title></caption><graphic xlink:href="pone.0323631.g002" position="float"/></fig><p>The update of semantic tags is divided into three parts, one is the obstacle detection module, the other is the distance calculation module, and the last part is the Marker addition module. When adding semantic tags, the obstacle detection module is responsible for detecting the target obstacles, substituting the color image into the recognition module to obtain the semantic information of the target obstacles and the position of the center point of the bounding box in the color image. The distance calculation module calculates the position of the center point of the boundary box in the depth map to obtain the relative position of the obstacles. According to the coordinate conversion, the global position of the obstacles is obtained. The Marker addition module obtains the semantic tag of the target obstacle by subscribing to the semantic information and global position of the target obstacle.</p></sec><sec id="sec007"><title>Construction of mountain environment map based on laser SLAM</title><p>Through the laser SLAM technology, based on the semantic information of the obstacles extracted in Neural network-based semantic information extraction for obstacles, combined with the edge and plane features of the mountain environment extracted by the laser radar scanning, the tractor pose change is estimated using the point-to-point constraint relationship, and the pose and point cloud data are updated iteratively through the graph optimization method, finally the global mountain environment point cloud map is constructed.</p><p>Firstly, remove outliers in the point cloud that are too far from the mean. <inline-formula id="pone.0323631.e052"><alternatives><graphic xlink:href="pone.0323631.e052.jpg" id="pone.0323631.e052g" position="anchor"/><mml:math id="M52" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">&#x02016;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">&#x02016;</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, Among them, <inline-formula id="pone.0323631.e053"><alternatives><graphic xlink:href="pone.0323631.e053.jpg" id="pone.0323631.e053g" position="anchor"/><mml:math id="M53" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the <inline-formula id="pone.0323631.e054"><alternatives><graphic xlink:href="pone.0323631.e054.jpg" id="pone.0323631.e054g" position="anchor"/><mml:math id="M54" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> -th point in the point cloud, and <inline-formula id="pone.0323631.e055"><alternatives><graphic xlink:href="pone.0323631.e055.jpg" id="pone.0323631.e055g" position="anchor"/><mml:math id="M55" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the value of the point cloud. If <inline-formula id="pone.0323631.e056"><alternatives><graphic xlink:href="pone.0323631.e056.jpg" id="pone.0323631.e056g" position="anchor"/><mml:math id="M56" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x0003e;</mml:mo><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> (threshold), it is removed and changed to, and the data is denoised according to the above point cloud filtering. Assuming that the <inline-formula id="pone.0323631.e057"><alternatives><graphic xlink:href="pone.0323631.e057.jpg" id="pone.0323631.e057g" position="anchor"/><mml:math id="M57" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> -th scan of the processed LiDAR extracts a set of edge points in the mountainous environment as <inline-formula id="pone.0323631.e058"><alternatives><graphic xlink:href="pone.0323631.e058.jpg" id="pone.0323631.e058g" position="anchor"/><mml:math id="M58" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,Suppose the set of edge points extracted in the <inline-formula id="pone.0323631.e059"><alternatives><graphic xlink:href="pone.0323631.e059.jpg" id="pone.0323631.e059g" position="anchor"/><mml:math id="M59" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> th scan of lidar is <inline-formula id="pone.0323631.e060"><alternatives><graphic xlink:href="pone.0323631.e060.jpg" id="pone.0323631.e060g" position="anchor"/><mml:math id="M60" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and the set of plane points extracted is denoted as <inline-formula id="pone.0323631.e061"><alternatives><graphic xlink:href="pone.0323631.e061.jpg" id="pone.0323631.e061g" position="anchor"/><mml:math id="M61" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, the sets of edge point in the mountains environment extracted by the <inline-formula id="pone.0323631.e062"><alternatives><graphic xlink:href="pone.0323631.e062.jpg" id="pone.0323631.e062g" position="anchor"/><mml:math id="M62" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> th lidar scan is <inline-formula id="pone.0323631.e063"><alternatives><graphic xlink:href="pone.0323631.e063.jpg" id="pone.0323631.e063g" position="anchor"/><mml:math id="M63" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and the sets of flat point in the extracted areas is <inline-formula id="pone.0323631.e064"><alternatives><graphic xlink:href="pone.0323631.e064.jpg" id="pone.0323631.e064g" position="anchor"/><mml:math id="M64" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The point cloud transformation relation between two moments is changed to find the edge feature correspondence and plane feature correspondence between two moments respectively. Pick a point <inline-formula id="pone.0323631.e065"><alternatives><graphic xlink:href="pone.0323631.e065.jpg" id="pone.0323631.e065g" position="anchor"/><mml:math id="M65" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in <inline-formula id="pone.0323631.e066"><alternatives><graphic xlink:href="pone.0323631.e066.jpg" id="pone.0323631.e066g" position="anchor"/><mml:math id="M66" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, taking the nearest point <inline-formula id="pone.0323631.e067"><alternatives><graphic xlink:href="pone.0323631.e067.jpg" id="pone.0323631.e067g" position="anchor"/><mml:math id="M67" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> to <inline-formula id="pone.0323631.e068"><alternatives><graphic xlink:href="pone.0323631.e068.jpg" id="pone.0323631.e068g" position="anchor"/><mml:math id="M68" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in <inline-formula id="pone.0323631.e069"><alternatives><graphic xlink:href="pone.0323631.e069.jpg" id="pone.0323631.e069g" position="anchor"/><mml:math id="M69" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and the nearest point <inline-formula id="pone.0323631.e070"><alternatives><graphic xlink:href="pone.0323631.e070.jpg" id="pone.0323631.e070g" position="anchor"/><mml:math id="M70" display="inline" overflow="scroll"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in the scanning line bundle adjacent to <inline-formula id="pone.0323631.e071"><alternatives><graphic xlink:href="pone.0323631.e071.jpg" id="pone.0323631.e071g" position="anchor"/><mml:math id="M71" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in <inline-formula id="pone.0323631.e072"><alternatives><graphic xlink:href="pone.0323631.e072.jpg" id="pone.0323631.e072g" position="anchor"/><mml:math id="M72" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, three points are obtained, the coordinates of which are noted as <inline-formula id="pone.0323631.e073"><alternatives><graphic xlink:href="pone.0323631.e073.jpg" id="pone.0323631.e073g" position="anchor"/><mml:math id="M73" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>&#x03001; <inline-formula id="pone.0323631.e074"><alternatives><graphic xlink:href="pone.0323631.e074.jpg" id="pone.0323631.e074g" position="anchor"/><mml:math id="M74" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>&#x03001; <inline-formula id="pone.0323631.e075"><alternatives><graphic xlink:href="pone.0323631.e075.jpg" id="pone.0323631.e075g" position="anchor"/><mml:math id="M75" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the constraint formula for the edge features can be expressed as follows:</p><disp-formula id="pone.0323631.e076">
<alternatives><graphic xlink:href="pone.0323631.e076.jpg" id="pone.0323631.e076g" position="anchor"/><mml:math id="M76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">|</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>[8]</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e077"><alternatives><graphic xlink:href="pone.0323631.e077.jpg" id="pone.0323631.e077g" position="anchor"/><mml:math id="M77" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the distance between pairs of matching feature points in the set of edge points.</p><p>Looking for a point <inline-formula id="pone.0323631.e078"><alternatives><graphic xlink:href="pone.0323631.e078.jpg" id="pone.0323631.e078g" position="anchor"/><mml:math id="M78" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in <inline-formula id="pone.0323631.e079"><alternatives><graphic xlink:href="pone.0323631.e079.jpg" id="pone.0323631.e079g" position="anchor"/><mml:math id="M79" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, find the point l closest to the point <inline-formula id="pone.0323631.e080"><alternatives><graphic xlink:href="pone.0323631.e080.jpg" id="pone.0323631.e080g" position="anchor"/><mml:math id="M80" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in <inline-formula id="pone.0323631.e081"><alternatives><graphic xlink:href="pone.0323631.e081.jpg" id="pone.0323631.e081g" position="anchor"/><mml:math id="M81" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, find the nearest point <inline-formula id="pone.0323631.e082"><alternatives><graphic xlink:href="pone.0323631.e082.jpg" id="pone.0323631.e082g" position="anchor"/><mml:math id="M82" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> of the laser scanning line beam adjacent to the point <inline-formula id="pone.0323631.e083"><alternatives><graphic xlink:href="pone.0323631.e083.jpg" id="pone.0323631.e083g" position="anchor"/><mml:math id="M83" display="inline" overflow="scroll"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> from Hk, and then find the point <inline-formula id="pone.0323631.e084"><alternatives><graphic xlink:href="pone.0323631.e084.jpg" id="pone.0323631.e084g" position="anchor"/><mml:math id="M84" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> closest to the point <inline-formula id="pone.0323631.e085"><alternatives><graphic xlink:href="pone.0323631.e085.jpg" id="pone.0323631.e085g" position="anchor"/><mml:math id="M85" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in the adjacent frame, four points are obtained, the coordinates of which are noted as <inline-formula id="pone.0323631.e086"><alternatives><graphic xlink:href="pone.0323631.e086.jpg" id="pone.0323631.e086g" position="anchor"/><mml:math id="M86" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>&#x03001; <inline-formula id="pone.0323631.e087"><alternatives><graphic xlink:href="pone.0323631.e087.jpg" id="pone.0323631.e087g" position="anchor"/><mml:math id="M87" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>&#x03001; <inline-formula id="pone.0323631.e088"><alternatives><graphic xlink:href="pone.0323631.e088.jpg" id="pone.0323631.e088g" position="anchor"/><mml:math id="M88" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>&#x03001; <inline-formula id="pone.0323631.e089"><alternatives><graphic xlink:href="pone.0323631.e089.jpg" id="pone.0323631.e089g" position="anchor"/><mml:math id="M89" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The constraint formula for a planar feature can be expressed as follows:</p><disp-formula id="pone.0323631.e090">
<alternatives><graphic xlink:href="pone.0323631.e090.jpg" id="pone.0323631.e090g" position="anchor"/><mml:math id="M90" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">|</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>&#x000d7;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>[9]</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e091"><alternatives><graphic xlink:href="pone.0323631.e091.jpg" id="pone.0323631.e091g" position="anchor"/><mml:math id="M91" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the distance between pairs of matching feature points in a planar point set.</p><p>The output of the pose solution part of the laser SLAM problem is the pose information of the tractor, which includes the translation vector <inline-formula id="pone.0323631.e092"><alternatives><graphic xlink:href="pone.0323631.e092.jpg" id="pone.0323631.e092g" position="anchor"/><mml:math id="M92" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and Euler angles <inline-formula id="pone.0323631.e093"><alternatives><graphic xlink:href="pone.0323631.e093.jpg" id="pone.0323631.e093g" position="anchor"/><mml:math id="M93" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. <inline-formula id="pone.0323631.e094"><alternatives><graphic xlink:href="pone.0323631.e094.jpg" id="pone.0323631.e094g" position="anchor"/><mml:math id="M94" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>&#x03001; <inline-formula id="pone.0323631.e095"><alternatives><graphic xlink:href="pone.0323631.e095.jpg" id="pone.0323631.e095g" position="anchor"/><mml:math id="M95" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0323631.e096"><alternatives><graphic xlink:href="pone.0323631.e096.jpg" id="pone.0323631.e096g" position="anchor"/><mml:math id="M96" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> indicate roll, pitch and heading angles, respectively. <inline-formula id="pone.0323631.e097"><alternatives><graphic xlink:href="pone.0323631.e097.jpg" id="pone.0323631.e097g" position="anchor"/><mml:math id="M97" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c2;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>&#x03001; <inline-formula id="pone.0323631.e098"><alternatives><graphic xlink:href="pone.0323631.e098.jpg" id="pone.0323631.e098g" position="anchor"/><mml:math id="M98" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c2;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0323631.e099"><alternatives><graphic xlink:href="pone.0323631.e099.jpg" id="pone.0323631.e099g" position="anchor"/><mml:math id="M99" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c2;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> are the displacement in the direction <inline-formula id="pone.0323631.e100"><alternatives><graphic xlink:href="pone.0323631.e100.jpg" id="pone.0323631.e100g" position="anchor"/><mml:math id="M100" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>&#x03001; <inline-formula id="pone.0323631.e101"><alternatives><graphic xlink:href="pone.0323631.e101.jpg" id="pone.0323631.e101g" position="anchor"/><mml:math id="M101" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0323631.e102"><alternatives><graphic xlink:href="pone.0323631.e102.jpg" id="pone.0323631.e102g" position="anchor"/><mml:math id="M102" display="inline" overflow="scroll"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, six-degree-of-freedom variables is denoted by <inline-formula id="pone.0323631.e103"><alternatives><graphic xlink:href="pone.0323631.e103.jpg" id="pone.0323631.e103g" position="anchor"/><mml:math id="M103" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p><p>Using the constraint relation of the point to the surface, the variation <inline-formula id="pone.0323631.e104"><alternatives><graphic xlink:href="pone.0323631.e104.jpg" id="pone.0323631.e104g" position="anchor"/><mml:math id="M104" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c2;</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> of the vertical dimension of the tractor is estimated by matching the correspondence of the plane features extracted into <inline-formula id="pone.0323631.e105"><alternatives><graphic xlink:href="pone.0323631.e105.jpg" id="pone.0323631.e105g" position="anchor"/><mml:math id="M105" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> by the lidar scan, matching the edge features in <inline-formula id="pone.0323631.e106"><alternatives><graphic xlink:href="pone.0323631.e106.jpg" id="pone.0323631.e106g" position="anchor"/><mml:math id="M106" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> estimates the variations of the level dimension <inline-formula id="pone.0323631.e107"><alternatives><graphic xlink:href="pone.0323631.e107.jpg" id="pone.0323631.e107g" position="anchor"/><mml:math id="M107" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c2;</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c2;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, obtaining the nonlinear function formula as:</p><disp-formula id="pone.0323631.e108">
<alternatives><graphic xlink:href="pone.0323631.e108.jpg" id="pone.0323631.e108g" position="anchor"/><mml:math id="M108" display="block" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>[10]</label>
</disp-formula><p>The common Levenberg-Marquardt method is used to optimize this nonlinear function to find the minimum distance transformation between two successive scans. Assuming that the objective function is <inline-formula id="pone.0323631.e109"><alternatives><graphic xlink:href="pone.0323631.e109.jpg" id="pone.0323631.e109g" position="anchor"/><mml:math id="M109" display="inline" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the error function is given by <inline-formula id="pone.0323631.e110"><alternatives><graphic xlink:href="pone.0323631.e110.jpg" id="pone.0323631.e110g" position="anchor"/><mml:math id="M110" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c8;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, the least squares optimization of the objective function can be expressed as follows:</p><disp-formula id="pone.0323631.e111">
<alternatives><graphic xlink:href="pone.0323631.e111.jpg" id="pone.0323631.e111g" position="anchor"/><mml:math id="M111" display="block" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">&#x02016;</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>[11]</label>
</disp-formula><p>Since the objective function is difficult to derive, it is not possible to derive the optimal solution of <inline-formula id="pone.0323631.e112"><alternatives><graphic xlink:href="pone.0323631.e112.jpg" id="pone.0323631.e112g" position="anchor"/><mml:math id="M112" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, therefore an iterative method is used to find the incremental <inline-formula id="pone.0323631.e113"><alternatives><graphic xlink:href="pone.0323631.e113.jpg" id="pone.0323631.e113g" position="anchor"/><mml:math id="M113" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x025b3;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, making the error <inline-formula id="pone.0323631.e114"><alternatives><graphic xlink:href="pone.0323631.e114.jpg" id="pone.0323631.e114g" position="anchor"/><mml:math id="M114" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">&#x02016;</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x025b3;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> reach a minimal value. Given an initial value <inline-formula id="pone.0323631.e115"><alternatives><graphic xlink:href="pone.0323631.e115.jpg" id="pone.0323631.e115g" position="anchor"/><mml:math id="M115" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, for the <inline-formula id="pone.0323631.e116"><alternatives><graphic xlink:href="pone.0323631.e116.jpg" id="pone.0323631.e116g" position="anchor"/><mml:math id="M116" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> th iteration, set the trust region, in the fulfillment of <inline-formula id="pone.0323631.e117"><alternatives><graphic xlink:href="pone.0323631.e117.jpg" id="pone.0323631.e117g" position="anchor"/><mml:math id="M117" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">&#x02016;</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mi>&#x025b3;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>&#x02264;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> finding the minimum value, the formula is expressed as follows:</p><disp-formula id="pone.0323631.e118">
<alternatives><graphic xlink:href="pone.0323631.e118.jpg" id="pone.0323631.e118g" position="anchor"/><mml:math id="M118" display="block" overflow="scroll"><mml:mrow><mml:mo>min</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">&#x02016;</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>&#x025b3;</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">&#x02016;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></alternatives>
<label>[12]</label>
</disp-formula><p>Where, <inline-formula id="pone.0323631.e119"><alternatives><graphic xlink:href="pone.0323631.e119.jpg" id="pone.0323631.e119g" position="anchor"/><mml:math id="M119" display="inline" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the radius of confidence. <inline-formula id="pone.0323631.e120"><alternatives><graphic xlink:href="pone.0323631.e120.jpg" id="pone.0323631.e120g" position="anchor"/><mml:math id="M120" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the matrix of coefficients; <inline-formula id="pone.0323631.e121"><alternatives><graphic xlink:href="pone.0323631.e121.jpg" id="pone.0323631.e121g" position="anchor"/><mml:math id="M121" display="inline" overflow="scroll"><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the initial Jacobi matrix.</p><p>Solve the incremental equation by simplifying the derivatives with the Lagrangian function, and the objective function <inline-formula id="pone.0323631.e122"><alternatives><graphic xlink:href="pone.0323631.e122.jpg" id="pone.0323631.e122g" position="anchor"/><mml:math id="M122" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>&#x003c8;</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is substituting into the incremental equation, can obtain:</p><disp-formula id="pone.0323631.e123">
<alternatives><graphic xlink:href="pone.0323631.e123.jpg" id="pone.0323631.e123g" position="anchor"/><mml:math id="M123" display="block" overflow="scroll"><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>O</mml:mi><mml:mi>&#x003c8;</mml:mi><mml:mtext>=</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003d6;</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:math></alternatives>
<label>[13]</label>
</disp-formula><disp-formula id="pone.0323631.e124">
<alternatives><graphic xlink:href="pone.0323631.e124.jpg" id="pone.0323631.e124g" position="anchor"/><mml:math id="M124" display="block" overflow="scroll"><mml:mrow><mml:mi>&#x025b3;</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003d6;</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>O</mml:mi><mml:mi>&#x003c8;</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:math></alternatives>
<label>[14]</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e125"><alternatives><graphic xlink:href="pone.0323631.e125.jpg" id="pone.0323631.e125g" position="anchor"/><mml:math id="M125" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003d6;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the Lagrange multiplier.</p><p>By continuously iterating the incremental equations, update the position estimates <inline-formula id="pone.0323631.e126"><alternatives><graphic xlink:href="pone.0323631.e126.jpg" id="pone.0323631.e126g" position="anchor"/><mml:math id="M126" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>.</p><disp-formula id="pone.0323631.e127">
<alternatives><graphic xlink:href="pone.0323631.e127.jpg" id="pone.0323631.e127g" position="anchor"/><mml:math id="M127" display="block" overflow="scroll"><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02202;</mml:mo><mml:mi>&#x003c8;</mml:mi><mml:mi>&#x025b3;</mml:mi><mml:mi>T</mml:mi><mml:mo>/</mml:mo><mml:mo>&#x02202;</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives>
<label>[15]</label>
</disp-formula><p>Figure optimized laser SLAM includes two structures: node and edge, which correspond to the position and attitude of the tractor and their relative transformation relationship [<xref rid="pone.0323631.ref016" ref-type="bibr">16</xref>]. Image optimization laser SLAM obtains point cloud data for each frame through laser radar scanning, matches two consecutive frames of point cloud data, and calculates the tractor pose. <xref rid="pone.0323631.g003" ref-type="fig">Fig 3</xref> shows the structure diagram of the graph optimized laser SLAM algorithm, where different waypoints can be observed when the tractor obtains different poses as the input changes [<xref rid="pone.0323631.ref017" ref-type="bibr">17</xref>]. The waypoints obtained at different times can constitute the pose estimation results of mountainous environments. Nodes <inline-formula id="pone.0323631.e128"><alternatives><graphic xlink:href="pone.0323631.e128.jpg" id="pone.0323631.e128g" position="anchor"/><mml:math id="M128" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0323631.e129"><alternatives><graphic xlink:href="pone.0323631.e129.jpg" id="pone.0323631.e129g" position="anchor"/><mml:math id="M129" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> in the figure are represented as the position information of the LiDAR (tractor) and the information of the observed road marking points, respectively. The edges of position node-position node and position node-feature node in the figure represent the motion constraints and observation constraints of the LiDAR, respectively.</p><fig position="float" id="pone.0323631.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.g003</object-id><label>Fig 3</label><caption><title>Structural diagram of optimized laser SLAM algorithm.</title></caption><graphic xlink:href="pone.0323631.g003" position="float"/></fig><p>The side from the pose node to the feature node is the observation side, which can be obtained from the observation equation of the laser radar. The edge from the pose node to the next pose node is the moving edge, which can be obtained from the location part of SLAM algorithm.</p><p>After pose estimation, the point cloud data is optimized through non-uniform quadtree to reduce redundant points and improve the resolution and accuracy of the map. Divide the map into multiple quadtree nodes, with each node representing an area. Dynamically adjust the size of nodes based on point cloud density and terrain complexity. <inline-formula id="pone.0323631.e130"><alternatives><graphic xlink:href="pone.0323631.e130.jpg" id="pone.0323631.e130g" position="anchor"/><mml:math id="M130" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>max</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pone.0323631.e131"><alternatives><graphic xlink:href="pone.0323631.e131.jpg" id="pone.0323631.e131g" position="anchor"/><mml:math id="M131" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the edge length of the node, <inline-formula id="pone.0323631.e132"><alternatives><graphic xlink:href="pone.0323631.e132.jpg" id="pone.0323631.e132g" position="anchor"/><mml:math id="M132" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>max</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the maximum edge length of the node, and <inline-formula id="pone.0323631.e133"><alternatives><graphic xlink:href="pone.0323631.e133.jpg" id="pone.0323631.e133g" position="anchor"/><mml:math id="M133" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the number of layers the node is divided into. After each LiDAR scan, insert the newly acquired point cloud data into the quadtree map. Dynamically adjust the resolution of quadtree nodes based on point cloud distribution and terrain features.</p><disp-formula id="pone.0323631.e134">
<alternatives><graphic xlink:href="pone.0323631.e134.jpg" id="pone.0323631.e134g" position="anchor"/><mml:math id="M134" display="block" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>\arg</mml:mi><mml:munder><mml:mrow><mml:mrow><mml:mo>min</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mspace width="0.167em"/><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x0003e;</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(16)</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e135"><alternatives><graphic xlink:href="pone.0323631.e135.jpg" id="pone.0323631.e135g" position="anchor"/><mml:math id="M135" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the number of points within the <inline-formula id="pone.0323631.e136"><alternatives><graphic xlink:href="pone.0323631.e136.jpg" id="pone.0323631.e136g" position="anchor"/><mml:math id="M136" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> th layer node, and <inline-formula id="pone.0323631.e137"><alternatives><graphic xlink:href="pone.0323631.e137.jpg" id="pone.0323631.e137g" position="anchor"/><mml:math id="M137" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the point density threshold. Based on the semantic information of the obstacles mentioned above, associate the semantic labels of the target obstacles obtained in Neural network-based semantic information extraction for obstacles with the quadtree nodes.</p><disp-formula id="pone.0323631.e138">
<alternatives><graphic xlink:href="pone.0323631.e138.jpg" id="pone.0323631.e138g" position="anchor"/><mml:math id="M138" display="block" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>_</mml:mi><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02208;</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true"/></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(17)</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e139"><alternatives><graphic xlink:href="pone.0323631.e139.jpg" id="pone.0323631.e139g" position="anchor"/><mml:math id="M139" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the semantic label of node n, and <inline-formula id="pone.0323631.e140"><alternatives><graphic xlink:href="pone.0323631.e140.jpg" id="pone.0323631.e140g" position="anchor"/><mml:math id="M140" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the semantic label of point <inline-formula id="pone.0323631.e141"><alternatives><graphic xlink:href="pone.0323631.e141.jpg" id="pone.0323631.e141g" position="anchor"/><mml:math id="M141" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. And using the constraint relationship between points and surfaces mentioned above, estimate the pose changes of the tractor.</p><disp-formula id="pone.0323631.e142">
<alternatives><graphic xlink:href="pone.0323631.e142.jpg" id="pone.0323631.e142g" position="anchor"/><mml:math id="M142" display="block" overflow="scroll"><mml:mrow><mml:mi>&#x003be;</mml:mi><mml:mo>=</mml:mo><mml:mi>\arg</mml:mi><mml:munder><mml:mrow><mml:mrow><mml:mo>min</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>&#x003be;</mml:mi></mml:mrow></mml:munder><mml:mspace width="0.167em"/><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(18)</label>
</disp-formula><p>Combine the point cloud data of each frame with the sub map data generated in the previous frame to generate a single sub map. Namely:</p><disp-formula id="pone.0323631.e143">
<alternatives><graphic xlink:href="pone.0323631.e143.jpg" id="pone.0323631.e143g" position="anchor"/><mml:math id="M143" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x022c3;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(19)</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e144"><alternatives><graphic xlink:href="pone.0323631.e144.jpg" id="pone.0323631.e144g" position="anchor"/><mml:math id="M144" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the sub map of frame <inline-formula id="pone.0323631.e145"><alternatives><graphic xlink:href="pone.0323631.e145.jpg" id="pone.0323631.e145g" position="anchor"/><mml:math id="M145" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pone.0323631.e146"><alternatives><graphic xlink:href="pone.0323631.e146.jpg" id="pone.0323631.e146g" position="anchor"/><mml:math id="M146" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the point cloud data of frame <inline-formula id="pone.0323631.e147"><alternatives><graphic xlink:href="pone.0323631.e147.jpg" id="pone.0323631.e147g" position="anchor"/><mml:math id="M147" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. All sub maps are iteratively updated to generate a global point cloud map, <inline-formula id="pone.0323631.e148"><alternatives><graphic xlink:href="pone.0323631.e148.jpg" id="pone.0323631.e148g" position="anchor"/><mml:math id="M148" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x022c3;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Through the above steps and mathematical expressions, combined with the optimization processing steps of non-uniform quadtree maps, high-precision global point cloud maps can be generated.</p></sec><sec id="sec008"><title>Tractor obstacle avoidance path planning based on A* algorithm and DWA algorithm</title><p>A* algorithm is a widely used path search and graph traversal algorithm, especially suitable for finding the shortest path between two points [<xref rid="pone.0323631.ref018" ref-type="bibr">18</xref>]. In the autonomous obstacle avoidance of mountain tractors, according to the obstacle information in the semantic map, we can identify the obstacles that may affect the tractor&#x02019;s driving, and evaluate their danger level, that is, the cost. A* algorithm can help the tractor find the optimal obstacle avoidance path from the current position to the target position. Set up <inline-formula id="pone.0323631.e149"><alternatives><graphic xlink:href="pone.0323631.e149.jpg" id="pone.0323631.e149g" position="anchor"/><mml:math id="M149" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denotes the coordinate position node in the mountain environment where the tractor is currently located, <inline-formula id="pone.0323631.e150"><alternatives><graphic xlink:href="pone.0323631.e150.jpg" id="pone.0323631.e150g" position="anchor"/><mml:math id="M150" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the target point. According to the idea of map search, the mountain map is divided into squares with identical specifications. The detailed search steps of the A* algorithm are as follows:</p><list list-type="simple"><list-item><p>(1) Cost function definition. The A* algorithm evaluates the cost from the tractor start to an arbitrary node <inline-formula id="pone.0323631.e151"><alternatives><graphic xlink:href="pone.0323631.e151.jpg" id="pone.0323631.e151g" position="anchor"/><mml:math id="M151" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> in the mountain map using the cost function <inline-formula id="pone.0323631.e152"><alternatives><graphic xlink:href="pone.0323631.e152.jpg" id="pone.0323631.e152g" position="anchor"/><mml:math id="M152" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. It usually consists of two parts, the actual moving cost <inline-formula id="pone.0323631.e153"><alternatives><graphic xlink:href="pone.0323631.e153.jpg" id="pone.0323631.e153g" position="anchor"/><mml:math id="M153" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> of the tractor from the starting point to the node <inline-formula id="pone.0323631.e154"><alternatives><graphic xlink:href="pone.0323631.e154.jpg" id="pone.0323631.e154g" position="anchor"/><mml:math id="M154" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and the estimated cost <inline-formula id="pone.0323631.e155"><alternatives><graphic xlink:href="pone.0323631.e155.jpg" id="pone.0323631.e155g" position="anchor"/><mml:math id="M155" display="inline" overflow="scroll"><mml:mrow><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> from the node <inline-formula id="pone.0323631.e156"><alternatives><graphic xlink:href="pone.0323631.e156.jpg" id="pone.0323631.e156g" position="anchor"/><mml:math id="M156" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> to the target point <inline-formula id="pone.0323631.e157"><alternatives><graphic xlink:href="pone.0323631.e157.jpg" id="pone.0323631.e157g" position="anchor"/><mml:math id="M157" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Thus, the complete cost function can be expressed as follows:</p></list-item></list><disp-formula id="pone.0323631.e158">
<alternatives><graphic xlink:href="pone.0323631.e158.jpg" id="pone.0323631.e158g" position="anchor"/><mml:math id="M158" display="block" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:math></alternatives>
<label>[20]</label>
</disp-formula><list list-type="simple"><list-item><p>(2) Initialization and node expansion. Put the starting point <inline-formula id="pone.0323631.e159"><alternatives><graphic xlink:href="pone.0323631.e159.jpg" id="pone.0323631.e159g" position="anchor"/><mml:math id="M159" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> into the open list (list of nodes to be searched) and the cost of the nodes around its tractor is calculated. These surrounding nodes are added to the open list and sorted according to the cost function.</p></list-item><list-item><p>(3) Select the next node. Select the node with the lowest cost from the open list as the current node and move it to the closed list.</p></list-item><list-item><p>(4) Continue searching. Expand all the neighboring nodes of the current node which are not added to the closed list, calculate their cost and update the open list.</p></list-item><list-item><p>(5) Termination conditions. If the target point <inline-formula id="pone.0323631.e160"><alternatives><graphic xlink:href="pone.0323631.e160.jpg" id="pone.0323631.e160g" position="anchor"/><mml:math id="M160" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is added to the open list, the shortest path from the starting point to the target point is found by backtracking. If the open list is empty but the target point is still not in it, the tractor has no passable obstacle avoidance path.</p></list-item></list><p>After the tractor gets the target point, the accessible obstacle avoidance path of the tractor in the mountainous environment is obtained through A* algorithm, and then the track calculation is carried out using DWA algorithm to obtain the corresponding tractor movement strategy. DWA algorithm is a real-time trajectory planning algorithm for mobile tractors in a dynamic environment, which can consider the dynamic constraints and obstacle avoidance requirements of tractors [<xref rid="pone.0323631.ref019" ref-type="bibr">19</xref>]. The detailed operation steps of DWA algorithm are as follows:</p><p>(1) Velocity space sampling. Samples within the speed space <inline-formula id="pone.0323631.e161"><alternatives><graphic xlink:href="pone.0323631.e161.jpg" id="pone.0323631.e161g" position="anchor"/><mml:math id="M161" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> of the tractor to generate a series of possible control commands.</p><p>(2) Trajectory simulation. For each sampling point, the movement trajectory of the tractor within a given time interval <inline-formula id="pone.0323631.e162"><alternatives><graphic xlink:href="pone.0323631.e162.jpg" id="pone.0323631.e162g" position="anchor"/><mml:math id="M162" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> was simulated. Use formula [<xref rid="pone.0323631.ref001" ref-type="bibr">1</xref>] to calculate the new position <inline-formula id="pone.0323631.e163"><alternatives><graphic xlink:href="pone.0323631.e163.jpg" id="pone.0323631.e163g" position="anchor"/><mml:math id="M163" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and the new heading angle <inline-formula id="pone.0323631.e164"><alternatives><graphic xlink:href="pone.0323631.e164.jpg" id="pone.0323631.e164g" position="anchor"/><mml:math id="M164" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, the formula is expressed as:</p><disp-formula id="pone.0323631.e165">
<alternatives><graphic xlink:href="pone.0323631.e165.jpg" id="pone.0323631.e165g" position="anchor"/><mml:math id="M165" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:msup><mml:mi>x</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:msup><mml:mi>y</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:mspace width="negativethinmathspace"/><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" form="postfix" stretchy="true"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>[21]</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e166"><alternatives><graphic xlink:href="pone.0323631.e166.jpg" id="pone.0323631.e166g" position="anchor"/><mml:math id="M166" display="inline" overflow="scroll"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denotes the linear speed of the tractor. <inline-formula id="pone.0323631.e167"><alternatives><graphic xlink:href="pone.0323631.e167.jpg" id="pone.0323631.e167g" position="anchor"/><mml:math id="M167" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> denotes the angular velocity of the tractor; <inline-formula id="pone.0323631.e168"><alternatives><graphic xlink:href="pone.0323631.e168.jpg" id="pone.0323631.e168g" position="anchor"/><mml:math id="M168" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> indicates the initial heading angle.</p><p>(3) Space time constraint setting</p><p>In the cost function of the A * algorithm, add the terrain complexity factor <inline-formula id="pone.0323631.e169"><alternatives><graphic xlink:href="pone.0323631.e169.jpg" id="pone.0323631.e169g" position="anchor"/><mml:math id="M169" display="inline" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>:</p><disp-formula id="pone.0323631.e170">
<alternatives><graphic xlink:href="pone.0323631.e170.jpg" id="pone.0323631.e170g" position="anchor"/><mml:math id="M170" display="block" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>&#x003c1;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:math></alternatives>
<label>(22)</label>
</disp-formula><p>Among them, <inline-formula id="pone.0323631.e171"><alternatives><graphic xlink:href="pone.0323631.e171.jpg" id="pone.0323631.e171g" position="anchor"/><mml:math id="M171" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the slope at node <inline-formula id="pone.0323631.e172"><alternatives><graphic xlink:href="pone.0323631.e172.jpg" id="pone.0323631.e172g" position="anchor"/><mml:math id="M172" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0323631.e173"><alternatives><graphic xlink:href="pone.0323631.e173.jpg" id="pone.0323631.e173g" position="anchor"/><mml:math id="M173" display="inline" overflow="scroll"><mml:mrow><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the roughness at node <inline-formula id="pone.0323631.e174"><alternatives><graphic xlink:href="pone.0323631.e174.jpg" id="pone.0323631.e174g" position="anchor"/><mml:math id="M174" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pone.0323631.e175"><alternatives><graphic xlink:href="pone.0323631.e175.jpg" id="pone.0323631.e175g" position="anchor"/><mml:math id="M175" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0323631.e176"><alternatives><graphic xlink:href="pone.0323631.e176.jpg" id="pone.0323631.e176g" position="anchor"/><mml:math id="M176" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> are weight coefficients.</p><p>Add safety distance constraints to obstacles in the trajectory scoring of DWA algorithm:</p><disp-formula id="pone.0323631.e177">
<alternatives><graphic xlink:href="pone.0323631.e177.jpg" id="pone.0323631.e177g" position="anchor"/><mml:math id="M177" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mo>min</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msqrt><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(23)</label>
</disp-formula><p>Among them, (xt, yt) are points on the trajectory, and (xo, yo) are the positions of obstacles.</p><p>(4) Trajectory rating</p><p>Based on the above constraint settings,trajectory scoring. The trajectories of each simulated tractor were scored to evaluate the merits. The scoring function <inline-formula id="pone.0323631.e178"><alternatives><graphic xlink:href="pone.0323631.e178.jpg" id="pone.0323631.e178g" position="anchor"/><mml:math id="M178" display="inline" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> considering deviations from the target heading <inline-formula id="pone.0323631.e179"><alternatives><graphic xlink:href="pone.0323631.e179.jpg" id="pone.0323631.e179g" position="anchor"/><mml:math id="M179" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, deviation from the target position <inline-formula id="pone.0323631.e180"><alternatives><graphic xlink:href="pone.0323631.e180.jpg" id="pone.0323631.e180g" position="anchor"/><mml:math id="M180" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and velocity deviations <inline-formula id="pone.0323631.e181"><alternatives><graphic xlink:href="pone.0323631.e181.jpg" id="pone.0323631.e181g" position="anchor"/><mml:math id="M181" display="inline" overflow="scroll"><mml:mrow><mml:mi>\</mml:mi><mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, the formula is expressed as follows:</p><disp-formula id="pone.0323631.e182">
<alternatives><graphic xlink:href="pone.0323631.e182.jpg" id="pone.0323631.e182g" position="anchor"/><mml:math id="M182" display="block" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>&#x003b8;</mml:mi><mml:mi>&#x02032;</mml:mi></mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives>
<label>[24]</label>
</disp-formula><p>(5) Selection of optimal trajectories. The tractor trajectory with the highest score is selected as the optimal trajectory, and the tractor movement is controlled according to the first control command of this trajectory.</p><p>(6) Iteration. In order to ensure that the tractor can reach its destination safely, efficiently and without any obstacles [<xref rid="pone.0323631.ref020" ref-type="bibr">20</xref>], the above process is repeated continuously in order to continuously adjust the robot&#x02019;s trajectory.</p><p>The combination of A* algorithm and DWA algorithm can realize autonomous obstacle avoidance path planning of mobile tractor in mountainous environment. Algorithm A is responsible for global path planning and finding the approximate path from the starting point to the target point. DWA algorithm is responsible for local trajectory planning and real-time obstacle avoidance to ensure that the tractor can safely and accurately travel along the global path planned by A* algorithm in complex mountain environment. Through the cooperation of these two algorithms, the autonomous obstacle avoidance ability of mobile tractors in complex mountain environment can be significantly improved.</p></sec></sec><sec id="sec009"><title>Experimental analysis</title><sec id="sec010"><title>Experimental environment</title><p>In order to verify the effectiveness of the autonomous obstacle avoidance method proposed in this article for mountain tractors, a representative test area was selected in hilly and mountainous areas, which includes complex terrain such as slopes of different slopes, ravines of varying depths, and dense tree clusters. The slope changes frequently in hilly and mountainous areas, which poses higher requirements for the tractor&#x02019;s power system and suspension system. Compared with the flat terrain of traditional farmland, mountain tractors need to maintain stability and maneuverability on slopes of different slopes, and the autonomous obstacle avoidance system needs to accurately determine the impact of slope on the driving path. Gullies of varying depths and scattered stones and rocks are common obstacles in mountainous environments. These obstacles not only have irregular shapes, but their positions are also difficult to predict, increasing the complexity and difficulty of obstacle avoidance systems. In contrast, obstacles in traditional farmland are usually more regular and easy to identify. Comprehensively test the tractor&#x02019;s autonomous obstacle avoidance ability in different environments in this complex environment. In the testing area, simulate obstacles such as stones, wooden stakes, and simulated trees according to the experimental design. Selecting the Dongfanghong hilly mountain tractor as the experimental object.</p><p>As shown in <xref rid="pone.0323631.g004" ref-type="fig">Fig 4</xref>. Ensure that the Dongfanghong tractor is in good working condition, with all mechanical components and electrical systems functioning properly. Install laser radar, GPS receiver, IMU, and other sensors on the Dongfanghong tractor according to the predetermined plan. Lidar should be installed at the front of the tractor to obtain detailed data on the environment ahead; The GPS receiver and IMU should be installed on the top of the tractor or in an appropriate position to reduce signal interference and ensure data accuracy. During installation, use specialized tools and calibration equipment to ensure that the sensor&#x02019;s position and angle are correct. The parameters of the experimental equipment used in this paper are shown in <xref rid="pone.0323631.t001" ref-type="table">Table 1</xref>.</p><table-wrap position="float" id="pone.0323631.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.t001</object-id><label>Table 1</label><caption><title>Experimental equipment parameters.</title></caption><alternatives><graphic xlink:href="pone.0323631.t001" id="pone.0323631.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Equipment name</th><th align="left" rowspan="1" colspan="1">Model/Specification</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Laser Radar</td><td align="left" rowspan="1" colspan="1">RPLIDAR-A1</td></tr><tr><td align="left" rowspan="1" colspan="1">Tractor platform</td><td align="left" rowspan="1" colspan="1">Dongfanghong tractor</td></tr><tr><td align="left" rowspan="1" colspan="1">Edge computing Device</td><td align="left" rowspan="1" colspan="1">NVIDIA Jetson AGX Xavier</td></tr><tr><td align="left" rowspan="1" colspan="1">GPS receiver</td><td align="left" rowspan="1" colspan="1">Ublox NEO-M8N</td></tr><tr><td align="left" rowspan="1" colspan="1">IMU (Inertial Measurement Unit)</td><td align="left" rowspan="1" colspan="1">Xsens MTI-30</td></tr><tr><td align="left" rowspan="1" colspan="1">Sensor bracket and installation accessories</td><td align="left" rowspan="1" colspan="1">Customized</td></tr><tr><td align="left" rowspan="1" colspan="1">Wireless communication module</td><td align="left" rowspan="1" colspan="1">Wi-Fi</td></tr><tr><td align="left" rowspan="1" colspan="1">Data storage device</td><td align="left" rowspan="1" colspan="1">SSD</td></tr><tr><td align="left" rowspan="1" colspan="1">Monitor</td><td align="left" rowspan="1" colspan="1">Portable monitor</td></tr><tr><td align="left" rowspan="1" colspan="1">RVIZ (Visualization Tool)</td><td align="left" rowspan="1" colspan="1">Shanhai Whale</td></tr></tbody></table></alternatives></table-wrap><fig position="float" id="pone.0323631.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.g004</object-id><label>Fig 4</label><caption><title>Dongfanghong tractor.</title></caption><graphic xlink:href="pone.0323631.g004" position="float"/></fig><p>Synchronize the data acquisition time of laser radar, GPS receiver, IMU and other sensors, and calibrate each sensor separately to ensure that the data they acquire can accurately reflect the relative relationship between the tractor and the surrounding environment. Start the laser radar and other sensors, collect and process the mountain environment data, set the starting point and target point of the tractor in the mountain environment, and obtain the obstacle detection results of the tractor when the tractor is running, as shown in <xref rid="pone.0323631.g005" ref-type="fig">Fig 5</xref>.</p><fig position="float" id="pone.0323631.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.g005</object-id><label>Fig 5</label><caption><title>Obstacle detection results in mountainous environment.</title></caption><graphic xlink:href="pone.0323631.g005" position="float"/></fig><p>It can be clearly seen from <xref rid="pone.0323631.g005" ref-type="fig">Fig 5</xref> that the smooth surface and irregular shape of rocks in mountain environment are successfully captured by the method in this paper, which effectively distinguishes the difference between rocks and surrounding environment, accurately identifies and marks a large rock at the center of the photo as an obstacle, proving that even under the interference of complex grassland background and disordered branches and grass leaves, It still maintains a high degree of accuracy and robustness, and shows significant effectiveness in the obstacle detection task in mountain environment.</p></sec><sec id="sec011"><title>Analysis of experimental results</title><p>In order to verify the validity of the mountain environment map constructed by the method of this paper, the mountain environment is continuously monitored for a long time in order to obtain the radar data of the mountain environment, which is processed to extract the geomorphological features, and the results of the construction of the mountain environment map by the method of this paper are shown in <xref rid="pone.0323631.g006" ref-type="fig">Fig 6</xref>.</p><fig position="float" id="pone.0323631.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.g006</object-id><label>Fig 6</label><caption><title>Construction Results of Mountain Environment Map.</title></caption><graphic xlink:href="pone.0323631.g006" position="float"/></fig><p>As can be seen in <xref rid="pone.0323631.g006" ref-type="fig">Fig 6</xref>, the mountain environment map constructed in this paper provides real-time and comprehensive environmental information for mobile devices such as mountain tractors by finely depicting the temperature differences and terrain features in the region. During the traveling process, the tractor can use the highlighted areas in the map to identify potential heat sources or terrain obstacles, such as steep slopes, rock piles, or dense vegetation areas, so as to plan an avoidance path in advance and avoid the risk of collision. In addition, the temperature data in the map can also help identify changes in soil moisture and stability that may result from temperature differences, further reducing the occurrence of accidents such as stuck vehicles.</p><p>In order to verify the effectiveness of the method in this paper for estimating the position and attitude of mountain tractors, simulate the position and attitude of tractors in five different scenes, use high-precision measuring equipment to record the real position and attitude data of tractors, including the angle of the tractor, the distance between the tractor and the x axis, and the distance between the tractor and the y axis. At the same time, apply the GET method FGM method and the method in this paper estimate the position and attitude of the tractor, record the estimated values, and verify the accuracy and reliability of the three methods in the position and attitude estimation of the tractor. The verification results are shown in <xref rid="pone.0323631.t002" ref-type="table">Table 2</xref>.</p><table-wrap position="float" id="pone.0323631.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.t002</object-id><label>Table 2</label><caption><title>Tractor pose estimation results using different methods.</title></caption><alternatives><graphic xlink:href="pone.0323631.t002" id="pone.0323631.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Number of experiments</th><th align="left" rowspan="1" colspan="1">Test indicators</th><th align="left" rowspan="1" colspan="1">True value</th><th align="left" rowspan="1" colspan="1">GET method</th><th align="left" rowspan="1" colspan="1">FGM method</th><th align="left" rowspan="1" colspan="1">Proposed method</th></tr></thead><tbody><tr><td align="left" rowspan="3" colspan="1">1</td><td align="left" rowspan="1" colspan="1">Angle/&#x000b0;</td><td align="left" rowspan="1" colspan="1">123.4</td><td align="left" rowspan="1" colspan="1">122.8</td><td align="left" rowspan="1" colspan="1">122.5</td><td align="left" rowspan="1" colspan="1">123.2</td></tr><tr><td align="left" rowspan="1" colspan="1">X-axis distance/cm</td><td align="left" rowspan="1" colspan="1">234.5</td><td align="left" rowspan="1" colspan="1">233.8</td><td align="left" rowspan="1" colspan="1">233.5</td><td align="left" rowspan="1" colspan="1">234</td></tr><tr><td align="left" rowspan="1" colspan="1">Y-axis distance/cm</td><td align="left" rowspan="1" colspan="1">345.6</td><td align="left" rowspan="1" colspan="1">345</td><td align="left" rowspan="1" colspan="1">344.8</td><td align="left" rowspan="1" colspan="1">345.4</td></tr><tr><td align="left" rowspan="3" colspan="1">2</td><td align="left" rowspan="1" colspan="1">Angle/&#x000b0;</td><td align="left" rowspan="1" colspan="1">135.7</td><td align="left" rowspan="1" colspan="1">135.2</td><td align="left" rowspan="1" colspan="1">135</td><td align="left" rowspan="1" colspan="1">135.5</td></tr><tr><td align="left" rowspan="1" colspan="1">X-axis distance/cm</td><td align="left" rowspan="1" colspan="1">456.7</td><td align="left" rowspan="1" colspan="1">456.1</td><td align="left" rowspan="1" colspan="1">455.9</td><td align="left" rowspan="1" colspan="1">456.3</td></tr><tr><td align="left" rowspan="1" colspan="1">Y-axis distance/cm</td><td align="left" rowspan="1" colspan="1">567.8</td><td align="left" rowspan="1" colspan="1">567.3</td><td align="left" rowspan="1" colspan="1">567.1</td><td align="left" rowspan="1" colspan="1">567.6</td></tr><tr><td align="left" rowspan="3" colspan="1">3</td><td align="left" rowspan="1" colspan="1">Angle/&#x000b0;</td><td align="left" rowspan="1" colspan="1">90.1</td><td align="left" rowspan="1" colspan="1">89.8</td><td align="left" rowspan="1" colspan="1">89.6</td><td align="left" rowspan="1" colspan="1">90</td></tr><tr><td align="left" rowspan="1" colspan="1">X-axis distance/cm</td><td align="left" rowspan="1" colspan="1">678.9</td><td align="left" rowspan="1" colspan="1">678.2</td><td align="left" rowspan="1" colspan="1">677.9</td><td align="left" rowspan="1" colspan="1">678.5</td></tr><tr><td align="left" rowspan="1" colspan="1">Y-axis distance/cm</td><td align="left" rowspan="1" colspan="1">789</td><td align="left" rowspan="1" colspan="1">788.5</td><td align="left" rowspan="1" colspan="1">788.3</td><td align="left" rowspan="1" colspan="1">788.8</td></tr><tr><td align="left" rowspan="3" colspan="1">4</td><td align="left" rowspan="1" colspan="1">Angle/&#x000b0;</td><td align="left" rowspan="1" colspan="1">150.2</td><td align="left" rowspan="1" colspan="1">149.8</td><td align="left" rowspan="1" colspan="1">149.6</td><td align="left" rowspan="1" colspan="1">150</td></tr><tr><td align="left" rowspan="1" colspan="1">X-axis distance/cm</td><td align="left" rowspan="1" colspan="1">890.1</td><td align="left" rowspan="1" colspan="1">889.5</td><td align="left" rowspan="1" colspan="1">889.3</td><td align="left" rowspan="1" colspan="1">889.8</td></tr><tr><td align="left" rowspan="1" colspan="1">Y-axis distance/cm</td><td align="left" rowspan="1" colspan="1">901.2</td><td align="left" rowspan="1" colspan="1">900.8</td><td align="left" rowspan="1" colspan="1">900.6</td><td align="left" rowspan="1" colspan="1">901</td></tr><tr><td align="left" rowspan="3" colspan="1">5</td><td align="left" rowspan="1" colspan="1">Angle/&#x000b0;</td><td align="left" rowspan="1" colspan="1">67.3</td><td align="left" rowspan="1" colspan="1">67</td><td align="left" rowspan="1" colspan="1">66.8</td><td align="left" rowspan="1" colspan="1">67.2</td></tr><tr><td align="left" rowspan="1" colspan="1">X-axis distance/cm</td><td align="left" rowspan="1" colspan="1">101.2</td><td align="left" rowspan="1" colspan="1">100.8</td><td align="left" rowspan="1" colspan="1">100.6</td><td align="left" rowspan="1" colspan="1">101</td></tr><tr><td align="left" rowspan="1" colspan="1">Y-axis distance/cm</td><td align="left" rowspan="1" colspan="1">112.3</td><td align="left" rowspan="1" colspan="1">112</td><td align="left" rowspan="1" colspan="1">111.8</td><td align="left" rowspan="1" colspan="1">112.2</td></tr></tbody></table></alternatives></table-wrap><p>As can be seen from <xref rid="pone.0323631.t002" ref-type="table">Table 2</xref>, the method in this paper shows high accuracy in angle and distance estimation in all experimental times. Compared with the true value, the estimation error of this method is the smallest, especially the angle estimation is almost close to the true value. In contrast, GET method and FGM method can also give reasonable estimates, but the error is relatively large. At the same time, the estimated value of the method in this paper fluctuates slightly in different experimental times, showing a high stability. It is proved that the profit method in this paper has strong adaptability to different environmental conditions and tractor conditions.</p><p>In order to verify the autonomous obstacle avoidance ability of the tractor in the mountain environment, we set the starting point and the end point as the starting and ending positions of the tractor&#x02019;s traveling path, and set a number of irregularly shaped obstacles in the journey, start the tractor, observe and record the tractor&#x02019;s response to the obstacles that may appear along the way from the starting point to the end position, and obtain the tractor&#x02019;s path planning results in <xref rid="pone.0323631.g007" ref-type="fig">Fig 7</xref>.</p><fig position="float" id="pone.0323631.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.g007</object-id><label>Fig 7</label><caption><title>Obstacle avoidance results of mountain tractors using the method proposed in this article.</title></caption><graphic xlink:href="pone.0323631.g007" position="float"/></fig><p>As can be seen in <xref rid="pone.0323631.g007" ref-type="fig">Fig 7</xref>, the tractor started from the set starting point and skillfully avoided all the irregularly shaped obstacles along the way, showing high environmental adaptability and intelligent navigation ability. It is especially worth noting that its driving path not only successfully bypasses every obstacle, but also realizes the optimization of the path, ensuring that the distance from the starting point to the end point is the shortest, and there is no unnecessary detouring or turning back in the whole process. It proves the accuracy and efficiency of this method, and also fully proves the reliability and practicality of this method in the complex mountain environment.</p><p>In order to further verify the effectiveness of different methods for obstacle avoidance of mountain tractors, a unified starting point and end point are set in the mountain environment, and a number of obstacles with different shapes and positions are arranged between them to simulate the obstacle avoidance challenges in the real work scene. The GET method, FGM method and the method in this paper are applied to the same tractor, and the key performance indicators under each method are recorded, including the driving path length, driving time, number of successful obstacle avoidance, turn back times and driving stability scores. In order to ensure the fairness and repeatability of the experimental results, the same experimental conditions and test standards were used, and repeated tests were conducted for each method for many times to obtain the average data as the final evaluation basis. The comparison results of obstacle avoidance performance of mountain tractors with three methods are shown in <xref rid="pone.0323631.t003" ref-type="table">Table 3</xref>.</p><table-wrap position="float" id="pone.0323631.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.t003</object-id><label>Table 3</label><caption><title>Comparison results of obstacle avoidance performance of mountain tractors using different methods.</title></caption><alternatives><graphic xlink:href="pone.0323631.t003" id="pone.0323631.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Evaluating indicator</th><th align="left" rowspan="1" colspan="1">GET method</th><th align="left" rowspan="1" colspan="1">FGM method</th><th align="left" rowspan="1" colspan="1">Proposed method</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Road length/m</td><td align="left" rowspan="1" colspan="1">131.6</td><td align="left" rowspan="1" colspan="1">135.3</td><td align="left" rowspan="1" colspan="1">127.8</td></tr><tr><td align="left" rowspan="1" colspan="1">Travel time/m</td><td align="left" rowspan="1" colspan="1">182.5</td><td align="left" rowspan="1" colspan="1">187.4</td><td align="left" rowspan="1" colspan="1">159.7</td></tr><tr><td align="left" rowspan="1" colspan="1">Number of successfully avoided obstacles/each</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">5</td></tr><tr><td align="left" rowspan="1" colspan="1">Number of turns/times</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">0</td></tr><tr><td align="left" rowspan="1" colspan="1">Stability score (over 10 points)</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">10</td></tr></tbody></table></alternatives></table-wrap><p>It can be seen from <xref rid="pone.0323631.t003" ref-type="table">Table 3</xref> that the method in this paper performs well in two key indicators: path length and travel time. Compared with GET method and FGM method, the path length is shortened by 2.8% and 5.5% respectively, and the travel time is significantly reduced by 12.5% and 14.8%, which proves that the method in this paper has obvious advantages in path planning and optimization, and can effectively shorten the travel distance and improve operating efficiency. In terms of the number of successful obstacle avoidance, the method in this paper has reached 5, significantly higher than the GET method and the FGM method, which proves that the method in this paper is more flexible and effective in dealing with diverse obstacles in complex mountain environment, and has stronger obstacle avoidance ability. In terms of smoothness score, the method in this paper gets a full score of 10 points, while GET method and FGM method get 7 points and 8 points respectively, which fully proves that the method in this paper has significant effects on improving the driving smoothness of tractors, which helps to reduce mechanical wear, improve operation quality and enhance driver comfort. At the same time, the method in this paper did not turn back during the experiment. In contrast, the GET method and the FGM method turned back twice and three times respectively, indicating that the method in this paper is more accurate and efficient in the path planning and execution process, can effectively avoid unnecessary turns and detours, and further improve the work efficiency.</p><p>To further validate the applicability of the design method and test the power consumption and delay tolerance in different distance environments, a comparative test analysis was conducted between the reinforcement learning based vehicle control algorithm in reference 9 and the Bezier curve based vehicle control algorithm in reference 10. The results are shown below.</p><p>According to the data in <xref rid="pone.0323631.t004" ref-type="table">Table 4</xref>, the power consumption of the Proposed method is significantly lower than the other two algorithms at a distance of 500 meters. The power consumption of the Proposed method is 0.2 kWh, while the algorithms based on reinforcement learning and Bezier curve are 0.86 kWh and 0.77 kWh, respectively. This indicates that the proposed method performs the best in terms of power consumption and has higher energy efficiency. And the delay tolerance of the Proposed method * * is significantly lower than the other two algorithms. At a distance of 500 meters, the delay tolerance of the Proposed method is 0.12 seconds, while the reinforcement learning based algorithm and the Bezier curve based algorithm are 1.5 seconds and 1.4 seconds, respectively. This indicates that the proposed method performs the best in terms of delay tolerance, with higher real-time performance and response speed.</p><table-wrap position="float" id="pone.0323631.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.t004</object-id><label>Table 4</label><caption><title>Results of comparing power consumption and minimum obstacle avoidance distance at different distances.</title></caption><alternatives><graphic xlink:href="pone.0323631.t004" id="pone.0323631.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="2" colspan="1">distance/m</th><th align="left" colspan="2" rowspan="1">A vehicle control algorithm based on reinforcement learning</th><th align="left" colspan="2" rowspan="1">Vehicle control algorithm based on Bezer curve</th><th align="left" colspan="2" rowspan="1">Proposed method</th></tr><tr><th align="left" rowspan="1" colspan="1">Power consumption/kWh</th><th align="left" rowspan="1" colspan="1">Delay tolerance/s</th><th align="left" rowspan="1" colspan="1">Power consumption/kWh</th><th align="left" rowspan="1" colspan="1">Delay tolerance/s</th><th align="left" rowspan="1" colspan="1">Power consumption/kWh</th><th align="left" rowspan="1" colspan="1">Delay tolerance/s</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">500</td><td align="left" rowspan="1" colspan="1">0.86</td><td align="left" rowspan="1" colspan="1">1.5</td><td align="left" rowspan="1" colspan="1">0.77</td><td align="left" rowspan="1" colspan="1">1.4</td><td align="left" rowspan="1" colspan="1">0.2</td><td align="left" rowspan="1" colspan="1">0.12</td></tr><tr><td align="left" rowspan="1" colspan="1">1000</td><td align="left" rowspan="1" colspan="1">0.99</td><td align="left" rowspan="1" colspan="1">1.9</td><td align="left" rowspan="1" colspan="1">0.86</td><td align="left" rowspan="1" colspan="1">1.9</td><td align="left" rowspan="1" colspan="1">0.39</td><td align="left" rowspan="1" colspan="1">0.24</td></tr><tr><td align="left" rowspan="1" colspan="1">1500</td><td align="left" rowspan="1" colspan="1">1.20</td><td align="left" rowspan="1" colspan="1">2.1</td><td align="left" rowspan="1" colspan="1">0.92</td><td align="left" rowspan="1" colspan="1">2.1</td><td align="left" rowspan="1" colspan="1">0.61</td><td align="left" rowspan="1" colspan="1">0.33</td></tr><tr><td align="left" rowspan="1" colspan="1">2000</td><td align="left" rowspan="1" colspan="1">1.35</td><td align="left" rowspan="1" colspan="1">2.5</td><td align="left" rowspan="1" colspan="1">1.32</td><td align="left" rowspan="1" colspan="1">2.3</td><td align="left" rowspan="1" colspan="1">0.77</td><td align="left" rowspan="1" colspan="1">0.41</td></tr><tr><td align="left" rowspan="1" colspan="1">2500</td><td align="left" rowspan="1" colspan="1">1.41</td><td align="left" rowspan="1" colspan="1">5.9</td><td align="left" rowspan="1" colspan="1">1.50</td><td align="left" rowspan="1" colspan="1">2.5</td><td align="left" rowspan="1" colspan="1">0.95</td><td align="left" rowspan="1" colspan="1">0.52</td></tr></tbody></table></alternatives></table-wrap><p>To further verify the safety performance of the design method, a minimum obstacle avoidance distance test analysis was conducted, and the results are shown in the following figure&#x000a0;(<xref rid="pone.0323631.g008" ref-type="fig">Fig 8</xref>).</p><fig position="float" id="pone.0323631.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.g008</object-id><label>Fig 8</label><caption><title>Minimum obstacle avoidance distance test results.</title></caption><graphic xlink:href="pone.0323631.g008" position="float"/></fig><p>Based on the above results, it can be seen that the minimum obstacle avoidance distance of the algorithm in this paper is always around 8m and relatively stable, while the minimum obstacle avoidance distance of the vehicle control algorithm based on reinforcement learning fluctuates greatly between 1-5m and is unstable; The minimum obstacle avoidance distance of the vehicle control algorithm based on the Beizer curve hovers between 0.6 and 4 meters, which is too small and poses a certain degree of danger, and the fluctuation is greater than that of the vehicle control algorithm based on reinforcement learning. This indicates that the security performance of the algorithm in this article is relatively good.</p><p>To verify whether the method proposed in this article is applicable to embedded devices, a memory usage analysis was conducted, and the results are shown in the following <xref rid="pone.0323631.t005" ref-type="table">Table 5</xref>:</p><table-wrap position="float" id="pone.0323631.t005"><object-id pub-id-type="doi">10.1371/journal.pone.0323631.t005</object-id><label>Table 5</label><caption><title>Comparison results of memory occupancy at different times.</title></caption><alternatives><graphic xlink:href="pone.0323631.t005" id="pone.0323631.t005g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Time/ day</th><th align="left" rowspan="1" colspan="1">Vehicle control algorithm based on reinforcement learning/%</th><th align="left" rowspan="1" colspan="1">Vehicle control algorithm based on Bezer curve/%</th><th align="left" rowspan="1" colspan="1">Proposed method/%</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">11</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">8</td></tr><tr><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">9</td></tr><tr><td align="left" rowspan="1" colspan="1">15</td><td align="left" rowspan="1" colspan="1">18</td><td align="left" rowspan="1" colspan="1">16</td><td align="left" rowspan="1" colspan="1">10</td></tr><tr><td align="left" rowspan="1" colspan="1">20</td><td align="left" rowspan="1" colspan="1">19</td><td align="left" rowspan="1" colspan="1">17</td><td align="left" rowspan="1" colspan="1">10</td></tr><tr><td align="left" rowspan="1" colspan="1">25</td><td align="left" rowspan="1" colspan="1">21</td><td align="left" rowspan="1" colspan="1">19</td><td align="left" rowspan="1" colspan="1">12</td></tr><tr><td align="left" rowspan="1" colspan="1">30</td><td align="left" rowspan="1" colspan="1">22</td><td align="left" rowspan="1" colspan="1">20</td><td align="left" rowspan="1" colspan="1">14</td></tr></tbody></table></alternatives></table-wrap><p>The memory usage of the proposed method is significantly lower than the other two algorithms. At 5 days, the proposed method had a memory usage rate of 8%, while the reinforcement learning based algorithm and the Bezier curve based algorithm had memory usage rates of 11% and 10%, respectively. As time goes on, the memory usage of the proposed method grows relatively steadily. This indicates that the proposed method performs the best in terms of memory usage and has higher memory efficiency. Can effectively embed devices.</p></sec></sec><sec sec-type="conclusions" id="sec012"><title>Conclusion</title><p>This paper studies the autonomous obstacle avoidance method of mountain tractors based on semantic neural network and laser SLAM technology, and comprehensively verifies the excellent performance of this method through experiments. The experimental conclusions are as follows. This method can sensitively capture and distinguish various obstacles in the complex mountain environment, and also realize the fine construction and real-time update of the environment map. It can intelligently plan a safe and efficient driving path, effectively shorten the driving distance and reduce the time consumption. The driving smoothness of the tractor is significantly enhanced, the mechanical wear is reduced through precise control, the operation quality is improved, and the driver&#x02019;s comfort and satisfaction are greatly improved. In the process of experiment, the efficient path execution and precise control capabilities demonstrated by this method further confirmed its reliability and practicability in the complex mountain environment.</p></sec><sec id="sec013" sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="pone.0323631.s001" position="float" content-type="local-data"><label>S1 File</label><caption><title>Data for parameters.</title><p>(XLSX)</p></caption><media xlink:href="pone.0323631.s001.xlsx"/></supplementary-material></sec></body><back><ack><p>Thank you for the experimental assistance provided by the School of Vehicle and Transportation Engineering, Henan University of Science and Technology.</p></ack><ref-list><title>References</title><ref id="pone.0323631.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>Z</given-names></name>, <name><surname>Zhao</surname><given-names>X</given-names></name>, <name><surname>Wei</surname><given-names>Q</given-names></name>, <name><surname>Su</surname><given-names>C</given-names></name>, <name><surname>Meng</surname><given-names>J</given-names></name>. <article-title>Trajectory planning for multi-robot coordinated towing system based on stability</article-title>. <source>High Technol Lett</source>. <year>2024</year>;<volume>30</volume>(<issue>1</issue>):<fpage>43</fpage>&#x02013;<lpage>51</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3772/j.issn.1006-6748.2024.01.005</pub-id></mixed-citation></ref><ref id="pone.0323631.ref002"><label>2</label><mixed-citation publication-type="other">Lambert S. A different type of tractor. Tractor &#x00026; Machinery: For the tractor enthusiast, collector &#x00026; restorer, 2023; 29(7): 48&#x02013;51.</mixed-citation></ref><ref id="pone.0323631.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>J</given-names></name>, <name><surname>Shi</surname><given-names>X</given-names></name>, <name><surname>Liang</surname><given-names>P</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Lv</surname><given-names>Y</given-names></name>, <name><surname>Zhong</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Research on Gait Trajectory Planning of Wall-Climbing Robot Based on Improved PSO Algorithm</article-title>. <source>J Bionic Eng</source>. <year>2024</year>;<volume>21</volume>(<issue>4</issue>):<fpage>1747</fpage>&#x02013;<lpage>60</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s42235-024-00538-y</pub-id></mixed-citation></ref><ref id="pone.0323631.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>J</given-names></name>, <name><surname>Yan</surname><given-names>Y</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>. <article-title>Research on Anthropomorphic Obstacle Avoidance Trajectory Planning for Adaptive Driving Scenarios Based on Inverse Reinforcement Learning Theory</article-title>. <source>Engineering</source>. <year>2024</year>;<volume>33</volume>:<fpage>133</fpage>&#x02013;<lpage>45</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.eng.2023.07.018</pub-id></mixed-citation></ref><ref id="pone.0323631.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>&#x000d6;zdemir</surname><given-names>A</given-names></name>, <name><surname>Bogosyan</surname><given-names>SO</given-names></name>. <article-title>Gap Based Elastic Trees as a Novel Approach for Fast and Reliable Obstacle Avoidance for UGVs</article-title>. <source>J Intell Robot Syst</source>. <year>2023</year>;<volume>107</volume>(<issue>1</issue>):9&#x02013;23. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10846-022-01792-0</pub-id></mixed-citation></ref><ref id="pone.0323631.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Adamiec&#x02013;W&#x000f3;jcik</surname><given-names>I</given-names></name>, <name><surname>Brzozowska</surname><given-names>L</given-names></name>, <name><surname>Dr&#x00105;g</surname><given-names>&#x00141;</given-names></name>, <name><surname>Wojciech</surname><given-names>S</given-names></name>. <article-title>Optimisation of riser reentry process and obstacle avoidance</article-title>. <source>Ocean Engineering</source>. <year>2023</year>;<volume>268</volume>:<fpage>113561</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.oceaneng.2022.113561</pub-id></mixed-citation></ref><ref id="pone.0323631.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Sezer</surname><given-names>V</given-names></name>. <article-title>An Optimized Path Tracking Approach Considering Obstacle Avoidance and Comfort</article-title>. <source>J Intell Robot Syst</source>. <year>2022</year>;<volume>105</volume>(<issue>1</issue>):<fpage>2121</fpage>&#x02013;<lpage>3434</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10846-022-01636-x</pub-id></mixed-citation></ref><ref id="pone.0323631.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Conte</surname><given-names>D</given-names></name>, <name><surname>Furukawa</surname><given-names>T</given-names></name>. <article-title>Autonomous Bayesian escorting of a human integrating intention and obstacle avoidance</article-title>. <source>Journal of Field Robotics</source>. <year>2022</year>;<volume>39</volume>(<issue>6</issue>):<fpage>679</fpage>&#x02013;<lpage>93</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/rob.22070</pub-id></mixed-citation></ref><ref id="pone.0323631.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Irshayyid</surname><given-names>A</given-names></name>, <name><surname>Chen</surname><given-names>J</given-names></name>, <name><surname>Xiong</surname><given-names>G</given-names></name>. <article-title>A review on reinforcement learning-based highway autonomous vehicle control</article-title>. <source>Green Energy and Intelligent Transportation</source>. <year>2024</year>;<volume>3</volume>(<issue>4</issue>):<fpage>100156</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.geits.2024.100156</pub-id></mixed-citation></ref><ref id="pone.0323631.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>H</given-names></name>, <name><surname>Pang</surname><given-names>H</given-names></name>, <name><surname>Xia</surname><given-names>H</given-names></name>, <name><surname>Huang</surname><given-names>Y</given-names></name>, <name><surname>Zeng</surname><given-names>X</given-names></name>. <article-title>Research on Trajectory Planning Method Based on B&#x000e9;zier Curves for Dynamic Scenarios</article-title>. <source>Electronics</source>. <year>2025</year>;<volume>14</volume>(<issue>3</issue>):<fpage>494</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/electronics14030494</pub-id></mixed-citation></ref><ref id="pone.0323631.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Ma</surname><given-names>Y</given-names></name>, <name><surname>Shao</surname><given-names>P</given-names></name>, <name><surname>Liu</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>. <article-title>Collaborative Trajectory Planning for Stereoscopic Agricultural Multi-UAVs Driven by the Aquila Optimizer</article-title>. <source>CMC</source>. <year>2025</year>;<volume>82</volume>(<issue>1</issue>):<fpage>1349</fpage>&#x02013;<lpage>76</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.32604/cmc.2024.058294</pub-id></mixed-citation></ref><ref id="pone.0323631.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Sun</surname><given-names>J</given-names></name>, <name><surname>Xu</surname><given-names>G</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Long</surname><given-names>T</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <article-title>Safe flight corridor constrained sequential convex programming for efficient trajectory generation of fixed-wing UAVs</article-title>. <source>Chinese Journal of Aeronautics</source>. <year>2025</year>;<volume>38</volume>(<issue>1</issue>):<fpage>103174</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cja.2024.08.005</pub-id></mixed-citation></ref><ref id="pone.0323631.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Ren</surname><given-names>B</given-names></name>, <name><surname>Du</surname><given-names>S</given-names></name>, <name><surname>Cui</surname><given-names>Z</given-names></name>, <name><surname>Xu</surname><given-names>Y</given-names></name>, <name><surname>Zhao</surname><given-names>Q</given-names></name>. <article-title>High-precision trajectory tracking control of helicopter based on ant colony optimization-slime mould algorithm</article-title>. <source>Chinese Journal of Aeronautics</source>. <year>2025</year>;<volume>38</volume>(<issue>1</issue>):<fpage>103172</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cja.2024.08.003</pub-id></mixed-citation></ref><ref id="pone.0323631.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Intraraprasit</surname><given-names>M</given-names></name>, <name><surname>Chitsobhuk</surname><given-names>O</given-names></name>. <article-title>Filter pruning based on local gradient activation mapping in convolutional neural networks</article-title>. <source>Int J Innov Comput Inf Control</source>. <year>2023</year>;<volume>19</volume>(<issue>6</issue>):<fpage>1697</fpage>&#x02013;<lpage>715</lpage>.</mixed-citation></ref><ref id="pone.0323631.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Zhai</surname><given-names>Y</given-names></name>, <name><surname>Cui</surname><given-names>J</given-names></name>, <name><surname>Meng</surname><given-names>F</given-names></name>, <name><surname>Xie</surname><given-names>H</given-names></name>, <name><surname>Hou</surname><given-names>C</given-names></name>, <name><surname>Li</surname><given-names>B</given-names></name>. <article-title>Ship Path Planning Based on Sparse A* Algorithm</article-title>. <source>J Marine Sci Appl</source>. <year>2024</year>;<volume>24</volume>(<issue>1</issue>):<fpage>238</fpage>&#x02013;<lpage>48</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11804-024-00430-5</pub-id></mixed-citation></ref><ref id="pone.0323631.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>C</given-names></name>, <name><surname>Yao</surname><given-names>L</given-names></name>, <name><surname>Mi</surname><given-names>C</given-names></name>. <article-title>Fusion Algorithm Based on Improved A* and DWA for USV Path Planning</article-title>. <source>J Marine Sci Appl</source>. <year>2024</year>;<volume>24</volume>(<issue>1</issue>):<fpage>224</fpage>&#x02013;<lpage>37</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11804-024-00434-1</pub-id></mixed-citation></ref><ref id="pone.0323631.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>W</given-names></name>, <name><surname>Zhang</surname><given-names>T</given-names></name>. <article-title>Path Planning for Thermal Power Plant Fan Inspection Robot Based on Improved A* Algorithm</article-title>. <source>JERA</source>. <year>2025</year>;<volume>9</volume>(<issue>1</issue>):<fpage>233</fpage>&#x02013;<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.26689/jera.v9i1.9460</pub-id></mixed-citation></ref><ref id="pone.0323631.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Vidhya</surname><given-names>K</given-names></name>, <name><surname>Saraswathi</surname><given-names>A</given-names></name>. <article-title>An improved a search algorithm for the shortest path under interval-valued Pythagorean fuzzy environment</article-title>. <source>Granul Comput</source>. <year>2022</year>;<volume>8</volume>(<issue>2</issue>):<fpage>241</fpage>&#x02013;<lpage>51</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s41066-022-00326-1</pub-id></mixed-citation></ref><ref id="pone.0323631.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Fu</surname><given-names>T</given-names></name>, <name><surname>Hu</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>T</given-names></name>, <name><surname>Bi</surname><given-names>Q</given-names></name>, <name><surname>Song</surname><given-names>X</given-names></name>. <article-title>Physics-Informed Neural Networks-Based Online Excavation Trajectory Planning for Unmanned Excavator</article-title>. <source>Chin J Mech Eng</source>. <year>2024</year>;<volume>37</volume>(<issue>1</issue>):332&#x02013;348. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s10033-024-01109-2</pub-id></mixed-citation></ref><ref id="pone.0323631.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Hatori</surname><given-names>Y</given-names></name>, <name><surname>Uchimura</surname><given-names>Y</given-names></name>. <article-title>Obstacle Avoidance during Teleoperation by Model Predictive Control with Time-varying Delay</article-title>. <source>IEEJ Journal IA</source>. <year>2023</year>;<volume>12</volume>(<issue>2</issue>):<fpage>117</fpage>&#x02013;<lpage>24</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1541/ieejjia.22004524</pub-id></mixed-citation></ref></ref-list></back><sub-article article-type="author-comment" id="pone.0323631.r001" specific-use="rebutted-decision-letter-unavailable"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323631.r001</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">13 Feb 2025</named-content>
</p></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0323631.r002" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323631.r002</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Lei</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Lei Zhang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Lei Zhang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323631" id="rel-obj002" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">27 Feb 2025</named-content>
</p><p>PONE-D-25-03307Research on Autonomous Obstacle Avoidance of Mountainous Tractors Based on Semantic Neural Network and Laser SLAMPLOS ONE</p><p>Dear Dr. Xu,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.&#x000a0;This paper proposes a research on autonomous obstacle avoidance of mountainous tractors based on semantic neural network and laser SLAM. But revisions are needed in terms of comparison studies, writing, table and figure formulation and literature review comprehensiveness.&#x000a0;</p><p>Please submit your revised manuscript by Apr 13 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email> . When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:</p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link> . Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link> .</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Lei Zhang, PhD</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal Requirements:</p><p>When submitting your revision, we need you to address these additional requirements.</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at <ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and <ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link></p><p>2. In your Methods section, please provide additional information regarding the permits you obtained for the work. Please ensure you have included the full name of the authority that approved the field site access and, if no permits were required, a brief statement explaining why.</p><p>3. Thank you for stating the following financial disclosure:</p><p>Key Research and Development Project of Henan Province, 231111112600; Natural Science Foundation of Henan Province, 242300420370; Training Program for Young Backbone Teachers in Undergraduate Universities in Henan Province, 2024GGJS051; Heluo Youth Talent Support Project, 2024 HLTJ03; Henan Province University Science and Technology Innovation Team Support Program Project, 24IRTSTHN029; Key Research Project Plan for Higher Education Institutions in Henan Province, 25B460004</p><p>Please state what role the funders took in the study. If the funders had no role, please state: "The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."</p><p>If this statement is not correct you must amend it as needed.</p><p>Please include this amended Role of Funder statement in your cover letter; we will change the online submission form on your behalf.</p><p>4. We note that your Data Availability Statement is currently as follows: All relevant data are within the manuscript and its Supporting Information files.</p><p>Please confirm at this time whether or not your submission contains all raw data required to replicate the results of your study. Authors must share the &#x0201c;minimal data set&#x0201d; for their submission. PLOS defines the minimal data set to consist of the data required to replicate all study findings reported in the article, as well as related metadata and methods (<ext-link xlink:href="https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition)." ext-link-type="uri">https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition</ext-link>).</p><p>For example, authors should submit the following data:</p><p>- The values behind the means, standard deviations and other measures reported;</p><p>- The values used to build graphs;</p><p>- The points extracted from images for analysis.</p><p>Authors do not need to submit their entire data set if only a portion of the data was used in the reported study.</p><p>If your submission does not contain these data, please either upload them as Supporting Information files or deposit them to a stable, public repository and provide us with the relevant URLs, DOIs, or accession numbers. For a list of recommended repositories, please see <ext-link xlink:href="https://journals.plos.org/plosone/s/recommended-repositories." ext-link-type="uri">https://journals.plos.org/plosone/s/recommended-repositories</ext-link>.</p><p>If there are ethical or legal restrictions on sharing a de-identified data set, please explain them in detail (e.g., data contain potentially sensitive information, data are owned by a third-party organization, etc.) and who has imposed them (e.g., an ethics committee). Please also provide contact information for a data access committee, ethics committee, or other institutional body to which data requests may be sent. If data are owned by a third party, please indicate how others may request data access.</p><p>5. We note that Figure 4 includes an image of a [patient / participant / in the study].</p><p>As per the PLOS ONE policy (<ext-link xlink:href="http://journals.plos.org/plosone/s/submission-guidelines#loc-human-subjects-research)" ext-link-type="uri">http://journals.plos.org/plosone/s/submission-guidelines#loc-human-subjects-research</ext-link>) on papers that include identifying, or potentially identifying, information, the individual(s) or parent(s)/guardian(s) must be informed of the terms of the PLOS open-access (CC-BY) license and provide specific permission for publication of these details under the terms of this license. Please download the Consent Form for Publication in a PLOS Journal (<ext-link xlink:href="http://journals.plos.org/plosone/s/file?id=8ce6/plos-consent-form-english.pdf)." ext-link-type="uri">http://journals.plos.org/plosone/s/file?id=8ce6/plos-consent-form-english.pdf</ext-link>). The signed consent form should not be submitted with the manuscript, but should be securely filed in the individual's case notes. Please amend the methods section and ethics statement of the manuscript to explicitly state that the patient/participant has provided consent for publication: &#x0201c;The individual in this manuscript has given written informed consent (as outlined in PLOS consent form) to publish these case details&#x0201d;.</p><p>Please respond by return e-mail with an amended manuscript. We can upload this to your submission on your behalf.</p><p>If you are unable to obtain consent from the subject of the photograph, please either instruct us to remove the figure or supply a replacement figure by return e-mail for which you hold the relevant copyright permissions and subject consents. In some cases, you may need to specify in the text that the image used in the figure is not the original image used in the study, but a similar image used for illustrative purposes only. We can make any changes on your behalf.</p><p>Additional Editor Comments :</p><p>This paper proposes a research on autonomous obstacle avoidance of mountainous tractors based on semantic neural network and laser SLAM. But revisions are needed in terms of comparison studies, writing, table and figure formulation and literature review comprehensiveness. Pay attention to that Reviewer 1's comments are presented in the attached file.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p>2. Has the statistical analysis been performed appropriately and rigorously? </p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p>5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p><p>Reviewer #1:&#x000a0;Restructure the methodology section to clarify technical details; Expand the experimental section (dynamic scenarios, multi-sensor comparisons ); Add feasibility analysis for practical deployment.</p><p>Reviewer #2:&#x000a0;1. The experimental environment was mentioned in the paper, but the details were insufficient. Suggest providing a detailed description of the terrain features of the testing area, such as slope, soil type, vegetation density, etc., as well as the types and arrangement of obstacles. This will help other researchers replicate the experiment and validate the results.</p><p>2. Please carefully check and confirm whether the figures, tables, etc. in the article conform to the format</p><p>3. In addition to comparing with GET and FGM methods, it is recommended to add comparisons with other advanced methods (such as deep learning based obstacle avoidance methods) to more comprehensively evaluate the performance of the proposed method.</p><p>4. Some variables that appear for the first time in the formula have not been explained in the main text, such as formula 5. It is recommended to add explanations.</p><p>5. Increase the review of existing research, especially the latest research progress related to semantic neural networks and laser SLAM technology. This will help readers better understand the innovation and background of the proposed method. Recommended quotes include "Enhancing Vehicle Re identification by Pair flexible Pose Guided Vehicle Image Synthesis" and "Deep Transfer Learning for Intelligent Vehicle Perception: A Survey"</p><p>6. Please ensure that the charts in the paper are clear and easy to read, and that the captions are detailed and accurate. For complex charts, it is recommended to add necessary explanations and clarifications</p><p>7. It is recommended to add a comparative analysis with the proposed method when describing existing research methods, clearly pointing out the shortcomings of the existing methods and the areas for improvement of the proposed method. This will help highlight the research contributions and innovative points of the paper.</p><p>**********</p><p>6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link> ). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link> .</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link> . PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email> . Please note that Supporting Information files do not need this step.</p><supplementary-material id="pone.0323631.s002" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Research on Autonomou.docx</named-content></p></caption><media xlink:href="pone.0323631.s002.docx"/></supplementary-material></body></sub-article><sub-article article-type="author-comment" id="pone.0323631.r003"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323631.r003</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323631" id="rel-obj003" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">7 Apr 2025</named-content>
</p><p>1. The introduction does not clearly articulate the unique challenges of autonomous obstacle avoidance for mountain tractors. A detailed scenario analysis is needed to highlight the differences from traditional farmland environments.</p><p>Response: Provide detailed scenarios following your recommendations to clarify the differences from traditional farmland environments, such as:</p><p>In order to verify the effectiveness of the autonomous obstacle avoidance method proposed in this article for mountain tractors, a representative test area was selected in hilly and mountainous areas, which includes complex terrain such as slopes of different slopes, ravines of varying depths, and dense tree clusters. The slope changes frequently in hilly and mountainous areas, which poses higher requirements for the tractor's power system and suspension system. Compared with the flat terrain of traditional farmland, mountain tractors need to maintain stability and maneuverability on slopes of different slopes, and the autonomous obstacle avoidance system needs to accurately determine the impact of slope on the driving path. Gullies of varying depths and scattered stones and rocks are common obstacles in mountainous environments. These obstacles not only have irregular shapes, but their positions are also difficult to predict, increasing the complexity and difficulty of obstacle avoidance systems. In contrast, obstacles in traditional farmland are usually more regular and easy to identify. Comprehensively test the tractor's autonomous obstacle avoidance ability in different environments in this complex environment. In the testing area, simulate obstacles such as stones, wooden stakes, and simulated trees according to the experimental design. Selecting the John Deere 6125R hilly mountain tractor as the experimental object,</p><p>2. The mathematical derivation of plane feature matching in Equations (8)-(10) is overly simplified, with no explanation of how noise in point cloud registration is addressed. It is recommended to refer to the GEITS paper "A non-uniform quadtree map building method including dead-end semantics extraction" (DOI:10.1016/j.geits.2023.100071) for map construction methods.</p><p>Response: Following your suggestion, we have added a feature matching process for the non-uniform quadtree map construction method, such as:</p><p>Firstly, remove outliers in the point cloud that are too far from the mean. , Among them, is the -th point in the point cloud, and is the value of the point cloud. If (threshold), it is removed and changed to, and the data is denoised according to the above point cloud filtering.</p><p>After pose estimation, the point cloud data is optimized through non-uniform quadtree to reduce redundant points and improve the resolution and accuracy of the map. Divide the map into multiple quadtree nodes, with each node representing an area. Dynamically adjust the size of nodes based on point cloud density and terrain complexity. , where is the edge length of the node, is the maximum edge length of the node, and is the number of layers the node is divided into. After each LiDAR scan, insert the newly acquired point cloud data into the quadtree map. Dynamically adjust the resolution of quadtree nodes based on point cloud distribution and terrain features.</p><p>&#x0fffd;<!--&#x0fffd;-->16&#x0fffd;<!--&#x0fffd;--><!--&#x0fffd;--><!--&#x0ff09;--></p><p>Among them, is the number of points within the th layer node, and is the point density threshold. Based on the semantic information of the obstacles mentioned above, associate the semantic labels of the target obstacles obtained in section 2.2 with the quadtree nodes.</p><p>&#x0fffd;<!--&#x0fffd;-->17&#x0fffd;<!--&#x0fffd;--><!--&#x0fffd;--><!--&#x0ff09;--></p><p>Among them, is the semantic label of node n, and is the semantic label of point . And using the constraint relationship between points and surfaces mentioned above, estimate the pose changes of the tractor.</p><p>&#x0fffd;<!--&#x0fffd;-->18&#x0fffd;<!--&#x0fffd;--><!--&#x0fffd;--><!--&#x0ff09;--></p><p>Combine the point cloud data of each frame with the sub map data generated in the previous frame to generate a single sub map. Namely:</p><p>&#x0fffd;<!--&#x0fffd;-->19&#x0fffd;<!--&#x0fffd;--><!--&#x0fffd;--><!--&#x0ff09;--></p><p>Among them, is the sub map of frame , and is the point cloud data of frame . All sub maps are iteratively updated to generate a global point cloud map, . Through the above steps and mathematical expressions, combined with the optimization processing steps of non-uniform quadtree maps, high-precision global point cloud maps can be generated.</p><p>3. Only GET and FGM algorithms are compared, without including mainstream methods like RRT* or deep reinforcement learning. It is recommended to reference the baseline methods in the GEITS paper "A review on reinforcement learning-based highway autonomous vehicle control" (DOI:10.1016/j.geits.2024.100156).</p><p>Response: We have supplemented the above references as per your suggestion and conducted comparative analysis with them, such as:</p><p>Irshayyid A et al. first reviewed the different traffic scenarios discussed in the literature [9], and then conducted a comprehensive review of DRL technology, such as the state representation method for capturing the interactive dynamics required for safe and efficient merging, and the reward formula for managing key indicators such as safety, efficiency, comfort, and adaptability. The insights from this review can guide future research towards realizing the potential of DRL in complex traffic automation under uncertainty. A dynamic adaptive trajectory planning method based on Bezier curve is proposed.</p><p>To further validate the applicability of the design method and test the power consumption and delay tolerance in different distance environments, a comparative test analysis was conducted between the reinforcement learning based vehicle control algorithm in reference [9] and the Bezier curve based vehicle control algorithm in reference [10]. The results are shown below.</p><p>Table 4 Results of comparing power consumption and minimum obstacle avoidance distance at different distances</p><p>distance /m A vehicle control algorithm based on reinforcement learning Vehicle control algorithm based on Bezer curve Proposed method</p><p>Power consumption/kWh Delay tolerance/s Power consumption/kWh Delay tolerance/s Power consumption/kWh Delay tolerance/s</p><p>500 0.86 1.5 0.77 1.4 0.2 0.12</p><p>1000 0.99 1.9 0.86 1.9 0.39 0.24</p><p>1500 1.20 2.1 0.92 2.1 0.61 0.33</p><p>2000 1.35 2.5 1.32 2.3 0.77 0.41</p><p>2500 1.41 5.9 1.50 2.5 0.95 0.52</p><p>According to the data in Table 4, the power consumption of the Proposed method is significantly lower than the other two algorithms at a distance of 500 meters. The power consumption of the Proposed method is 0.2 kWh, while the algorithms based on reinforcement learning and Bezier curve are 0.86 kWh and 0.77 kWh, respectively. This indicates that the proposed method performs the best in terms of power consumption and has higher energy efficiency. And the delay tolerance of the Proposed method * * is significantly lower than the other two algorithms. At a distance of 500 meters, the delay tolerance of the Proposed method is 0.12 seconds, while the reinforcement learning based algorithm and the Bezier curve based algorithm are 1.5 seconds and 1.4 seconds, respectively. This indicates that the proposed method performs the best in terms of delay tolerance, with higher real-time performance and response speed.</p><p>4. The coordination mechanism between A* and DWA is vaguely described, with no explanation of how global paths are dynamically updated to adapt to environmental changes. It is recommended to refer to the spatiotemporal constraint optimization methods in the GEITS paper "Spatiotemporal-restricted A&#x02217; algorithm as a support for lane-free traffic" (DOI:10.1016/j.geits.2024.100159).</p><p>Response: We have referred to relevant papers as per your suggestion, supplemented with spatiotemporal constraints and other conditions to adapt to environmental changes, such as:</p><p>(3) Space time constraint setting</p><p>In the cost function of the A * algorithm, add the terrain complexity factor :</p><p>&#x0fffd;<!--&#x0fffd;-->22&#x0fffd;<!--&#x0fffd;--><!--&#x0fffd;--><!--&#x0ff09;--></p><p>Among them, is the slope at node , is the roughness at node , and and are weight coefficients.</p><p>Add safety distance constraints to obstacles in the trajectory scoring of DWA algorithm:</p><p>&#x0fffd;<!--&#x0fffd;-->23&#x0fffd;<!--&#x0fffd;--><!--&#x0fffd;--><!--&#x0ff09;--></p><p>Among them, (xt, yt) are points on the trajectory, and (xo, yo) are the positions of obstacles.</p><p>(4) Trajectory rating:</p><p>Based on the above constraint settings,trajectory scoring. The trajectories of each simulated tractor were scored to evaluate the merits. The scoring function considering deviations from the target heading , deviation from the target position and velocity deviations , the formula is expressed as follows:</p><p>(24)</p><p>5. Key metrics such as inference speed (FPS) and memory usage of the lightweight network are missing, making it impossible to prove its suitability for embedded devices.</p><p>Response: Complthe comparative analysis following your recommendations, such as:</p><p>To verify whether the method proposed in this article is applicable to embedded devices, a memory usage analysis was conducted, and the results are shown in the following table:</p><p>Table 4 Comparison results of memory occupancy at different times</p><p>Time / day Vehicle control algorithm based on reinforcement learning /% Vehicle control algorithm based on Bezer curve/% Proposed method/%</p><p>5 11 10 8</p><p>10 15 15 9</p><p>15 18 16 10</p><p>20 19 17 10</p><p>25 21 19 12</p><p>30 22 20 14</p><p>The memory usage of the proposed method is significantly lower than the other two algorithms. At 5 days, the proposed method had a memory usage rate of 8%, while the reinforcement learning based algorithm and the Bezier curve based algorithm had memory usage rates of 11% and 10%, respectively. As time goes on, the memory usage of the proposed method grows relatively steadily. This indicates that the proposed method performs the best in terms of memory usage and has higher memory efficiency. Can effectively embed devices.</p><p>6. The data availability statement claims "data is within the manuscript," but no point cloud dataset or code links are provided, violating reproducibility principles. Data should be uploaded to a public platform.</p><p>Response: Regarding the issue you mentioned that the point cloud dataset and code links were not provided, we would like to clarify that some of the data involved in this study do contain proprietary information, which cannot be fully uploaded to public platforms. The publicly available data has been provided with relevant information in the article to ensure the transparency and verifiability of the research.</p><p>7. The system's power consumption and its impact on tractor endurance, especially in mountainous areas without charging facilities, are not discussed. This issue is critical and should be addressed.</p><p>Response: The power consumption test analysis of this system has been supplemented by following your recommendations, such as:</p><p>To further validate the applicability of the design method and test the power consumption and delay tolerance in different distance environments, a comparative test analysis was conducted between the reinforcement learning based vehicle control algorithm in reference [9] and the Bezier curve based vehicle control algorithm in reference [10]. The results are shown below.</p><p>Table 4 Results of comparing power consumption and minimum obstacle avoidance distance at different distances</p><p>distance /m A vehicle control algorithm based on reinforcement learning Vehicle control algorithm based on Bezer curve Proposed method</p><p>Power consumption/kWh Delay tolerance/s Power consumption/kWh Delay tolerance/s Power consumption/kWh Delay tolerance/s</p><p>500 0.86 1.5 0.77 1.4 0.2 0.12</p><p>1000 0.99 1.9 0.86 1.9 0.39 0.24</p><p>1500 1.20 2.1 0.92 2.1 0.61 0.33</p><p>2000 1.35 2.5 1.32 2.3 0.77 0.41</p><p>2500 1.41 5.9 1.50 2.5 0.95 0.52</p><p>According to the data in Table 4, the power consumption of the Proposed method is significantly lower than the other two algorithms at a distance of 500 meters. The power consumption of the Proposed method is 0.2 kWh, while the algorithms based on reinforcement learning and Bezier curve are 0.86 kWh and 0.77 kWh, respectively. This indicates that the proposed method performs the best in terms of power consumption and has higher energy efficiency. And the delay tolerance of the Proposed method * * is significantly lower than the other two algorithms. At a distance of 500 meters, the delay tolerance of the Proposed method is 0.12 seconds, while the reinforcement learning based algorithm and the Bezier curve based algorithm are 1.5 seconds and 1.4 seconds, respectively. This indicates that the proposed method performs the best in terms of delay tolerance, with higher real-time performance and response speed.</p><p>8. Safety is only measured by "comfort score," without introducing quantitative metrics from agricultural machinery safety standards (e.g., ISO 25119), such as minimum obstacle avoidance distance or emergency braking response time.</p><p>Response: The minimum obstacle avoidance distance test analysis has been supplemented by following your recommendations, such as:</p><p>To further verify the safety performance of the design method, a minimum obstacle avoidance distance test analysis was conducted, and the results are shown in the following figure.</p><p>Figure 8 Minimum obstacle avoidance distance test results</p><p>Based on the above results, it can be seen that the minimum obstacle avoidance distance of the algorithm in this paper is always around 8m and relatively stable, while the minimum obstacle avoidance distance of the vehicle control algorithm based on reinforcement learning fluctuates greatly between 1-5m and is unstable; The minimum obstacle avoidance distance of the vehicle control algorithm based on the Beizer curve hovers between 0.6 and 4 meters, which is too small and poses a certain degree of danger, and the fluctuation is greater than that of the vehicle control algorithm based on reinforcement learning. This indicates that the security performance of the algorithm in this article is relatively good.</p><p>9. In distributed control scenarios, communication delays between LiDAR and computing units may affect real-time performance. Delay tolerance testing should be added.</p><p>Response: The delay tolerance test analysis has been supplemented by following your recommendations, such as:</p><p>To further validate the applicability of the design method and test the power consumption and delay tolerance in different distance environments, a comparative test analysis was conducted between the reinforcement learning based vehicle control algorithm in reference [9] and the Bezier curve based vehicle control algorithm in reference [10]. The results are shown below.</p><p>Table 4 Results of comparing power consumption and minimum obstacle avoidance distance at different distances</p><p>distance /m A vehicle control algorithm based on reinforcement learning Vehicle control algorithm based on Bezer curve Proposed method</p><p>Power consumption/kWh Delay tolerance/s Power consumption/kWh Delay tolerance/s Power consumption/kWh Delay tolerance/s</p><p>500 0.86 1.5 0.77 1.4 0.2 0.12</p><p>1000 0.99 1.9 0.86 1.9 0.39 0.24</p><p>1500 1.20 2.1 0.92 2.1 0.61 0.33</p><p>2000 1.35 2.5 1.32 2.3 0.77 0.41</p><p>2500 1.41 5.9 1.50 2.5 0.95 0.52</p><p>According to the data in Table 4, the power consumption of the Proposed method is significantly lower than the other two algorithms at a distance of 500 meters. The power consumption of the Proposed method is 0.2 kWh, while the algorithms based on reinforcement learning and Bezier curve are 0.86 kWh and 0.77 kWh, respectively. This indicates that the proposed method performs the best in terms of power consumption and has higher energy efficiency. And the delay tolerance of the Proposed method * * is significantly lower than the other two algorithms. At a distance of 500 meters, the delay tolerance of the Proposed method is 0.12 seconds, while the reinforcement learning based algorithm and the Bezier curve based algorithm are 1.5 seconds and 1.4 seconds, respectively. This indicates that the proposed method performs the best in terms of delay tolerance, with higher real-time performance and response speed.</p><p>10. Some references (e.g., [9] on laser SLAM) are from before 2023 and you can cite the latest GEITS paper "Deep transfer learning for intelligent vehicle perception: A survey" (DOI:</p><supplementary-material id="pone.0323631.s004" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Research_on_Autonomou_auresp_1.docx</named-content></p></caption><media xlink:href="pone.0323631.s004.docx"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0323631.r004" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323631.r004</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Lei</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Lei Zhang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Lei Zhang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323631" id="rel-obj004" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">11 Apr 2025</named-content>
</p><p>Research on Autonomous Obstacle Avoidance of Mountainous Tractors Based on Semantic Neural Network and Laser SLAM</p><p>PONE-D-25-03307R1</p><p>Dear Dr. Xu,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">Editorial Manager&#x000ae;</ext-link> &#x000a0;and clicking the &#x02018;Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at authorbilling@plos.org.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>Kind regards,</p><p>Lei Zhang, PhD</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>The revised version is publishable.</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.</p><p>Reviewer #1:&#x000a0;All comments have been addressed</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>**********</p><p>2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>**********</p><p>3. Has the statistical analysis been performed appropriately and rigorously? </p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>**********</p><p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>**********</p><p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>**********</p><p>6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p><p>Reviewer #1:&#x000a0;This paper presents an autonomous obstacle avoidance method for mountainous tractors using semantic neural networks and laser SLAM. It integrates LiDAR-based environment scanning, a pruned YOLOv3 model for obstacle detection, and A* with DWA algorithms for path planning. Experimental results demonstrate improved accuracy, efficiency, and safety over traditional methods, with lower power consumption and memory usage. However, the study lacks comparisons with advanced algorithms like RRT* and detailed validation under dynamic conditions, necessitating further refinement for real-world deployment.</p><p>Reviewer #2:&#x000a0;(No Response)</p><p>**********</p><p>7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link> ). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link> .</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p></body></sub-article><sub-article article-type="editor-report" id="pone.0323631.r005" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0323631.r005</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Lei</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Lei Zhang</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Lei Zhang</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> , which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0323631" id="rel-obj005" related-article-type="reviewed-article"/></front-stub><body><p>PONE-D-25-03307R1</p><p>PLOS ONE</p><p>Dear Dr. Xu,</p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p><p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p><p>* All references, tables, and figures are properly cited</p><p>* All relevant supporting information is included in the manuscript submission,</p><p>* There are no issues that prevent the paper from being properly typeset</p><p>You will receive further&#x000a0;instructions from the production team, including instructions on how to review your proof when it&#x000a0;is ready. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few days to review your paper and let you know the next and final steps.</p><p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>If we can help with anything else, please email us at customercare@plos.org.</p><p>Thank you for submitting your work to PLOS ONE and supporting open access.</p><p>Kind regards,</p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Dr. Lei Zhang</p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>