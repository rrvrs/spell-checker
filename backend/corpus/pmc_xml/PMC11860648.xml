<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006456</article-id><article-id pub-id-type="pmc">PMC11860648</article-id><article-id pub-id-type="doi">10.3390/s25041227</article-id><article-id pub-id-type="publisher-id">sensors-25-01227</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Study on Systematic Improvement of Transformer Models for Object Pose Estimation</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9091-1729</contrib-id><name><surname>Lee</surname><given-names>Jungwoo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01227" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8673-4528</contrib-id><name><surname>Suh</surname><given-names>Jinho</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af2-sensors-25-01227" ref-type="aff">2</xref><xref rid="c1-sensors-25-01227" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Xu</surname><given-names>Haitao</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>An</surname><given-names>Fengping</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Ye</surname><given-names>Chuyang</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01227"><label>1</label>Smart Mobility Research Center, Korea Institute of Robotics and Technology Convergence (KIRO), Pohang 37666, Republic of Korea; <email>ricow@kiro.re.kr</email></aff><aff id="af2-sensors-25-01227"><label>2</label>Major of Mechanical System Engineering, Pukyong National University, Busan 48513, Republic of Korea</aff><author-notes><corresp id="c1-sensors-25-01227"><label>*</label>Correspondence: <email>suhgang@pknu.ac.kr</email></corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1227</elocation-id><history><date date-type="received"><day>05</day><month>2</month><year>2025</year></date><date date-type="rev-recd"><day>13</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Transformer architecture, initially developed for natural language processing and time series analysis, has been successfully adapted to various generative models in several domains. Object pose estimation, which uses images to determine the 3D position and orientation of an object, is essential for tasks such as robotic manipulation. This study introduces a transformer-based deep learning model for object pose estimation in computer vision, which determines the 3D position and orientation of objects from images. A baseline model derived from an encoder-only transformer faces challenges with high GPU memory usage when handling multiple objects. To improve training efficiency and support multi-object inference, it reduces memory consumption by adjusting the transformer&#x02019;s attention layer and incorporates low-rank weight decomposition to decrease parameters. Additionally, GQA and RMS normalization enhance multi-object pose estimation performance, resulting in reduced memory usage and improved training accuracy. The improved model implementation with an extended matrix dimension reduced the GPU memory usage to only 2.5% of the baseline model, although it increased the number of model weight parameters. To mitigate this, the number of weight parameters was reduced by 28% using low-rank weight decomposition in the linear layer of attention. In addition, a 17% improvement in rotation training accuracy over the baseline model was achieved by applying GQA and RMS normalization.</p></abstract><kwd-group><kwd>transformer</kwd><kwd>object pose estimation</kwd><kwd>low-rank weight decomposition</kwd></kwd-group><funding-group><award-group><funding-source>Ministry of Trade, Industry &#x00026; Energy</funding-source><award-id>RS-2024-00507783</award-id></award-group><funding-statement>This work was supported by the Ministry of Trade, Industry &#x00026; Energy (MOTIE, Korea) under the Industrial Technology Innovation Program. Grant No. RS-2024-00507783, &#x0201c;The development of AI-based predictive maintenance and mobile autonomous robots for high-risk tasks in steel manufacturing processes&#x0201d;.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01227"><title>1. Introduction</title><p>Transformer architecture [<xref rid="B1-sensors-25-01227" ref-type="bibr">1</xref>] was designed to solve natural language processing and time series sequencing problems. It has also been applied to generative models in various domains, such as computer vision captioning [<xref rid="B2-sensors-25-01227" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01227" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01227" ref-type="bibr">4</xref>], object classification [<xref rid="B5-sensors-25-01227" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01227" ref-type="bibr">6</xref>], and object detection [<xref rid="B7-sensors-25-01227" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01227" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01227" ref-type="bibr">9</xref>]. This architecture has shown excellent performance as a complement to existing RNN and CNN-based models.</p><p>Object pose estimation is the accurate determination of the three-dimensional position and orientation of an object. This technique provides a computational understanding of the pose of a given object using image or sensor data, often from cameras. Accurate objects pose information is critical for the perception and interaction of robotic systems with their environment, enabling tasks such as object manipulation. In addition, object pose estimation is fundamental for safe navigation by facilitating recognition of surrounding vehicles, pedestrians, and traffic lights in the context of autonomous driving.</p><p>Traditional methods for object pose estimation, which rely on feature point extraction and model matching algorithms, often struggle with robustness in a variety of environments, especially those characterized by varying illumination and occlusion. Recently, deep learning-based models [<xref rid="B10-sensors-25-01227" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01227" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01227" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01227" ref-type="bibr">13</xref>] have been used to predict the pose of objects directly from images.</p><p>In this study, we present a transformer-based model for object pose estimation and demonstrate that systematically optimizing each component of the model architecture not only minimizes hardware resource requirements, such as GPU memory, but also improves estimation accuracy through efficient design.</p><p>The main contributions of this study are summarized as follows:<list list-type="bullet"><list-item><p>We introduce a baseline model that uses a transformer for object pose estimation. In the transformer architecture of this model, input data consisting of a 3D point cloud derived from object information is serialized. In addition, position encoding for embedding the input data is omitted since it is not affected by considerations in the temporal domain.</p></list-item><list-item><p>To improve the performance of the transformer model for object pose estimation, the following modifications are introduced: (i) the output dimension in the attention layer of the transformer is expanded to support multiple object estimation; (ii) the input weight matrices in the attention layer are approximated by two smaller matrices, reducing the total number of model parameters; and (iii) multi-head attention is replaced by grouped query attention (GQA) [<xref rid="B14-sensors-25-01227" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01227" ref-type="bibr">15</xref>], RMS normalization is incorporated instead of traditional layer normalization, and the activation function is switched from ReLU to SiLU.</p></list-item></list></p></sec><sec id="sec2-sensors-25-01227"><title>2. Related Works</title><p>In recent years, transformer architecture has attracted considerable attention in the field of natural language processing (NLP) due to its breakthrough performance. Transformers have achieved state-of-the-art (SOTA) results in various NLP tasks by overcoming the limitations of traditional recurrent neural networks (RNNs) in learning long-term dependencies and have been widely adopted. The core mechanism of transformers, self-attention, computes interactions between all words in an input sequence and assigns importance weights to effectively capture contextual information. Multi-head attention [<xref rid="B16-sensors-25-01227" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01227" ref-type="bibr">17</xref>] extends this capability by applying multiple self-attention mechanisms in parallel. This allows the model to analyze input sequences from different representational subspaces. However, the transformer architecture lacks an inherent mechanism for explicitly modeling word order due to its reliance on parallel processing. To overcome this limitation, positional encoding is incorporated to provide the model with information about word positions. This allows the model to efficiently capture order-dependent semantic variation.</p><p>In this study, a transformer model with fundamental architecture was utilized. The transformer is a sequence-to-sequence model comprising an encoder and a decoder that processes an input sequence to generate an output sequence. The encoder consists of N identical layers, transforming the input sequence into a fixed-length vector representation. Each layer uses a multi-head self-attention mechanism and a feedforward network (FFN) to capture relationships between all words in the input sequence and perform nonlinear transformations. The decoder generates the output sequence using the context vectors generated by the encoder. It incorporates a masked multi-head self-attention mechanism and cross-attention to learn dependencies between previously generated words and interactions with the encoder output.</p><p>The overall baseline structure of the transformer is shown in <xref rid="sensors-25-01227-f001" ref-type="fig">Figure 1</xref> below.</p><p>Object pose estimation is a fundamental research area in computer vision. It focuses on determining the position and orientation of an object in three-dimensional space. This problem is typically formulated as the estimation of the six degrees of freedom, which have three translational and three rotational components. Accurate pose estimation is critical for a variety of applications, such as robotic manipulation, augmented reality, and 3D reconstruction. Recent research efforts have focused on improving the accuracy and robustness of pose estimation algorithms, especially in complex and challenging scenarios, by using deep learning and probabilistic frameworks.</p><p>In transformer-based pose estimation methods, the TransPose framework [<xref rid="B18-sensors-25-01227" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01227" ref-type="bibr">19</xref>] employs a geometry-aware transformer encoder to extract local features from point clouds while simultaneously incorporating global context, thereby improving robustness to occlusions. CatFormer [<xref rid="B20-sensors-25-01227" ref-type="bibr">20</xref>] improves category-level pose estimation through a multi-stage deformation process, which uses transformers to model geometric relationships and iteratively refine feature representations. For depth-based pose estimation, the SwinDePose approach [<xref rid="B21-sensors-25-01227" ref-type="bibr">21</xref>] exclusively utilizes depth images, computing angles between normal vectors and coordinate axes. These features are then processed using a swin transformer to achieve accurate pose estimation. In addition, a novel end-to-end method [<xref rid="B22-sensors-25-01227" ref-type="bibr">22</xref>] uses implicit representations to bridge discrete and continuous feature maps, thereby improving the accuracy of 6D pose estimation. This approach utilizes a compact dual-stream network to estimate object surface features, while singular value decomposition (SVD) is applied for rotation representation to further refine pose predictions. This technique has demonstrated promising performance on the LINEMOD benchmark.</p></sec><sec sec-type="methods" id="sec3-sensors-25-01227"><title>3. Methods</title><sec id="sec3dot1-sensors-25-01227"><title>3.1. Baseline Transformer Model for Single Object Pose Estimation</title><p>The proposed baseline transformer architecture for object pose estimation utilizes an encoder-only transformer to convert point cloud data into a feature vector, which is then processed by a linear multi-layer perceptron (MLP) to predict the object&#x02019;s rotation and translation. Unlike sequential data, point cloud inputs represent spatial information without a temporal order, eliminating the need for positional encoding. Consequently, no positional encoding is required during the embedding process.</p><p>The vision foundation model (VFM) is used to perform object detection and mask segmentation on the color images of objects captured by the camera. For object detection, models such as DINO [<xref rid="B23-sensors-25-01227" ref-type="bibr">23</xref>] and Yolo-World [<xref rid="B24-sensors-25-01227" ref-type="bibr">24</xref>] are used, while the mask segmentation is performed using the Segment Anything Model (SAM) [<xref rid="B25-sensors-25-01227" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01227" ref-type="bibr">26</xref>] and the Segment-Everything-Everywhere-All-At-Once (SEEM) [<xref rid="B27-sensors-25-01227" ref-type="bibr">27</xref>].</p><p>The depth image captured by the camera contains a distance value for each pixel. The depth information is efficiently transformed into point cloud data using the pseudocode of the conversion algorithm, as shown in <xref rid="sensors-25-01227-t001" ref-type="table">Table 1</xref> below.</p><p>The &#x02018;w&#x02019; and &#x02018;h&#x02019; are each pixel number of width and height axis. The &#x02018;pt_base&#x02019; array represents the homogeneous coordinates of each pixel in the image. The &#x02018;ppxy&#x02019; array represents the principal point coordinates of the camera. The &#x02018;fxy&#x02019; matrix represents the inverse of the camera&#x02019;s intrinsic matrix, specifically the inverse of the focal lengths. The &#x02018;pt_vertex&#x02019; array stores the 3D coordinates of each pixel in the camera coordinate system.</p><p>The conversion of depth images into point clouds can be affected by several sources of error. These include sensor noise, resolution limitations, occlusions, material surface reflectivity, and others. However, by using methods such as sensor calibration, noise filtering, adaptive thresholding, lens distortion correction, and post-processing techniques, these errors can be effectively reduced, resulting in more accurate and reliable point clouds.</p><p>Using the location and mask information of the object detected by VFM, point clouds are generated that contain only the object with the background removed, along with the corresponding color data. The depth image of the object is segmented into a predefined number of grids in 2D space. For each grid cell, a representative depth value and an average color value are computed by averaging a subset of the included point clouds. These values are then organized into a two-dimensional matrix. Finally, each row of the matrix is concatenated to transform it into a one-dimensional sequence that serves as input to the transformer model. This process is illustrated in <xref rid="sensors-25-01227-f002" ref-type="fig">Figure 2</xref>.</p><p>A baseline transformer model for single object pose-estimation processes a one-dimensional serialized representation of the normalized point cloud data as an input and produces feature vectors as an output. This model is designed as an encoder-only transformer, adapted from the standard vanilla transformer architecture. During the encoding process, the residual connections incorporate the layer normalization, and the feed-forward network uses the ReLU activation function. The structure of this encoder-only transformer-based model is shown in <xref rid="sensors-25-01227-f003" ref-type="fig">Figure 3</xref> below.</p></sec><sec id="sec3dot2-sensors-25-01227"><title>3.2. Extending the Matrix Dimension of the Attention Layer to Estimate Multi-Object Pose in a Single Prediction</title><p>The baseline model above uses a one-dimensional linearized input format for object pose estimation. To estimate the poses of multiple objects, a concatenated one-dimensional input vector must be fed into the model, and its length increases significantly. However, as the input vector grows, so does the memory required for matrix multiplication in the multi-head attention layer of the transformer. This leads to challenges in maintaining an appropriate batch size during training due to hardware constraints, and significantly increases the computational time required for processing.</p><p>To improve computational efficiency, we propose a novel approach to reduce memory consumption within the multi-head attention layer. We optimize matrix multiplication operations by restructuring the one-dimensional input vector into a two-dimensional matrix where the number of columns is equal to the number of objects.</p><p>The 3D tensor output of the traditional attention mechanism is reshaped into a 4D tensor by reducing the size of the last two dimensions of the output matrix. In the scaled dot product operation, matrix multiplication is performed on the last two dimensions of the multi-dimensional matrix, while the earlier dimensions are handled as batches. Thus, this approach effectively reduces both the memory space and computational load required for matrix multiplication.</p><p>This optimization requires adjusting the output dimension of the attention layer. We increase the output dimension to match the number of objects, thereby reducing the input size for the subsequent cumulative encoder layer. The computational load of the attention mechanism is effectively reduced by this dimension extension. This matrix dimension extension process for the output of the attention layer is illustrated in <xref rid="sensors-25-01227-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-01227-f005" ref-type="fig">Figure 5</xref>.</p></sec><sec id="sec3dot3-sensors-25-01227"><title>3.3. Low-Rank Weight Decomposition Adaptation Within the Attention Layer for Model Parameter Reduction</title><p>Transformer models improve inference performance by iteratively stacking encoder blocks, but this leads to an increase in model parameters. To enable efficient training while incorporating additional encoder blocks within constrained hardware resources, it is essential to reduce the previously expanded weight parameters.</p><p>Low-rank weight decomposition (LoRA) [<xref rid="B28-sensors-25-01227" ref-type="bibr">28</xref>] is implemented in the linear blocks responsible for processing queries, keys, and values within the multi-head attention layer of the transformer model. This technique approximates the weight matrices in the linear blocks with two smaller matrices. By using low-rank matrices, LoRA effectively reduces the number of trainable parameters, resulting in lower computational cost, reduced memory consumption, and mitigation of overfitting during model training.</p><p>In this study, the size of the number of columns in the front small matrix and the number of rows in the back matrix are divided by the number of heads in the multi-head attention. This scaling ensures that the dimensions of the matrices undergoing decomposition are appropriately adjusted for the specified number of attention heads. This process is illustrated in <xref rid="sensors-25-01227-f006" ref-type="fig">Figure 6</xref>.</p></sec><sec id="sec3dot4-sensors-25-01227"><title>3.4. Using Grouped Query Attention and RMS Normalization in the Transformer Model</title><p>Grouped query attention (GQA) [<xref rid="B29-sensors-25-01227" ref-type="bibr">29</xref>] is an advanced mechanism designed to enhance the efficiency of transformer architectures by addressing the memory and computational constraints associated with traditional multi-head attention (MHA). By grouping queries and enabling shared key and value heads, GQA significantly reduces the total number of parameters and memory consumption while maintaining model accuracy.</p><p>RMS normalization [<xref rid="B30-sensors-25-01227" ref-type="bibr">30</xref>] is a technique for adjusting the scale of input data in a neural network to improve the stability and performance of the learning process. RMS normalization is like the layer normalization used in previous transform models. However, RMS normalization differs in that it normalizes the input data by calculating the squared mean for each input dimension, adding a small constant, taking the square root, and then dividing. With fewer computations than layer normalization, RMS normalization speeds up the training of the model. It also stabilizes the learning process by scaling the input data and reducing sensitivity to the learning rate.</p><p>In this study, we improved the attention layer by increasing the matrix dimension of its output layer to handle multiple objects, while also incorporating low-rank weight decomposition into the internal linear block. In addition, the GQA mechanism was applied, with the number of groups in GQA set as a multiple of 2 greater than the square root of the number of heads. The encoder normalization layer was changed from traditional layer normalization to RMS normalization, and the activation function in the feed-forward network was changed from ReLU to SiLU [<xref rid="B31-sensors-25-01227" ref-type="bibr">31</xref>], as shown in <xref rid="sensors-25-01227-f007" ref-type="fig">Figure 7</xref> below.</p></sec></sec><sec id="sec4-sensors-25-01227"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-01227"><title>4.1. Baseline Transformer Model for Single-Object Pose Estimation</title><p>The performance evaluations for the systematic improvement of the transformer model were conducted using the LINEMOD (LM) dataset [<xref rid="B32-sensors-25-01227" ref-type="bibr">32</xref>]. This dataset is designed to support model-based training, detection, and pose estimation tasks in complex scenes, as well as the development and evaluation of techniques for detecting and estimating the six-degrees-of-freedom pose of texture-less 3D objects. The LM dataset consists of 15 video sequences, each with over 1100 frames of 15 different household objects with unique features such as color, shape, and size.</p><p>The hyperparameters of a baseline model of encoder-only transformer are shown in <xref rid="sensors-25-01227-t002" ref-type="table">Table 2</xref>.</p><p>The hardware used for testing was Intel NUC 12 Extreme and NVIDIA RTX 4090 24G. In each test instance, the number of samples in the LM dataset used for training was 30,000. The number of steps per epoch was 100, the number of epochs was 300, and the batch size varied from 8 to 128.</p><p>The peak GPU memory usage of a model is affected by the batch size of the training data. The batch size, which represents the number of data samples processed in a forward and backward pass during training, plays a critical role in the model&#x02019;s learning performance and efficiency. A smaller batch size can improve generalization by allowing more frequent updates to capture different patterns in the data, but it can also result in slower learning and increased loss variability. In contrast, a larger batch size stabilizes updates and takes advantage of parallel processing capabilities, although it may degrade generalization, increase the likelihood of convergence to a local minimum, and require more memory.</p><p>The change in the peak memory usage as a function of batch size is shown in <xref rid="sensors-25-01227-f008" ref-type="fig">Figure 8</xref> below. When the batch size is 64, approximately 16.34 G of GPU memory is used. If a larger batch size is applied, it will not run in this test hardware environment.</p><p><xref rid="sensors-25-01227-f009" ref-type="fig">Figure 9</xref> below illustrates the variations in training loss and accuracy for the rotation and translation of the object pose of the baseline model when the batch size is set to 64 and the training epoch is run up to 300.</p></sec><sec id="sec4dot2-sensors-25-01227"><title>4.2. Extending the Matrix Dimension of the Attention Layer to Estimate Multi-Object Pose in a Single Prediction</title><p>We evaluated the peak GPU memory usage of the modified model that extends the matrix dimension of the attention layer output in the context of multi-object pose estimation. Increasing the matrix dimension axis raises the number of weight parameters, but reduces the complexity of the matrix dot product operation. This reduction in complexity leads to lower GPU memory usage during training, allowing for larger batch sizes to improve learning efficiency.</p><p><xref rid="sensors-25-01227-f010" ref-type="fig">Figure 10</xref> shows the comparison of peak GPU memory usage based on batch size and the number of objects for pose estimation in a single inference. With a batch size of 64 and a single object, the peak memory usage of the baseline model was 16.34 GB, while the peak memory usage of the improved model was reduced to 0.41 GB, which is a significant reduction of 2.5% compared to the baseline model. When the number of objects increases to 15 with the same batch size of 64, the peak memory usage of the improved model is 4.84 GB, reflecting a reduction in GPU memory usage of approximately 75% compared to the baseline model.</p><p>For a single object, the execution time of each training epoch was 1.2 s per epoch, compared to 31 s for the baseline model. In the improved model, the execution time per epoch was 2.5 s while training 15 objects simultaneously. This is about 8% of the time required by the baseline model to train a single object.</p><p>The number of weight parameters in the improved model is 7,700,503, about 50% more than the number of parameters in the baseline model. The number of parameters is not affected by the change in batch size.</p><p>When the improved model is trained for 300 epochs with a batch size of 64, the training loss and accuracy of its rotation, which represent the model&#x02019;s output, are compared to those of the baseline model, as illustrated in <xref rid="sensors-25-01227-f011" ref-type="fig">Figure 11</xref>. The results show that the improved model obtains a faster reduction in loss and a greater improvement in accuracy compared to the baseline model. This improvement not only accelerates loss reduction and improves accuracy but also optimizes GPU memory usage during training by extending the output axis of the attention layer in the encoder.</p><p><xref rid="sensors-25-01227-f012" ref-type="fig">Figure 12</xref> shows a comparison of the training accuracy for rotation and translation, which was evaluated by varying the number of objects for the simultaneous inference.</p></sec><sec id="sec4dot3-sensors-25-01227"><title>4.3. Low-Rank Weight Decomposition Adaptation Within the Attention Layer for Model Parameter Reduction</title><p>To mitigate the increase in model weight parameters resulting from the dimensional extension of the output layer of the attention, a weight-decomposed low-rank adaptation is incorporated into the input linear block of the attention module. With this adaptation, the total weight parameters of the model are reduced to 5,562,625, which is about 28% less than the previous model, but about 9% more than the baseline model. This result is shown in <xref rid="sensors-25-01227-f013" ref-type="fig">Figure 13</xref> below.</p><p>As shown in <xref rid="sensors-25-01227-f014" ref-type="fig">Figure 14</xref> below, the number of weight parameters in the improved model has decreased, but the peak GPU memory usage has increased slightly to about 2% due to the addition of internal matrix multiplication.</p><p>The training loss and accuracy of the improved model, specifically for rotation and translation, were compared to those of the previous model when trained for 300 epochs with a batch size of 64. The incorporation of low-rank weight decomposition reduced the size of the matrices within the attention layer, leading to a reduction in the number of weight parameters, while increasing the depth of the model through the addition of matrices. As a result, the rotation accuracy decreased slightly during training, while translation accuracy improved, as shown in <xref rid="sensors-25-01227-f015" ref-type="fig">Figure 15</xref>. Despite the significant reduction in weight parameters, inference accuracy remained largely unchanged.</p><p><xref rid="sensors-25-01227-f016" ref-type="fig">Figure 16</xref> compares training accuracy for translation by adjusting the number of objects for simultaneous inference.</p><p><xref rid="sensors-25-01227-f017" ref-type="fig">Figure 17</xref> shows a comparison of training accuracy for translation based on the number of ranks in the low-rank weight decomposition. To optimize the number of ranks while considering the model&#x02019;s weight parameters, the ideal rank selection is equal to the number of linear units in the multi-head attention mechanism divided by the number of heads.</p></sec><sec id="sec4dot4-sensors-25-01227"><title>4.4. Using Grouped Query Attention and RMS Normalization in the Transformer Model</title><p>The use of GQA and RMS normalization to improve model inference efficiency is also expected to improve training performance. Compared to previous models, no significant variations were observed in the number of weight parameters, GPU memory usage, or execution time per epoch during training, as shown in <xref rid="sensors-25-01227-f018" ref-type="fig">Figure 18</xref> and <xref rid="sensors-25-01227-f019" ref-type="fig">Figure 19</xref>.</p><p>The model with GQA and RMS normalization was trained for 300 epochs with a batch size of 64, and its training loss and accuracy for rotation and translation were evaluated against the previous model. The training accuracy for rotation improved by about 10%, and the training accuracy for translation improved by about 4% over the previous model when GQA and RMS normalization were applied. As shown in <xref rid="sensors-25-01227-f020" ref-type="fig">Figure 20</xref>, these results indicate a positive impact on loss convergence and overall accuracy improvement during training.</p><p><xref rid="sensors-25-01227-f021" ref-type="fig">Figure 21</xref> compares the training accuracy of rotation and translation while varying the number of objects for simultaneous inference. It can be observed that the training accuracy improves as the number of objects increases due to the higher amount of training per epoch.</p><p>Starting from the baseline model, <xref rid="sensors-25-01227-f022" ref-type="fig">Figure 22</xref> shows the change in the training accuracy for rotation and translation with the successive improvements. The reduction in computation time per training step with model improvements is shown in <xref rid="sensors-25-01227-f023" ref-type="fig">Figure 23</xref>.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01227"><title>5. Conclusions</title><p>Transformer architecture was originally designed for natural language processing and time series analysis but has since been utilized in generative models across multiple fields. It has proven to be an effective complement to RNN- and CNN-based approaches.</p><p>Object pose estimation refers to the process of identifying an object&#x02019;s 3D position and orientation using image or sensor inputs. This method plays a vital role in robotic perception and interaction, facilitating operations like object manipulation.</p><p>We have incorporated a transformer architecture into a deep learning model for object pose estimation in computer vision. Using a vision foundation model, the input data is preprocessed by extracting object masks from color and depth images. Depth information is employed to generate a point cloud, which is treated as a sequential dataset.</p><p>The baseline model adopts a transformer architecture with an encoder-only implementation of the vanilla transformer. For multi-object pose estimation, input vectors corresponding to different objects are concatenated into a one-dimensional sequence. However, as the length of the input vectors increases, the model requires significant GPU memory for training. Consequently, the batch size cannot be increased significantly, limiting improvements in training efficiency.</p><p>We systematically improve the training efficiency while allowing for the simultaneous inference of multiple objects. First, we modify the output dimension of the transformer&#x02019;s attention layer to optimize GPU memory usage during training. These improvements facilitate multi-object inference. Next, we incorporate low-rank weight decomposition into the linear input of the attention layer to reduce the model&#x02019;s weight parameters while maintaining training accuracy. In addition, we refine the model for multi-object pose estimation by integrating GQA and RMS regularization, resulting in improved learning performance, faster loss convergence, and higher accuracy.</p><p>When evaluating the model with the implemented improvements, the peak GPU memory usage of the baseline model reached 16.34 GB while training on a single object. However, after incorporating the extended matrix dimension, the memory consumption dropped to 0.41 GB, which is approximately 2.5% of the memory consumed by the baseline model during training. The training execution time for the improved model was approximately 1.2 s per epoch, as opposed to 31 s for the baseline model.</p><p>By increasing the output dimension of the attention layer, the number of weight parameters in the model increased by approximately 50% relative to the baseline model. To address this issue, we implemented low-rank weight decomposition on the linear input layer of attention, which reduced the amount of parameter increase to about 9% relative to the baseline model.</p><p>Without changing the number of model weight parameters or GPU memory usage, the application of GQA and RMS normalization facilitated rapid loss convergence and improved training accuracy. As a result, training accuracy was improved by approximately 10% for rotation and 4% for translation.</p><p>These improvements in the transformer model have been evaluated in experiments using relatively low-performance hardware and small datasets. In the future, it will be necessary to compare performance using large datasets and batch sizes in environments with more GPU memory and powerful computing power.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.L.; methodology, J.L.; software, J.L.; validation, J.L.; formal analysis, J.L.; investigation, J.L.; resources, J.L.; data curation, J.L.; writing&#x02014;original draft preparation, J.L.; writing&#x02014;review and editing, J.L. and J.S.; visualization, J.L.; supervision, J.S.; project administration, J.S.; funding acquisition, J.S. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01227"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Vaswani</surname><given-names>A.</given-names></name>
<name><surname>Shazeer</surname><given-names>N.</given-names></name>
<name><surname>Parmar</surname><given-names>N.</given-names></name>
<name><surname>Uszkoreit</surname><given-names>J.</given-names></name>
<name><surname>Jones</surname><given-names>L.</given-names></name>
<name><surname>Gomez</surname><given-names>A.N.</given-names></name>
<name><surname>Kaiser</surname><given-names>L.</given-names></name>
<name><surname>Polosukhin</surname><given-names>I.</given-names></name>
</person-group><article-title>Attention Is All You Need</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1706.03762</pub-id></element-citation></ref><ref id="B2-sensors-25-01227"><label>2.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>K.</given-names></name>
<name><surname>Li</surname><given-names>L.</given-names></name>
<name><surname>Lin</surname><given-names>C.-C.</given-names></name>
<name><surname>Ahmed</surname><given-names>F.</given-names></name>
<name><surname>Gan</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Lu</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
</person-group><article-title>SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#x02013;24 June 2022</conf-date><pub-id pub-id-type="doi">10.1109/cvpr52688.2022.01742</pub-id></element-citation></ref><ref id="B3-sensors-25-01227"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cao</surname><given-names>Q.</given-names></name>
<name><surname>Huang</surname><given-names>H.</given-names></name>
<name><surname>Liao</surname><given-names>M.</given-names></name>
<name><surname>Mao</surname><given-names>X.</given-names></name>
</person-group><article-title>Ada-SwinBERT: Adaptive Token Selection for Efficient Video Captioning with Online Self-Distillation</article-title><source>Proceedings of the 2023 IEEE International Conference on Multimedia and Expo (ICME)</source><conf-loc>Brisbane, Australia</conf-loc><conf-date>10&#x02013;14 July 2023</conf-date><fpage>7</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1109/icme55011.2023.00010</pub-id></element-citation></ref><ref id="B4-sensors-25-01227"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Zhao</surname><given-names>M.</given-names></name>
<name><surname>Shi</surname><given-names>F.</given-names></name>
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>S.</given-names></name>
</person-group><article-title>Enhancing Ocean Scene Video Captioning with Multimodal Pre-Training and Video-Swin-Transformer</article-title><source>Proceedings of the IECON 2023&#x02014;49th Annual Conference of the IEEE Industrial Electronics Society</source><conf-loc>Singapore</conf-loc><conf-date>16&#x02013;19 October 2023</conf-date><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/iecon51785.2023.10312358</pub-id></element-citation></ref><ref id="B5-sensors-25-01227"><label>5.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Dosovitskiy</surname><given-names>A.</given-names></name>
<name><surname>Beyer</surname><given-names>L.</given-names></name>
<name><surname>Kolesnikov</surname><given-names>A.</given-names></name>
<name><surname>Weissenborn</surname><given-names>D.</given-names></name>
<name><surname>Zhai</surname><given-names>X.</given-names></name>
<name><surname>Unterthiner</surname><given-names>T.</given-names></name>
<name><surname>Dehghani</surname><given-names>M.</given-names></name>
<name><surname>Minderer</surname><given-names>M.</given-names></name>
<name><surname>Heigold</surname><given-names>G.</given-names></name>
<name><surname>Gelly</surname><given-names>S.</given-names></name>
<etal/>
</person-group><article-title>An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale</article-title><source>Proceedings of the International Conference on Learning Representations</source><conf-loc>Vienna, Austria</conf-loc><conf-date>4 May 2021</conf-date></element-citation></ref><ref id="B6-sensors-25-01227"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Shan</surname><given-names>C.</given-names></name>
</person-group><article-title>Vision Transformers Are Active Learners for Image Copy Detection</article-title><source>Neurocomputing</source><year>2024</year><volume>587</volume><fpage>127687</fpage><pub-id pub-id-type="doi">10.1016/j.neucom.2024.127687</pub-id></element-citation></ref><ref id="B7-sensors-25-01227"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Carion</surname><given-names>N.</given-names></name>
<name><surname>Massa</surname><given-names>F.</given-names></name>
<name><surname>Synnaeve</surname><given-names>G.</given-names></name>
<name><surname>Usunier</surname><given-names>N.</given-names></name>
<name><surname>Kirillov</surname><given-names>A.</given-names></name>
<name><surname>Zagoruyko</surname><given-names>S.</given-names></name>
</person-group><article-title>End-to-End Object Detection with Transformers</article-title><source>Proceedings of the European Conference on Computer Vision&#x02014;ECCV 2020</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#x02013;28 August 2020</conf-date><comment>Lecture Notes in Computer Science</comment><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2020</year><fpage>213</fpage><lpage>229</lpage></element-citation></ref><ref id="B8-sensors-25-01227"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>Z.</given-names></name>
<name><surname>Tao</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>X.</given-names></name>
</person-group><article-title>NAN-DETR: Noising Multi-Anchor Makes DETR Better for Object Detection</article-title><source>Front. Neurorobot.</source><year>2024</year><volume>18</volume><elocation-id>1484088</elocation-id><pub-id pub-id-type="doi">10.3389/fnbot.2024.1484088</pub-id><pub-id pub-id-type="pmid">39468993</pub-id>
</element-citation></ref><ref id="B9-sensors-25-01227"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>X.</given-names></name>
<name><surname>Yang</surname><given-names>X.</given-names></name>
<name><surname>Shao</surname><given-names>L.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Gao</surname><given-names>Q.</given-names></name>
<name><surname>Shi</surname><given-names>H.</given-names></name>
</person-group><article-title>GM-DETR: Research on a Defect Detection Method Based on Improved DETR</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>3610</elocation-id><pub-id pub-id-type="doi">10.3390/s24113610</pub-id><pub-id pub-id-type="pmid">38894399</pub-id>
</element-citation></ref><ref id="B10-sensors-25-01227"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Lin</surname><given-names>K.-Y.</given-names></name>
<name><surname>Zhang</surname><given-names>G.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
</person-group><article-title>RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization</article-title><source>Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>New Orleans, LA, USA</conf-loc><conf-date>18&#x02013;24 June 2022</conf-date><fpage>14860</fpage><lpage>14870</lpage><pub-id pub-id-type="doi">10.1109/cvpr52688.2022.01446</pub-id></element-citation></ref><ref id="B11-sensors-25-01227"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>C.</given-names></name>
<name><surname>Song</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>Q.</given-names></name>
</person-group><article-title>HybridPose: 6D Object Pose Estimation under Hybrid Representations</article-title><source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date><fpage>428</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1109/cvpr42600.2020.00051</pub-id></element-citation></ref><ref id="B12-sensors-25-01227"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Chin</surname><given-names>T.-J.</given-names></name>
<name><surname>Klimavicius</surname><given-names>M.</given-names></name>
</person-group><article-title>Occlusion-Robust Object Pose Estimation with Holistic Representation</article-title><source>Proceedings of the 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#x02013;8 January 2022</conf-date><fpage>2223</fpage><lpage>2233</lpage><pub-id pub-id-type="doi">10.1109/wacv51458.2022.00228</pub-id></element-citation></ref><ref id="B13-sensors-25-01227"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>M.</given-names></name>
<name><surname>Dong</surname><given-names>H.</given-names></name>
</person-group><article-title>GenPOSE: Generative Category-Level Object Pose Estimation via Diffusion Models</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arxiv.2306.10531</pub-id><pub-id pub-id-type="arxiv">2306.10531</pub-id></element-citation></ref><ref id="B14-sensors-25-01227"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Khan</surname><given-names>Z.</given-names></name>
<name><surname>Khaquan</surname><given-names>M.</given-names></name>
<name><surname>Tafveez</surname><given-names>O.</given-names></name>
<name><surname>Samiwala</surname><given-names>B.</given-names></name>
<name><surname>Raza</surname><given-names>A.A.</given-names></name>
</person-group><article-title>Beyond Uniform Query Distribution: Key-Driven Grouped Query Attention</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arxiv.2408.08454</pub-id><pub-id pub-id-type="arxiv">2408.08454</pub-id></element-citation></ref><ref id="B15-sensors-25-01227"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chinnakonduru</surname><given-names>S.S.</given-names></name>
<name><surname>Mohapatra</surname><given-names>A.</given-names></name>
</person-group><article-title>Weighted Grouped Query Attention in Transformers</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arxiv.2407.10855</pub-id><pub-id pub-id-type="arxiv">2407.10855</pub-id></element-citation></ref><ref id="B16-sensors-25-01227"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cordonnier</surname><given-names>J.-B.</given-names></name>
<name><surname>Loukas</surname><given-names>A.</given-names></name>
<name><surname>Jaggi</surname><given-names>M.</given-names></name>
</person-group><article-title>Multi-Head Attention: Collaborate Instead of Concatenate</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="doi">10.48550/arxiv.2006.16362</pub-id><pub-id pub-id-type="arxiv">2006.16362</pub-id></element-citation></ref><ref id="B17-sensors-25-01227"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jin</surname><given-names>P.</given-names></name>
<name><surname>Zhu</surname><given-names>B.</given-names></name>
<name><surname>Yuan</surname><given-names>L.</given-names></name>
<name><surname>Yan</surname><given-names>S.</given-names></name>
</person-group><article-title>MOH: Multi-Head Attention as Mixture-of-Head Attention</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arxiv.2410.11842</pub-id><pub-id pub-id-type="arxiv">2410.11842</pub-id></element-citation></ref><ref id="B18-sensors-25-01227"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>D.</given-names></name>
<name><surname>Zhou</surname><given-names>G.</given-names></name>
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>Q.</given-names></name>
</person-group><article-title>TransPose: 6D Object Pose Estimation with Geometry-Aware Transformer</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2310.16279</pub-id><pub-id pub-id-type="doi">10.1016/j.neucom.2024.127652</pub-id></element-citation></ref><ref id="B19-sensors-25-01227"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Abdulsalam</surname><given-names>M.</given-names></name>
<name><surname>Aouf</surname><given-names>N.</given-names></name>
</person-group><article-title>TransPose: A Transformer-Based 6D Object Pose Estimation Network with Depth Refinement</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arxiv.2307.05561</pub-id><pub-id pub-id-type="arxiv">2307.05561</pub-id></element-citation></ref><ref id="B20-sensors-25-01227"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>S.</given-names></name>
<name><surname>Zhai</surname><given-names>D.-H.</given-names></name>
<name><surname>Xia</surname><given-names>Y.</given-names></name>
</person-group><article-title>CatFormer: Category-Level 6D Object Pose Estimation with Transformer</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2024</year><volume>38</volume><fpage>6808</fpage><lpage>6816</lpage><pub-id pub-id-type="doi">10.1609/aaai.v38i7.28505</pub-id></element-citation></ref><ref id="B21-sensors-25-01227"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Stamos</surname><given-names>I.</given-names></name>
</person-group><article-title>Depth-Based 6DOF Object Pose Estimation Using SWIN Transformer</article-title><source>Proceedings of the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Detroit, MI, USA</conf-loc><conf-date>1&#x02013;5 October 2023</conf-date><fpage>1185</fpage><lpage>1191</lpage><pub-id pub-id-type="doi">10.1109/iros55552.2023.10342215</pub-id></element-citation></ref><ref id="B22-sensors-25-01227"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cao</surname><given-names>C.</given-names></name>
<name><surname>Yu</surname><given-names>B.</given-names></name>
<name><surname>Xu</surname><given-names>W.</given-names></name>
<name><surname>Chen</surname><given-names>G.</given-names></name>
<name><surname>Ai</surname><given-names>Y.</given-names></name>
</person-group><article-title>End-to-End Implicit Object Pose Estimation</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>5721</elocation-id><pub-id pub-id-type="doi">10.3390/s24175721</pub-id><pub-id pub-id-type="pmid">39275632</pub-id>
</element-citation></ref><ref id="B23-sensors-25-01227"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Caron</surname><given-names>M.</given-names></name>
<name><surname>Touvron</surname><given-names>H.</given-names></name>
<name><surname>Misra</surname><given-names>I.</given-names></name>
<name><surname>Jegou</surname><given-names>H.</given-names></name>
<name><surname>Mairal</surname><given-names>J.</given-names></name>
<name><surname>Bojanowski</surname><given-names>P.</given-names></name>
<name><surname>Joulin</surname><given-names>A.</given-names></name>
</person-group><article-title>Emerging Properties in Self-Supervised Vision Transformers</article-title><source>Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>10&#x02013;17 October 2021</conf-date><pub-id pub-id-type="doi">10.1109/iccv48922.2021.00951</pub-id></element-citation></ref><ref id="B24-sensors-25-01227"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>T.</given-names></name>
<name><surname>Song</surname><given-names>L.</given-names></name>
<name><surname>Ge</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Shan</surname><given-names>Y.</given-names></name>
</person-group><article-title>YOLO-World: Real-Time Open-Vocabulary Object Detection</article-title><source>Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#x02013;22 June 2024</conf-date><fpage>16901</fpage><lpage>16911</lpage><pub-id pub-id-type="doi">10.1109/cvpr52733.2024.01599</pub-id></element-citation></ref><ref id="B25-sensors-25-01227"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kirillov</surname><given-names>A.</given-names></name>
<name><surname>Mintun</surname><given-names>E.</given-names></name>
<name><surname>Ravi</surname><given-names>N.</given-names></name>
<name><surname>Mao</surname><given-names>H.</given-names></name>
<name><surname>Rolland</surname><given-names>C.</given-names></name>
<name><surname>Gustafson</surname><given-names>L.</given-names></name>
<name><surname>Xiao</surname><given-names>T.</given-names></name>
<name><surname>Whitehead</surname><given-names>S.</given-names></name>
<name><surname>Berg</surname><given-names>A.C.</given-names></name>
<name><surname>Lo</surname><given-names>W.-Y.</given-names></name>
<etal/>
</person-group><article-title>Segment Anything</article-title><source>Proceedings of the 2023 IEEE/CVF International Conference on Computer Vision (ICCV)</source><conf-loc>Paris, France</conf-loc><conf-date>1&#x02013;6 October 2023</conf-date><pub-id pub-id-type="doi">10.1109/iccv51070.2023.00371</pub-id></element-citation></ref><ref id="B26-sensors-25-01227"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ravi</surname><given-names>N.</given-names></name>
<name><surname>Gabeur</surname><given-names>V.</given-names></name>
<name><surname>Hu</surname><given-names>Y.-T.</given-names></name>
<name><surname>Hu</surname><given-names>R.</given-names></name>
<name><surname>Ryali</surname><given-names>C.</given-names></name>
<name><surname>Ma</surname><given-names>T.</given-names></name>
<name><surname>Khedr</surname><given-names>H.</given-names></name>
<name><surname>R&#x000e4;dle</surname><given-names>R.</given-names></name>
<name><surname>Rolland</surname><given-names>C.</given-names></name>
<name><surname>Gustafson</surname><given-names>L.</given-names></name>
<etal/>
</person-group><article-title>SAM 2: Segment Anything in Images and Videos</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arxiv.2408.00714</pub-id><pub-id pub-id-type="arxiv">2408.00714</pub-id></element-citation></ref><ref id="B27-sensors-25-01227"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zou</surname><given-names>X.</given-names></name>
<name><surname>Yang</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>F.</given-names></name>
<name><surname>Li</surname><given-names>L.</given-names></name>
<name><surname>Gao</surname><given-names>J.</given-names></name>
<name><surname>Lee</surname><given-names>Y.J.</given-names></name>
</person-group><article-title>Segment Everything Everywhere All at Once</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arxiv.2304.06718</pub-id><pub-id pub-id-type="arxiv">2304.06718</pub-id></element-citation></ref><ref id="B28-sensors-25-01227"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>S.-Y.</given-names></name>
<name><surname>Wang</surname><given-names>C.-Y.</given-names></name>
<name><surname>Yin</surname><given-names>H.</given-names></name>
<name><surname>Molchanov</surname><given-names>P.</given-names></name>
<name><surname>Wang</surname><given-names>Y.-C.F.</given-names></name>
<name><surname>Cheng</surname><given-names>K.-T.</given-names></name>
<name><surname>Chen</surname><given-names>M.-H.</given-names></name>
</person-group><article-title>DORA: Weight-Decomposed Low-Rank Adaptation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arxiv.2402.09353</pub-id><pub-id pub-id-type="arxiv">2402.09353</pub-id></element-citation></ref><ref id="B29-sensors-25-01227"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ainslie</surname><given-names>J.</given-names></name>
<name><surname>Lee-Thorp</surname><given-names>J.</given-names></name>
<name><surname>Michiel</surname><given-names>D.J.</given-names></name>
<name><surname>Zemlyanskiy</surname><given-names>Y.</given-names></name>
<name><surname>Lebr&#x000f3;n</surname><given-names>F.</given-names></name>
<name><surname>Sanghai</surname><given-names>S.</given-names></name>
</person-group><article-title>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arxiv.2305.13245</pub-id><pub-id pub-id-type="arxiv">2305.13245</pub-id></element-citation></ref><ref id="B30-sensors-25-01227"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>Sennrich</surname><given-names>R.</given-names></name>
</person-group><article-title>Root Mean Square Layer Normalization</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="doi">10.48550/arxiv.1910.07467</pub-id><pub-id pub-id-type="arxiv">1910.07467</pub-id></element-citation></ref><ref id="B31-sensors-25-01227"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Elfwing</surname><given-names>S.</given-names></name>
<name><surname>Uchibe</surname><given-names>E.</given-names></name>
<name><surname>Doya</surname><given-names>K.</given-names></name>
</person-group><article-title>Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1702.03118</pub-id><pub-id pub-id-type="doi">10.1016/j.neunet.2017.12.012</pub-id><pub-id pub-id-type="pmid">29395652</pub-id>
</element-citation></ref><ref id="B32-sensors-25-01227"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hinterstoisser</surname><given-names>S.</given-names></name>
<name><surname>Lepetit</surname><given-names>V.</given-names></name>
<name><surname>Ilic</surname><given-names>S.</given-names></name>
<name><surname>Holzer</surname><given-names>S.</given-names></name>
<name><surname>Bradski</surname><given-names>G.</given-names></name>
<name><surname>Konolige</surname><given-names>K.</given-names></name>
<name><surname>Navab</surname><given-names>N.</given-names></name>
</person-group><article-title>Model Based Training, Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes</article-title><source>Proceedings of the Asian Conference on Computer Vision&#x02014;ACCV 2012</source><conf-loc>Daejeon, Republic of Korea</conf-loc><conf-date>5&#x02013;9 November 2012</conf-date><comment>Lecture Notes in Computer Science</comment><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2013</year><fpage>548</fpage><lpage>562</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01227-f001"><label>Figure 1</label><caption><p>Architecture of the vanilla transformer.</p></caption><graphic xlink:href="sensors-25-01227-g001" position="float"/></fig><fig position="float" id="sensors-25-01227-f002"><label>Figure 2</label><caption><p>Process for extracting object information from camera images using VFM and generating one-dimensional model input data.</p></caption><graphic xlink:href="sensors-25-01227-g002" position="float"/></fig><fig position="float" id="sensors-25-01227-f003"><label>Figure 3</label><caption><p>The baseline structure of the encoder-only transformer model.</p></caption><graphic xlink:href="sensors-25-01227-g003" position="float"/></fig><fig position="float" id="sensors-25-01227-f004"><label>Figure 4</label><caption><p>Method for extracting information about a plurality of objects from camera images using VFM and generating two-dimensional model input data.</p></caption><graphic xlink:href="sensors-25-01227-g004" position="float"/></fig><fig position="float" id="sensors-25-01227-f005"><label>Figure 5</label><caption><p>Modification of the output dimension of the extension of the attention layer.</p></caption><graphic xlink:href="sensors-25-01227-g005" position="float"/></fig><fig position="float" id="sensors-25-01227-f006"><label>Figure 6</label><caption><p>Modification of the adaptation of the low-rank weight decomposition within the attention layer.</p></caption><graphic xlink:href="sensors-25-01227-g006" position="float"/></fig><fig position="float" id="sensors-25-01227-f007"><label>Figure 7</label><caption><p>Modification of the application of the GQA and the RMS normalization to the transformer model.</p></caption><graphic xlink:href="sensors-25-01227-g007" position="float"/></fig><fig position="float" id="sensors-25-01227-f008"><label>Figure 8</label><caption><p>Comparison of peak memory usage relative to batch size in a baseline model.</p></caption><graphic xlink:href="sensors-25-01227-g008" position="float"/></fig><fig position="float" id="sensors-25-01227-f009"><label>Figure 9</label><caption><p>Training loss and accuracy for rotation and translation of the object pose in a baseline model.</p></caption><graphic xlink:href="sensors-25-01227-g009" position="float"/></fig><fig position="float" id="sensors-25-01227-f010"><label>Figure 10</label><caption><p>Comparison of the peak GPU memory usage as a function of the batch size and the number of objects for the pose estimation during a single inference.</p></caption><graphic xlink:href="sensors-25-01227-g010" position="float"/></fig><fig position="float" id="sensors-25-01227-f011"><label>Figure 11</label><caption><p>Comparison of training loss and accuracy for rotation between the baseline model and the model that extends the dimension of the matrix.</p></caption><graphic xlink:href="sensors-25-01227-g011" position="float"/></fig><fig position="float" id="sensors-25-01227-f012"><label>Figure 12</label><caption><p>Evaluation of training accuracy for rotation and translation by variation of the number of objects during simultaneous inference by extending the dimension of the matrix.</p></caption><graphic xlink:href="sensors-25-01227-g012" position="float"/></fig><fig position="float" id="sensors-25-01227-f013"><label>Figure 13</label><caption><p>Comparison of the number of weight parameters between baseline, extending matrix dimension, and low-rank weight decomposition models.</p></caption><graphic xlink:href="sensors-25-01227-g013" position="float"/></fig><fig position="float" id="sensors-25-01227-f014"><label>Figure 14</label><caption><p>Comparison of the peak GPU memory usage in relation to the size of the batch and the number of objects between extending matrix dimension and low-rank weight decomposition models.</p></caption><graphic xlink:href="sensors-25-01227-g014" position="float"/></fig><fig position="float" id="sensors-25-01227-f015"><label>Figure 15</label><caption><p>Comparison of training loss and accuracy for rotation and translation between the previous model and the model that adapts the low-rank weight decomposition within the attention layer.</p></caption><graphic xlink:href="sensors-25-01227-g015" position="float"/></fig><fig position="float" id="sensors-25-01227-f016"><label>Figure 16</label><caption><p>Evaluation of training accuracy for translation by number of objects for simultaneous inference of models with low-rank weight decompositions.</p></caption><graphic xlink:href="sensors-25-01227-g016" position="float"/></fig><fig position="float" id="sensors-25-01227-f017"><label>Figure 17</label><caption><p>Evaluation of training accuracy for translation by number of ranks of weight decompositions.</p></caption><graphic xlink:href="sensors-25-01227-g017" position="float"/></fig><fig position="float" id="sensors-25-01227-f018"><label>Figure 18</label><caption><p>Comparison of the number of weight parameters between each model.</p></caption><graphic xlink:href="sensors-25-01227-g018" position="float"/></fig><fig position="float" id="sensors-25-01227-f019"><label>Figure 19</label><caption><p>Comparison of the peak GPU memory usage in relation to the size of the batch and the number of objects.</p></caption><graphic xlink:href="sensors-25-01227-g019" position="float"/></fig><fig position="float" id="sensors-25-01227-f020"><label>Figure 20</label><caption><p>Comparison between the previous model and the model that adapts GQA and RMS normalization in terms of training loss and accuracy for rotation and translation.</p></caption><graphic xlink:href="sensors-25-01227-g020" position="float"/></fig><fig position="float" id="sensors-25-01227-f021"><label>Figure 21</label><caption><p>Evaluation of training accuracy for rotation and translation by number of objects for simultaneous inference of models with GQA and RMS normalization.</p></caption><graphic xlink:href="sensors-25-01227-g021" position="float"/></fig><fig position="float" id="sensors-25-01227-f022"><label>Figure 22</label><caption><p>Changes in the training accuracy of the rotation and translation between the baseline model and the models with each improvement applied.</p></caption><graphic xlink:href="sensors-25-01227-g022" position="float"/></fig><fig position="float" id="sensors-25-01227-f023"><label>Figure 23</label><caption><p>Changes in the execution time of the training step between the baseline model and the models with each improvement applied.</p></caption><graphic xlink:href="sensors-25-01227-g023" position="float"/></fig><table-wrap position="float" id="sensors-25-01227-t001"><object-id pub-id-type="pii">sensors-25-01227-t001_Table 1</object-id><label>Table 1</label><caption><p>Pseudocode for converting depth information into point clouds.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">&#x000a0;For each pixel (w, h) in the image:<break/>&#x02003;&#x000a0;1: Calculate the base point:<break/>&#x02003;&#x02003;&#x02003;pt_base = [w, h, 1]<break/>&#x02003;&#x000a0;2: Calculate the principal point offset:&#x02003;&#x02003;<break/>&#x02003;&#x02003;&#x02003;ppxy = [camera_ppx, camera_ppy, 0]<break/>&#x02003;&#x000a0;3: Calculate the inverse focal length matrix:<break/>&#x02003;&#x02003;&#x02003;fxy = [[1/camera_fx, 0, 0], [0, 1/camera_fy, 0], [0, 0, 1]]<break/>&#x02003;&#x000a0;4: Calculate the 3D point in camera coordinates:<break/>&#x02003;&#x02003;&#x02003;pt_vertex = (pt_base &#x02212; ppxy) * fxy * depth_scale</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01227-t002"><object-id pub-id-type="pii">sensors-25-01227-t002_Table 2</object-id><label>Table 2</label><caption><p>The hyperparameters of a baseline model of encoder-only transformer.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Multi-head attention</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Linear units</td><td align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">256</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of heads</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Feed forward network</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Linear units</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">256</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normalization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LayerNorm</td></tr><tr><td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Encoder-only transformer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of encoder block</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Number of weight parameter</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5,088,841</td></tr></tbody></table></table-wrap></floats-group></article>