<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40397922</article-id><article-id pub-id-type="pmc">PMC12094719</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0322776</article-id><article-id pub-id-type="publisher-id">PONE-D-24-51935</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Science Policy</subject><subj-group><subject>Research Integrity</subject><subj-group><subject>Research Ethics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Language</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Sociology</subject><subj-group><subject>Culture</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Database and Informatics Methods</subject><subj-group><subject>Bioinformatics</subject><subj-group><subject>Sequence Analysis</subject><subj-group><subject>Sequence Alignment</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Reasoning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Reasoning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Reasoning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Sociology</subject><subj-group><subject>Social Status</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Large-scale moral machine experiment on large language models</article-title><alt-title alt-title-type="running-head">Large-scale moral machine experiment on large language models</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zaim bin Ahmad</surname><given-names>Muhammad Shahrul</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6355-1366</contrib-id><name><surname>Takemoto</surname><given-names>Kazuhiro</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Department of Bioscience and Bioinformatics, Kyushu Institute of Technology, Iizuka, Japan</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Faculty of Engineering and Technology, Multimedia University, Melaka, Malaysia</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Data Science and AI Research Center, Kyushu Institute of Technology, Iizuka, Japan</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Carrasco-Farr&#x000e9;</surname><given-names>Carlos</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Toulouse Business School: TBS Education, SPAIN</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>takemoto@bio.kyutech.ac.jp</email></corresp></author-notes><pub-date pub-type="epub"><day>21</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0322776</elocation-id><history><date date-type="received"><day>13</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>26</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Shahrul Zaim bin Ahmad, Takemoto</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Shahrul Zaim bin Ahmad, Takemoto</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0322776.pdf">
</self-uri><abstract><p>The rapid advancement of Large Language Models (LLMs) and their potential integration into autonomous driving systems necessitates understanding their moral decision-making capabilities. While our previous study examined four prominent LLMs using the Moral Machine experimental framework, the dynamic landscape of LLM development demands a more comprehensive analysis. Here, we evaluate moral judgments across 52 different LLMs, including multiple versions of proprietary models (GPT, Claude, Gemini) and open-source alternatives (Llama, Gemma), to assess their alignment with human moral preferences in autonomous driving scenarios. Using a conjoint analysis framework, we evaluated how closely LLM responses aligned with human preferences in ethical dilemmas and examined the effects of model size, updates, and architecture. Results showed that proprietary models and open-source models exceeding 10 billion parameters demonstrated relatively close alignment with human judgments, with a significant negative correlation between model size and distance from human judgments in open-source models. However, model updates did not consistently improve alignment with human preferences, and many LLMs showed excessive emphasis on specific ethical principles. These findings suggest that while increasing model size may naturally lead to more human-like moral judgments, practical implementation in autonomous driving systems requires careful consideration of the trade-off between judgment quality and computational efficiency. Our comprehensive analysis provides crucial insights for the ethical design of autonomous systems and highlights the importance of considering cultural contexts in AI moral decision-making.</p></abstract><funding-group><award-group id="award001"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap>
</funding-source><award-id>21H03545</award-id><principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6355-1366</contrib-id>
<name><surname>Takemoto</surname><given-names>Kazuhiro</given-names></name>
</principal-award-recipient></award-group><funding-statement>This study was supported by JSPS KAKENHI (Grant Number 21H03545). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="6"/><table-count count="0"/><page-count count="20"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data are within the manuscript and its Supporting Information files. The code to replicate the results, including prompt generation, analysis scripts, and all associated materials are publicly available in our GitHub repository (<ext-link xlink:href="https://github.com/kztakemoto/mmllm" ext-link-type="uri">https://github.com/kztakemoto/mmllm)</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data are within the manuscript and its Supporting Information files. The code to replicate the results, including prompt generation, analysis scripts, and all associated materials are publicly available in our GitHub repository (<ext-link xlink:href="https://github.com/kztakemoto/mmllm" ext-link-type="uri">https://github.com/kztakemoto/mmllm)</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>The rapid advancement of Large Language Models (LLMs) has inaugurated a new era in artificial intelligence (AI), demonstrating unprecedented capabilities across diverse domains [<xref rid="pone.0322776.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0322776.ref004" ref-type="bibr">4</xref>]. These models, exemplified by ChatGPT [<xref rid="pone.0322776.ref005" ref-type="bibr">5</xref>,<xref rid="pone.0322776.ref006" ref-type="bibr">6</xref>] (developed by OpenAI) and its counterparts, exhibit remarkable proficiency in natural language processing, content generation, complex problem-solving, and decision-making. As these models advance from laboratory experiments to real-world applications, understanding their decision-making processes, particularly in ethical contexts, becomes crucial for ensuring their responsible deployment. This has become a subject of intense scientific investigation [<xref rid="pone.0322776.ref007" ref-type="bibr">7</xref>&#x02013;<xref rid="pone.0322776.ref009" ref-type="bibr">9</xref>].</p><p>Autonomous driving represents a field where LLM application necessitates particularly careful evaluation [<xref rid="pone.0322776.ref010" ref-type="bibr">10</xref>&#x02013;<xref rid="pone.0322776.ref013" ref-type="bibr">13</xref>]. When autonomous vehicles make decisions on public roads, their choices directly affect human safety and well-being. The automotive industry&#x02019;s investigation of LLM integration for enhancing autonomous vehicles&#x02019; capabilities includes various aspects: analyzing complex road conditions, interpreting traffic situations, planning appropriate responses, and importantly, making ethical decisions in challenging scenarios [<xref rid="pone.0322776.ref014" ref-type="bibr">14</xref>&#x02013;<xref rid="pone.0322776.ref020" ref-type="bibr">20</xref>]. This integration spans critical functions from environmental comprehension to decision making, highlighting the need for thorough understanding of these models&#x02019; capabilities and limitations.</p><p>The Moral Machine experiment [<xref rid="pone.0322776.ref011" ref-type="bibr">11</xref>] provides a systematic framework for evaluating ethical decision-making in autonomous systems. This study presented participants with scenarios involving unavoidable accidents to assess how autonomous vehicles should behave in morally challenging situations. The experiment revealed clear patterns in human moral preferences, such as prioritizing human lives over animals and favoring the preservation of a greater number of lives. While acknowledging its limitations including methodological constraints in scenario design, challenges in cultural representation, and gaps between theoretical choices and practical implementation [<xref rid="pone.0322776.ref021" ref-type="bibr">21</xref>&#x02013;<xref rid="pone.0322776.ref026" ref-type="bibr">26</xref>], this work has stimulated extensive research into the ethical implications of AI in autonomous systems [<xref rid="pone.0322776.ref027" ref-type="bibr">27</xref>&#x02013;<xref rid="pone.0322776.ref030" ref-type="bibr">30</xref>]. These foundational insights, despite their limitations which we explore in detail in our discussion, have provided crucial frameworks for examining ethical decision making in autonomous systems.</p><p>While some studies have explored AI responses to standard ethical dilemmas [<xref rid="pone.0322776.ref031" ref-type="bibr">31</xref>,<xref rid="pone.0322776.ref032" ref-type="bibr">32</xref>], such as the classic trolley problem [<xref rid="pone.0322776.ref033" ref-type="bibr">33</xref>], the intricate scenarios presented in the Moral Machine experiment offer a more comprehensive exploration of AI moral preferences. Building on this approach, our previous study [<xref rid="pone.0322776.ref034" ref-type="bibr">34</xref>] applied the Moral Machine framework to a limited set of LLMs, specifically focusing on ChatGPT (including both GPT-3.5 and GPT-4 versions), Llama 2, and PaLM 2. This investigation examined the moral judgments made by these AI systems in complex autonomous driving scenarios. The results indicated that LLMs were capable of producing ethical judgments that often aligned with human preferences in many aspects. However, we also observed some notable discrepancies between LLM outputs and human choices, particularly in scenarios involving multiple ethical considerations</p><p>Despite these initial findings, the rapid evolution of LLM technology necessitates a more comprehensive analysis. This dynamic landscape is evidenced by the frequent release and update of both proprietary models like ChatGPT, Claude [<xref rid="pone.0322776.ref035" ref-type="bibr">35</xref>] (Anthropic), and Gemini [<xref rid="pone.0322776.ref036" ref-type="bibr">36</xref>] (Google), as well as open-source alternatives such as Llama 3 [<xref rid="pone.0322776.ref037" ref-type="bibr">37</xref>] (Meta) and Gemma [<xref rid="pone.0322776.ref038" ref-type="bibr">38</xref>] (Google). The emergence of these diverse models underscores the importance of examining the consistency and evolution of moral judgments across different LLM versions, architectures, and development approaches.</p><p>This study aims to conduct a more extensive examination of moral judgment trends in the latest LLM models and investigate the effects of model updates on these judgments. By comparing a wider array of LLMs, we seek to provide insights into the approaches different LLMs take to moral dilemmas, how these approaches may change over time, and the potential implications for AI system deployment in critical decision-making roles, particularly in autonomous driving. This study will contribute to a more nuanced understanding of AI outputs in high-stakes scenarios, informing the development and deployment of AI systems in complex, real-world contexts.</p></sec><sec sec-type="materials|methods" id="sec002"><title>Materials and methods</title><sec id="sec003"><title>Moral machine scenario generation</title><p>To comprehensively evaluate how different LLMs approach moral decision-making in autonomous driving scenarios and assess the effects of model updates on these judgments, we generated a systematic set of test scenarios following our previous methodology. These scenarios presented dilemmas involving an autonomous vehicle facing an unavoidable accident, requiring a choice between two outcomes (&#x0201c;Case 1&#x0201d; and &#x0201c;Case 2&#x0201d;) with varying ethical implications.</p><p>The scenarios were designed through a process of constrained randomization, exploring six primary dimensions: species (humans vs. pets), social value (higher vs. lower perceived social status), gender (female vs. male), age (younger vs. older), fitness (physically favored vs. less fit individuals), and utilitarianism (smaller vs. larger group). Additionally, three secondary dimensions were incorporated: interventionism (swerving vs. continuing straight), relationship to the autonomous vehicle (passengers vs. pedestrians), and legal considerations (e.g., adherence to traffic signals). The scenarios were carefully constructed to balance complexity and clarity, ensuring that they effectively probed the ethical reasoning capabilities of the LLMs under study.</p></sec><sec id="sec004"><title>Large language models</title><p>To investigate how moral judgment patterns vary across different model architectures and evolve through updates, we expanded upon our previous study [<xref rid="pone.0322776.ref034" ref-type="bibr">34</xref>] by incorporating a comprehensive set of LLMs, including the latest versions available. Our analysis encompasses both base language models and models specifically optimized for dialogue interactions. While these models may differ in their training approaches, they share fundamental capabilities in processing and responding to complex queries, including moral scenarios. These LLMs were utilized to generate responses to moral machine scenarios.</p><p>We initially incorporated the results from [<xref rid="pone.0322776.ref034" ref-type="bibr">34</xref>], which included responses from three proprietary models: GPT-3.5 (gpt-3.5-turbo-0613), GPT-4 (gpt-4-0613), and PaLM 2, as well as one open source model: Llama 2 (7B). Additional models were selected based on their prominence in the field, technological advancements, and public recognition. This selection aimed to provide a representative sample of current LLM technology while exploring variations in ethical reasoning across different model types.</p><p>Among the proprietary models, we evaluated multiple versions of OpenAI models, including the snapshot versions of GPT-3.5 [<xref rid="pone.0322776.ref005" ref-type="bibr">5</xref>] and GPT-4 [<xref rid="pone.0322776.ref006" ref-type="bibr">6</xref>] from March 2023 to April 2024, as well as the newer GPT-4o models [<xref rid="pone.0322776.ref039" ref-type="bibr">39</xref>] (May and August 2024 versions), GPT-4o-mini, o1 and o1-mini [<xref rid="pone.0322776.ref040" ref-type="bibr">40</xref>]. Google DeepMind&#x02019;s Gemini models were also assessed, including Gemini 1.0 Pro [<xref rid="pone.0322776.ref036" ref-type="bibr">36</xref>] and multiple snapshot versions of Gemini 1.5 Pro and Gemini 1.5 Flash [<xref rid="pone.0322776.ref041" ref-type="bibr">41</xref>]. From Anthropic, we evaluated the latest versions of their Claude models, including Claude 3 Haiku, Sonnet, Opus [<xref rid="pone.0322776.ref035" ref-type="bibr">35</xref>], and Claude 3.5 Sonnet [<xref rid="pone.0322776.ref042" ref-type="bibr">42</xref>].</p><p>In the open-source domain, we included Meta&#x02019;s Llama family [<xref rid="pone.0322776.ref037" ref-type="bibr">37</xref>], comprising Llama 3 and Llama 3.1 in 8B and 70B versions, Llama 3.2 in 1B and 3B versions, and Llama 3.3 in 70B version. A comprehensive evaluation of Google&#x02019;s Gemma model family [<xref rid="pone.0322776.ref038" ref-type="bibr">38</xref>,<xref rid="pone.0322776.ref043" ref-type="bibr">43</xref>] was conducted, encompassing multiple versions and scales from 2B-it to 27B-it, including the derivative DataGemma RIG 27B-it model [<xref rid="pone.0322776.ref044" ref-type="bibr">44</xref>]. Additional open-source models evaluated in this study included Large Model Systems Organization&#x02019;s Vicuna v1.5 (7B and 13B) [<xref rid="pone.0322776.ref045" ref-type="bibr">45</xref>], Mistral (7B Instruct v0.2) [<xref rid="pone.0322776.ref046" ref-type="bibr">46</xref>], Mistral-NeMo [<xref rid="pone.0322776.ref047" ref-type="bibr">47</xref>], CohereForAI&#x02019;s Command R+ [<xref rid="pone.0322776.ref048" ref-type="bibr">48</xref>], and Microsoft&#x02019;s Phi-3.5 [<xref rid="pone.0322776.ref049" ref-type="bibr">49</xref>] (both MoE-instruct and mini-instruct versions).</p><p>For a comprehensive list of all models and their specific versions used in this study, please refer to our GitHub repository (<ext-link xlink:href="https://github.com/kztakemoto/mmllm" ext-link-type="uri">https://github.com/kztakemoto/mmllm</ext-link>).</p><p>All models were used with their default parameter settings. The input prompts were consistent with those used in [<xref rid="pone.0322776.ref034" ref-type="bibr">34</xref>], requiring each model to select either &#x0201c;Case 1&#x0201d; or &#x0201c;Case 2&#x0201d; for each scenario. The complete code for prompt generation, analysis, and resulting data is available in our GitHub repository.</p><p>While most models were evaluated using 50,000 scenarios, following our previous study [<xref rid="pone.0322776.ref034" ref-type="bibr">34</xref>], constraints related to application programming interface (API) usage costs and computational time necessitated the use of fewer scenarios for certain models. The majority of the models were assessed using the full set of 50,000 scenarios. However, a substantial group of models, including the GPT-4 and GPT-4o series, Claude 3 Opus and Claude 3.5 family, all Gemini family, Llama 3 and subsequent versions, OpenAI o1 mini, Mistral-NeMo, Gemma-2 family, DataGemma, Command R+, and Phi-3.5 family, were evaluated using a reduced set of 10,000 scenarios. The OpenAI o1 model, facing even more significant API cost limitations, was evaluated using a further reduced set of 5,000 scenarios.</p></sec><sec id="sec005"><title>Data analysis</title><p>To systematically evaluate how different LLMs align with human moral preferences and how this alignment changes across model versions and architectures, we analyzed the LLM responses using the conjoint analysis framework as described in [<xref rid="pone.0322776.ref050" ref-type="bibr">50</xref>] and implemented in the original study [<xref rid="pone.0322776.ref011" ref-type="bibr">11</xref>] on the Moral Machine experiment and our previous study [<xref rid="pone.0322776.ref034" ref-type="bibr">34</xref>]. This approach allows for a robust, non-parametric identification of causal effects without specific modeling assumptions.</p><p>The analysis process involved pre-processing LLM responses and dummy variable coding for scenario attributes, which correspond to the primary and secondary dimensions described in the scenario generation. We then calculated the Average Marginal Component Effect (AMCE) for each attribute. The AMCE quantifies an attribute&#x02019;s influence on the LLM&#x02019;s ethical decisions, enabling systematic comparison of moral preferences across different LLM models and versions.</p><p>To compare the moral preference patterns between LLMs and humans, we performed comparative analyses using the calculated AMCE values across all nine attributes. The human preference data were obtained from the original Moral Machine experiment [<xref rid="pone.0322776.ref011" ref-type="bibr">11</xref>]. We quantified the alignment between LLM and human preferences by computing the Euclidean distances between their respective AMCE values. Furthermore, to better understand the relationship patterns among different models and human preferences, we employed principal component analysis (PCA) followed by cluster analysis on the AMCE values.</p><p>All statistical analyses, including AMCE calculations, distance measurements, PCA, and subsequent analyses were performed using R statistical software (version 4.4.1).</p></sec></sec><sec sec-type="results" id="sec006"><title>Results</title><sec id="sec007"><title>Moral judgments of large language models</title><p>Moral judgments of large language models To comprehensively evaluate how different LLMs approach moral decision-making in autonomous driving scenarios, we conducted a systematic analysis of their responses across multiple ethical dimensions. Our analysis framework focused on quantifying and comparing the moral preferences of various LLM families with human judgments.</p><p><xref rid="pone.0322776.g001" ref-type="fig">Fig 1</xref> presents radar charts characterizing the moral judgment tendencies of various LLMs across nine preferences quantified by Average Marginal Component Effect (AMCE) values (see <xref rid="pone.0322776.s002" ref-type="supplementary-material">S1 Table</xref> for exact numerical data). These charts enable direct comparison of preferences across multiple versions and model sizes of major LLM families, including GPT-3.5, GPT-4, GPT-4o/o1, Claude, Gemini, Llama, Gemma, and other LLMs, against established human preferences (see <xref rid="pone.0322776.s001" ref-type="supplementary-material">S1 Fig</xref> for detailed differences between each model and human values).</p><fig position="float" id="pone.0322776.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0322776.g001</object-id><label>Fig 1</label><caption><title>Radar plots of moral preferences across different LLM families.</title><p>AMCE values indicate preferences: Species (+: humans, &#x02013;: pets), social value (+: high status, &#x02013;: low status), relation to AV (+: pedestrians, -: passengers), number (+: more, &#x02013;: fewer), law (+: lawful, &#x02013;: unlawful), intervention (+: inaction, &#x02013;: action), gender (+: female, &#x02013;: male), fitness (+: fit, &#x02013;: unfit/obese), age (+: young, &#x02013;: elderly). Gray-filled areas represent human preferences. Each subplot represents a different model family: (A) GPT-3.5, (B) GPT-4, (C) GPT-4o/o1, (D) Claude, (E) Gemini, (F) Llama, (G) Gemma, and (H) other models.</p></caption><graphic xlink:href="pone.0322776.g001" position="float"/></fig><p>Our initial analysis revealed three key patterns in LLM moral judgments. First, many LLMs demonstrated strong alignment with human preferences in fundamental moral decisions, showing large positive values (&#x0003e;0.5) for No. Characters and Species dimensions. This indicates consistent tendencies in prioritizing the preservation of more lives and favoring humans over animals, suggesting that LLMs can capture basic human moral intuitions. Second, we observed significant variations in judgment patterns across different model families, with some models exhibiting inverse priorities compared to human preferences. Third, certain models showed disproportionately strong preferences in the same direction as humans (differences in AMCE values approximately &#x0003e;0.3 compared to human values), indicating potential overemphasis of specific moral principles.</p><p>These patterns provide important insights into both the capabilities and limitations of LLMs in moral reasoning tasks. Below, we elaborate on these findings for each model family, examining how different architectures influence moral judgments, and analyzing the effects of model updates on alignment with human preferences.</p><sec id="sec008"><title>GPT-3.5 family.</title><p>Analysis of the GPT-3.5 family (<xref rid="pone.0322776.g001" ref-type="fig">Fig 1A</xref>) revealed significant temporal evolution in moral judgments across model updates. While models from June 2023 to January 2024 exhibited similar patterns, the March 2023 version showed distinctive tendencies, particularly in several key ethical dimensions.</p><p>In the Law dimension, the March version showed strong positive values favoring law-abiding individuals (AMCE = 0.7), but subsequent versions shifted toward more neutral judgments (AMCE <inline-formula id="pone.0322776.e001"><alternatives><graphic xlink:href="pone.0322776.e001.jpg" id="pone.0322776.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02248;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> 0). Similar temporal changes were observed in Gender-related judgments, where initial strong female preference (AMCE = 0.5 in March 2023) gradually reduced toward human judgment levels (AMCE = 0.2 in January 2024, human AMCE = 0.1). Fitness-related judgments also evolved from initially contradicting human preferences (AMCE &#x02009;=&#x02009;&#x02212;0.2 in March 2023) to showing better alignment in later versions (AMCE = 0.1 in January 2024).</p><p>However, we observed persistent deviations from human preferences in certain dimensions. The latest versions maintained near-neutral age-related judgments (AMCE <inline-formula id="pone.0322776.e002"><alternatives><graphic xlink:href="pone.0322776.e002.jpg" id="pone.0322776.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02248;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) despite human preference for younger individuals (AMCE = 0.5), and showed consistently stronger preferences for pedestrians over passengers (AMCE = 0.6 vs. human 0.1). Species-related judgments demonstrated an interesting pattern where initial alignment with human values in earlier versions (AMCE = 0.7) weakened in later updates (AMCE = 0.2) while maintaining the same directional preference.</p></sec><sec id="sec009"><title>GPT-4 family.</title><p>Analysis of the GPT-4 family (<xref rid="pone.0322776.g001" ref-type="fig">Fig 1B</xref>) revealed remarkable consistency in moral judgments across different versions from March 2023 to January 2024. In contrast to GPT-3.5, model updates showed minimal impact on judgment patterns, with all versions maintaining similar preference strengths across ethical dimensions.</p><p>The GPT-4 family demonstrated strong alignment with human judgments in direction but often exceeded human preference magnitudes. Most notably, Species and No. Characters preferences showed near-deterministic values (AMCE &#x0003e;0.9) compared to human values (AMCE = 0.6), indicating substantially stronger preferences for saving humans over animals and larger groups over smaller ones. Similar amplification appeared in Gender (AMCE = 0.4 vs. human 0.1), Law (AMCE = 0.6 vs. human 0.3), and Relation to AV (AMCE = 0.5 vs. human 0.1) categories.</p><p>The only notable divergence appeared in the Fitness category, where GPT-4 models showed a slight preference for overweight individuals (AMCE &#x02009;=&#x02009;&#x02212;0.1) contrary to human preference for physically fit individuals (AMCE = 0.2). This deviation, however, was minimal compared to the strong alignment observed in other dimensions.</p></sec><sec id="sec010"><title>GPT-4o/o1 family.</title><p>Analysis of the GPT-4o family (<xref rid="pone.0322776.g001" ref-type="fig">Fig 1C</xref>) revealed a pattern of consistent moral judgments across models, with two notable exceptions: GPT-4o-mini and o1-mini showed distinctive characteristics. The family exhibited closest alignment with human preferences in Gender (AMCE = 0.1) and No. Characters (AMCE = 0.7) categories, matching human values within <inline-formula id="pone.0322776.e003"><alternatives><graphic xlink:href="pone.0322776.e003.jpg" id="pone.0322776.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000b1;</mml:mi><mml:mn>0.1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> AMCE.</p><p>Most GPT-4o models showed stronger ethical preferences than both humans and the GPT-4 family in several dimensions. In Relation to AV, all models except GPT-4o-mini demonstrated extreme pedestrian preference (AMCE &#x0003e;0.5 vs. human 0.1). Similarly, they showed amplified preferences for law-abiding individuals (AMCE = 0.8 vs. human 0.4) and humans over pets (AMCE = 0.9 vs. human 0.6). However, these models showed weaker age-related preferences (AMCE = 0.2 vs. human 0.5) and maintained slight preferences for overweight individuals in Fitness judgments (AMCE &#x02009;=&#x02009;&#x02212;0.1).</p><p>The o1 variants showed interesting divergences: while the standard o1 followed GPT-4o patterns, o1-mini demonstrated unique characteristics, including near-neutral Relation to AV preferences (AMCE = 0.1) but extremely strong preferences for larger groups (AMCE <inline-formula id="pone.0322776.e004"><alternatives><graphic xlink:href="pone.0322776.e004.jpg" id="pone.0322776.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02248;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> 1.0). This suggests that model size significantly influences moral judgment patterns within this family.</p></sec><sec id="sec011"><title>Claude family.</title><p>Analysis of the Claude family (<xref rid="pone.0322776.g001" ref-type="fig">Fig 1D</xref>) revealed a clear bifurcation in moral judgment patterns, forming two distinct groups with contrasting ethical preferences. The first group (Claude 3 Sonnet, Opus, and 3.5 Haiku) and second group (Claude 3 Haiku and Claude 3.5 Sonnet) showed markedly different alignment with human preferences.</p><p>The first group demonstrated significant divergence from human judgments in several dimensions. They showed inverse preferences in Relation to AV (AMCE &#x02009;=&#x02009;&#x02212;0.4), Fitness (AMCE <inline-formula id="pone.0322776.e005"><alternatives><graphic xlink:href="pone.0322776.e005.jpg" id="pone.0322776.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02248;</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>), and Social Status (AMCE &#x02009;=&#x02009;&#x02212;0.3) categories, contrasting with human values (AMCE = 0.1, 0.2, and 0.3 respectively). While they aligned with humans in Gender and Species preferences, their magnitude was notably stronger in both dimensions (Gender: AMCE = 0.7 vs. human 0.3; Species: AMCE = 0.9 vs. human 0.6).</p><p>The second group showed different patterns: Claude 3 Haiku and Claude 3.5 Sonnet exhibited excessive preferences in Relation to AV (AMCE = 0.5) and Law (AMCE = 0.7) categories compared to humans (AMCE = 0.1 and 0.4, respectively). Claude 3.5 Sonnet particularly showed strong preferences in Species and No. Characters dimensions (AMCE &#x0003e;0.8).</p><p>Model updates showed inconsistent effects: while the Sonnet 3 to 3.5 update improved human alignment, the Haiku 3 to 3.5 update increased divergence. Subsequent updates within Claude 3.5 Sonnet showed minimal changes in judgment patterns.</p></sec><sec id="sec012"><title>Gemini family.</title><p>Analysis of the Gemini family (<xref rid="pone.0322776.g001" ref-type="fig">Fig 1E</xref>) revealed significant evolution in moral judgments across model updates, particularly marked by changes in the Relation to AV dimension. We observed a clear transition from Gemini 1.0 Pro to the 1.5 series, with distinct changes in ethical preferences.</p><p>The most notable shift occurred in the Relation to AV category, where Gemini 1.0 Pro initially showed preference for passengers (AMCE &#x02009;=&#x02009;&#x02212;0.4), while the Gemini 1.5 Pro series shifted to strongly favor pedestrians (AMCE = 0.7 vs. human 0.1). The Gemini 1.5 Flash family showed a similar evolution: Flash Preview and 001 favored passengers (AMCE &#x02009;=&#x02009;&#x02212;0.7), but Flash 002 shifted to strong pedestrian preference (AMCE = 0.7).</p><p>Gender and Species preferences also showed distinctive patterns. Only Gemini 1.5 Pro 002 aligned closely with human gender preferences (AMCE = 0.2 vs. human 0.1), while earlier versions showed stronger female preference (AMCE &#x0003e;0.4). All versions showed strong preferences than humans for prioritizing humans over pets (AMCE &#x0003e;0.7) and consistently showed inverse fitness-related preferences (AMCE &#x02009;=&#x02009;&#x02212;0.3 vs. human 0.2).</p><p>Notably, compared to Google&#x02019;s previous PaLM 2 model, which contradicted human preferences in group size decisions (AMCE = &#x02013;0.3 vs. human 0.7), all Gemini versions maintained consistent alignment with human preferences in preserving larger groups (AMCE = 0.4&#x02013;0.8).</p></sec><sec id="sec013"><title>Llama family.</title><p>Analysis of the Llama family (<xref rid="pone.0322776.g001" ref-type="fig">Fig 1F</xref>) revealed that model size significantly influenced moral judgment patterns, with larger models showing more consistent alignment with human preferences. Most models demonstrated human-like preferences in the No. Characters category (AMCE = 0.2&#x02013;0.9), with Llama 3.1 8B being the notable exception (AMCE &#x02009;=&#x02009;&#x02212;0.3).</p><p>Ethical preferences varied substantially across different model sizes and versions. In gender-related decisions, smaller models (Llama 2 7B, 3.2 1B, 3.2 3B) showed male preference (AMCE &#x0003c;&#x02013;0.1) contrary to human judgments (AMCE = 0.1), while larger models aligned with human preferences. Species-based decisions showed similar size-dependent patterns: 70B models (Llama 3, 3.1, and 3.3) demonstrated close alignment with human judgments (AMCE = 0.5&#x02013;0.6), while smaller models (e.g., Llama 3.2 1B and 3B) showed inverse preferences (AMCE &#x0003c;&#x02013;0.1).</p><p>The 70B models showed relatively small variations in judgments between versions and demonstrated closer alignment with human preferences. For example, these models maintained consistent preferences in law compliance (AMCE = 0.5) which happened to align with human values (0.4), while other models, excluding 3 8B model, exhibited neutral preferences. This observation suggests that model size may influence the consistency of judgment patterns.</p></sec><sec id="sec014"><title>Gemma family.</title><p>Analysis of the Gemma family (<xref rid="pone.0322776.g001" ref-type="fig">Fig 1G</xref>) revealed a mix of consistent patterns and model-dependent variations in moral judgments. Across all model variations, we observed consistent trends in law compliance (AMCE <inline-formula id="pone.0322776.e006"><alternatives><graphic xlink:href="pone.0322776.e006.jpg" id="pone.0322776.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02248;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> 0) and Relation to AV preferences (AMCE &#x0003c;0), with the latter diverging from human judgments (human AMCE = 0.1).</p><p>Judgment patterns varied significantly across different model sizes and versions. In the No. Characters category, smaller Gemma 2 models (2B, 9B) showed inverse preferences (AMCE &#x02009;=&#x02009;&#x02212;0.4) compared to human values (AMCE = 0.7), while larger models aligned with human preferences. The Species category showed similar variation: the Gemma 2 series demonstrated human-aligned preferences (AMCE = 0.6), while other versions showed weak or neutral species preferences (AMCE &#x0003c;0). Social Status preferences varied notably, with only Gemma 7B and 1.1 7B matching human preferences (AMCE = 0.3), while other versions showed neutral or inverse preferences.</p><p>The variation in judgment patterns across different model sizes suggests that parameter count significantly influences moral reasoning capabilities, although this relationship appears more complex than in other model families.</p></sec><sec id="sec015"><title>Other large language models.</title><p>Analysis of other LLMs (<xref rid="pone.0322776.g001" ref-type="fig">Fig 1H</xref>) revealed distinct patterns of alignment and divergence from human moral preferences, with notable variations across model architectures. In the Gender category, most models except Vicuna showed substantially stronger female preference (AMCE &#x0003e;0.4) compared to human values (AMCE = 0.1).</p><p>Key differences emerged in utilitarian judgments: while Mistral and Command R+ aligned with humans in favoring larger groups, their preferences were markedly stronger (AMCE = 0.9 vs. human 0.6). Characteristic divergences appeared in the Relation to AV and Fitness categories, where most models except Vicuna 13B showed inverse preferences (Relation to AV: AMCE &#x0003c;0 vs. human 0.1; Fitness: AMCE &#x0003c;0 vs. human 0.2).</p><p>Vicuna models demonstrated unique patterns. While Vicuna 7B showed neutral judgments (AMCE <inline-formula id="pone.0322776.e007"><alternatives><graphic xlink:href="pone.0322776.e007.jpg" id="pone.0322776.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02248;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> 0) across multiple categories (Gender, Age, Species), Vicuna 13B exhibited distinctive preferences contrary to humans (Gender: AMCE = &#x02013;0.2 vs. human 0.1; Species: AMCE = &#x02013;0.2 vs. human 0.6). Similarly, Phi 3.5 models showed size-dependent variations, with the mini version demonstrating notably different preferences than the MoE version. These results suggest potential influences of model size on ethical judgments.</p></sec></sec><sec id="sec016"><title>Quantitative comparison with human moral judgments</title><p>To quantitatively evaluate the similarity between LLM and human moral judgments, we compared the Euclidean distances of AMCE values across nine categories for each model family (<xref rid="pone.0322776.g002" ref-type="fig">Fig 2</xref>; see <xref rid="pone.0322776.s003" ref-type="supplementary-material">S2 Table</xref> for exact numerical data). This analysis revealed differences between model families and changes in judgment distances from humans through model evolution.</p><fig position="float" id="pone.0322776.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0322776.g002</object-id><label>Fig 2</label><caption><title>Distances between LLMs and human moral judgments across model families.</title><p>Violin plots show the distribution of distances from human judgments for each model family, with individual models represented as points. Different colors indicate different model families. Model names are labeled.</p></caption><graphic xlink:href="pone.0322776.g002" position="float"/></fig><p>Proprietary models generally showed closer alignment with human judgments than open-source alternatives. The GPT-4 family achieved the closest alignment (minimum distance = 0.6 with January 2024 version), followed by GPT-3.5 (distance = 0.8&#x02013;0.9). However, the newer GPT-4o/o1 family showed slightly increased distances (0.9&#x02013;1.0), with o1-mini being an exception (distance = 0.7).</p><p>Model updates showed varying effects across different families. In the Claude family, while Claude 3 Haiku and Claude 3.5 Sonnet demonstrated relatively close alignment with human judgments (distances = 0.9&#x02013;1.0), Claude 3 Sonnet and Opus showed notably larger distances (1.2). The Gemini family similarly exhibited significant version-dependent variations, with Gemini 1.5 Preview (April 2024) showing the minimum distance of 0.9, while subsequent versions sometimes demonstrated larger distances of 1.1. Additionally, 1.5 Flash models showed greater distances compared to 1.5 Pro models.</p><p>Open-source model performance strongly correlated with model size. The Llama family showed clear size-dependent improvement, with 70B models achieving closer alignment (distance = 0.7&#x02013;0.8) compared to smaller 1&#x02013;8B models (distances = 1.2&#x02013;1.6). Conversely, recent Gemma 2 models showed larger distances (2B/9B: distance = 2.0) despite being newer, though performance improved with size. Among other LLMs, Mistral-Nemo and Command R+ showed relatively good alignment (distances = 0.8&#x02013;0.9), while Vicuna and Phi 3.5 demonstrated larger divergence (distances = 1.0&#x02013;1.6).</p></sec><sec id="sec017"><title>Factors influencing judgment distance</title><sec id="sec018"><title>Proprietary vs. open-source models.</title><p>To identify factors influencing the distance from human judgments, we first investigated whether the proprietary or open-source nature of models influenced their alignment with human moral judgments. This analysis was motivated by previous research suggesting general performance advantages of proprietary models over open-source alternatives [<xref rid="pone.0322776.ref051" ref-type="bibr">51</xref>].</p><p>Our analysis classified models into three distinct groups (<xref rid="pone.0322776.g003" ref-type="fig">Fig 3</xref>): proprietary models (e.g., GPT-4, Claude, Gemini), general open-source models, and large open-source models (&#x0003e;10B parameters). Statistical comparison revealed significant differences between proprietary and general open-source models, with proprietary models showing closer alignment to human judgments (median distance: 0.9 vs. 1.2, Wilcoxon test <italic toggle="yes">p</italic>&#x02009;=&#x02009;0.011).</p><fig position="float" id="pone.0322776.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0322776.g003</object-id><label>Fig 3</label><caption><title>Comparison of moral judgment distances between proprietary and open-source models.</title><p>Violin plots with embedded box plots compare the distribution of distances from human judgments across three model categories: proprietary models, all open-source models, and large open-source models with parameters exceeding 10B. Individual models are represented as points.</p></caption><graphic xlink:href="pone.0322776.g003" position="float"/></fig><p>However, when comparing proprietary models with large open-source models specifically, this difference disappeared. Large open-source models (&#x0003e;10B parameters) achieved similar alignment with human judgments (median distance = 0.9) as proprietary models, showing no statistically significant difference (Wilcoxon test <italic toggle="yes">p</italic>&#x02009;=&#x02009;0.92). Notably, models like Llama-70B achieved distances (0.7&#x02013;0.8) comparable to or better than some proprietary models.</p><p>These findings suggest that model size, rather than the proprietary/open-source distinction, is the primary factor influencing alignment with human moral judgments. This observation has important implications for the development of ethical AI systems, suggesting that large open-source models can potentially match the moral reasoning capabilities of proprietary alternatives.</p></sec><sec id="sec019"><title>Impact of model size.</title><p>We analyzed the relationship between model size and alignment with human moral judgments, focusing on open-source models where parameter counts are publicly available (<xref rid="pone.0322776.g004" ref-type="fig">Fig 4</xref>). Statistical analysis revealed a significant negative correlation between model size and distance from human judgments (Spearman&#x02019;s rank correlation coefficient <inline-formula id="pone.0322776.e008"><alternatives><graphic xlink:href="pone.0322776.e008.jpg" id="pone.0322776.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>0.54</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, <italic toggle="yes">p</italic>&#x02009;=&#x02009;0.0076), though this relationship showed notable complexities.</p><fig position="float" id="pone.0322776.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0322776.g004</object-id><label>Fig 4</label><caption><title>Relationship between model size and distance from human judgments in open-source models.</title><p>Different model families are represented by different colors. Model names are labeled. Horizontal axis (model size in billion parameters) are represented in logarithmic scale.</p></caption><graphic xlink:href="pone.0322776.g004" position="float"/></fig><p>The Llama family demonstrated the clearest size-dependent improvements in alignment. Large (70B) models achieved substantially better alignment (distance = 0.7&#x02013;0.8) compared to smaller models (1-3B parameters, distances = 1.2&#x02013;1.5). A similar pattern emerged in the Gemma family, where the 27B model showed improved alignment (distance = 1.1) compared to smaller 2&#x02013;9B models (distances = 1.2&#x02013;1.9). These improvements suggest that increased model capacity enables more nuanced moral judgments.</p><p>However, the relationship between size and alignment showed important nuances within model families. Within the Llama family, the 3.2 series showed unexpected variation, with the 1B model achieving better alignment (distance = 1.2) than the larger 3B model (distance = 1.5). Similar inconsistencies appeared among 8B models, where version 3 demonstrated notably better alignment (distance = 0.8) than the subsequent version 3.1 (distance = 1.6). The Gemma family showed analogous patterns, with some larger models demonstrating increased distances despite their greater parameter counts.</p><p>These variations suggest that while model size significantly influences moral judgment alignment, other factors such as architecture design and training methodology play crucial roles. The moderate correlation coefficient (<inline-formula id="pone.0322776.e009"><alternatives><graphic xlink:href="pone.0322776.e009.jpg" id="pone.0322776.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>0.54</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) quantitatively supports this observation.</p><p>While proprietary models were excluded from this analysis due to undisclosed parameter counts, their generally strong alignment with human judgments (median distance = 0.9) is consistent with the observed trend that larger models tend to demonstrate better alignment with human moral preferences.</p></sec><sec id="sec020"><title>Impact of model updates.</title><p>Analysis of model updates revealed that newer versions did not consistently improve alignment with human moral judgments, suggesting a complex relationship between model evolution and ethical reasoning capabilities. We examined this relationship across both open-source and proprietary model families, finding significant variations in the impact of updates.</p><p>In open-source models, updates often showed unpredictable effects on moral judgment alignment. Within the Gemma family, successive updates led to increasing divergence from human judgments: the 2B model showed progressive increase in distances (Gemma 2B: 1.1, Gemma 1.1 2B: 1.2, Gemma 2 2B: 2.0). The Llama family exhibited mixed effects: in 7-8B models, while the update from Llama 2 to 3 improved alignment (distance decreased from 1.1 to 0.8), the subsequent update to 3.1 significantly increased divergence (distance = 1.6). In the 70B series, while Llama 3 and 3.1 maintained strong alignment (distance = 0.7), the subsequent Llama 3.3 showed a slight increase in divergence (distance = 0.8). These results indicate that even for larger models, updates do not consistently improve alignment with human moral preferences.</p><p>Proprietary models showed more stable but still variable patterns across updates (<xref rid="pone.0322776.g005" ref-type="fig">Fig 5</xref>). The GPT-4 family maintained relatively consistent alignment (distances = 0.7&#x02013;0.8) from March 2023 to January 2024, with the January version achieving the best alignment (distance: 0.6) before slightly increasing in April (distance: 0.8). GPT-3.5 showed similar stability (distances = 0.7&#x02013;0.9), while the newer GPT-4o family demonstrated slightly larger distances (0.9&#x02013;1.0) compared to traditional GPT-4.</p><fig position="float" id="pone.0322776.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0322776.g005</object-id><label>Fig 5</label><caption><title>Temporal changes in distance from human judgments across proprietary model families.</title><p>Different model families are represented by different colors. Model names are labeled.</p></caption><graphic xlink:href="pone.0322776.g005" position="float"/></fig><p>The Claude and Gemini families exhibited more pronounced update effects. Claude Sonnet&#x02019;s update from 3 to 3.5 improved alignment (distance decreased from 1.2 to 0.9), but the October 2024 update slightly increased divergence (distance: 1.0). The Gemini family maintained relatively stable alignment through version 1.5 Pro Preview (distance: 0.9), but later versions showed increased distances (1.1), with Flash models consistently showing larger divergence than Pro models.</p><p>These patterns suggest that model updates, while often aimed at improving general capabilities, do not guarantee better alignment with human moral preferences. This observation raises important questions about the relationship between general model improvements and ethical reasoning capabilities.</p></sec></sec><sec id="sec021"><title>Comprehensive analysis of moral judgments</title><p>To synthesize our findings and visualize the overall landscape of moral judgment patterns, we conducted PCA using AMCE values across all nine ethical categories (<xref rid="pone.0322776.g006" ref-type="fig">Fig 6</xref>). This analysis captured 74.2% of the total variance in the first two principal components (PC1: 46.1%, PC2: 28.1%), providing a comprehensive two-dimensional representation of moral judgment similarities.</p><fig position="float" id="pone.0322776.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0322776.g006</object-id><label>Fig 6</label><caption><title>Principal component analysis of LLM moral judgments compared to human preferences.</title><p>The first two principal components derived from AMCE values across nine moral preference categories. Different model families are represented by different colors, and human judgment is marked with a black diamond.</p></caption><graphic xlink:href="pone.0322776.g006" position="float"/></fig><p>This analysis revealed several characteristic patterns. First, the GPT-4 family, GPT-4o/o1 family, Claude 3 Haiku, Claude 3.5 Sonnet, Gemini Pro 1.5 family, and GPT-3.5 family formed clusters relatively close to human judgments. Notably, these models concentrated in the positive region of the first principal component, suggesting commonalities in their judgment patterns.</p><p>Conversely, Claude 3 Sonnet and Opus, along with Gemini 1.5 Flash Pre and Flash 001, positioned in strongly negative regions of the first principal component, demonstrating judgment patterns distinct from humans and other proprietary models. The Gemma 2 family (2B, 9B) showed large positive values in the second principal component, confirming distinctively different judgment patterns from other models.</p><p>Interestingly, these results aligned with our previous distance analysis in multiple aspects. Models showing smaller distances from human judgments tended to position closer to humans in this two-dimensional plane. Similar trends were observed regarding model size effects. For instance, within the Llama family, 70B models (Llama 3, 3.1, and 3.3) positioned near humans, while smaller 1-8B models distributed farther away.</p><p>Changes due to model updates were also clearly observable in this two-dimensional plane. For example, GPT-3.5 approached human positions most closely in the June 2023 version but moved away with subsequent updates. Similarly, while Claude Sonnet moved closer to humans with the update from 3 to 3.5, it slightly diverged with the October version of 3.5. These trajectories support our previous finding that updates do not necessarily lead to closer alignment with human judgments.</p></sec></sec><sec sec-type="conclusions" id="sec022"><title>Discussion</title><p>In this study, we investigated two critical questions in AI ethics: how different LLM architectures approach moral decision-making in autonomous driving contexts, and how these moral judgments evolve through model updates. By expanding our analysis to 52 different LLMs using the Moral Machine experimental framework (<xref rid="pone.0322776.g001" ref-type="fig">Figs 1</xref> and <xref rid="pone.0322776.g002" ref-type="fig">2</xref>), we revealed several key patterns. Proprietary models and open-source models exceeding 10B parameters demonstrated relatively small distances from human judgments (<xref rid="pone.0322776.g003" ref-type="fig">Figs 3</xref> and <xref rid="pone.0322776.g004" ref-type="fig">4</xref>), showing consistent alignment with human preferences in fundamental moral decisions, such as prioritizing humans over animals and favoring the preservation of more lives. However, model updates did not necessarily lead to closer alignment with human judgments, and variations in judgment tendencies were observed across updates (<xref rid="pone.0322776.g005" ref-type="fig">Fig 5</xref>). This comprehensive analysis provides crucial insights into both the current state and evolution of moral reasoning capabilities in large language models.</p><p>Our analysis revealed a complex relationship between model characteristics and moral judgment capabilities. While the incomplete alignment between LLM and human moral judgments was expected, as these models are not explicitly trained for moral reasoning, we identified several significant patterns that inform our understanding of how different architectures approach ethical decision-making.</p><p>A key finding was the significant negative correlation between model size and distance from human judgments in open-source models (<xref rid="pone.0322776.g004" ref-type="fig">Fig 4</xref>). This relationship aligns with general scaling laws in language model behavior, suggesting that increased model capacity might naturally lead to more human-like moral reasoning. However, our cross-version analysis revealed that this relationship is not straightforward: within the same model family, size increases did not always result in improved alignment with human judgments, indicating that factors such as architecture design and training methodology play crucial roles.</p><p>These findings have important implications for the practical implementation of AI ethics in autonomous driving systems. The GPT-4 family achieved the closest alignment with human judgments (distances = 0.6&#x02013;0.8), while among open-source alternatives, Llama 70B demonstrated promising performance (distances = 0.7&#x02013;0.8). However, real-world deployment faces a crucial challenge: the trade-off between ethical reasoning quality and computational efficiency. Smaller models, which are more practical for real-time decision-making, showed notably larger divergence from human judgments (distances = 1.2&#x02013;1.5). This creates a significant challenge for implementing ethical AI in autonomous systems where both moral reliability and operational efficiency are essential.</p><p>Among these implementation challenges, the computational efficiency requirements are particularly critical in autonomous driving, where decisions must be made in milliseconds. While our study demonstrates that larger models generally make better ethical decisions, their computational requirements make them impractical for real-time deployment. This challenge necessitates innovative approaches such as model distillation [<xref rid="pone.0322776.ref052" ref-type="bibr">52</xref>] or specialized ethical reasoning modules that can maintain high-quality moral judgment while meeting the strict latency requirements of autonomous systems.</p><p>The identified trade-off between model size and ethical reasoning quality thus presents fundamental challenges for practical implementation. While recent research has produced LLMs specifically fine-tuned for autonomous driving tasks [<xref rid="pone.0322776.ref018" ref-type="bibr">18</xref>], these efforts have primarily focused on technical capabilities (perception, control, navigation) rather than ethical reasoning. Our findings suggest that developing practically viable AI systems requires addressing three key challenges: optimizing smaller models for better ethical reasoning, managing excessive ethical preferences, and maintaining cultural adaptability.</p><p>The tendency of LLMs to exhibit exaggerated ethical preferences represents a particular concern. Our analysis showed that while models often directionally aligned with human preferences (e.g., prioritizing humans over pets, pedestrians over passengers), they frequently demonstrated substantially stronger preferences than humans. While such strong safety preferences partially align with established guidelines, as exemplified by the German Ethics Code on Automated and Connected Driving [<xref rid="pone.0322776.ref053" ref-type="bibr">53</xref>], their excessive nature could lead to suboptimal decision-making in complex real-world scenarios. Specifically, while Guidelines 5 and 7 emphasize protecting vulnerable road users and preventing personal injury, the observed extreme preferences in LLMs suggest a need for more nuanced ethical calibration.</p><p>Our analysis revealed particularly significant findings regarding utilitarian decision-making in LLMs. The models showed extreme emphasis on maximizing lives saved, reflecting a strong utilitarian bias that exceeds typical human preferences. This bias likely stems from the predominance of Western individualistic and utilitarian values in training data [<xref rid="pone.0322776.ref011" ref-type="bibr">11</xref>,<xref rid="pone.0322776.ref054" ref-type="bibr">54</xref>], raising important questions about cultural representation in AI ethics. This observation becomes especially significant when considered against formal ethical frameworks like the German Ethics Code, whose Guideline 2 explicitly warns against purely utilitarian approaches in autonomous vehicle decision-making.</p><p>The analysis also uncovered complex patterns in how LLMs handle social biases and protective principles. We observed inverse patterns to human judgments in cases involving physical fitness and social status, where LLMs showed stronger preferences for protecting potentially vulnerable groups. Similarly, while LLMs aligned with human tendencies in gender-related decisions and legal compliance, they demonstrated significantly amplified preferences. These patterns suggest that LLMs may be extracting and amplifying certain ethical principles from their training data, such as historical protective principles (e.g., &#x02018;Women and Children First&#x0201d; principle [<xref rid="pone.0322776.ref055" ref-type="bibr">55</xref>]) and fundamental concepts of justice [<xref rid="pone.0322776.ref056" ref-type="bibr">56</xref>], but applying them with excessive rigidity.</p><p>Our findings highlight fundamental challenges for implementing ethical AI in autonomous driving systems. A critical issue emerged from the contrast between human moral flexibility and the rigid, often amplified moral preferences displayed by LLMs. This rigidity becomes particularly problematic when considered alongside recent findings [<xref rid="pone.0322776.ref057" ref-type="bibr">57</xref>] demonstrating that LLMs exhibit varying moral biases across different languages, suggesting inconsistent ethical reasoning even within individual models.</p><p>These observations raise two critical concerns for practical implementation. First, the potential for AI systems to influence human decision-making [] means that LLMs&#x02019; amplified moral preferences could disproportionately shape social attitudes toward ethical decisions in autonomous driving. Second, the observed variation in moral judgments across languages and cultures challenges the goal of developing globally deployable autonomous systems. These issues are particularly significant when considered alongside established ethical guidelines. For instance, the German Ethics Code&#x02019;s Guideline 9 emphasizes equal treatment regardless of demographic factors, but this principle appears challenging to implement given the observed biases in LLM decision-making.</p><p>Another critical consideration for practical deployment is the need for transparency and explainability in LLMs&#x02019; ethical decision-making processes. While our study quantifies the alignment between LLM and human moral preferences, the black-box nature of these models poses significant challenges for real-world implementation, particularly in life-critical decisions. Unlike rule-based systems, LLMs&#x02019; ethical judgments emerge from complex neural network interactions, making them difficult to interpret or verify. This lack of transparency is particularly problematic in autonomous driving scenarios where stakeholders need to understand and trust the basis for ethical decisions [<xref rid="pone.0322776.ref030" ref-type="bibr">30</xref>].</p><p>Our findings raise significant concerns about the cultural adaptability of AI ethics in autonomous systems. The observed tendency of LLMs to reflect predominantly Western values and apply ethical principles in extreme ways could significantly impact the global acceptance of autonomous driving systems. This challenge is compounded by our observation that LLMs&#x02019; moral judgments can diverge substantially from human preferences across different cultural and linguistic contexts, suggesting a need for more culturally adaptive approaches to ethical AI implementation.</p><p>These cultural adaptability challenges point to specific implementation considerations for autonomous driving systems. Regions with distinct driving cultures and ethical priorities may require different thresholds for risk assessment and decision-making parameters. This is particularly crucial in areas where different cultural zones intersect, such as international borders or multicultural urban centers, where autonomous vehicles must navigate varying expectations of driving behavior while maintaining consistent safety standards.</p><p>A key limitation of our study stems from our use of globally aggregated moral preferences as the benchmark for human judgments. Moral decisions, particularly those involving life-and-death choices, are deeply influenced by cultural and social contexts. For example, the strong utilitarian preferences we observed in LLMs might align with some cultural perspectives while conflicting with others. While our analysis demonstrates clear patterns in LLM moral reasoning, it does not fully capture the nuanced variations in human moral preferences across different cultural contexts.</p><p>Our methodology, based on the Moral Machine experiment framework, carries inherent limitations that affect the interpretation of our findings. While this framework enabled systematic comparison across models, it simplifies the complexity of real-world ethical decisions through binary choices and trolley-type dilemmas. As noted in [<xref rid="pone.0322776.ref034" ref-type="bibr">34</xref>], this approach cannot capture nuanced factors such as road users&#x02019; intentions or evaluate everyday moral decisions in low-risk traffic situations. Recent work has proposed more ecologically valid approaches, such as combining virtual reality-based traffic scenarios with psychological frameworks like the Agent-Deed-Consequences model [<xref rid="pone.0322776.ref058" ref-type="bibr">58</xref>].</p><p>The emergence of multimodal large language models (MLLMs) offers potential pathways for addressing these methodological limitations. Recent research has demonstrated MLLMs&#x02019; capability to integrate visual information with language understanding in autonomous driving contexts [<xref rid="pone.0322776.ref019" ref-type="bibr">19</xref>,<xref rid="pone.0322776.ref020" ref-type="bibr">20</xref>], suggesting possibilities for more comprehensive ethical evaluation frameworks. Unlike the text-based scenarios used in our study, MLLMs could enable assessment of ethical decision-making in more realistic, visually-rendered traffic scenarios, potentially offering deeper insights into AI moral reasoning in real-world contexts.</p><p>Despite these limitations, our comprehensive analysis of 52 different LLMs provides three key contributions to understanding AI ethics in autonomous systems. First, we established a clear relationship between model size and ethical judgment capabilities, offering insights for future model development. Second, our systematic characterization of judgment patterns across model families revealed both consistent trends and concerning biases in AI moral reasoning. Third, we identified specific implementation challenges that must be addressed for practical deployment of ethical AI in autonomous vehicles. These findings, combined with our assessment of current methodological constraints, provide crucial guidance for both the technical development and ethical design of future autonomous driving systems.</p></sec><sec id="sec023" sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="pone.0322776.s001" position="float" content-type="local-data"><label>S1 Fig</label><caption><title>Differences in AMCE values between LLMs and human preferences.</title><p>Heatmap showing the differences between LLM and human AMCE values for nine moral preference categories.</p><p>(PDF)</p></caption><media xlink:href="pone.0322776.s001.pdf"/></supplementary-material><supplementary-material id="pone.0322776.s002" position="float" content-type="local-data"><label>S1 Table</label><caption><title>AMCE values for nine moral preference categories across all analyzed LLMs.</title><p>(CSV)</p></caption><media xlink:href="pone.0322776.s002.csv"/></supplementary-material><supplementary-material id="pone.0322776.s003" position="float" content-type="local-data"><label>S2 Table</label><caption><title>Comprehensive model information and evaluation metrics.</title><p>Summary of all analyzed LLMs, including their characteristics and performance metrics. The &#x02018;open-source&#x02019; column indicates model accessibility (yes/no), &#x02018;Size&#x02019; shows the number of parameters in billions where available, &#x02018;Date&#x02019; indicates the model version&#x02019;s release date, &#x02018;Distance&#x02019; shows the Euclidean distance between the model&#x02019;s and human AMCE values across nine moral preference categories, and &#x02018;valid response rate&#x02019; represents the proportion of valid responses in the evaluation scenarios.</p><p>(CSV)</p></caption><media xlink:href="pone.0322776.s003.csv"/></supplementary-material></sec></body><back><ref-list><title>References</title><ref id="pone.0322776.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Fraiwan</surname><given-names>M</given-names></name>, <name><surname>Khasawneh</surname><given-names>N</given-names></name>. <article-title>A review of chatgpt applications in education, marketing, software engineering, and healthcare: Benefits, drawbacks, and research directions</article-title>. <source>arXiv preprint</source>. <year>2023</year>. <comment>doi: arXiv:230500237</comment></mixed-citation></ref><ref id="pone.0322776.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Sallam</surname><given-names>M</given-names></name>. <article-title>ChatGPT utility in healthcare education, research, and practice: Systematic review on the promising perspectives and valid concerns</article-title>. <source>Healthcare (Basel)</source>. <year>2023</year>;<volume>11</volume>(<issue>6</issue>):<fpage>887</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/healthcare11060887</pub-id>
<pub-id pub-id-type="pmid">36981544</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Ray</surname><given-names>PP</given-names></name>. <article-title>ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope</article-title>. <source>Internet Things Cyber-Phys Syst</source>. <year>2023</year>;<volume>3</volume>:<fpage>121</fpage>&#x02013;<lpage>54</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.iotcps.2023.04.003</pub-id></mixed-citation></ref><ref id="pone.0322776.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Chowdhury</surname><given-names>AK</given-names></name>, <name><surname>Sujon</surname><given-names>SR</given-names></name>, <name><surname>Shafi</surname><given-names>MdSS</given-names></name>, <name><surname>Ahmmad</surname><given-names>T</given-names></name>, <name><surname>Ahmed</surname><given-names>S</given-names></name>, <name><surname>Hasib</surname><given-names>KM</given-names></name>, <etal>et al</etal>. <article-title>Harnessing large language models over transformer models for detecting Bengali depressive social media text: A comprehensive study</article-title>. <source>Nat Lang Proc J</source>. <year>2024</year>;<volume>7</volume>:<fpage>100075</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.nlp.2024.100075</pub-id></mixed-citation></ref><ref id="pone.0322776.ref005"><label>5</label><mixed-citation publication-type="other">OpenAI. Introducing ChatGPT; 2022. OpenAI. Available from: <ext-link xlink:href="https://openai.com/index/chatgpt/" ext-link-type="uri">https://openai.com/index/chatgpt/</ext-link></mixed-citation></ref><ref id="pone.0322776.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Achiam</surname><given-names>J</given-names></name>, <name><surname>Adler</surname><given-names>S</given-names></name>, <name><surname>Agarwal</surname><given-names>S</given-names></name>, <name><surname>Ahmad</surname><given-names>L</given-names></name>, <name><surname>Akkaya</surname><given-names>I</given-names></name>, <name><surname>Aleman</surname><given-names>FL</given-names></name>, <etal>et al</etal>. <article-title>GPT-4 technical report</article-title>. <source>arXiv preprint arXiv</source>:230308774. <year>2023</year>.</mixed-citation></ref><ref id="pone.0322776.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Bostrom</surname><given-names>N</given-names></name>, <name><surname>Yudkowsky</surname><given-names>E</given-names></name>. <article-title>The ethics of artificial intelligence</article-title>. <source>Artif Intell Saf Secur</source>. <year>2018</year>:<fpage>57</fpage>&#x02013;<lpage>69</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1201/9781351251389-4</pub-id></mixed-citation></ref><ref id="pone.0322776.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Nath</surname><given-names>R</given-names></name>, <name><surname>Sahu</surname><given-names>V</given-names></name>. <article-title>The problem of machine ethics in artificial intelligence</article-title>. <source>AI Soc</source>. <year>2017</year>;<volume>35</volume>(<issue>1</issue>):<fpage>103</fpage>&#x02013;<lpage>11</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00146-017-0768-6</pub-id></mixed-citation></ref><ref id="pone.0322776.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Hagendorff</surname><given-names>T</given-names></name>. <article-title>The ethics of AI ethics: An evaluation of guidelines</article-title>. <source>Minds Mach</source>. <year>2020</year>;<volume>30</volume>(<issue>1</issue>):<fpage>99</fpage>&#x02013;<lpage>120</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11023-020-09517-8</pub-id></mixed-citation></ref><ref id="pone.0322776.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Bonnefon</surname><given-names>J-F</given-names></name>, <name><surname>Shariff</surname><given-names>A</given-names></name>, <name><surname>Rahwan</surname><given-names>I</given-names></name>. <article-title>The social dilemma of autonomous vehicles</article-title>. <source>Science</source>. <year>2016</year>;<volume>352</volume>(<issue>6293</issue>):<fpage>1573</fpage>&#x02013;<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.aaf2654</pub-id>
<pub-id pub-id-type="pmid">27339987</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Awad</surname><given-names>E</given-names></name>, <name><surname>Dsouza</surname><given-names>S</given-names></name>, <name><surname>Kim</surname><given-names>R</given-names></name>, <name><surname>Schulz</surname><given-names>J</given-names></name>, <name><surname>Henrich</surname><given-names>J</given-names></name>, <name><surname>Shariff</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>The moral machine experiment</article-title>. <source>Nature</source>. <year>2018</year>;<volume>563</volume>(7729):<fpage>59</fpage>&#x02013;<lpage>64</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41586-018-0637-6</pub-id>
<pub-id pub-id-type="pmid">30356211</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Faulhaber</surname><given-names>AK</given-names></name>, <name><surname>Dittmer</surname><given-names>A</given-names></name>, <name><surname>Blind</surname><given-names>F</given-names></name>, <name><surname>W&#x000e4;chter</surname><given-names>MA</given-names></name>, <name><surname>Timm</surname><given-names>S</given-names></name>, <name><surname>S&#x000fc;tfeld</surname><given-names>LR</given-names></name>, <etal>et al</etal>. <article-title>Human decisions in moral dilemmas are largely described by utilitarianism: Virtual car driving study provides guidelines for autonomous driving vehicles</article-title>. <source>Sci Eng Ethics</source>. <year>2019</year>;<volume>25</volume>(<issue>2</issue>):<fpage>399</fpage>&#x02013;<lpage>418</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11948-018-0020-x</pub-id>
<pub-id pub-id-type="pmid">29357047</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Gill</surname><given-names>T</given-names></name>. <article-title>Ethical dilemmas are really important to potential adopters of autonomous vehicles</article-title>. <source>Ethics Inf Technol</source>. <year>2021</year>;<volume>23</volume>(<issue>4</issue>):<fpage>657</fpage>&#x02013;<lpage>73</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10676-021-09605-y</pub-id>
<pub-id pub-id-type="pmid">34248401</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>H</given-names></name>, <name><surname>Yuan</surname><given-names>K</given-names></name>, <name><surname>Huang</surname><given-names>Y</given-names></name>, <name><surname>Guo</surname><given-names>L</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Chen</surname><given-names>J</given-names></name>. <article-title>Feedback is all you need: From ChatGPT to autonomous driving</article-title>. <source>Sci China Inf Sci</source>. <year>2023</year>;<volume>66</volume>(<issue>6</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11432-023-3740-x</pub-id></mixed-citation></ref><ref id="pone.0322776.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Gao</surname><given-names>Y</given-names></name>, <name><surname>Tong</surname><given-names>W</given-names></name>, <name><surname>Wu</surname><given-names>EQ</given-names></name>, <name><surname>Chen</surname><given-names>W</given-names></name>, <name><surname>Zhu</surname><given-names>G</given-names></name>, <name><surname>Wang</surname><given-names>F-Y</given-names></name>. <article-title>Chat with ChatGPT on interactive engines for intelligent driving</article-title>. <source>IEEE Trans Intell Veh</source>. <year>2023</year>;<volume>8</volume>(<issue>3</issue>):<fpage>2034</fpage>&#x02013;<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tiv.2023.3252571</pub-id></mixed-citation></ref><ref id="pone.0322776.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Du</surname><given-names>H</given-names></name>, <name><surname>Teng</surname><given-names>S</given-names></name>, <name><surname>Chen</surname><given-names>H</given-names></name>, <name><surname>Ma</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Gou</surname><given-names>C</given-names></name>. <article-title>Chat with chatgpt on intelligent vehicles: An IEEE TIV perspective</article-title>. <source>IEEE Trans Intell Veh</source>. <year>2023</year>;<volume>8</volume>(<issue>3</issue>):<fpage>2020</fpage>&#x02013;<lpage>6</lpage>.</mixed-citation></ref><ref id="pone.0322776.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Lei</surname><given-names>L</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Yang</surname><given-names>S</given-names></name>. <article-title>ChatGPT in connected and autonomous vehicles: Benefits and challenges</article-title>. <source>Intell Robot</source>. <year>2023</year>;<volume>3</volume>(<issue>2</issue>):<fpage>145</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0322776.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>Z</given-names></name>, <name><surname>Jia</surname><given-names>X</given-names></name>, <name><surname>Li</surname><given-names>H</given-names></name>, <name><surname>Yan</surname><given-names>J</given-names></name>. <article-title>Llm4drive: A survey of large language models for autonomous driving</article-title>. <source>arXiv e-prints</source>. <year>2023</year>:arXiv-2311.</mixed-citation></ref><ref id="pone.0322776.ref019"><label>19</label><mixed-citation publication-type="confproc"><name><surname>Cui</surname><given-names>C</given-names></name>, <name><surname>Ma</surname><given-names>Y</given-names></name>, <name><surname>Cao</surname><given-names>X</given-names></name>, <name><surname>Ye</surname><given-names>W</given-names></name>, <name><surname>Zhou</surname><given-names>Y</given-names></name>, <name><surname>Liang</surname><given-names>K</given-names></name>. <article-title>A survey on multimodal large language models for autonomous driving</article-title>. <conf-name>Proceedings of the IEEE/CVF winter conference on applications of computer vision</conf-name>; <year>2024</year>. p. 958&#x02013;<lpage>79</lpage>.</mixed-citation></ref><ref id="pone.0322776.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>X</given-names></name>, <name><surname>Liu</surname><given-names>M</given-names></name>, <name><surname>Yurtsever</surname><given-names>E</given-names></name>, <name><surname>Zagar</surname><given-names>B</given-names></name>, <name><surname>Zimmer</surname><given-names>W</given-names></name>, <name><surname>Cao</surname><given-names>H</given-names></name>. <article-title>Vision language models in autonomous driving: A survey and outlook</article-title>. <source>IEEE Trans Intell Veh</source>. <year>2024</year>.</mixed-citation></ref><ref id="pone.0322776.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Dewitt</surname><given-names>B</given-names></name>, <name><surname>Fischhoff</surname><given-names>B</given-names></name>, <name><surname>Sahlin</surname><given-names>N-E</given-names></name>. <article-title>&#x0201c;Moral machine&#x0201d; experiment is no basis for policymaking</article-title>. <source>Nature</source>. <year>2019</year>;<volume>567</volume>(<issue>7746</issue>):<fpage>31</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/d41586-019-00766-x</pub-id>
<pub-id pub-id-type="pmid">30837734</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Bigman</surname><given-names>YE</given-names></name>, <name><surname>Gray</surname><given-names>K</given-names></name>. <article-title>Life and death decisions of autonomous vehicles</article-title>. <source>Nature</source>. <year>2020</year>;<volume>579</volume>(<issue>7797</issue>):E1&#x02013;<lpage>2</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41586-020-1987-4</pub-id>
<pub-id pub-id-type="pmid">32132695</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Etienne</surname><given-names>H</given-names></name>. <article-title>The dark side of the &#x02018;Moral Machine&#x02019; and the fallacy of computational ethical decision-making for autonomous vehicles</article-title>. <source>Law Innov Technol</source>. <year>2021</year>;<volume>13</volume>(<issue>1</issue>):<fpage>85</fpage>&#x02013;<lpage>107</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/17579961.2021.1898310</pub-id></mixed-citation></ref><ref id="pone.0322776.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Furey</surname><given-names>H</given-names></name>, <name><surname>Hill</surname><given-names>S</given-names></name>. <article-title>MIT&#x02019;s moral machine project is a psychological roadblock to self-driving cars</article-title>. <source>AI Ethics</source>. <year>2020</year>;<volume>1</volume>(<issue>2</issue>):<fpage>151</fpage>&#x02013;<lpage>5</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s43681-020-00018-z</pub-id></mixed-citation></ref><ref id="pone.0322776.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>LaCroix</surname><given-names>T</given-names></name>. <article-title>Moral dilemmas for moral machines</article-title>. <source>AI Ethics</source>. <year>2022</year>;<volume>2</volume>(<issue>4</issue>):<fpage>737</fpage>&#x02013;<lpage>46</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s43681-022-00134-y</pub-id></mixed-citation></ref><ref id="pone.0322776.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Schuessler</surname><given-names>D</given-names></name>. <article-title>The probability problems of the Moral Machine Experiment</article-title>. <source>AI Ethics.</source>
<year>2023</year>;<volume>4</volume>(<issue>2</issue>):<fpage>501</fpage>&#x02013;<lpage>10</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s43681-023-00287-4</pub-id></mixed-citation></ref><ref id="pone.0322776.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Winfield</surname><given-names>AF</given-names></name>, <name><surname>Michael</surname><given-names>K</given-names></name>, <name><surname>Pitt</surname><given-names>J</given-names></name>, <name><surname>Evers</surname><given-names>V</given-names></name>. <article-title>Machine ethics: The design and governance of ethical AI and autonomous systems [scanning the issue]</article-title>. <source>Proc IEEE</source>. <year>2019</year>;<volume>107</volume>(<issue>3</issue>):<fpage>509</fpage>&#x02013;<lpage>17</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jproc.2019.2900622</pub-id></mixed-citation></ref><ref id="pone.0322776.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Etienne</surname><given-names>H</given-names></name>. <article-title>When AI ethics goes astray: A case study of autonomous vehicles</article-title>. <source>Soc Sci Comput Rev</source>. <year>2020</year>;<volume>40</volume>(<issue>1</issue>):<fpage>236</fpage>&#x02013;<lpage>46</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/0894439320906508</pub-id></mixed-citation></ref><ref id="pone.0322776.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Meyer-Waarden</surname><given-names>L</given-names></name>, <name><surname>Cloarec</surname><given-names>J</given-names></name>. <article-title>&#x0201c;Baby, you can drive my car&#x0201d;: Psychological antecedents that drive consumers&#x02019; adoption of AI-powered autonomous vehicles</article-title>. <source>Technovation</source>. <year>2022</year>;<volume>109</volume>:102348. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.technovation.2021.102348</pub-id></mixed-citation></ref><ref id="pone.0322776.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Atakishiyev</surname><given-names>S</given-names></name>, <name><surname>Salameh</surname><given-names>M</given-names></name>, <name><surname>Yao</surname><given-names>H</given-names></name>, <name><surname>Goebel</surname><given-names>R</given-names></name>. <article-title>Explainable artificial intelligence for autonomous driving: A comprehensive overview and field guide for future research directions</article-title>. <source>IEEE Access</source>. <year>2024</year>.</mixed-citation></ref><ref id="pone.0322776.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Kr&#x000fc;gel</surname><given-names>S</given-names></name>, <name><surname>Ostermaier</surname><given-names>A</given-names></name>, <name><surname>Uhl</surname><given-names>M</given-names></name>. <article-title>ChatGPT&#x02019;s inconsistent moral advice influences users&#x02019; judgment</article-title>. <source>Sci Rep</source>. <year>2023</year>;<volume>13</volume>(<issue>1</issue>):4569. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-023-31341-0</pub-id>
<pub-id pub-id-type="pmid">37024502</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Jin</surname><given-names>Z</given-names></name>, <name><surname>Levine</surname><given-names>S</given-names></name>, <name><surname>Kleiman-Weiner</surname><given-names>M</given-names></name>, <name><surname>Piatti</surname><given-names>G</given-names></name>, <name><surname>Liu</surname><given-names>J</given-names></name>, <name><surname>Adauto</surname><given-names>FG</given-names></name>, <etal>et al</etal>. <article-title>Multilingual trolley problems for language models</article-title>. <source>arXiv preprint arXiv</source>:240702273. <year>2024</year>.</mixed-citation></ref><ref id="pone.0322776.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Bruers</surname><given-names>S</given-names></name>, <name><surname>Braeckman</surname><given-names>J</given-names></name>. <article-title>A review and systematization of the trolley problem</article-title>. <source>Philosophia</source>. <year>2013</year>;<volume>42</volume>(<issue>2</issue>):<fpage>251</fpage>&#x02013;<lpage>69</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11406-013-9507-5</pub-id></mixed-citation></ref><ref id="pone.0322776.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Takemoto</surname><given-names>K</given-names></name>. <article-title>The moral machine experiment on large language models</article-title>. <source>R Soc Open Sci</source>. <year>2024</year>;<volume>11</volume>(<issue>2</issue>):231393. <comment>doi: </comment><pub-id pub-id-type="doi">10.1098/rsos.231393</pub-id>
<pub-id pub-id-type="pmid">38328569</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref035"><label>35</label><mixed-citation publication-type="other">Anthropic A. Claude 3 model card. Claude 3 model family: Opus, sonnet, haiku. 2024;1:1.</mixed-citation></ref><ref id="pone.0322776.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Team</surname><given-names>G</given-names></name>, <name><surname>Anil</surname><given-names>R</given-names></name>, <name><surname>Borgeaud</surname><given-names>S</given-names></name>, <name><surname>Wu</surname><given-names>Y</given-names></name>, <name><surname>Alayrac</surname><given-names>JB</given-names></name>, <name><surname>Yu</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Gemini: A family of highly capable multimodal models</article-title>. <source>arXiv preprint arXiv</source>:231211805. <year>2023</year>.</mixed-citation></ref><ref id="pone.0322776.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Dubey</surname><given-names>A</given-names></name>, <name><surname>Jauhri</surname><given-names>A</given-names></name>, <name><surname>Pandey</surname><given-names>A</given-names></name>, <name><surname>Kadian</surname><given-names>A</given-names></name>, <name><surname>Al-Dahle</surname><given-names>A</given-names></name>, <name><surname>Letman</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>The llama 3 herd of models</article-title>. <source>arXiv preprint arXiv</source>:240721783. <year>2024</year>.</mixed-citation></ref><ref id="pone.0322776.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Team</surname><given-names>G</given-names></name>, <name><surname>Mesnard</surname><given-names>T</given-names></name>, <name><surname>Hardin</surname><given-names>C</given-names></name>, <name><surname>Dadashi</surname><given-names>R</given-names></name>, <name><surname>Bhupatiraju</surname><given-names>S</given-names></name>, <name><surname>Pathak</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Gemma: Open models based on gemini research and technology</article-title>. <source>arXiv preprint arXiv</source>:240308295. <year>2024</year>.</mixed-citation></ref><ref id="pone.0322776.ref039"><label>39</label><mixed-citation publication-type="other">OpenAI. Hello GPT-4o; 2024. OpenAI. Available from: <ext-link xlink:href="https://openai.com/index/hello-gpt-4o/" ext-link-type="uri">https://openai.com/index/hello-gpt-4o/</ext-link></mixed-citation></ref><ref id="pone.0322776.ref040"><label>40</label><mixed-citation publication-type="other">OpenAI. Introducing OpenAI o1; 2024. OpenAI. Available from: <ext-link xlink:href="https://openai.com/o1/" ext-link-type="uri">https://openai.com/o1/</ext-link></mixed-citation></ref><ref id="pone.0322776.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Reid</surname><given-names>M</given-names></name>, <name><surname>Savinov</surname><given-names>N</given-names></name>, <name><surname>Teplyashin</surname><given-names>D</given-names></name>, <name><surname>Lepikhin</surname><given-names>D</given-names></name>, <name><surname>Lillicrap</surname><given-names>T</given-names></name>, <name><surname>Alayrac</surname><given-names>Jb</given-names></name>, <etal>et al</etal>. <article-title>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</article-title>. <source>arXiv preprint arXiv</source>:240305530. <year>2024</year>.</mixed-citation></ref><ref id="pone.0322776.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Anthropic</surname><given-names>A</given-names></name>. <article-title>Claude 3.5 sonnet model card addendum</article-title>. <source>Claude-35 Model Card</source>. <year>2024</year>.</mixed-citation></ref><ref id="pone.0322776.ref043"><label>43</label><mixed-citation publication-type="journal"><name><surname>Team</surname><given-names>G</given-names></name>, <name><surname>Riviere</surname><given-names>M</given-names></name>, <name><surname>Pathak</surname><given-names>S</given-names></name>, <name><surname>Sessa</surname><given-names>PG</given-names></name>, <name><surname>Hardin</surname><given-names>C</given-names></name>, <name><surname>Bhupatiraju</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Gemma 2: Improving open language models at a practical size</article-title>. <source>arXiv preprint arXiv</source>:240800118. <year>2024</year>.</mixed-citation></ref><ref id="pone.0322776.ref044"><label>44</label><mixed-citation publication-type="journal"><name><surname>Radhakrishnan</surname><given-names>P</given-names></name>, <name><surname>Chen</surname><given-names>J</given-names></name>, <name><surname>Xu</surname><given-names>B</given-names></name>, <name><surname>Ramaswami</surname><given-names>P</given-names></name>, <name><surname>Pho</surname><given-names>H</given-names></name>, <name><surname>Olmos</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Knowing when to ask&#x02013;bridging large language models and data</article-title>. <source>arXiv preprint arXiv</source>:240913741. <year>2024</year>.</mixed-citation></ref><ref id="pone.0322776.ref045"><label>45</label><mixed-citation publication-type="other">Chiang WL, Li Z, Lin Z, Sheng Y, Wu Z, Zhang H, et al. Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality; 2023. Available from: <ext-link xlink:href="https://lmsys.org/blog/2023-03-30-vicuna/" ext-link-type="uri">https://lmsys.org/blog/2023-03-30-vicuna/</ext-link></mixed-citation></ref><ref id="pone.0322776.ref046"><label>46</label><mixed-citation publication-type="journal"><name><surname>Jiang</surname><given-names>AQ</given-names></name>, <name><surname>Sablayrolles</surname><given-names>A</given-names></name>, <name><surname>Mensch</surname><given-names>A</given-names></name>, <name><surname>Bamford</surname><given-names>C</given-names></name>, <name><surname>Chaplot</surname><given-names>DS</given-names></name>, <name><surname>Casas</surname><given-names>Ddl</given-names></name>, <etal>et al</etal>. <article-title>Mistral 7B</article-title>. <source>arXiv preprint arXiv</source>:231006825. <year>2023</year>.</mixed-citation></ref><ref id="pone.0322776.ref047"><label>47</label><mixed-citation publication-type="other">MistralAIteam. Mistral NeMo; 2024. Mistral AI. Available from: <ext-link xlink:href="https://mistral.ai/news/mistral-nemo/" ext-link-type="uri">https://mistral.ai/news/mistral-nemo/</ext-link></mixed-citation></ref><ref id="pone.0322776.ref048"><label>48</label><mixed-citation publication-type="other">Gomez A. Introducing command R+: A scalable LLM built for business; 2024. The Cohere Blog. Available from: <ext-link xlink:href="https://cohere.com/blog/command-r-plus-microsoft-azure" ext-link-type="uri">https://cohere.com/blog/command-r-plus-microsoft-azure</ext-link></mixed-citation></ref><ref id="pone.0322776.ref049"><label>49</label><mixed-citation publication-type="journal"><name><surname>Abdin</surname><given-names>M</given-names></name>, <name><surname>Jacobs</surname><given-names>SA</given-names></name>, <name><surname>Awan</surname><given-names>AA</given-names></name>, <name><surname>Aneja</surname><given-names>J</given-names></name>, <name><surname>Awadallah</surname><given-names>A</given-names></name>, <name><surname>Awadalla</surname><given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Phi-3 technical report: A highly capable language model locally on your phone</article-title>. <source>arXiv preprint arXiv</source>:240414219. <year>2024</year>.</mixed-citation></ref><ref id="pone.0322776.ref050"><label>50</label><mixed-citation publication-type="journal"><name><surname>Hainmueller</surname><given-names>J</given-names></name>, <name><surname>Hopkins</surname><given-names>DJ</given-names></name>, <name><surname>Yamamoto</surname><given-names>T</given-names></name>. <article-title>Causal inference in conjoint analysis: Understanding multidimensional choices via stated preference experiments</article-title>. <source>Polit Anal</source>. <year>2014</year>;<volume>22</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>30</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/pan/mpt024</pub-id></mixed-citation></ref><ref id="pone.0322776.ref051"><label>51</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>G</given-names></name>, <name><surname>Jin</surname><given-names>Q</given-names></name>, <name><surname>Zhou</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>S</given-names></name>, <name><surname>Idnay</surname><given-names>B</given-names></name>, <name><surname>Luo</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Closing the gap between open source and commercial large language models for medical evidence summarization</article-title>. <source>NPJ Digit Med</source>. <year>2024</year>;<volume>7</volume>(<issue>1</issue>):<fpage>239</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41746-024-01239-w</pub-id>
<pub-id pub-id-type="pmid">39251804</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref052"><label>52</label><mixed-citation publication-type="confproc"><name><surname>Tang</surname><given-names>Y</given-names></name>, <name><surname>Da Costa</surname><given-names>AAB</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Patrick</surname><given-names>I</given-names></name>, <name><surname>Khastgir</surname><given-names>S</given-names></name>, <name><surname>Jennings</surname><given-names>P</given-names></name>. <article-title>Domain knowledge distillation from large language model: An empirical study in the autonomous driving domain</article-title>. <conf-name>2023 IEEE 26th international conference on intelligent transportation systems (ITSC)</conf-name>; <year>2023</year>. p. <fpage>3893</fpage>&#x02013;<lpage>900</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/itsc57777.2023.10422308</pub-id></mixed-citation></ref><ref id="pone.0322776.ref053"><label>53</label><mixed-citation publication-type="journal"><name><surname>Luetge</surname><given-names>C</given-names></name>. <article-title>The German ethics code for automated and connected driving</article-title>. <source>Philos Technol</source>. <year>2017</year>;<volume>30</volume>(<issue>4</issue>):<fpage>547</fpage>&#x02013;<lpage>58</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s13347-017-0284-0</pub-id></mixed-citation></ref><ref id="pone.0322776.ref054"><label>54</label><mixed-citation publication-type="journal"><name><surname>Kagitcibasi</surname><given-names>C</given-names></name>. <article-title>Individualism and collectivism</article-title>. <source>Handbook of cross-cultural psychology</source>. <year>1997</year>;<volume>3</volume>:<fpage>1</fpage>&#x02013;<lpage>49</lpage>.</mixed-citation></ref><ref id="pone.0322776.ref055"><label>55</label><mixed-citation publication-type="journal"><name><surname>Annas</surname><given-names>GJ</given-names></name>. <article-title>Women and children first</article-title>. <source>N Engl J Med</source>. <year>1995</year>;<volume>333</volume>(<issue>24</issue>):<fpage>1647</fpage>&#x02013;<lpage>51</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1056/NEJM199512143332420</pub-id>
<pub-id pub-id-type="pmid">7477216</pub-id>
</mixed-citation></ref><ref id="pone.0322776.ref056"><label>56</label><mixed-citation publication-type="other">Sandel MJ. Justice: What&#x02019;s the right thing to do? Farrar, Straus and Giroux; 2010. Available from: <ext-link xlink:href="https://books.google.co.jp/books?id=BrdNDG7TTUEC" ext-link-type="uri">https://books.google.co.jp/books?id=BrdNDG7TTUEC</ext-link></mixed-citation></ref><ref id="pone.0322776.ref057"><label>57</label><mixed-citation publication-type="journal"><name><surname>Vida</surname><given-names>K</given-names></name>, <name><surname>Damken</surname><given-names>F</given-names></name>, <name><surname>Lauscher</surname><given-names>A</given-names></name>. <article-title>Decoding multilingual moral preferences: Unveiling LLM&#x02019;s biases through the moral machine experiment. AIES</article-title>. <year>2024</year>;<volume>7</volume>:<fpage>1490</fpage>&#x02013;<lpage>501</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1609/aies.v7i1.31741</pub-id></mixed-citation></ref><ref id="pone.0322776.ref058"><label>58</label><mixed-citation publication-type="journal"><name><surname>Cecchini</surname><given-names>D</given-names></name>, <name><surname>Brantley</surname><given-names>S</given-names></name>, <name><surname>Dubljevi&#x00107;</surname><given-names>V</given-names></name>. <article-title>Moral judgment in realistic traffic scenarios: Moving beyond the trolley paradigm for ethics of autonomous vehicles</article-title>. <source>AI &#x00026; SOCIETY</source>; <year>2023</year>. p. <fpage>1</fpage>&#x02013;<lpage>12</lpage>.</mixed-citation></ref></ref-list></back></article>