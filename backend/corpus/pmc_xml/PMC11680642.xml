<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Psychon Bull Rev</journal-id><journal-id journal-id-type="iso-abbrev">Psychon Bull Rev</journal-id><journal-title-group><journal-title>Psychonomic Bulletin &#x00026; Review</journal-title></journal-title-group><issn pub-type="ppub">1069-9384</issn><issn pub-type="epub">1531-5320</issn><publisher><publisher-name>Springer US</publisher-name><publisher-loc>New York</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">38689187</article-id><article-id pub-id-type="pmc">PMC11680642</article-id>
<article-id pub-id-type="publisher-id">2508</article-id><article-id pub-id-type="doi">10.3758/s13423-024-02508-1</article-id><article-categories><subj-group subj-group-type="heading"><subject>Brief Report</subject></subj-group></article-categories><title-group><article-title>Contextual facilitation: Separable roles of contextual guidance and context suppression in visual search</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3090-6183</contrib-id><name><surname>Chen</surname><given-names>Siyi</given-names></name><address><email>Siyi.Chen@psy.lmu.de</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>M&#x000fc;ller</surname><given-names>Hermann J.</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Shi</surname><given-names>Zhuanghua</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00bxsm637</institution-id><institution-id institution-id-type="GRID">grid.7324.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 0643 3659</institution-id><institution>Allgemeine und Experimentelle Psychologie, Department Psychologie, </institution><institution>LMU M&#x000fc;nchen, </institution></institution-wrap>Leopoldstr. 13, D-80802 Munich, Germany </aff></contrib-group><pub-date pub-type="epub"><day>30</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>30</day><month>4</month><year>2024</year></pub-date><pub-date pub-type="ppub"><year>2024</year></pub-date><volume>31</volume><issue>6</issue><fpage>2672</fpage><lpage>2680</lpage><history><date date-type="accepted"><day>2</day><month>4</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Visual search is facilitated when targets are repeatedly encountered at a fixed position relative to an invariant distractor layout, compared to random distractor arrangements. However, standard investigations of this <italic>contextual-facilitation</italic> effect employ fixed distractor layouts that predict a constant target location, which does not always reflect real-world situations where the target location may vary relative to an invariant distractor arrangement. To explore the mechanisms involved in contextual learning, we employed a training-test procedure, introducing not only the standard full-repeated displays with fixed target-distractor locations but also distractor-repeated displays in which the distractor arrangement remained unchanged but the target locations varied. During the training phase, participants encountered three types of display: full-repeated, distractor-repeated, and random arrangements. The results revealed full-repeated displays to engender larger performance gains than distractor-repeated displays, relative to the random-display baseline. In the test phase, the gains were substantially reduced when full-repeated displays changed into distractor-repeated displays, while the transition from distractor-repeated to full-repeated displays failed to yield additional gains. We take this pattern to indicate that contextual learning can improve performance with both predictive and non-predictive (repeated) contexts, employing distinct mechanisms: contextual guidance and context suppression, respectively. We consider how these mechanisms might be implemented (neuro-)computationally.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Contextual cueing</kwd><kwd>Visual search</kwd><kwd>Context-based guidance</kwd><kwd>Context suppression</kwd></kwd-group><funding-group><award-group><funding-source><institution>Ludwig-Maximilians-Universit&#x000e4;t M&#x000fc;nchen (1024)</institution></funding-source></award-group><open-access><p>Open Access funding enabled and organized by Projekt DEAL.</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Psychonomic Society, Inc. 2024</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Goal-related targets in our everyday life do not appear in isolation, they coexist with multiple non-target items in a scene. When the target&#x02019;s location remains stable relative to its surroundings, our ability to find it improves, owing to the learning of the invariant spatial relations &#x02013; known as &#x0201c;contextual cueing&#x0201d; (e.g., Chun &#x00026; Jiang, <xref ref-type="bibr" rid="CR6">1998</xref>; Geyer et al., <xref ref-type="bibr" rid="CR9">2010</xref>). In a typical contextual-cueing study (Chun &#x00026; Jiang, <xref ref-type="bibr" rid="CR6">1998</xref>), observers search for a target &#x0201c;T&#x0201d; amongst distractor &#x0201c;Ls&#x0201d; &#x02013; a task that offers minimal feature-based search guidance (Moran et al., <xref ref-type="bibr" rid="CR19">2016</xref>). Critically, unbeknown to observers, the spatial arrangements of the target and distractors are repeated in half of the trials (&#x0201c;old&#x0201d; contexts) and random in the other half (&#x0201c;new&#x0201d; contexts). One common finding is that just a few repetitions of &#x0201c;old&#x0201d; contexts, interspersed among random-context displays, suffice to facilitate search performance, often without explicit recognition of &#x0201c;old&#x0201d; contexts (but see Annac et al., <xref ref-type="bibr" rid="CR1">2019</xref>; Vadillo et al., <xref ref-type="bibr" rid="CR27">2016</xref>).</p><p id="Par3">Since Chun and Jiang&#x02019;s (<xref ref-type="bibr" rid="CR6">1998</xref>) seminal study, most investigations of contextual cueing have adopted similar experimental setups, randomly repeating exact old contexts multiple times throughout the experiment, without altering their configural predictiveness (Chun, <xref ref-type="bibr" rid="CR5">2000</xref>; Chun &#x00026; Jiang, <xref ref-type="bibr" rid="CR7">1999</xref>; Geyer et al., <xref ref-type="bibr" rid="CR9">2010</xref>; Goujon et al., <xref ref-type="bibr" rid="CR10">2015</xref>; Shi et al., <xref ref-type="bibr" rid="CR24">2013</xref>; Sisk et al., <xref ref-type="bibr" rid="CR25">2019</xref>; Wolfe &#x00026; Horowitz, <xref ref-type="bibr" rid="CR29">2017</xref>). The results support the attention-guidance account (Chun &#x00026; Jiang, <xref ref-type="bibr" rid="CR6">1998</xref>), according to which learnt spatial associations between the target and distractors (stored in long-term memory and activated by a given old display) &#x0201c;cue&#x0201d; attention to &#x02013; or predict &#x02013; the target location (e.g., Brockmole &#x00026; Henderson, 2006; Chen et al., 2021; Geyer et al., <xref ref-type="bibr" rid="CR9">2010</xref>; Giesbrecht et al., 2013; Johnson et al., 2007; Peterson &#x00026; Kramer, 2001; Schlagbauer et al., 2017; Tseng &#x00026; Li, 2004; Zinchenko et al., <xref ref-type="bibr" rid="CR34">2020</xref>). Additionally, contextual cueing might also be attributable (in part) to facilitation of the late processing stage of response selection and/or motor execution, when participants decide which motor (hand) effector to use for a correct response (e.g., Chen et al., 2021; Hout &#x00026; Goldinger, 2012; Kunar et al., 2007; Schankin &#x00026; Schub&#x000f6;, 2010).</p><p id="Par4">However, learnt predictive spatial contexts may not always be beneficial. When the target changes its position while the non-target items remain at their locations, learnt contextual layouts can impede search performance &#x02013; because attention is misguided to the old target location (Manginelli &#x00026; Pollmann, <xref ref-type="bibr" rid="CR18">2009</xref>; Zellin et al., <xref ref-type="bibr" rid="CR31">2013</xref>; Zinchenko et al., <xref ref-type="bibr" rid="CR34">2020</xref>). Interestingly, when the fixed distractor layout does not predict the target location during learning (e.g., when the target changes its location on every trial, while the non-target items stay in their old positions), such non-predictive &#x0201c;old&#x0201d; contexts can still facilitate search (Vadillo et al., <xref ref-type="bibr" rid="CR26">2021</xref>; Wang et al., <xref ref-type="bibr" rid="CR28">2020</xref>). Vadillo and colleagues (2021) argued that, in this case, search is facilitated by inhibiting the locations occupied by the old distractors, making searching more efficient by (relative to the distractors) increasing the attentional priority of the target. These findings align with a core assumption of Beesley et al.&#x02019;s (<xref ref-type="bibr" rid="CR2">2015</xref>) connectionist model scheme, namely, that contextual cueing may involve not only learning to predict the target location, but also acquiring associations among the distractor locations themselves. That is, either the target-distractor relations or the distractor-context alone can be effectively learned.<xref ref-type="fn" rid="Fn1">1</xref></p><p id="Par5">In more &#x0201c;cognitive&#x0201d; terms, the contextual-<italic>guidance</italic> (e.g., Manginelli &#x00026; Pollmann, <xref ref-type="bibr" rid="CR18">2009</xref>; Zellin et al., <xref ref-type="bibr" rid="CR31">2013</xref>; Zinchenko et al., <xref ref-type="bibr" rid="CR34">2020</xref>) and the context-<italic>suppression</italic> accounts (Vadillo et al., <xref ref-type="bibr" rid="CR26">2021</xref>) may be understood to assume a qualitative difference in the learnt search modes. Observers may come to adopt either a contextual-guidance mode for full-repeated displays or a context-suppression mode for distractor-repeated displays &#x02013; depending on the non-/predictability of the target location from the distractor context. If observers adopt the contextual-guidance mode, transitioning from full-repeated displays to distractor-repeated displays may lead to performance costs, due to the persistent miscueing of attention to the original target location, which takes extended training to unlearn (e.g., Manginelli &#x00026; Pollmann, <xref ref-type="bibr" rid="CR18">2009</xref>; Zellin et al., <xref ref-type="bibr" rid="CR31">2013</xref>; Zinchenko et al., <xref ref-type="bibr" rid="CR34">2020</xref>). In contrast, there may be little cost when transitioning from distractor-repeated to full-repeated displays: the suppression strategy does not depend on the target&#x02019;s location, and so the learnt distractor context can still be effectively suppressed, while opening the possibility of acquiring contextual-guidance &#x0201c;cues,&#x0201d; and so affording operation in the &#x0201c;guidance&#x0201d; search mode.</p><p id="Par6">The aim of the present study was to investigate the interplay of the contextual-guidance and -suppression search modes when the predictivity, or, respectively, non-predictivity, of the distractor context with respect to the target location changes from an initial learning to a subsequent test phase. During the training phase, one-third of displays consisted of full repetitions of the (fixed) target-context arrangements (full-repeated displays). In another third, the arrangement of the distractors remained identical across repetitions, as in full-repeated displays, but the location of the target changed across the repetitions (distractor-repeated displays). The remaining third comprised displays with all distractors appearing at random locations (new, baseline displays). In the test phase, while the new displays were newly generated, the old contexts in initially full-repeated displays (from the training phase) were rendered non-predictive with respect to the target location, and the old contexts in initially distractor-repeated displays were rendered fully predictive of the target location, without altering the distractor configuration. Importantly, the number of possible target locations in each display condition was fixed (including a change of the target locations from the training to the test phase in each condition) &#x02013; a standard procedure to control for influences of absolute (&#x0201c;context-less&#x0201d;) target-location learning (cf. Chun &#x00026; Jiang, <xref ref-type="bibr" rid="CR6">1998</xref>).</p><sec id="Sec2"><title>Method</title><sec id="Sec3"><title>Participants</title><p id="Par7">Twenty-five participants (12 females; age: <italic>M</italic> = 23.25 years) took part in the study. All were right-handed and had normal or corrected-to-normal visual acuity. The sample size was determined based on a meta-analysis of the contextual-cueing effect (<italic>d</italic><sub><italic>z</italic></sub> = 0.60) from studies with distractor-repeated context and random target locations (Vadillo et al., <xref ref-type="bibr" rid="CR26">2021</xref>), which was found to be significantly smaller than the standard contextual-cueing effect (<italic>d</italic><sub><italic>z</italic></sub> = 0.97). We aimed for 85% power with an alpha level of .05, which yielded 22 participants. The study was approved by the Ethics Committee of the Department of Psychology, Ludwig-Maximilians-Universit&#x000e4;t, Munich. All participants provided written informed consent prior to the experiment and received 9 &#x020ac;/h for their service.</p></sec><sec id="Sec4"><title>Apparatus and stimuli</title><p id="Par8">The experiment was conducted in a dimly lit cabin (0.35 cd/m<sup>2</sup>). Participants were seated comfortably, with their heads supported by a chin rest and forehead support, and viewed the stimuli presented on a 19-in. CRT color monitor from a distance of 60 cm (CRT screen resolution: 1,024 &#x000d7; 768 pixels; refresh rate: 85 Hz). Event scheduling and response recording were controlled by the customized Matlab code with the Psychophysics Toolbox (Brainard, 1997; Pelli, 1997).</p><p id="Par9">The search displays consisted of 16 gray items (25.38 cd/m<sup>2</sup>; Fig. <xref rid="Fig1" ref-type="fig">1</xref>), 15 of them being L-shapes, randomly rotated in one of four possible directions (0&#x000b0;, 90&#x000b0;, 180&#x000b0;, or 270&#x000b0;), and one T-shaped target, rotated either 90&#x000b0; or 270&#x000b0; (pointing to the left or right), all subtending 1.0&#x000b0; of visual angle. They were randomly placed at 16 of 64 possible locations, arranged around four concentric (invisible) circles with radii of 2&#x000b0;, 4&#x000b0;, 6&#x000b0;, and 8&#x000b0;, respectively, on a dark background (1.76 cd/m<sup>2</sup>). To avoid salient clusters, each display quadrant contained an equal number of items. The target item could only appear on the second or third ring (32 possible locations), avoiding influences of eccentricity on response speed (Zang et al., <xref ref-type="bibr" rid="CR30">2020</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><p>The trial sequence in the experimental paradigm. Each trial began with a central fixation marker, followed by a 1-s search display, and replaced by a blank screen for 2&#x02013;2.2 s. Participants were required to respond within 2 s after display onset; otherwise, the response was invalid. The task was to find the target T among the L-type distractors and discriminate its (left/right) orientation, as quickly and accurately as possible. In the example shown, the target T is titled to the right</p></caption><graphic xlink:href="13423_2024_2508_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec5"><title>Procedure and design</title><p id="Par10">Each trial began with a central fixation marker (0.4&#x000b0;&#x000d7; 0.4&#x000b0;) for 1 s, which was followed by a 1-s search display. The search display disappeared automatically, being replaced by a blank screen. Participants had to make a response within 2 s from the onset of the search display; otherwise, the response was considered invalid. After a blank interval of 2&#x02013;2.2 s, the next trial began. Participants underwent 24 practice trials, followed by 720 trials of the experiment proper. The trials were divided into 60 blocks, each of 12 trials. The first 30 blocks constituted the training phase, the subsequent 30 blocks the test phase. Participants were allowed a break after each block, with a mandatory short break every 10 blocks.</p><p id="Par11">In the training phase, there were four distinct full-repeated displays, in which all elements, including the target, remained at the same location across repetitions. Each target occupied a distinct (invariant) location in one particular quadrant relative to the repeated distractor context in the respective display, with the targets appearing in different quadrants in the four displays. There were also four distinct distractor-repeated displays. In these displays, the configuration of distractors remained identical across repetitions, as in the full-repeated displays, but the target changed its location from one repetition to the next, appearing with equal probability in the four quadrants. All eight full-repeated and distractor-repeated displays were tested once within each block. The remaining four trials within each block constituted the new condition, in which all distractors appeared at random locations that differed across trials, while the target appeared equally at a position in each of the four quadrants. Thus, the target locations themselves were repeated equally in full-repeated, distractor-repeated, and new displays. That is, in each block of 12 trials, four distinct positions were used for targets in the full-repeated condition, and four positions for targets in the distractor-repeated condition (with the target positions in the full-repeated and distractor-repeated conditions never overlapping with any of the distractor locations in the two conditions), and the other four positions were used for the new condition. This was meant to ensure that any performance gains in the &#x0201c;repeated&#x0201d; conditions were attributable only to the effects of repeated spatial context-target arrangements or, respectively, the repeated contexts alone, rather than repeated target locations as such (see, e.g., Chun &#x00026; Jiang, <xref ref-type="bibr" rid="CR6">1998</xref>, for a similar approach to rule out an influence of absolute target-location learning, cf. Geng &#x00026; Behrmann, <xref ref-type="bibr" rid="CR8">2005</xref>).</p><p id="Par12">During the test phase, the old contexts in initially full-repeated displays (from the training phase) were rendered non-predictive of the target location, by now randomizing selecting the four target locations (relative to the repeated contexts) across blocks, whereas the old contexts in initially distractor-repeated displays were rendered fully predictive of the (now fixed) target location, without altering the distractor configuration. Of note, 12 new target locations were used in the test phase, four positions for targets in the full-repeated condition, four for targets in the distractor-repeated condition, and the other four for the new condition. We selected four new, different target locations per display type in the test phase (which had not been used in the preceding training phase) to rule out that, e.g., a given target location in one type of display in the training phase was paired with the target position in another type of display in the test phase. In adopting this procedure, we effectively equated the opportunity to learn the absolute target locations (i.e., absolute target-location probability learning along the lines of Geng &#x00026; Behrmann, <xref ref-type="bibr" rid="CR8">2005</xref>) across all three types of display, in both phases. In all other respects, the experiment remained the same as in the training phase.</p><p id="Par13">After completing the formal visual search task, participants performed a &#x0201c;display-recognition&#x0201d; test, which involved 32 displays. Eight displays with old contexts (eight full-repeated displays from the training and test phases) were repeated twice, randomly interspersed with 16 newly generated displays. Participants had to classify each display as an &#x0201c;old&#x0201d; or a &#x0201c;new&#x0201d; one, pressing the left or the right arrow key, respectively.</p></sec><sec id="Sec6"><title>Data analyses</title><p id="Par14">Trials with response errors (1.6%) and those with extreme reaction times (RTs), either below 200 ms or above 2.5 standard deviations from an individual&#x02019;s average RT (2.4%), were excluded from RT analysis. To achieve a reasonably stable estimate of the contextual-facilitation effect, we averaged the data over two consecutive blocks, forming one &#x0201c;Epoch.&#x0201d; Since linear-mixed-model (LMM) analysis offers a more nuanced view than standard analyses of variance (ANOVAs), in particular for the trends, here we adopted LMMs for the dependent variables (RT and accuracy), incorporating the treatment factors Epoch, Display, and Phase, as fixed effects and participant/s as the random effect. We examined whether different display types would show differential learning effects over epochs and any sudden changes between the training and test phases.</p><p id="Par15">We observed a learning effect characterized by a marked decrease in RTs with increasing &#x0201c;epoch&#x0201d; of task performance, with RTs exhibiting an approximately linear relationship to the logarithmic transformation of Epoch. Performance accuracy showed less distinct trends, with accuracy fluctuating between 96% and 100%. Given this, we employed the following LMM (R formula) for the RTs:<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$RT\sim Display+log(Epoch)+log(Epoch):Display+log(Epoch):Display:Phase+(1|sub),$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>:</mml:mo><mml:mi>P</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13423_2024_2508_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par16">where &#x0201c;:&#x0201d; stands for an interaction and &#x0201c;(1|sub)&#x0201d; for the random effect of participant/s, and Phase is a dummy variable with 0 for the training phase and 1 for the test phase.</p><p id="Par17">And we fitted the accuracy data using the following LMM:<disp-formula id="Equb"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Accuracy\sim Display + Epoch + Epoch:Display+Display:Phase+(1|sub).$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>:</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>y</mml:mi><mml:mo>:</mml:mo><mml:mi>P</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="13423_2024_2508_Article_Equb.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par18">We then compared the intercept estimates (<italic>a</italic>), reflecting RT and, respectively, accuracy in Epoch 1, and the slope estimates (<italic>b</italic>), indexing the changes in (RT and accuracy) performance across epochs (i.e., learning rate), among the various conditions. Finally, we tested the interaction between log(Epoch), Display, and Phase in the RT data, reflecting the change of the learning rate for each display condition across the two phases. For accuracy, we included only the interaction between Display and Phase, assuming that at most the general accuracy level would differ between the two phases. As confirmed by the AIC values associated with the above models, these models turned out to have the lowest AIC values while also being simpler compared to other models.</p></sec></sec><sec id="Sec7"><title>Results</title><sec id="Sec8"><title>Response times</title><p id="Par19">Figure <xref rid="Fig2" ref-type="fig">2</xref>a presents the observed RTs (indicated by the dots with associated error bars) and predicted RTs (indicated by the curves) from the LMM. We used the novel displays as a baseline for comparisons. In terms of the LMM intercept (at Epoch 1), new displays showed a mean RT of 875.83 ms (<italic>a</italic> = 875.83 [835.23, 916.42], <italic>p</italic> &#x0003c; 0.001). Compared to new displays, full-repeated displays exhibited a significant gain in RT (<italic>a</italic> = -52.66 [-82, -23.32], <italic>p</italic> &#x0003c; 0.001), whereas distractor-repeated displays failed to show a significant decrease (<italic>a</italic> = -14.82 [-44.16, 14.53], <italic>p</italic> = 0.32). A repeated-measures ANOVA on the very first block of task performance revealed no significant effect of Display (<italic>F</italic> (1.88, 45.07) = 0.54, <italic>p</italic> = .58, <italic>&#x003b7;</italic><sub><italic>p</italic></sub><sup><italic>2</italic></sup> = .02), suggesting the significant Epoch intercept effect derived mainly came from block 2 (of Epoch 1).<fig id="Fig2"><label>Fig. 2</label><caption><p>Mean reaction time (RT) (<bold>a</bold>) and mean accuracy (<bold>b</bold>) observed during the experiment (dots) and RT/accuracy estimated by the linear-mixed-model analyses (LMMs) (lines) for the various display types as a function of Epoch number (each epoch averaging two blocks), separately for training and test phases (with the transition marked by a dashed vertical line). The transition from &#x0201c;full-repeated to distractor-repeated&#x0201d; indicates that the old contexts in initially full-repeated displays (in the training phase) were rendered non-predictive of the target location (in the test phase), while the distractor configurations remained the same. The transition from &#x0201c;distractor-repeated to full-repeated&#x0201d; indicates that the old contexts in initially distractor-repeated displays (in the training phase) rendered fully predictive of the target location (in the test phase), without any change in the distractor configurations. Error bars indicate the within-subject standard error of the mean</p></caption><graphic xlink:href="13423_2024_2508_Fig2_HTML" id="MO2"/></fig></p><p id="Par20">We used the absolute slope of log(Epoch) as the learning rate. The baseline learning rate with novel displays was 34.87 ms per log(Epoch) (<italic>b</italic> = -34.87 [-45.22, -24.51], <italic>p</italic> &#x0003c; 0.001). The learning rate was increased by 30.39 ms per log(Epoch) for full-repeated (relative to novel) displays (<italic>b</italic> = -30.39 [-45.03, -15.75],<italic> p</italic> &#x0003c; 0.001), and by 17.10 ms per log(Epoch) for distractor-repeated displays (<italic>b</italic> = -17.10 [-31.74, -2.45],<italic> p</italic> = 0.02). Relative to procedural learning in the (no-context) baseline condition, this pattern is indicative of additional, context-dependent learning with both types of display.</p><p id="Par21">Crucially, there was interaction between log(Epoch), Display and Phase, marked by a significant decrease in the learning rate (decreasing 29.21 ms per log(Epoch)) when full-repeated displays in the training phase transformed into distractor-repeated displays in the test phase (<italic>l</italic> = 29.21 [23.67, 34.74],<italic> p</italic> &#x0003c; 0.001). Critically, the transition from distractor-repeated to full-repeated displays did not yield a significant phase difference (<italic>l</italic> = -0.84 [-6.38, 4.69],<italic> p</italic> = 0.77), nor did the change of the (within a given phase fixed) target locations in new displays (<italic>l</italic> = -4.26 [-9.80, 1.27],<italic> p</italic> = 0.13). In other words, only the shift from full-repeated to distractor-repeated displays engendered a transition cost.</p></sec><sec id="Sec9"><title>Accuracy</title><p id="Par22">Accuracies fluctuated between 96% and 100% (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b), exhibiting no discernible trends. The mean accuracies were 98.6% for the full-repeated to distractor-repeated display-change condition, 98.4% for the distractor-repeated to full-repeated condition, and 98.3% for the new-display condition. The LMM failed to yield any effects, all <italic>p</italic>s &#x0003e;.05. The accuracies were also comparable in the very first block of task performance (<italic>p</italic> = .48, <italic>&#x003b7;</italic><sub><italic>p</italic></sub><sup><italic>2</italic></sup> = .03).</p></sec><sec id="Sec10"><title>Transition cost/benefit</title><p id="Par23">To explore the phase-transition cost, we conducted a repeated-measures ANOVA with the within-subject factors Phase (training, test) and Display (full-repeated to distractor-repeated, distractor-repeated to full-repeated, and new displays) comparing the mean RTs between the last two epochs (Epochs 14 and 15) in the training phase with those in the first two epochs (Epochs 16 and 17) in the test phase (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>a). This ANOVA yielded a significant main effect of Display (<italic>F</italic>(1.95, 46.72) = 35.23, <italic>p</italic> &#x0003c; .001, <italic>&#x003b7;</italic><sub><italic>p</italic></sub><sup><italic>2</italic></sup> = .59) and a marginal effect of Phase (<italic>F</italic>(1, 24) = 3.59, <italic>p</italic> = .07, <italic>&#x003b7;</italic><sub><italic>p</italic></sub><sup><italic>2</italic></sup> = .13), as well as a significant Phase &#x000d7; Display interaction (<italic>F</italic>(1.92, 46.17) = 4.06, <italic>p</italic> = .025, <italic>&#x003b7;</italic><sub><italic>p</italic></sub><sup><italic>2</italic></sup> = .14). Specifically, there was a significant <italic>increase</italic> in RT when the full-repeated displays transformed into distractor-repeated displays (mean difference = -53.88 ms, <italic>p</italic> &#x0003c; .001, <italic>d</italic><sub><italic>z</italic></sub> = -0.79; one-tailed t-test was applied to all pairwise comparisons). However, there was no significant difference when the distractor-repeated displays changed into full-repeated displays (mean difference = 5.45 ms, <italic>p</italic> = .64, <italic>d</italic><sub><italic>z</italic></sub> = 0.07), or for the novel displays across the two phases (mean difference = -5.53 ms, <italic>p</italic> = .39, <italic>d</italic><sub><italic>z</italic></sub> = -0.06). Further, there was a significant difference between the (initially) full-repeated and (initially) distractor-repeated displays in the training phase (mean difference = -58.75 ms, <italic>p</italic> = 0.001, <italic>d</italic><sub><italic>z</italic></sub> = -0.68), but no difference between them after the transition (mean difference = 0.58 ms, <italic>p</italic> = 0.52, <italic>d</italic><sub><italic>z</italic></sub> = 0.008). In both the training and the test phases, both full-repeated and distractor-repeated displays engendered faster RTs than novel displays (training: full-repeated vs. new: mean difference = -126.20 ms, <italic>p</italic> &#x0003c; 0.001, <italic>d</italic><sub><italic>z</italic></sub> = -1.34; distractor-repeated vs. new: mean difference = -67.44 ms, <italic>p</italic> = 0.003, <italic>d</italic><sub><italic>z</italic></sub> = -0.62; test: full-repeated vs. new (post-transition): mean difference = -77.85 ms, <italic>p</italic> &#x0003c; 0.001, <italic>d</italic><sub><italic>z</italic></sub> = -1.14; distractor-repeated vs. new: mean difference = -78.43 ms, <italic>p</italic> &#x0003c; 0.001, <italic>d</italic><sub><italic>z</italic></sub> = -1.22).</p><p id="Par24">An analogous ANOVA of performance accuracy in the last epoch in the training and the first epoch in the test phase (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>a) revealed no significant effects (<italic>all p</italic>s &#x0003e; .47, <italic>&#x003b7;</italic><sub><italic>p</italic></sub><sup><italic>2</italic></sup>s &#x0003c; .02). The accuracies for the full-repeated, distractor-repeated, and new displays showed no discernible differences across the two phases (98.5%, 97.8%, and 98.8%, respectively, in the training phase, compared to 98.5%, 98.8%, and 98.5%, respectively, in the test phase).</p></sec><sec id="Sec11"><title>Recognition task</title><p id="Par25">Participants&#x02019; explicit recognition performance &#x02013; that is, their ability to tell apart repeated displays (&#x0201c;signals&#x0201d;) from non-repeated displays (&#x0201c;noise&#x0201d;) &#x02013; was assessed by the signal-detection sensitivity parameter <italic>d&#x02019;</italic> (Green &#x00026; Swets, <xref ref-type="bibr" rid="CR11">1966</xref>), treating correct recognition of repeated displays as &#x0201c;hits&#x0201d; and incorrect &#x0201c;recognition&#x0201d; of novel displays as &#x0201c;false alarms.&#x0201d; The mean <italic>d&#x02019;</italic> score (<italic>d&#x02019;</italic> = 0.25) turned out greater than zero, <italic>t</italic> (24) = 3.55, <italic>p</italic> = .002,<italic> d</italic> = 0.71 &#x02013; making it possible that participants had learnt to explicitly recognize (at least some of the) repeated displays during the search task.</p></sec></sec></sec><sec id="Sec12"><title>Discussion</title><p id="Par26">Using a training-/test-phase design, the present study investigated how acquired contextual-<italic>guidance</italic> and context-<italic>suppression</italic> mechanisms adapt to sudden but persistent changes in target-location predictivity across the two phases. During the training phase, participants encountered three types of display: full-repeated displays with fixed target-context arrangements; novel displays with randomly placed distractors; and distractor-repeated displays with fixed distractor contexts, but (across repetitions) randomly variable target locations. In the subsequent test phase, in addition to newly generated novel displays, the target location in (initially) full-repeated displays in the training phase was randomized (transforming them into distractor-repeated displays); conversely, the target location in (initially) distractor-repeated displays in the training phase was fixed (transforming them into full-repeated displays). As expected, a contextual-facilitation effect was observed for both full-repeated and distractor-repeated displays, evidenced by more pronounced RT gains over time-on-task (epochs) compared to novel displays. Crucially, there was an interaction between Display and Phase in the learning rate, owing to a significant decrease in the learning rate after full-repeated displays in the training phase transformed into distractor-repeated displays in the test phase. In contrast, there was no significant difference for the transition from distractor-repeated to full-repeated displays (and there was no phase difference for new displays). That is, only the shift from full-repeated to distractor-repeated displays engendered a transition cost. This was corroborated by direct comparisons of the last two epochs of training versus the first two epochs of the test phase, which revealed a significant RT cost when full-repeated displays transformed into distractor-repeated displays, but no cost, or gains, when distractor-repeated displays turned into full-repeated displays, and no cost associated with the change of the target locations in novel displays. In both the learning and the test phases, RTs were significantly faster compared to the novel-display baseline for both types of repeated displays, evidencing contextual-facilitation effects. However, while the facilitation effect was larger for full-repeated versus distractor-repeated displays in the last epochs of the training phase, it was no longer greater (numerically, it was smaller) after transition.</p><p id="Par27">Overall, our results indicate that RTs decreased faster, across time-on-task (epochs), for both full-repeated and distractor-repeated displays compared to novel displays, during both the training and test phases, indicative of contextual learning. For novel displays, the learning rate remained stable even though a different set of target locations was introduced in the test phase. Since new displays afford no contextual learning, learning in new displays is considered to largely reflect general procedural task learning (cf. Seitz et al., <xref ref-type="bibr" rid="CR22">2023</xref>).<xref ref-type="fn" rid="Fn2">2</xref> The expedited learning rates in the two &#x0201c;repeated&#x0201d; conditions confirm that contextual learning can occur even in the absence of any fixed relationship between the target and the distractor configuration, as long as there is an invariant distractor configuration across blocks. These findings replicate both the classic contextual-cueing effect (Chun &#x00026; Jiang, <xref ref-type="bibr" rid="CR6">1998</xref>) and Vadillo et al.&#x02019;s (<xref ref-type="bibr" rid="CR26">2021</xref>) observation that repeated contexts with random target locations alone can give rise to facilitated search performance, though the facilitation is less with such distractor-repeated than with full-repeated displays.</p><p id="Par28">This pattern of results can be explained by assuming that, in the training phase, observers come to associate, and then operate, two different modes of search with the two types of repeated display: contextual-guidance with full-repeated displays and context-suppression with distractor-repeated displays. In full-repeated displays, the target location is uniquely predicted by the distractor configuration, so observers learn to use this configuration to direct attention to the target location. That is, activation of the distractor context enhances, via acquired associative links, the priority of the target location, making the target more competitive for summoning (overt or covert) attention (cf. Brady &#x00026; Chun, <xref ref-type="bibr" rid="CR4">2007</xref>). In distractor-repeated displays, by contrast, the distractor context does not predict the target location. So, a relatively more efficient processing mode would be to (learn to) lower the priority of &#x02013; that is, suppress &#x02013; the (associatively linked) distractor locations. This context suppression would leave the target, which is not associatively linked with the distractors, unaffected: its priority signal would not be lowered. However, by reducing the signaling of the distractors, context suppression would confer an advantage to the target in the competition for attentional selection, wherever the target appears in the display. The target-directed contextual-guidance mode is more efficient than the context-suppression mode, conceivably because full-context learning raises the priority of the target location more compared to the activation difference between the target and the distractor locations when operating in distractor-context suppression mode. This would explain why, in the initial training phase, relative to new-display baseline, the contextual learning rate and learning gains are larger for full-repeated versus distractor-repeated displays (a pattern consistent with Vadillo et al., <xref ref-type="bibr" rid="CR26">2021</xref>).</p><p id="Par29">In the test phase, the initially distractor-repeated displays are turned into full-repeated displays (i.e., the target location now becomes predictable), while the initially full-repeated displays are turned into distractor-repeated displays (i.e., the target location is now non-predictable). Of note, in both conditions, the (four) target locations are changed, that is, targets do not appear at any of the locations occupied by targets in the preceding training phase. Now, initially distractor-repeated displays still trigger the context-suppression mode in the test phase, which does curtail the learning of the newly fixed target locations: learnt suppression of the distractor configuration prevents the target location from being effectively linked to this configuration, limiting the acquisition of contextual guidance (see also Kunar &#x00026; Wolfe, <xref ref-type="bibr" rid="CR12">2011</xref>). This would explain why, in the distractor-repeated to full-repeated transition condition, observers perform in exactly the same way in the test phase as in the training phase, even though the test phase offers the opportunity for contextual guidance: witness, in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a, the &#x0201c;seamless&#x0201d; continuation of the RT curve across the transition, with the decreasing slope (across epochs) in the two phases showing no discernible difference.</p><p id="Par30">In contrast, the transition from full-repeated to distractor-repeated displays gives rise to a qualitative shift in performance: the absolute RT gain from the preceding training phase is diminished and the learning rate is reduced in the test versus the training phase. This shift can be attributed to the sudden (and then continuing) failure of the contextual-guidance mode after the change in context predictivity: while the initially acquired contextual cues would still guide attention to the predicted target location, that location is no longer occupied by a target &#x02013; causing a &#x0201c;mis-guidance&#x0201d; cost (cf. Zellin et al., <xref ref-type="bibr" rid="CR31">2013</xref>, <xref ref-type="bibr" rid="CR32">2014</xref>; Zinchenko et al., <xref ref-type="bibr" rid="CR34">2020</xref>). Interestingly, however, this cost is not as large as in previous &#x0201c;target-relocation&#x0201d; studies, where performance fell back to the baseline level and it took a massive amount of practice to re-integrate the new target location into the previously learnt distractor context (e.g., Zellin et al., <xref ref-type="bibr" rid="CR31">2013</xref>, <xref ref-type="bibr" rid="CR32">2014</xref>). In the present study, the general performance level is only insignificantly worse than in the reverse (i.e., distractor-repeated to full-repeated) transition condition, but significantly better compared to the novel-display baseline. This suggests that observers did not persist in operating a contextual-guidance mode, partly because this mode produced persistent misguidance and partly because the non-predictable positioning of the target location relative to the distractor-repeated context rendered attempts to integrate the new target locations in the old contexts futile. As a result, observers may have switched to a contextual-suppression mode, potentially aided by the fact that the distractor contexts themselves remained the same as in the training phase. That is, they (relatively flexibly) switched mode, thus capitalizing on what had been learnt previously, rather than engaging in any new learning. This would be similar to the classical notion of &#x0201c;latent inhibition,&#x0201d; accounting for the difficulty, in learning, to associate something new with an already familiar stimulus (Lubow, <xref ref-type="bibr" rid="CR14">1973</xref>; Lubow &#x00026; Kaplan, <xref ref-type="bibr" rid="CR15">2005</xref>).</p><p id="Par31">Thus, the assumption of two qualitatively different acquired context-processing modes, together with the constraints imposed by the context conditions on what information can be extracted and utilized after the switch, would coherently explain the present pattern of findings.</p><p id="Par32">The above account appears most compatible with Beesley et al.&#x02019;s (<xref ref-type="bibr" rid="CR2">2015</xref>) (neuro-) computational model scheme, which was designed to account for their finding that pre-exposure to repeated distractor configurations subsequently enhances contextual cueing of (in relation to the configurations) consistently positioned target items. Their model assumes that learning of repeated display arrangements involves the acquisition of not just distractor-target associations &#x02013; in the facilitatory links connecting &#x0201c;distractors&#x0201d; in the input layer/map to the target in the output (i.e., the search-guiding priority) layer/map (involving supervised learning), as in Brady and Chun&#x02019;s (<xref ref-type="bibr" rid="CR4">2007</xref>) model, but also of associations among the invariantly placed distractors &#x02013; conceivably implemented by inhibitory links among distractors within the input map (involving unsupervised learning). That is, although the inhibitory (suppressive) and facilitatory mechanisms are separate, they would normally operate in tandem: with full-repeated displays, the system always operates in both the context-suppression and contextual-guidance modes, and the resulting contextual facilitation would be the additive effect of both mechanisms (consistent with Ogawa et al., <xref ref-type="bibr" rid="CR21">2007</xref><xref ref-type="fn" rid="Fn3">3</xref>). In contrast, the scheme envisaged in our account involves a relatively flexible switching between separable modes, in particular: switching from contextual guidance to contextual suppression. In a model scheme along the lines of Beesley et al. (<xref ref-type="bibr" rid="CR2">2015</xref>), this would be equivalent to re-initializing (or un-learning of) the weights connecting (distractors in) the input layer to (targets in) the output layer.</p><p id="Par33">Of course, this is only one of various alternative &#x0201c;model&#x0201d; implementations (see, e.g., Seitz et al., <xref ref-type="bibr" rid="CR23">2024</xref>, for a &#x0201c;procedural&#x0201d; learning model that successfully simulates contextual facilitation, without &#x0201c;knowing&#x0201d; anything about which item is a target or a distractor). Of note, Beesley et al. (<xref ref-type="bibr" rid="CR2">2015</xref>) actually conceives of the &#x0201c;auto-associative&#x0201d; links among the distractor items as facilitatory, strengthening the context-target association &#x02013; contrasting with the notion of &#x0201c;latent inhibition&#x0201d; (see above). Yet another explanation for the reduced cueing effect and learning rate following the transition from full-repeated to distractor-repeated displays might be that the distractor configuration somehow loses its salience over time, owing to its diminished associability with a target location (cf. Mackintosh, <xref ref-type="bibr" rid="CR16">1975</xref>). Accordingly, further empirical and modeling work would be required to corroborate our &#x0201c;distinct-mode&#x0201d; assumption (vs. alternative assumptions of &#x0201c;mode-mixing&#x0201d;) and how this may be implemented in connectionist terms and in the brain.</p></sec></body><back><fn-group><fn id="Fn1"><label>1</label><p id="Par35">Conceivably, also, both may be learnt, producing additive effects. This would be consistent with Ogawa et al. (<xref ref-type="bibr" rid="CR21">2007</xref>): using a dot-probe detection task, they found detection of the probe to be facilitated at the target location and inhibited at distractor locations in repeated relative to non-repeated target-distractor arrangements. We consider the possibility of additive effects further in the <italic>Discussion</italic>.</p></fn><fn id="Fn2"><label>2</label><p id="Par36">Of course, the four recurrent target locations may also be learnt in novel displays, so the improvement in performance may also involve some absolute target-position learning. However, since there were also four recurrent target locations in the full-repeated and distractor-repeated conditions, any absolute target-position learning would be equated across all display conditions and the expedited learning rates (relative to novel displays) in the latter two conditions are attributable to contextual learning.</p></fn><fn id="Fn3"><label>3</label><p id="Par37">Note that Ogawa et al.&#x02019;s (<xref ref-type="bibr" rid="CR21">2007</xref>) evidence, based on the dot-probe RTs, for context suppression in full-repeated displays is somewhat equivocal with regard to when, during search performance, the suppression actually arises: post-selectively, i.e., as a by-effect of target selection (cf. M&#x000fc;ller et al., <xref ref-type="bibr" rid="CR20">2007</xref>), or pre-selectively, actually facilitating target selection.</p></fn><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This study was supported by German research foundation (DFG) research grants CH 3093/1-1 and SH 166/10-1, awarded to SC and ZS, respectively.</p></ack><notes notes-type="funding-information"><title>Funding</title><p>Open Access funding enabled and organized by Projekt DEAL. Deutsche Forschungsgemeinschaft,CH 3093/1-1 ,SH 166/10-1</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The data and materials have been made available via the Open Science Framework and can be accessed at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/pjq2f/">https://osf.io/pjq2f/</ext-link>.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par34">The authors declare no competing financial interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><mixed-citation publication-type="other">Annac, E., Pointner, M., Khader, P. H., M&#x000fc;ller, H. J., Zang, X., &#x00026; Geyer, T. (2019). Recognition of incidentally learned visual search arrays is supported by fixational eye movements. In <italic>Journal of Experimental Psychology: Learning, Memory, and Cognition</italic>. 10.1037/xlm0000702</mixed-citation></ref><ref id="CR2"><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Beesley</surname><given-names>T</given-names></name><name><surname>Vadillo</surname><given-names>MA</given-names></name><name><surname>Pearson</surname><given-names>D</given-names></name><name><surname>Shanks</surname><given-names>DR</given-names></name></person-group><article-title>Pre-exposure of repeated search configurations facilitates subsequent contextual cuing of visual search</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><year>2015</year><volume>41</volume><issue>2</issue><fpage>348</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1037/xlm0000033</pub-id><pub-id pub-id-type="pmid">24999706</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Beesley, T., Vadillo, M. A., Pearson, D., &#x00026; Shanks, D. R. (2015). Pre-exposure of repeated search configurations facilitates subsequent contextual cuing of visual search. <italic>Journal of Experimental Psychology: Learning, Memory, and Cognition,</italic><italic>41</italic>(2), 348&#x02013;362. 10.1037/xlm0000033<pub-id pub-id-type="pmid">24999706</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><mixed-citation publication-type="other">Brady, T. F., &#x00026; Chun, M. M. (2007). Spatial constraints on learning in visual search: modeling contextual cuing. <italic>Journal of Experimental Psychology. Human Perception and Performance</italic>, <italic>33</italic>(4), 798&#x02013;815. 10.1037/0096-1523.33.4.798</mixed-citation></ref><ref id="CR5"><mixed-citation publication-type="other">Chun, M. M. (2000). Contextual cueing of visual attention. <italic>Trends in Cognitive Sciences, 4</italic>(5), 170&#x02013;178. 10.1016/s1364-6613(00)01476-5</mixed-citation></ref><ref id="CR6"><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Chun</surname><given-names>MM</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name></person-group><article-title>Contextual cueing: implicit learning and memory of visual context guides spatial attention</article-title><source>Cognitive Psychology</source><year>1998</year><volume>36</volume><issue>1</issue><fpage>28</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1006/cogp.1998.0681</pub-id><pub-id pub-id-type="pmid">9679076</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Chun, M. M., &#x00026; Jiang, Y. (1998). Contextual cueing: implicit learning and memory of visual context guides spatial attention. <italic>Cognitive Psychology,</italic><italic>36</italic>(1), 28&#x02013;71. 10.1006/cogp.1998.0681<pub-id pub-id-type="pmid">9679076</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Chun</surname><given-names>MM</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name></person-group><article-title>Top-Down Attentional Guidance Based on Implicit Learning of Visual Covariation</article-title><source>Psychological Science</source><year>1999</year><volume>10</volume><issue>4</issue><fpage>360</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00168</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Chun, M. M., &#x00026; Jiang, Y. (1999). Top-Down Attentional Guidance Based on Implicit Learning of Visual Covariation. <italic>Psychological Science,</italic><italic>10</italic>(4), 360&#x02013;365. 10.1111/1467-9280.00168</mixed-citation></citation-alternatives></ref><ref id="CR8"><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Geng</surname><given-names>JJ</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><article-title>Spatial probability as an attentional cue in visual search</article-title><source>Perception &#x00026; Psychophysics</source><year>2005</year><volume>67</volume><issue>7</issue><fpage>1252</fpage><lpage>1268</lpage><pub-id pub-id-type="doi">10.3758/bf03193557</pub-id><pub-id pub-id-type="pmid">16502846</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Geng, J. J., &#x00026; Behrmann, M. (2005). Spatial probability as an attentional cue in visual search. <italic>Perception &#x00026; Psychophysics,</italic><italic>67</italic>(7), 1252&#x02013;1268. 10.3758/bf03193557<pub-id pub-id-type="pmid">16502846</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><mixed-citation publication-type="other">Geyer, T., Shi, Z., &#x00026; M&#x000fc;ller, H. J. (2010). Contextual cueing in multiconjunction visual search is dependent on color- and configuration-based intertrial contingencies. <italic>Journal of Experimental Psychology. Human Perception and Performance</italic>, <italic>36</italic>(3), 515&#x02013;532. 10.1037/a0017448</mixed-citation></ref><ref id="CR10"><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Goujon</surname><given-names>A</given-names></name><name><surname>Didierjean</surname><given-names>A</given-names></name><name><surname>Thorpe</surname><given-names>S</given-names></name></person-group><article-title>Investigating implicit statistical learning mechanisms through contextual cueing</article-title><source>Trends in Cognitive Sciences</source><year>2015</year><volume>19</volume><issue>9</issue><fpage>524</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.07.009</pub-id><pub-id pub-id-type="pmid">26255970</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Goujon, A., Didierjean, A., &#x00026; Thorpe, S. (2015). Investigating implicit statistical learning mechanisms through contextual cueing. <italic>Trends in Cognitive Sciences,</italic><italic>19</italic>(9), 524&#x02013;533. 10.1016/j.tics.2015.07.009<pub-id pub-id-type="pmid">26255970</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><mixed-citation publication-type="other">Green, D. M., &#x00026; Swets, J. A. (1966). <italic>Signal detection theory and psychophysics</italic> (Vol. 1, pp. 1969-2012). New York: Wiley.</mixed-citation></ref><ref id="CR12"><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name><surname>Kunar</surname><given-names>MA</given-names></name><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>Target absent trials in configural contextual cuing</article-title><source>Attention, Perception &#x00026; Psychophysics</source><year>2011</year><volume>73</volume><issue>7</issue><fpage>2077</fpage><lpage>2091</lpage><pub-id pub-id-type="doi">10.3758/s13414-011-0164-0</pub-id></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Kunar, M. A., &#x00026; Wolfe, J. M. (2011). Target absent trials in configural contextual cuing. <italic>Attention, Perception &#x00026; Psychophysics,</italic><italic>73</italic>(7), 2077&#x02013;2091. 10.3758/s13414-011-0164-0</mixed-citation></citation-alternatives></ref><ref id="CR14"><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Lubow</surname><given-names>RE</given-names></name></person-group><article-title>Latent inhibition</article-title><source>Psychological Bulletin</source><year>1973</year><volume>79</volume><issue>6</issue><fpage>398</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1037/h0034425</pub-id><pub-id pub-id-type="pmid">4575029</pub-id>
</element-citation><mixed-citation id="mc-CR14" publication-type="journal">Lubow, R. E. (1973). Latent inhibition. <italic>Psychological Bulletin,</italic><italic>79</italic>(6), 398&#x02013;407. 10.1037/h0034425<pub-id pub-id-type="pmid">4575029</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR15"><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name><surname>Lubow</surname><given-names>RE</given-names></name><name><surname>Kaplan</surname><given-names>O</given-names></name></person-group><article-title>The visual search analogue of latent inhibition: implications for theories of irrelevant stimulus processing in normal and schizophrenic groups</article-title><source>Psychonomic Bulletin &#x00026; Review</source><year>2005</year><volume>12</volume><issue>2</issue><fpage>224</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.3758/bf03196368</pub-id><pub-id pub-id-type="pmid">16082802</pub-id>
</element-citation><mixed-citation id="mc-CR15" publication-type="journal">Lubow, R. E., &#x00026; Kaplan, O. (2005). The visual search analogue of latent inhibition: implications for theories of irrelevant stimulus processing in normal and schizophrenic groups. <italic>Psychonomic Bulletin &#x00026; Review,</italic><italic>12</italic>(2), 224&#x02013;243. 10.3758/bf03196368<pub-id pub-id-type="pmid">16082802</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR16"><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Mackintosh</surname><given-names>NJ</given-names></name></person-group><article-title>A theory of attention: Variations in the associability of stimuli with reinforcement</article-title><source>Psychological Review</source><year>1975</year><volume>82</volume><issue>4</issue><fpage>276</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1037/h0076778</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Mackintosh, N. J. (1975). A theory of attention: Variations in the associability of stimuli with reinforcement. <italic>Psychological Review,</italic><italic>82</italic>(4), 276&#x02013;298. 10.1037/h0076778</mixed-citation></citation-alternatives></ref><ref id="CR18"><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Manginelli</surname><given-names>AA</given-names></name><name><surname>Pollmann</surname><given-names>S</given-names></name></person-group><article-title>Misleading contextual cues: how do they affect visual search?</article-title><source>Psychological Research</source><year>2009</year><volume>73</volume><issue>2</issue><fpage>212</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1007/s00426-008-0211-1</pub-id><pub-id pub-id-type="pmid">19082622</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Manginelli, A. A., &#x00026; Pollmann, S. (2009). Misleading contextual cues: how do they affect visual search? <italic>Psychological Research,</italic><italic>73</italic>(2), 212&#x02013;221. 10.1007/s00426-008-0211-1<pub-id pub-id-type="pmid">19082622</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><mixed-citation publication-type="other">Moran, R., Zehetleitner, M., Liesefeld, H. R., M&#x000fc;ller, H. J., &#x00026; Usher, M. (2016). Serial vs. parallel models of attention in visual search: accounting for benchmark RT-distributions. <italic>Psychonomic Bulletin &#x00026; Review</italic>, <italic>23</italic>(5), 1300&#x02013;1315. 10.3758/s13423-015-0978-1</mixed-citation></ref><ref id="CR20"><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>M&#x000fc;ller</surname><given-names>HJ</given-names></name><name><surname>von M&#x000fc;hlenen</surname><given-names>A</given-names></name><name><surname>Geyer</surname><given-names>T</given-names></name></person-group><article-title>Top-down inhibition of search distractors in parallel visual search</article-title><source>Perception &#x00026; Psychophysics</source><year>2007</year><volume>69</volume><fpage>1373</fpage><lpage>1388</lpage><pub-id pub-id-type="doi">10.3758/BF03192953</pub-id><pub-id pub-id-type="pmid">18078228</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">M&#x000fc;ller, H. J., von M&#x000fc;hlenen, A., &#x00026; Geyer, T. (2007). Top-down inhibition of search distractors in parallel visual search. <italic>Perception &#x00026; Psychophysics,</italic><italic>69</italic>, 1373&#x02013;1388. 10.3758/BF03192953<pub-id pub-id-type="pmid">18078228</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><mixed-citation publication-type="other">Ogawa, H., Takeda, Y., &#x00026; Kumada, T. (2007). Probing attentional modulation of contextual cueing.<italic>Visual Cognition</italic>, <italic>15:3</italic>, 276&#x02013;289. 10.1080/13506280600756977</mixed-citation></ref><ref id="CR22"><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Seitz</surname><given-names>W</given-names></name><name><surname>Zinchenko</surname><given-names>A</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>HJ</given-names></name><name><surname>Geyer</surname><given-names>T</given-names></name></person-group><article-title>Contextual cueing of visual search reflects the acquisition of an optimal, one-for-all oculomotor scanning strategy</article-title><source>Communications Psychology</source><year>2023</year><volume>1</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s44271-023-00019-8</pub-id><pub-id pub-id-type="pmid">38665246</pub-id>
</element-citation><mixed-citation id="mc-CR22" publication-type="journal">Seitz, W., Zinchenko, A., M&#x000fc;ller, H. J., &#x00026; Geyer, T. (2023). Contextual cueing of visual search reflects the acquisition of an optimal, one-for-all oculomotor scanning strategy. <italic>Communications Psychology,</italic><italic>1</italic>(1), 1&#x02013;12. 10.1038/s44271-023-00019-8<pub-id pub-id-type="pmid">38665246</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR23"><mixed-citation publication-type="other">Seitz, W., Zinchenko, A., M&#x000fc;ller, H. J., &#x00026; Geyer, T. (2024). <italic>Learning how: a &#x0201c;mindless&#x0201d; procedure alone can produce contextual facilitation &#x02013; a connectionist model of statistical context learning in visual search</italic>. Unpublished manuscript (under review).</mixed-citation></ref><ref id="CR24"><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Z</given-names></name><name><surname>Zang</surname><given-names>X</given-names></name><name><surname>Jia</surname><given-names>L</given-names></name><name><surname>Geyer</surname><given-names>T</given-names></name><name><surname>Muller</surname><given-names>HJ</given-names></name></person-group><article-title>Transfer of contextual cueing in full-icon display remapping</article-title><source>Journal of Vision</source><year>2013</year><volume>13</volume><fpage>2</fpage><pub-id pub-id-type="doi">10.1167/13.3.2</pub-id><pub-id pub-id-type="pmid">23444391</pub-id>
</element-citation><mixed-citation id="mc-CR24" publication-type="journal">Shi, Z., Zang, X., Jia, L., Geyer, T., &#x00026; Muller, H. J. (2013). Transfer of contextual cueing in full-icon display remapping. <italic>Journal of Vision,</italic><italic>13</italic>, 2. 10.1167/13.3.2<pub-id pub-id-type="pmid">23444391</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR25"><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Sisk</surname><given-names>CA</given-names></name><name><surname>Remington</surname><given-names>RW</given-names></name><name><surname>Jiang</surname><given-names>YV</given-names></name></person-group><article-title>Mechanisms of contextual cueing: A tutorial review</article-title><source>Attention, Perception &#x00026; Psychophysics</source><year>2019</year><volume>81</volume><issue>8</issue><fpage>2571</fpage><lpage>2589</lpage><pub-id pub-id-type="doi">10.3758/s13414-019-01832-2</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Sisk, C. A., Remington, R. W., &#x00026; Jiang, Y. V. (2019). Mechanisms of contextual cueing: A tutorial review. <italic>Attention, Perception &#x00026; Psychophysics,</italic><italic>81</italic>(8), 2571&#x02013;2589. 10.3758/s13414-019-01832-2</mixed-citation></citation-alternatives></ref><ref id="CR26"><mixed-citation publication-type="other">Vadillo, M. A., Gim&#x000e9;nez-Fern&#x000e1;ndez, T., Beesley, T., Shanks, D. R., &#x00026; Luque, D. (2021). There is more to contextual cuing than meets the eye: Improving visual search without attentional guidance toward predictable target locations. <italic>Journal of Experimental Psychology. Human Perception and Performance</italic>, <italic>47</italic>(1), 116&#x02013;120. 10.1037/xhp0000780</mixed-citation></ref><ref id="CR27"><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Vadillo</surname><given-names>MA</given-names></name><name><surname>Konstantinidis</surname><given-names>E</given-names></name><name><surname>Shanks</surname><given-names>DR</given-names></name></person-group><article-title>Underpowered samples, false negatives, and unconscious learning</article-title><source>Psychonomic Bulletin &#x00026; Review</source><year>2016</year><volume>23</volume><issue>1</issue><fpage>87</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0892-6</pub-id><pub-id pub-id-type="pmid">26122896</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Vadillo, M. A., Konstantinidis, E., &#x00026; Shanks, D. R. (2016). Underpowered samples, false negatives, and unconscious learning. <italic>Psychonomic Bulletin &#x00026; Review,</italic><italic>23</italic>(1), 87&#x02013;102. 10.3758/s13423-015-0892-6<pub-id pub-id-type="pmid">26122896</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR28"><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Bai</surname><given-names>X</given-names></name><name><surname>Hui</surname><given-names>Y</given-names></name><name><surname>Song</surname><given-names>C</given-names></name><name><surname>Zhao</surname><given-names>G</given-names></name><name><surname>Haponenko</surname><given-names>H</given-names></name><name><surname>Milliken</surname><given-names>B</given-names></name><name><surname>Sun</surname><given-names>H-J</given-names></name></person-group><article-title>Learning of association between a context and multiple possible target locations in a contextual cueing paradigm</article-title><source>Attention, Perception &#x00026; Psychophysics</source><year>2020</year><volume>82</volume><issue>7</issue><fpage>3374</fpage><lpage>3386</lpage><pub-id pub-id-type="doi">10.3758/s13414-020-02090-3</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Wang, C., Bai, X., Hui, Y., Song, C., Zhao, G., Haponenko, H., Milliken, B., &#x00026; Sun, H.-J. (2020). Learning of association between a context and multiple possible target locations in a contextual cueing paradigm. <italic>Attention, Perception &#x00026; Psychophysics,</italic><italic>82</italic>(7), 3374&#x02013;3386. 10.3758/s13414-020-02090-3</mixed-citation></citation-alternatives></ref><ref id="CR29"><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name><name><surname>Horowitz</surname><given-names>TS</given-names></name></person-group><article-title>Five factors that guide attention in visual search</article-title><source>Nature Human Behaviour</source><year>2017</year><volume>1</volume><fpage>0058</fpage><pub-id pub-id-type="doi">10.1038/s41562-017-0058</pub-id><pub-id pub-id-type="pmid">36711068</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Wolfe, J. M., &#x00026; Horowitz, T. S. (2017). Five factors that guide attention in visual search. <italic>Nature Human Behaviour,</italic><italic>1</italic>, 0058. 10.1038/s41562-017-0058<pub-id pub-id-type="pmid">36711068</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name><surname>Zang</surname><given-names>X</given-names></name><name><surname>Huang</surname><given-names>L</given-names></name><name><surname>Zhu</surname><given-names>X</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>HJ</given-names></name><name><surname>Shi</surname><given-names>Z</given-names></name></person-group><article-title>Influences of luminance contrast and ambient lighting on visual context learning and retrieval</article-title><source>Attention, Perception &#x00026; Psychophysics</source><year>2020</year><volume>82</volume><issue>8</issue><fpage>4007</fpage><lpage>4024</lpage><pub-id pub-id-type="doi">10.3758/s13414-020-02106-y</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Zang, X., Huang, L., Zhu, X., M&#x000fc;ller, H. J., &#x00026; Shi, Z. (2020). Influences of luminance contrast and ambient lighting on visual context learning and retrieval. <italic>Attention, Perception &#x00026; Psychophysics,</italic><italic>82</italic>(8), 4007&#x02013;4024. 10.3758/s13414-020-02106-y</mixed-citation></citation-alternatives></ref><ref id="CR31"><mixed-citation publication-type="other">Zellin, M., von M&#x000fc;hlenen, A., M&#x000fc;ller, H. J., &#x00026; Conci, M. (2013). Statistical learning in the past modulates contextual cueing in the future. <italic>Journal of Vision</italic>, <italic>13</italic>(3). 10.1167/13.3.19</mixed-citation></ref><ref id="CR32"><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Zellin</surname><given-names>M</given-names></name><name><surname>von M&#x000fc;hlenen</surname><given-names>A</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>HJ</given-names></name><name><surname>Conci</surname><given-names>M</given-names></name></person-group><article-title>Long-term adaptation to change in implicit contextual learning</article-title><source>Psychonomic Bulletin &#x00026; Review</source><year>2014</year><volume>21</volume><issue>4</issue><fpage>1073</fpage><lpage>1079</lpage><pub-id pub-id-type="doi">10.3758/s13423-013-0568-z</pub-id><pub-id pub-id-type="pmid">24395095</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">Zellin, M., von M&#x000fc;hlenen, A., M&#x000fc;ller, H. J., &#x00026; Conci, M. (2014). Long-term adaptation to change in implicit contextual learning. <italic>Psychonomic Bulletin &#x00026; Review,</italic><italic>21</italic>(4), 1073&#x02013;1079. 10.3758/s13423-013-0568-z<pub-id pub-id-type="pmid">24395095</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Zinchenko</surname><given-names>A</given-names></name><name><surname>Conci</surname><given-names>M</given-names></name><name><surname>T&#x000f6;llner</surname><given-names>T</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>HJ</given-names></name><name><surname>Geyer</surname><given-names>T</given-names></name></person-group><article-title>Automatic Guidance (and Misguidance) of Visuospatial Attention by Acquired Scene Memory: Evidence From an N1pc Polarity Reversal</article-title><source>Psychological Science</source><year>2020</year><volume>31</volume><issue>12</issue><fpage>1531</fpage><lpage>1543</lpage><pub-id pub-id-type="doi">10.1177/0956797620954815</pub-id><pub-id pub-id-type="pmid">33119432</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">Zinchenko, A., Conci, M., T&#x000f6;llner, T., M&#x000fc;ller, H. J., &#x00026; Geyer, T. (2020). Automatic Guidance (and Misguidance) of Visuospatial Attention by Acquired Scene Memory: Evidence From an N1pc Polarity Reversal. <italic>Psychological Science,</italic><italic>31</italic>(12), 1531&#x02013;1543. 10.1177/0956797620954815<pub-id pub-id-type="pmid">33119432</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>