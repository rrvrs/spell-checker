<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="review-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006477</article-id><article-id pub-id-type="pmc">PMC11861736</article-id><article-id pub-id-type="doi">10.3390/s25041248</article-id><article-id pub-id-type="publisher-id">sensors-25-01248</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title>Sensor-Fusion Based Navigation for Autonomous Mobile Robot</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>U&#x00161;inskis</surname><given-names>Vygantas</given-names></name></contrib><contrib contrib-type="author"><name><surname>Nowicki</surname><given-names>Micha&#x00142;</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0665-8829</contrib-id><name><surname>Dzedzickis</surname><given-names>Andrius</given-names></name></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2458-7243</contrib-id><name><surname>Bu&#x0010d;inskas</surname><given-names>Vytautas</given-names></name><xref rid="c1-sensors-25-01248" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Rizos</surname><given-names>Chris</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01248">Department of Mechatronics, Robotics and Digital Manufacturing, Faculty of Mechanics, Vilnius Gediminas Technical University, LT-10105 Vilnius, Lithuania; <email>vygantas.usinskis@vilniustech.lt</email> (V.U.); <email>michal.nowicki@vilniustech.lt</email> (M.N.); <email>andrius.dzedzickis@vilniustech.lt</email> (A.D.)</aff><author-notes><corresp id="c1-sensors-25-01248"><label>*</label>Correspondence: <email>vytautas.bucinskas@vilniustech.lt</email>; Tel.: +370-5-237-0668</corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1248</elocation-id><history><date date-type="received"><day>16</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>03</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>15</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Navigation systems are developing rapidly; nevertheless, tasks are becoming more complex, significantly increasing the number of challenges for robotic systems. Navigation can be separated into global and local navigation. While global navigation works according to predefined data about the environment, local navigation uses sensory data to dynamically react and adjust the trajectory. Tasks are becoming more complex with the addition of dynamic obstacles, multiple robots, or, in some cases, inspection of places that are not physically reachable by humans. Cognitive tasks require not only detecting an object but also evaluating it without direct recognition. For this purpose, sensor fusion methods are employed. However, sensors of different physical nature sometimes cannot directly extract required information. As a result, AI methods are becoming increasingly popular for evaluating acquired information and for controlling and generating robot trajectories. In this work, a review of sensors for mobile robot localization is presented by comparing them and listing advantages and disadvantages of their combinations. Also, integration with path-planning methods is looked into. Moreover, sensor fusion methods are analyzed and evaluated. Furthermore, a concept for channel robot navigation, designed based on the research literature, is presented. Lastly, discussion and conclusions are drawn.</p></abstract><kwd-group><kwd>sensor fusion</kwd><kwd>mobile robot</kwd><kwd>navigation</kwd><kwd>machine learning</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01248"><title>1. Introduction</title><p>In the rapid development of the automation and robotics world, making of autonomous vehicles and mobile robots is a big step toward operational efficiency, safety, and autonomy. At the heart of this technological revolution is the intricate domain of sensor fusion, a paradigm that merges data from different sensors to make cohesive and accurate perceptions of operational environments. This paper goes into the realm of sensor-fusion-based navigation systems for autonomous robots, spotlighting diverse methodologies that underpin their functionality and emerging trends that shape their evolution.</p><p>Navigational autonomy in robots is paramount for their effective deployment across a spectrum of applications, from industrial automation to exploration in inaccessible terrains. Traditional navigation methodologies, while foundational, often grapple with complexities and dynamic changes intrinsic to real-world environments. Bridging this gap, advanced navigation systems harness the synergy of global and local navigation methods [<xref rid="B1-sensors-25-01248" ref-type="bibr">1</xref>]. Global navigation operates on the premise of pre-acquired environmental knowledge, facilitating formulation and adherence to predetermined paths. In contrast, local navigation equips mobile robots with agility to dynamically refine their paths in real time, utilizing an arsenal of external sensors&#x02014;ranging from infrared and ultrasonic sensors to LASER, LIDAR, and cameras [<xref rid="B2-sensors-25-01248" ref-type="bibr">2</xref>]. This sensorial diversity, when orchestrated by sophisticated software algorithms, enables autonomous correction of robot orientation and trajectory, ensuring navigational resilience against unforeseen obstacles and alterations in environment [<xref rid="B3-sensors-25-01248" ref-type="bibr">3</xref>].</p><p>The dichotomy of global and local navigation methods embodies methodological diversity in robotic navigation, allowing robots to chart optimal paths and fulfil their designated tasks within varied environmental contexts. Nevertheless, reliance on prior environmental knowledge or capability for real-time path adjustment underscores limitations of classic navigation approaches [<xref rid="B4-sensors-25-01248" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01248" ref-type="bibr">5</xref>]. These systems often operate within a deterministic framework, wherein navigation paths are predetermined, or a non-deterministic framework that allows for probabilistic path planning based on sensor input and environmental interaction [<xref rid="B6-sensors-25-01248" ref-type="bibr">6</xref>].</p><p>A non-deterministic framework becomes very relevant in applications that require navigation in hazardous and physically difficult to reach places for humans&#x02014;for example, inspection of narrow underground channels. That kind of working environment lacks global reference points that could be used for a deterministic framework. Furthermore, there is the probability of encountering unexpected obstacles. For these reasons, integration of sensors for robot localization is a must.</p><p>Amidst these methodologies, optical data-based localization emerges as a critical area of focus, leveraging visual information to enhance a robot&#x02019;s environmental awareness and decision-making capability. However, reliance on optical data introduces unique challenges for navigation, including the need for sophisticated object recognition algorithms and the ability to define navigational paths without explicit recognition cues [<xref rid="B7-sensors-25-01248" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01248" ref-type="bibr">8</xref>].</p><p>As we delve deeper into the big picture of research in sensor-fusion-based navigation, this paper aims to elucidate myriad localization methods that empower mobile robots to traverse and interact with their surroundings effectively. By analyzing limitations of classic localization approaches and addressing challenges posed by optical data reliance, we seek to highlight the transformative potential of sensor fusion in crafting more adaptable, reliable, and sophisticated autonomous navigation solutions primarily focused on local path planning.</p><p>In anthropocentric terms, localization methods can be classified into vision-based and non-vision-based approaches, which makes the distinction easier to grasp. Vision-based methods rely on imaging cameras to capture visual information, similar to human sight, which is then analyzed in various ways to understand and navigate the environment. Non-vision-based methods, in contrast, use sensors like LIDAR, radar, ultrasonic etc., which perceive the environment through means that are alien to human senses, such as detecting distances through sound waves or localizing oneself through RFID tags.</p><p>The manuscript is organized to provide a comprehensive review of sensor-fusion-based navigation systems. <xref rid="sec2-sensors-25-01248" ref-type="sec">Section 2</xref>: A literature search method details systematic processes, databases, and inclusion criteria used to gather relevant studies. <xref rid="sec3-sensors-25-01248" ref-type="sec">Section 3</xref>: Navigation methods review global and local approaches, discussing their principles, strengths, and limitations. <xref rid="sec4-sensors-25-01248" ref-type="sec">Section 4</xref>: Analysis of non-vision-based localization systems highlights technologies like ultrasonic, infrared, LiDAR, and radar sensors, while <xref rid="sec5-sensors-25-01248" ref-type="sec">Section 5</xref>: Analysis of vision-based localization systems examines both standalone and hybrid configurations, focusing on integration and challenges. <xref rid="sec6-sensors-25-01248" ref-type="sec">Section 6</xref>: Essential sensor fusion systems classify fusion architectures into cooperative, complementary, and competitive approaches, exploring key methodologies. <xref rid="sec7-sensors-25-01248" ref-type="sec">Section 7</xref>: A solution for channel robot navigation presents exemplary cost efficient sensor fusion based local navigation system intended for mobile robots functioning in channels that cannot be physically reached by a human, combining RGB cameras, laser pointers, and pseudo-LiDAR. Finally, <xref rid="sec8-sensors-25-01248" ref-type="sec">Section 8</xref>: Discussion and conclusions summarize key findings, emerging trends, and future directions in sensor fusion for robotics. </p></sec><sec id="sec2-sensors-25-01248"><title>2. Literature Search Method</title><p>The literature search method was based on the systemic process presented in article [<xref rid="B9-sensors-25-01248" ref-type="bibr">9</xref>], which focuses on preferred reporting items of systematic reviews and meta-analyses (PRISMA) statement. Four main databases were utilized, including MDPI, IEEE Xplore, Google Scholar, and Science direct. Other specific databases were also used if there was no other way to access a required paper. The main criteria focusing on autonomous robot navigation topic were formed for the inclusion in this survey, such as:<list list-type="bullet"><list-item><p>Focused on sensor application</p></list-item><list-item><p>Focused on path planning</p></list-item><list-item><p>Focused on mapping techniques</p></list-item><list-item><p>Focused on sensor fusion method adaptions</p></list-item><list-item><p>Focused on machine learning adaptions</p></list-item></list></p><p>Additional criteria for narrowing the main topic:<list list-type="bullet"><list-item><p>Articles that are older than 5 years were excluded with some exceptions if specific points needed more investigation.</p></list-item><list-item><p>Articles that do not focus on mobile robot navigation were excluded except if specific technology being investigated needed more input.</p></list-item><list-item><p>Articles focusing on railways and sea navigation were not taken into consideration with the exception of several articles presenting air navigation systems.</p></list-item></list></p><p>The main keywords that were selected for research on sensor fusion and autonomous mobile robot included in this manuscript were: &#x0201c;Sensor fusion&#x0201d;, &#x0201c;YOLO&#x0201d;, &#x0201c;Mobile robot&#x0201d;, &#x0201c;Kalman filter&#x0201d;, &#x0201c;Sensors for navigation&#x0201d;, &#x0201c;Path planning methods&#x0201d;, &#x0201c;LiDAR and camera fusion&#x0201d;, and &#x0201c;ML based sensor fusion&#x0201d;. A simplified workflow of the concluded survey for this manuscript is shown in <xref rid="sensors-25-01248-f001" ref-type="fig">Figure 1</xref>.</p></sec><sec id="sec3-sensors-25-01248"><title>3. Navigation Methods</title><p>In global navigation, knowing the environment beforehand is base for making complete paths from start to end. This method needs a detailed map of the terrain, where the robot&#x02019;s journey is decided by the environmental map it has. Some of the most popular path planning methods for global navigation are shown in <xref rid="sensors-25-01248-t001" ref-type="table">Table 1</xref>. The challenge here is for the robot to match its planned path with real situations it meets, which is made harder by dynamic changes in the environment or if the global target point cannot be accurately established because of obstructions.</p><p>On other side, local navigation relies on robot&#x02019;s ability to adjust in moment, using different external sensors for making decisions on the go. From the accuracy of LASER and LIDAR to depth seeing by cameras, these sensors are the robot&#x02019;s eyes and ears, letting it see and react to obstacles with agility. Software algorithms work like a conductor in this, mixing data to guide the robot&#x02019;s moves every moment. Some of the most popular path planning methods for local navigation are shown in <xref rid="sensors-25-01248-t001" ref-type="table">Table 1</xref>.</p><p>Merging global and local navigation shows a mixed way, where a robot is given a wide environmental model but also keeps flexibility to change as needed. This mix improves the robot&#x02019;s wayfinding, giving it paths that are both planned and reactive.</p><p>Sensor fusion stands as a key part in evolving navigation systems, bringing together different data streams into one clear understanding of surroundings. By putting together strengths of various sensors, from wide views of LIDAR to detailed capture by cameras, robots obtain a fuller view of their surroundings. This richer sensing not only makes path planning better but also helps robots move through complex, unstructured places.</p><p>But the path of innovation in robot path planning is ongoing, with new explorations and improvements always on horizon. Moving forward, bringing in new techs, with advances in machine learning and artificial intelligence, opens new possibilities in autonomous navigation. Bringing together global and local methods, backed by the power of sensor fusion, points to a future where robots move with unmatched precision, efficiency, and autonomy. Some of the technologies widely used for autonomous robot localization are shown in <xref rid="sensors-25-01248-f002" ref-type="fig">Figure 2</xref>.</p></sec><sec id="sec4-sensors-25-01248"><title>4. Analysis of Non-Vision-Based Localization Systems</title><p>Non-vision-based localization technologies play a crucial role in the field of robotics, especially in environments where visual data may be unreliable or unavailable. These technologies encompass a variety of methods and sensors designed to enhance a robot&#x02019;s ability to localize and navigate itself within its environment, leveraging alternative sensory data to achieve precise and reliable navigation. The same is true of the common non-vision-based technologies, which are shown on the left side of <xref rid="sensors-25-01248-f002" ref-type="fig">Figure 2</xref>.</p><p>One significant branch of non-vision-based localization focuses on target localization. This involves determining the position of specific targets within an environment, utilizing technologies such as Ultra-Wideband (UWB), Bluetooth Low Energy (BLE), and Radio-Frequency Identification (RFID). UWB technology, known for its high accuracy and reliability, is widely used in indoor positioning systems due to its ability to provide precise location information even in complex environments [<xref rid="B21-sensors-25-01248" ref-type="bibr">21</xref>]. BLE, on the other hand, is commonly employed for proximity detection and location tracking, benefiting from its low power consumption and widespread use in consumer electronics [<xref rid="B22-sensors-25-01248" ref-type="bibr">22</xref>]. RFID systems offer another layer of versatility, allowing for the identification and tracking of objects through electromagnetic fields [<xref rid="B23-sensors-25-01248" ref-type="bibr">23</xref>]. These technologies collectively enhance the ability of robots to locate and interact with various targets, crucial for applications such as inventory management and asset tracking.</p><p>Robot localization, another critical aspect of non-vision-based localization, involves methods that enable robots to determine their own position within an environment. Infrared (IR) sensors are versatile tools used in both target and robot localization, providing reliable distance measurements and object detection capabilities [<xref rid="B24-sensors-25-01248" ref-type="bibr">24</xref>]. Tactile sensors, which detect physical contact with objects, are particularly useful in cluttered environments where precise positioning is essential [<xref rid="B25-sensors-25-01248" ref-type="bibr">25</xref>]. Ultrasonic sensors, employing sound waves to measure distances, are effective for obstacle detection and navigation in various conditions, including occluded vision due to fog or smoke or underwater environments [<xref rid="B26-sensors-25-01248" ref-type="bibr">26</xref>]. Lidar (Light Detection and Ranging) systems stand out due to their ability to create high-resolution maps of the environment using laser pulses, offering unparalleled accuracy and detail [<xref rid="B27-sensors-25-01248" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-01248" ref-type="bibr">28</xref>]. Radar systems, which use radio waves, provide robust performance in diverse environmental conditions, making them indispensable for applications requiring reliable distance, angle, and velocity measurements [<xref rid="B29-sensors-25-01248" ref-type="bibr">29</xref>]. To unravel and compare non-vision sensors for robot localization, methods proposed in the literature were analyzed and presented in <xref rid="sensors-25-01248-t002" ref-type="table">Table 2</xref>.</p><p>From <xref rid="sensors-25-01248-t002" ref-type="table">Table 2</xref>, we can see a variety of solutions to effectively achieve local navigation by incorporating proximity and contact sensors of different physical nature to detect obstacles. Due to field view limitations, it is noticeable that ultrasonic and IR distance sensors are usually used in combinations to compensate for those disadvantages. LiDAR and radar sensors have higher accuracy and field of view but require more efficient mapping techniques to increase performance. Further comparison of analyzed sensors is shown in <xref rid="sensors-25-01248-t003" ref-type="table">Table 3</xref>.</p><p>As shown in <xref rid="sensors-25-01248-t003" ref-type="table">Table 3</xref>, tactile sensors computationally lack proximity evaluation capabilities but are very computationally efficient. They are a great addition not only for obstacle detection purposes but also for collaborative function with human operators. Also, it is worth mentioning that in recent studies, tactile sensors vary in complexity and can even become a system of several sensors to measure contact and deformation phenomena. For example, in article [<xref rid="B46-sensors-25-01248" ref-type="bibr">46</xref>], an optical tactile sensing system is presented, which can measure force distribution for arial mobile robot purposes.</p><p>The integration of these non-vision-based navigation technologies into robotic systems addresses several challenges associated with visual data reliance. For instance, varying lighting conditions and the need for sophisticated object recognition algorithms can complicate vision-based navigation. Non-vision-based systems, leveraging a combination of sensory inputs such as IR, tactile, ultrasonic, lidar, and radar, can navigate and localize effectively without the constraints of visual data. This adaptability is particularly advantageous in environments like warehouses, underwater explorations, and subterranean locales such as mines or tunnels where visual cues are limited or non-existent.</p></sec><sec id="sec5-sensors-25-01248"><title>5. Analysis of Vision-Based Localization Systems</title><sec id="sec5dot1-sensors-25-01248"><title>5.1. Standalone Vision Navigation Systems</title><p>Vision capability is an essential feature for mobile robot navigation systems. Many cameras were proven to work in this scenario with corresponding advantages and disadvantages, some of which are shown one the right side of <xref rid="sensors-25-01248-f002" ref-type="fig">Figure 2</xref>.</p><p>Camera devices can be separated into single and 3D cameras. Single camera can take 2D images. Most commonly used single cameras frequently used in robotic systems are RGB cameras based on CCD or CMOS sensors, which represent each taken pixel in an extensive spectrum of colors extracted from red, green, and blue color space [<xref rid="B47-sensors-25-01248" ref-type="bibr">47</xref>]. They are highly applied for navigation. In article [<xref rid="B48-sensors-25-01248" ref-type="bibr">48</xref>], an RGB camera is used to detect road lines according to the color so vehicles could follow the path in combination with other sensors. Other notable examples of single cameras are NIR cameras, which are less sensitive to visible light, meaning images are not corrupted by reflections [<xref rid="B49-sensors-25-01248" ref-type="bibr">49</xref>]. Also, a fisheye camera is a powerful omni-directional perception sensor. It is used in navigation systems because of its wide field of view. In article [<xref rid="B50-sensors-25-01248" ref-type="bibr">50</xref>], a fisheye camera is used to take images from 180 angle using the ASIFT algorithm to extract features of obstacles. Another edition for visions devices is the polarized camera, which had polarization systems able to extract orientation of the light oscillations reflected from perceived surfaces [<xref rid="B51-sensors-25-01248" ref-type="bibr">51</xref>]. It is very convenient for detecting objects in crowded environments by filtering unwanted reflections and glare and enhancing the image contrast.</p><p>Depth measurement capability allows not only color recognition but also evaluation of object 3D geometry. One of the most frequently used cameras for this purpose is the RGB-D camera, which emits a predefined pattern of infrared light rays and the depth of each pixel is calculated by the reflection of rays [<xref rid="B52-sensors-25-01248" ref-type="bibr">52</xref>]. Similarly, time of flight IR cameras work by illuminating present objects with modulated light and observing reflections, allowing the robot to perceive depth [<xref rid="B53-sensors-25-01248" ref-type="bibr">53</xref>], although color cannot be perceived with this camera. Another increasing in popularity is the event-based camera, frequently employing DVS sensors, which capture pixel intensity changes, and robust compared to other cameras [<xref rid="B54-sensors-25-01248" ref-type="bibr">54</xref>]. These cameras can also calculate depth by event capture, although it is computationally demanding and methods for efficiency are needed.</p><p>All mentioned cameras have corresponding advantages and disadvantages. To evaluate their properties and functionality, some of the researched methods for mobile robots and other types of navigation that integrate cameras in their systems will be analyzed. The researched methods are presented in <xref rid="sensors-25-01248-t004" ref-type="table">Table 4</xref>.</p><p>From <xref rid="sensors-25-01248-t004" ref-type="table">Table 4</xref>, we can see a wide application of cameras for navigation purposes. Several techniques to effectively use vision devices for recognition were mentioned. One of the most popular techniques improving rapidly is you only look once (YOLO) and its advanced versions, which can work with high accuracy and speed in real time. It converts a target detection problem into a regression problem, dividing images in grids and making predictions for each grid cell separately [<xref rid="B61-sensors-25-01248" ref-type="bibr">61</xref>]. YOLO incorporates convolutional neural network (CNN) principles to train and predict image data [<xref rid="B62-sensors-25-01248" ref-type="bibr">62</xref>]. A typical YOLO network architecture is shown in <xref rid="sensors-25-01248-f003" ref-type="fig">Figure 3</xref>.</p><p>The first 24 convolutional layers extract features from the image, and the two fully connected layers predict the output bounding boxes and class probabilities directly from image pixels. Models from YOLO-v1 to the newly developed YOLO-v9 improved significantly. Going from YOLO-v1 to YOLO-v8 increased processing speed from 45 to 280 FPS and increased detection accuracy of 53.9% [<xref rid="B64-sensors-25-01248" ref-type="bibr">64</xref>]. As stated in article [<xref rid="B65-sensors-25-01248" ref-type="bibr">65</xref>] newly developed YOLO-v9 further increases detection accuracy by reducing information loss, which is encountered in sequential feature extraction process by utilization of programmable gradient information.</p><p>To select the most effective visions system for a specific project, it is important to know not only image recognition methods but also properties of devices. From the research papers, the properties of the most used cameras were summed up for comparison in <xref rid="sensors-25-01248-t005" ref-type="table">Table 5</xref>.</p><p>From <xref rid="sensors-25-01248-t005" ref-type="table">Table 5</xref>, it can be seen that CCD and CMOS cameras are the most cost efficient and have established methods for efficient object recognition tasks. They lack depth capability compared to other cameras in the table. Nevertheless, if it is convenient for a project because of the advantages mentioned, it is possible to measure depth with these cameras to a certain accuracy. For example, in a previously mentioned article [<xref rid="B57-sensors-25-01248" ref-type="bibr">57</xref>], the triangulation principle was used to detect changes in lase pointer projection to estimate distance. Also, using similar triangulation principles, two cameras positioned at slightly different positions can measure depth by matching taken images [<xref rid="B78-sensors-25-01248" ref-type="bibr">78</xref>]. By measuring the required time for reflected light to go from the source and come back, the concept of ToF sensors is designed. ToF cameras working in an infrared range are very convenient for accurate depth estimation. On the other hand, RGB-D can not only estimate depth based on similar principles but also detect a wide range of colors, but it is moderately more expensive than previous cameras and requires smarter algorithms for more efficient matching of color and depth. For example, in article [<xref rid="B79-sensors-25-01248" ref-type="bibr">79</xref>], adaptive color-depth matching is proposed using a transformer-based framework to enhance computational performance. Lastly, event-based cameras enhance capabilities of object detection even more with high dynamic range and depth measuring capabilities. Nevertheless, these cameras are more expensive and challenge current AI-based methods for more effective performance.</p></sec><sec id="sec5dot2-sensors-25-01248"><title>5.2. Hybrid Visions Localization Systems</title><p>As previously explained, depth and field of view estimation with cameras is limited and, in some cases, expensive. Moreover, certain surfaces introduce challenges for detection. For this reason, in robotic navigation systems, cameras are commonly integrated in combination with other distance measurement sensors to enhance overall perception of working environments. Some of the common fusion combinations are shown in <xref rid="sensors-25-01248-f002" ref-type="fig">Figure 2</xref>.</p><p>As navigation environments are becoming more complex with dynamic obstacles and crowded spaces, infrastructures having more than one sensor became the staple of localization, combining sensors that can detect different physical phenomena. To obtain a better understanding of the advantages and disadvantages of hybrid systems, proposed methods in the literature were analyzed. Some of the methods are shown in <xref rid="sensors-25-01248-t006" ref-type="table">Table 6</xref>.</p><p>Going through analyzed approaches of hybrid sensor methods in <xref rid="sensors-25-01248-t006" ref-type="table">Table 6</xref>, it is clear that richer data can be acquired from working environments. Combining distance and visual sensors enables significantly more accurate object detection, which is achieved by mapping accurate distance data with visual data. On the other hand, all presented methods deal with high computational resources. To increase performance of mapping sensor data, several methods were established in time. One of the most widely used is simultaneous localization and mapping (SLAM), which utilizes data from the camera, distance, and other sensors and concurrently estimates sensor poses to generate a comprehensive 3D representation of the surrounding environments [<xref rid="B86-sensors-25-01248" ref-type="bibr">86</xref>]. LiDAR and visual SLAM are well-known techniques, but the need to fuse different sensors established new algorithms. For example, in article [<xref rid="B87-sensors-25-01248" ref-type="bibr">87</xref>], LiDAR inertial camera SLAM is proposed, enabling accurate tracking and photorealistic map reconstruction using 3D Gaussian splatting.</p><p>The core of hybrid sensors systems are fusion methods including Kalman filters, particle filters, and AI methods, which drastically affect the performance of the system. These methods will be introduced further in the next chapter. It is also important to choose the right devices for the project according to sensor properties, which affect the overall performance of the system. A comparison between different hybrid sensors combinations is presented in <xref rid="sensors-25-01248-t007" ref-type="table">Table 7</xref>.</p><p>From <xref rid="sensors-25-01248-t007" ref-type="table">Table 7</xref>, it can be seen that depth capabilities of RGB, RGB-D, and DVS cameras are enhancing significantly in fusion with distance sensors. To achieve the highest accuracy, DVS and Lidar solutions show a lot of promise, because DVS cameras also have low sensitivity to disturbances. If cost-efficient solutions with range capabilities are needed, then combining ultrasonic or radar sensors with cameras is a way to go. Combination of tactile sensors with cameras might not provide range but can be used for force-sensitive applications to detect and inspect object geometry and even material properties.</p></sec></sec><sec id="sec6-sensors-25-01248"><title>6. Essential Sensor Fusion Systems</title><p>Sensor fusion is an essential part of navigation because standalone systems based on one or two sensors cannot cope with increasing complexity of working environments and required tasks. As mentioned before, the addition of cooperating or competitive sensors allows an increase in the overall properties of the system including field of view and accuracy, taking into account different physical phenomena to generate better understanding about working environments and internal processes. To maximize the performance of sensor fusion, it is important to choose appropriate architecture depending on required tasks and chosen sensors. For better understanding, sensor fusion is classified by several factors in the literature. One of the main factors that regularly appears in the literature [<xref rid="B100-sensors-25-01248" ref-type="bibr">100</xref>,<xref rid="B101-sensors-25-01248" ref-type="bibr">101</xref>] defines how early sensor data are interconnected during data processing steps. It can be interpreted as abstraction level. Sensor fusion level according to abstraction can be classified as:<list list-type="bullet"><list-item><p>Low-level&#x02014;indicates that raw sensor data are directly sent to fusion module. This way no data are lost because of noise introduced by postprocessing, meaning some relevant data would not be overlooked. For example, in article [<xref rid="B102-sensors-25-01248" ref-type="bibr">102</xref>], LiDAR 3D point cloud points are augmented by semantically strong image features significantly increasing the number of detected 3D bounding boxes. Nevertheless, high computational resources are required to compute raw data. Also, fusion modules are less adaptive because adding new sensors requires adjustments to the new sensor format.</p></list-item><list-item><p>Medium-level (Feature)&#x02014;involves extracting some key features from raw sensors. Due to this, bandwidth is reduced before carrying data fusion and similar efficiency of extracting relevant data is achieved. Also, this structure is more adaptive and adjustable. This is a very commonly used fusion method, then optimization is important. For instance, in article [<xref rid="B103-sensors-25-01248" ref-type="bibr">103</xref>], encoder, color image, and depth image are first pre-processed before fusion. Unnecessary noise is removed from images to filter only required regions, and encoder provides orientation, ultimately creating a system capable of object recognition and robot localization.</p></list-item><list-item><p>High-level&#x02014;according to this structure, each sensor is postprocessed and carries out its task independently, and then high-level fusion of detected objects or trajectories by each sensor is performed. This type of fusion has high modularity and simplicity. On the other hand, key sensor data at lower levels are lost.</p></list-item></list></p><p>Another way to classify sensor fusion architectures is by relationship among the sources as listed in article [<xref rid="B104-sensors-25-01248" ref-type="bibr">104</xref>], separating into three groups:<list list-type="bullet"><list-item><p>Complementary&#x02014;sensor information does not directly depend on one another but then combined can provide a more complete picture of observed phenomena.</p></list-item><list-item><p>Competitive (redundant)&#x02014;same or similar information is received from sensors to reduce uncertainties and errors, which could appear if using sensors separately.</p></list-item><list-item><p>Cooperative&#x02014;involves combined information extraction that cannot be acquired using one sensor. Involves active sensor collaboration exchanging insight and or intermediate data and increasing accuracy and reliability of overall fusion system.</p></list-item></list></p><p>To design proposed architectures and realize sensor fusion, specific methods and algorithms are required including Kalman, particle filters, novel neural network approaches, etc. To obtain a better understanding of sensor fusion architectures and methods used for mobile robot navigation and classification tasks, proposed solutions in literature are analyzed and presented in in <xref rid="sensors-25-01248-t008" ref-type="table">Table 8</xref>, <xref rid="sensors-25-01248-t009" ref-type="table">Table 9</xref> and <xref rid="sensors-25-01248-t010" ref-type="table">Table 10</xref> below.</p><p>Low-level fusion architecture is useful for systems that require maximizing acquired data from the sensors with no loss for higher accuracy. Systems presented in the Table are designed for obstacle and human detection tracking tasks. These tasks must be performed with upmost accuracy to ensure safety in for all elements in the working environment. To integrate low-level fusion architecture in modern systems, which require real-time capabilities and communication between various software and hardware elements, optimization is necessary to reduce computational load.</p><p>High-level sensor fusion requires significant computing resources, and often these facilities are located remotely and connected via a fast network; therefore, known realized cases are less numerous except the previous ones.</p><p>Comparing analyzed sensor fusion approaches, it can be seen that for mobile robot navigation systems, which mainly focus on robot and target localization, cooperative mid-level sensor fusion architectures are dominant. Navigation requires not only accuracy but also efficiency to perform localization tasks faster. Due to this, mid-level sensor fusion architectures are convenient. Nevertheless, the system has to evaluate more phenomena which are not directly dependent on one another, and complementary fusion becomes handy. This is especially common in vehicle-to-everything communication. For example, in article [<xref rid="B119-sensors-25-01248" ref-type="bibr">119</xref>], high-level fusion structure is presented where LiDAR and Radar is tasked with distance measurement and obstacle detection, and the camera complements the system by classification of objects. There are also plenty of modular-type sensor fusion architectures in autonomous robotic systems. For example, in article [<xref rid="B120-sensors-25-01248" ref-type="bibr">120</xref>], a human detection system is designed with complementary sensor fusion. There, LiDAR is used to detect the lower part of a human and camera for the upper part of pose recognition.</p><p>To realize the designed structure of sensor fusion, the next step is to choose appropriate methods and algorithms to interconnect sensor data for correct estimation of system state. Going through the analyzed approaches in <xref rid="sensors-25-01248-t009" ref-type="table">Table 9</xref>, several methods can be distinguished, which will be presented below.</p><sec id="sec6dot1-sensors-25-01248"><title>6.1. Sensor Fusion Using Kalman Filter</title><p>Kalman filter is a common method for sensor fusion because it can estimate parameters of a constantly changing system in real time, minimizing error covariance [<xref rid="B121-sensors-25-01248" ref-type="bibr">121</xref>], although standard Kalman filter is not suitable for non-linear systems. Nowadays, several advanced Kalman filter methods are used for robotic systems, which were briefly mentioned before. For example, extended Kalman filter (EKF) is commonly used for non-linear systems [<xref rid="B122-sensors-25-01248" ref-type="bibr">122</xref>]. First it constructs linear estimation, but then it is subsequentially updated. It is especially useful for merging sensor data with varying measurement models like GPS, IMU, and vision systems. Nevertheless, subsequential update of linear estimation requires calculating partial derivatives in each step, significantly increasing computational load. Unscanned Kalman filter (UKF) was created to work around the shortcomings of EKF, which can be applied for non-linear systems without direct laterization using sigma point approach for calculation mean and covariance. This method is very useful for accelerometer, GNSS and rotation sensordata fusion as presented in article [<xref rid="B123-sensors-25-01248" ref-type="bibr">123</xref>].</p><p>Going further, cubature Kalman filter (CKF) was built upon its predecessors, which can deal with non-linear data with accuracy and reliability by performing high-dimensional state estimation. Nevertheless, it showed to suffer from error accumulation in long-term operations. In article [<xref rid="B124-sensors-25-01248" ref-type="bibr">124</xref>], utilization of trifocal tensor geometry (TTG) for the CKF algorithm was suggested to increase filter estimation accuracy for long-term visual inertial odometry application.</p><p>Another recent filter showing great results for tracking large-scale moving objects is probabilistic Kalman filter (PKF). It simplifies conventional state variables, thus reducing computational load and making non-uniform modelling more effective. For example, in article [<xref rid="B125-sensors-25-01248" ref-type="bibr">125</xref>], PKF-based non-uniform formulation is proposed for tackling escape problems in multi-object tracking and introducing a first fully GPU-based tracker paradigm. Non-uniform motion is modelled as uniform motion by transforming a time variable into a related displacement variable allowing to integrate deacceleration strategy into a control input model.</p></sec><sec id="sec6dot2-sensors-25-01248"><title>6.2. Sensor Fusion Using Particle Filter</title><p>It is another class of estimation algorithms that involves a probabilistic approach to estimate the state of the system. Particle filter (PF) stands out because of its ability to deal with non-linear system models and non-Gaussian noise. They also show great potential for localization and object detection tasks. For example, in article [<xref rid="B126-sensors-25-01248" ref-type="bibr">126</xref>], PF is used for two ultrasonic sensors and radar fusion for a system that is able to navigate in unknown environments with static and dynamic obstacles. However, basic PF approaches are not suitable for real-time applications especially if the required number particles for accurate estimation is very high [<xref rid="B127-sensors-25-01248" ref-type="bibr">127</xref>]. In article [<xref rid="B128-sensors-25-01248" ref-type="bibr">128</xref>], an enhanced particle filter-weighted differential evolution (EPF-WDE) scheme is proposed, which is used to manage a non-linear and multidimensional system involving a variety of smartphone sensors with notable gains in accuracy and convergence.</p></sec><sec id="sec6dot3-sensors-25-01248"><title>6.3. Deep Learning for Sensor Fusion</title><p>Navigation systems are becoming increasingly complex with a large number of sensors with different physical nature. This amounts to large amount of imperfect raw data. Multi-modal sensor fusion architecture is essential in these cases, and deep learning (DP) techniques are emerging to tackle these tasks. DP is very effective because of non-linear mapping capabilities. Furthermore, DP models have deep layers that can generate high-dimensional representations, which are more comprehensive compared to previously mentioned methods. Also, it is very flexible and can be applied to a variety of applications [<xref rid="B129-sensors-25-01248" ref-type="bibr">129</xref>]. In article [<xref rid="B130-sensors-25-01248" ref-type="bibr">130</xref>], an adaptive-network-based fuzzy interface system (ANFIS) is proposed for LiDAR and inertial navigation system GNSS/INS fusion to localize indoor mobile robot. It incorporated human-like decision making with neural networks, which enables learning from data and improving performance, and it resulted in a lower standard deviation error compared to more classical EKF method. Another deep learning-based high-level fusion method for LiDAR and camera data is presented in article [<xref rid="B131-sensors-25-01248" ref-type="bibr">131</xref>]. The author proposes high-order Attention Mechanism Fusion Networks (HAMFNs) for multi-scale learning and image expression analysis. It is capable of more accurate perception of surrounding the objects&#x02019; state, which is essential in autonomous driving.</p></sec></sec><sec id="sec7-sensors-25-01248"><title>7. Solution for Channel Robot Navigation System</title><p>Channel navigation is a special task due to restricted communication in working environments, complex layouts, lack of light, and unexpected obstacles. These problems are especially relevant for difficult-to-reach channels, which cannot be inspected in advance by the human eye. This kind of environment also requires focusing mainly on local navigation methods. Focusing on this scenario concept for obstacle detection, there is a robot localization and path planning procedure proposed. In addition, cost-effectiveness is taken into consideration.</p><sec id="sec7dot1-sensors-25-01248"><title>7.1. Obstacle Detection</title><p>Starting with obstacle detection, vision-based method were looked into. Obstacle detection in tight channels requires high accuracy and short-range detection. Depth camera and 3D time-of-flight sensors are a viable option because of their ability to evaluate distance to an object. Nevertheless, as discussed previously, implementation cost and computational resources are very high. For this reason, the combination of an RGB camera with a linear laser pointer was looked into. By using a modified laser triangulation method, it is relatively easy to detect an obstacle by focusing RGB and linear laser pointer to the same point and determining the change of linear laser projection in the presence of obstacles as shown in <xref rid="sensors-25-01248-f004" ref-type="fig">Figure 4</xref>.</p><p>The RGB camera image generated with CCD matrix can then be filtered to distinguish red color from the background, thus enabling to evaluate laser pointer projection as researched in articles [<xref rid="B132-sensors-25-01248" ref-type="bibr">132</xref>,<xref rid="B133-sensors-25-01248" ref-type="bibr">133</xref>,<xref rid="B134-sensors-25-01248" ref-type="bibr">134</xref>,<xref rid="B135-sensors-25-01248" ref-type="bibr">135</xref>], where this method was applied for obstacle avoidance. This method is also applied for object scanning as researched in articles [<xref rid="B136-sensors-25-01248" ref-type="bibr">136</xref>], allowing to reach high accuracy. Projection will change according to the shape because the laser is pointed in an angle as shown in <xref rid="sensors-25-01248-f005" ref-type="fig">Figure 5</xref>.</p><p>Depending on the change in <italic toggle="yes">y</italic>, projection displacement <italic toggle="yes">x</italic> changes in CCD matrix. In article [<xref rid="B138-sensors-25-01248" ref-type="bibr">138</xref>], two laser pointers were used for surface scanners because some smaller objects can be hiding behind bigger objects in front of the scanning system. One laser pointer can observe only one surface and obstacles with a base that starts from this surface. For instance, if one laser pointer is focused on the ground, it will not detect obstacles or detect them too late, then the base starts from the ceiling as shown in <xref rid="sensors-25-01248-f006" ref-type="fig">Figure 6</xref>.</p><p>This is an important factor to take into account when discussing navigation in the unknown channels because some unexpected obstacle bases can start from the ceiling or the wall. Configuration of the RGB camera and laser pointers was designed accordingly and is presented in <xref rid="sensors-25-01248-f007" ref-type="fig">Figure 7</xref>.</p><p>The configuration of four laser pointers was chosen to be able to detect obstacles from all four directions that could appear in the observed rectangle field. The size of the field is slightly bigger than robot geometry with fixed tolerance. The observed field should also be calibrated at a specific distance from the robot chassis, which depends on laser projection width and chosen angle. Moreover, pseudo-2D LiDAR of the fixed angle is added to widen the view of observation and to be able to double check information provided from the camera if an obstacle coincided with the observed plane. LiDAR sensor is also important for wall observation to perform wall-following navigations tasks, which will be explained further. Furthermore, accelerometers and gyroscopes are taken into consideration for relative and absolute rotation angle measurement. This allows performing correct motion then rotating robot chassis by having feedback from the sensors. Having multiple sensors allows increasing coverage and reliability of the system.</p></sec><sec sec-type="methods" id="sec7dot2-sensors-25-01248"><title>7.2. Sensor Fusion and Path Following Methodology</title><p>The next step for implementing the chosen sensors is to apply sensor fusion methods to obtain required data and optimize data processing. The main scheme of the sensor fusion and workflow of the system is shown in <xref rid="sensors-25-01248-f008" ref-type="fig">Figure 8</xref> below.</p><p>Sensors are interconnected using mid-level fusion to minimize computational resources required for directly processing raw data. This is of big importance for a cost-effective navigation system to maximize performance. Image data and 2D point cloud obtained from the RGB camera and LiDAR are converted into an occupancy grid expressed as one-dimensional arrays to simplify raw sensor data and make it easier to process it later using a path-planning algorithm. After that, camera and LiDAR arrays are fused into one 2D occupancy grid. Representation of an occupancy grid was selected because of the chosen vector field histogram (VFH) path planning algorithm because it is able to deal with complex obstacles and has good performance, although it has local minimum issues. The idea behind this method is that mobile robot working environment is converted into the grid and each cell represents a value of how close the object is from the robot as shown in <xref rid="sensors-25-01248-f009" ref-type="fig">Figure 9</xref>.</p><p>Then, according to the processed input data, obstacle values are combined to create an artificial vector, which represents repulsive force. According to the standard VHF algorithm, the repulsive force vector and attractive force vector are summed up to obtain a third vector, which represents moving direction as explained in articles [<xref rid="B139-sensors-25-01248" ref-type="bibr">139</xref>,<xref rid="B140-sensors-25-01248" ref-type="bibr">140</xref>]. Nevertheless, in our case, we have no global target position, and the system is purely local. For this reason, it is planned to adjust VHF method for wall following. Robot movement direction is determined by normal direction, which is placed 90 degrees from the combined repulsive force clockwise or counterclockwise depending on initial condition. Field of view is also separated into red and green regions, corresponding to combined laser and LiDAR field and solely LiDAR field, respectively. The idea is that obstacle avoidance initiates, then the obstacle intersects with the red field. On the other hand, the green field must always see an obstacle, and if that is not the case, must search for an obstacle. Due to this feature, the proposed system is able to follow the wall as a reference for the path and procced forward in the channel. In case of U-shaped obstacles, local minimum issues are reduced, although it is taken into account that maze-like channels will not have intersections and edges.</p><p>Channel robots as special cases for autonomous robot navigation is not the only solution for reviewed navigation systems. In general, development of robot navigation systems continues during ongoing robot technology, and recent findings in the review make a scratch for today&#x02019;s situation.</p></sec></sec><sec sec-type="discussion" id="sec8-sensors-25-01248"><title>8. Discussion and Conclusions</title><p>Going through the analyzed literature concerning mobile robot navigation, it is clear that hybrid sensor localization systems will be applied even more in the future. The combination of vision and distance sensors enhances the ability of object detection with accurate distance, color, and dynamic behavior estimation. Sensor hardware is also improving, in some cases creating modules combining several functions&#x02014;for example, modules incorporating infrared, RGB, and ToF functionality. Furthermore, dynamic vision sensors (DVS) are rapidly improving with significant advantages over standard cameras with low latency, high dynamic range, and ability to estimate depth. Polarization filters also proved advantageous for vision technology by enhancing vision contrast and allowing detection of more difficult to perceive objects. Tactile sensor technology improves independently from optical navigation technologies. The soft structure of tactile sensors is able to inspect contact with obstacles with high accuracy, allowing navigation in very narrow spaces or even evaluating terrain properties and slippage.</p><p>Nevertheless, hardware technology advancement is relatively stable in comparison to software development, which will ring the main advantage in the future to enhanced performance of multi sensor systems. AI methods are proving to be effective in all stages of multi-sensor systems starting from post-processing individual sensor data to fusing and mapping the overall picture of the environment. For vision technology, new versions of YOLOv8 and YOLOv9 object detection systems are being built upon further for distinguishing small details in the image from large datasets. Deep learning architectures are progressively improving. CNN networks are commonly used for LiDAR and camera mapping. Nevertheless, transformer networks are being researched, which can increase performance of classification and mapping tasks especially when working with large datasets.</p><p>Advancement AI techniques for multi-sensor data processing, mapping, and path planning also allow use of cost-efficient sensors by enhancing software performance. The concept of a cost-efficient multi-sensor autonomous channel robot was presented incorporating a laser-RGB camera scanner, pseudo-LiDAR, and inertial sensors odometry. Future work will focus on incorporating deep learning methods for data fusion and path planning according to the research conducted in this survey.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, V.U. and V.B.; methodology, A.D.; software, M.N.; validation, M.N., A.D. and V.U.; formal analysis, V.U.; investigation, V.U. and V.B., resources, V.B.; writing&#x02014;original draft preparation, V.U.; writing&#x02014;review and editing, V.B. and A.D.; visualization, V.U.; supervision, V.B.; project administration, A.D.; funding acquisition, V.B. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>No new data were created or analyzed in this study. Data sharing is not applicable to this article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01248"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sekaran</surname><given-names>J.F.</given-names></name>
<name><surname>Sugumari</surname><given-names>T.</given-names></name>
</person-group><article-title>A Review of Perception-Based Navigation System for Autonomous Mobile Robots</article-title><source>Recent Pat. Eng.</source><year>2022</year><volume>17</volume><fpage>e290922209298</fpage><pub-id pub-id-type="doi">10.2174/1872212117666220929142031</pub-id></element-citation></ref><ref id="B2-sensors-25-01248"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Yang</surname><given-names>S.</given-names></name>
</person-group><article-title>Tightly Coupled LiDAR-Inertial Odometry and Mapping for Underground Environments</article-title><source>Sensors</source><year>2023</year><volume>23</volume><elocation-id>6834</elocation-id><pub-id pub-id-type="doi">10.3390/s23156834</pub-id><pub-id pub-id-type="pmid">37571617</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01248"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tatsch</surname><given-names>C.</given-names></name>
<name><surname>Bredu</surname><given-names>J.A.</given-names></name>
<name><surname>Covell</surname><given-names>D.</given-names></name>
<name><surname>Tulu</surname><given-names>I.B.</given-names></name>
<name><surname>Gu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Rhino: An Autonomous Robot for Mapping Underground Mine Environments</article-title><source>Proceedings of the 2023 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>28&#x02013;30 June 2023</conf-date><fpage>1166</fpage><lpage>1173</lpage><pub-id pub-id-type="doi">10.1109/AIM46323.2023.10196202</pub-id></element-citation></ref><ref id="B4-sensors-25-01248"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>P.</given-names></name>
<name><surname>Qian</surname><given-names>S.</given-names></name>
<name><surname>Quan</surname><given-names>H.</given-names></name>
<name><surname>Miao</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>M.</given-names></name>
<name><surname>Hu</surname><given-names>Y.</given-names></name>
<name><surname>Memetimin</surname><given-names>E.</given-names></name>
</person-group><article-title>Path Planning Technique for Mobile Robots: A Review</article-title><source>Machines</source><year>2023</year><volume>11</volume><elocation-id>980</elocation-id><pub-id pub-id-type="doi">10.3390/machines11100980</pub-id></element-citation></ref><ref id="B5-sensors-25-01248"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>F.</given-names></name>
<name><surname>Zuo</surname><given-names>Z.</given-names></name>
<name><surname>Huang</surname><given-names>Q.</given-names></name>
</person-group><article-title>Real-Time Lidar Odometry and Mapping with Loop Closure</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>4373</elocation-id><pub-id pub-id-type="doi">10.3390/s22124373</pub-id><pub-id pub-id-type="pmid">35746155</pub-id>
</element-citation></ref><ref id="B6-sensors-25-01248"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qin</surname><given-names>H.</given-names></name>
<name><surname>Shao</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>T.</given-names></name>
<name><surname>Yu</surname><given-names>X.</given-names></name>
<name><surname>Jiang</surname><given-names>Y.</given-names></name>
<name><surname>Cao</surname><given-names>Z.</given-names></name>
</person-group><article-title>Review of Autonomous Path Planning Algorithms for Mobile Robots</article-title><source>Drones</source><year>2023</year><volume>7</volume><elocation-id>211</elocation-id><pub-id pub-id-type="doi">10.3390/drones7030211</pub-id></element-citation></ref><ref id="B7-sensors-25-01248"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jwo</surname><given-names>D.J.</given-names></name>
<name><surname>Biswal</surname><given-names>A.</given-names></name>
<name><surname>Mir</surname><given-names>I.A.</given-names></name>
</person-group><article-title>Artificial Neural Networks for Navigation Systems: A Review of Recent Research</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>4475</elocation-id><pub-id pub-id-type="doi">10.3390/app13074475</pub-id></element-citation></ref><ref id="B8-sensors-25-01248"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Lekkala</surname><given-names>K.</given-names></name>
<name><surname>Itti</surname><given-names>L.</given-names></name>
</person-group><article-title>World Model Based Sim2Real Transfer for Visual Navigation</article-title><source>arXiv</source><year>2023</year><pub-id pub-id-type="arxiv">2310.18847</pub-id></element-citation></ref><ref id="B9-sensors-25-01248"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Almeida</surname><given-names>J.</given-names></name>
<name><surname>Rufino</surname><given-names>J.</given-names></name>
<name><surname>Alam</surname><given-names>M.</given-names></name>
<name><surname>Ferreira</surname><given-names>J.</given-names></name>
</person-group><article-title>A Survey on Fault Tolerance Techniques for Wireless Vehicular Networks</article-title><source>Electronics</source><year>2019</year><volume>8</volume><elocation-id>1358</elocation-id><pub-id pub-id-type="doi">10.3390/electronics8111358</pub-id></element-citation></ref><ref id="B10-sensors-25-01248"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alshammrei</surname><given-names>S.</given-names></name>
<name><surname>Boubaker</surname><given-names>S.</given-names></name>
<name><surname>Kolsi</surname><given-names>L.</given-names></name>
</person-group><article-title>Improved Dijkstra Algorithm for Mobile Robot Path Planning and Obstacle Avoidance</article-title><source>Comput. Mater. Contin.</source><year>2022</year><volume>72</volume><fpage>5939</fpage><lpage>5954</lpage><pub-id pub-id-type="doi">10.32604/cmc.2022.028165</pub-id></element-citation></ref><ref id="B11-sensors-25-01248"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Qin</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>OpenStreetMap-Based Autonomous Navigation for the Four Wheel-Legged Robot Via 3D-Lidar and CCD Camera</article-title><source>IEEE Trans. Ind. Electron.</source><year>2022</year><volume>69</volume><fpage>2708</fpage><lpage>2717</lpage><pub-id pub-id-type="doi">10.1109/TIE.2021.3070508</pub-id></element-citation></ref><ref id="B12-sensors-25-01248"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Martins</surname><given-names>O.O.</given-names></name>
<name><surname>Adekunle</surname><given-names>A.A.</given-names></name>
<name><surname>Olaniyan</surname><given-names>O.M.</given-names></name>
<name><surname>Bolaji</surname><given-names>B.O.</given-names></name>
</person-group><article-title>An Improved Multi-Objective a-Star Algorithm for Path Planning in a Large Workspace: Design, Implementation, and Evaluation</article-title><source>Sci. Afr.</source><year>2022</year><volume>15</volume><fpage>e01068</fpage><pub-id pub-id-type="doi">10.1016/j.sciaf.2021.e01068</pub-id></element-citation></ref><ref id="B13-sensors-25-01248"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Qi</surname><given-names>X.</given-names></name>
<name><surname>Lou</surname><given-names>S.</given-names></name>
<name><surname>Jing</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>W.</given-names></name>
</person-group><article-title>An Efficient and Robust Improved A* Algorithm for Path Planning</article-title><source>Symmetry</source><year>2021</year><volume>13</volume><elocation-id>2213</elocation-id><pub-id pub-id-type="doi">10.3390/sym13112213</pub-id></element-citation></ref><ref id="B14-sensors-25-01248"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Abdulsaheb</surname><given-names>J.A.</given-names></name>
<name><surname>Kadhim</surname><given-names>D.J.</given-names></name>
</person-group><article-title>Classical and Heuristic Approaches for Mobile Robot Path Planning: A Survey</article-title><source>Robotics</source><year>2023</year><volume>12</volume><elocation-id>93</elocation-id><pub-id pub-id-type="doi">10.3390/robotics12040093</pub-id></element-citation></ref><ref id="B15-sensors-25-01248"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Fu</surname><given-names>Z.</given-names></name>
<name><surname>Zhou</surname><given-names>J.</given-names></name>
<name><surname>Fu</surname><given-names>M.</given-names></name>
<name><surname>Ruan</surname><given-names>L.</given-names></name>
</person-group><article-title>Cooperative Collision Avoidance for Unmanned Surface Vehicles Based on Improved Genetic Algorithm</article-title><source>Ocean Eng.</source><year>2021</year><volume>222</volume><fpage>108612</fpage><pub-id pub-id-type="doi">10.1016/j.oceaneng.2021.108612</pub-id></element-citation></ref><ref id="B16-sensors-25-01248"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>N.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>D.</given-names></name>
<name><surname>Song</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>G.</given-names></name>
<name><surname>Gao</surname><given-names>T.</given-names></name>
</person-group><article-title>Local Path Planning of Mobile Robot Based on Long Short-Term Memory Neural Network</article-title><source>Autom. Control Comput. Sci.</source><year>2021</year><volume>55</volume><fpage>53</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.3103/S014641162101003X</pub-id></element-citation></ref><ref id="B17-sensors-25-01248"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zohaib</surname><given-names>M.</given-names></name>
<name><surname>Pasha</surname><given-names>S.M.</given-names></name>
<name><surname>Javaid</surname><given-names>N.</given-names></name>
<name><surname>Iqbal</surname><given-names>J.</given-names></name>
</person-group><article-title>IBA: Intelligent Bug Algorithm&#x02014;A Novel Strategy to Navigate Mobile Robots Autonomously</article-title><source>Commun. Comput. Inf. Sci.</source><year>2013</year><volume>414</volume><fpage>291</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-10987-9_27</pub-id></element-citation></ref><ref id="B18-sensors-25-01248"><label>18.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>van Breda</surname><given-names>R.</given-names></name>
<name><surname>Smit</surname><given-names>W.J.</given-names></name>
</person-group><article-title>Applicability of Vector Field Histogram Star (Vfh*) on Multicopters</article-title><source>Proceedings of the International Micro Air Vehicle Competition and Conference 2016</source><conf-loc>Beijing, China</conf-loc><conf-date>17&#x02013;21 October 2016</conf-date><fpage>62</fpage><lpage>69</lpage></element-citation></ref><ref id="B19-sensors-25-01248"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kobayashi</surname><given-names>M.</given-names></name>
<name><surname>Motoi</surname><given-names>N.</given-names></name>
</person-group><article-title>Local Path Planning: Dynamic Window Approach with Virtual Manipulators Considering Dynamic Obstacles</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>17018</fpage><lpage>17029</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3150036</pub-id></element-citation></ref><ref id="B20-sensors-25-01248"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mishra</surname><given-names>D.K.</given-names></name>
<name><surname>Thomas</surname><given-names>A.</given-names></name>
<name><surname>Kuruvilla</surname><given-names>J.</given-names></name>
<name><surname>Kalyanasundaram</surname><given-names>P.</given-names></name>
<name><surname>Prasad</surname><given-names>K.R.</given-names></name>
<name><surname>Haldorai</surname><given-names>A.</given-names></name>
</person-group><article-title>Design of Mobile Robot Navigation Controller Using Neuro-Fuzzy Logic System</article-title><source>Comput. Electr. Eng.</source><year>2022</year><volume>101</volume><fpage>108044</fpage><pub-id pub-id-type="doi">10.1016/j.compeleceng.2022.108044</pub-id></element-citation></ref><ref id="B21-sensors-25-01248"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Durodi&#x000e9;</surname><given-names>Y.</given-names></name>
<name><surname>Decoster</surname><given-names>T.</given-names></name>
<name><surname>Van Herbruggen</surname><given-names>B.</given-names></name>
<name><surname>Vanhie-Van Gerwen</surname><given-names>J.</given-names></name>
<name><surname>De Poorter</surname><given-names>E.</given-names></name>
<name><surname>Munteanu</surname><given-names>A.</given-names></name>
<name><surname>Vanderborght</surname><given-names>B.</given-names></name>
</person-group><article-title>A UWB-Ego-Motion Particle Filter for Indoor Pose Estimation of a Ground Robot Using a Moving Horizon Hypothesis</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>2164</elocation-id><pub-id pub-id-type="doi">10.3390/s24072164</pub-id><pub-id pub-id-type="pmid">38610375</pub-id>
</element-citation></ref><ref id="B22-sensors-25-01248"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Roddelkopf</surname><given-names>T.</given-names></name>
<name><surname>Thurow</surname><given-names>K.</given-names></name>
</person-group><article-title>BLE Beacon-Based Floor Detection for Mobile Robots in a Multi-Floor Automation Laboratory</article-title><source>Transp. Saf. Environ.</source><year>2023</year><volume>6</volume><fpage>tdad024</fpage><pub-id pub-id-type="doi">10.1093/tse/tdad024</pub-id></element-citation></ref><ref id="B23-sensors-25-01248"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tripicchio</surname><given-names>P.</given-names></name>
<name><surname>D&#x02019;Avella</surname><given-names>S.</given-names></name>
<name><surname>Unetti</surname><given-names>M.</given-names></name>
<name><surname>Motroni</surname><given-names>A.</given-names></name>
<name><surname>Nepa</surname><given-names>P.</given-names></name>
</person-group><article-title>A UHF Passive RFID Tag Position Estimation Approach Exploiting Mobile Robots: Phase-Only 3D Multilateration Particle Filters With No Unwrapping</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>58778</fpage><lpage>58788</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3393127</pub-id></element-citation></ref><ref id="B24-sensors-25-01248"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>&#x000d6;zcan</surname><given-names>M.</given-names></name>
<name><surname>Aliew</surname><given-names>F.</given-names></name>
<name><surname>G&#x000f6;rg&#x000fc;n</surname><given-names>H.</given-names></name>
</person-group><article-title>Accurate and Precise Distance Estimation for Noisy IR Sensor Readings Contaminated by Outliers</article-title><source>Measurement</source><year>2020</year><volume>156</volume><fpage>107633</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2020.107633</pub-id></element-citation></ref><ref id="B25-sensors-25-01248"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Pan</surname><given-names>C.</given-names></name>
<name><surname>Dai</surname><given-names>H.</given-names></name>
<name><surname>Sun</surname><given-names>H.</given-names></name>
<name><surname>Pan</surname><given-names>Y.</given-names></name>
<name><surname>Lai</surname><given-names>X.</given-names></name>
<name><surname>Lyu</surname><given-names>C.</given-names></name>
<name><surname>Tang</surname><given-names>D.</given-names></name>
<name><surname>Fu</surname><given-names>J.</given-names></name>
<etal/>
</person-group><article-title>Wireless Flexible Magnetic Tactile Sensor with Super-Resolution in Large-Areas</article-title><source>ACS Nano</source><year>2022</year><volume>16</volume><fpage>19271</fpage><lpage>19280</lpage><pub-id pub-id-type="doi">10.1021/acsnano.2c08664</pub-id><pub-id pub-id-type="pmid">36227202</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01248"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Khaleel</surname><given-names>H.Z.</given-names></name>
<name><surname>Oleiwi</surname><given-names>B.K.</given-names></name>
</person-group><article-title>Ultrasonic Sensor Decision-Making Algorithm for Mobile Robot Motion in Maze Environment</article-title><source>Bull. Electr. Eng. Inform.</source><year>2024</year><volume>13</volume><fpage>109</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.11591/eei.v13i1.6560</pub-id></element-citation></ref><ref id="B27-sensors-25-01248"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>De Heuvel</surname><given-names>J.</given-names></name>
<name><surname>Zeng</surname><given-names>X.</given-names></name>
<name><surname>Shi</surname><given-names>W.</given-names></name>
<name><surname>Sethuraman</surname><given-names>T.</given-names></name>
<name><surname>Bennewitz</surname><given-names>M.</given-names></name>
</person-group><article-title>Spatiotemporal Attention Enhances Lidar-Based Robot Navigation in Dynamic Environments</article-title><source>IEEE Robot. Autom. Lett.</source><year>2024</year><volume>9</volume><fpage>4202</fpage><lpage>4209</lpage><pub-id pub-id-type="doi">10.1109/LRA.2024.3373988</pub-id></element-citation></ref><ref id="B28-sensors-25-01248"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ca&#x000f1;adas-Ar&#x000e1;nega</surname><given-names>F.</given-names></name>
<name><surname>Blanco-Claraco</surname><given-names>J.L.</given-names></name>
<name><surname>Moreno</surname><given-names>J.C.</given-names></name>
<name><surname>Rodriguez-Diaz</surname><given-names>F.</given-names></name>
</person-group><article-title>Multimodal Mobile Robotic Dataset for a Typical Mediterranean Greenhouse: The GREENBOT Dataset</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>1874</elocation-id><pub-id pub-id-type="doi">10.3390/s24061874</pub-id><pub-id pub-id-type="pmid">38544137</pub-id>
</element-citation></ref><ref id="B29-sensors-25-01248"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Brescia</surname><given-names>W.</given-names></name>
<name><surname>Gomes</surname><given-names>P.</given-names></name>
<name><surname>Toni</surname><given-names>L.</given-names></name>
<name><surname>Mascolo</surname><given-names>S.</given-names></name>
<name><surname>De Cicco</surname><given-names>L.</given-names></name>
</person-group><article-title>MilliNoise: A Millimeter-Wave Radar Sparse Point Cloud Dataset in Indoor Scenarios</article-title><source>Proceedings of the MMSys &#x02018;24: Proceedings of the 15th ACM Multimedia Systems Conference</source><conf-loc>Bari, Italy</conf-loc><conf-date>15&#x02013;18 April 2024</conf-date><fpage>422</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1145/3625468.3652189</pub-id></element-citation></ref><ref id="B30-sensors-25-01248"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ou</surname><given-names>X.</given-names></name>
<name><surname>You</surname><given-names>Z.</given-names></name>
<name><surname>He</surname><given-names>X.</given-names></name>
</person-group><article-title>Local Path Planner for Mobile Robot Considering Future Positions of Obstacles</article-title><source>Processes</source><year>2024</year><volume>12</volume><elocation-id>984</elocation-id><pub-id pub-id-type="doi">10.3390/pr12050984</pub-id></element-citation></ref><ref id="B31-sensors-25-01248"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>C.</given-names></name>
<name><surname>Zang</surname><given-names>X.</given-names></name>
<name><surname>Song</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Ang</surname><given-names>M.H.</given-names></name>
</person-group><article-title>Virtual Tactile POMDP-Based Path Planning for Object Localization and Grasping</article-title><source>Meas. J. Int. Meas. Confed.</source><year>2024</year><volume>230</volume><fpage>114480</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2024.114480</pub-id></element-citation></ref><ref id="B32-sensors-25-01248"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Armleder</surname><given-names>S.</given-names></name>
<name><surname>Dean-Leon</surname><given-names>E.</given-names></name>
<name><surname>Bergner</surname><given-names>F.</given-names></name>
<name><surname>Guadarrama Olvera</surname><given-names>J.R.</given-names></name>
<name><surname>Cheng</surname><given-names>G.</given-names></name>
</person-group><article-title>Tactile-Based Negotiation of Unknown Objects during Navigation in Unstructured Environments with Movable Obstacles</article-title><source>Adv. Intell. Syst.</source><year>2024</year><volume>6</volume><fpage>21</fpage><pub-id pub-id-type="doi">10.1002/aisy.202300621</pub-id></element-citation></ref><ref id="B33-sensors-25-01248"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Al-Mallah</surname><given-names>M.</given-names></name>
<name><surname>Ali</surname><given-names>M.</given-names></name>
<name><surname>Al-Khawaldeh</surname><given-names>M.</given-names></name>
</person-group><article-title>Obstacles Avoidance for Mobile Robot Using Type-2 Fuzzy Logic Controller</article-title><source>Robotics</source><year>2022</year><volume>11</volume><elocation-id>130</elocation-id><pub-id pub-id-type="doi">10.3390/robotics11060130</pub-id></element-citation></ref><ref id="B34-sensors-25-01248"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wondosen</surname><given-names>A.</given-names></name>
<name><surname>Shiferaw</surname><given-names>D.</given-names></name>
</person-group><article-title>Fuzzy Logic Controller Design for Mobile Robot Outdoor Navigation</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">2401.01756</pub-id></element-citation></ref><ref id="B35-sensors-25-01248"><label>35.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Sabra</surname><given-names>M.</given-names></name>
<name><surname>Tayeh</surname><given-names>N.</given-names></name>
</person-group><article-title>Maze Solver Robot</article-title><year>2024</year><comment>Available online: <ext-link xlink:href="https://hdl.handle.net/20.500.11888/18671" ext-link-type="uri">https://hdl.handle.net/20.500.11888/18671</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-15">(accessed on 15 December 2024)</date-in-citation></element-citation></ref><ref id="B36-sensors-25-01248"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>K.</given-names></name>
<name><surname>Kim</surname><given-names>J.</given-names></name>
<name><surname>Jiang</surname><given-names>X.</given-names></name>
<name><surname>Kim</surname><given-names>T.</given-names></name>
</person-group><article-title>Static Force Measurement Using Piezoelectric Sensors</article-title><source>J. Sens.</source><year>2021</year><volume>2021</volume><fpage>6664200</fpage><pub-id pub-id-type="doi">10.1155/2021/6664200</pub-id></element-citation></ref><ref id="B37-sensors-25-01248"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kong</surname><given-names>Y.</given-names></name>
<name><surname>Cheng</surname><given-names>G.</given-names></name>
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
<name><surname>Meng</surname><given-names>W.</given-names></name>
<name><surname>Tian</surname><given-names>X.</given-names></name>
<name><surname>Sun</surname><given-names>B.</given-names></name>
<name><surname>Yang</surname><given-names>F.</given-names></name>
<name><surname>Wei</surname><given-names>D.</given-names></name>
</person-group><article-title>Highly Efficient Recognition of Similar Objects Based on Ionic Robotic Tactile Sensors</article-title><source>Sci. Bull.</source><year>2024</year><volume>69</volume><fpage>2089</fpage><lpage>2098</lpage><pub-id pub-id-type="doi">10.1016/j.scib.2024.04.060</pub-id></element-citation></ref><ref id="B38-sensors-25-01248"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Sun</surname><given-names>F.</given-names></name>
<name><surname>Bao</surname><given-names>L.</given-names></name>
<name><surname>Shan</surname><given-names>J.</given-names></name>
<name><surname>Gao</surname><given-names>Y.</given-names></name>
<name><surname>Fang</surname><given-names>B.</given-names></name>
</person-group><article-title>A Compact Visuo-Tactile Robotic Skin for Micron-Level Tactile Perception</article-title><source>IEEE Sens. J.</source><year>2024</year><volume>24</volume><fpage>15273</fpage><lpage>15282</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2024.3376574</pub-id></element-citation></ref><ref id="B39-sensors-25-01248"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Verellen</surname><given-names>T.</given-names></name>
<name><surname>Kerstens</surname><given-names>R.</given-names></name>
<name><surname>Steckel</surname><given-names>J.</given-names></name>
</person-group><article-title>High-Resolution Ultrasound Sensing for Robotics Using Dense Microphone Arrays</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>190083</fpage><lpage>190093</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3032177</pub-id></element-citation></ref><ref id="B40-sensors-25-01248"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Okuda</surname><given-names>K.</given-names></name>
<name><surname>Miyake</surname><given-names>M.</given-names></name>
<name><surname>Takai</surname><given-names>H.</given-names></name>
<name><surname>Tachibana</surname><given-names>K.</given-names></name>
</person-group><article-title>Obstacle Arrangement Detection Using Multichannel Ultrasonic Sonar for Indoor Mobile Robots</article-title><source>Artif. Life Robot.</source><year>2010</year><volume>15</volume><fpage>229</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1007/s10015-010-0799-2</pub-id></element-citation></ref><ref id="B41-sensors-25-01248"><label>41.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Nair</surname><given-names>S.</given-names></name>
<name><surname>Joladarashi</surname><given-names>S.</given-names></name>
<name><surname>Ganesh</surname><given-names>N.</given-names></name>
</person-group><article-title>Evaluation of Ultrasonic Sensor in Robot Mapping</article-title><source>Proceedings of the 2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)</source><conf-loc>Tirunelveli, India</conf-loc><conf-date>23&#x02013;25 April 2019</conf-date></element-citation></ref><ref id="B42-sensors-25-01248"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Q.</given-names></name>
<name><surname>Zhu</surname><given-names>H.</given-names></name>
</person-group><article-title>Performance Evaluation of 2D LiDAR SLAM Algorithms in Simulated Orchard Environments</article-title><source>Comput. Electron. Agric.</source><year>2024</year><volume>221</volume><fpage>108994</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2024.108994</pub-id></element-citation></ref><ref id="B43-sensors-25-01248"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Belkin</surname><given-names>I.</given-names></name>
<name><surname>Abramenko</surname><given-names>A.</given-names></name>
<name><surname>Yudin</surname><given-names>D.</given-names></name>
</person-group><article-title>Real-Time Lidar-Based Localization of Mobile Ground Robot</article-title><source>Procedia Comput. Sci.</source><year>2021</year><volume>186</volume><fpage>440</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2021.04.164</pub-id></element-citation></ref><ref id="B44-sensors-25-01248"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Yin</surname><given-names>Y.</given-names></name>
<name><surname>Jing</surname><given-names>Q.</given-names></name>
</person-group><article-title>Comparative Analysis of 3D LiDAR Scan-Matching Methods for State Estimation of Autonomous Surface Vessel</article-title><source>J. Mar. Sci. Eng.</source><year>2023</year><volume>11</volume><elocation-id>840</elocation-id><pub-id pub-id-type="doi">10.3390/jmse11040840</pub-id></element-citation></ref><ref id="B45-sensors-25-01248"><label>45.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Adams</surname><given-names>M.</given-names></name>
<name><surname>Jose</surname><given-names>E.</given-names></name>
<name><surname>Vo</surname><given-names>B.-N.</given-names></name>
</person-group><source>Robotic Navigation and Mapping with Radar</source><publisher-name>Artech</publisher-name><publisher-loc>Morristown, NJ, USA</publisher-loc><year>2012</year><isbn>9781608074839</isbn></element-citation></ref><ref id="B46-sensors-25-01248"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Aucone</surname><given-names>E.</given-names></name>
<name><surname>Sferrazza</surname><given-names>C.</given-names></name>
<name><surname>Gregor</surname><given-names>M.</given-names></name>
<name><surname>D&#x02019;Andrea</surname><given-names>R.</given-names></name>
<name><surname>Mintchev</surname><given-names>S.</given-names></name>
</person-group><article-title>Optical Tactile Sensing for Aerial Multi-Contact Interaction: Design, Integration, and Evaluation</article-title><source>IEEE Trans. Robot.</source><year>2024</year><volume>41</volume><fpage>364</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1109/TRO.2024.3508140</pub-id></element-citation></ref><ref id="B47-sensors-25-01248"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Omar</surname><given-names>E.Z.</given-names></name>
<name><surname>Al-Tahhan</surname><given-names>F.E.</given-names></name>
</person-group><article-title>A Novel Hybrid Model Based on Integrating RGB and YCrCb Color Spaces for Demodulating the Phase Map of Fibres Using a Color Phase-Shifting Profilometry Technique</article-title><source>Optik</source><year>2024</year><volume>306</volume><fpage>171792</fpage><pub-id pub-id-type="doi">10.1016/j.ijleo.2024.171792</pub-id></element-citation></ref><ref id="B48-sensors-25-01248"><label>48.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Maitlo</surname><given-names>N.</given-names></name>
<name><surname>Noonari</surname><given-names>N.</given-names></name>
<name><surname>Arshid</surname><given-names>K.</given-names></name>
<name><surname>Ahmed</surname><given-names>N.</given-names></name>
<name><surname>Duraisamy</surname><given-names>S.</given-names></name>
</person-group><article-title>AINS: Affordable Indoor Navigation Solution via Line Color Identification Using Mono-Camera for Autonomous Vehicles</article-title><source>Proceedings of the IEEE 9th International Conference for Convergence in Technology (I2CT)</source><conf-loc>Pune, India</conf-loc><conf-date>5&#x02013;7 April 2024</conf-date></element-citation></ref><ref id="B49-sensors-25-01248"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lan</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>E.</given-names></name>
<name><surname>Jung</surname><given-names>C.</given-names></name>
</person-group><article-title>Face Reflection Removal Network Using Multispectral Fusion of RGB and NIR Images</article-title><source>IEEE Open J. Signal Process.</source><year>2024</year><volume>5</volume><fpage>383</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1109/OJSP.2024.3351472</pub-id></element-citation></ref><ref id="B50-sensors-25-01248"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Ma</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>L.</given-names></name>
</person-group><article-title>Achieving Widely Distributed Feature Matches Using Flattened-Affine-SIFT Algorithm for Fisheye Images</article-title><source>Opt. Express</source><year>2024</year><volume>32</volume><fpage>7969</fpage><pub-id pub-id-type="doi">10.1364/OE.513531</pub-id><pub-id pub-id-type="pmid">38439466</pub-id>
</element-citation></ref><ref id="B51-sensors-25-01248"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Moreau</surname><given-names>J.</given-names></name>
<name><surname>Ibanez-guzman</surname><given-names>J.</given-names></name>
</person-group><article-title>Emergent Visual Sensors for Autonomous Vehicles</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2023</year><volume>24</volume><fpage>4716</fpage><lpage>4737</lpage><pub-id pub-id-type="doi">10.1109/TITS.2023.3248483</pub-id></element-citation></ref><ref id="B52-sensors-25-01248"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tychola</surname><given-names>K.A.</given-names></name>
<name><surname>Tsimperidis</surname><given-names>I.</given-names></name>
<name><surname>Papakostas</surname><given-names>G.A.</given-names></name>
</person-group><article-title>On 3D Reconstruction Using RGB-D Cameras</article-title><source>Digital</source><year>2022</year><volume>2</volume><fpage>401</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.3390/digital2030022</pub-id></element-citation></ref><ref id="B53-sensors-25-01248"><label>53.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Varghese</surname><given-names>G.</given-names></name>
<name><surname>Reddy</surname><given-names>T.G.C.</given-names></name>
<name><surname>Menon</surname><given-names>A.K.</given-names></name>
<name><surname>Paul</surname><given-names>A.</given-names></name>
<name><surname>Kochuvila</surname><given-names>S.</given-names></name>
<name><surname>Varma Divya</surname><given-names>R.</given-names></name>
<name><surname>Bhat</surname><given-names>R.</given-names></name>
<name><surname>Kumar</surname><given-names>N.</given-names></name>
</person-group><article-title>Multi-Robot System for Mapping and Localization</article-title><source>Proceedings of the 2023 8th International Conference on Robotics and Automation Engineering (ICRAE)</source><conf-loc>Singapore</conf-loc><conf-date>17&#x02013;19 November 2023</conf-date><fpage>79</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1109/ICRAE59816.2023.10458504</pub-id></element-citation></ref><ref id="B54-sensors-25-01248"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ghosh</surname><given-names>D.K.</given-names></name>
<name><surname>Jung</surname><given-names>Y.J.</given-names></name>
</person-group><article-title>Two-Stage Cross-Fusion Network for Stereo Event-Based Depth Estimation</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>241</volume><fpage>122743</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2023.122743</pub-id></element-citation></ref><ref id="B55-sensors-25-01248"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>T.</given-names></name>
<name><surname>Lim</surname><given-names>S.</given-names></name>
<name><surname>Shin</surname><given-names>G.</given-names></name>
<name><surname>Sim</surname><given-names>G.</given-names></name>
<name><surname>Yun</surname><given-names>D.</given-names></name>
</person-group><article-title>An Open-Source Low-Cost Mobile Robot System with an RGB-D Camera and Efficient Real-Time Navigation Algorithm</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>127871</fpage><lpage>127881</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2022.3226784</pub-id></element-citation></ref><ref id="B56-sensors-25-01248"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Canovas</surname><given-names>B.</given-names></name>
<name><surname>N&#x000e8;gre</surname><given-names>A.</given-names></name>
<name><surname>Rombaut</surname><given-names>M.</given-names></name>
</person-group><article-title>Onboard Dynamic RGB-D Simultaneous Localization and Mapping for Mobile Robot Navigation</article-title><source>ETRI J.</source><year>2021</year><volume>43</volume><fpage>617</fpage><lpage>629</lpage><pub-id pub-id-type="doi">10.4218/etrij.2021-0061</pub-id></element-citation></ref><ref id="B57-sensors-25-01248"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Abukhalil</surname><given-names>T.</given-names></name>
<name><surname>Alksasbeh</surname><given-names>M.</given-names></name>
<name><surname>Alqaralleh</surname><given-names>B.</given-names></name>
<name><surname>Abukaraki</surname><given-names>A.</given-names></name>
</person-group><article-title>Robot Navigation System Using Laser and a Monocular Camera</article-title><source>J. Theor. Appl. Inf. Technol.</source><year>2020</year><volume>98</volume><fpage>714</fpage><lpage>724</lpage></element-citation></ref><ref id="B58-sensors-25-01248"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tsujimura</surname><given-names>T.</given-names></name>
<name><surname>Minato</surname><given-names>Y.</given-names></name>
<name><surname>Izumi</surname><given-names>K.</given-names></name>
</person-group><article-title>Shape Recognition of Laser Beam Trace for Human-Robot Interface</article-title><source>Pattern Recognit. Lett.</source><year>2013</year><volume>34</volume><fpage>1928</fpage><lpage>1935</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2013.03.023</pub-id></element-citation></ref><ref id="B59-sensors-25-01248"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Romero-Godoy</surname><given-names>D.</given-names></name>
<name><surname>S&#x000e1;nchez-Rodr&#x000ed;guez</surname><given-names>D.</given-names></name>
<name><surname>Alonso-Gonz&#x000e1;lez</surname><given-names>I.</given-names></name>
<name><surname>Delgado-Raj&#x000f3;</surname><given-names>F.</given-names></name>
</person-group><article-title>A Low Cost Collision Avoidance System Based on a ToF Camera for SLAM Approaches</article-title><source>Rev. Tecnol. Marcha</source><year>2022</year><volume>35</volume><fpage>137</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.18845/tm.v35i8.6465</pub-id></element-citation></ref><ref id="B60-sensors-25-01248"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Iaboni</surname><given-names>C.</given-names></name>
<name><surname>Lobo</surname><given-names>D.</given-names></name>
<name><surname>Choi</surname><given-names>J.W.</given-names></name>
<name><surname>Abichandani</surname><given-names>P.</given-names></name>
</person-group><article-title>Event-Based Motion Capture System for Online Multi-Quadrotor Localization and Tracking</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>3240</elocation-id><pub-id pub-id-type="doi">10.3390/s22093240</pub-id><pub-id pub-id-type="pmid">35590931</pub-id>
</element-citation></ref><ref id="B61-sensors-25-01248"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>P.</given-names></name>
<name><surname>Ergu</surname><given-names>D.</given-names></name>
<name><surname>Liu</surname><given-names>F.</given-names></name>
<name><surname>Cai</surname><given-names>Y.</given-names></name>
<name><surname>Ma</surname><given-names>B.</given-names></name>
</person-group><article-title>A Review of Yolo Algorithm Developments</article-title><source>Procedia Comput. Sci.</source><year>2021</year><volume>199</volume><fpage>1066</fpage><lpage>1073</lpage><pub-id pub-id-type="doi">10.1016/j.procs.2022.01.135</pub-id></element-citation></ref><ref id="B62-sensors-25-01248"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Lan</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>T.</given-names></name>
</person-group><article-title>Human Figure Detection in Han Portrait Stone Images via Enhanced YOLO-V5</article-title><source>Herit. Sci.</source><year>2024</year><volume>12</volume><fpage>119</fpage><pub-id pub-id-type="doi">10.1186/s40494-024-01232-2</pub-id></element-citation></ref><ref id="B63-sensors-25-01248"><label>63.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Plastiras</surname><given-names>G.</given-names></name>
<name><surname>Kyrkou</surname><given-names>C.</given-names></name>
<name><surname>Theocharides</surname><given-names>T.</given-names></name>
</person-group><article-title>Efficient Convnet-Based Object Detection for Unmanned Aerial Vehicles by Selective Tile Processing</article-title><source>Proceedings of the ICDSC &#x02018;18: Proceedings of the 12th International Conference on Distributed Smart Cameras</source><conf-loc>Eindhoven, The Netherlands</conf-loc><conf-date>3&#x02013;4 September 2018</conf-date><pub-id pub-id-type="doi">10.1145/3243394.3243692</pub-id></element-citation></ref><ref id="B64-sensors-25-01248"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hussain</surname><given-names>M.</given-names></name>
</person-group><article-title>YOLOv1 to v8: Unveiling Each Variant-A Comprehensive Review of YOLO</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>42816</fpage><lpage>42833</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3378568</pub-id></element-citation></ref><ref id="B65-sensors-25-01248"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Verma</surname><given-names>T.</given-names></name>
<name><surname>Singh</surname><given-names>J.</given-names></name>
<name><surname>Bhartari</surname><given-names>Y.</given-names></name>
<name><surname>Jarwal</surname><given-names>R.</given-names></name>
<name><surname>Singh</surname><given-names>S.</given-names></name>
<name><surname>Singh</surname><given-names>S.</given-names></name>
</person-group><article-title>SOAR: Advancements in Small Body Object Detection for Aerial Imagery Using State Space Models and Programmable Gradients</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2405.01699</pub-id></element-citation></ref><ref id="B66-sensors-25-01248"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Minz</surname><given-names>P.S.</given-names></name>
<name><surname>Saini</surname><given-names>C.S.</given-names></name>
</person-group><article-title>RGB Camera-Based Image Technique for Color Measurement of Flavored Milk</article-title><source>Meas. Food</source><year>2021</year><volume>4</volume><fpage>100012</fpage><pub-id pub-id-type="doi">10.1016/j.meafoo.2021.100012</pub-id></element-citation></ref><ref id="B67-sensors-25-01248"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sohl</surname><given-names>M.A.</given-names></name>
<name><surname>Mahmood</surname><given-names>S.A.</given-names></name>
</person-group><article-title>Low-Cost UAV in Photogrammetric Engineering and Remote Sensing: Georeferencing, DEM Accuracy, and Geospatial Analysis</article-title><source>J. Geovisualization Spat. Anal.</source><year>2024</year><volume>8</volume><fpage>14</fpage><pub-id pub-id-type="doi">10.1007/s41651-024-00176-2</pub-id></element-citation></ref><ref id="B68-sensors-25-01248"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Haruta</surname><given-names>M.</given-names></name>
<name><surname>Kikkawa</surname><given-names>J.</given-names></name>
<name><surname>Kimoto</surname><given-names>K.</given-names></name>
<name><surname>Kurata</surname><given-names>H.</given-names></name>
</person-group><article-title>Comparison of Detection Limits of Direct-Counting CMOS and CCD Cameras in EELS Experiments</article-title><source>Ultramicroscopy</source><year>2022</year><volume>240</volume><fpage>113577</fpage><pub-id pub-id-type="doi">10.1016/j.ultramic.2022.113577</pub-id><pub-id pub-id-type="pmid">35728341</pub-id>
</element-citation></ref><ref id="B69-sensors-25-01248"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>&#x000dc;nal</surname><given-names>Z.</given-names></name>
<name><surname>K&#x00131;z&#x00131;ldeniz</surname><given-names>T.</given-names></name>
<name><surname>&#x000d6;zden</surname><given-names>M.</given-names></name>
<name><surname>Akta&#x0015f;</surname><given-names>H.</given-names></name>
<name><surname>Karag&#x000f6;z</surname><given-names>&#x000d6;.</given-names></name>
</person-group><article-title>Detection of Bruises on Red Apples Using Deep Learning Models</article-title><source>Sci. Hortic.</source><year>2024</year><volume>329</volume><fpage>113021</fpage><pub-id pub-id-type="doi">10.1016/j.scienta.2024.113021</pub-id></element-citation></ref><ref id="B70-sensors-25-01248"><label>70.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Furmonas</surname><given-names>J.</given-names></name>
<name><surname>Liobe</surname><given-names>J.</given-names></name>
<name><surname>Barzdenas</surname><given-names>V.</given-names></name>
</person-group><article-title>Analytical Review of Event-Based Camera Depth Estimation Methods and Systems</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>1201</elocation-id><pub-id pub-id-type="doi">10.3390/s22031201</pub-id><pub-id pub-id-type="pmid">35161946</pub-id>
</element-citation></ref><ref id="B71-sensors-25-01248"><label>71.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Hidalgo-Carrio</surname><given-names>J.</given-names></name>
<name><surname>Gehrig</surname><given-names>D.</given-names></name>
<name><surname>Scaramuzza</surname><given-names>D.</given-names></name>
</person-group><article-title>Learning Monocular Dense Depth from Events</article-title><source>Proceedings of the 2020 International Conference on 3D Vision (3DV)</source><conf-loc>Fukuoka, Japan</conf-loc><conf-date>25&#x02013;28 November 2020</conf-date><fpage>534</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1109/3DV50981.2020.00063</pub-id></element-citation></ref><ref id="B72-sensors-25-01248"><label>72.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fan</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Jiang</surname><given-names>C.</given-names></name>
<name><surname>Wu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Unsupervised Depth Completion and Denoising for RGB-D Sensors</article-title><source>Proceedings of the 2022 International Conference on Robotics and Automation (ICRA)</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>23&#x02013;27 May 2022</conf-date><fpage>8734</fpage><lpage>8740</lpage><pub-id pub-id-type="doi">10.1109/ICRA46639.2022.9812392</pub-id></element-citation></ref><ref id="B73-sensors-25-01248"><label>73.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Miranda</surname><given-names>J.C.</given-names></name>
<name><surname>Arn&#x000f3;</surname><given-names>J.</given-names></name>
<name><surname>Gen&#x000e9;-Mola</surname><given-names>J.</given-names></name>
<name><surname>Lordan</surname><given-names>J.</given-names></name>
<name><surname>As&#x000ed;n</surname><given-names>L.</given-names></name>
<name><surname>Gregorio</surname><given-names>E.</given-names></name>
</person-group><article-title>Assessing Automatic Data Processing Algorithms for RGB-D Cameras to Predict Fruit Size and Weight in Apples</article-title><source>Comput. Electron. Agric.</source><year>2023</year><volume>214</volume><fpage>108302</fpage><pub-id pub-id-type="doi">10.1016/j.compag.2023.108302</pub-id></element-citation></ref><ref id="B74-sensors-25-01248"><label>74.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>S.</given-names></name>
<name><surname>Yoon</surname><given-names>S.C.</given-names></name>
<name><surname>Li</surname><given-names>L.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Zhuang</surname><given-names>H.</given-names></name>
<name><surname>Wei</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group><article-title>Recognition and Positioning of Fresh Tea Buds Using YOLOv4-Lighted + ICBAM Model and RGB-D Sensing</article-title><source>Agriculture</source><year>2023</year><volume>13</volume><elocation-id>518</elocation-id><pub-id pub-id-type="doi">10.3390/agriculture13030518</pub-id></element-citation></ref><ref id="B75-sensors-25-01248"><label>75.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Osvaldov&#x000e1;</surname><given-names>K.</given-names></name>
<name><surname>Gajdo&#x00161;ech</surname><given-names>L.</given-names></name>
<name><surname>Kocur</surname><given-names>V.</given-names></name>
<name><surname>Madaras</surname><given-names>M.</given-names></name>
</person-group><article-title>Enhancement of 3D Camera Synthetic Training Data with Noise Models</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2402.16514</pub-id></element-citation></ref><ref id="B76-sensors-25-01248"><label>76.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hou</surname><given-names>C.</given-names></name>
<name><surname>Qiao</surname><given-names>T.</given-names></name>
<name><surname>Dong</surname><given-names>H.</given-names></name>
<name><surname>Wu</surname><given-names>H.</given-names></name>
</person-group><article-title>Coal Flow Volume Detection Method for Conveyor Belt Based on TOF Vision</article-title><source>Meas. J. Int. Meas. Confed.</source><year>2024</year><volume>229</volume><fpage>114468</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2024.114468</pub-id></element-citation></ref><ref id="B77-sensors-25-01248"><label>77.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Horaud</surname><given-names>R.</given-names></name>
<name><surname>Hansard</surname><given-names>M.</given-names></name>
<name><surname>Evangelidis</surname><given-names>G.</given-names></name>
<name><surname>M&#x000e9;nier</surname><given-names>C.</given-names></name>
</person-group><article-title>An Overview of Depth Cameras and Range Scanners Based on Time-of-Flight Technologies</article-title><source>Mach. Vis. Appl.</source><year>2016</year><volume>27</volume><fpage>1005</fpage><lpage>1020</lpage><pub-id pub-id-type="doi">10.1007/s00138-016-0784-4</pub-id></element-citation></ref><ref id="B78-sensors-25-01248"><label>78.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Condotta</surname><given-names>I.C.F.S.</given-names></name>
<name><surname>Brown-Brandl</surname><given-names>T.M.</given-names></name>
<name><surname>Pitla</surname><given-names>S.K.</given-names></name>
<name><surname>Stinn</surname><given-names>J.P.</given-names></name>
<name><surname>Silva-Miranda</surname><given-names>K.O.</given-names></name>
</person-group><article-title>Evaluation of Low-Cost Depth Cameras for Agricultural Applications</article-title><source>Biol. Syst. Eng.</source><year>2020</year><volume>173</volume><elocation-id>105394</elocation-id><pub-id pub-id-type="doi">10.1016/j.compag.2020.105394</pub-id></element-citation></ref><ref id="B79-sensors-25-01248"><label>79.</label><element-citation publication-type="other"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>X.-F.</given-names></name>
<name><surname>Xu</surname><given-names>T.</given-names></name>
<name><surname>Wu</surname><given-names>X.-J.</given-names></name>
</person-group><article-title>Adaptive Colour-Depth Aware Attention for RGB-D Object Tracking</article-title><source>IEEE Signal Process. Lett.</source><year>2024</year><comment>
<italic toggle="yes">early access</italic>
</comment><pub-id pub-id-type="doi">10.1109/LSP.2024.3376960</pub-id></element-citation></ref><ref id="B80-sensors-25-01248"><label>80.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mac</surname><given-names>T.T.</given-names></name>
<name><surname>Lin</surname><given-names>C.Y.</given-names></name>
<name><surname>Huan</surname><given-names>N.G.</given-names></name>
<name><surname>Nhat</surname><given-names>L.D.</given-names></name>
<name><surname>Hoang</surname><given-names>P.C.</given-names></name>
<name><surname>Hai</surname><given-names>H.H.</given-names></name>
</person-group><article-title>Hybrid Slam-Based Exploration of a Mobile Robot for 3d Scenario Reconstruction and Autonomous Navigation</article-title><source>Acta Polytech. Hung.</source><year>2021</year><volume>18</volume><fpage>197</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.12700/APH.18.6.2021.6.11</pub-id></element-citation></ref><ref id="B81-sensors-25-01248"><label>81.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gomez-Rosal</surname><given-names>D.A.</given-names></name>
<name><surname>Bergau</surname><given-names>M.</given-names></name>
<name><surname>Fischer</surname><given-names>G.K.J.</given-names></name>
<name><surname>Wachaja</surname><given-names>A.</given-names></name>
<name><surname>Grater</surname><given-names>J.</given-names></name>
<name><surname>Odenweller</surname><given-names>M.</given-names></name>
<name><surname>Piechottka</surname><given-names>U.</given-names></name>
<name><surname>Hoeflinger</surname><given-names>F.</given-names></name>
<name><surname>Gosala</surname><given-names>N.</given-names></name>
<name><surname>Wetzel</surname><given-names>N.</given-names></name>
<etal/>
</person-group><article-title>A Smart Robotic System for Industrial Plant Supervision</article-title><source>Proceedings of the 2023 IEEE SENSORS</source><conf-loc>Vienna, Austria</conf-loc><conf-date>29 October&#x02013;1 November 2023</conf-date><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1109/SENSORS56945.2023.10325162</pub-id></element-citation></ref><ref id="B82-sensors-25-01248"><label>82.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>X.</given-names></name>
<name><surname>Dong</surname><given-names>X.</given-names></name>
<name><surname>Ma</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>K.</given-names></name>
<name><surname>Ahmed</surname><given-names>S.</given-names></name>
<name><surname>Lin</surname><given-names>J.</given-names></name>
<name><surname>Qiu</surname><given-names>B.</given-names></name>
</person-group><article-title>The Improved A* Obstacle Avoidance Algorithm for the Plant Protection UAV with Millimeter Wave Radar and Monocular Camera Data Fusion</article-title><source>Remote Sens.</source><year>2021</year><volume>13</volume><elocation-id>3364</elocation-id><pub-id pub-id-type="doi">10.3390/rs13173364</pub-id></element-citation></ref><ref id="B83-sensors-25-01248"><label>83.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Chaki</surname><given-names>N.</given-names></name>
<name><surname>Devarakonda</surname><given-names>N.</given-names></name>
<name><surname>Cortesi</surname><given-names>A.</given-names></name>
<name><surname>Seetha</surname><given-names>H.</given-names></name>
</person-group><source>Proceedings of International Conference on Computational Intelligence and Data Engineering: ICCIDE 2021 (Lecture Notes on Data Engineering and Communications Technologies)</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2021</year><isbn>9789811671814</isbn></element-citation></ref><ref id="B84-sensors-25-01248"><label>84.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Saucedo</surname><given-names>M.A.V.</given-names></name>
<name><surname>Patel</surname><given-names>A.</given-names></name>
<name><surname>Sawlekar</surname><given-names>R.</given-names></name>
<name><surname>Saradagi</surname><given-names>A.</given-names></name>
<name><surname>Kanellakis</surname><given-names>C.</given-names></name>
<name><surname>Agha-Mohammadi</surname><given-names>A.A.</given-names></name>
<name><surname>Nikolakopoulos</surname><given-names>G.</given-names></name>
</person-group><article-title>Event Camera and LiDAR Based Human Tracking for Adverse Lighting Conditions in Subterranean Environments</article-title><source>IFAC-PapersOnLine</source><year>2023</year><volume>56</volume><fpage>9257</fpage><lpage>9262</lpage><pub-id pub-id-type="doi">10.1016/j.ifacol.2023.10.008</pub-id></element-citation></ref><ref id="B85-sensors-25-01248"><label>85.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Le</surname><given-names>N.M.D.</given-names></name>
<name><surname>Nguyen</surname><given-names>N.H.</given-names></name>
<name><surname>Nguyen</surname><given-names>D.A.</given-names></name>
<name><surname>Ngo</surname><given-names>T.D.</given-names></name>
<name><surname>Ho</surname><given-names>V.A.</given-names></name>
</person-group><article-title>ViART: Vision-Based Soft Tactile Sensing for Autonomous Robotic Vehicles</article-title><source>IEEE/ASME Trans. Mechatron.</source><year>2023</year><volume>29</volume><fpage>1420</fpage><lpage>1430</lpage><pub-id pub-id-type="doi">10.1109/TMECH.2023.3301022</pub-id></element-citation></ref><ref id="B86-sensors-25-01248"><label>86.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cai</surname><given-names>Y.</given-names></name>
<name><surname>Ou</surname><given-names>Y.</given-names></name>
<name><surname>Qin</surname><given-names>T.</given-names></name>
</person-group><article-title>Improving SLAM Techniques with Integrated Multi-Sensor Fusion for 3D Reconstruction</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>2033</elocation-id><pub-id pub-id-type="doi">10.3390/s24072033</pub-id><pub-id pub-id-type="pmid">38610245</pub-id>
</element-citation></ref><ref id="B87-sensors-25-01248"><label>87.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Xiong</surname><given-names>F.</given-names></name>
<name><surname>Xu</surname><given-names>M.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Zuo</surname><given-names>X.</given-names></name>
<name><surname>Lv</surname><given-names>J.</given-names></name>
</person-group><article-title>Gaussian-LIC: Photo-Realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2404.06926</pub-id></element-citation></ref><ref id="B88-sensors-25-01248"><label>88.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bhattacharjee</surname><given-names>T.</given-names></name>
<name><surname>Shenoi</surname><given-names>A.A.</given-names></name>
<name><surname>Park</surname><given-names>D.</given-names></name>
<name><surname>Rehg</surname><given-names>J.M.</given-names></name>
<name><surname>Kemp</surname><given-names>C.C.</given-names></name>
</person-group><article-title>Combining Tactile Sensing and Vision for Rapid Haptic Mapping</article-title><source>Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><conf-loc>Hamburg, Germany</conf-loc><conf-date>28 September&#x02013;2 October 2015</conf-date><fpage>1200</fpage><lpage>1207</lpage><pub-id pub-id-type="doi">10.1109/IROS.2015.7353522</pub-id></element-citation></ref><ref id="B89-sensors-25-01248"><label>89.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>&#x000c1;lvarez</surname><given-names>D.</given-names></name>
<name><surname>Roa</surname><given-names>M.A.</given-names></name>
<name><surname>Moreno</surname><given-names>L.</given-names></name>
</person-group><article-title>Visual and Tactile Fusion for Estimating the Pose of a Grasped Object</article-title><source>Adv. Intell. Syst. Comput.</source><year>2020</year><volume>1093 AISC</volume><fpage>184</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-36150-1_16</pub-id></element-citation></ref><ref id="B90-sensors-25-01248"><label>90.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Naga</surname><given-names>P.S.B.</given-names></name>
<name><surname>Hari</surname><given-names>P.J.</given-names></name>
<name><surname>Sinduja</surname><given-names>R.</given-names></name>
<name><surname>Prathap</surname><given-names>S.</given-names></name>
<name><surname>Ganesan</surname><given-names>M.</given-names></name>
</person-group><article-title>Realization of SLAM and Object Detection Using Ultrasonic Sensor and RGB-HD Camera</article-title><source>Proceedings of the 2022 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET)</source><conf-loc>Chennai, India</conf-loc><conf-date>24&#x02013;26 March 2022</conf-date><fpage>167</fpage><lpage>171</lpage></element-citation></ref><ref id="B91-sensors-25-01248"><label>91.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>Luo</surname><given-names>L.</given-names></name>
</person-group><article-title>Multi-Feature Fusion Tree Trunk Detection and Orchard Mobile Robot Localization Using Camera/Ultrasonic Sensors</article-title><source>Comput. Electron. Agric.</source><year>2018</year><volume>147</volume><fpage>91</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1016/j.compag.2018.02.009</pub-id></element-citation></ref><ref id="B92-sensors-25-01248"><label>92.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>Z.</given-names></name>
<name><surname>Gao</surname><given-names>Z.</given-names></name>
<name><surname>Chen</surname><given-names>B.M.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
</person-group><article-title>Accurate LiDAR-Camera Fused Odometry and RGB-Colored Mapping</article-title><source>IEEE Robot. Autom. Lett.</source><year>2024</year><volume>9</volume><fpage>2495</fpage><lpage>2502</lpage><pub-id pub-id-type="doi">10.1109/LRA.2024.3356982</pub-id></element-citation></ref><ref id="B93-sensors-25-01248"><label>93.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>You</surname><given-names>H.</given-names></name>
<name><surname>Xu</surname><given-names>F.</given-names></name>
<name><surname>Ye</surname><given-names>Y.</given-names></name>
<name><surname>Xia</surname><given-names>P.</given-names></name>
<name><surname>Du</surname><given-names>J.</given-names></name>
</person-group><article-title>Adaptive LiDAR Scanning Based on RGB Information</article-title><source>Autom. Constr.</source><year>2024</year><volume>160</volume><fpage>105337</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2024.105337</pub-id></element-citation></ref><ref id="B94-sensors-25-01248"><label>94.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jing</surname><given-names>J.</given-names></name>
</person-group><article-title>Simulation Analysis of Fire-Fighting Path Planning Based On SLAM</article-title><source>Highlights Sci. Eng. Technol.</source><year>2024</year><volume>85</volume><fpage>434</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.54097/35ybya58</pub-id></element-citation></ref><ref id="B95-sensors-25-01248"><label>95.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>C.J.</given-names></name>
<name><surname>Ogawa</surname><given-names>S.</given-names></name>
<name><surname>Hayashi</surname><given-names>T.</given-names></name>
<name><surname>Janthori</surname><given-names>T.</given-names></name>
<name><surname>Tominaga</surname><given-names>A.</given-names></name>
<name><surname>Hayashi</surname><given-names>E.</given-names></name>
</person-group><article-title>3D Semantic Mapping Based on RGB-D Camera and LiDAR Sensor in Beach Environment</article-title><source>Proceedings of the 2024 1st International Conference on Robotics, Engineering, Science, and Technology (RESTCON)</source><conf-loc>Pattaya, Thailand</conf-loc><conf-date>16&#x02013;18 February 2024</conf-date><fpage>21</fpage><lpage>26</lpage></element-citation></ref><ref id="B96-sensors-25-01248"><label>96.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qiao</surname><given-names>G.</given-names></name>
<name><surname>Ning</surname><given-names>N.</given-names></name>
<name><surname>Zuo</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>P.</given-names></name>
<name><surname>Sun</surname><given-names>M.</given-names></name>
<name><surname>Hu</surname><given-names>S.</given-names></name>
<name><surname>Yu</surname><given-names>Q.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Spatio-Temporal Fusion Spiking Neural Network for Frame-Based and Event-Based Camera Sensor Fusion</article-title><source>IEEE Trans. Emerg. Top. Comput. Intell.</source><year>2024</year><volume>8</volume><fpage>2446</fpage><lpage>2456</lpage><pub-id pub-id-type="doi">10.1109/TETCI.2024.3363071</pub-id></element-citation></ref><ref id="B97-sensors-25-01248"><label>97.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zuo</surname><given-names>Y.F.</given-names></name>
<name><surname>Xu</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Kneip</surname><given-names>L.</given-names></name>
</person-group><article-title>Cross-Modal Semidense 6-DOF Tracking of an Event Camera in Challenging Conditions</article-title><source>IEEE Trans. Robot.</source><year>2024</year><volume>40</volume><fpage>1600</fpage><lpage>1616</lpage><pub-id pub-id-type="doi">10.1109/TRO.2024.3355370</pub-id></element-citation></ref><ref id="B98-sensors-25-01248"><label>98.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yadav</surname><given-names>R.</given-names></name>
<name><surname>Vierling</surname><given-names>A.</given-names></name>
<name><surname>Berns</surname><given-names>K.</given-names></name>
</person-group><article-title>Radar+RGB Attentive Fusion for Robust Object Detection in Autonomous Vehicles</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">2008.13642</pub-id></element-citation></ref><ref id="B99-sensors-25-01248"><label>99.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yao</surname><given-names>S.</given-names></name>
<name><surname>Guan</surname><given-names>R.</given-names></name>
<name><surname>Huang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Sha</surname><given-names>X.</given-names></name>
<name><surname>Yue</surname><given-names>Y.</given-names></name>
<name><surname>Lim</surname><given-names>E.G.</given-names></name>
<name><surname>Seo</surname><given-names>H.</given-names></name>
<name><surname>Man</surname><given-names>K.L.</given-names></name>
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<etal/>
</person-group><article-title>Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review</article-title><source>IEEE Trans. Intell. Veh.</source><year>2024</year><volume>9</volume><fpage>2094</fpage><lpage>2128</lpage><pub-id pub-id-type="doi">10.1109/TIV.2023.3307157</pub-id></element-citation></ref><ref id="B100-sensors-25-01248"><label>100.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<name><surname>Aeberhard</surname><given-names>M.</given-names></name>
<name><surname>Kaempchen</surname><given-names>N.</given-names></name>
</person-group><article-title>High-Level Sensor Data Fusion Architecture for Vehicle Surround Environment Perception</article-title><year>2015</year><comment>Available online: <ext-link xlink:href="https://www.researchgate.net/publication/267725657" ext-link-type="uri">https://www.researchgate.net/publication/267725657</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-12-10">(accessed on 10 December 2024)</date-in-citation></element-citation></ref><ref id="B101-sensors-25-01248"><label>101.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Thakur</surname><given-names>A.</given-names></name>
<name><surname>Mishra</surname><given-names>S.K.</given-names></name>
</person-group><article-title>An In-Depth Evaluation of Deep Learning-Enabled Adaptive Approaches for Detecting Obstacles Using Sensor-Fused Data in Autonomous Vehicles</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>133</volume><fpage>108550</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.108550</pub-id></element-citation></ref><ref id="B102-sensors-25-01248"><label>102.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>R&#x000f6;vid</surname><given-names>A.</given-names></name>
<name><surname>Remeli</surname><given-names>V.</given-names></name>
<name><surname>Szalay</surname><given-names>Z.</given-names></name>
</person-group><article-title>Raw Fusion of Camera and Sparse LiDAR for Detecting Distant Objects Fusion von Kameradaten Und Sp&#x000e4;rlichem LiDAR-Rohsignal Zur Erkennung Entfernter Objekte</article-title><source>At-Automatisierungstechnik</source><year>2020</year><volume>68</volume><fpage>337</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1515/auto-2019-0086</pub-id></element-citation></ref><ref id="B103-sensors-25-01248"><label>103.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>F.</given-names></name>
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Xu</surname><given-names>W.</given-names></name>
<name><surname>Huang</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>D.</given-names></name>
<name><surname>Cai</surname><given-names>S.</given-names></name>
<name><surname>Yang</surname><given-names>M.</given-names></name>
<name><surname>Xiong</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
</person-group><article-title>A Mobile Robot Visual SLAM System with Enhanced Semantics Segmentation</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>25442</fpage><lpage>25458</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2970238</pub-id></element-citation></ref><ref id="B104-sensors-25-01248"><label>104.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hassani</surname><given-names>S.</given-names></name>
<name><surname>Dackermann</surname><given-names>U.</given-names></name>
<name><surname>Mousavi</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>A Systematic Review of Data Fusion Techniques for Optimized Structural Health Monitoring</article-title><source>Inf. Fusion</source><year>2024</year><volume>103</volume><fpage>102136</fpage><pub-id pub-id-type="doi">10.1016/j.inffus.2023.102136</pub-id></element-citation></ref><ref id="B105-sensors-25-01248"><label>105.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Thakur</surname><given-names>A.</given-names></name>
<name><surname>Pachamuthu</surname><given-names>R.</given-names></name>
</person-group><article-title>LiDAR and Camera Raw Data Sensor Fusion in Real-Time for Obstacle Detection</article-title><source>Proceedings of the 2023 IEEE Sensors Applications Symposium (SAS)</source><conf-loc>Ottawa, ON, Canada</conf-loc><conf-date>18&#x02013;20 July 2023</conf-date></element-citation></ref><ref id="B106-sensors-25-01248"><label>106.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Risti</surname><given-names>D.</given-names></name>
<name><surname>Gao</surname><given-names>G.</given-names></name>
<name><surname>Leu</surname><given-names>A.</given-names></name>
</person-group><article-title>Low-Level Sensor Fusion-Based Human Tracking</article-title><source>Risti&#x00107;-Durrant</source><year>2016</year><volume>15</volume><fpage>17</fpage><lpage>32</lpage></element-citation></ref><ref id="B107-sensors-25-01248"><label>107.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Puriyanto</surname><given-names>R.D.</given-names></name>
<name><surname>Mustofa</surname><given-names>A.K.</given-names></name>
<name><surname>Dahlan</surname><given-names>U.A.</given-names></name>
<name><surname>Author</surname><given-names>C.</given-names></name>
</person-group><article-title>Design and Implementation of Fuzzy Logic for Obstacle Avoidance in Differential Drive Mobile Robot</article-title><source>J. Robot. Control. JRC</source><year>2024</year><volume>5</volume><fpage>132</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.18196/jrc.v5i1.20524</pub-id></element-citation></ref><ref id="B108-sensors-25-01248"><label>108.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>T.</given-names></name>
<name><surname>Member</surname><given-names>S.</given-names></name>
<name><surname>Kang</surname><given-names>G.</given-names></name>
<name><surname>Member</surname><given-names>S.</given-names></name>
</person-group><article-title>Development of an Indoor Delivery Mobile Robot for a Multi-Floor Environment</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>45202</fpage><lpage>45215</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3381489</pub-id></element-citation></ref><ref id="B109-sensors-25-01248"><label>109.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Azhar</surname><given-names>G.A.</given-names></name>
<name><surname>Kusuma</surname><given-names>A.C.</given-names></name>
<name><surname>Izza</surname><given-names>S.</given-names></name>
</person-group><article-title>Differential Drive Mobile Robot Motion Accuracy Improvement with Odometry-Compass Sensor Fusion Implementation</article-title><source>ELKHA</source><year>2023</year><volume>15</volume><fpage>24</fpage><lpage>31</lpage></element-citation></ref><ref id="B110-sensors-25-01248"><label>110.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
</person-group><article-title>Visual Perception System Design for Rock Breaking Robot Based on Multi-Sensor Fusion</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>83</volume><fpage>24795</fpage><lpage>24814</lpage><pub-id pub-id-type="doi">10.1007/s11042-023-16189-w</pub-id></element-citation></ref><ref id="B111-sensors-25-01248"><label>111.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Park</surname><given-names>G.</given-names></name>
</person-group><article-title>Optimal Vehicle Position Estimation Using Adaptive Unscented Kalman Filter Based on Sensor Fusion</article-title><source>Mechatronics</source><year>2024</year><volume>99</volume><fpage>103144</fpage><pub-id pub-id-type="doi">10.1016/j.mechatronics.2024.103144</pub-id></element-citation></ref><ref id="B112-sensors-25-01248"><label>112.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>P.</given-names></name>
<name><surname>Hu</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>T.</given-names></name>
<name><surname>Lv</surname><given-names>K.</given-names></name>
<name><surname>Guo</surname><given-names>T.</given-names></name>
<name><surname>Jiang</surname><given-names>J.</given-names></name>
<name><surname>Hu</surname><given-names>W.</given-names></name>
</person-group><article-title>Research on a Visual/Ultra-Wideband Tightly Coupled Fusion Localization Algorithm</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>1710</elocation-id><pub-id pub-id-type="doi">10.3390/s24051710</pub-id><pub-id pub-id-type="pmid">38475246</pub-id>
</element-citation></ref><ref id="B113-sensors-25-01248"><label>113.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hu</surname><given-names>K.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Kang</surname><given-names>H.</given-names></name>
<name><surname>Tang</surname><given-names>Y.</given-names></name>
</person-group><article-title>3D Vision Technologies for a Self-Developed Structural External Crack Damage Recognition Robot</article-title><source>Autom. Constr.</source><year>2024</year><volume>159</volume><fpage>105262</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2023.105262</pub-id></element-citation></ref><ref id="B114-sensors-25-01248"><label>114.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sarmento</surname><given-names>J.</given-names></name>
<name><surname>Neves dos Santos</surname><given-names>F.</given-names></name>
<name><surname>Silva Aguiar</surname><given-names>A.</given-names></name>
<name><surname>Filipe</surname><given-names>V.</given-names></name>
<name><surname>Valente</surname><given-names>A.</given-names></name>
</person-group><article-title>Fusion of Time-of-Flight Based Sensors with Monocular Cameras for a Robotic Person Follower</article-title><source>J. Intell. Robot. Syst. Theory Appl.</source><year>2024</year><volume>110</volume><fpage>30</fpage><pub-id pub-id-type="doi">10.1007/s10846-023-02037-4</pub-id></element-citation></ref><ref id="B115-sensors-25-01248"><label>115.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>X.</given-names></name>
<name><surname>Ji</surname><given-names>S.</given-names></name>
<name><surname>Pan</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>K.</given-names></name>
<name><surname>Wu</surname><given-names>C.</given-names></name>
</person-group><article-title>NeurlT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2404.08939</pub-id></element-citation></ref><ref id="B116-sensors-25-01248"><label>116.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>K.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Luo</surname><given-names>H.</given-names></name>
</person-group><article-title>Engineering Applications of Artificial Intelligence Multisensor Data Fusion Approach for Sediment Assessment of Sewers in Operation</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>132</volume><fpage>107965</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.107965</pub-id></element-citation></ref><ref id="B117-sensors-25-01248"><label>117.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gao</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Song</surname><given-names>X.</given-names></name>
</person-group><article-title>A Fusion Strategy for Vehicle Positioning at Intersections Utilizing UWB and Onboard Sensors</article-title><source>Sensors</source><year>2024</year><volume>24</volume><elocation-id>476</elocation-id><pub-id pub-id-type="doi">10.3390/s24020476</pub-id><pub-id pub-id-type="pmid">38257571</pub-id>
</element-citation></ref><ref id="B118-sensors-25-01248"><label>118.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ming</surname><given-names>Z.</given-names></name>
<name><surname>Berrio</surname><given-names>J.S.</given-names></name>
<name><surname>Shan</surname><given-names>M.</given-names></name>
<name><surname>Worrall</surname><given-names>S.</given-names></name>
</person-group><article-title>OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework for 3D Occupancy Prediction</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2403.01644v3</pub-id></element-citation></ref><ref id="B119-sensors-25-01248"><label>119.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kocic</surname><given-names>J.</given-names></name>
<name><surname>Jovicic</surname><given-names>N.</given-names></name>
<name><surname>Drndarevic</surname><given-names>V.</given-names></name>
</person-group><article-title>Sensors and Sensor Fusion in Autonomous Vehicles</article-title><source>Proceedings of the 2018 26th Telecommunications Forum (TELFOR)</source><conf-loc>Belgrade, Serbia</conf-loc><conf-date>20&#x02013;21 November 2018</conf-date><pub-id pub-id-type="doi">10.1109/TELFOR.2018.8612054</pub-id></element-citation></ref><ref id="B120-sensors-25-01248"><label>120.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Luo</surname><given-names>R.C.</given-names></name>
<name><surname>Chang</surname><given-names>N.W.</given-names></name>
<name><surname>Lin</surname><given-names>S.C.</given-names></name>
<name><surname>Wu</surname><given-names>S.C.</given-names></name>
</person-group><article-title>Human Tracking and Following Using Sensor Fusion Approach for Mobile Assistive Companion Robot</article-title><source>Proceedings of the 2009 35th Annual Conference of IEEE Industrial Electronics</source><conf-loc>Porto, Portugal</conf-loc><conf-date>3&#x02013;5 November 2009</conf-date><fpage>2235</fpage><lpage>2240</lpage><pub-id pub-id-type="doi">10.1109/IECON.2009.5415185</pub-id></element-citation></ref><ref id="B121-sensors-25-01248"><label>121.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Khodarahmi</surname><given-names>M.</given-names></name>
<name><surname>Maihami</surname><given-names>V.</given-names></name>
</person-group><article-title>A Review on Kalman Filter Models</article-title><source>Arch. Comput. Methods Eng.</source><year>2023</year><volume>30</volume><fpage>727</fpage><lpage>747</lpage><pub-id pub-id-type="doi">10.1007/s11831-022-09815-7</pub-id></element-citation></ref><ref id="B122-sensors-25-01248"><label>122.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Arpitha Shankar</surname><given-names>S.I.</given-names></name>
<name><surname>Shivakumar</surname><given-names>M.</given-names></name>
</person-group><article-title>Sensor Fusion Based Multiple Robot Navigation in an Indoor Environment</article-title><source>Int. J. Interact. Des. Manuf.</source><year>2024</year><volume>18</volume><fpage>4841</fpage><lpage>4852</lpage><pub-id pub-id-type="doi">10.1007/s12008-024-01774-6</pub-id></element-citation></ref><ref id="B123-sensors-25-01248"><label>123.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yara</surname><given-names>R.</given-names></name>
<name><surname>Konstantinos</surname><given-names>T.</given-names></name>
<name><surname>Roland</surname><given-names>H.</given-names></name>
<name><surname>John</surname><given-names>C.</given-names></name>
<name><surname>Eleni</surname><given-names>C.</given-names></name>
<name><surname>Markus</surname><given-names>R.</given-names></name>
</person-group><article-title>Unscented Kalman Filter&#x02013;Based Fusion of GNSS, Accelerometer, and Rotation Sensors for Motion Tracking</article-title><source>J. Struct. Eng.</source><year>2024</year><volume>150</volume><fpage>5024002</fpage><pub-id pub-id-type="doi">10.1061/JSENDH.STENG-12872</pub-id></element-citation></ref><ref id="B124-sensors-25-01248"><label>124.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nguyen</surname><given-names>T.</given-names></name>
<name><surname>Mann</surname><given-names>G.K.I.</given-names></name>
<name><surname>Vardy</surname><given-names>A.</given-names></name>
<name><surname>Gosine</surname><given-names>R.G.</given-names></name>
</person-group><article-title>CKF-Based Visual Inertial Odometry for Long-Term Trajectory Operations</article-title><source>J. Robot.</source><year>2020</year><volume>2020</volume><fpage>7362952</fpage><pub-id pub-id-type="doi">10.1155/2020/7362952</pub-id></element-citation></ref><ref id="B125-sensors-25-01248"><label>125.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
</person-group><article-title>FastTrack: A Highly Efficient and Generic GPU-Based Multi-Object Tracking Method with Parallel Kalman Filter</article-title><source>Int. J. Comput. Vis.</source><year>2023</year><volume>132</volume><fpage>1463</fpage><lpage>1483</lpage><pub-id pub-id-type="doi">10.1007/s11263-023-01933-4</pub-id></element-citation></ref><ref id="B126-sensors-25-01248"><label>126.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Damsgaard</surname><given-names>B.</given-names></name>
<name><surname>Gaasdal</surname><given-names>S.S.</given-names></name>
<name><surname>Bonnerup</surname><given-names>S.</given-names></name>
</person-group><article-title>Multi-Sensor Fusion with Radar and Ultrasound for Obstacle Avoidance on Capra Hircus 1.0</article-title><source>Proceedings of the 11th Student Symposium on Mechanical and Manufacturing Engineering</source><conf-loc>Parit Raja, Malaysia</conf-loc><conf-date>25&#x02013;26 August 2021</conf-date><fpage>1</fpage><lpage>8</lpage></element-citation></ref><ref id="B127-sensors-25-01248"><label>127.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cai</surname><given-names>Y.</given-names></name>
<name><surname>Qin</surname><given-names>T.</given-names></name>
<name><surname>Ou</surname><given-names>Y.</given-names></name>
<name><surname>Wei</surname><given-names>R.</given-names></name>
</person-group><article-title>Intelligent Systems in Motion: A Comprehensive Review on Multi-Sensor Fusion and Information Processing From Sensing to Navigation in Path Planning</article-title><source>Int. J. Semant. Web Inf. Syst.</source><year>2023</year><volume>19</volume><fpage>1</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.4018/IJSWIS.333056</pub-id></element-citation></ref><ref id="B128-sensors-25-01248"><label>128.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jamil</surname><given-names>H.</given-names></name>
<name><surname>Jian</surname><given-names>Y.</given-names></name>
</person-group><article-title>An Evolutionary Enhanced Particle Filter-Based Fusion Localization Scheme for Fast Tracking of Smartphone Users in Tall Complex Buildings for Hazardous Situations</article-title><source>IEEE Sens. J.</source><year>2024</year><volume>24</volume><fpage>6799</fpage><lpage>6812</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2024.3352599</pub-id></element-citation></ref><ref id="B129-sensors-25-01248"><label>129.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tang</surname><given-names>Q.</given-names></name>
<name><surname>Liang</surname><given-names>J.</given-names></name>
<name><surname>Zhu</surname><given-names>F.</given-names></name>
</person-group><article-title>A Comparative Review on Multi-Modal Sensors Fusion Based on Deep Learning</article-title><source>Signal Process.</source><year>2023</year><volume>213</volume><fpage>109165</fpage><pub-id pub-id-type="doi">10.1016/j.sigpro.2023.109165</pub-id></element-citation></ref><ref id="B130-sensors-25-01248"><label>130.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Thepsit</surname><given-names>T.</given-names></name>
<name><surname>Konghuayrob</surname><given-names>P.</given-names></name>
<name><surname>Saenthon</surname><given-names>A.</given-names></name>
<name><surname>Yanyong</surname><given-names>S.</given-names></name>
</person-group><article-title>Localization for Outdoor Mobile Robot Using LiDAR and RTK-GNSS/INS</article-title><source>Sensors Mater.</source><year>2024</year><volume>36</volume><fpage>1405</fpage><lpage>1418</lpage><pub-id pub-id-type="doi">10.18494/SAM4841</pub-id></element-citation></ref><ref id="B131-sensors-25-01248"><label>131.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>H.</given-names></name>
<name><surname>Lu</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
<name><surname>Shi</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep Learning-Based Fusion Networks with High-Order Attention Mechanism for 3D Object Detection in Autonomous Driving Scenarios</article-title><source>Appl. Soft Comput.</source><year>2024</year><volume>152</volume><fpage>111253</fpage><pub-id pub-id-type="doi">10.1016/j.asoc.2024.111253</pub-id></element-citation></ref><ref id="B132-sensors-25-01248"><label>132.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fu</surname><given-names>G.</given-names></name>
<name><surname>Menciassi</surname><given-names>A.</given-names></name>
<name><surname>Dario</surname><given-names>P.</given-names></name>
</person-group><article-title>Development of a Low-Cost Active 3D Triangulation Laser Scanner for Indoor Navigation of Miniature Mobile Robots</article-title><source>Rob. Auton. Syst.</source><year>2012</year><volume>60</volume><fpage>1317</fpage><lpage>1326</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2012.06.002</pub-id></element-citation></ref><ref id="B133-sensors-25-01248"><label>133.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Klan&#x000c4;nik</surname><given-names>S.</given-names></name>
<name><surname>Bali&#x000c4;</surname><given-names>J.</given-names></name>
<name><surname>Planin&#x00161;i&#x000c4;</surname><given-names>P.</given-names></name>
</person-group><article-title>Obstacle Detection with Active Laser Triangulation</article-title><source>Adv. Prod. Eng. Manag.</source><year>2007</year><volume>2</volume><fpage>79</fpage><lpage>90</lpage></element-citation></ref><ref id="B134-sensors-25-01248"><label>134.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fran&#x000e7;a</surname><given-names>J.G.D.M.</given-names></name>
<name><surname>Gazziro</surname><given-names>M.A.</given-names></name>
<name><surname>Ide</surname><given-names>A.N.</given-names></name>
<name><surname>Saito</surname><given-names>J.H.</given-names></name>
</person-group><article-title>A 3D Scanning System Based on Laser Triangulation and Variable Field of View</article-title><source>Proceedings of the IEEE International Conference on Image Processing 2005</source><conf-loc>Genova, Italy</conf-loc><conf-date>14 September 2005</conf-date><volume>Volume 1</volume><fpage>425</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1109/ICIP.2005.1529778</pub-id></element-citation></ref><ref id="B135-sensors-25-01248"><label>135.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fu</surname><given-names>G.</given-names></name>
<name><surname>Corradi</surname><given-names>P.</given-names></name>
<name><surname>Menciassi</surname><given-names>A.</given-names></name>
<name><surname>Dario</surname><given-names>P.</given-names></name>
</person-group><article-title>An Integrated Triangulation Laser Scanner for Obstacle Detection of Miniature Mobile Robots in Indoor Environment</article-title><source>IEEE/ASME Trans. Mechatron.</source><year>2011</year><volume>16</volume><fpage>778</fpage><lpage>783</lpage><pub-id pub-id-type="doi">10.1109/TMECH.2010.2084582</pub-id></element-citation></ref><ref id="B136-sensors-25-01248"><label>136.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Schlarp</surname><given-names>J.</given-names></name>
<name><surname>Csencsics</surname><given-names>E.</given-names></name>
<name><surname>Schitter</surname><given-names>G.</given-names></name>
</person-group><article-title>Design and Evaluation of an Integrated Scanning Laser Triangulation Sensor</article-title><source>Mechatronics</source><year>2020</year><volume>72</volume><fpage>102453</fpage><pub-id pub-id-type="doi">10.1016/j.mechatronics.2020.102453</pub-id></element-citation></ref><ref id="B137-sensors-25-01248"><label>137.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ding</surname><given-names>D.</given-names></name>
<name><surname>Ding</surname><given-names>W.</given-names></name>
<name><surname>Huang</surname><given-names>R.</given-names></name>
<name><surname>Fu</surname><given-names>Y.</given-names></name>
<name><surname>Xu</surname><given-names>F.</given-names></name>
</person-group><article-title>Research Progress of Laser Triangulation On-Machine Measurement Technology for Complex Surface: A Review</article-title><source>Meas. J. Int. Meas. Confed.</source><year>2023</year><volume>216</volume><fpage>113001</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2023.113001</pub-id></element-citation></ref><ref id="B138-sensors-25-01248"><label>138.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>So</surname><given-names>E.W.Y.</given-names></name>
<name><surname>Munaro</surname><given-names>M.</given-names></name>
<name><surname>Michieletto</surname><given-names>S.</given-names></name>
<name><surname>Antonello</surname><given-names>M.</given-names></name>
<name><surname>Menegatti</surname><given-names>E.</given-names></name>
</person-group><article-title>Real-Time 3D Model Reconstruction with a Dual-Laser Triangulation System for Assembly Line Completeness Inspection</article-title><source>Adv. Intell. Syst. Comput.</source><year>2013</year><volume>194 AISC</volume><fpage>707</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-33932-5_66</pub-id></element-citation></ref><ref id="B139-sensors-25-01248"><label>139.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dong</surname><given-names>H.</given-names></name>
<name><surname>Weng</surname><given-names>C.Y.</given-names></name>
<name><surname>Guo</surname><given-names>C.</given-names></name>
<name><surname>Yu</surname><given-names>H.</given-names></name>
<name><surname>Chen</surname><given-names>I.M.</given-names></name>
</person-group><article-title>Real-Time Avoidance Strategy of Dynamic Obstacles via Half Model-Free Detection and Tracking with 2D Lidar for Mobile Robots</article-title><source>IEEE/ASME Trans. Mechatron.</source><year>2021</year><volume>26</volume><fpage>2215</fpage><lpage>2225</lpage><pub-id pub-id-type="doi">10.1109/TMECH.2020.3034982</pub-id></element-citation></ref><ref id="B140-sensors-25-01248"><label>140.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gul</surname><given-names>F.</given-names></name>
<name><surname>Rahiman</surname><given-names>W.</given-names></name>
<name><surname>Nazli Alhady</surname><given-names>S.S.</given-names></name>
</person-group><article-title>A Comprehensive Study for Robot Navigation Techniques</article-title><source>Cogent Eng.</source><year>2019</year><volume>6</volume><fpage>1632046</fpage><pub-id pub-id-type="doi">10.1080/23311916.2019.1632046</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01248-f001"><label>Figure 1</label><caption><p>A systematic literature review workflow.</p></caption><graphic xlink:href="sensors-25-01248-g001" position="float"/></fig><fig position="float" id="sensors-25-01248-f002"><label>Figure 2</label><caption><p>Summarized common hybrid sensor combinations for mobile robots.</p></caption><graphic xlink:href="sensors-25-01248-g002" position="float"/></fig><fig position="float" id="sensors-25-01248-f003"><label>Figure 3</label><caption><p>Typical YOLO network architecture consisting of 24 convolutional layers [<xref rid="B63-sensors-25-01248" ref-type="bibr">63</xref>].</p></caption><graphic xlink:href="sensors-25-01248-g003" position="float"/></fig><fig position="float" id="sensors-25-01248-f004"><label>Figure 4</label><caption><p>Obstacle scanning method using RGB camera and laser pointer.</p></caption><graphic xlink:href="sensors-25-01248-g004" position="float"/></fig><fig position="float" id="sensors-25-01248-f005"><label>Figure 5</label><caption><p>Laser triangulation method for displacement measurement [<xref rid="B137-sensors-25-01248" ref-type="bibr">137</xref>].</p></caption><graphic xlink:href="sensors-25-01248-g005" position="float"/></fig><fig position="float" id="sensors-25-01248-f006"><label>Figure 6</label><caption><p>Ground- and ceiling-level obstacle detection.</p></caption><graphic xlink:href="sensors-25-01248-g006" position="float"/></fig><fig position="float" id="sensors-25-01248-f007"><label>Figure 7</label><caption><p>Channel robot navigation principal scheme.</p></caption><graphic xlink:href="sensors-25-01248-g007" position="float"/></fig><fig position="float" id="sensors-25-01248-f008"><label>Figure 8</label><caption><p>Sensor fusion and workflow of channel robot navigation system.</p></caption><graphic xlink:href="sensors-25-01248-g008" position="float"/></fig><fig position="float" id="sensors-25-01248-f009"><label>Figure 9</label><caption><p>Modified wall following VHF-based path-planning scheme.</p></caption><graphic xlink:href="sensors-25-01248-g009" position="float"/></fig><table-wrap position="float" id="sensors-25-01248-t001"><object-id pub-id-type="pii">sensors-25-01248-t001_Table 1</object-id><label>Table 1</label><caption><p>Common mobile robot path planning methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Navigation G(Global), L(Local)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Working Principle</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref.</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dijkstra</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Shortest path planning between established nodes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B10-sensors-25-01248" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01248" ref-type="bibr">11</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A star (A*)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Graphical search for shortest path to destination node</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B12-sensors-25-01248" ref-type="bibr">12</xref>,<xref rid="B13-sensors-25-01248" ref-type="bibr">13</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Artificial<break/>protentional field (APF)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Defined obstacles generate artificial repulsive force which in sum with attractive target force creates a path</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B14-sensors-25-01248" ref-type="bibr">14</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Genetic<break/>algorithm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Heuristic methods that use mutation principle for optimal path generation from defined scenarios</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B15-sensors-25-01248" ref-type="bibr">15</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">G</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neural<break/>network (NN)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning algorithm that can be trained with known trajectory inputs and outputs to generate a path.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B16-sensors-25-01248" ref-type="bibr">16</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bug</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moves in straight line to the target until obstacle is detected and evaded moving from one obstacle to another</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B17-sensors-25-01248" ref-type="bibr">17</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Vector field histogram (VHF)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Occupancy grid is generated using sensor data, and target artificial force is attracting the robot while discrete obstacle data are pushing the robot; thus obstacles are evaded</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B18-sensors-25-01248" ref-type="bibr">18</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dynamic window</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sensor field of view is discretized in separate windows, which react to obstacles and maneuver to the target avoiding them</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B19-sensors-25-01248" ref-type="bibr">19</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">L</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fuzzy logic</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rule based method which can work with imprecise data using fuzzy values</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B20-sensors-25-01248" ref-type="bibr">20</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01248-t002"><object-id pub-id-type="pii">sensors-25-01248-t002_Table 2</object-id><label>Table 2</label><caption><p>Non-vision-based robot localization technology review.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensor</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methodology</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Path Planning Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Advantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Disadvantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref.</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LiDAR </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2D LiDAR data are transformed in polar coordinates and clustering is performed using Euclidean algorithm </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improved time elastic band (TEB) method for local obstacle avoidance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">System is able accurately react to dynamic obstacles, also by evaluating dynamic obstacle velocities, they can be evaded faster</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Very dependent on localization algorithm accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B30-sensors-25-01248" ref-type="bibr">30</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tactile</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tactile sensors are used for target localization with robot hand setup</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Path planning is realized using novel Virtual Tactile POMDP (VT-POMDP)-based method dedicated for partially observable domains</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Allows to mimic human touch for object localization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Localization is not solved for scenarios with additional objects and obstacles</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B31-sensors-25-01248" ref-type="bibr">31</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Using tactile sensors, robot is able to react to obstacles and adjust trajectory to the goal</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">For global planning trajectory to the goal is estimated with A* algorithm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">System is able to react not only to stationary but also to dynamic obstacles.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Field of view is low for obstacle detection, thus trajectory optimization is low. Contact is required for obstacle detection.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B32-sensors-25-01248" ref-type="bibr">32</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Three coordinated IR distance sensors estimating distance to an obstacle.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Type 2 Fuzzy controller for local path planning</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Good dynamic response and accuracy of the system</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Very close objects cannot be detected also field of view is narrow</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B33-sensors-25-01248" ref-type="bibr">33</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ultrasonic</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ultrasonic sensors mounted on four sides of the robot for obstacle detection</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fuzzy controller is used for local path planning</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Effective robot localization in simple maps</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Not effective in very narrow spaces also influenced by sensors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B34-sensors-25-01248" ref-type="bibr">34</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Radar</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A point cloud dataset called Milli noise captured with radar in indoor navigation scenarios</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dijkstra global path planning methods were used</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accurate point-wise labeling can be provided</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High computational resources</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B29-sensors-25-01248" ref-type="bibr">29</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01248-t003"><object-id pub-id-type="pii">sensors-25-01248-t003_Table 3</object-id><label>Table 3</label><caption><p>Non-vision robot localization technology comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Criteria/<break/>Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Operating<break/>Distance</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Field of View</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity to<break/>Disturbance</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Computational <break/>Resources</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Implementation<break/>Cost</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref.</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">IR</td><td align="center" valign="middle" rowspan="1" colspan="1">10&#x02013;400 cm</td><td align="center" valign="middle" rowspan="1" colspan="1">20&#x02013;60&#x000b0;</td><td align="center" valign="middle" rowspan="1" colspan="1">2 mm</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B24-sensors-25-01248" ref-type="bibr">24</xref>,<xref rid="B35-sensors-25-01248" ref-type="bibr">35</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tactile</td><td align="center" valign="middle" rowspan="1" colspan="1">Contact</td><td align="center" valign="middle" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" rowspan="1" colspan="1">Up to 1%</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B36-sensors-25-01248" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-01248" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-01248" ref-type="bibr">38</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ultrasonic</td><td align="center" valign="middle" rowspan="1" colspan="1">2 cm&#x02013;10 m</td><td align="center" valign="middle" rowspan="1" colspan="1">15&#x02013;30&#x000b0;</td><td align="center" valign="middle" rowspan="1" colspan="1">1&#x02013;3 cm</td><td align="center" valign="middle" rowspan="1" colspan="1">Very high</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B39-sensors-25-01248" ref-type="bibr">39</xref>,<xref rid="B40-sensors-25-01248" ref-type="bibr">40</xref>,<xref rid="B41-sensors-25-01248" ref-type="bibr">41</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lidar</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1&#x02013;100 m</td><td align="center" valign="middle" rowspan="1" colspan="1">0&#x02013;360&#x000b0; (3D, 2D)</td><td align="center" valign="middle" rowspan="1" colspan="1">1&#x02013;3 cm (3D), &#x0003c;1 cm (2D)</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">Very High</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B42-sensors-25-01248" ref-type="bibr">42</xref>,<xref rid="B43-sensors-25-01248" ref-type="bibr">43</xref>,<xref rid="B44-sensors-25-01248" ref-type="bibr">44</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Radar</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1&#x02013;300 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">120&#x02013;360&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1&#x02013;10 cm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B45-sensors-25-01248" ref-type="bibr">45</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01248-t004"><object-id pub-id-type="pii">sensors-25-01248-t004_Table 4</object-id><label>Table 4</label><caption><p>Vision based robot localization technology review.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensor</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methodology</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Path Planning Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Advantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Disadvantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref.</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB-D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Travers ability map is extracted from raw depth images using tiny-YOLOv3 to endure safe driving for low-body mobile robots </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A* with fast marching method for faster distance cost calculation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Low-cost system enabling obstacle detection and path planning in real-time</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Refresh rate is relatively slow for avoiding dynamically moving pedestrians</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B55-sensors-25-01248" ref-type="bibr">55</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB-D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Closed-loop real-time dense RGB-D SLAM algorithm incorporating tiny-YOLOv4, which reconstructs dense 3D background for indoor mobile robot navigation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Optimal optimalRRT planner, which accelerates computation of faltered robot-centric point cloud for path planning</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Provides faster computation of path planning in real time compared to conventional SLAM methods</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy is affected by surface color, varying distance to the static, and dynamic objects</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B56-sensors-25-01248" ref-type="bibr">56</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pattern recognition using two laser pointers to detect and avoid obstacles using LaGrange interpolation formula to determine the distance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Rotation angle of the robot is adjusted according to the calculated distance of two laser pointers</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lower computation load and cost-efficient mobile robot navigation system</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Influenced by light in a way that observed view does not have enough color contrast, also influenced by camera proximity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B57-sensors-25-01248" ref-type="bibr">57</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RGB CCD camera takes an image recognizing the red color shape drawn with laser pointer calculating velocity vector</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mobile robot performs steering tasks according to drawn shapes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cost-efficient vision system enables to detect laser pointer trajectory</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision of shape detection is affected by surface color and reflectivity</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B58-sensors-25-01248" ref-type="bibr">58</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ToF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ToF camera is used for indoor obstacle detection where GPS signal is weaker then outside</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Global path planning and local obstacle avoidance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cost-efficient system for obstacle detection that is relatively accurate with different lighting of surfaces</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">In complex scenes, light can deflect multiple times, causing calculation problems</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B59-sensors-25-01248" ref-type="bibr">59</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DVS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-quadrotor localization and tracking is performed using event-based camera and deep learning network based on YOLOv5 and k-dimensional tree</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MINLP-based motion planner, which enables quadrotor to calculate its position velocity and distance to other obstacles and quadrotors </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Relatively cost-efficient system that is able to perform localization and path planning of multi-quadrotor systems</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Limited field of view when object is close to camera, requires sufficient training data</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B60-sensors-25-01248" ref-type="bibr">60</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01248-t005"><object-id pub-id-type="pii">sensors-25-01248-t005_Table 5</object-id><label>Table 5</label><caption><p>Vision-based robot localization technology comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Criteria/<break/>Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Range</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Color</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Depth Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity to<break/>Disturbance</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Computational <break/>Resources</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Implementation<break/>Cost</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref.</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">RGB CCD</td><td align="center" valign="middle" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02714;</td><td align="center" valign="middle" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B66-sensors-25-01248" ref-type="bibr">66</xref>,<xref rid="B67-sensors-25-01248" ref-type="bibr">67</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RGB CMOS</td><td align="center" valign="middle" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02714;</td><td align="center" valign="middle" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" rowspan="1" colspan="1">Very high</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B68-sensors-25-01248" ref-type="bibr">68</xref>,<xref rid="B69-sensors-25-01248" ref-type="bibr">69</xref>] </td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">DVS</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6&#x02013;30 m</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02714;</td><td align="center" valign="middle" rowspan="1" colspan="1">61&#x02013;98%</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Very high</td><td align="center" valign="middle" rowspan="1" colspan="1">Very high</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B70-sensors-25-01248" ref-type="bibr">70</xref>,<xref rid="B71-sensors-25-01248" ref-type="bibr">71</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RGB-D</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5&#x02013;10 m</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02714;</td><td align="center" valign="middle" rowspan="1" colspan="1">Up to 97%</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B72-sensors-25-01248" ref-type="bibr">72</xref>,<xref rid="B73-sensors-25-01248" ref-type="bibr">73</xref>,<xref rid="B74-sensors-25-01248" ref-type="bibr">74</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ToF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.35&#x02013;10 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">N/A</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Up to 99%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B75-sensors-25-01248" ref-type="bibr">75</xref>,<xref rid="B76-sensors-25-01248" ref-type="bibr">76</xref>,<xref rid="B77-sensors-25-01248" ref-type="bibr">77</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01248-t006"><object-id pub-id-type="pii">sensors-25-01248-t006_Table 6</object-id><label>Table 6</label><caption><p>Hybrid robot localization technology review.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensor</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methodology</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Path Planning Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Advantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Disadvantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref.</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2D<break/>LiDAR, RGB-D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual-based SLAM and laser-based Slam is used incorporating EKF based LiDAR and RGB-D fusion for environment mapping and robot localization</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">RRT* (Rapidly exploring random tree) global path planning and Fuzzy PID controller for following trajectory accurately</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Integration of visual input provides richer data especially when LiDAR has a lack of data in wide areas</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Additional visual maps increase computational load significantly</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B80-sensors-25-01248" ref-type="bibr">80</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2D <break/>LiDAR RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reinforced learning method uses visual data acquired with CenterNet depicting obstacles and projects these data in birds-eye view using LiDAR point cloud </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">While A* is implemented for global path planning, timed elastic bands (TEB) are implemented locally, complemented by reinforced learning</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">More accurate representation of distant and close objects using sensor fusion</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High computational load, requires training data for image recognition and localization tasks</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B81-sensors-25-01248" ref-type="bibr">81</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Radar, RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Visual data are inspected using Canny edge and then spatial fusion is used for camera and MMV radar to obtain data about the same target</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Improved A* global method is used adding dynamic heuristic function for dynamic adjustment of cost between two points</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Significantly improved object recognition and distance estimation for more accurate obstacle avoidance</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Camera and radar require calibration for accurate data fusion result also sensitive to the distance to an object</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B82-sensors-25-01248" ref-type="bibr">82</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ultrasonic,<break/>RGB CMOS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLOv3 based on CNN is used to detect obstacles with camera, and fusion with ultrasonic sensor is used for distance estimation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Tested capability of obstacle detection for local navigations tasks</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Allows to estimate distance to an object recognized by camera in real time</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accuracy of 90% for distance estimation, and recognition is relatively low</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B83-sensors-25-01248" ref-type="bibr">83</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D <break/>Lidar, DVS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Event camera compares changes in intensity for detection and acquired data is fused with point cloud by pairing clusters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Nonlinear Model Predictive Control (NMPC) is used for human tracking</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Human detection is effective in high contrast zones</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High computational resources</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B84-sensors-25-01248" ref-type="bibr">84</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IR, <break/>Tactile, RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fisheye camera to detect IR markers on soft skin structure is respective coordinate systems to detect tactile changes</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Robot has three defined conditions including move towards the goal, move backward, and move along the object, which are controlled with PI controller</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Highly accurate tactile data enabling to navigate in very narrow spaces </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Complex calibration is required, friction with obstacles influences navigation accuracy</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B85-sensors-25-01248" ref-type="bibr">85</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01248-t007"><object-id pub-id-type="pii">sensors-25-01248-t007_Table 7</object-id><label>Table 7</label><caption><p>Hybrid robot localization technology comparison.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Criteria/<break/>Method</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Range</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Field of View</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensitivity to<break/>Disturbance</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Computational <break/>Resources</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Implementation<break/>Cost</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref.</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Tactile/<break/>RGB-D</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5&#x02013;10 m</td><td align="center" valign="middle" rowspan="1" colspan="1">60&#x02013;180&#x000b0;</td><td align="center" valign="middle" rowspan="1" colspan="1">Up to 1%</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B88-sensors-25-01248" ref-type="bibr">88</xref>,<xref rid="B89-sensors-25-01248" ref-type="bibr">89</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Ultrasonic/<break/>RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">2 cm&#x02013;10 m</td><td align="center" valign="middle" rowspan="1" colspan="1">60&#x02013;180&#x000b0;</td><td align="center" valign="middle" rowspan="1" colspan="1">1&#x02013;3 cm</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B90-sensors-25-01248" ref-type="bibr">90</xref>,<xref rid="B91-sensors-25-01248" ref-type="bibr">91</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lidar/RGB</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1&#x02013;100 m</td><td align="center" valign="middle" rowspan="1" colspan="1">360&#x000b0; (3D, 2D)</td><td align="center" valign="middle" rowspan="1" colspan="1">1&#x02013;3 cm (3D), &#x0003c;1 cm (2D)</td><td align="center" valign="middle" rowspan="1" colspan="1">Very high</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B92-sensors-25-01248" ref-type="bibr">92</xref>,<xref rid="B93-sensors-25-01248" ref-type="bibr">93</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lidar/<break/>RGB-D</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1&#x02013;100 m</td><td align="center" valign="middle" rowspan="1" colspan="1">360&#x000b0; (3D, 2D)</td><td align="center" valign="middle" rowspan="1" colspan="1">1&#x02013;3 cm (3D), &#x0003c;1 cm (2D)</td><td align="center" valign="middle" rowspan="1" colspan="1">High</td><td align="center" valign="middle" rowspan="1" colspan="1">Very high</td><td align="center" valign="middle" rowspan="1" colspan="1">Very high</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B94-sensors-25-01248" ref-type="bibr">94</xref>,<xref rid="B95-sensors-25-01248" ref-type="bibr">95</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lidar/<break/>DVS</td><td align="center" valign="middle" rowspan="1" colspan="1">0.1&#x02013;100 m</td><td align="center" valign="middle" rowspan="1" colspan="1">360&#x000b0; (3D, 2D)</td><td align="center" valign="middle" rowspan="1" colspan="1">1&#x02013;3 cm (3D), &#x0003c;1 cm (2D)</td><td align="center" valign="middle" rowspan="1" colspan="1">Low</td><td align="center" valign="middle" rowspan="1" colspan="1">Very high</td><td align="center" valign="middle" rowspan="1" colspan="1">Very high</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B96-sensors-25-01248" ref-type="bibr">96</xref>,<xref rid="B97-sensors-25-01248" ref-type="bibr">97</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Radar/RGB</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1&#x02013;300 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">120&#x02013;360&#x000b0;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1&#x02013;10 cm</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Moderate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B98-sensors-25-01248" ref-type="bibr">98</xref>,<xref rid="B99-sensors-25-01248" ref-type="bibr">99</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01248-t008"><object-id pub-id-type="pii">sensors-25-01248-t008_Table 8</object-id><label>Table 8</label><caption><p>Low-level cooperative sensor fusion methods for mobile robot navigation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensor</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methodology</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Advantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Disadvantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref.</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LiDAR, Camera</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">The point cloud generated using LiDAR is projected onto the image in real time. Point cloud is projected by colors referencing depth information</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accurate representation of the environment in real time for autonomous vehicle </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High computational resources. Raw data has to be projected at a very high rate, faster than the acquisition rate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B105-sensors-25-01248" ref-type="bibr">105</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Stereo camera, LRF</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fusion-based human detection and tracking algorithm combining laser data-based search window and Kalman filter for recursive estimation of target position in robots coordinate system</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Able to detect and track fast human movements in real time</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High computational resources</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B106-sensors-25-01248" ref-type="bibr">106</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01248-t009"><object-id pub-id-type="pii">sensors-25-01248-t009_Table 9</object-id><label>Table 9</label><caption><p>Mid-level cooperative sensor fusion methods for mobile robot navigation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensor</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methodology</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Advantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Disadvantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref.</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Encoder, ultrasonic</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fuzzy logic algorithm input is acquired from ultrasonic sensors for obstacle detection and the motion is executed with feedback from wheel encoders</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fats and cost-efficient obstacle avoidance system</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Slippage can introduce motion execution inaccuracies</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B107-sensors-25-01248" ref-type="bibr">107</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D <break/>LiDAR, GMSL camera </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLACT image semantic segmentation algorithm for obstacle detection is used and then LiDAR point cloud is matched with the shape corresponding to camera pixels</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Semantic segmentation algorithm allows for more accurate obstacle detection evaluating its shape</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D LiDAR requires high computational resources because of large point cloud</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B108-sensors-25-01248" ref-type="bibr">108</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Encoder, compass</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Odometry data are obtained based on wheels and encoders data and then fused with compass data using extended Kalman filter providing data for further movement to the target</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Increased position accuracy to not more than 0.15 m</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inaccurate data are common because of wheel slippage, which cannot be evaluated by selected sensors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B109-sensors-25-01248" ref-type="bibr">109</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LiDAR, RGB <break/>camera</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LiDAR and Camera joint calibration is initiated with spatial relationship, then target detection is initiated using deep learning PP-YOLOv2 and identification of surface using point cloud segmentation based on RANSAC.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Allows accurate detection of objects with suitable surfaces required for processing. Segmentation accuracy of 75.46%.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Suitable only for calibrated type of processed material. If material type changes, tuning is required.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B110-sensors-25-01248" ref-type="bibr">110</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMU, LiDAR, RGB camera</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">A dense 3D map is obtained in real time using simultaneous localization and mapping (SLAM) while IMU sensors track short-term motion. Using reinforced learning RL and CNN algorithms obstacles are avoided mapping image and LiDAR data </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Enables fully autonomous system allowing not only object recognition but also identification of various environmental factors</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High computational resources. Requires training and large number of labeled data for ML algorithms</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B81-sensors-25-01248" ref-type="bibr">81</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Global <break/>positioning system (GPS), IMU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GPS and IMU sensor data are interconnected using adaptive covariance matrix and adaptive unscented Kalman filter (AUKF) for vehicle position estimation</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Outputs robust and accurate vehicle position estimation. AUKF yields better results than UKF or EKF filters</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">System is still affected by environment obstructions like buildings and depends on accurate kinematic model</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B111-sensors-25-01248" ref-type="bibr">111</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ultra-wideband (UWB), RGB-D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Kalman filter is used to reduce noise of UWB data, which are then fused with localization data acquired from image data processed with ORB-SLAM2. EKF is used for ORB-SLAM2/UWB fusion.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Significantly improves positioning accuracy compared to standalone UWB systems</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Positioning accuracy strongly depends on field size because of UWB range limitations</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B112-sensors-25-01248" ref-type="bibr">112</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D LiDAR, RGB-D</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">For data fusion, first LiDAR and cameras are calibrated using edges for relative orientation parameters. Canny edge is used for extracting color image features. RANSAC is used for point cloud depth mapping. Extrinsic matrix is used for projecting point cloud onto an image.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Able to identify and locate cracks and evaluate geometric size with accuracy not more than 0.1mm using MobileNetV2-DeepLabV3</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High computational resources for generating dense 3D point cloud and image semantic segmentation.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B113-sensors-25-01248" ref-type="bibr">113</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UWB, monocular camera</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Histogram filter (RHF) is used for sensor fusion, which can handle exponential and Gaussian systems. Range information of UWB is fused with angle estimations received from the camera</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.67% reduction in angular error is achieved compared to standalone UWB systems</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Positioning accuracy strongly depends on field size and anchor infrastructure because of UWB range limitations.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B114-sensors-25-01248" ref-type="bibr">114</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gyroscope, accelerometer, magnetometer</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neural inertial tracking system (NeurIT) is incorporated, which incorporates RNN and transformers</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Enables accurate indoor tracking, minimizing the drift appearing from extended periods or distances</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">System is only suitable for tracking</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B115-sensors-25-01248" ref-type="bibr">115</xref>]</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01248-t010"><object-id pub-id-type="pii">sensors-25-01248-t010_Table 10</object-id><label>Table 10</label><caption><p>High-level complementary sensor fusion methods for mobile robot navigation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sensor</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Methodology</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Advantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Disadvantages</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Ref.</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Gyroscope, <break/>accelerometer, odometer, <break/>sonar</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Unscented Kalman filter and Rauch&#x02013;Tung&#x02013;Striebel are applied to fuse raw Gyroscope, accelerometer, and odometer data for precise localization of the robot, then sonar point cloud is fused with sensor data for offset adjustment and environment calculations</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accurate robot localization and orientation acquisition. </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D environment representation. Sonar-based measurement introduces noise in closed environments.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B116-sensors-25-01248" ref-type="bibr">116</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UWB, <break/>encoder, speed sensor, <break/>accelerometer, GBSS, gyroscope</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UWB and vehicle on board sensor fusion, which consisted of three components including multi sensor module, ARIMA-GARCH for UWB data processing, and global fusion module using AIMM and extended Kalman filter</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Increases positioning accuracy in GNSS-challenging environments</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Additional infrastructure for UWB is required, readability of communications must be ensured.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B117-sensors-25-01248" ref-type="bibr">117</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LiDAR,<break/>Camera,<break/>Radar</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Image data are extracted using ResNet101-DCN, then dense 3D point cloud and sparse 3D point cloud are generated using VoxelNet, and then the postprocessed data of all sensors are merged using BEVFusion and SENet</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Accurate and robust occupancy prediction of working environment even with challenging night and rainy scenarios</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">High computational resources for generating dense 3D point cloud</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B118-sensors-25-01248" ref-type="bibr">118</xref>]</td></tr></tbody></table></table-wrap></floats-group></article>