<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">ArXiv</journal-id><journal-id journal-id-type="iso-abbrev">ArXiv</journal-id><journal-id journal-id-type="publisher-id">arxiv</journal-id><journal-title-group><journal-title>ArXiv</journal-title></journal-title-group><issn pub-type="epub">2331-8422</issn><publisher><publisher-name>Cornell University</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40386574</article-id><article-id pub-id-type="pmc">PMC12083706</article-id><article-id pub-id-type="arxiv">arXiv:2505.04556v1</article-id><article-id pub-id-type="publisher-id">2505.04556</article-id><article-version-alternatives><article-version article-version-type="number">1</article-version><article-version article-version-type="status">preprint</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Comparing CPU and GPU compute of PERMANOVA on MI300A</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sfiligoi</surname><given-names>Igor</given-names></name><aff id="A1">University of California San Diego, San Diego Supercomputer Center, La Jolla, CA 92093, USA</aff></contrib></contrib-group><author-notes><corresp id="CR1">
<email>isfiligoi@sdsc.edu</email>
</corresp></author-notes><pub-date pub-type="epub"><day>7</day><month>5</month><year>2025</year></pub-date><elocation-id>arXiv:2505.04556v1</elocation-id><permissions><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbysalicense">https://creativecommons.org/licenses/by-sa/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</ext-link>, which allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. The license allows for commercial use. If you remix, adapt, or build upon the material, you must license the modified material under identical terms.</license-p></license></permissions><self-uri content-type="pdf">nihpp-2505.04556v1.pdf</self-uri><abstract id="ABS1"><p id="P1">Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is often challenging, due to the drastically different memory subsystems on host CPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both CPU and GPU cores in a single package, all backed by the same type of HBM memory. In this paper we analyze the performance of Permutational Multivariate Analysis of Variance (PERMANOVA), a non-parametric method that tests whether two or more groups of objects are significantly different based on a categorical factor. This method is memory-bound and has been recently optimized for CPU cache locality. Our tests show that GPU cores on the MI300A prefer the brute force approach instead, significantly outperforming the CPU-based implementation. The significant benefit of Simultaneous Multithreading (SMT) was also a pleasant surprise.</p></abstract><kwd-group><kwd>gpu</kwd><kwd>memory</kwd><kwd>permanova</kwd><kwd>benchmarking</kwd><kwd>apu</kwd></kwd-group></article-meta></front><body><sec id="S1"><label>1</label><title>INTRODUCTION</title><p id="P2">The Permutational Multivariate Analysis of Variance (PERMANOVA) [<xref rid="R1" ref-type="bibr">1</xref>&#x02013;<xref rid="R2" ref-type="bibr">2</xref>] is a popular method used in the microbiome field. It is is a non-parametric method that tests whether two or more groups of objects (e.g., samples) are significantly different based on a categorical factor, taking a distance matrix as its input. The statistical significance is assessed via a permutation test, with the assignment of objects to groups randomly permuted a number of times. This makes is extremely memory-intensive. The reference implementation is available in <italic toggle="yes">scikit-bio</italic> [<xref rid="R3" ref-type="bibr">3</xref>], with a further optimized version available in <italic toggle="yes">unifrac-binaries</italic> [<xref rid="R4" ref-type="bibr">4</xref>].</p><p id="P3">The AMD MI300A [<xref rid="R5" ref-type="bibr">5</xref>] is the first data-center Accelerated Processor Unit (APU) publicly available, and has been installed in several High-Performance Computing (HPC) centers. Unlike other compute setups, the MI300A combines both Central Processing Units (CPU) and Graphics Processing Units (GPU) in a single package, all of them paired with on-chip High Bandwidth Memory (HBM). This uniformity makes it a great target for comparing memory-heavy algorithmic performance between CPU and GPU implementations.</p></sec><sec id="S2"><label>2</label><title>PERMANOVA INNER ALGORITHM</title><p id="P4">This section describes the most time-consuming part of the PERMANOVA algorithm, the parallel pseudo-F partial statistics computation (<italic toggle="yes">permanova_f_stat_sW</italic>). While the actual PERMANOVA includes several other steps that happen before and after, they add minimal overhead and will thus be ignored in this paper. The interested reader can find the complete source code at [<xref rid="R6" ref-type="bibr">6</xref>].</p><p id="P5">A typical PERMANOVA invocation uses a distance matrix between 1k<sup>2</sup> and 100k<sup>2</sup> elements, and computes the pseudo- F partial statistic on between 1k and 1M permutations. Each permutation is independent, making that dimension the most obvious parallelization target. The original, brute force implementation of PERMANOVA&#x02019;s parallel pseudo-F partial statistic is presented in <xref rid="T1" ref-type="table">Algorithm 1</xref>.</p><table-wrap position="anchor" id="T1"><label>ALGORITHM 1:</label><caption><p id="P6">Original brute force permanova_f_stat_sW</p></caption><table frame="hsides" rules="none"><colgroup span="1"><col align="left" valign="middle" span="1"/></colgroup><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">float <monospace><bold>permanova_f_stat_sW_one</bold></monospace>(mat[], n_dims, grouping[], inv_group_sizes[]) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">s_W = 0.0;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">for (int row=0; row &#x0003c; (n_dims-1); row++) { // no columns in last row</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;for (int col=row+1; col &#x0003c; n_dims; col++) { // diagonal is always zero</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;int group_idx = grouping[row];</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;if (grouping[col] == group_idx) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;const float * mat_row = mat + uint64_t(row)*uint64_t(n_dims);</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;val = mat_row[col];</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;s_W += val * val * inv_group_sizes[group_idx];;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;}</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;}}</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;return s_W;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">}</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">void <monospace><bold>permanova_f_stat_sW_T</bold></monospace>(mat[], n_dims, groupings[], n_perms, inv_group_sizes[], group_sWs[]) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">#pragma omp parallel for</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">for (int p=0; p &#x0003c; n_perms; p++) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;group_sWs[p] = permanova_f_stat_sW_one(mat, n_dims, groupings + p*n_dims, inv_group_sizes);</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">}</td></tr></tbody></table></table-wrap><p id="P7">A careful look at the logic shows that the <italic toggle="yes">grouping</italic> array is accessed in a tiled manner, so the obvious step on CPU cores was to implement a tiled version of the algorithm. Unfortunately, many compilers do not support, or properly implement, the OpenMP <italic toggle="yes">tile</italic> directive on non-square nested loops, so we had to explicitly split the two loops by hand. As a side effect, we discovered that we could reuse the access to <italic toggle="yes">inv_group_sizes</italic> array in the innermost loop, saving both memory accesses and compute. The tiled version is available as <xref rid="T2" ref-type="table">Algorithm 2</xref>.</p><table-wrap position="anchor" id="T2"><label>ALGORITHM 2:</label><caption><p id="P8">Tiled permanova_f_stat_sW</p></caption><table frame="hsides" rules="none"><colgroup span="1"><col align="left" valign="middle" span="1"/></colgroup><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">float <monospace><bold>permanova_f_stat_sW_one</bold></monospace>(mat[], n_dims, grouping[], inv_group_sizes[]) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">s_W = 0.0;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">for (int trow=0; trow &#x0003c; (n_dims-1); trow+=TILE) { // no columns in last row</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;for (int tcol=trow+1; tcol &#x0003c; n_dims; tcol+=TILE) { // diagonal is always zero</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;for (uint32_t row=trow; row &#x0003c; min(trow+TILE,n_dims-1); row++) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;min_col = std::max(tcol,row+1); max_col = std::min(tcol+TILE,n_dims);</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;float * mat_row = mat + row*n_dims;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;int group_idx = grouping[row];</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;local_s_W = 0.0;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;for (uint32_t col=min_col; col &#x0003c; max_col; col++) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;&#x02003;if (grouping[col] == group_idx) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;val = mat_row[col];</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;local_s_W += val * val;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;&#x02003;}</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;}</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;&#x02003;s_W += local_s_W*inv_group_sizes[group_idx];</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;}}}</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;return s_W;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;}</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">void <monospace><bold>permanova_f_stat_sW_T</bold></monospace>(mat[], n_dims, groupings[], n_perms, inv_group_sizes[], group_sWs[]) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">#pragma omp parallel for</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">for (int p=0; p &#x0003c; n_perms; p++) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;group_sWs[p] = permanova_f_stat_sW_one(mat, n_dims, groupings + p*n_dims, inv_group_sizes);</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">}</td></tr></tbody></table></table-wrap><p id="P9">When it came time to port the code to GPU compute, we chose to use the GPU-offloading syntax of OpenMP [<xref rid="R7" ref-type="bibr">7</xref>], which required the insertion of only two pragma lines. Due to the much higher parallel nature of the GPUs, we parallelize both at permutation and distance matrix level there, as shown in <xref rid="T3" ref-type="table">Algorithm 3</xref>. Note that we ended up using the brute force version of the algorithm, as any attempt to tile the algorithm resulted in drastically slower execution.</p><table-wrap position="anchor" id="T3"><label>ALGORITHM 3:</label><caption><p id="P10">GPU implementation of the brute force permanova_f_stat_sW</p></caption><table frame="hsides" rules="none"><colgroup span="1"><col align="left" valign="middle" span="1"/></colgroup><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">float <monospace><bold>permanova_f_stat_sW_one</bold></monospace>(mat[], n_dims, grouping[], inv_group_sizes[]) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">s_W = 0.0;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">#pragma omp parallel for collapse(2) reduction(+:s_W)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">for (int row=0; row &#x0003c; (n_dims-1); row++) { // no columns in last row</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;for (int col=row+1; col &#x0003c; n_dims; col++) { // diagonal is always zero</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;int group_idx = grouping[row];</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;if (grouping[col] == group_idx) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;const float * mat_row = mat + uint64_t(row)*uint64_t(n_dims);</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;val = mat_row[col];</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;&#x02003;s_W += val * val * inv_group_sizes[group_idx];;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;&#x02003;}</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;}}</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;return s_W;</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">}</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">void <monospace><bold>permanova_f_stat_sW_T</bold></monospace>(mat[], n_dims, groupings[], n_perms, inv_group_sizes[], group_sWs[]) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">#pragma omp target teams distribute</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">for (int p=0; p &#x0003c; n_perms; p++) {</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">&#x02003;group_sWs[p] = permanova_f_stat_sW_T_one(mat, n_dims, groupings + p*n_dims, inv_group_sizes);</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">}</td></tr></tbody></table></table-wrap></sec><sec id="S3"><label>3</label><title>BENCHMARK RESULTS ON MI300A</title><p id="P11">The above algorithms have been benchmarked as part of the early-user phase of the SDSC Cosmos system. Each Cosmos node contains 4 MI300A, but we used only one at a time, to contain the memory traffic within the single chip. The CPU code was compiled using gcc 14.2, while the GPU code was compiled using the clang-based AOMP 20.0 [<xref rid="R8" ref-type="bibr">8</xref>].</p><p id="P12">The input distance matrix had a size of 25145<sup>2</sup> and was the result of computing the Unweighted Unifrac [<xref rid="R9" ref-type="bibr">9</xref>] on the Earth Microbiome Project (EMP) data. The number of permutations was 3999; the magnitude was chosen to be both large enough to exploit the GPU parallelism, small enough to result in reasonable execution time and would represent a realistic use case. The summary results for both CPU and GPU execution are available in <xref rid="F1" ref-type="fig">Figure 1</xref>.</p><p id="P13">As can be seen, using the GPU compute units indeed results is drastically faster execution time. Compared to the same brute force algorithm on a non-SMT CPU setup, the GPU implementation is over 6x faster. That said, the more flexible nature of the CPU compute units allows for smarter algorithms, which claw back some of that advantage. This is especially noticeable when paired with Simultaneous Multithreading (SMT), that exposes two hardware threads for each physical CPU core.</p><p id="P14">In summary, even with an identical memory subsystem attached to both CPU and GPU compute units, massively parallel but memory-intensive algorithms like PERMANOVA do benefit from GPU compute. Some of the CPU-focused optimizations may however not directly translate to the GPU implementations, thus likely requiring some device-specific code.</p></sec><sec sec-type="supplementary-material" id="SM1"><title>Supplementary Material</title><supplementary-material id="SD1" position="float" content-type="local-data"><label>Supplement 1</label><media xlink:href="NIHPP2505.04556v1-supplement-1.pdf" id="d67e399" position="anchor"/></supplementary-material></sec></body><back><ack id="S4"><title>ACKNOWLEDGMENTS</title><p id="P15">Data was collected as part of the early-user phase in the NSF-funded SDSC Cosmos system. All benchmark results should be considered preliminary, and results may change once the system is in production stage. This work was partially funded by the US National Research Foundation (NSF) under grants OAC-2404323 and DBI-2038509, and the US National Institutes of Health (NIH) grant R01CA241728.</p></ack><ref-list><title>REFERENCES</title><ref id="R1"><label>[1]</label><mixed-citation publication-type="journal"><name><surname>Anderson</surname><given-names>Marti J.</given-names></name>, <year>2001</year>. <article-title>A new method for non-parametric multivariate analysis of variance</article-title>. <source>Austral Ecology 26</source>.<volume>1</volume> (<issue>2001</issue>): <fpage>32</fpage>&#x02013;<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1111/j.1442-9993.2001.01070.pp.x</pub-id></mixed-citation></ref><ref id="R2"><label>[2]</label><mixed-citation publication-type="book"><name><surname>Anderson</surname><given-names>Marti J.</given-names></name>. <year>2017</year>. <part-title>Permutational Multivariate Analysis of Variance (PERMANOVA)</part-title>. In <source>Wiley StatsRef: Statistics Reference Online</source> (eds <name><surname>Balakrishnan</surname><given-names>N.</given-names></name>, <name><surname>Colton</surname><given-names>T.</given-names></name>, <name><surname>Everitt</surname><given-names>B.</given-names></name>, <name><surname>Piegorsch</surname><given-names>W.</given-names></name>, <name><surname>Ruggeri</surname><given-names>F</given-names></name>. and <name><surname>Teugels</surname><given-names>J.L.</given-names></name>). <pub-id pub-id-type="doi">10.1002/9781118445112.stat07841</pub-id></mixed-citation></ref><ref id="R3"><label>[3]</label><mixed-citation publication-type="webpage"><source>SkBio Permanova documentation</source>, Online, Accessed <date-in-citation>Mar 22nd, 2025</date-in-citation>. <ext-link xlink:href="https://scikit.bio/docs/dev/generated/skbio.stats.distance.permanova.html" ext-link-type="uri">https://scikit.bio/docs/dev/generated/skbio.stats.distance.permanova.html</ext-link></mixed-citation></ref><ref id="R4"><label>[4]</label><mixed-citation publication-type="webpage"><source>Unifrac-Binaries git repository</source>. Online. <date-in-citation>Accessed Mar 22nd, 2025</date-in-citation>. <ext-link xlink:href="https://github.com/biocore/unifrac-binaries" ext-link-type="uri">https://github.com/biocore/unifrac-binaries</ext-link></mixed-citation></ref><ref id="R5"><label>[5]</label><mixed-citation publication-type="webpage"><source>AMD CDNA3 Architecture</source>, Online. Accessed <date-in-citation>Mar 22nd, 2025</date-in-citation>. <ext-link xlink:href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf" ext-link-type="uri">https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf</ext-link></mixed-citation></ref><ref id="R6"><label>[6]</label><mixed-citation publication-type="webpage"><source>PERMANOVA implementation, unifrac-binaries github repository</source>. Online, Accessed <date-in-citation>Mar 22nd, 2025</date-in-citation>. <ext-link xlink:href="https://github.com/biocore/unifrac-binaries/blob/caf727d24af62bf8150d44446c37832b93c04913/src/skbio_alt.cpp#L839" ext-link-type="uri">https://github.com/biocore/unifrac-binaries/blob/caf727d24af62bf8150d44446c37832b93c04913/src/skbio_alt.cpp#L839</ext-link></mixed-citation></ref><ref id="R7"><label>[7]</label><mixed-citation publication-type="webpage"><source>OPENMP API Specification</source>, Online, Accessed <date-in-citation>Mar 22nd, 2025</date-in-citation>. <ext-link xlink:href="https://www.openmp.org/spec-html/5.0/openmpsu78.html" ext-link-type="uri">https://www.openmp.org/spec-html/5.0/openmpsu78.html</ext-link></mixed-citation></ref><ref id="R8"><label>[8]</label><mixed-citation publication-type="webpage"><source>ROCm aomp github repository</source>. Online, Accessed <date-in-citation>Mar 22nd, 2025</date-in-citation>. <ext-link xlink:href="https://github.com/ROCm/aomp" ext-link-type="uri">https://github.com/ROCm/aomp</ext-link></mixed-citation></ref><ref id="R9"><label>[9]</label><mixed-citation publication-type="journal"><name><surname>Sfiligoi</surname><given-names>I.</given-names></name>, <name><surname>Armstrong</surname><given-names>G.</given-names></name>, <name><surname>Gonzalez</surname><given-names>A.</given-names></name>, <name><surname>McDonald</surname><given-names>D.</given-names></name>, <name><surname>Knight</surname><given-names>R.</given-names></name>., <year>2022</year>. <article-title>Optimizing UniFrac with OpenACC Yields Greater Than One Thousand Times Speed Increase</article-title>. <source>mSystems</source><volume>7</volume>:<fpage>e00028</fpage>&#x02013;<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1128/msystems.00028-22</pub-id><pub-id pub-id-type="pmid">35638356</pub-id>
</mixed-citation></ref></ref-list></back><floats-group><fig position="float" id="F1"><label>Figure 1:</label><caption><p id="P16">PERMANOVA execution time by algorithm and resource. The horizontal axis is expressed in seconds (lower is better).</p></caption><graphic xlink:href="nihpp-2505.04556v1-f0001" position="float"/></fig><boxed-text id="BX1" position="float"><caption><title>CCS CONCEPTS</title></caption><list list-type="bullet" id="L1"><list-item><p id="P17">Computing methodologies~Parallel computing methodologies</p></list-item></list></boxed-text></floats-group></article>