<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" id="jeo270135" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">J Exp Orthop</journal-id><journal-id journal-id-type="iso-abbrev">J Exp Orthop</journal-id><journal-id journal-id-type="doi">10.1002/(ISSN)2197-1153</journal-id><journal-id journal-id-type="publisher-id">JEO2</journal-id><journal-title-group><journal-title>Journal of Experimental Orthopaedics</journal-title></journal-title-group><issn pub-type="epub">2197-1153</issn><publisher><publisher-name>John Wiley and Sons Inc.</publisher-name><publisher-loc>Hoboken</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39749288</article-id><article-id pub-id-type="pmc">PMC11693985</article-id>
<article-id pub-id-type="doi">10.1002/jeo2.70135</article-id><article-id pub-id-type="publisher-id">JEO270135</article-id><article-categories><subj-group subj-group-type="overline"><subject>Original Paper</subject></subj-group><subj-group subj-group-type="heading"><subject>Original Paper</subject></subj-group></article-categories><title-group><article-title>Utility of ChatGPT as a preparation tool for the Orthopaedic In&#x02010;Training Examination</article-title></title-group><contrib-group><contrib id="jeo270135-cr-0001" contrib-type="author" corresp="yes"><name><surname>Mendiratta</surname><given-names>Dhruv</given-names></name><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-2081-1102</contrib-id><xref rid="jeo270135-aff-0001" ref-type="aff">
<sup>1</sup>
</xref><address><email>dm1332@njms.rutgers.edu</email></address></contrib><contrib id="jeo270135-cr-0002" contrib-type="author"><name><surname>Herzog</surname><given-names>Isabel</given-names></name><xref rid="jeo270135-aff-0001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib id="jeo270135-cr-0003" contrib-type="author"><name><surname>Singh</surname><given-names>Rohan</given-names></name><xref rid="jeo270135-aff-0001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib id="jeo270135-cr-0004" contrib-type="author"><name><surname>Para</surname><given-names>Ashok</given-names></name><xref rid="jeo270135-aff-0001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib id="jeo270135-cr-0005" contrib-type="author"><name><surname>Joshi</surname><given-names>Tej</given-names></name><xref rid="jeo270135-aff-0001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib id="jeo270135-cr-0006" contrib-type="author"><name><surname>Vosbikian</surname><given-names>Michael</given-names></name><xref rid="jeo270135-aff-0001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib id="jeo270135-cr-0007" contrib-type="author"><name><surname>Kaushal</surname><given-names>Neil</given-names></name><xref rid="jeo270135-aff-0001" ref-type="aff">
<sup>1</sup>
</xref></contrib></contrib-group><aff id="jeo270135-aff-0001">
<label>
<sup>1</sup>
</label>
<named-content content-type="organisation-division">Department of Orthopaedic Surgery</named-content>
<institution>Rutgers New Jersey Medical School</institution>
<city>Newark</city>
<named-content content-type="country-part">New Jersey</named-content>
<country country="US">USA</country>
</aff><author-notes><corresp id="correspondenceTo">
<label>*</label>
<bold>Correspondence</bold> Dhruv Mendiratta, 165 S Orange Ave, Newark, NJ 07103, USA.<break/>
Email: <email>dm1332@njms.rutgers.edu</email>
<break/>
</corresp></author-notes><pub-date pub-type="epub"><day>02</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>1</month><year>2025</year></pub-date><volume>12</volume><issue seq="83">1</issue><issue-id pub-id-type="doi">10.1002/jeo2.v12.1</issue-id><elocation-id>e70135</elocation-id><history>
<date date-type="rev-recd"><day>17</day><month>11</month><year>2024</year></date>
<date date-type="received"><day>01</day><month>9</month><year>2024</year></date>
<date date-type="accepted"><day>21</day><month>11</month><year>2024</year></date>
</history><permissions><!--&#x000a9; 2025 The Author(s). Journal of Experimental Orthopaedics published by John Wiley & Sons Ltd on behalf of European Society of Sports Traumatology, Knee Surgery and Arthroscopy.--><copyright-statement content-type="article-copyright">&#x000a9; 2025 The Author(s). <italic toggle="yes">Journal of Experimental Orthopaedics</italic> published by John Wiley &#x00026; Sons Ltd on behalf of European Society of Sports Traumatology, Knee Surgery and Arthroscopy.</copyright-statement><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="file:JEO2-12-e70135.pdf"/><abstract><title>Abstract</title><sec id="jeo270135-sec-0010"><title>Purpose</title><p>Chat Generative Pre&#x02010;Trained Transformer (ChatGPT) may have implications as a novel educational resource. There are differences in opinion on the best resource for the Orthopaedic In&#x02010;Training Exam (OITE) as information changes from year to year. This study assesses ChatGPT's performance on the OITE for use as a potential study resource for residents.</p></sec><sec id="jeo270135-sec-0020"><title>Methods</title><p>Questions for the OITE data set were sourced from the American Academy of Orthopaedic Surgeons (AAOS) website. All questions from the 2022 OITE were included. All questions, including those with images, were included in the analysis. The questions were formatted in the same manner as presented on the AAOS website, with the question, narrative text and answer choices separated by a line. Each question was evaluated in a new chat session to minimize confounding variables. Answers from ChatGPT were characterized by whether they contained logical, internal or external information. Incorrect responses were further categorized into logical, informational or explicit fallacies.</p></sec><sec id="jeo270135-sec-0030"><title>Results</title><p>ChatGPT yielded an overall success rate of 48.3% based on the 2022 AAOS OITE. ChatGPT demonstrated the ability to apply logic and stepwise thinking in 67.6% of the questions. ChatGPT effectively utilized internal information from the question stem in 68.1% of the questions. ChatGPT also demonstrated the ability to incorporate external information in 68.1% of the questions. The utilization of logical reasoning (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.001), internal information (<italic toggle="yes">p</italic>&#x02009;=&#x02009;0.004) and external information (p&#x02009;=&#x02009;0.009) was greater among correct responses than incorrect responses. Informational fallacy was the most common shortcoming of ChatGPT's responses. There was no difference in correct responses based on whether or not an image was present (<italic toggle="yes">p</italic>&#x02009;=&#x02009;0.320).</p></sec><sec id="jeo270135-sec-0040"><title>Conclusions</title><p>ChatGPT demonstrates logical, informational and explicit fallacies which, at this time, may lead to misinformation and hinder resident education.</p></sec><sec id="jeo270135-sec-0050"><title>Level of Evidence</title><p>Level V.</p></sec></abstract><kwd-group><kwd id="jeo270135-kwd-0001">American Academy of Orthopaedic Surgeons (AAOS)</kwd><kwd id="jeo270135-kwd-0002">Chat Generative Pre&#x02010;Trained Transformer (ChatGPT)</kwd><kwd id="jeo270135-kwd-0003">Orthopaedic In&#x02010;Training Exam (OITE)</kwd><kwd id="jeo270135-kwd-0004">resident education</kwd></kwd-group><counts><fig-count count="0"/><table-count count="1"/><page-count count="7"/><word-count count="4417"/></counts><custom-meta-group><custom-meta><meta-name>source-schema-version-number</meta-name><meta-value>2.0</meta-value></custom-meta><custom-meta><meta-name>cover-date</meta-name><meta-value>January 2025</meta-value></custom-meta><custom-meta><meta-name>details-of-publishers-convertor</meta-name><meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.5.4 mode:remove_FC converted:27.02.2025</meta-value></custom-meta></custom-meta-group></article-meta><notes><p content-type="self-citation">
<mixed-citation publication-type="journal" id="jeo270135-cit-0031">
<string-name>
<surname>Mendiratta</surname>, <given-names>D.</given-names>
</string-name>, <string-name>
<surname>Herzog</surname>, <given-names>I.</given-names>
</string-name>, <string-name>
<surname>Singh</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Para</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Joshi</surname>, <given-names>T.</given-names>
</string-name>, <string-name>
<surname>Vosbikian</surname>, <given-names>M.</given-names>
</string-name> et al. (<year>2025</year>) <article-title>Utility of ChatGPT as a preparation tool for the Orthopaedic In&#x02010;Training Examination</article-title>. <source>Journal of Experimental Orthopaedics</source>, <volume>12</volume>, <elocation-id>e70135</elocation-id>. <pub-id pub-id-type="doi">10.1002/jeo2.70135</pub-id>
<pub-id pub-id-type="pmid">39749288</pub-id>
</mixed-citation>
</p></notes></front><body><def-list list-content="abbreviations"><title>Abbreviations</title><def-item><term>AAOS</term><def><p>American Academy of Orthopaedic Surgeons</p></def></def-item><def-item><term>ChatGPT</term><def><p>Chat Generative Pre&#x02010;Trained Transformer</p></def></def-item><def-item><term>LLM</term><def><p>large language model</p></def></def-item><def-item><term>OITE</term><def><p>Orthopaedic In&#x02010;Training Exam</p></def></def-item><def-item><term>PGY</term><def><p>postgraduate year</p></def></def-item><def-item><term>PSITE</term><def><p>Plastic Surgery In&#x02010;Service Examination</p></def></def-item></def-list><sec id="jeo270135-sec-0060"><title>INTRODUCTION</title><p>Examinations at the resident level are vital in evaluating future success, especially in surgery [<xref rid="jeo270135-bib-0029" ref-type="bibr">29</xref>]. Performance on in&#x02010;training examinations is predictive of future board certification [<xref rid="jeo270135-bib-0016" ref-type="bibr">16</xref>, <xref rid="jeo270135-bib-0027" ref-type="bibr">27</xref>]. Conversely, poor performance on in&#x02010;training examinations is associated with resident burnout [<xref rid="jeo270135-bib-0026" ref-type="bibr">26</xref>]. For the Orthopaedic In&#x02010;Training Examination (OITE), eleven domains are captured: basic science, foot and ankle, hand, hip and knee, oncology, paediatrics, shoulder and elbow, spine, sports medicine, trauma and practice management [<xref rid="jeo270135-bib-0018" ref-type="bibr">18</xref>]. The breadth of knowledge required for the OITE emphasizes the importance of understanding not only the procedural aspect of an individual's training, but also the growing research behind it. Hence, the resources used for exam preparation are under constant scrutiny due to the constantly evolving nature of the information that the questions are based on. While reviewing past examinations had the most positive impact on OITE score [<xref rid="jeo270135-bib-0023" ref-type="bibr">23</xref>], explanations will change from year to year, prompting residents to seek updated information from supplementary resources.</p><p>The recent introduction of Chat Generative Pre&#x02010;Trained Transformer (ChatGPT) may have implications as a novel educational resource when studying for the OITE. ChatGPT uses deep learning techniques to generate human&#x02010;like responses to user prompts. At the medical student level, ChatGPT has already been shown to achieve a passing score on USMLE Step 1 and Step 2&#x02010;CK, while providing applicable context and explanations for all questions [<xref rid="jeo270135-bib-0011" ref-type="bibr">11</xref>]. At the resident education level, ChatGPT has been utilized to answer the Plastic Surgery In&#x02010;Service Examination (PSITE) from 2022, as well as the PSITEs from recent years [<xref rid="jeo270135-bib-0013" ref-type="bibr">13</xref>, <xref rid="jeo270135-bib-0015" ref-type="bibr">15</xref>]. Given its validity as a resource in other realms of medicine, we aim to assess ChatGPT's performance on the OITE, ability to provide reasoning, and fallacies through evaluation of the generated responses.</p></sec><sec sec-type="materials-and-methods" id="jeo270135-sec-0070"><title>MATERIALS AND METHODS</title><p>The questions comprising the data set were sourced from the American Academy of Orthopaedic Surgeons (AAOS) website, which is a recognized and accepted question bank for OITE preparation. To cover the evolving orthopaedic surgery curriculum, all questions from the 2022 OITE were included. ChatGPT 3.5, developed by OpenAI, was used for this study.</p><p>All questions, even those with visual content, were included in the analysis. The questions were formatted in the same manner as presented on the AAOS website, with the question, narrative text and answer choices separated by a line. Each question was evaluated in a new chat session to minimize confounding variables, considering ChatGPT's ability to learn from mistakes and gather external information. ChatGPT has the ability to generate texts based on previous inputs [<xref rid="jeo270135-bib-0021" ref-type="bibr">21</xref>].</p><p>The questions and their corresponding answers, along with the analysis, were copied from the AAOS OITE website into a Microsoft Word document. Once ChatGPT provided responses, they were added to a shared Microsoft Word document. Two independent reviewers evaluated each response, and any discrepancies were resolved by a third reviewer. The responses provided by ChatGPT were assessed based on the principles of natural coherence [<xref rid="jeo270135-bib-0028" ref-type="bibr">28</xref>], which consider the prevalence of the following three characteristics in the responses:
<list list-type="bullet"><list-item><p>Logical reasoning: The system employed logical thinking and stepwise analysis to generate responses based on the information provided in the question stem.</p></list-item><list-item><p>Internal information: The system utilized the information available within the question stem to derive appropriate responses.</p></list-item><list-item><p>External information: The system incorporated external knowledge, including references to literature, to support its responses.</p></list-item></list>
</p><p>This method has been previously used in a similar study by Gupta et al. [<xref rid="jeo270135-bib-0013" ref-type="bibr">13</xref>].</p><p>For instance, for a question regarding a post&#x02010;hip arthroplasty patient presenting with persistent limp with torso tilting toward the ipsilateral surgical side, ChatGPT selected the correct answer with the response: &#x02018;This patient likely has abductor muscle weakness resulting in a Trendelenburg gait. This is a common complication following total hip arthroplasty and may lead to early implant failure if not addressed. The initial treatment is usually conservative with observation, non&#x02010;steroidal anti&#x02010;inflammatory drugs, and physical therapy to strengthen the abductor muscles. If conservative measures fail, operative intervention may be necessary. Magnetic resonance imaging or metal ion level testing may be indicated if an adverse local tissue reaction is suspected, but it is not the initial step in management for this patient's presentation. A tendon transfer would not be the first&#x02010;line treatment for this condition. Bone scan is not indicated for this patient's presentation&#x02019;. By proceeding in a stepwise fashion from the physical exam finding in the question (internal information) to deducing that the patient's treatment will likely be conservative, ChatGPT displayed logical reasoning. Further, by assessing potential complications, it exhibited its ability to call upon external information to answer the question correctly. For an incorrect response, the following categories were used, as per a similar study [<xref rid="jeo270135-bib-0013" ref-type="bibr">13</xref>]:
<list list-type="bullet"><list-item><p>Logical fallacy: The response demonstrated logical thinking but failed to provide an accurate answer to the question.</p></list-item><list-item><p>Informational fallacy: The response displayed logical reasoning but missed an essential piece of information from the question stem necessary for a correct answer.</p></list-item><list-item><p>Explicit fallacy: The response lacked logical reasoning and failed to utilize the information in the question stem to answer correctly.</p></list-item></list>
</p><p>For this categorization, only one option could be selected. An example of an explicit fallacy response was &#x02018;Positive alpha&#x02010;defensin test is a major criterion for infection in the 2018 definition of periprosthetic hip and knee infection&#x02019;, where ChatGPT did not use any logic or internal information and gave a randomly generated output. An instance in which logical fallacy occurs is when ChatGPT's logic makes an accurate interpretation of the problem but fails to reach proper treatment: &#x02018;The symptoms described in the scenario are suggestive of dorsal radial wrist impingement, also known as intersection syndrome. In this condition, the extensor pollicis longus and abductor pollicis longus tendons in the second dorsal compartment become inflamed and irritated as they cross over the tendons of the extensor carpi radialis brevis and longus in the first dorsal compartment. Therefore, it is most appropriate for surgery to be directed at the release of the first dorsal wrist compartment tendon sheath&#x02019;. The answer, however, was to perform a release of the second dorsal wrist compartment due to the tendons originating from the second compartment resulting in the tenosynovitis/bursitis. Specific questions could not be included due to copyright concerns. Data analysis, including accuracy assessment and statistical analysis, was performed using Microsoft Excel. To test for the precision of responses, five simulations were conducted in addition to the first simulation. To detect variation, a one&#x02010;sample <italic toggle="yes">t</italic> test was used. Unpaired chi&#x02010;square tests were conducted to evaluate the statistical significance of categorical variables, with a significance level of <italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.05. Mean comparisons were made using one&#x02010;sample or independent samples <italic toggle="yes">t</italic> tests where appropriate. In order to generate percentile data by postgraduate year (PGY) level, the OITE Technical Report from 2022 was used as the benchmark (<ext-link xlink:href="https://www.aaos.org/globalassets/education/product-pages/oite/oite-2022-technical-report-20230125.pdf" ext-link-type="uri">https://www.aaos.org/globalassets/education/product-pages/oite/oite-2022-technical-report-20230125.pdf</ext-link>).</p></sec><sec sec-type="results" id="jeo270135-sec-0080"><title>RESULTS</title><sec id="jeo270135-sec-0090"><title>ChatGPT performance</title><p>ChatGPT was utilized to answer a total of 207 questions in the OITE data set. Among the 207 questions, ChatGPT provided accurate responses to 100 questions, yielding an overall success rate of 48.3% based on the 2022 AAOS OITE. A manual simulation of ChatGPT was run five times, which yielded the following number of questions correct: 104 (50.2%), 103 (49.8%), 99 (47.8%), 102 (49.3%) and 99 (47.8%). This is an average of 101.4 questions correct out of 207 (49.0%). A one&#x02010;sample <italic toggle="yes">t</italic> test comparing our hypothetical mean (100 questions) to the simulation yielded no statistical significance (<italic toggle="yes">p</italic>&#x02009;=&#x02009;0.246). Using &#x02018;C&#x02019; as the standard answer choice yielded a 21.3% success rate, which was significantly different from our simulation results (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.001). Using the OITE technical report, ChatGPT&#x02010;3.5 performed at a 34th percentile PGY&#x02010;1 in the test taken in 2022. However, there was a drop&#x02010;off for every year thereafter until it performed below the first percentile at the PGY&#x02010;4 and &#x02010;5 levels. These findings are illustrated in Table&#x000a0;<xref rid="jeo270135-tbl-0001" ref-type="table">1</xref>.</p><table-wrap position="float" id="jeo270135-tbl-0001" content-type="Table"><label>Table 1</label><caption><p>2022 Performance on OITE by year.</p></caption><table frame="hsides" rules="groups"><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><col align="left" style="border-right:solid 1px #000000; border-bottom:solid 1px #000000" span="1"/><thead valign="bottom"><tr style="border-bottom:solid 1px #000000" valign="bottom"><th align="left" valign="bottom" rowspan="1" colspan="1"/><th align="left" valign="bottom" rowspan="1" colspan="1">ChatGPT&#x02010;3.5</th><th align="left" valign="bottom" rowspan="1" colspan="1">PGY&#x02010;1</th><th align="left" valign="bottom" rowspan="1" colspan="1">PGY&#x02010;2</th><th align="left" valign="bottom" rowspan="1" colspan="1">PGY&#x02010;3</th><th align="left" valign="bottom" rowspan="1" colspan="1">PGY&#x02010;4</th><th align="left" valign="bottom" rowspan="1" colspan="1">PGY&#x02010;5</th></tr></thead><tbody valign="top"><tr><td align="left" valign="top" rowspan="1" colspan="1">Mean number of questions correct (out of 275 (SD))</td><td align="left" valign="top" rowspan="1" colspan="1">134.75</td><td align="left" valign="top" rowspan="1" colspan="1">143.55 (20.91)</td><td align="left" valign="top" rowspan="1" colspan="1">159.69 (17.69)</td><td align="left" valign="top" rowspan="1" colspan="1">175.87 (23.27)</td><td align="left" valign="top" rowspan="1" colspan="1">185.72 (17.48)</td><td align="left" valign="top" rowspan="1" colspan="1">191.55 (16.61)</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Percentage correct</td><td align="left" valign="top" rowspan="1" colspan="1">49</td><td align="left" valign="top" rowspan="1" colspan="1">52.2</td><td align="left" valign="top" rowspan="1" colspan="1">58.1</td><td align="left" valign="top" rowspan="1" colspan="1">64</td><td align="left" valign="top" rowspan="1" colspan="1">67.5</td><td align="left" valign="top" rowspan="1" colspan="1">69.7</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">GPT&#x02010;3.5 percentile rank vs. cohort</td><td align="left" valign="top" rowspan="1" colspan="1">&#x02010;</td><td align="left" valign="top" rowspan="1" colspan="1">34</td><td align="left" valign="top" rowspan="1" colspan="1">8</td><td align="left" valign="top" rowspan="1" colspan="1">4</td><td align="left" valign="top" rowspan="1" colspan="1">&#x0003c;1</td><td align="left" valign="top" rowspan="1" colspan="1">&#x0003c;1</td></tr></tbody></table><table-wrap-foot><fn id="jeo270135-tbl1-note-0001"><p>Abbreviations: ChatGPT, Chat Generative Pre&#x02010;Trained Transformer; OITE, Orthopaedic In&#x02010;Training Exam; PGY, postgraduate year; SD, standard deviation.</p></fn></table-wrap-foot><permissions><copyright-holder>John Wiley &#x00026; Sons, Ltd.</copyright-holder></permissions></table-wrap></sec><sec id="jeo270135-sec-0100"><title>Evaluation of coherence and information utilization</title><p>ChatGPT's responses were evaluated based on the principles of natural coherence. In terms of logical reasoning, ChatGPT demonstrated the ability to apply logic and stepwise thinking in 67.6% of the questions. Furthermore, ChatGPT utilized internal information from the question stem in 68.1% of the questions. In addition to internal information, ChatGPT incorporated external information in 68.1% of the questions.</p></sec><sec id="jeo270135-sec-0110"><title>Performance comparison</title><p>When comparing the responses for correct answers to those for incorrect answers, several factors were assessed. In terms of logical reasoning, ChatGPT employed logical thinking in 80.0% of the questions with correct responses; however, logical reasoning was observed in only 56.1% of the incorrect questions. Furthermore, internal information utilization was observed in 77.0% of the questions with correct responses and 59.8% of the questions with incorrect responses. In terms of external information, ChatGPT utilized this resource in 76.0% of the questions with correct responses and 60.1% of the questions with incorrect responses. Statistical analysis revealed that the utilization of logical reasoning (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.001), internal information (p&#x02009;=&#x02009;0.004) and external information (<italic toggle="yes">p</italic>&#x02009;=&#x02009;0.009) was greater among correct responses than incorrect responses.</p><p>Regarding the impact of clinical context, when a vignette was present, ChatGPT answered 50.7% correctly compared to 42.1%, where no vignette was given. However, this was not statistically significant (<italic toggle="yes">p</italic>&#x02009;=&#x02009;0.057). When examining whether the presence of a vignette in an image&#x02010;based question impacted the response, ChatGPT was more likely to get the answer correct (47.4% vs. 33.3%, <italic toggle="yes">p</italic>&#x02009;=&#x02009;0.023). On the other hand, in a non&#x02010;image&#x02010;based question, ChatGPT maintained no difference in answering the question based on context (54.1% vs. 42.9%, <italic toggle="yes">p</italic>&#x02009;=&#x02009;0.472).</p></sec><sec id="jeo270135-sec-0120"><title>Identification of response fallacies</title><p>During the evaluation process, three types of response fallacies were identified for incorrect answers provided by ChatGPT:
<list list-type="bullet"><list-item><p>Logical fallacy: In 18.7% of the incorrect responses, ChatGPT demonstrated stepwise thinking but failed to provide an accurate answer to the question.</p></list-item><list-item><p>Informational fallacy: Approximately 66.4% of the incorrect responses displayed logical reasoning but failed to attribute a crucial piece of information from the question stem, resulting in an incorrect answer.</p></list-item><list-item><p>Explicit fallacy: In 15.0% of the incorrect responses, ChatGPT failed to demonstrate logical reasoning and was unable to utilize the information provided in the question stem to answer correctly.</p></list-item></list>
</p></sec><sec id="jeo270135-sec-0130"><title>Impact of visual content</title><p>Among the questions included in the study, 39.1% of them presented images in the question stem. For these image&#x02010;based questions, ChatGPT provided the correct answer 46.9% of the time. On the other hand, for questions without images, ChatGPT achieved an accuracy rate of 49.2%. However, statistical analysis indicated that the difference in performance between image&#x02010;based and non&#x02010;image&#x02010;based questions was not statistically significant, with a <italic toggle="yes">p</italic> value of 0.320.</p></sec></sec><sec sec-type="discussion" id="jeo270135-sec-0140"><title>DISCUSSION</title><p>This study demonstrates ChatGPT's efficacy in answering almost half of the questions on a 2022 AAOS OITE examination. Further, ChatGPT displayed the potential to provide logical, internal and external information on nearly two thirds of the questions it answered. When it answered incorrectly, it was most frequently due to an informational error, as opposed to a logical or explicit error. Gupta et al. produced similar findings in a study conducted on the 2022 PSITE wherein the total accuracy of answers was 54.96%, and external information was used more frequently in correct answers [<xref rid="jeo270135-bib-0013" ref-type="bibr">13</xref>]. By delineating the shortcomings of ChatGPT's responses, residents may be better advised on how to best utilize this tool in their OITE preparation.</p><p>In a recent study, Lum et al. posited that ChatGPT performed at the level of a 40th percentile PGY&#x02010;1 resident on the OITE, but fell below the 10th percentile when compared to all subsequent years [<xref rid="jeo270135-bib-0019" ref-type="bibr">19</xref>]. Our study corroborated these findings. Given the structure of an orthopaedic residency intern year, wherein residents are required to rotate in non&#x02010;orthopaedic specialities, a significant lack of exposure to surgical techniques may result in a shortage of information, contributing to the comparable performance of ChatGPT [<xref rid="jeo270135-bib-0008" ref-type="bibr">8</xref>].</p><p>ChatGPT demonstrated logical reasoning in 67.6% of all questions and included internal information from the prompt in 68.1% of all questions, incorporating both factors significantly more often when providing correct answers. These factors highlight ChatGPT's ability to comprehend questions and provide individualized feedback; the greater prevalence of correct answers indicates that stepwise rationale and identification of key details strongly influence the accuracy of the model [<xref rid="jeo270135-bib-0001" ref-type="bibr">1</xref>, <xref rid="jeo270135-bib-0009" ref-type="bibr">9</xref>]. When considering the inclusion of external information, ChatGPT provided sources from the literature in approximately 68% of all explanations, notably more significantly in correct answers. Increased familiarity with the literature has been associated with improved resident OITE performance; thus, the incorporation of external sources contributing to increased accuracy of the model is comprehensible [<xref rid="jeo270135-bib-0010" ref-type="bibr">10</xref>, <xref rid="jeo270135-bib-0030" ref-type="bibr">30</xref>]. However, given ChatGPT's known shortcomings in fabricating literature, it is difficult to assess the true source of the information [<xref rid="jeo270135-bib-0004" ref-type="bibr">4</xref>, <xref rid="jeo270135-bib-0012" ref-type="bibr">12</xref>, <xref rid="jeo270135-bib-0024" ref-type="bibr">24</xref>]. Future studies should compare the frequency of true sources and fictional sources to help elicit the model's tendency to fabricate when providing answers and explanations to resident&#x02010;level questions.</p><p>Interestingly, ChatGPT did not demonstrate increased difficulty in answering image&#x02010;based questions, a result which has been corroborated by its performance on the Radiology Board Examination [<xref rid="jeo270135-bib-0005" ref-type="bibr">5</xref>]. Given a clinical vignette with an image, ChatGPT was more likely to get the answer correct as opposed to when no context was given. Other studies have omitted image&#x02010;based questions when utilizing ChatGPT&#x02010;3.5, as it is difficult to draw conclusions regarding whether the accuracy of the model is due to efficacy or hallucination [<xref rid="jeo270135-bib-0003" ref-type="bibr">3</xref>]. The use of ChatGPT&#x02010;4 has been acknowledged as a solution to this limitation as the updated platform provides the ability to input images when providing prompts, improving the accuracy of the model in answering image&#x02010;based questions [<xref rid="jeo270135-bib-0002" ref-type="bibr">2</xref>].</p><p>In addition to characterizing correct responses provided by ChatGPT to understand factors influencing the model's accuracy, it is equally important to understand the limitations of the model by exploring the incorrect answers provided. While only approximately one&#x02010;fifth of the incorrect responses encountered a logical error, the fallacy indicates a major gap in the model. In these incorrect responses, ChatGPT provided a correct methodology for answering the question, however, was unable to elicit a correct response at the final step. The prevalence of logical fallacy highlights ChatGPT's established shortcoming of providing logical reasoning yet reaching inaccurate conclusions leading to misinformation [<xref rid="jeo270135-bib-0007" ref-type="bibr">7</xref>, <xref rid="jeo270135-bib-0017" ref-type="bibr">17</xref>]. Moreover, the majority of incorrect responses demonstrated informational fallacy, in which the model supplied stepwise reasoning but failed to incorporate key details from the prompt to provide a correct answer. ChatGPT's inability to correctly identify pertinent information while maintaining stepwise thinking has also been demonstrated on a medical&#x02010;student level parasitology examination and a board&#x02010;level ophthalmology examination, indicating a key shortcoming of the model in determining relevant information especially in a medical context [<xref rid="jeo270135-bib-0014" ref-type="bibr">14</xref>, <xref rid="jeo270135-bib-0022" ref-type="bibr">22</xref>]. In less than one&#x02010;fifth of the incorrect answers, ChatGPT demonstrated explicit fallacies where the model failed to provide either logical reasoning or key details from the question, emphasizing ChatGPT's tendency to hallucinate. A major concern with ChatGPT is its ability to provide fabricated information that seems to be reputable, particularly pertaining to science [<xref rid="jeo270135-bib-0006" ref-type="bibr">6</xref>, <xref rid="jeo270135-bib-0020" ref-type="bibr">20</xref>, <xref rid="jeo270135-bib-0025" ref-type="bibr">25</xref>]. The occurrence of explicit fallacies, albeit the frequency, increases vulnerability to misinformation which may potentially harm resident education. The recent development of ChatGPT&#x02010;4 provides a possible improvement on the current limitations of ChatGPT&#x02010;3.5.</p><p>As it currently stands, however, ChatGPT cannot serve as a standalone tool in the preparation for OITE examinations. Currently, a concern amongst orthopaedic educators is the rudimentary or superficial understanding of concepts in orthopaedic surgery, due to shift away from textbooks and research articles toward condensed guides and flashcards. Hence, the utilization of ChatGPT as another flashcard&#x02010;like modality would feed into the current memorization paradigm that exists in medical education. However, a potential utility of this resource would be to further explain correct or incorrect answers provided with the proper prompting information, which would incorporate evidence&#x02010;based articles to further learning. Due to the ever&#x02010;changing field of orthopaedic surgery and rapidly evolving techniques and guidelines, ChatGPT has the potential to serve as a foundational resource than current question banks, due to their growing obsolescence by day.</p><p>Limitations of this study include the use of only one OITE practice exam. Had this study been reproduced with multiple practice sets, it may have held more external validity. Further, bias on the part of the researcher may have influenced these findings, as ChatGPT could have used input from previous questions to formulate an answer for later questions. To mitigate this, a new conversation was utilized for each simulation that was performed. However, it is still uncertain whether ChatGPT has access to the other conversations. Overall, this allowed for improved internal validity. Finally, the omission of some elements of the question (tables or images) may yield erroneous results. Future studies should reproduce this study with newer large language models (LLMs) to mitigate these obstacles.</p></sec><sec sec-type="conclusions" id="jeo270135-sec-0150"><title>CONCLUSION</title><p>Preparation for the OITE is important for the success of orthopaedic surgery residents. LLMs, such as ChatGPT&#x02010;3.5, provide a unique learning resource that can improve resident education. However, ChatGPT currently represents a poor source of information reliability for medical education and cannot be recommended as a study source.</p></sec><sec id="jeo270135-sec-0160"><title>AUTHOR CONTRIBUTIONS</title><p>Dhruv Mendiratta, Isabel Herzog and Rohan Singh were responsible for the research design, acquisition, analysis and interpretation of data. Dhruv Mendiratta, Isabel Herzog, Rohan Singh, Ashok Para, Tej Joshi, Neil Kaushal and Michael Vosbikian were responsible for drafting and critical revision of the manuscript. All authors have read and approved the final submitted manuscript.</p></sec><sec sec-type="COI-statement" id="jeo270135-sec-0170"><title>CONFLICT OF INTEREST STATEMENT</title><p>The authors declare no conflicts of interest.</p></sec><sec id="jeo270135-sec-0200"><title>ETHICS STATEMENT</title><p>This study did not require ethical approval, as per institutional guidelines. Since there were no patients in this study, no informed consent was required.</p></sec></body><back><sec sec-type="data-availability" id="jeo270135-sec-0190"><title>DATA AVAILABILITY STATEMENT</title><p>Data are available upon reasonable request to the authors.</p></sec><ref-list content-type="cited-references" id="jeo270135-bibl-0001"><title>REFERENCES</title><ref id="jeo270135-bib-0001"><label>1</label><mixed-citation publication-type="journal" id="jeo270135-cit-0001">
<string-name>
<surname>Abid Haleem</surname>, <given-names>M.J.</given-names>
</string-name> &#x00026; <string-name>
<surname>Singh</surname>, <given-names>R.P.</given-names>
</string-name> (<year>2022</year>) <article-title>An era of ChatGPT as a significant futuristic support tool: a study on features, abilities, and challenges</article-title>. <source>BenchCouncil Transactions on Benchmarks, Standards and Evaluations</source>, <volume>2</volume>, <elocation-id>100089</elocation-id>.</mixed-citation></ref><ref id="jeo270135-bib-0002"><label>2</label><mixed-citation publication-type="journal" id="jeo270135-cit-0002">
<string-name>
<surname>Ali</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Tang</surname>, <given-names>O.Y.</given-names>
</string-name>, <string-name>
<surname>Connolly</surname>, <given-names>I.D.</given-names>
</string-name>, <string-name>
<surname>Fridley</surname>, <given-names>J.S.</given-names>
</string-name>, <string-name>
<surname>Shin</surname>, <given-names>J.H.</given-names>
</string-name>, <string-name>
<surname>Zadnik Sullivan</surname>, <given-names>P.L.</given-names>
</string-name> et al. (<year>2023</year>) <article-title>Performance of ChatGPT, GPT&#x02010;4, and google bard on a neurosurgery oral boards preparation question bank</article-title>. <source>Neurosurgery</source>, <volume>93</volume>, <fpage>1090</fpage>&#x02013;<lpage>1098</lpage>. Available from: <pub-id pub-id-type="doi">10.1227/neu.0000000000002551</pub-id>
<pub-id pub-id-type="pmid">37306460</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0003"><label>3</label><mixed-citation publication-type="journal" id="jeo270135-cit-0003">
<string-name>
<surname>Antaki</surname>, <given-names>F.</given-names>
</string-name>, <string-name>
<surname>Touma</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<surname>Milad</surname>, <given-names>D.</given-names>
</string-name>, <string-name>
<surname>El&#x02010;Khoury</surname>, <given-names>J.</given-names>
</string-name> &#x00026; <string-name>
<surname>Duval</surname>, <given-names>R.</given-names>
</string-name> (<year>2023</year>) <article-title>Evaluating the performance of ChatGPT in ophthalmology</article-title>. <source>Ophthalmology Science</source>, <volume>3</volume>, <elocation-id>100324</elocation-id>. Available from: <pub-id pub-id-type="doi">10.1016/j.xops.2023.100324</pub-id>
<pub-id pub-id-type="pmid">37334036</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0004"><label>4</label><mixed-citation publication-type="journal" id="jeo270135-cit-0004">
<string-name>
<surname>Bhattacharyya</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Miller</surname>, <given-names>V.M.</given-names>
</string-name>, <string-name>
<surname>Bhattacharyya</surname>, <given-names>D.</given-names>
</string-name> &#x00026; <string-name>
<surname>Miller</surname>, <given-names>L.E.</given-names>
</string-name> (<year>2023</year>) <article-title>High rates of fabricated and inaccurate references in ChatGPT&#x02010;generated medical content</article-title>. <source>Cureus</source>, <volume>15</volume>, <elocation-id>e39238</elocation-id>. Available from: <pub-id pub-id-type="doi">10.7759/cureus.39238</pub-id>
<pub-id pub-id-type="pmid">37337480</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0005"><label>5</label><mixed-citation publication-type="journal" id="jeo270135-cit-0005">
<string-name>
<surname>Bhayana</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Krishna</surname>, <given-names>S.</given-names>
</string-name> &#x00026; <string-name>
<surname>Bleakney</surname>, <given-names>R.R.</given-names>
</string-name> (<year>2023</year>) <article-title>Performance of ChatGPT on a radiology board&#x02010;style examination: insights into current strengths and limitations</article-title>. <source>Radiology</source>, <volume>307</volume>, <elocation-id>e230582</elocation-id>. Available from: <pub-id pub-id-type="doi">10.1148/radiol.230582</pub-id>
<pub-id pub-id-type="pmid">37191485</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0006"><label>6</label><mixed-citation publication-type="journal" id="jeo270135-cit-0006">
<string-name>
<surname>De Angelis</surname>, <given-names>L.</given-names>
</string-name>, <string-name>
<surname>Baglivo</surname>, <given-names>F.</given-names>
</string-name>, <string-name>
<surname>Arzilli</surname>, <given-names>G.</given-names>
</string-name>, <string-name>
<surname>Privitera</surname>, <given-names>G.P.</given-names>
</string-name>, <string-name>
<surname>Ferragina</surname>, <given-names>P.</given-names>
</string-name>, <string-name>
<surname>Tozzi</surname>, <given-names>A.E.</given-names>
</string-name> et al. (<year>2023</year>) <article-title>ChatGPT and the rise of large language models: the new AI&#x02010;driven infodemic threat in public health</article-title>. <source>Frontiers in Public Health</source>, <volume>11</volume>, <elocation-id>1166120</elocation-id>. Available from: <pub-id pub-id-type="doi">10.3389/fpubh.2023.1166120</pub-id>
<pub-id pub-id-type="pmid">37181697</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0007"><label>7</label><mixed-citation publication-type="journal" id="jeo270135-cit-0007">
<string-name>
<surname>Deebel</surname>, <given-names>N.A.</given-names>
</string-name> &#x00026; <string-name>
<surname>Terlecki</surname>, <given-names>R.</given-names>
</string-name> (<year>2023</year>) <article-title>ChatGPT performance on the American Urological Association Self&#x02010;assessment Study Program and the potential influence of artificial intelligence in urologic training</article-title>. <source>Urology</source>, <volume>177</volume>, <fpage>29</fpage>&#x02013;<lpage>33</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.urology.2023.05.010</pub-id>
<pub-id pub-id-type="pmid">37209880</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0008"><label>8</label><mixed-citation publication-type="journal" id="jeo270135-cit-0008">
<string-name>
<surname>Dougherty</surname>, <given-names>P.J.</given-names>
</string-name> &#x00026; <string-name>
<surname>Marcus</surname>, <given-names>R.E.</given-names>
</string-name> (<year>2013</year>) <article-title>ACGME and ABOS changes for the orthopaedic surgery PGY&#x02010;1 (intern) year</article-title>. <source>Clinical Orthopaedics &#x00026; Related Research</source>, <volume>471</volume>, <fpage>3412</fpage>&#x02013;<lpage>3416</lpage>. Available from: <pub-id pub-id-type="doi">10.1007/s11999-013-3227-9</pub-id>
<pub-id pub-id-type="pmid">23934034</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0009"><label>9</label><mixed-citation publication-type="journal" id="jeo270135-cit-0009">
<string-name>
<surname>Firat</surname>, <given-names>M.</given-names>
</string-name> (<year>2023</year>) <article-title>How Chat GPT can transform autodidactic experiences and open education?</article-title>
<source>OSF Preprints</source>. Available from: <pub-id pub-id-type="doi">10.31219/osf.io/9ge8m</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0010"><label>10</label><mixed-citation publication-type="journal" id="jeo270135-cit-0010">
<string-name>
<surname>Franklin</surname>, <given-names>C.C.</given-names>
</string-name>, <string-name>
<surname>Bosch</surname>, <given-names>P.P.</given-names>
</string-name>, <string-name>
<surname>Grudziak</surname>, <given-names>J.S.</given-names>
</string-name>, <string-name>
<surname>Dede</surname>, <given-names>O.</given-names>
</string-name>, <string-name>
<surname>Ramirez</surname>, <given-names>R.N.</given-names>
</string-name>, <string-name>
<surname>Mendelson</surname>, <given-names>S.A.</given-names>
</string-name> et al. (<year>2017</year>) <article-title>Does a weekly didactic conference improve resident performance on the pediatric domain of the orthopaedic in&#x02010;training examination?</article-title>
<source>Journal of Pediatric Orthopaedics</source>, <volume>37</volume>, <fpage>149</fpage>&#x02013;<lpage>153</lpage>. Available from: <pub-id pub-id-type="doi">10.1097/BPO.0000000000000726</pub-id>
<pub-id pub-id-type="pmid">26866645</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0011"><label>11</label><mixed-citation publication-type="journal" id="jeo270135-cit-0011">
<string-name>
<surname>Gilson</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Safranek</surname>, <given-names>C.W.</given-names>
</string-name>, <string-name>
<surname>Huang</surname>, <given-names>T.</given-names>
</string-name>, <string-name>
<surname>Socrates</surname>, <given-names>V.</given-names>
</string-name>, <string-name>
<surname>Chi</surname>, <given-names>L.</given-names>
</string-name>, <string-name>
<surname>Taylor</surname>, <given-names>R.A.</given-names>
</string-name> et al. (<year>2023</year>) <article-title>How does ChatGPT perform on the United States Medical Licensing Examination (USMLE)? The implications of large language models for medical education and knowledge assessment</article-title>. <source>JMIR Medical Education</source>, <volume>9</volume>, <elocation-id>e45312</elocation-id>. Available from: <pub-id pub-id-type="doi">10.2196/45312</pub-id>
<pub-id pub-id-type="pmid">36753318</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0012"><label>12</label><mixed-citation publication-type="journal" id="jeo270135-cit-0012">
<string-name>
<surname>Grigio</surname>, <given-names>T.R.</given-names>
</string-name>, <string-name>
<surname>Timmerman</surname>, <given-names>H.</given-names>
</string-name> &#x00026; <string-name>
<surname>Wolff</surname>, <given-names>A.P.</given-names>
</string-name> (<year>2023</year>) <article-title>ChatGPT in anaesthesia research: risk of fabrication in literature searches</article-title>. <source>British Journal of Anaesthesia</source>, <volume>131</volume>, <fpage>e29</fpage>&#x02013;<lpage>e30</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.bja.2023.04.009</pub-id>
<pub-id pub-id-type="pmid">37183102</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0013"><label>13</label><mixed-citation publication-type="journal" id="jeo270135-cit-0013">
<string-name>
<surname>Gupta</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Herzog</surname>, <given-names>I.</given-names>
</string-name>, <string-name>
<surname>Park</surname>, <given-names>J.B.</given-names>
</string-name>, <string-name>
<surname>Weisberger</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Firouzbakht</surname>, <given-names>P.</given-names>
</string-name>, <string-name>
<surname>Ocon</surname>, <given-names>V.</given-names>
</string-name> et al. (<year>2023</year>) <article-title>Performance of ChatGPT on the plastic surgery inservice training examination</article-title>. <source>Aesthetic Surgery Journal</source>, <volume>43</volume>, <fpage>NP1078</fpage>&#x02013;<lpage>NP1082</lpage>. Available from: <pub-id pub-id-type="doi">10.1093/asj/sjad128</pub-id>
<pub-id pub-id-type="pmid">37128784</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0014"><label>14</label><mixed-citation publication-type="journal" id="jeo270135-cit-0014">
<string-name>
<surname>Huh</surname>, <given-names>S.</given-names>
</string-name> (<year>2023</year>) <article-title>Are ChatGPT's knowledge and interpretation ability comparable to those of medical students in Korea for taking a parasitology examination?: a descriptive study</article-title>. <source>Journal of Educational Evaluation for Health Professions</source>, <volume>20</volume>, <elocation-id>1</elocation-id>. Available from: <pub-id pub-id-type="doi">10.3352/jeehp.2023.20.1</pub-id>
<pub-id pub-id-type="pmid">36627845</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0015"><label>15</label><mixed-citation publication-type="journal" id="jeo270135-cit-0015">
<string-name>
<surname>Humar</surname>, <given-names>P.</given-names>
</string-name>, <string-name>
<surname>Asaad</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Bengur</surname>, <given-names>F.B.</given-names>
</string-name> &#x00026; <string-name>
<surname>Nguyen</surname>, <given-names>V.</given-names>
</string-name> (<year>2023</year>) <article-title>ChatGPT is equivalent to first year plastic surgery residents: evaluation of ChatGPT on the plastic surgery in&#x02010;service exam</article-title>. <source>Aesthetic Surgery Journal</source>, <volume>43</volume>, <fpage>NP1085</fpage>&#x02013;<lpage>NP1089</lpage>. Available from: <pub-id pub-id-type="doi">10.1093/asj/sjad130</pub-id>
<pub-id pub-id-type="pmid">37140001</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0016"><label>16</label><mixed-citation publication-type="journal" id="jeo270135-cit-0016">
<string-name>
<surname>Jones</surname>, <given-names>A.T.</given-names>
</string-name>, <string-name>
<surname>Biester</surname>, <given-names>T.W.</given-names>
</string-name>, <string-name>
<surname>Buyske</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Lewis</surname>, <given-names>F.R.</given-names>
</string-name> &#x00026; <string-name>
<surname>Malangoni</surname>, <given-names>M.A.</given-names>
</string-name> (<year>2014</year>) <article-title>Using the American Board of Surgery In&#x02010;Training Examination to predict board certification: a cautionary study</article-title>. <source>Journal of Surgical Education</source>, <volume>71</volume>, <fpage>e144</fpage>&#x02013;<lpage>e148</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.jsurg.2014.04.004</pub-id>
<pub-id pub-id-type="pmid">24913429</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0017"><label>17</label><mixed-citation publication-type="journal" id="jeo270135-cit-0017">
<string-name>
<surname>Kung</surname>, <given-names>T.H.</given-names>
</string-name>, <string-name>
<surname>Cheatham</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Medenilla</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Sillos</surname>, <given-names>C.</given-names>
</string-name>, <string-name>
<surname>De Leon</surname>, <given-names>L.</given-names>
</string-name>, <string-name>
<surname>Elepa&#x000f1;o</surname>, <given-names>C.</given-names>
</string-name> et al. (<year>2023</year>) <article-title>Performance of ChatGPT on USMLE: potential for AI&#x02010;assisted medical education using large language models</article-title>. <source>PLOS Digital Health</source>, <volume>2</volume>, <elocation-id>e0000198</elocation-id>. Available from: <pub-id pub-id-type="doi">10.1371/journal.pdig.0000198</pub-id>
<pub-id pub-id-type="pmid">36812645</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0018"><label>18</label><mixed-citation publication-type="journal" id="jeo270135-cit-0018">
<string-name>
<surname>Le</surname>, <given-names>H.V.</given-names>
</string-name>, <string-name>
<surname>Wick</surname>, <given-names>J.B.</given-names>
</string-name>, <string-name>
<surname>Haus</surname>, <given-names>B.M.</given-names>
</string-name> &#x00026; <string-name>
<surname>Dyer</surname>, <given-names>G.S.M.</given-names>
</string-name> (<year>2021</year>) <article-title>Orthopaedic in&#x02010;training examination: history, perspective, and tips for residents</article-title>. <source>Journal of the American Academy of Orthopaedic Surgeons</source>, <volume>29</volume>, <fpage>e427</fpage>&#x02013;<lpage>e437</lpage>. Available from: <pub-id pub-id-type="doi">10.5435/JAAOS-D-20-01020</pub-id>
<pub-id pub-id-type="pmid">33417380</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0019"><label>19</label><mixed-citation publication-type="journal" id="jeo270135-cit-0019">
<string-name>
<surname>Lum</surname>, <given-names>Z.C.</given-names>
</string-name> (<year>2023</year>) <article-title>Can artificial intelligence pass the American Board of Orthopaedic Surgery examination? Orthopaedic residents versus ChatGPT</article-title>. <source>Clinical Orthopaedics &#x00026; Related Research</source>, <volume>481</volume>, <fpage>1623</fpage>&#x02013;<lpage>1630</lpage>. Available from: <pub-id pub-id-type="doi">10.1097/CORR.0000000000002704</pub-id>
<pub-id pub-id-type="pmid">37220190</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0020"><label>20</label><mixed-citation publication-type="journal" id="jeo270135-cit-0020">
<string-name>
<surname>Manohar</surname>, <given-names>N.</given-names>
</string-name> &#x00026; <string-name>
<surname>Prasad</surname>, <given-names>S.S.</given-names>
</string-name> (<year>2023</year>) <article-title>Use of ChatGPT in Academic Publishing: a rare case of seronegative systemic lupus erythematosus in a patient with HIV infection</article-title>. <source>Cureus</source>, <volume>15</volume>, <elocation-id>e34616</elocation-id>. Available from: <pub-id pub-id-type="doi">10.7759/cureus.34616</pub-id>
<pub-id pub-id-type="pmid">36895547</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0021"><label>21</label><mixed-citation publication-type="journal" id="jeo270135-cit-0021">
<string-name>
<surname>Menon</surname>, <given-names>D.</given-names>
</string-name> &#x00026; <string-name>
<surname>Shilpa</surname>, <given-names>K.</given-names>
</string-name> (<year>2023</year>) <article-title>&#x0201c;Chatting with ChatGPT&#x0201d;: analyzing the factors influencing users' intention to use the open AI's ChatGPT using the UTAUT model</article-title>. <source>Heliyon</source>, <volume>9</volume>, <elocation-id>e20962</elocation-id>. Available from: <pub-id pub-id-type="doi">10.1016/j.heliyon.2023.e20962</pub-id>
<pub-id pub-id-type="pmid">37928033</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0022"><label>22</label><mixed-citation publication-type="journal" id="jeo270135-cit-0022">
<string-name>
<surname>Mihalache</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Popovic</surname>, <given-names>M.M.</given-names>
</string-name> &#x00026; <string-name>
<surname>Muni</surname>, <given-names>R.H.</given-names>
</string-name> (<year>2023</year>) <article-title>Performance of an artificial intelligence chatbot in ophthalmic knowledge assessment</article-title>. <source>JAMA Ophthalmology</source>, <volume>141</volume>, <fpage>589</fpage>&#x02013;<lpage>597</lpage>. Available from: <pub-id pub-id-type="doi">10.1001/jamaophthalmol.2023.1144</pub-id>
<pub-id pub-id-type="pmid">37103928</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0023"><label>23</label><mixed-citation publication-type="journal" id="jeo270135-cit-0023">
<string-name>
<surname>Rowe</surname>, <given-names>N.</given-names>
</string-name>, <string-name>
<surname>Familia</surname>, <given-names>M.C.</given-names>
</string-name>, <string-name>
<surname>Brown</surname>, <given-names>S.M.</given-names>
</string-name> &#x00026; <string-name>
<surname>Mulcahey</surname>, <given-names>M.K.</given-names>
</string-name> (<year>2021</year>) <article-title>Orthopaedic in&#x02010;training exam preparation among orthopaedic surgery residency programs</article-title>. <source>Journal of Surgical Education</source>, <volume>78</volume>, <fpage>2146</fpage>&#x02013;<lpage>2151</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.jsurg.2021.04.022</pub-id>
<pub-id pub-id-type="pmid">34052142</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0024"><label>24</label><mixed-citation publication-type="journal" id="jeo270135-cit-0024">
<string-name>
<surname>Sanchez&#x02010;Ramos</surname>, <given-names>L.</given-names>
</string-name>, <string-name>
<surname>Lin</surname>, <given-names>L.</given-names>
</string-name> &#x00026; <string-name>
<surname>Romero</surname>, <given-names>R.</given-names>
</string-name> (<year>2023</year>) <article-title>Beware of references when using ChatGPT as a source of information to write scientific articles</article-title>. <source>American Journal of Obstetrics and Gynecology</source>, <volume>229</volume>, <fpage>356</fpage>&#x02013;<lpage>357</lpage>. Available from: <pub-id pub-id-type="doi">10.1016/j.ajog.2023.04.004</pub-id>
<pub-id pub-id-type="pmid">37031761</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0025"><label>25</label><mixed-citation publication-type="journal" id="jeo270135-cit-0025">
<string-name>
<surname>Shen</surname>, <given-names>Y.</given-names>
</string-name>, <string-name>
<surname>Heacock</surname>, <given-names>L.</given-names>
</string-name>, <string-name>
<surname>Elias</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Hentel</surname>, <given-names>K.D.</given-names>
</string-name>, <string-name>
<surname>Reig</surname>, <given-names>B.</given-names>
</string-name>, <string-name>
<surname>Shih</surname>, <given-names>G.</given-names>
</string-name> et al. (<year>2023</year>) <article-title>ChatGPT and other large language models are double&#x02010;edged swords</article-title>. <source>Radiology</source>, <volume>307</volume>, <elocation-id>e230163</elocation-id>. Available from: <pub-id pub-id-type="doi">10.1148/radiol.230163</pub-id>
<pub-id pub-id-type="pmid">36700838</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0026"><label>26</label><mixed-citation publication-type="journal" id="jeo270135-cit-0026">
<string-name>
<surname>Strauss</surname>, <given-names>E.J.</given-names>
</string-name>, <string-name>
<surname>Markus</surname>, <given-names>D.H.</given-names>
</string-name>, <string-name>
<surname>Kingery</surname>, <given-names>M.T.</given-names>
</string-name>, <string-name>
<surname>Zuckerman</surname>, <given-names>J.</given-names>
</string-name> &#x00026; <string-name>
<surname>Egol</surname>, <given-names>K.A.</given-names>
</string-name> (<year>2019</year>) <article-title>Orthopaedic resident burnout is associated with poor in&#x02010;training examination performance</article-title>. <source>Journal of Bone and Joint Surgery</source>, <volume>101</volume>, <elocation-id>e102</elocation-id>. Available from: <pub-id pub-id-type="doi">10.2106/JBJS.18.00979</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0027"><label>27</label><mixed-citation publication-type="journal" id="jeo270135-cit-0027">
<string-name>
<surname>Swanson</surname>, <given-names>D.</given-names>
</string-name>, <string-name>
<surname>Marsh</surname>, <given-names>J.L.</given-names>
</string-name>, <string-name>
<surname>Hurwitz</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<surname>DeRosa</surname>, <given-names>G.P.</given-names>
</string-name>, <string-name>
<surname>Holtzman</surname>, <given-names>K.</given-names>
</string-name>, <string-name>
<surname>Bucak</surname>, <given-names>S.D.</given-names>
</string-name> et al. (<year>2013</year>) <article-title>Utility of AAOS OITE scores in predicting ABOS Part I outcomes: AAOS exhibit selection</article-title>. <source>The Journal of Bone and Joint Surgery. American Volume</source>, <volume>95</volume>, <elocation-id>e84</elocation-id>. Available from: <pub-id pub-id-type="doi">10.2106/JBJS.L.00457</pub-id>
<pub-id pub-id-type="pmid">23783215</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0028"><label>28</label><mixed-citation publication-type="journal" id="jeo270135-cit-0028">
<string-name>
<surname>Trabasso</surname>, <given-names>T.</given-names>
</string-name> (<year>1991</year>) <article-title>The development of coherence in narratives by understanding intentional action</article-title>. <source>Advances in Psychology</source>, <volume>79</volume>, <fpage>297</fpage>&#x02013;<lpage>314</lpage>.</mixed-citation></ref><ref id="jeo270135-bib-0029"><label>29</label><mixed-citation publication-type="journal" id="jeo270135-cit-0029">
<string-name>
<surname>Wade</surname>, <given-names>T.P.</given-names>
</string-name>, <string-name>
<surname>Andrus</surname>, <given-names>C.H.</given-names>
</string-name> &#x00026; <string-name>
<surname>Kaminski</surname>, <given-names>D.L.</given-names>
</string-name> (<year>1993</year>) <article-title>Evaluations of surgery resident performance correlate with success in board examinations</article-title>. <source>Surgery</source>, <volume>113</volume>, <fpage>644</fpage>&#x02013;<lpage>648</lpage>.<pub-id pub-id-type="pmid">8506522</pub-id>
</mixed-citation></ref><ref id="jeo270135-bib-0030"><label>30</label><mixed-citation publication-type="journal" id="jeo270135-cit-0030">
<string-name>
<surname>Weglein</surname>, <given-names>D.G.</given-names>
</string-name>, <string-name>
<surname>Gugala</surname>, <given-names>Z.</given-names>
</string-name>, <string-name>
<surname>Simpson</surname>, <given-names>S.</given-names>
</string-name> &#x00026; <string-name>
<surname>Lindsey</surname>, <given-names>R.W.</given-names>
</string-name> (<year>2015</year>) <article-title>Impact of a weekly reading program on orthopedic surgery residents' in&#x02010;training examination</article-title>. <source>Orthopedics</source>, <volume>38</volume>, <fpage>e387</fpage>&#x02013;<lpage>e393</lpage>. Available from: <pub-id pub-id-type="doi">10.3928/01477447-20150504-55</pub-id>
<pub-id pub-id-type="pmid">25970365</pub-id>
</mixed-citation></ref></ref-list></back></article>