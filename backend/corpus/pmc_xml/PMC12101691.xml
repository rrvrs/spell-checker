<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408327</article-id><article-id pub-id-type="pmc">PMC12101691</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0318372</article-id><article-id pub-id-type="publisher-id">PONE-D-24-35478</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Audio Signal Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Decision Making</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject><subj-group><subject>Recurrent Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject><subj-group><subject>Recurrent Neural Networks</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Autonomous agents: Augmenting visual information with raw audio data</article-title><alt-title alt-title-type="running-head">Autonomous agents: Augmenting visual information with raw audio data</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2527-6375</contrib-id><name><surname>Solomon</surname><given-names>Enoch</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff"/><xref rid="cor001" ref-type="corresp">*</xref></contrib></contrib-group><aff id="aff001">
<addr-line>Department of Computer Science, Virginia State University, Petersburg, Virginia, United States of America</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Mastroianni</surname><given-names>Michele</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>University of Foggia: Universita degli Studi di Foggia, ITALY</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>No author has competing interests.</p></fn><corresp id="cor001">* E-mail: <email>esolomon@vsu.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0318372</elocation-id><history><date date-type="received"><day>17</day><month>8</month><year>2024</year></date><date date-type="accepted"><day>14</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Enoch Solomon</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Enoch Solomon</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0318372.pdf">
</self-uri><abstract><p>In the realm of game playing, deep reinforcement learning predominantly relies on visual input to map states to actions. The visual data extracted from the game environment serves as the primary foundation for state representation in reinforcement learning agents. However, humans leverage additional sensory inputs, such as audio cues, which play a pivotal role in perception and decision-making. Therefore, incorporating raw audio along with visual information shows potential for offering valuable insights to reinforcement learning agents. This study advocates for the integration of raw audio samples as complementary information to visual data in state representation. By using raw audio with visual cues, our objective is to enrich the decision-making process of the agent at each stage. Experimental evaluation were conducted employing Deep Q Networks (DQN) and Proximal Policy Optimization (PPO) algorithms within ViZDoom and Unity reinforcement learning environments. The results of our experiments reveal that augmenting visual information with raw audio samples yields superior rewards and expedites the learning rate compared to relying solely on visual data. Additionally, the findings suggest that considering both visual and audio features enhances the agent&#x02019;s behavior, a trend observed across Unity and ViZDoom environments. This study underscores the potential advantages of incorporating multisensory information, particularly raw audio, into the state representation of reinforcement learning agents. Such insights contribute to advancing our understanding of how agents perceive and engage with their environments, ultimately enhancing performance in complex gaming scenarios.</p></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="8"/><table-count count="2"/><page-count count="19"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data are within the manuscript.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data are within the manuscript.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>1 Introduction</title><p>Reinforcement learning (RL) constitutes a key domain within machine learning, where an agent learns through iterative interactions with an environment to accomplish specific objectives. It delves into the study of problems and algorithms geared towards shaping policies that optimize decision-making to maximize rewards obtained from the environment [<xref rid="pone.0318372.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0318372.ref003" ref-type="bibr">3</xref>]. Central to RL is the conceptual framework of a Markov decision process, which models the problem to be resolved [<xref rid="pone.0318372.ref004" ref-type="bibr">4</xref>,<xref rid="pone.0318372.ref005" ref-type="bibr">5</xref>]. Core components encompass agents, environments, states, actions, and rewards, orchestrating a sequence where an agent selects actions, transitions to new states, and receives rewards contingent on its decisions.</p><p>Successful applications of RL span various domains, ranging from solving physics-based control problems [<xref rid="pone.0318372.ref006" ref-type="bibr">6</xref>] to diverse applications like Atari games [<xref rid="pone.0318372.ref007" ref-type="bibr">7</xref>], robotic manipulation [<xref rid="pone.0318372.ref008" ref-type="bibr">8</xref>], self-driving vehicles [<xref rid="pone.0318372.ref009" ref-type="bibr">9</xref>], and intelligent agents for 2D, 3D, and virtual reality games [<xref rid="pone.0318372.ref010" ref-type="bibr">10</xref>,<xref rid="pone.0318372.ref011" ref-type="bibr">11</xref>]. The crux of RL lies in formulating policies that optimize long-term rewards from the environment [<xref rid="pone.0318372.ref002" ref-type="bibr">2</xref>,<xref rid="pone.0318372.ref003" ref-type="bibr">3</xref>]. An optimal policy guides the agent to navigate the environment efficiently, maximizing cumulative rewards across episodes by considering future scenarios and rewards rather than solely focusing on immediate gains.</p><p>Recent advances in deep learning have unlocked the ability to extract high-level features from raw sensory data, propelling advancements in computer vision [<xref rid="pone.0318372.ref012" ref-type="bibr">12</xref>&#x02013;<xref rid="pone.0318372.ref017" ref-type="bibr">17</xref>] and speech recognition [<xref rid="pone.0318372.ref018" ref-type="bibr">18</xref>]. In particular, convolutional neural networks (CNNs) have demonstrated remarkable prowess in visual recognition tasks [<xref rid="pone.0318372.ref019" ref-type="bibr">19</xref>] and audio analysis [<xref rid="pone.0318372.ref020" ref-type="bibr">20</xref>,<xref rid="pone.0318372.ref021" ref-type="bibr">21</xref>]. Despite these breakthroughs, existing RL approaches predominantly rely on visual information for state representation [<xref rid="pone.0318372.ref022" ref-type="bibr">22</xref>], overlooking the potential benefits of integrating audio cues.</p><p>However, human perception benefits from diverse sensory inputs, including auditory cues, which are pivotal in providing crucial gameplay cues and environmental awareness in gaming contexts [<xref rid="pone.0318372.ref023" ref-type="bibr">23</xref>]. Audio cues, such as sounds and music, are instrumental in conveying environmental information to agents [<xref rid="pone.0318372.ref024" ref-type="bibr">24</xref>]. For example, audio cues can signal changes in direction, alerting the agent to potential dangers or opportunities. The absence of such auditory inputs in RL models utilizing only visual information may hinder optimal performance and impede learning efficiency, particularly in scenarios where critical events may be missed, as in the case of a loud crash occurring out of sight. Moreover, the incorporation of audio cues is indispensable for enhancing gaming experiences for visually impaired individuals [<xref rid="pone.0318372.ref025" ref-type="bibr">25</xref>].</p><p>Reinforcement Learning (RL) has been applied in various multi-modal learning tasks. For example, [<xref rid="pone.0318372.ref026" ref-type="bibr">26</xref>] utilized RL to improve machine translation across different data modalities. Similarly, [<xref rid="pone.0318372.ref027" ref-type="bibr">27</xref>] applied RL to image-based question answering, an inherently multi-modal learning task. RL has also been used in the context of visual dialogues, integrating multiple modalities [<xref rid="pone.0318372.ref028" ref-type="bibr">28</xref>]. However, none of these studies have explored the use of RL for Active Learning (AL) with multimodal data. The most similar approach to ours is the multi-view AL framework [<xref rid="pone.0318372.ref029" ref-type="bibr">29</xref>], which employs standard heuristic AL strategies to select data from multiple views.</p><p>Learning solely from raw visual data presents challenges, particularly in scenarios requiring intricate spatial navigation where direct line-of-sight is obstructed. For instance, consider a searching task where an agent equipped solely with visual information struggles to systematically locate a target. In such instances, leveraging raw audio samples alongside visual inputs could furnish valuable information, enhancing the agent&#x02019;s decision-making capabilities. Accordingly, this work advocates for the integration of raw audio samples as supplementary input alongside visual information in RL tasks, exploring their impact on decision-making processes. Additionally, we evaluate the efficacy of using solely raw audio samples in gaming environments.</p><p>Inspired by the natural inclination to guide others using spoken language, prompting the query, "Why do autonomous agents not leverage auditory cues?", our investigation aims to shed light on the untapped potential of incorporating auditory inputs directly into RL frameworks.</p><p>To demonstrate the efficacy of integrating raw audio samples into reinforcement learning tasks, we assess the proposed system&#x02019;s performance across two prominent reinforcement learning environments: ViZDoom [<xref rid="pone.0318372.ref030" ref-type="bibr">30</xref>] and Unity ML [<xref rid="pone.0318372.ref010" ref-type="bibr">10</xref>,<xref rid="pone.0318372.ref031" ref-type="bibr">31</xref>]. Our experiments encompass two main setups: utilizing only visual information and incorporating both visual data and audio features. The experimental outcomes, conducted within both Unity and ViZDoom environments, reveal that a reinforcement learning agent trained using visual information alongside raw audio samples achieves superior average rewards and demonstrates accelerated learning compared to relying solely on visual inputs.</p><p>This work makes significant strides in the field of reinforcement learning (RL) by addressing the limitations of conventional approaches that predominantly rely on visual information for state representation. While deep reinforcement learning has shown remarkable success in various domains, its reliance on visual data alone overlooks the potential benefits of incorporating additional sensory inputs, such as audio cues. Human perception and decision-making are inherently multisensory, integrating auditory, visual, and other sensory information to navigate complex environments effectively. Inspired by this natural multisensory integration, our research proposes and validates a novel framework that combines raw audio samples with visual data, aiming to enhance the performance and learning efficiency of RL agents.</p><p>The main contributions of this work are multi-faceted:</p><list list-type="bullet"><list-item><p><bold>Innovative Multisensory Integration Framework:</bold> We introduce and validate a framework that combines raw audio cues with visual information to enrich the state representation of RL agents. This approach leverages the complementary nature of auditory and visual inputs, reflecting the multifaceted way humans perceive and interact with their environments.</p></list-item><list-item><p><bold>Comprehensive Experimental Evaluation:</bold> Using Deep Q Networks (DQN) and Proximal Policy Optimization (PPO) algorithms, we rigorously evaluate the proposed multisensory framework in two prominent RL environments: ViZDoom and Unity ML. Our experimental results demonstrate that incorporating raw audio samples alongside visual data leads to significantly higher average rewards and accelerates the learning process compared to agents relying solely on visual inputs. This empirical validation highlights the practical benefits of multisensory integration in complex RL tasks.</p></list-item><list-item><p><bold>Enhanced Agent Performance and Behavior:</bold> Our findings reveal that the addition of audio features substantially improves agent performance. Specifically, agents trained with both visual and audio inputs exhibit more effective decision-making and behavior, overcoming limitations of visual-only models, such as missing critical events or navigating challenging scenarios where visual information is obstructed.</p></list-item><list-item><p><bold>Advancement of Sensory Integration in RL:</bold> This study underscores the critical importance of incorporating diverse sensory inputs into RL frameworks. By demonstrating the advantages of raw audio in state representation, we provide new insights into how RL agents can better perceive and engage with their environments. This advancement is particularly relevant for scenarios where visual data alone is insufficient, such as in environments with obstructed views or for individuals with visual impairments.</p></list-item><list-item><p><bold>Future Research Directions:</bold> Our work opens several avenues for future exploration. These include investigating the specific types of auditory cues that are most beneficial for decision-making, evaluating the impact of audio on various neural network architectures, such as recurrent neural networks, and extending our approach to other RL tasks and environments. Additionally, comparing agents with and without audio inputs using advanced neural network models could provide deeper insights into how multisensory data influences learning dynamics.</p></list-item></list><p>In summary, this work makes a significant contribution by demonstrating the value of integrating raw audio with visual information in reinforcement learning. Our findings advance the field by enhancing agent performance and providing a foundation for future research into multisensory integration in complex RL environments. This work not only enhances current RL methodologies but also opens up exciting new avenues for research and application, ultimately contributing to the development of more sophisticated and capable AI systems.</p><p>The remainder of this paper is as follows. The subsequent sections provide an overview of the deep reinforcement learning techniques employed in our experiments and delineate the architecture of the proposed multimodal reinforcement learning system. Sections V and VI discuss into the experimental results and draw conclusions, respectively.</p></sec><sec id="sec002"><title>2 Deep reinforcement learning</title><p>Reinforcement learning methods are designed to tackle tasks within a given environment by accumulating experiences from interactions with that environment and subsequently learning from them [<xref rid="pone.0318372.ref002" ref-type="bibr">2</xref>,<xref rid="pone.0318372.ref032" ref-type="bibr">32</xref>]. The environment is typically formalized as a Markov Decision Process (MDP), characterized by a set of possible states <inline-formula id="pone.0318372.e001"><alternatives><graphic xlink:href="pone.0318372.e001.jpg" id="pone.0318372.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, a set of possible actions <inline-formula id="pone.0318372.e002"><alternatives><graphic xlink:href="pone.0318372.e002.jpg" id="pone.0318372.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mi>a</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, a reward function <inline-formula id="pone.0318372.e003"><alternatives><graphic xlink:href="pone.0318372.e003.jpg" id="pone.0318372.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mi>&#x0211d;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and a transition distribution between states <inline-formula id="pone.0318372.e004"><alternatives><graphic xlink:href="pone.0318372.e004.jpg" id="pone.0318372.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The notation distinguishes values at different time steps, denoted by <inline-formula id="pone.0318372.e005"><alternatives><graphic xlink:href="pone.0318372.e005.jpg" id="pone.0318372.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x02115;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Within each state, a policy <inline-formula id="pone.0318372.e006"><alternatives><graphic xlink:href="pone.0318372.e006.jpg" id="pone.0318372.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003c0;</mml:mi><mml:mi>:</mml:mi><mml:mi>s</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> dictates the selection of an action <italic toggle="yes">a</italic><sub><italic toggle="yes">t</italic></sub>, following which the environment transitions to the next state <inline-formula id="pone.0318372.e007"><alternatives><graphic xlink:href="pone.0318372.e007.jpg" id="pone.0318372.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>~</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Subsequently, the policy (agent) receives a reward <inline-formula id="pone.0318372.e008"><alternatives><graphic xlink:href="pone.0318372.e008.jpg" id="pone.0318372.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> based on this experience. To streamline the discussion, we focus on <italic toggle="yes">episodic games</italic>, wherein the game concludes upon the agent reaching a terminal state.</p><p>MDP within reinforcement learning enhances the agent&#x02019;s ability to make decisions based on richer, multi-modal sensory inputs. The process involves combining the strengths of both types of data, requiring careful design of neural architectures, efficient learning algorithms, and methods to handle temporal and spatial dependencies across modalities. By addressing these challenges, agents can perform more effectively in real-world environments, where multi-modal data is often essential for accurate perception and decision-making.</p><p>The goal of reinforcement learning is to learn a policy that maximizes the return from any time step <inline-formula id="pone.0318372.e009"><alternatives><graphic xlink:href="pone.0318372.e009.jpg" id="pone.0318372.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x02115;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> until the end of the episode <inline-formula id="pone.0318372.e010"><alternatives><graphic xlink:href="pone.0318372.e010.jpg" id="pone.0318372.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x02115;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula></p><disp-formula id="pone.0318372.e011"><alternatives><graphic xlink:href="pone.0318372.e011.jpg" id="pone.0318372.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover></mml:mstyle><mml:msup><mml:mi>&#x003b3;</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><p>where the <italic toggle="yes">discount factor</italic>
<inline-formula id="pone.0318372.e012"><alternatives><graphic xlink:href="pone.0318372.e012.jpg" id="pone.0318372.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is used to stabilize the learning and/or weight the importance of future states vs. nearby states.</p><p><italic toggle="yes">Proximal Policy Optimization</italic> (PPO) [<xref rid="pone.0318372.ref033" ref-type="bibr">33</xref>] stands out as one of the most successful deep reinforcement learning methods, consistently achieving state-of-the-art performance across a diverse array of challenging tasks [<xref rid="pone.0318372.ref033" ref-type="bibr">33</xref>,<xref rid="pone.0318372.ref034" ref-type="bibr">34</xref>]. Notably, it has become one of the most widely adopted reinforcement learning algorithms within popular frameworks [<xref rid="pone.0318372.ref010" ref-type="bibr">10</xref>,<xref rid="pone.0318372.ref035" ref-type="bibr">35</xref>,<xref rid="pone.0318372.ref036" ref-type="bibr">36</xref>]. PPO combines elements from actor-critic and policy-gradient methods, leveraging a compact set of experiences from the environment before performing gradient ascent to maximize expected return (see Equation (<xref rid="pone.0318372.e011" ref-type="disp-formula">1</xref>)). Moreover, PPO incorporates constraints to prevent drastic updates, a feature that has demonstrated enhanced results compared to standard actor-critic methods [<xref rid="pone.0318372.ref033" ref-type="bibr">33</xref>].</p><p>Reinforcement learning algorithms have proven successful in various domains, as evidenced by their application in tasks such as object localization under diverse surface conditions [<xref rid="pone.0318372.ref037" ref-type="bibr">37</xref>]. In this context, we employ the Actor-Critic approach for our PPO agent, which comprises two deep neural networks: the Actor and the Critic. The Actor learns to select actions based on observed environmental states, while the Critic evaluates the efficacy of these actions in improving the environment&#x02019;s state. Interaction with the game environment occurs over a fixed number of steps, during which experiences are collected to update the models&#x02019; policies. In our setup, the reinforcement learning agent receives either RGB images of the game or raw audio samples as input and outputs one of the feasible actions.</p><p>Subsequently, the predicted action is executed in the game environment, and the resulting outcomes are observed. Positive outcomes yield rewards, which are then processed by the Critic model. The Critic&#x02019;s primary function is to assess whether the action improves the environment&#x02019;s state and provide feedback to the Actor accordingly. It generates a real-valued rating (Q-value) for the action taken in the preceding state. By comparing this rating, the Actor can refine its policy to make better decisions.</p><p><italic toggle="yes">Deep Q-networks</italic> (DQN), introduced by [<xref rid="pone.0318372.ref007" ref-type="bibr">7</xref>], have enabled the application of Q-learning [<xref rid="pone.0318372.ref039" ref-type="bibr">39</xref>,<xref rid="pone.0318372.ref042" ref-type="bibr">42</xref>] in complex environments like video games. DQN is a model-free off-policy algorithm designed to estimate the long-term expected returns from executing actions in specific states. These estimated returns, known as Q-values, indicate the potential long-term rewards associated with particular actions. DQN leverages deep learning to estimate the value function instead of a discrete table, employs two techniques&#x02014;replay memory and target networks&#x02014;to stabilize learning. Notably, DQN has demonstrated success in playing various Atari games [<xref rid="pone.0318372.ref007" ref-type="bibr">7</xref>] and Doom [<xref rid="pone.0318372.ref030" ref-type="bibr">30</xref>]. Due to its efficacy and simplicity, we employ DQN in our experiments.</p><p>Integrating audio and visual information into Q-learning offers a more comprehensive understanding of the environment, which can significantly improve decision-making in tasks involving multi-modal sensory data. By combining convolutional and recurrent networks to process visual and auditory inputs, respectively, and updating Q-values based on the enriched state representation, agents can perform more robustly in real-world tasks where multi-modal cues are essential.</p><p>Q-learning [<xref rid="pone.0318372.ref039" ref-type="bibr">39</xref>] addresses this challenge by learning the <italic toggle="yes">state-action value function</italic>
<inline-formula id="pone.0318372.e013"><alternatives><graphic xlink:href="pone.0318372.e013.jpg" id="pone.0318372.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>Q</mml:mi><mml:mi>:</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.278em"/><mml:mi>a</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, which represents the expected return from taking action <italic toggle="yes">a</italic><sub><italic toggle="yes">t</italic></sub> in state <italic toggle="yes">s</italic><sub><italic toggle="yes">t</italic></sub>. If the true state-action values were known, selecting the action with the highest value, <inline-formula id="pone.0318372.e014"><alternatives><graphic xlink:href="pone.0318372.e014.jpg" id="pone.0318372.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>\arg</mml:mi><mml:msub><mml:mo>max</mml:mo><mml:mi>a</mml:mi></mml:msub><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, would yield the optimal policy [<xref rid="pone.0318372.ref039" ref-type="bibr">39</xref>]. Q-learning achieves this by updating the value of each state-action pair iteratively based on experiences collected during interactions with the environment.</p><disp-formula id="pone.0318372.e015"><alternatives><graphic xlink:href="pone.0318372.e015.jpg" id="pone.0318372.e015g" position="anchor"/><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mstyle displaystyle="true"><mml:munder><mml:mo>max</mml:mo><mml:mi>a</mml:mi></mml:munder></mml:mstyle><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label></disp-formula><p>where <italic toggle="yes">the learning rate</italic>
<inline-formula id="pone.0318372.e016"><alternatives><graphic xlink:href="pone.0318372.e016.jpg" id="pone.0318372.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>&#x0211d;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> controls the learning rate. DQN extends this by using a deep neural network rather than a table to estimate the <italic toggle="yes">Q</italic>-function. This allows the use of Q-learning with high-dimensional states and actions (e.g. video games, where states are pixels of an image). The values of Q-values are learned iteratively by updating the current Q-value estimate towards the observed reward and the max Q-value over all actions a in state s. The update rule of DQN (<xref rid="pone.0318372.e015" ref-type="disp-formula">Eq 2</xref>) is modified for training neural networks.</p><p>The Deep Q Network (DQN) relies on three key techniques to stabilize learning: Replay Memory: Experiences are stored in a replay memory and sampled uniformly during training. This allows the agent to learn from a diverse set of past experiences, mitigating the impact of temporal correlations in sequential data. Target Network: A separate target network is employed to provide updates to the main network. This helps stabilize training by decoupling the target Q-values from the current parameters of the main network, reducing the risk of divergence during training. Adaptive Learning Rate: An adaptive learning rate method, such as RMSProp [<xref rid="pone.0318372.ref043" ref-type="bibr">43</xref>], maintains a per-parameter learning rate and adjusts the learning rate according to the history of gradient updates to that parameter. This adaptive scheme improves the efficiency of learning by dynamically adjusting the learning rates based on the local geometry of the loss landscape.</p><p>To encourage exploration during both training and testing, we employ an <inline-formula id="pone.0318372.e017"><alternatives><graphic xlink:href="pone.0318372.e017.jpg" id="pone.0318372.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>-greedy exploration strategy. The DQN selects a random action with probability <inline-formula id="pone.0318372.e018"><alternatives><graphic xlink:href="pone.0318372.e018.jpg" id="pone.0318372.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, and the optimal action with probability <inline-formula id="pone.0318372.e019"><alternatives><graphic xlink:href="pone.0318372.e019.jpg" id="pone.0318372.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Initially, <inline-formula id="pone.0318372.e020"><alternatives><graphic xlink:href="pone.0318372.e020.jpg" id="pone.0318372.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is set high at the beginning of training to encourage exploration, and then annealed gradually towards zero over training to shift towards exploitation, favoring optimal actions. During testing, the value of <inline-formula id="pone.0318372.e021"><alternatives><graphic xlink:href="pone.0318372.e021.jpg" id="pone.0318372.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is fixed to a low value, typically 0.05, to prioritize exploitation and ensures consistent performance.</p><p>In this work, we utilize DQN and Proximal Policy Optimization (PPO) agents as the basis for experiments in ViZDoom and Unity ML environments, respectively. These algorithms are chosen for their versatility in handling different types of data, including visual information and raw audio samples.</p></sec><sec id="sec003"><title>3 Proposed multimodal reinforcement learning system architecture</title><p>Multimodal Reinforcement Learning (MRL) involves leveraging multiple input modalities&#x02014;such as images, audio, and text&#x02014;to improve the performance of reinforcement learning (RL) agents. In environments where both visual and auditory information is crucial (e.g., autonomous driving with sound-based navigation cues or interactive robotics using both vision and sound), integrating these different types of sensory inputs can lead to more robust and efficient learning.</p><p>In this proposed method, we will explore how Deep Reinforcement Learning (DRL) can be enhanced by integrating both image (vision) and audio (sound) inputs. This involves creating a multimodal representation that fuses visual and auditory data, enabling the agent to make decisions based on a richer understanding of its environment.</p><p>This work introduces the utilization of raw audio samples in reinforcement learning tasks, implemented in both ViZDoom and Unity ML environments. In both environments, raw audio samples are integrated alongside visual information. Notably, the impact of solely utilizing raw audio samples is evaluated specifically in the Unity environment.</p><p><xref rid="pone.0318372.g001" ref-type="fig">Fig 1</xref> illustrates the architecture of the proposed reinforcement learning system, which incorporates two input sources: pixels and raw audio samples. These sources of learned features are combined in the final layer of the network by concatenating the outputs of two dense networks. The system leverages both Deep Q Network (DQN) and Proximal Policy Optimization (PPO) algorithms to validate its performance.</p><fig position="float" id="pone.0318372.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0318372.g001</object-id><label>Fig 1</label><caption><title>Proposed reinforcement learning architecture.</title></caption><graphic xlink:href="pone.0318372.g001" position="float"/></fig><p>The proposed method integrates convolutional neural networks (CNNs) for processing image data and LSTMs (Long Short-Term Memory), for processing audio data. The fused features from both modalities are used in the reinforcement learning model. The image is passed through several convolutional layers followed by pooling layers, ending in a fully connected layer to extract high-level features. Whereas the audio input is converted into a spectrogram (a 2D representation of the frequency content of audio over time). This representation transforms raw audio into a form that a neural network can process more easily, LSTMs, can be used to capture temporal dependencies in the audio signals, allowing the network to understand sequences of auditory events. After extracting the features from both the image and audio, the next step is to combine them into a unified representation for the RL agent to make decisions.</p><disp-formula id="pone.0318372.e022"><alternatives><graphic xlink:href="pone.0318372.e022.jpg" id="pone.0318372.e022g" position="anchor"/><mml:math id="M22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>:</mml:mi><mml:mi>&#x1d433;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">[</mml:mo><mml:msub><mml:mi>&#x1d433;</mml:mi><mml:mrow><mml:mtext>image&#x000a0;</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x1d433;</mml:mi><mml:mrow><mml:mtext>audio&#x000a0;</mml:mtext></mml:mrow></mml:msub><mml:mo fence="true" form="postfix" stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(3)</label></disp-formula><p>where <inline-formula id="pone.0318372.e023"><alternatives><graphic xlink:href="pone.0318372.e023.jpg" id="pone.0318372.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x1d433;</mml:mi><mml:mrow><mml:mtext>image&#x000a0;</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the feature vector from the CNN and <inline-formula id="pone.0318372.e024"><alternatives><graphic xlink:href="pone.0318372.e024.jpg" id="pone.0318372.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x1d433;</mml:mi><mml:mrow><mml:mtext>audio&#x000a0;</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> is the feature vector from the LSTM.</p><p>In the ViZDoom environment [<xref rid="pone.0318372.ref030" ref-type="bibr">30</xref>], where direct access to raw audio sources is not feasible, we simulate audio by calculating the distance between the agent and the target (i.e., distance to the goal) and incorporating these values into the neural network. This approach allows us to incorporate audio-like features into the learning process, with the magnitude of the samples increasing as the agent approaches the goal.</p><p>The loss function for training the RL models for DQN and PPO are described below respectively.</p><disp-formula id="pone.0318372.e025"><alternatives><graphic xlink:href="pone.0318372.e025.jpg" id="pone.0318372.e025g" position="anchor"/><mml:math id="M25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">Q</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x1d53c;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">[</mml:mo><mml:msup><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>current&#x000a0;</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mstyle displaystyle="true"><mml:munder><mml:mo>max</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder></mml:mstyle><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>target&#x000a0;</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo fence="true" form="postfix" stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(4)</label></disp-formula><disp-formula id="pone.0318372.e026"><alternatives><graphic xlink:href="pone.0318372.e026.jpg" id="pone.0318372.e026g" position="anchor"/><mml:math id="M26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">O</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x1d53c;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">[</mml:mo><mml:mo>min</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>clip</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:msub><mml:mi>&#x003b8;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003f5;</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>&#x003f5;</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(5)</label></disp-formula><p>Hence, since Unity environment [<xref rid="pone.0318372.ref010" ref-type="bibr">10</xref>] provides direct access to audio generated by the Unity engine, enabling us to seamlessly integrate raw audio samples. Here, the audio source is attached to the goal object, while an audio listener is attached to the agent. As the agent navigates the environment, the volume of the audio clip dynamically adjusts based on the agent&#x02019;s proximity to the goal object. The raw audio samples extracted from the audio clip are fed into the neural network alongside the raw visual information at each step of the learning process. These raw audio features, representing the signal itself, are appropriately normalized to [0,1] for neural network processing. By directly utilizing raw audio samples, the need for feature extraction is circumvented, although this approach may present challenges due to the complexity of the data for the learning algorithm.</p></sec><sec id="sec004"><title>4 Experimental setup</title><p>In this section, we outline the experimental setups for both the ViZDoom and Unity environments. ViZDoom is chosen for its versatility, ease of use, and efficient 3D platform, allowing for the development of artificial intelligence bots that navigate Doom [<xref rid="pone.0318372.ref038" ref-type="bibr">38</xref>] using only visual information (i.e., the screen buffer). This environment is particularly well-suited for research in vision-based machine learning and deep reinforcement learning.</p><p>Both the Unity and ViZDoom agents are trained using LSTMs (Long Short-Term Memory) recurrent neural networks for processing audio samples and CNN architectures for handling visual information. The agent relies on observations of its environment to infer the state of the world. In Unity ML, observations can be visual or vector-based, while ViZDoom exclusively utilizes visual observations, with the camera image attached to the agent.</p><p>The experiments were conducted on a machine equipped with a 16-core Intel Xeon processor, Nvidia RTX2080ti GPU, CUDA 12.3, Python 3.6, TensorFlow 2.13, and Ubuntu 18.04.</p><p>To enhance learning stability, the replay technique is applied to store the agent&#x02019;s experiences and randomly select batches for network training in the DQN implementation. This technique prevents the network from solely focusing on its immediate actions in the environment, enabling it to learn from a broader range of past experiences. Each experience is stored as a tuple of &#x0003c;<italic toggle="yes">state</italic>, <italic toggle="yes">action</italic>, <italic toggle="yes">reward</italic>, <italic toggle="yes">next state</italic>&#x0003e;. The experience replay buffer maintains a fixed number of recent memories, replacing old ones as new experiences are added. This approach ensures that the network learns from a diverse set of past interactions, contributing to more robust learning.</p><sec id="sec005"><title>4.1 Scenarios</title><p>We have used two different scenarios for the ViZDoom and Unity ML environment.</p><sec id="sec006"><title>4.1.1 ViZDoom.</title><p>One of the most important features of ViZDoom is its ability to run custom scenarios. This ViZDoom offers the capability to run custom scenarios, enabling the creation of tailored maps, environment mechanics, terminal conditions, and rewards. We leveraged this feature by creating our own scenario map using SLADE.</p><p>Our custom scenario map was designed to evaluate the performance of our proposed approach. In this scenario, the agent&#x02019;s objective is to navigate through five rooms and locate the target. The episode terminates either when the agent reaches the goal or upon reaching a timeout. Upon successfully reaching the goal, the agent receives a reward of 1. However, a penalty of &#x02013;1 is incurred on every game tick.</p></sec><sec id="sec007"><title>4.1.2 Unity.</title><p>Similarly, we developed our own scenario in Unity ML (depicted in <xref rid="pone.0318372.g002" ref-type="fig">Figs 2</xref> and <xref rid="pone.0318372.g003" ref-type="fig">3</xref>) comprising four small rooms. At the start of the game, both the agent and the target are randomly spawned in any of the four rooms or outside. The agent must navigate through the rooms to locate the target. This environment is characterized by sparse rewarding, as the agent only receives a positive reward upon reaching the goal. The episode concludes upon the agent reaching the goal or upon reaching a timeout. Upon successfully reaching the goal, the agent receives a reward of 1, otherwise, it receives 0. Notably, when raw audio samples are utilized in the Unity ML environment, the audio source is attached to the target, while the audio listener is attached to the agent.</p><fig position="float" id="pone.0318372.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0318372.g002</object-id><label>Fig 2</label><caption><title>The scenario used in Unity environment. Both the agent and target are spawned randomly at the start of the game anywhere in the game environment for each episode.</title></caption><graphic xlink:href="pone.0318372.g002" position="float"/></fig><fig position="float" id="pone.0318372.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0318372.g003</object-id><label>Fig 3</label><caption><title>The images shown correspond the downscaled images an agent receives in Unity environment.</title></caption><graphic xlink:href="pone.0318372.g003" position="float"/></fig><p>In our study, we chose different experimental settings to accommodate the unique requirements and characteristics of the algorithms employed in the ViZDoom and Unity ML environments. ViZDoom utilizes a modified version of the Deep Q-Network (DQN) algorithm, which is well-suited for handling the high-dimensional visual inputs of first-person shooter games. Conversely, the Unity ML environment allows for the implementation of Proximal Policy Optimization (PPO), a policy gradient method known for its stability and efficiency in continuous action spaces. By selecting these algorithms, we ensure that each environment is optimally addressed, leveraging the strengths of DQN in discrete action settings and PPO in more complex, multi-agent scenarios. Additionally, both algorithms utilize tailored neural network architectures that are specifically designed to process the multimodal inputs&#x02014;raw audio and visual data&#x02014;maximizing the agents&#x02019; learning capabilities and performance in their respective environments. This careful consideration of experimental settings and neural network choice underscores our commitment to achieving robust and generalizable results across diverse reinforcement learning tasks.</p></sec></sec><sec id="sec008"><title>4.2 Training</title><p>ML-Agents [<xref rid="pone.0318372.ref010" ref-type="bibr">10</xref>] offer an implementation of the Proximal Policy Optimization (PPO) reinforcement learning algorithm, utilizing a neural network to approximate the optimal function mapping an agent&#x02019;s observations to the most advantageous action given a state. This PPO implementation within ML-Agents is built using TensorFlow and operates within a separate Python process.</p><p>The training process begins with the initialization of parameters such as the discount factor and learning rate. Training proceeds over multiple epochs, each comprising numerous learning steps (e.g., 500,000 training steps). Initially, the agent takes random actions to explore the environment, followed by actions selected based on the learned policy to exploit the environment&#x02019;s rewards. At the conclusion of each training epoch, the network weights are saved, and these saved models are subsequently evaluated to assess the final performance of the trained agent.</p><p>In both the Unity and ViZDoom environments, Convolutional Neural Network (CNN) architectures are employed for processing visual information, while fully connected layers are utilized for handling raw audio samples. When raw audio samples are combined with visual information, each stream is initially processed independently before being merged at the final layer, as illustrated in <xref rid="pone.0318372.g001" ref-type="fig">Fig 1</xref>. The first convolutional layer for both images and raw audio samples comprises 16 filters of size 8 and a stride of 4, followed by a second convolutional layer with 32 filters of size 4 and a stride of 2. Subsequently, a fully connected layer transforms the input to 512 units, which are further processed by another fully connected layer to yield an output size equal to the number of actions available in the game.</p></sec><sec id="sec009"><title>4.3 Hyperparameters</title><p>The hyperparametes of Unity and ViZDoom are described as follows:</p><sec id="sec010"><title>4.3.1 Unity.</title><p>We identified the hyperparameters that most affect the model&#x02019;s performance in the Unity ML environment such as batch size, beta, buffer size, epsilon, lambda, learning rate, number of epochs, time horizon (number of steps of experience to collect per agent before adding it to the experience buffer) and sequence length. For each hyperparameter, we defined a reasonable search space (range of values). Then, we leverage the gradient-based optimization search strategy. We used cross-validation to evaluate the model&#x02019;s performance for each hyperparameter configuration. This helps in assessing the generalization ability of the model and reduces overfitting to a specific validation set. For each combination of hyperparameters, we train the model on the training data and evaluate it on the validation data. Then we compare the performance of different hyperparameter configurations and chose the configuration that yields the highest performance. Once we identified the best hyperparameters, we retrain the model using the full training dataset and evaluate its performance on a separate test set to check for generalization.</p><p>ML-Agents also offer modularity in defining reward signals. In this work, we utilize two reward signals: extrinsic reward and curiosity [<xref rid="pone.0318372.ref040" ref-type="bibr">40</xref>], aiming to shape the agent&#x02019;s behavior effectively. For the extrinsic reward, we set the strength to 1 and the discount factor to 0.99. Regarding the curiosity reward signal, the strength is set to 0.02, and the discount factor is 0.99. Additionally, we conduct an experiment without incorporating curiosity.</p></sec><sec id="sec011"><title>4.3.2 ViZDoom.</title><p>For the ViZDoom experiment, we train the DQN agent using the RMSProp algorithm with minibatches of size 64. We store the most recent 10,000 timesteps in the replay memory. The discount factor is set to 0.99, while the learning rate remains fixed at 0.00025. During training, we employ an <inline-formula id="pone.0318372.e027"><alternatives><graphic xlink:href="pone.0318372.e027.jpg" id="pone.0318372.e027g" position="anchor"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>-greedy policy, where the agent selects a random action with probability <inline-formula id="pone.0318372.e028"><alternatives><graphic xlink:href="pone.0318372.e028.jpg" id="pone.0318372.e028g" position="anchor"/><mml:math id="M28" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Initially, <inline-formula id="pone.0318372.e029"><alternatives><graphic xlink:href="pone.0318372.e029.jpg" id="pone.0318372.e029g" position="anchor"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is set to 1.0 and gradually annealed to 0.1 after a certain number of steps.</p><p>In the ViZDoom experiments, we utilize a frame-skip of 10 frames, meaning the agent has the opportunity to select an action only every 10th game frame, corresponding to 285ms of in-game time, as the game runs at 35 frames-per-second. The action chosen in the previous step is repeated across all frames between these agent steps. Additionally, we adjust the magnitude of the audio based on the Euclidean distance from the agent to the goal. As ViZDoom does not allow direct inclusion of actual audio, we manipulate the vector values of raw audio accordingly. When the agent is close to the goal, the raw audio samples have higher values, whereas they have lower values when the agent is far.</p><p>Given the relatively high resolution image provided by the ViZDoom environment (640 x 480), we downsample the resolution to 80 x 60 at each frame step to reduce computational time during neural network training. Conversely, in the Unity game environment, we utilize an image size of 84 x 84 at each frame step, along with all three channels of the image. For the audio features, we feed vectors of size 100 raw audio samples at each time step.</p></sec></sec><sec id="sec012"><title>4.4 Performance metric</title><p>To evaluate the performance of reinforcement learning, it&#x02019;s common to plot the time versus episodic reward and observe how the agent&#x02019;s performance evolves during training. In this work, we assess the average reward per episode to compare the performance of the baseline (without audio samples) and the proposed method. Since the average rewards of individual training runs can vary significantly, it&#x02019;s typical to conduct multiple runs and average the results [<xref rid="pone.0318372.ref041" ref-type="bibr">41</xref>]. Therefore, the reported results in the following subsections are averaged over three experiment runs. In other words, each point in the learning curve figures represents the average of three episodes.</p></sec></sec><sec id="sec013"><title>5 Experimental results</title><p>To assess the performance of the proposed approach, we initially trained a neural network model using only visual information in both ViZDoom and Unity ML environments. These experiments with image pixels serve as the baseline system. Subsequently, we compared the performance of incorporating raw audio samples alongside visual information in both ViZDoom and Unity ML environments. Additionally, we evaluated the impact of using raw audio samples alone in this study. For this evaluation, we employed both PPO and DQN algorithms to assess the performance of the proposed system across Unity and ViZDoom reinforcement learning environments.</p><p><xref rid="pone.0318372.g004" ref-type="fig">Fig 4</xref> presents the experimental results of using visual information and raw audio samples in the ViZDoom environment with the PPO algorithm. The baseline system relies solely on visual information. The figure illustrates that the agent struggles to reach the target consistently when using only visual information. However, the addition of complementary raw audio samples alongside visual information enables the agent to reach the goal more reliably. Specifically, while using visual information and audio features provides an average reward of &#x02013;550 during the last episodes, using only visual information yields an average reward of &#x02013;1300 in the same episodes. Thus, the learning curves in <xref rid="pone.0318372.g004" ref-type="fig">Fig 4</xref> demonstrate that leveraging visual information along with audio features results in better mean reward and a faster learning rate compared to using only visual information.</p><fig position="float" id="pone.0318372.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0318372.g004</object-id><label>Fig 4</label><caption><title>The mean reward results of using visual information and audio samples in the ViZDoom environment.</title></caption><graphic xlink:href="pone.0318372.g004" position="float"/></fig><p>Furthermore, we conducted additional test experiments to evaluate the performance of the trained system. The test experiment involved running the trained model for 100 episodes. <xref rid="pone.0318372.t001" ref-type="table">Table 1</xref> displays the test results, comparing the average success rate of using visual information alone versus using visual information with raw audio samples. The results indicate that augmenting visual information with raw audio samples leads to a higher average success rate compared to using only visual information. Specifically, using only visual information results in an average success rate of 43%, whereas augmenting visual information with raw audio samples increases the average success rate to 87%. These results highlight the challenge faced by the agent in making intelligent moves to reach the target using only visual information in difficult scenarios.</p><table-wrap position="float" id="pone.0318372.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0318372.t001</object-id><label>Table 1</label><caption><title>Average success rate (%) of the test experiment on ViZDoom and Unity environments.</title></caption><alternatives><graphic xlink:href="pone.0318372.t001" id="pone.0318372.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">gray Reinforcement Learning System</th><th align="left" rowspan="1" colspan="1">PPO with curiosity</th><th align="left" rowspan="1" colspan="1">PPO without curiosity</th><th align="left" rowspan="1" colspan="1">DQN</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">visual information</td><td align="left" rowspan="1" colspan="1">20</td><td align="left" rowspan="1" colspan="1">18</td><td align="left" rowspan="1" colspan="1">44</td></tr><tr><td align="left" rowspan="1" colspan="1">visual information + raw audio samples (25%)</td><td align="left" rowspan="1" colspan="1">92</td><td align="left" rowspan="1" colspan="1">94</td><td align="left" rowspan="1" colspan="1">86</td></tr><tr><td align="left" rowspan="1" colspan="1">visual information + raw audio samples (50%)</td><td align="left" rowspan="1" colspan="1">93</td><td align="left" rowspan="1" colspan="1">92</td><td align="left" rowspan="1" colspan="1">86</td></tr><tr><td align="left" rowspan="1" colspan="1">visual information + raw audio samples (75%)</td><td align="left" rowspan="1" colspan="1">95</td><td align="left" rowspan="1" colspan="1">96</td><td align="left" rowspan="1" colspan="1">88</td></tr><tr><td align="left" rowspan="1" colspan="1">visual information + raw audio samples (100%)</td><td align="left" rowspan="1" colspan="1">99</td><td align="left" rowspan="1" colspan="1">100</td><td align="left" rowspan="1" colspan="1">90</td></tr></tbody></table></alternatives></table-wrap><p>In addition to the ViZDoom environment, we also evaluated the impact of using raw audio samples for a searching task in the stable baselines environment. Consistent with the results observed in the ViZDoom environment, incorporating raw audio samples alongside visual information facilitated faster target acquisition compared to using only visual information, regardless of whether DQN or PPO learning algorithms were employed.</p><p>In the Unity ML experiments, we explored the use of curiosity reward signals to encourage exploration in environments with sparse extrinsic rewards. Consequently, we evaluated experiments both with and without curiosity reward signals and compared their performances, considering both visual information and raw audio samples.</p><p><xref rid="pone.0318372.g005" ref-type="fig">Fig 5</xref> illustrates the experimental results of using visual information and audio samples in the Unity environment, both with and without curiosity. The baseline system relies solely on visual information. The figure indicates that the agent achieves better rewards when leveraging both raw audio samples and visual information, compared to using only pixel images, regardless of the presence of curiosity.</p><fig position="float" id="pone.0318372.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0318372.g005</object-id><label>Fig 5</label><caption><title>The mean and standard deviation of the test results are presented for the use of both visual information and raw audio samples in the Unity ML environment.</title><p>The x-axis represents the timesteps, scaled by a factor of 100.</p></caption><graphic xlink:href="pone.0318372.g005" position="float"/></fig><p>Specifically, for the PPO algorithm, using only visual information fails to reliably guide the agent to the target most of the time, as depicted in the figure. However, by incorporating raw audio samples alongside visual information, the agent consistently reaches the target, achieving near-maximum rewards. Notably, the learning curves show that while using only visual information requires 500,000 steps to achieve even a reward of 0.2, the combined use of visual and auditory information reaches maximum rewards within 100,000 steps.</p><p>Furthermore, the figure demonstrates that the average rewards obtained with and without the curiosity reward signals yield similar results in our experimental setup. Hence, the addition of raw audio samples to visual information significantly improves the agent&#x02019;s performance in reaching the target, regardless of whether curiosity is employed or not.</p><p>The test experiment is carried out on 100 game episodes using the last training model. Raw audio samples (%) is the probability that the raw audio samples are used in the game environment. The visual information is used all the time (100%). The baseline system is the one based on only visual information.</p><p>The test results of 100 episodes using visual information, along with visual information with raw audio samples, with and without curiosity, are summarized in <xref rid="pone.0318372.t001" ref-type="table">Table 1</xref>. The table clearly illustrates the stark difference in performance between the different setups.</p><p>When relying solely on visual information, the success rate is a mere <inline-formula id="pone.0318372.e030"><alternatives><graphic xlink:href="pone.0318372.e030.jpg" id="pone.0318372.e030g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:mn>15</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. However, integrating raw audio samples with visual information significantly improves the success rate to <inline-formula id="pone.0318372.e031"><alternatives><graphic xlink:href="pone.0318372.e031.jpg" id="pone.0318372.e031g" position="anchor"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mn>100</mml:mn><mml:mi>%</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, regardless of the presence of curiosity. This indicates that the addition of raw audio samples enables the agent to consistently reach the target, overcoming the limitations of relying solely on visual observations.</p><p>The results further reinforce the efficacy of incorporating raw audio samples alongside visual information for enhancing the agent&#x02019;s performance in target-reaching tasks. Whether curiosity is employed or not, the addition of raw audio samples consistently enables the agent to successfully navigate the environment and reach the target in every episode, highlighting the robustness and effectiveness of the proposed approach.</p><p>The experimental results presented in <xref rid="pone.0318372.g005" ref-type="fig">Fig 5</xref> reveal another intriguing finding: the use of only raw audio samples, both with and without curiosity, yields better average rewards compared to using only visual information. While the average reward peaks at 0.2 when relying solely on visual information, it significantly improves to 0.5 and 0.48 during the same training steps when using only audio samples, for setups with and without curiosity, respectively.</p><p>This observation suggests that audio samples play a crucial role in enhancing the agent&#x02019;s performance, particularly in scenarios where visual information alone may be insufficient. Notably, when the target is spawned behind walls within rooms, the agent may struggle to navigate solely based on visual cues. However, the inclusion of audio samples provides additional environmental context, enabling the agent to make more informed decisions and achieve better rewards.</p><p>Furthermore, we compared the average total rewards of the last 100 episodes for both PPO (with and without curiosity) and DQN in both Unity ML and ViZDoom environments. <xref rid="pone.0318372.t001" ref-type="table">Table 1</xref> highlights that utilizing both visual information and raw audio samples consistently leads to better rewards compared to relying solely on visual information. This finding underscores the effectiveness of incorporating audio information into the reinforcement learning process, as it significantly improves the agent&#x02019;s ability to learn and perform tasks in complex environments.</p><p><xref rid="pone.0318372.g006" ref-type="fig">Fig 6</xref> offers a comprehensive view of the reward ranges associated with different feature sets, namely visual information alone, audio samples alone, and visual information combined with audio samples, in the Unity ML environment. The box plot visualizes key statistics such as the minimum, lower quartile, median, upper quartile, and maximum reward for each feature set.</p><fig position="float" id="pone.0318372.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0318372.g006</object-id><label>Fig 6</label><caption><title>Box plot of Unity ML experiments using visual information, audio samples and combination of visual information and audio samples.</title></caption><graphic xlink:href="pone.0318372.g006" position="float"/></fig><p>Notably, the plot highlights distinct patterns in reward distribution across the different feature combinations. Specifically, the system utilizing visual information together with audio samples exhibits the highest mean and median rewards, suggesting superior performance compared to other configurations. Conversely, relying solely on visual information yields the lowest mean and median rewards.</p><p>Furthermore, the plot showcases the variability in reward outcomes, as evidenced by the differences in minimum and maximum values across the feature sets. Interestingly, the combination of visual information and raw audio samples demonstrates the highest minimum and maximum reward values, indicating the potential for both improved performance and occasional exceptional outcomes when leveraging multimodal sensory inputs.</p><p>Overall, the box plot provides valuable insights into the distribution and variability of rewards associated with different feature combinations, underscoring the effectiveness of incorporating raw audio samples alongside visual information for enhancing the agent&#x02019;s performance in the Unity ML environment.</p><p>Introducing occasional raw audio samples alongside visual information presents an interesting approach to leveraging multimodal sensory inputs during training. This experiment aims to assess the impact of intermittently incorporating audio cues on the agent&#x02019;s learning and decision-making process, particularly in scenarios where visual information alone may be sufficient.</p><p>By varying the probability of receiving raw audio samples at each time step, ranging from 25% to 75%, you can examine how different levels of audio input affect the agent&#x02019;s performance compared to the baseline system that relies solely on visual information. This analysis allows for a nuanced understanding of when and to what extent raw audio samples contribute to the agent&#x02019;s decision-making process and overall task performance.</p><p>The results of this experiment can provide insights into the optimal balance between visual and auditory cues in reinforcement learning tasks, shedding light on the relevance and utility of raw audio samples in different environmental contexts. Moreover, it offers practical implications for designing more efficient and adaptive multimodal reinforcement learning systems tailored to specific task requirements and environmental conditions.</p><p><xref rid="pone.0318372.g007" ref-type="fig">Fig 7</xref> and <xref rid="pone.0318372.g008" ref-type="fig">Figure 8</xref> provide additional insights into the performance of the reinforcement learning agents when raw audio samples are occasionally introduced alongside visual information in ViZDoom and Unity ML environments, respectively. These figures complement the findings from experiments where raw audio samples are consistently used throughout training.</p><fig position="float" id="pone.0318372.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0318372.g007</object-id><label>Fig 7</label><caption><title>Mean reward of using only pixels and pixels with raw audio samples in ViZDoom environment.</title><p>The audio features are occasionally used with 20% and 50% probability chances.</p></caption><graphic xlink:href="pone.0318372.g007" position="float"/></fig><fig position="float" id="pone.0318372.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0318372.g008</object-id><label>Fig 8</label><caption><title>Mean reward of using visual information, visual information with raw audio samples in Unity ML environment.</title><p>The raw audio samples are occasionally used with 50% and 75% probability chances.</p></caption><graphic xlink:href="pone.0318372.g008" position="float"/></fig><p>The learning curves depicted in these figures suggest that intermittently providing raw audio samples yield improvements in the average reward compared to using only visual information. This indicates that even occasional auditory cues can enhance the agent&#x02019;s ability to navigate and accomplish tasks in complex environments. Moreover, similar to the results observed when raw audio samples are consistently used, the occasional introduction of audio features facilitates successful target acquisition by the learning agent.</p><p>These findings underscore the value of multimodal sensory inputs in reinforcement learning, highlighting the potential benefits of integrating auditory cues alongside visual information to enhance the agent&#x02019;s decision-making process and overall performance. Furthermore, this approach offers flexibility in adapting to diverse environmental conditions, allowing the agent to leverage relevant sensory modalities as needed to effectively accomplish tasks.</p><p>The results summarized in <xref rid="pone.0318372.t002" ref-type="table">Table 2</xref> provide further insights into the performance of reinforcement learning agents trained with visual information and raw audio samples in both ViZDoom and Unity environments. The average rewards obtained over the last 100 episodes serve as a metric for evaluating the effectiveness of different input modalities.</p><table-wrap position="float" id="pone.0318372.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0318372.t002</object-id><label>Table 2</label><caption><title>Mean scores of the last 100 episodes using PPO and DQN in Unity environment and ViZDoom.</title></caption><alternatives><graphic xlink:href="pone.0318372.t002" id="pone.0318372.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">gray Reinforcement Learning System</th><th align="left" rowspan="1" colspan="1">PPO with curiosity</th><th align="left" rowspan="1" colspan="1">PPO without curiosity</th><th align="left" rowspan="1" colspan="1">DQN</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">visual information</td><td align="left" rowspan="1" colspan="1">0.18</td><td align="left" rowspan="1" colspan="1">18</td><td align="left" rowspan="1" colspan="1">-1645</td></tr><tr><td align="left" rowspan="1" colspan="1">visual information + raw audio samples (25%)</td><td align="left" rowspan="1" colspan="1">0.84</td><td align="left" rowspan="1" colspan="1">85</td><td align="left" rowspan="1" colspan="1">-648</td></tr><tr><td align="left" rowspan="1" colspan="1">visual information + raw audio samples (50%)</td><td align="left" rowspan="1" colspan="1">0.86</td><td align="left" rowspan="1" colspan="1">87</td><td align="left" rowspan="1" colspan="1">-620</td></tr><tr><td align="left" rowspan="1" colspan="1">visual information + raw audio samples (75%)</td><td align="left" rowspan="1" colspan="1">0.85</td><td align="left" rowspan="1" colspan="1">89</td><td align="left" rowspan="1" colspan="1">-614</td></tr><tr><td align="left" rowspan="1" colspan="1">visual information + raw audio samples (100%)</td><td align="left" rowspan="1" colspan="1">0.89</td><td align="left" rowspan="1" colspan="1">0.91</td><td align="left" rowspan="1" colspan="1">-621</td></tr></tbody></table></alternatives></table-wrap><p>Consistently, the table highlights that utilizing raw audio samples alongside visual information leads to higher average rewards compared to using visual information alone. Moreover, the highest average rewards are achieved when raw audio samples are consistently incorporated into the agent&#x02019;s input data. This trend underscores the importance of multimodal sensory inputs, particularly in scenarios where visual cues alone may not suffice for effective decision-making and task completion.</p><p>Furthermore, the variability in rewards across different experimental conditions demonstrates the robustness and generalizability of the proposed approach. By leveraging both visual and auditory information, reinforcement learning agents can adapt to diverse environmental conditions and achieve superior performance in challenging tasks.</p><p>The experimental results presented here validate the efficacy of raw audio samples for reinforcement learning tasks in both ViZDoom and Unity ML environments. These findings underscore the potential of multimodal learning approaches and highlight the importance of incorporating auditory cues to enhance the capabilities of learning agents, particularly in complex and dynamic environments where visual information alone may be insufficient.</p></sec><sec sec-type="conclusions" id="sec014"><title>6 Conclusions</title><p>This study highlights a critical gap in current reinforcement learning methodologies, which predominantly rely solely on visual information to interpret environments and make decisions. Yet, humans leverage various sensory inputs, including auditory cues, for enhanced perception and action. Notably, individuals with visual impairments face significant challenges in tasks such as playing video games solely relying on visual data. Therefore, integrating audio inputs becomes pivotal in such contexts. Consequently, we propose the utilization of raw audio samples alongside visual information for reinforcement learning tasks, assessing their efficacy across Unity and ViZDoom environments using DQN and PPO algorithms.</p><p>Our experimental findings across ViZDoom and Unity ML environments underscore the benefits of supplementing visual data with complementary audio samples. Specifically, incorporating audio features alongside visual cues leads to improved average rewards compared to relying solely on visual inputs. Moreover, our results highlight the utility of audio features in scenarios where visual information may be limited or absent.</p><p>The empirical outcomes of our study underscore the significance of raw audio samples in reinforcement learning, emphasizing their role in enhancing agent performance. Future investigations could extend these findings to diverse reinforcement learning environments and tasks, such as alternative video games or high-fidelity audio simulations. Additionally, exploring the specific auditory cues the agent learns to leverage for decision-making warrants further analysis. Another intriguing avenue involves comparing agents utilizing recurrent neural network architectures, such as Long-Short Term Memory, with and without access to audio information, to elucidate the impact of audio inputs on learning dynamics.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0318372.ref001"><label>1</label><mixed-citation publication-type="other">Ernst D, Louette A. Introduction to reinforcement learning. In: Feuerriegel S, Hartmann J, Janiesch C, Zschech P. Generative AI. Bus Inf Syst Eng. 2024;66:111&#x02013;26.</mixed-citation></ref><ref id="pone.0318372.ref002"><label>2</label><mixed-citation publication-type="book"><name><surname>Sutton</surname><given-names>R</given-names></name>, <name><surname>Barto</surname><given-names>A</given-names></name>, <name><surname>Bach</surname><given-names>F</given-names></name>. <source>Introduction to reinforcement learning.</source>
<publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>1998</year>.</mixed-citation></ref><ref id="pone.0318372.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Kaelbling</surname><given-names>LP</given-names></name>, <name><surname>Littman</surname><given-names>ML</given-names></name>, <name><surname>Moore</surname><given-names>AW</given-names></name>. <article-title>Reinforcement learning: a survey</article-title>. <source>J Artif Intell Res</source>. <year>1996</year>;<volume>4</volume>:<fpage>237</fpage>&#x02013;<lpage>85</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1613/jair.301</pub-id></mixed-citation></ref><ref id="pone.0318372.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Bellman</surname><given-names>R</given-names></name>. <article-title>A Markovian decision process</article-title>. <source>Indiana Univ Math J</source>. <year>1957</year>;<volume>6</volume>(<issue>4</issue>):<fpage>679</fpage>&#x02013;<lpage>84</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1512/iumj.1957.6.56038</pub-id></mixed-citation></ref><ref id="pone.0318372.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Solomon</surname><given-names>E</given-names></name>, <name><surname>Cios</surname><given-names>KJ</given-names></name>. <article-title>FASS: face anti-spoofing system using image quality features and deep learning</article-title>. <source>Electronics</source>. <year>2023</year>;<volume>12</volume>(<issue>10</issue>):<fpage>2199</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/electronics12102199</pub-id></mixed-citation></ref><ref id="pone.0318372.ref006"><label>6</label><mixed-citation publication-type="confproc"><name><surname>Heess</surname><given-names>N</given-names></name>, <name><surname>Wayne</surname><given-names>G</given-names></name>, <name><surname>Silver</surname><given-names>D</given-names></name>, <name><surname>Lillicrap</surname><given-names>T</given-names></name>, <name><surname>Erez</surname><given-names>T</given-names></name>, <name><surname>Tassa</surname><given-names>Y</given-names></name>. <conf-name>Learning continuous control policies by stochastic value gradients</conf-name>. In: <conf-name>Advances in Neural Information Processing Systems</conf-name>. <year>2015</year>, pp. <fpage>2944</fpage>&#x02013;<lpage>52</lpage>.</mixed-citation></ref><ref id="pone.0318372.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Mnih</surname><given-names>V</given-names></name>, <name><surname>Kavukcuoglu</surname><given-names>K</given-names></name>, <name><surname>Silver</surname><given-names>D</given-names></name>, <name><surname>Rusu</surname><given-names>AA</given-names></name>, <name><surname>Veness</surname><given-names>J</given-names></name>, <name><surname>Bellemare</surname><given-names>MG</given-names></name>, <etal>et al</etal>. <article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>518</volume>(<issue>7540</issue>):<fpage>529</fpage>&#x02013;<lpage>33</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nature14236</pub-id>
<pub-id pub-id-type="pmid">25719670</pub-id>
</mixed-citation></ref><ref id="pone.0318372.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Levine</surname><given-names>S</given-names></name>, <name><surname>Finn</surname><given-names>C</given-names></name>, <name><surname>Darrell</surname><given-names>T</given-names></name>, <name><surname>Abbeel</surname><given-names>P</given-names></name>. <article-title>End-to-end training of deep visuomotor policies</article-title>. <source>J Mach Learn Res</source>. <year>2016</year>;<volume>17</volume>:<fpage>1334</fpage>&#x02013;<lpage>73</lpage>.</mixed-citation></ref><ref id="pone.0318372.ref009"><label>9</label><mixed-citation publication-type="confproc"><name><surname>Shalev-Shwartz</surname><given-names>S</given-names></name>, <name><surname>Shammah</surname><given-names>S</given-names></name>, <name><surname>Shashua</surname><given-names>A</given-names></name>. <conf-name>Safe, multi-agent, reinforcement learning for autonomous driving</conf-name>. arXiv, preprint. <year>2016</year>.</mixed-citation></ref><ref id="pone.0318372.ref010"><label>10</label><mixed-citation publication-type="confproc"><name><surname>Juliani</surname><given-names>A</given-names></name>, <name><surname>Berges</surname><given-names>V</given-names></name>, <name><surname>Vckay</surname><given-names>E</given-names></name>, <name><surname>Gao</surname><given-names>Y</given-names></name>, <name><surname>Henry</surname><given-names>H</given-names></name>, <name><surname>Mattar</surname><given-names>M</given-names></name>, <etal>et al</etal>. <conf-name>Unity: a general platform for intelligent agents</conf-name>. arXiv, preprint. <year>2018</year>.</mixed-citation></ref><ref id="pone.0318372.ref011"><label>11</label><mixed-citation publication-type="confproc"><name><surname>Solomon</surname><given-names>E</given-names></name>, <name><surname>Cios</surname><given-names>K</given-names></name>. <article-title>HDLHC: hybrid face anti-spoofing method concatenating deep learning and hand-crafted features.</article-title> In: <conf-name>2023 IEEE 6th International Conference on Electronic Information and Communication Technology (ICEICT)</conf-name>. <year>2023</year>, pp. <fpage>470</fpage>&#x02013;<lpage>4</lpage>.</mixed-citation></ref><ref id="pone.0318372.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Sallans</surname><given-names>B</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>. <article-title>Reinforcement learning with factored states and actions</article-title>. <source>J Mach Learn Res</source>. <year>2004</year>;<volume>5</volume>:<fpage>1063</fpage>&#x02013;<lpage>88</lpage>.</mixed-citation></ref><ref id="pone.0318372.ref013"><label>13</label><mixed-citation publication-type="other">Solomon E. Face anti-spoofing and deep learning based unsupervised image recognition systems. Scholarscompass.vcu.edu. 2023.</mixed-citation></ref><ref id="pone.0318372.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Solomon</surname><given-names>E</given-names></name>, <name><surname>Woubie</surname><given-names>A</given-names></name>, <name><surname>Cios</surname><given-names>KJ</given-names></name>. <article-title>UFace: an unsupervised deep learning face verification system</article-title>. <source>Electronics</source>. <year>2022</year>;<volume>11</volume>(<issue>23</issue>):<fpage>3909</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/electronics11233909</pub-id></mixed-citation></ref><ref id="pone.0318372.ref015"><label>15</label><mixed-citation publication-type="confproc"><name><surname>Solomon</surname><given-names>E</given-names></name>, <name><surname>Woubie</surname><given-names>A</given-names></name>, <name><surname>Emiru</surname><given-names>ES</given-names></name>. <article-title>Self-supervised deep learning based end-to-end face verification method using Siamese network.</article-title> In: <conf-name>2023 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI)</conf-name>. <publisher-name>IEEE</publisher-name>; <year>2023</year>, pp. <fpage>1</fpage>&#x02013;<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/soli60636.2023.10425759</pub-id></mixed-citation></ref><ref id="pone.0318372.ref016"><label>16</label><mixed-citation publication-type="confproc"><name><surname>Solomon</surname><given-names>E</given-names></name>, <name><surname>Woubie</surname><given-names>A</given-names></name>, <name><surname>Emiru</surname><given-names>ES</given-names></name>. <article-title>Nearest neighbor based unsupervised deep learning image recognition method.</article-title> In: <conf-name>2023 International Conference on Modeling, Simulation &#x00026;amp; Intelligent Computing (MoSICom)</conf-name>. <publisher-name>IEEE</publisher-name>; <year>2023</year>, pp. <fpage>592</fpage>&#x02013;<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/mosicom59118.2023.10458764</pub-id></mixed-citation></ref><ref id="pone.0318372.ref017"><label>17</label><mixed-citation publication-type="confproc"><name><surname>Solomon</surname><given-names>E</given-names></name>, <name><surname>Woubie</surname><given-names>A</given-names></name>. <conf-name>Federated learning method for preserving privacy in face recognition system</conf-name>. arXiv, preprint. <year>2024</year>.</mixed-citation></ref><ref id="pone.0318372.ref018"><label>18</label><mixed-citation publication-type="confproc"><name><surname>Graves</surname><given-names>A</given-names></name>, <name><surname>Mohamed</surname><given-names>A</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>. <article-title>Speech recognition with deep recurrent neural networks.</article-title> In: <conf-name>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</conf-name>. <publisher-name>IEEE</publisher-name>; <year>2013</year>, pp. <fpage>6645</fpage>&#x02013;<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/icassp.2013.6638947</pub-id></mixed-citation></ref><ref id="pone.0318372.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Russakovsky</surname><given-names>O</given-names></name>, <name><surname>Deng</surname><given-names>J</given-names></name>, <name><surname>Su</surname><given-names>H</given-names></name>, <name><surname>Krause</surname><given-names>J</given-names></name>, <name><surname>Satheesh</surname><given-names>S</given-names></name>, <name><surname>Ma</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>ImageNet large scale visual recognition challenge</article-title>. <source>Int J Comput Vis</source>. <year>2015</year>;<volume>115</volume>(<issue>3</issue>):<fpage>211</fpage>&#x02013;<lpage>52</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></mixed-citation></ref><ref id="pone.0318372.ref020"><label>20</label><mixed-citation publication-type="confproc"><name><surname>Lederle</surname><given-names>M</given-names></name>, <name><surname>Wilhelm</surname><given-names>B</given-names></name>. <conf-name>Combining high-level features of raw audio waves and mel-spectrograms for audio tagging</conf-name>. arXiv, preprint, <year>2018</year>.</mixed-citation></ref><ref id="pone.0318372.ref021"><label>21</label><mixed-citation publication-type="confproc"><name><surname>Piczak</surname><given-names>KJ</given-names></name>. <article-title>Environmental sound classification with convolutional neural networks.</article-title> In: <conf-name>2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)</conf-name>. <publisher-name>IEEE</publisher-name>; <year>2015</year>, pp. <fpage>1</fpage>&#x02013;<lpage>6</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/mlsp.2015.7324337</pub-id></mixed-citation></ref><ref id="pone.0318372.ref022"><label>22</label><mixed-citation publication-type="confproc"><name><surname>Vinyals</surname><given-names>O</given-names></name>, <name><surname>Ewalds</surname><given-names>T</given-names></name>, <name><surname>Bartunov</surname><given-names>S</given-names></name>, <name><surname>Georgiev</surname><given-names>P</given-names></name>, <name><surname>Vezhnevets</surname><given-names>A</given-names></name>, <name><surname>Yeo</surname><given-names>M</given-names></name>, <etal>et al</etal>. <conf-name>Starcraft II: a new challenge for reinforcement learning</conf-name>. arXiv, preprint, <year>2017</year>.</mixed-citation></ref><ref id="pone.0318372.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Fu</surname><given-names>X</given-names></name>. <article-title>The influence of background music of video games on immersion</article-title>. <source>J Psychol Psychother</source>. <year>2015</year>;<volume>5</volume>(<issue>1</issue>).</mixed-citation></ref><ref id="pone.0318372.ref024"><label>24</label><mixed-citation publication-type="confproc"><name><surname>Z&#x000e9;nouda</surname><given-names>H</given-names></name>. <article-title>New musical organology: the audio-games.</article-title> In: <conf-name>MISSI&#x02019;12 - International Conference on Multimedia &#x00026; Network Information Systems</conf-name>, <year>2012</year>.</mixed-citation></ref><ref id="pone.0318372.ref025"><label>25</label><mixed-citation publication-type="book"><name><surname>Yuan</surname><given-names>B</given-names></name>. <source>Towards generalized accessibility of video games for the visually impaired</source>. <publisher-loc>Reno</publisher-loc>: <publisher-name>University of Nevada</publisher-name>; <year>2009</year>.</mixed-citation></ref><ref id="pone.0318372.ref026"><label>26</label><mixed-citation publication-type="other">Qian X, Zhong Z, Zhou J. Multimodal machine translation with reinforcement learning. arXiv, preprint, 2018.</mixed-citation></ref><ref id="pone.0318372.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Jiang</surname><given-names>A-W</given-names></name>, <name><surname>Liu</surname><given-names>B</given-names></name>, <name><surname>Wang</surname><given-names>M-W</given-names></name>. <article-title>Deep multimodal reinforcement network with contextually guided recurrent attention for image question answering</article-title>. <source>J Comput Sci Technol</source>. <year>2017</year>;<volume>32</volume>(<issue>4</issue>):<fpage>738</fpage>&#x02013;<lpage>48</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11390-017-1755-6</pub-id></mixed-citation></ref><ref id="pone.0318372.ref028"><label>28</label><mixed-citation publication-type="other">Zhang J, Zhao T, Yu Z. Multimodal hierarchical reinforcement learning policy for task-oriented visual dialog. arXiv, preprint, 2018.</mixed-citation></ref><ref id="pone.0318372.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Muslea</surname><given-names>I</given-names></name>, <name><surname>Minton</surname><given-names>S</given-names></name>, <name><surname>Knoblock</surname><given-names>CA</given-names></name>. <article-title>Active learning with multiple views</article-title>. <source>J Artif Intell Res</source>. <year>2006</year>;<volume>27</volume>:<fpage>203</fpage>&#x02013;<lpage>33</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1613/jair.2005</pub-id></mixed-citation></ref><ref id="pone.0318372.ref030"><label>30</label><mixed-citation publication-type="confproc"><name><surname>Kempka</surname><given-names>M</given-names></name>, <name><surname>Wydmuch</surname><given-names>M</given-names></name>, <name><surname>Runc</surname><given-names>G</given-names></name>, <name><surname>Toczek</surname><given-names>J</given-names></name>, <name><surname>Ja&#x0015b;kowski</surname><given-names>W</given-names></name>. <article-title>ViZDoom: A doom-based AI research platform for visual reinforcement learning.</article-title> In: <conf-name>2016 IEEE Conference on Computational Intelligence and Games (CIG).</conf-name>
<year>2016</year>, pp. <fpage>341</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0318372.ref031"><label>31</label><mixed-citation publication-type="other">Solomon E, Woubie A, Emiru E. Unsupervised deep learning image verification method. arXiv, preprint, 2023.</mixed-citation></ref><ref id="pone.0318372.ref032"><label>32</label><mixed-citation publication-type="other">Solomon E, Woubie A, Emiru E. Autoencoder based face verification system. arXiv, preprint, 2023.</mixed-citation></ref><ref id="pone.0318372.ref033"><label>33</label><mixed-citation publication-type="confproc"><name><surname>Schulman</surname><given-names>J</given-names></name>, <name><surname>Wolski</surname><given-names>F</given-names></name>, <name><surname>Dhariwal</surname><given-names>P</given-names></name>, <name><surname>Radford</surname><given-names>A</given-names></name>, <name><surname>Klimov</surname><given-names>O</given-names></name>. <conf-name>Proximal policy optimization algorithms</conf-name>. arXiv, preprint, <year>2017</year>.</mixed-citation></ref><ref id="pone.0318372.ref034"><label>34</label><mixed-citation publication-type="other">OpenAI. OpenAI five. 2018.</mixed-citation></ref><ref id="pone.0318372.ref035"><label>35</label><mixed-citation publication-type="confproc"><name><surname>Woubie</surname><given-names>A</given-names></name>, <name><surname>Solomon</surname><given-names>E</given-names></name>, <name><surname>Emiru</surname><given-names>E</given-names></name>. <conf-name>Image clustering using restricted Boltzmann machine</conf-name>. arXiv, preprint, <year>2023</year>.</mixed-citation></ref><ref id="pone.0318372.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Hill</surname><given-names>A</given-names></name>, <name><surname>Raffin</surname><given-names>A</given-names></name>, <name><surname>Ernestus</surname><given-names>M</given-names></name>, <name><surname>Gleave</surname><given-names>A</given-names></name>, <name><surname>Kanervisto</surname><given-names>A</given-names></name>, <name><surname>Traore</surname><given-names>R</given-names></name>, <etal>et al</etal>. Stable baselines. Available from: <ext-link xlink:href="https://github.com/hill-a/stable-baselines" ext-link-type="uri">https://github.com/hill-a/stable-baselines</ext-link>. <year>2018</year>.</mixed-citation></ref><ref id="pone.0318372.ref037"><label>37</label><mixed-citation publication-type="other">Lee L. Robotic search and rescue via online multi-task reinforcement learning. arXiv, preprint, 2015.</mixed-citation></ref><ref id="pone.0318372.ref038"><label>38</label><mixed-citation publication-type="book"><source>Software doom</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>GT Interactive</publisher-name>; <year>1993</year>.</mixed-citation></ref><ref id="pone.0318372.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Watkins</surname><given-names>CJ</given-names></name>, <name><surname>Dayan</surname><given-names>P</given-names></name>. <article-title>Technical note: Q-learning.</article-title>&#x000a0;<source>Mach Learn</source>.&#x000a0;<year>1992</year>;<volume>8</volume>:<fpage>279</fpage>&#x02013;<lpage>92</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1023/A:1022676722315</pub-id></mixed-citation></ref><ref id="pone.0318372.ref040"><label>40</label><mixed-citation publication-type="confproc"><name><surname>Pathak</surname><given-names>D</given-names></name>, <name><surname>Agrawal</surname><given-names>P</given-names></name>, <name><surname>Efros</surname><given-names>A</given-names></name>, <name><surname>Darrell</surname><given-names>T</given-names></name>. <article-title>Curiosity-driven exploration by self-supervised prediction.</article-title> In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</conf-name>. <year>2017</year>, pp. <fpage>16</fpage>&#x02013;<lpage>7</lpage>.</mixed-citation></ref><ref id="pone.0318372.ref041"><label>41</label><mixed-citation publication-type="confproc"><name><surname>Henderson</surname><given-names>P</given-names></name>, <name><surname>Islam</surname><given-names>R</given-names></name>, <name><surname>Bachman</surname><given-names>P</given-names></name>, <name><surname>Pineau</surname><given-names>J</given-names></name>, <name><surname>Precup</surname><given-names>D</given-names></name>, <name><surname>Meger</surname><given-names>D</given-names></name>. <article-title>Deep reinforcement learning that matters.</article-title> In: <conf-name>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</conf-name>. <year>2018</year>.</mixed-citation></ref><ref id="pone.0318372.ref042"><label>42</label><mixed-citation publication-type="other">Solomon E, Woubie A, Emiru E. Deep learning based face recognition method using Siamese network. arXiv, preprint, 2023.</mixed-citation></ref><ref id="pone.0318372.ref043"><label>43</label><mixed-citation publication-type="other">Zeiler M. Adadelta: an adaptive learning rate method. arXiv, preprint, 2012.</mixed-citation></ref></ref-list></back></article>