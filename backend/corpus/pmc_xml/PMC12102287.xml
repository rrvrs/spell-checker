<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="data-paper" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Data</journal-id><journal-id journal-id-type="iso-abbrev">Sci Data</journal-id><journal-title-group><journal-title>Scientific Data</journal-title></journal-title-group><issn pub-type="epub">2052-4463</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40410191</article-id><article-id pub-id-type="pmc">PMC12102287</article-id>
<article-id pub-id-type="publisher-id">4833</article-id><article-id pub-id-type="doi">10.1038/s41597-025-04833-z</article-id><article-categories><subj-group subj-group-type="heading"><subject>Data Descriptor</subject></subj-group></article-categories><title-group><article-title>Open multi-center intracranial electroencephalography dataset with task probing conscious visual perception</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0009-0005-5119-9966</contrib-id><name><surname>Seedat</surname><given-names>Alia</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4191-1578</contrib-id><name><surname>Lepauvre</surname><given-names>Alex</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Jeschke</surname><given-names>Jay</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Gorska-Klimowska</surname><given-names>Urszula</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7905-1104</contrib-id><name><surname>Armendariz</surname><given-names>Marcelo</given-names></name><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><name><surname>Bendtz</surname><given-names>Katarina</given-names></name><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="aff" rid="Aff6">6</xref><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><name><surname>Henin</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7710-5159</contrib-id><name><surname>Hirschhorn</surname><given-names>Rony</given-names></name><xref ref-type="aff" rid="Aff8">8</xref></contrib><contrib contrib-type="author"><name><surname>Brown</surname><given-names>Tanya</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Jensen</surname><given-names>Erika</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kozma</surname><given-names>Csaba</given-names></name><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff9">9</xref></contrib><contrib contrib-type="author"><name><surname>Mazumder</surname><given-names>David</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><name><surname>Montenegro</surname><given-names>Stephanie</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Leyao</given-names></name><xref ref-type="aff" rid="Aff10">10</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5228-6918</contrib-id><name><surname>Bonacchi</surname><given-names>Niccol&#x000f2;</given-names></name><xref ref-type="aff" rid="Aff11">11</xref><xref ref-type="aff" rid="Aff12">12</xref></contrib><contrib contrib-type="author"><name><surname>Das</surname><given-names>Diptyajit</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Kahraman</surname><given-names>Kyle</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6361-2571</contrib-id><name><surname>Sripad</surname><given-names>Praveen</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6988-8103</contrib-id><name><surname>Taheriyan</surname><given-names>Fatemeh</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0044-4632</contrib-id><name><surname>Devinsky</surname><given-names>Orrin</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff13">13</xref></contrib><contrib contrib-type="author"><name><surname>Dugan</surname><given-names>Patricia</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff13">13</xref></contrib><contrib contrib-type="author"><name><surname>Doyle</surname><given-names>Werner</given-names></name><xref ref-type="aff" rid="Aff14">14</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1247-1283</contrib-id><name><surname>Flinker</surname><given-names>Adeen</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff10">10</xref></contrib><contrib contrib-type="author"><name><surname>Friedman</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff13">13</xref></contrib><contrib contrib-type="author"><name><surname>Lake</surname><given-names>Wendell</given-names></name><xref ref-type="aff" rid="Aff15">15</xref></contrib><contrib contrib-type="author"><name><surname>Pitts</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="Aff16">16</xref></contrib><contrib contrib-type="author"><name><surname>Mudrik</surname><given-names>Liad</given-names></name><xref ref-type="aff" rid="Aff8">8</xref><xref ref-type="aff" rid="Aff17">17</xref><xref ref-type="aff" rid="Aff18">18</xref></contrib><contrib contrib-type="author"><name><surname>Boly</surname><given-names>Melanie</given-names></name><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff15">15</xref></contrib><contrib contrib-type="author"><name><surname>Devore</surname><given-names>Sasha</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kreiman</surname><given-names>Gabriel</given-names></name><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8743-5071</contrib-id><name><surname>Melloni</surname><given-names>Lucia</given-names></name><address><email>lucia.melloni@ae.mpg.de</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff18">18</xref><xref ref-type="aff" rid="Aff19">19</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0190ak572</institution-id><institution-id institution-id-type="GRID">grid.137628.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8753</institution-id><institution>Department of Neurology, </institution><institution>New York University Grossman School of Medicine, </institution></institution-wrap>New York, NY 10016 USA </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/000rdbk18</institution-id><institution-id institution-id-type="GRID">grid.461782.e</institution-id><institution-id institution-id-type="ISNI">0000 0004 1795 8610</institution-id><institution>Neural Circuits, Consciousness and Cognition Research Group, </institution><institution>Max Planck Institute for Empirical Aesthetics, </institution></institution-wrap>Frankfurt am Main, 60322 Germany </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/016xsfp80</institution-id><institution-id institution-id-type="GRID">grid.5590.9</institution-id><institution-id institution-id-type="ISNI">0000 0001 2293 1605</institution-id><institution>Donders Institute for Brain, Cognition and Behavior, </institution><institution>Radboud University Nijmegen, </institution></institution-wrap>Nijmegen, 6500 HB the Netherlands </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01y2jtd41</institution-id><institution-id institution-id-type="GRID">grid.14003.36</institution-id><institution-id institution-id-type="ISNI">0000 0001 2167 3675</institution-id><institution>Department of Psychiatry, </institution><institution>University of Wisconsin-Madison, </institution></institution-wrap>Madison, WI 53719 USA </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03vek6s52</institution-id><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Boston Children&#x02019;s Hospital, </institution><institution>Harvard Medical School, </institution></institution-wrap>Boston, MA 02115 USA </aff><aff id="Aff6"><label>6</label>Center for Brains, Minds and Machines, Cambridge, MA 02139 USA </aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03vek6s52</institution-id><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Harvard Medical School, </institution></institution-wrap>Boston, MA 02115 USA </aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04mhzgx49</institution-id><institution-id institution-id-type="GRID">grid.12136.37</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0546</institution-id><institution>Sagol School of Neuroscience, </institution><institution>Tel Aviv University, </institution></institution-wrap>Tel Aviv, 6997801 Israel </aff><aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01kj2bm70</institution-id><institution-id institution-id-type="GRID">grid.1006.7</institution-id><institution-id institution-id-type="ISNI">0000 0001 0462 7212</institution-id><institution>Newcastle University, </institution></institution-wrap>Newcastle upon Tyne, NE4 5TG UK </aff><aff id="Aff10"><label>10</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0190ak572</institution-id><institution-id institution-id-type="GRID">grid.137628.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8753</institution-id><institution>Department of Biomedical Engineering, </institution><institution>New York University School of Engineering, </institution></institution-wrap>New York, NY 11201 USA </aff><aff id="Aff11"><label>11</label>Champalimaud Research, Lisbon, 1400-038 Portugal </aff><aff id="Aff12"><label>12</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/019yg0716</institution-id><institution-id institution-id-type="GRID">grid.410954.d</institution-id><institution-id institution-id-type="ISNI">0000 0001 2237 5901</institution-id><institution>William James Center for Research, </institution><institution>ISPA - Instituto Universitario, </institution></institution-wrap>Lisbon, 1149-041 Portugal </aff><aff id="Aff13"><label>13</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/005dvqh91</institution-id><institution-id institution-id-type="GRID">grid.240324.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2109 4251</institution-id><institution>Comprehensive Epilepsy Center, </institution><institution>NYU Langone Health, </institution></institution-wrap>New York, NY 10016 USA </aff><aff id="Aff14"><label>14</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/005dvqh91</institution-id><institution-id institution-id-type="GRID">grid.240324.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2109 4251</institution-id><institution>Department of Neurosurgery, </institution><institution>NYU Langone Health, </institution></institution-wrap>New York, NY 10016 USA </aff><aff id="Aff15"><label>15</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01y2jtd41</institution-id><institution-id institution-id-type="GRID">grid.14003.36</institution-id><institution-id institution-id-type="ISNI">0000 0001 2167 3675</institution-id><institution>Department of Neurology, </institution><institution>University of Wisconsin-Madison, </institution></institution-wrap>Madison, WI 53726 USA </aff><aff id="Aff16"><label>16</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00a6ram87</institution-id><institution-id institution-id-type="GRID">grid.182981.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 0456 0419</institution-id><institution>Psychology Department, </institution><institution>Reed College, </institution></institution-wrap>Portland, OR 97202 USA </aff><aff id="Aff17"><label>17</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04mhzgx49</institution-id><institution-id institution-id-type="GRID">grid.12136.37</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0546</institution-id><institution>School of Psychological Sciences, </institution><institution>Tel Aviv University, </institution></institution-wrap>Tel Aviv, 69978 Israel </aff><aff id="Aff18"><label>18</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01sdtdd95</institution-id><institution-id institution-id-type="GRID">grid.440050.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 0408 2525</institution-id><institution>Program for Brain, Mind, and Consciousness, </institution><institution>Canadian Institute for Advanced Research, </institution></institution-wrap>Toronto, Ontario Canada </aff><aff id="Aff19"><label>19</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04tsk2644</institution-id><institution-id institution-id-type="GRID">grid.5570.7</institution-id><institution-id institution-id-type="ISNI">0000 0004 0490 981X</institution-id><institution>Predictive Brain Department, Research Center One Health Ruhr, University Alliance Ruhr, Faculty of Psychology, </institution><institution>Ruhr University Bochum, </institution></institution-wrap>Bochum, 44801 Germany </aff></contrib-group><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>12</volume><elocation-id>854</elocation-id><history><date date-type="received"><day>12</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>14</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">We introduce an intracranial EEG (iEEG) dataset collected as part of an adversarial collaboration between proponents of two theories of consciousness: Global Neuronal Workspace Theory and Integrated Information Theory. The data were recorded from 38 patients undergoing intracranial monitoring of epileptic seizures across three research centers using the same experimental protocol. Participants were presented with suprathreshold visual stimuli belonging to four different categories (faces, objects, letters, false fonts) in three orientations (front, left, right view), and for three durations (0.5, 1.0, 1.5&#x02009;s). Participants engaged in a non-speeded Go/No-Go target detection task to identify infrequent targets with some stimuli becoming task-relevant and others task-irrelevant. Participants also engaged in a motor localizer task. The data were checked for its quality and converted to Brain Imaging Data Structure (BIDS). The de-identified dataset contains demographics, clinical information, electrode reconstruction, behavioral performance, and eye-tracking data. We also provide code to preprocess and analyze the data. This dataset holds promise for reuse in consciousness science and vision neuroscience to answer questions related to stimulus processing, target detection, and task-relevance, among many others.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Consciousness</kwd><kwd>Object vision</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100011730</institution-id><institution>Templeton World Charity Foundation (Templeton World Charity Foundation, Inc.)</institution></institution-wrap></funding-source><award-id>TWCF0389</award-id><award-id>TWCF0486</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><award-id>TWCF0389</award-id><principal-award-recipient><name><surname>Seedat</surname><given-names>Alia</given-names></name><name><surname>Lepauvre</surname><given-names>Alex</given-names></name><name><surname>Jeschke</surname><given-names>Jay</given-names></name><name><surname>Gorska-Klimowska</surname><given-names>Urszula</given-names></name><name><surname>Armendariz</surname><given-names>Marcelo</given-names></name><name><surname>Bendtz</surname><given-names>Katarina</given-names></name><name><surname>Henin</surname><given-names>Simon</given-names></name><name><surname>Hirschhorn</surname><given-names>Rony</given-names></name><name><surname>Brown</surname><given-names>Tanya</given-names></name><name><surname>Jensen</surname><given-names>Erika</given-names></name><name><surname>Kozma</surname><given-names>Csaba</given-names></name><name><surname>Mazumder</surname><given-names>David</given-names></name><name><surname>Montenegro</surname><given-names>Stephanie</given-names></name><name><surname>Yu</surname><given-names>Leyao</given-names></name><name><surname>Bonacchi</surname><given-names>Niccol&#x000f2;</given-names></name><name><surname>Das</surname><given-names>Diptyajit</given-names></name><name><surname>Kahraman</surname><given-names>Kyle</given-names></name><name><surname>Sripad</surname><given-names>Praveen</given-names></name><name><surname>Taheriyan</surname><given-names>Fatemeh</given-names></name><name><surname>Devinsky</surname><given-names>Orrin</given-names></name><name><surname>Dugan</surname><given-names>Patricia</given-names></name><name><surname>Doyle</surname><given-names>Werner</given-names></name><name><surname>Flinker</surname><given-names>Adeen</given-names></name><name><surname>Friedman</surname><given-names>Daniel</given-names></name><name><surname>Lake</surname><given-names>Wendell</given-names></name><name><surname>Pitts</surname><given-names>Michael</given-names></name><name><surname>Mudrik</surname><given-names>Liad</given-names></name><name><surname>Boly</surname><given-names>Melanie</given-names></name><name><surname>Devore</surname><given-names>Sasha</given-names></name><name><surname>Kreiman</surname><given-names>Gabriel</given-names></name><name><surname>Melloni</surname><given-names>Lucia</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background &#x00026; Summary</title><p id="Par2">Since the 1930s, intracranial electroencephalography (iEEG) has been the gold standard in identifying epileptogenic areas for surgical resections and other targeted treatment techniques<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. IEEG recording strategies vary across centers, ranging from the use of cortical grid and strip electrodes (Electrocorticography, ECoG) to the exclusive use of stereotactic depth electrodes (Stereoelectroencephalography, sEEG), or a combination of both<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Using iEEG, clinicians and scientists alike can characterize and localize signals acquired directly from the brain. During lulls in clinical care, willing research participants may choose to volunteer for studies that analyze signals acquired via iEEG<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Beyond their significant clinical utility, these signals provide a valuable tool for investigating the neurophysiology of cognition<sup><xref ref-type="bibr" rid="CR4">4</xref>&#x02013;<xref ref-type="bibr" rid="CR7">7</xref></sup>.</p><p id="Par3">These recordings, completed as part of the clinical standard of care, are rare, primarily due to the small number of patients requiring this procedure to begin with<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. In addition, the clinical nature of iEEG data necessitates taking greater precautions when sharing them openly to prioritize the privacy of the participants, compared to more typical non-invasive techniques such as functional magnetic resonance imaging (fMRI) or scalp electro- and magneto-encephalography (EEG and MEG) used to investigate cognitive processes. However, iEEG recordings offer much better spatio-temporal resolution over non-invasive neuro-imaging and non-invasive electrophysiology<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, which makes them critical to accelerating cognitive neuroscience research<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. These circumstances emphasize the importance of creating larger datasets for public availability and reuse, which is further amplified by the difficulty in collecting these data.</p><p id="Par4">Importantly, data must be shared with sufficient background information such that they can be easily reused by other researchers, in line with the FAIR (Findable, Accessible, Interoperable, Re-usable) principles<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Achieving this goal is particularly challenging in the case of iEEG recordings due to the high diversity and variability of data types associated with iEEG<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> as well as experimental setups. The placement of the iEEG electrodes is solely determined based on clinical needs and therefore varies greatly from participant to participant. Furthermore, a single participant dataset consists of electrophysiological recordings, neuro-imaging data, electrode localization, behavioral files, clinical information and additional metadata files. Therefore, there can be significant variability in file format, labeling and directory structure from center to center for each of the data types associated with iEEG recordings<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. In addition, the data are usually collected within the patient hospital bedroom, leading to a high variability in the experimental conditions under which the data were collected, which is rarely documented. Finally, there is ample variability between participants, associated with their age, disease diagnosis and clinical history. With this wealth and diversity of relevant information comes the challenge of effectively organizing data to enable reusability. Brain imaging data structure (BIDS)<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> provides a solution to some of these issues by providing a standardized nomenclature for file naming, organization, and documentation. BIDS defines file formats for each type of data, naming conventions for each file, folder structure for all the data, as well as which data and metadata are mandatory or optional for an iEEG data set<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. However, the BIDS specifications are currently under-specified for documenting the clinical information of the participants<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, but also the experimental context in which the research took place.</p><p id="Par5">In this article, we directly address these challenges by openly sharing a comprehensive iEEG dataset from 38 participants<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. The experimental paradigm and associated data shared here were collected as part of a large-scale international adversarial collaboration testing key contradictory predictions of two prominent theories of Consciousness:<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> Global Neuronal Workspace Theory (GNWT)<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>&#x02013;<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> and Integrated Information Theory (IIT)<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>&#x02013;<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. In this study, participants were exposed to foveally presented, suprathreshold stimuli varying in their category (faces, objects, letters and false fonts) orientation (left view, right view, front view), and duration (0.5, 1.0, and 1.5&#x02009;seconds). Those manipulations aimed to model the richness of conscious perception, and in doing so enabled testing key theoretical predictions of GNWT and IIT with respect to the brain areas claimed necessary for consciousness. Critically, to dissociate neural profiles of responses associated with consciousness from other confounding factors related to, for instance report and/or task performance, participants were asked to detect infrequent targets from two stimulus categories while the other categories remained task-irrelevant (counterbalanced across blocks). As such, the same categories were sometimes task-relevant, while in other blocks task-irrelevant. In contrast, both stimulus orientation and duration were always task-irrelevant. Multiple brain imaging modalities were collected in different groups of human participants. In addition to the iEEG dataset shared here, functional magnetic resonance imaging (fMRI) and simultaneous electroencephalography and magnetoencephalography (MEG) were also collected in an independent sample of neurotypical participants. These additional datasets will be shared in separate publications.</p><p id="Par6">Beyond its primary goal of adjudicating between these theories, the paradigm enables the investigation of a range of other cognitive and perceptual phenomena. For example, by including distinct categories of visual stimuli, it allows for the exploration of how different types of visual information are processed and how they are affected by task manipulations. Stimuli are presented for varying durations, providing an opportunity to examine how the temporal characteristics of a stimulus influence the persistence and continuity of conscious experience, thereby shedding light on the dynamics of sustained perception<sup><xref ref-type="bibr" rid="CR24">24</xref>&#x02013;<xref ref-type="bibr" rid="CR27">27</xref></sup>. The inclusion of orientation in the experimental paradigm allows for the examination of how neural representations of visual stimuli remain consistent or adapt across spatial transformations, independently of task demands<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Additionally, by defining task relevance through explicit instructions to detect certain stimuli as targets, the paradigm enables a systematic investigation of how task goals influence neural responses, allowing the dissociation of task-related processing from neural activations associated with visual perception<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>.</p><p id="Par7">This paper focuses solely on the iEEG data, including behavioral, eye tracking, iEEG recordings, and rich metadata specifications. The primary goal of sharing this dataset is to provide researchers with a meticulously curated repository of standardized high-quality iEEG recordings collected from three academic medical centers with the same experimental protocol ensuring generalization across populations, recording systems, experimenters, and patient populations.</p><p id="Par8">The dataset also includes metadata going beyond the minimum standards established by BIDS by incorporating extensive details ranging from data collection specification to clinical information, as recommended by Mercier <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> and Zheng <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. This effort represents a strong foundational commitment to fostering scientific inquiry and advancing the frontiers of our understanding of the human brain. We aim to empower our peers with this robust collection, and hope it supports future innovative studies that enable the scientific community to delve deeper into the neural basis of visual cognition (Fig. <xref rid="Fig1" ref-type="fig">1</xref>).</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Participants</title><p id="Par9">Data from 38 pharmaco-resistant epilepsy patients are shared. The dataset includes: iEEG recordings, vital signs (EKG, for a subset of patients, see supplement Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>), eye-tracking data, behavioral data, wiring diagrams for equipment used at each site (summarized in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> and in supplement Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>), standard operating procedures (SOPs) for data collection, and clinical background information for every patient. They were all collected at the Comprehensive Epilepsy Center at New York University (NYU) Langone Health, Brigham and Women&#x02019;s Hospital, Boston Children&#x02019;s Hospital (Harvard Medical School), and University of Wisconsin School of Medicine and Public Health (WU).<fig id="Fig1"><label>Fig. 1</label><caption><p>Experimental setup from one of the recording sites. Intracranial EEG (iEEG) data were transmitted from electrodes in the head, using touch-proof connectors to the data amplifier, and then to the base unit. Simultaneously, eye-tracking data, and behavioral data were collected via an eye-tracker and response box, respectively. The experimental PC sent photodiode pulses through the amplifier to the base unit. Extra efforts were made to standardize the acquisition setups and experimental context across the three laboratories. To that end, the same experimental setup was used across three laboratories, albeit with different amplifiers and eye-trackers. A shared standard operating procedure enabled comparable acquisition conditions across experimenters and labs. The above diagram corresponds to the experimental setup at Boston Children&#x02019;s Hospital and Brigham and Women&#x02019;s Hospital (Harvard Medical School). To ensure participants remained isolated from power sources, all computer systems (including eye-tracking equipment) were powered via an isolator box (NYU), an isolated ground outlet (WU), or a battery (HU). Wiring diagrams and further details for each laboratory are summarized in supplementary figures.</p></caption><graphic xlink:href="41597_2025_4833_Fig1_HTML" id="d33e750"/></fig></p><p id="Par10">The experiment and associated data collection devices were reviewed and approved by each of the site&#x02019;s independent institutional ethics committees (NYU Langone Health Institutional Review Board (IRB), Boston Children&#x02019;s Hospital IRB, Brigham and Women&#x02019;s Hospital IRB, and University of Wisconsin-Madison IRB). Before obtaining consent, all participants were confirmed to have the cognitive capacity to provide informed consent by a member of the patient&#x02019;s clinical care team. Upon receiving confirmation of cognitive capacity, all participants provided oral and written informed consent before beginning study procedures. They were informed that participation was strictly voluntary, and would not impact their clinical care. Participants were informed that they were free to withdraw participation in the study at any time, and that their data would be shared publicly following de-identification protocols. When recruiting minors, assent was obtained from minors and informed consent for data collection and data sharing was obtained from a parent or legal guardian. All study procedures were conducted in accordance with the Declaration of Helsinki (Table <xref rid="Tab1" ref-type="table">1</xref>).</p><sec id="Sec4"><title>Inclusion criteria</title><p id="Par11">We recruited 38 participants (23 female) between the ages of 10 and 65 years (M&#x02009;=&#x02009;29.89, SD&#x02009;=&#x02009;13.06). Adults provided both written and oral informed consent, while children provided written assent, accompanied by written and oral informed consent from a parent or legal guardian. Participants had an IQ of &#x0003e;70, with self-reported normal hearing, normal or corrected-to-normal vision, and cognitive and language abilities within or above the normal range in formal neuropsychological testing performed before surgery, when available. The study team postponed testing in cases where participants experienced an electrographic seizure within 3&#x02009;hours of scheduled testing, until the patient was comfortable proceeding with the experiment, and no clinical contraindications to completing the experiment were identified. When available, information concerning language dominance as assessed by the intracarotid sodium amobarbital (WADA<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>) is reported (see Table&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref>). Given the clinical context of the study, participants completed testing at various points during their surgical admission and therefore may have been on a range of medications including, but not limited to, steroids, antibiotics, pain relievers, or medications used to treat unrelated conditions. While anti-seizure medications are typically titrated down during the monitoring period, this process occurs gradually over several days, meaning the exact medication status and dosage varies across participants.</p></sec><sec id="Sec5"><title>Exclusion criteria</title><p id="Par12">Participants who were unable to complete a sufficient number of trials due to excessive muscular artifacts, movement, noisy recordings, or a decision by the participant to terminate the experiment were excluded. Out of the 42 recorded datasets, 38 met the inclusion criteria and were included in the shared sample, while 4 were excluded (one participant had only a single block recorded, another contained corrupted data, and two were recorded without triggers).</p></sec></sec><sec id="Sec6"><title>Experimental design</title><p id="Par13">IEEG data were collected on two tasks: a finger localizer task, and a Go/No-Go detection task. First, participants took part in a finger localizer task, which was aimed at identifying relevant motor areas associated with finger movements, particularly those motor responses executed during the Go/No-Go task. This control task ensures that neural responses associated with motor responses in the Go/No-Go task are correctly localized. During this task, participants were presented with four circles outlined in different colors matching the colors of the buttons on the response box (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). Arranged in a row from left to right, these colors were: white, yellow, blue, and pink. In each trial, a different circle would fill with the color of its outline, signaling them to press the corresponding colored button on the response box as soon as possible. The colored buttons on the response box were arranged in the same order as the circles presented on the laptop screen during the task. The filled circle persisted throughout the duration of the response period, followed by an additional delay of 200 milliseconds. Inter-Trial Intervals (ITIs) followed a uniform distribution, averaging 0.55&#x02009;seconds and ranging from 0.40 to 0.70&#x02009;seconds. This experiment consisted of 80 trials, evenly distributed across the four colors (20 trials per color), in a randomized sequence.<fig id="Fig2"><label>Fig. 2</label><caption><p>Schematic of the Finger Localizer task. During each trial, participants viewed four outlined circles (white, yellow, blue, and pink) arranged in a row. One circle filled with its corresponding color, signaling participants to press the matching colored button on the response box as quickly as possible. In this example, the blue circle is filled, indicating the correct response on the response box would be the blue button.</p></caption><graphic xlink:href="41597_2025_4833_Fig2_HTML" id="d33e787"/></fig></p><p id="Par14">In the main experiment, participants performed a visual Go/No-Go matching task with five critical experimental manipulations implemented in a factorial design. Stimuli were (1) of different categories (faces, objects, letters, and false-fonts), (2) identities (20 per category), and were presented (3) in different orientations (front, left and right views) and (4) for different durations (0.5, 1.0, and 1.5&#x02009;sec, see Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3b</xref>). The fifth factor, task relevance, had three levels: targets (which participants had to remember and press a button when appearing on the screen), task-relevant non-targets (of the same category as the target stimuli, but of a different identity) and task-irrelevant stimuli (of a different category than the targets, see Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a</xref>).<fig id="Fig3"><label>Fig. 3</label><caption><p>Experimental design and datasets summary. (<bold>a</bold>) Overview of the experimental paradigm, showing two example blocks of trials: At any moment, no more than one high-contrast stimulus was present at fixation. In each trial, participants were asked to detect target stimuli: either a face and an object or a letter and a false font in any of the three different orientations. Thus, each trial contained three stimuli types: targets (depicted in orange), task-relevant stimuli (belonging to the same categories as the targets, depicted in yellow), and task-irrelevant stimuli (belonging to the two other categories, depicted in purple). Colored frames are used here for illustration purposes only and did not appear in the experiment. The pictorial stimuli (faces/objects) were task-relevant in half of the blocks (upper row), while the symbolic stimuli (letters/false fonts) were relevant in the other half of the blocks (lower row), and vice versa. Blank intervals between stimuli were also included but are not depicted here. (<bold>b</bold>) The stimulus properties we manipulated were category (objects, faces, letters and false fonts), identity (each category contained 20 different exemplars), orientation (left, right, and front view), and duration (0.5, 1.0, and 1.5&#x02009;seconds). Example stimuli used in the study are shown here; for the full stimulus set see <ext-link ext-link-type="uri" xlink:href="https://github.com/Cogitate-consortium/cogitate-experiment-code/tree/ECOG-Exp1/stimuli">here</ext-link>. (<bold>c</bold>) Distribution of behavioral sensitivity scores (d&#x02019;) separate for each of the three data acquisition sites. Horizontal black lines depict average d&#x02019; per site, and dots depict individual participants&#x02019; d&#x02019;s. Average accuracy: M&#x02009;=&#x02009;95.52 (SD&#x02009;=&#x02009;7.58). Average reaction time (RT): M&#x02009;=&#x02009;0.64&#x02009;s (SD&#x02009;=&#x02009;0.14). (<bold>d</bold>) Average fixation (Eyelink)/gaze coordinates (Tobii) heat maps computed over a 0.5&#x02009;s window after stimulus onset, zoomed into the stimulus area for each recording site.</p></caption><graphic xlink:href="41597_2025_4833_Fig3_HTML" id="d33e819"/></fig></p></sec><sec id="Sec7"><title>Stimuli</title><p id="Par15">Stimuli covered approximately 6&#x02009;&#x000d7;&#x02009;6&#x000b0; of visual angle area on the screen. Faces were created with FaceGen Modeler 3.1; letter and false font stimuli were generated with MAXON CINEMA 4D Studio (RC - R20) 20.059; object stimuli were selected from the Object Databank<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. All stimuli were gray-scaled and scaled for equal luminance and size using the SHINE toolbox<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. To aid target identification, faces varied in hairstyle, ethnicity and gender. Stimulus orientation was balanced such that half of each category had a side view (equally facing either left by 30&#x000b0; or right by &#x02212;30&#x000b0;). The remaining half were front views.</p></sec><sec id="Sec8"><title>Experimental procedure</title><p id="Par16">The Go/No-Go task was divided into 20 blocks, with breaks between blocks paced by the participant. At the beginning of each block, two target stimuli were presented: a face and an object or a letter and a false font. Participants were instructed to press a button whenever they detected the occurrence of a target stimulus (face/object or letter/false font). Targets did not repeat across blocks. The blocks were ordered in an AABB sequence, where two consecutive face-object blocks were always followed by two consecutive letter-false font blocks and the pairing order was counterbalanced across the experiment.</p><p id="Par17">All stimuli were presented foveally for one of three durations (0.5, 1.0, and 1.5&#x02009;seconds). Next, a blank screen was presented to extend each trial to 2.0&#x02009;sec, followed by an additional jitter lasting for an average duration of 0.4&#x02009;s (truncated exponential distribution between 0.2 and 2.0&#x02009;s, median absolute deviation&#x02248;0.1&#x02009;s). This resulted in a mean trial length of 2.4&#x02009;s. Participants were instructed to maintain focus on a central fixation cross in between trials.</p><p id="Par18">Within each mini-block, half of the stimuli were task-relevant (i.e., they belonged to the same faces-object or letters-false fonts categories as the targets) and half task-irrelevant (i.e., they were from the two other categories). The stimulus identity varied randomly while appearing equally across trial durations and task conditions (Fig. <xref rid="Fig3" ref-type="fig">3</xref>).</p></sec><sec id="Sec9"><title>Data collection harmonization</title><p id="Par19">To minimize discrepancies between data collection sites, experimental devices were standardized across sites to the extent possible. The same response box (Millikey LH-8) was used across sites. For HU and NYU, the same laptop was used (Dell Precision 5540 laptop, with a 15.6&#x0201d; Ultrasharp screen), while a Dell D29M PC with an Acer 19.1&#x0201d; screen was used in WU. Across all three sites, we used the same custom photodiode device, positioned at a corner of the display to record luminance changes in a small square that switched from black to white. This captured the exact frame of each stimulus onset and offset. The photodiode signals were recorded alongside the iEEG channels, enabling offline extraction of event onsets to synchronize the recorded signals with on-screen events presentation. Except for three participants at WU (whose amplifier defaulted to a TTL output), the photodiode signal was recorded as an analog input. An audio/USB trigger system was available as a backup. Before data collection, a procedure similar to that described in Lepauvre <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> was applied in each data acquisition site to ensure consistent timing and experimental design across sites.</p><p id="Par20">At WU, clinical constraints prevented recording the photodiode on the same amplifier as the iEEG channels, as such the photodiode was recorded on a separate amplifier (Blackrock for the first three datasets, analog; Neuralynx for subsequent datasets, TTL). Because separate amplifiers run on different clocks, a subset of iEEG channels was duplicated onto those amplifiers, allowing the two recordings to be aligned via cross-correlation. This additional step precisely synchronized the photodiode signal with the main iEEG recordings (see technical validation).</p><p id="Par21">To mitigate potential procedural differences across sites, we developed a standardized operating procedure (SOP) outlining instructions for conducting experiments and which is shared alongside the data (<ext-link ext-link-type="uri" xlink:href="https://cogitate-consortium.github.io/cogitate-data/08_links/#links-and-reference-materials">https://cogitate-consortium.github.io/cogitate-data/08_links/#links-and-reference-materials</ext-link>, iEEG SOP). The SOP encompassed participant instructions, setup guidelines, and general experiment protocols. Although the instructions to participants and the way the task was conducted remained consistent across sites, the SOP also accommodated logistical differences specific to each location&#x02014;such as the source of necessary equipment, how to configure power connections (e.g., using a battery at Harvard versus an isolated socket at WU), and the processes for data storage and retrieval after each session. This comprehensive approach to quality assurance not only involved technical considerations but also extended to the establishment of standardized procedures, ensuring that the data collected across diverse sites could be reliably compared and utilized for meaningful scientific investigations.</p></sec><sec id="Sec10"><title>Data acquisition</title><sec id="Sec11"><title>Behavioral data acquisition</title><p id="Par22">The task was run on Matlab (The MathWorks Inc., 2019, Harvard: R2020b; NYU: R2020a, WU: 2021a)<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> using Psychtoolbox v.3.4<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. NYU and Harvard used a Dell Precision 5540 laptop, with a 15.6&#x02033; screen and WU used a Dell D29M PC with an Acer 19.1&#x02033; V196WL screen. Participants responded using an 8-button response box (Millikey LH-8; response hand(s) varied) using the button of their choice for targets.</p></sec><sec id="Sec12"><title>Eye tracking data acquisition</title><p id="Par23">Eye tracking and pupillometry data were collected using an EyeLink 1000 Plus on remote mode, sampled monocularly at 500&#x02009;Hz (from the left eye at WU, and from the left or right eye depending on the setup at Harvard), or on a Tobii-4C eye-tracker, sampled binocularly at 90&#x02009;Hz (NYU, see Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>). Eye trackers were calibrated at the beginning of the task (using a 13 points calibration for HU and WU, 9 points for NYU), and could be recalibrated as needed before every fourth block. For the eyelink eye trackers, the eyetracking recordings were synchronized with the behavioral tasks using triggers, which were sent via an ethernet protocol from the experimental computer to the Eyelink computer to mark each critical event. In the case of the Tobii eye-tracker, as the data were recorded directly on the experimental computer, no further synchronization was required.<table-wrap id="Tab1"><label>Table 1</label><caption><p>iEEG patients demographics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Participant ID</th><th>Gender</th><th>Age</th><th>Handedness</th><th>Electrode Scheme</th><th># electrodes</th><th>Hemispheric electrode placement</th></tr></thead><tbody><tr><td>CE103</td><td>F</td><td>49</td><td>left</td><td>sEEG</td><td>58</td><td>bilateral</td></tr><tr><td>CE106</td><td>F</td><td>18</td><td>right</td><td>sEEG</td><td>118</td><td>left</td></tr><tr><td>CE107</td><td>M</td><td>24</td><td>right</td><td>sEEG</td><td>168</td><td>left</td></tr><tr><td>CE108</td><td>F</td><td>16</td><td>right</td><td>sEEG</td><td>108</td><td>left</td></tr><tr><td>CE109</td><td>F</td><td>50</td><td>right</td><td>sEEG</td><td>104</td><td>bilateral</td></tr><tr><td>CE110</td><td>F</td><td>15</td><td>right</td><td>sEEG</td><td>186</td><td>right</td></tr><tr><td>CE112</td><td>F</td><td>17</td><td>right</td><td>sEEG</td><td>158</td><td>right</td></tr><tr><td>CE113</td><td>F</td><td>26</td><td>ambidextrous</td><td>sEEG</td><td>60</td><td>bilateral</td></tr><tr><td>CE115</td><td>M</td><td>17</td><td>right</td><td>sEEG</td><td>88</td><td>right</td></tr><tr><td>CE118</td><td>M</td><td>11</td><td>right</td><td>sEEG</td><td>164</td><td>left</td></tr><tr><td>CE119</td><td>M</td><td>29</td><td>right</td><td>sEEG</td><td>104</td><td>bilateral</td></tr><tr><td>CE120</td><td>M</td><td>12</td><td>left</td><td>sEEG</td><td>164</td><td>left</td></tr><tr><td>CE121</td><td>M</td><td>20</td><td>right</td><td>sEEG</td><td>191</td><td>left</td></tr><tr><td>CF102</td><td>F</td><td>30</td><td>right</td><td>ECoG &#x00026; sEEG</td><td>133</td><td>left</td></tr><tr><td>CF103</td><td>M</td><td>24</td><td>right</td><td>ECoG &#x00026; sEEG</td><td>189</td><td>left</td></tr><tr><td>CF104</td><td>F</td><td>23</td><td>right</td><td>ECoG &#x00026; sEEG</td><td>116</td><td>left</td></tr><tr><td>CF105</td><td>M</td><td>31</td><td>right</td><td>ECoG &#x00026; sEEG</td><td>176</td><td>left</td></tr><tr><td>CF106</td><td>M</td><td>17</td><td>right</td><td>ECoG &#x00026; sEEG</td><td>156</td><td>left</td></tr><tr><td>CF107</td><td>F</td><td>31</td><td>right</td><td>ECoG &#x00026; sEEG</td><td>242</td><td>right</td></tr><tr><td>CF109</td><td>F</td><td>30</td><td>left</td><td>ECoG &#x00026; sEEG</td><td>102</td><td>right</td></tr><tr><td>CF110</td><td>F</td><td>17</td><td>right</td><td>ECoG &#x00026; sEEG</td><td>174</td><td>left</td></tr><tr><td>CF112</td><td>M</td><td>23</td><td>right</td><td>ECoG &#x00026; sEEG</td><td>180</td><td>bilateral</td></tr><tr><td>CF113</td><td>F</td><td>38</td><td>right</td><td>ECoG &#x00026; sEEG</td><td>132</td><td>right</td></tr><tr><td>CF116</td><td>M</td><td>43</td><td>left</td><td>sEEG</td><td>166</td><td>bilateral</td></tr><tr><td>CF117</td><td>M</td><td>28</td><td>right</td><td>sEEG</td><td>174</td><td>bilateral</td></tr><tr><td>CF119</td><td>M</td><td>37</td><td>right</td><td>ECoG &#x00026; sEEG</td><td>104</td><td>left</td></tr><tr><td>CF120</td><td>F</td><td>61</td><td>right</td><td>sEEG</td><td>79</td><td>left</td></tr><tr><td>CF121</td><td>F</td><td>50</td><td>right</td><td>sEEG</td><td>75</td><td>right</td></tr><tr><td>CF122</td><td>F</td><td>27</td><td>right</td><td>sEEG</td><td>104</td><td>right</td></tr><tr><td>CF124</td><td>F</td><td>33</td><td>right</td><td>sEEG</td><td>146</td><td>bilateral</td></tr><tr><td>CF125</td><td>F</td><td>23</td><td>right</td><td>sEEG</td><td>138</td><td>left</td></tr><tr><td>CF126</td><td>F</td><td>23</td><td>right</td><td>sEEG</td><td>122</td><td>left</td></tr><tr><td>CG101</td><td>M</td><td>40</td><td>right</td><td>sEEG</td><td>104</td><td>bilateral</td></tr><tr><td>CG102</td><td>M</td><td>49</td><td>right</td><td>sEEG</td><td>86</td><td>bilateral</td></tr><tr><td>CG103</td><td>F</td><td>57</td><td>right</td><td>sEEG</td><td>76</td><td>bilateral</td></tr><tr><td>CG104</td><td>F</td><td>48</td><td>right</td><td>sEEG</td><td>72</td><td>bilateral</td></tr><tr><td>CG105</td><td>F</td><td>24</td><td>right</td><td>sEEG</td><td>70</td><td>bilateral</td></tr><tr><td>CG106</td><td>F</td><td>25</td><td>ambidextrous</td><td>sEEG</td><td>76</td><td>bilateral</td></tr></tbody></table><table-wrap-foot><p>Abbreviations: (F) female, (M) male, (ECoG) electro-corticography, (sEEG) stereo electro-encephalography. CE, CF, and CG denote the medical center in which the data were acquired.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec13"><title>IEEG data acquisition</title><p id="Par24">Intracranial brain activity was recorded using varying combinations of the following platinum-iridium electrodes depending on the recording site: subdural grids embedded SILASTIC sheets, or depth stereo-electroencephalographic, or Ad-Tech macro-micro depth electrodes (3 to 5.5&#x02009;mm spacing, micro wires data were not collected for the shared tasks). Grids had 8&#x02009;&#x000d7;&#x02009;8 contacts with 10&#x02009;mm center-to-center spacing, 8&#x02009;&#x000d7;&#x02009;16 contacts with 3&#x02009;mm spacing; or hybrid macro/micro 8&#x02009;&#x000d7;&#x02009;8 contacts with 10&#x02009;mm inter-contact distance, and 64 embedded microcontacts with 5&#x02009;mm inter-contact distance. Linear strips had 4&#x02013;12 contacts with 10&#x02009;mm inter-contact distance, and depth electrodes had 8 to 12 contacts with 1.5 to 2.43&#x02009;mm inter-contact distance. Macro grids, micro grids, strips and stereo EEG electrodes were acquired using 256-channels NATUS amplifier systems across all recording sites (Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>), with a sampling frequency varying between 512 and 2048&#x02009;Hz across participants, and are shared at their original sampling rate. The data includes a total of 4771 electrodes across 38 participants (1238 surface, 3533 depths).</p></sec><sec id="Sec14"><title>Electrodes reconstruction</title><p id="Par25">Across all sites, post-implant computed tomography (CT) images were co-registered with the pre-implant T1 MRI images using FLIRT<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> as implemented in FSL<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. Individual pial surfaces were reconstructed based on T1 MRI using the Freesurfer image analysis suite (&#x02018;recon-all&#x02019;). Electrode T1 coordinates were obtained by localizing the electrodes on the CT scan using site-specific custom algorithms and pipelines that reflect each site&#x02019;s standard practices. For NYU participants, electrode labels were assigned semi-automatically or manually using FLSView<sup><xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup>. For surface electrode grids, three corner electrodes were localized manually and the remaining electrode coordinates were then automatically interpolated along the shared plane using the known inter-electrode distances. A custom algorithm estimates the surface under each grid along the curve of a sphere larger than the brain. The algorithm then iteratively adjusted the projection of the grid plane, minimizing the error between the projected and known electrode locations. Electrode locations were then adjusted for estimated brain shift/swelling<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Subdural strips were localized manually. If sEEG depths did not follow a straight trajectory, they are localized manually. For WU participants (sEEG only), electrode labels were assigned manually or semi-manually using the SubNuclear toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/ckovach/SubNuclear">https://github.com/ckovach/SubNuclear</ext-link>). For Harvard participants (sEEG only), individual depth electrode contacts were labelled manually from CT using BioImageSuite&#x02019;s Electrode Editor tool<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, and converted to coordinates within T1 MRI-space with the iELVis toolbox<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>. For all sites, electrodes were converted from subject-specific T1 space to a common MNI space using either surface-based (cortical surface electrodes) or a linear-based transformation (depths) using the freesurfer average brain (fsaverage, MNI305<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>) (Table <xref rid="Tab2" ref-type="table">2</xref>).<fig id="Fig4"><label>Fig. 4</label><caption><p>Summary of electrodes coverage. (<bold>a</bold>) Participants&#x02019; demographic information including summary reports of distribution of (self-reported) gender, handedness, primary language and age across participants. (<bold>b</bold>) Summary of electrode counts and implantation schemes. The left pie chart depicts the total number of cortical (ECoG) and stereo-electrodes (sEEG) in the data set. The middle Venn diagram (implants) represents the number of participants implanted with only sEEG or both ECoG and sEEG. The right Venn diagram (scheme) represents the number of participants with electrodes located in the left hemisphere, right hemisphere or both hemispheres. (<bold>c</bold>) ECoG channels localization are depicted in white on fsaverage template brain. (<bold>d</bold>) sEEG channels localization are depicted in yellow on fsaverage template brain. (<bold>e</bold>) Number of electrodes in each region of interest based on the Destrieux cortical parcellation (74 labels per hemisphere).</p></caption><graphic xlink:href="41597_2025_4833_Fig4_HTML" id="d33e1566"/></fig><table-wrap id="Tab2"><label>Table 2</label><caption><p>List of devices used to record iEEG data, eye-tracker (ET) data, structural MR scans (MR) and CT data across the different sites.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>iEEG</th><th>ET</th><th>MR (T1 pre-op)</th><th>CT (post-op with electrodes)</th></tr></thead><tbody><tr><td>HU</td><td>NATUS Amplifier System</td><td>Eyelink 1000 Plus</td><td>n/a*</td><td>n/a*</td></tr><tr><td>NYU</td><td>NATUS Amplifier System</td><td>Tobii Eye tracker (IS4LPOO1)</td><td><p>SIEMENS 3&#x02009;T</p><p>Biograph_mMR</p></td><td>SIEMENS SOMATOM Force</td></tr><tr><td>WU</td><td>NATUS Amplifier System</td><td>Eyelink 1000 Plus</td><td>GE MEDICAL SYSTEMS 1.5&#x02009;T (Optima Artist, Optima MR450w), 3&#x02009;T (Optima Architect)</td><td>Canon Medical Systems Aquilion ONE</td></tr></tbody></table><table-wrap-foot><p>*MRI and CT scans from Harvard University are not included in the data release as those could not be openly shared.</p></table-wrap-foot></table-wrap></p></sec></sec></sec><sec id="Sec15"><title>Data Records</title><sec id="Sec16"><title>Data release formats/naming conventions</title><p id="Par26">The raw and BIDS (Brain Imaging Data Structure) formats of the data are available in two ways: (1) Archival Format (Bundles in a zip format on our website <ext-link ext-link-type="uri" xlink:href="https://www.arc-cogitate.com/data-release">https://www.arc-cogitate.com/data-release</ext-link><sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>), and (2) XNAT (eXtensible Neuroimaging Archive Toolkit at <ext-link ext-link-type="uri" xlink:href="http://cogitate-data.ae.mpg.de/">http://cogitate-data.ae.mpg.de/</ext-link><sup><xref ref-type="bibr" rid="CR47">47</xref></sup>). We chose to make the data available in two different ways to cater to various users and their various degrees of proficiency with digital tools.</p><sec id="Sec17"><title>Data bundles</title><sec id="Sec18"><title>Raw format</title><p id="Par27">Raw data bundles<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> follow the below naming convention. The project root is organized in sub-folders, where each participant&#x02019;s data is stored in a dedicated folder. The participant code is in the format &#x0201c;CX???&#x0201d;, where the two first letters reflect the site where the data were collected and the question marks represent the participant ID. The participant directories consist of various sub-directories along with a metadata folder that contains CRF (Case Report Form, containing notes taken by the experimenter during the recordings) and EXQU (Exit Questionnaire, containing the participants&#x02019; responses to the feedback questionnaire provided at the end of the experiment). Each subfolder follows the naming pattern SUBJECT_PARADIGM_MODALITY. SUBJECT refers to the participant ID. PARADIGM refers to the experimental paradigm used to collect the data, and MODALITY refers to the data type stored within the folder. For each participant, we recorded CT scans (CT), MR scans (MR), behavioral data (i.e., participants&#x02019; responses, BEH), eye-tracking data (ET), and iEEG data (ECOG) from the experimental task. Additionally, we collected event-related behavioral data (FingerLoc_BEH) and iEEG data (FingerLoc_ECOG) during the Finger Localization task. The dataset also includes files related to electrode coordinates, labels, and other metadata containing essential information about events and channels, all of which can be found in the ElecCoords directory. The CT and MR scans were acquired to obtain anatomical data of the participant&#x02019;s brain and to localize iEEG electrodes; no experimental paradigm was used for these scans. The MR and CT data are shared in their native, DICOM format. The PARADIGM is therefore left empty for MR and CT scans. In addition, the root directory also contains a metadata folder that includes several key files: a link to the analysis code (analysis_ECOG.json), a list of devices used for data acquisition (devices_ECOG.json), information on the labs involved in data collection along with other relevant details (labs.json and projects.json), a manifest of MR and CT datasets (sessions_manifest_ECOG.json), the experimental protocol for the task (protocols_ECOG.json), participants&#x02019; demographic data (subjects_demographics_ECOG.json), descriptions of the experimental paradigm (tasks_EXP1_ECOG.json), details of the Finger Localizer task (tasks_FingerLoc_ECOG.json), and a wiring diagram showing how the devices were connected (wirings_ECOG.pdf). Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> illustrates the directory structure for raw data and the format for each type of data is detailed in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>.<fig id="Fig5"><label>Fig. 5</label><caption><p>Raw data bundles folder structure.</p></caption><graphic xlink:href="41597_2025_4833_Fig5_HTML" id="d33e1672"/></fig><table-wrap id="Tab3"><label>Table 3</label><caption><p>Naming conventions and data formats.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Data type</th><th>Folder</th><th>File naming conventions</th><th>Data formats</th></tr></thead><tbody><tr><td>CT scan</td><td>CX???__CT</td><td><p>CX???_CT_1.nii.gz</p><p>or</p><p>CX???__CT/*.dcm</p></td><td>DICOM or NIFTI</td></tr><tr><td>Behavioral data</td><td>CX???_EXP1_BEH</td><td>CX???_Beh_1_RawDurR?</td><td>CSV</td></tr><tr><td>iEEG data</td><td>CX???_EXP1_ECOG</td><td>CX???_ECoG_1_DurR?</td><td>EDF</td></tr><tr><td>Eye-tracking data</td><td>CX???_EXP1_ET</td><td>CX???_ET_1_DurR?</td><td>ASC or CSV</td></tr><tr><td>MR scan</td><td>CX???__MR</td><td><p>CX???_MR_1.nii.gz or</p><p>CX???__MR/*.dcm</p></td><td>DICOM or NIFTI</td></tr><tr><td>iEEG data for the Finger Localizer task</td><td>CX???_FingerLoc_ECOG</td><td>CX???_ECOG_1_FingerLoc</td><td>EDF</td></tr><tr><td>Behavioral data for the Finger Localizer task</td><td>CX???_FingerLoc_BEH</td><td>CX???__FingersLocalizer_LOG</td><td>CSV</td></tr><tr><td>Electrode coordinates and additional data files</td><td>CX???_ElecCoords</td><td><p>CX???_ses-1_atlas-desikan_labels</p><p>CX???_ses-1_atlas-destrieux_labels</p><p>CX???_ses-1_laplace_mapping_ieeg</p><p>CX???_ses-1_space-fsaverage_electrodes</p><p>CX???_ses-1_space-fsaverage_coordsystem</p><p>CX???_ses-1_task-Dur_channels</p><p>CX???_ses-1_task-Dur_events</p></td><td>TSV and JSON</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec19"><title>BIDS data</title><p id="Par28">The raw data were also converted to BIDS using the MNE-bids package<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. The BIDS root<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> (Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>) contains a subfolder for each participant, using the format &#x0201c;sub-CX???&#x0201d;. The participant&#x02019;s folder contains a nested folder structure, with the highest level referring to the session the recording was collected from (ses-1), followed by the main iEEG data folder (ieeg) and MR scans data folder (anat). The data files are located at the lowest level of the directory structure (see Table&#x000a0;<xref rid="Tab4" ref-type="table">4</xref>). The iEEG data are stored in the BrainVision format named sub-CX???_ses-1_task-PARADIGM_ieeg. The events associated with the experimental task are stored in a tsv file format under the name sub-CX???_ses-1_task-PARADIGM_events.tsv and are accompanied by a json file of the same name containing information about the events. Electrode localization is stored in the tsv file sub-CX???_ses-1_space-fsaverage_electrodes.tsv in MNI space (fsaverage), as described in the associated sub-CX???_ses-1_space-fsaverage_coordsystem.json file (see electrodes reconstruction section). In addition, the sub-CX???_ses-1_task-Dur_channels.tsv file contains additional information about each channel in the recording, such as the type of electrode (seeg or ecog), online filtering used during data collection, and sampling frequency. In addition, we provide a status description for each electrode, containing annotations about the epileptic activity characterized by an epileptologist (only available for NYU data, participants CF102-CF126 in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>) and about noise levels based on visual inspection. For further specification, consult the BIDS specification for iEEG<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. Finally, the Laplace mapping json files contain the scheme to be used for Laplace re-referencing, subtracting activation of neighboring electrodes of a given electrode. Additional metadata (case report forms, exit questionnaires, subject demographics, information related to the experimental setup etc.) and CT scans are included along with the BIDS data under derivatives/additional_metadata and derivatives/ct respectively (Fig. <xref rid="Fig6" ref-type="fig">6</xref> and Table <xref rid="Tab4" ref-type="table">4</xref>)<fig id="Fig6"><label>Fig. 6</label><caption><p>BIDS data bundles folder structure.</p></caption><graphic xlink:href="41597_2025_4833_Fig6_HTML" id="d33e1813"/></fig><table-wrap id="Tab4"><label>Table 4</label><caption><p>Naming conventions of the BIDS converted iEEG data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Data type</th><th>File naming conventions</th><th>Data formats</th></tr></thead><tbody><tr><td>Experimental events</td><td>sub-CX???_ses-1_task-Dur_events</td><td>TSV and JSON</td></tr><tr><td>iEEG data</td><td>sub-CX???_ses-1_task-Dur_ieeg</td><td>BrainVision (vmrk, vhdr, eeg)</td></tr><tr><td>Electrodes coordinates</td><td><p>sub-CX???_ses-1_space-fsaverage_electrodes</p><p>sub-CX???_ses-1_space-fsaverage_coordsystem</p></td><td>TSV and JSON</td></tr><tr><td>Channels information</td><td>sub-CX???_ses-1_task-Dur_channels</td><td>TSV and JSON</td></tr><tr><td>Laplace mapping</td><td>sub-CX???_ses-1_laplace_mapping_ieeg</td><td>JSON</td></tr><tr><td>CT scan</td><td>sub-CX???_ses-1_ct</td><td>NIFTI and JSON</td></tr><tr><td>MR scan</td><td>sub-CX???_ses-1_T1w</td><td>NIFTI and JSON</td></tr><tr><td>Electrode labels</td><td><p>sub-CX???_ses-1_atlas_destrieux_labels</p><p>subCX???_ses-1_atlas_desikan_labels</p></td><td>TSV</td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec20"><title>XNAT</title><sec id="Sec21"><title>Raw format</title><p id="Par29">To simplify data downloading and offer more flexibility for accessing specific data, the raw data is also available on the XNAT platform<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. A key advantage of the XNAT release is that it enables searches over the data based on predefined criteria (e.g., gender, age, etc) in addition to the separate download of the data based on those predefined criteria. While the data content is identical to that of the raw bundles (which only allows for download of the complete dataset), it is organized in a different structure. The raw data is arranged as follows: In the project directory, a list of participants along with demographic information is available. Under the &#x02018;Resources&#x02019; section, project-level metadata such as demographics (for all participants), devices, analyses, protocols, and more can be found. Within each subject folder, EyeTracker and iEEG data are located under &#x02018;Experiments&#x02019;. Subject-specific demographics are available under the subject&#x02019;s &#x02018;Resources&#x02019;. Inside the iEEG experiment folder, data files such as BEH and BEHFingerLoc are stored under &#x02018;Resources&#x02019;. Additionally, metadata files, including CRF and EXQU, are present in this directory. MR and CT scans are available in two formats: NIFTI and DICOM. NIFTI scans can be found within the directory, while DICOM scans are organized as separate sessions, alongside ECOG and ET data, under &#x02018;Experiments&#x02019;.</p></sec><sec id="Sec22"><title>BIDS converted raw data</title><p id="Par30">The BIDS format of the data follows a similar structure and file organization as the BIDS data bundles.</p></sec></sec></sec></sec><sec id="Sec23"><title>Technical Validation</title><p id="Par31">Data were collected by three independent laboratories to ensure generalization across patient populations, recording systems, and experimenters. The exact timing of the stimulus presentation was recorded with the photodiode. Response boxes used by all laboratories have reported an average of one-millisecond latency (Millikey, LabHakkers).</p><p id="Par32">The data were checked at three levels (based on the principles described in Gorska-Klimowska*, Hirschhorn* <italic>et. al</italic>., in preparation). Firstly, we ensured that the data contained all expected files (see Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>), that they were of consistent naming conventions (see Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>), and that all personal information had been removed. Personal information was removed through on-site programmatic editing of the files headers. Additionally, programmatic verification was implemented on our shared data repository (XNAT) to reject any data with improper anonymization. Secondly, we established that our task manipulations were effective by testing behavior and eye-tracking data. This revealed that patient performance was relatively high (hit rate 94.89%, SD&#x02009;=&#x02009;4.22, false alarm rate 2.39% SD&#x02009;=&#x02009;2.02; Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3c</xref>), with only three participants having hit rates&#x02009;&#x0003c;70% and false alarm rates &#x0003e;30%. Moreover, fixations remained relatively stable for all the participants throughout the entire experiment duration (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3d</xref>). Finally, we checked the quality of the neural data. Channels localized within the epileptic onset zone were marked at NYU and WU by a certified epileptologist, and two independent validators marked contacts that were either damaged (flat or noisy) or implanted outside the brain tissue. This information was stored in the sub-CE103_ses-1_task-Dur_channels.tsv files. Overall noise in the data was assessed by computing the summary spectra before and after notch filtering. Participants&#x02019; data were excluded in cases of technical issues during recording (missing photodiode triggers or corrupted data preventing proper file reading) or if they completed only a single block, which was insufficient for analysis. As a result, four datasets were excluded from the data release (three from NYU, one from WU).</p><p id="Par33">Localization of the electrode contacts was validated by multiple steps of visual inspection by two independent validators. The location of each electrode contact was checked with respect to the individual anatomy. Individual labels were checked for their accordance with atlases, and atlas mapping in BIDS was further compared with the initial atlas outputs. Finally, to verify alignment, we plotted the time intervals between consecutive triggers in the neural recordings against the intervals between corresponding events in the log file, ensuring a precise match. Notably, the WU site required a cross-correlation procedure (using variable-sized kernels and detecting peaks above the mean correlation SD; see code for details) due to triggers being recorded on non-clinical amplifiers (see Data collection harmonization section above). The data being shared contains data where triggers have been aligned and further verified with the tests above. Pre-alignment data can be available upon request.</p></sec><sec id="Sec24"><title>Usage Notes</title><p id="Par34">The data can be accessed via our live XNAT database (<ext-link ext-link-type="uri" xlink:href="http://cogitate-data.ae.mpg.de/">http://cogitate-data.ae.mpg.de/</ext-link><sup><xref ref-type="bibr" rid="CR47">47</xref></sup>) which offers a web interface to navigate through the existing data and selectively download specific data (of specific participants, sessions, etc.). An API is also available to download the data programmatically (<ext-link ext-link-type="uri" xlink:href="https://wiki.xnat.org/documentation/the-xnat-api">https://wiki.xnat.org/documentation/the-xnat-api</ext-link>). Alternatively, data bundles can be downloaded from our website in a zip format (<ext-link ext-link-type="uri" xlink:href="https://www.arc-cogitate.com/data-release">https://www.arc-cogitate.com/data-release</ext-link><sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>). In both cases, users must create an account before gaining access to the downloading interfaces. More detailed information is available on our documentation website (<ext-link ext-link-type="uri" xlink:href="https://cogitate-consortium.github.io/cogitate-data/">https://cogitate-consortium.github.io/cogitate-data/</ext-link>).</p><p id="Par35">We further provide code to preprocess the data, identify onset responsive channels and perform temporal decoding of categorical information (e.g., faces vs. objects), illustrated in a Jupyter notebook (notebooks/ieeg-data-release.ipynb). Additionally, pipeline scripts with default parameters are available for analyses on all channels and participants. All provided pipelines are fully customizable through JSON files (docs/config-default.json).</p><p id="Par36">The preprocessing pipeline, based on Cogitate <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, includes the removal of line noise (60&#x02009;Hz), identifying and excluding defective channels based on visual inspection and clinical annotations, re-referencing using a Laplace scheme, computing high gamma power (70&#x02013;150&#x02009;Hz) and event-related potentials (ERPs, 0&#x02013;30&#x02009;Hz), as well as epoching around stimuli onsets and offsets. Standard iEEG analysis pipelines are provided for identifying onset-responsive channels and decoding categorical information (e.g., faces vs. objects). For instance, onset response detection compares high gamma activation pre- (&#x02212;0.3 to 0&#x02009;sec) and post-stimulus (0.05 to 0.350&#x02009;sec), using a paired t-test. Cross-temporal generalization<sup><xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR50">50</xref></sup> for decoding faces vs. objects employs support vector machines and 5-fold cross-validation. We also demonstrate how anatomical electrode labels from Freesurfer reconstruction atlases can be used to constrain analyses spatially, enabling investigating neural dynamics in specific brain regions. Finally, a Jupyter notebook (ieeg-single-participant-report.ipynb) generates a comprehensive report for each participant, preprocessing data and computing onset responsiveness, with results displayed on the fsaverage brain.</p></sec><sec id="Sec25" sec-type="supplementary-material"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41597_2025_4833_MOESM1_ESM.pdf"><caption><p>Supplementary information</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors contributed equally: Alia Seedat, Alex Lepauvre, Jay Jeschke, Urszula Gorska-Klimowska.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s41597-025-04833-z.</p></sec><ack><title>Acknowledgements</title><p>This work was supported by the Templeton World Charity Foundation (TWCF0389&#x02013;1872 DOI.ORG/10.54224/20389; TWCF0486 - DOI.ORG/10.54224/20486), and the Max Planck Society to L.M. Research implants for a select subset of participants at NYU Langone Health were supported by 1R01NS109367-01A1 (Adeen Flinker) and 5R01MH111417-05 (Jonathan Winawer). An extra special thank you to the participants and their families who generously donated their time to participate in this project. Open Access funding enabled and organized by Projekt DEAL.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Alia Seedat Data Curation, Data Quality, Investigation, Project Administration, Resources, Supervision, Visualization, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing. Alex Lepauvre Conceptualization, Data Curation, Data Quality, Formal Analysis, Investigation, Methodology, Project Administration, Software, Validation, Visualization, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing. Jay Jeschke Investigation, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing. Urszula Gorska-Klimowska Data Curation, Data Quality, Formal Analysis, Investigation, Project Administration, Software, Visualization, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing. Marcelo Armendariz Data Curation, Data Quality, Investigation, Writing &#x02013; review &#x00026; editing. Katarina Bendtz Data Curation, Data Quality, Formal Analysis, Investigation, Methodology, Software, Supervision. Simon Henin Data Curation, Data Quality, Formal Analysis, Methodology, Project Administration, Software, Supervision, Validation, Visualization, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing. Rony Hirschhorn Data Curation, Data Quality, Formal Analysis, Software, Writing &#x02013; review &#x00026; editing. Tanya Brown Data Curation, Project Administration, Resources, Supervision, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing. Erika Jensen Data Quality, Investigation, Resources. Csaba Kozma Data Curation, Data Quality, Investigation. David Mazumder Data Curation, Investigation. Stephanie Montenegro Data Curation, Data Quality, Investigation, Project Administration, Software. Leyao Yu Data Curation, Data Quality. Niccol&#x000f2; Bonacchi Conceptualization, Data Curation, Data Quality, Project. Administration, Software, Supervision, Validation, Writing &#x02013; review &#x00026; editing. Diptyajit Das Data Curation, Data Quality, Software, Validation. Kyle Kahraman Data Curation, Data Quality, Software. Praveen Sripad Data Curation, Data Quality, Software, Validation, Writing &#x02013; review &#x00026; editing. Fatemeh Taheriyan Data Curation, Validation, Writing &#x02013; review &#x00026; editing. Orrin Devinsky Conceptualization, Funding Acquisition, Investigation, Methodology, Project Administration, Resources, Supervision, Writing &#x02013; review &#x00026; editing. Patricia Dugan Data Curation, Resources, Supervision. Werner Doyle Investigation, Resources. Adeen Flinker Data Curation, Data Quality, Resources, Supervision, Validation. Daniel Friedman Data Curation, Resources, Supervision. Wendell Lake Funding Acquisition, Investigation, Project Administration, Resources. Michael Pitts Conceptualization, Data Quality, Funding Acquisition, Methodology, Project Administration, Resources, Supervision, Validation, Writing &#x02013; review &#x00026; editing. Liad Mudrik Conceptualization, Data Quality, Funding Acquisition, Methodology, Project Administration, Resources, Supervision, Validation, Writing &#x02013; review &#x00026; editing. Melanie Boly Project Administration, Resources, Supervision. Sasha Devore Data Curation, Data Quality, Investigation, Methodology, Resources, Supervision, Writing &#x02013; review &#x00026; editing. Gabriel Kreiman Project Administration, Supervision. Lucia Melloni Conceptualization, Data Curation, Data Quality, Formal Analysis, Funding Acquisition, Investigation, Methodology, Project Administration, Resources, Software, Supervision, Validation, Writing &#x02013; original draft, Writing &#x02013; review &#x00026; editing.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The Matlab code used to run the experiment is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Cogitate-consortium/cogitate-experiment-code.git">https://github.com/Cogitate-consortium/cogitate-experiment-code</ext-link>. The code implementing the preprocessing pipeline and analysis scripts is accessible at <ext-link ext-link-type="uri" xlink:href="https://github.com/Cogitate-consortium/iEEG-data-release">https://github.com/Cogitate-consortium/iEEG-data-release</ext-link><sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. All code is implemented in Python using MNE-python v.1.7<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> and Matlab. The repository&#x02019;s README file provides an overview of the codebase and instructions on how to set up the environment. While the README offers an overall guide, we recommend that users consult the Jupyter notebook (ieeg-data-release.html) available in the repository for detailed usage instructions. This notebook showcases how to download the data from our repository, implement the described analyses, and provides additional information on interacting with the dataset, including selecting specific conditions and customization options. Extensive information about the experimental paradigm, recording modalities and more can be found on the accompanying wiki: <ext-link ext-link-type="uri" xlink:href="https://cogitate-consortium.github.io/cogitate-data/">https://cogitate-consortium.github.io/cogitate-data/</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par38">P.D. is a consultant for and receives research funding from NeuroPace. These relationships do not constitute a conflict of interest with regard to the content of this work. No other authors declare a conflict of interest.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Ryvlin</surname><given-names>P</given-names></name><name><surname>Cross</surname><given-names>JH</given-names></name><name><surname>Rheims</surname><given-names>S</given-names></name></person-group><article-title>Epilepsy surgery in children and adults</article-title><source>Lancet Neurol.</source><year>2014</year><volume>13</volume><fpage>1114</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.1016/S1474-4422(14)70156-5</pub-id><pub-id pub-id-type="pmid">25316018</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Ryvlin, P., Cross, J. H. &#x00026; Rheims, S. Epilepsy surgery in children and adults. <italic>Lancet Neurol.</italic><bold>13</bold>, 1114&#x02013;1126, 10.1016/S1474-4422(14)70156-5 (2014).<pub-id pub-id-type="pmid">25316018</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Jobst</surname><given-names>BC</given-names></name><etal/></person-group><article-title>Intracranial EEG in the 21st Century</article-title><source>Epilepsy Curr.</source><year>2020</year><volume>20</volume><fpage>180</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1177/1535759720934852</pub-id><pub-id pub-id-type="pmid">32677484</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Jobst, B. C. <italic>et al</italic>. Intracranial EEG in the 21st Century. <italic>Epilepsy Curr.</italic><bold>20</bold>, 180&#x02013;188, 10.1177/1535759720934852 (2020).<pub-id pub-id-type="pmid">32677484</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Mercier</surname><given-names>MR</given-names></name><etal/></person-group><article-title>Advances in human intracranial electroencephalography research, guidelines and good practices</article-title><source>NeuroImage</source><year>2022</year><volume>260</volume><fpage>119438</fpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119438</pub-id><pub-id pub-id-type="pmid">35792291</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Mercier, M. R. <italic>et al</italic>. Advances in human intracranial electroencephalography research, guidelines and good practices. <italic>NeuroImage</italic><bold>260</bold>, 119438, 10.1016/j.neuroimage.2022.119438 (2022).<pub-id pub-id-type="pmid">35792291</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>J</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><article-title>Direct brain recordings fuel advances in cognitive electrophysiology</article-title><source>Trends Cogn. Sci.</source><year>2010</year><volume>14</volume><fpage>162</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.005</pub-id><pub-id pub-id-type="pmid">20189441</pub-id>
</element-citation><mixed-citation id="mc-CR4" publication-type="journal">Jacobs, J. &#x00026; Kahana, M. J. Direct brain recordings fuel advances in cognitive electrophysiology. <italic>Trends Cogn. Sci.</italic><bold>14</bold>, 162&#x02013;171, 10.1016/j.tics.2010.01.005 (2010).<pub-id pub-id-type="pmid">20189441</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>EL</given-names></name><name><surname>Kam</surname><given-names>JWY</given-names></name><name><surname>Tzovara</surname><given-names>A</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name></person-group><article-title>Insights into human cognition from intracranial EEG: A review of audition, memory, internal cognition, and causality</article-title><source>J. Neural Eng.</source><year>2020</year><volume>17</volume><fpage>051001</fpage><pub-id pub-id-type="doi">10.1088/1741-2552/abb7a5</pub-id><pub-id pub-id-type="pmid">32916678</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Johnson, E. L., Kam, J. W. Y., Tzovara, A. &#x00026; Knight, R. T. Insights into human cognition from intracranial EEG: A review of audition, memory, internal cognition, and causality. <italic>J. Neural Eng.</italic><bold>17</bold>, 051001, 10.1088/1741-2552/abb7a5 (2020).<pub-id pub-id-type="pmid">32916678</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Lachaux</surname><given-names>J-P</given-names></name><name><surname>Axmacher</surname><given-names>N</given-names></name><name><surname>Mormann</surname><given-names>F</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name><name><surname>Crone</surname><given-names>NE</given-names></name></person-group><article-title>High-frequency neural activity and human cognition: Past, present and possible future of intracranial EEG research</article-title><source>Prog. Neurobiol.</source><year>2012</year><volume>98</volume><fpage>279</fpage><lpage>301</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2012.06.008</pub-id><pub-id pub-id-type="pmid">22750156</pub-id>
</element-citation><mixed-citation id="mc-CR6" publication-type="journal">Lachaux, J.-P., Axmacher, N., Mormann, F., Halgren, E. &#x00026; Crone, N. E. High-frequency neural activity and human cognition: Past, present and possible future of intracranial EEG research. <italic>Prog. Neurobiol.</italic><bold>98</bold>, 279&#x02013;301, 10.1016/j.pneurobio.2012.06.008 (2012).<pub-id pub-id-type="pmid">22750156</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Liu, J. &#x00026; Xue, G. What Is the Contribution of iEEG as Compared to Other Methods to Cognitive Neuroscience? in <italic>Intracranial EEG: A Guide for Cognitive Neuroscientists</italic> (ed. Axmacher, N.) 103&#x02013;124. 10.1007/978-3-031-20910-9_8 (Springer International Publishing, Cham, 2023).</mixed-citation></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Berezutskaya</surname><given-names>J</given-names></name><etal/></person-group><article-title>Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film</article-title><source>Sci. Data</source><year>2022</year><volume>9</volume><fpage>91</fpage><pub-id pub-id-type="doi">10.1038/s41597-022-01173-0</pub-id><pub-id pub-id-type="pmid">35314718</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Berezutskaya, J. <italic>et al</italic>. Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film. <italic>Sci. Data</italic><bold>9</bold>, 91, 10.1038/s41597-022-01173-0 (2022).<pub-id pub-id-type="pmid">35314718</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Buzs&#x000e1;ki</surname><given-names>G</given-names></name><name><surname>Anastassiou</surname><given-names>CA</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><article-title>The origin of extracellular fields and currents &#x02014; EEG, ECoG, LFP and spikes</article-title><source>Nat. Rev. Neurosci.</source><year>2012</year><volume>13</volume><fpage>407</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1038/nrn3241</pub-id><pub-id pub-id-type="pmid">22595786</pub-id>
</element-citation><mixed-citation id="mc-CR9" publication-type="journal">Buzs&#x000e1;ki, G., Anastassiou, C. A. &#x00026; Koch, C. The origin of extracellular fields and currents &#x02014; EEG, ECoG, LFP and spikes. <italic>Nat. Rev. Neurosci.</italic><bold>13</bold>, 407&#x02013;420, 10.1038/nrn3241 (2012).<pub-id pub-id-type="pmid">22595786</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Wilkinson</surname><given-names>MD</given-names></name><etal/></person-group><article-title>The FAIR Guiding Principles for scientific data management and stewardship</article-title><source>Sci. Data</source><year>2016</year><volume>3</volume><fpage>160018</fpage><pub-id pub-id-type="doi">10.1038/sdata.2016.18</pub-id><pub-id pub-id-type="pmid">26978244</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Wilkinson, M. D. <italic>et al</italic>. The FAIR Guiding Principles for scientific data management and stewardship. <italic>Sci. Data</italic><bold>3</bold>, 160018, 10.1038/sdata.2016.18 (2016).<pub-id pub-id-type="pmid">26978244</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Holdgraf</surname><given-names>C</given-names></name><etal/></person-group><article-title>iEEG-BIDS, extending the Brain Imaging Data Structure specification to human intracranial electrophysiology</article-title><source>Sci. Data</source><year>2019</year><volume>6</volume><fpage>102</fpage><pub-id pub-id-type="doi">10.1038/s41597-019-0105-7</pub-id><pub-id pub-id-type="pmid">31239438</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Holdgraf, C. <italic>et al</italic>. iEEG-BIDS, extending the Brain Imaging Data Structure specification to human intracranial electrophysiology. <italic>Sci. Data</italic><bold>6</bold>, 102, 10.1038/s41597-019-0105-7 (2019).<pub-id pub-id-type="pmid">31239438</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Gorgolewski, K. J. <italic>et al</italic>. The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments.<italic> Sci. Data</italic><bold>3</bold>, 160044, 10.1038/sdata.2016.44 (2016).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Cogitate, C. <italic>et al</italic>. Adversarial testing of global neuronal workspace and integrated information theories of consciousness. <italic>Nature</italic>10.1038/s41586-025-08888-1 (2025).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Melloni, L. <italic>et al</italic>. An adversarial collaboration protocol for testing contrasting predictions of global neuronal workspace and integrated information theory. <italic>PLOS ONE</italic><bold>18</bold>, e0268577, 10.1371/journal.pone.0268577 (2023).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">George, M. A. <italic>et al</italic>. Conscious processing and the global neuronal workspace hypothesis. <italic>Neuron</italic><bold>105.5</bold>, 776-798 (2020).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Dehaene, S. &#x00026; Naccache, L. Towards a cognitive neuroscience of consciousness: basic evidence and a workspace framework. <italic>Cognition</italic><bold>79</bold>, 1&#x02013;37, 10.1016/S0010-0277(00)00123-2 (2001).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Dehaene, S. &#x00026; Changeux, J.-P. Experimental and Theoretical Approaches to Conscious Processing. <italic>Neuron</italic><bold>70</bold>, 200&#x02013;227, 10.1016/j.neuron.2011.03.018 (2011).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Dehaene, S., Kerszberg, M. &#x00026; Changeux, J.-P. A neuronal model of a global workspace in effortful cognitive tasks. <italic>Proc. Natl. Acad. Sci</italic>. <bold>95</bold>, 14529&#x02013;14534, 10.1073/pnas.95.24.14529 (1998).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Dehaene, S., Lau, H. &#x00026; Kouider, S. What is consciousness, and could machines have it? <italic>Science</italic><bold>358</bold>, 486&#x02013;492, 10.1126/science.aan8871 (2017).</mixed-citation></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Albantakis</surname><given-names>L</given-names></name><etal/></person-group><article-title>Integrated information theory (IIT) 4.0: Formulating the properties of phenomenal existence in physical terms</article-title><source>PLOS Comput. Biol.</source><year>2023</year><volume>19</volume><fpage>e1011465</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011465</pub-id><pub-id pub-id-type="pmid">37847724</pub-id>
</element-citation><mixed-citation id="mc-CR20" publication-type="journal">Albantakis, L. <italic>et al</italic>. Integrated information theory (IIT) 4.0: Formulating the properties of phenomenal existence in physical terms. <italic>PLOS Comput. Biol.</italic><bold>19</bold>, e1011465, 10.1371/journal.pcbi.1011465 (2023).<pub-id pub-id-type="pmid">37847724</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Tononi</surname><given-names>G</given-names></name></person-group><article-title>Integrated information theory of consciousness: an updated account</article-title><source>Arch. Ital. Biol.</source><year>2012</year><volume>150</volume><fpage>56</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.4449/aib.v149i5.1388</pub-id><pub-id pub-id-type="pmid">23165867</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Tononi, G. Integrated information theory of consciousness: an updated account. <italic>Arch. Ital. Biol.</italic><bold>150</bold>, 56&#x02013;90, 10.4449/aib.v149i5.1388 (2012).<pub-id pub-id-type="pmid">23165867</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name><surname>Tononi</surname><given-names>G</given-names></name><name><surname>Boly</surname><given-names>M</given-names></name><name><surname>Massimini</surname><given-names>M</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><article-title>Integrated information theory: from consciousness to its physical substrate</article-title><source>Nat. Rev. Neurosci.</source><year>2016</year><volume>17</volume><fpage>450</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.44</pub-id><pub-id pub-id-type="pmid">27225071</pub-id>
</element-citation><mixed-citation id="mc-CR22" publication-type="journal">Tononi, G., Boly, M., Massimini, M. &#x00026; Koch, C. Integrated information theory: from consciousness to its physical substrate. <italic>Nat. Rev. Neurosci.</italic><bold>17</bold>, 450&#x02013;461, 10.1038/nrn.2016.44 (2016).<pub-id pub-id-type="pmid">27225071</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Tononi, G. An information integration theory of consciousness. <italic>BMC Neurosci</italic>. <bold>5</bold>, 42 10.1186/1471-2202-5-42 (2004).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Vishne, G., Gerber, E. M., Knight, R. T. &#x00026; Deouell, L. Y. Distinct ventral stream and prefrontal cortex representational dynamics during sustained conscious visual perception. <italic>Cell Rep</italic>. <bold>42</bold>, 10.1016/j.celrep.2023.112752 (2023).</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Stigliani</surname><given-names>A</given-names></name><name><surname>Jeska</surname><given-names>B</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><article-title>Encoding model of temporal processing in human visual cortex</article-title><source>Proc. Natl. Acad. Sci.</source><year>2017</year><volume>114</volume><fpage>E11047</fpage><lpage>E11056</lpage><pub-id pub-id-type="doi">10.1073/pnas.1704877114</pub-id><pub-id pub-id-type="pmid">29208714</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Stigliani, A., Jeska, B. &#x00026; Grill-Spector, K. Encoding model of temporal processing in human visual cortex. <italic>Proc. Natl. Acad. Sci.</italic><bold>114</bold>, E11047&#x02013;E11056, 10.1073/pnas.1704877114 (2017).<pub-id pub-id-type="pmid">29208714</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Gerber</surname><given-names>EM</given-names></name><name><surname>Golan</surname><given-names>T</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Deouell</surname><given-names>LY</given-names></name></person-group><article-title>Cortical representation of persistent visual stimuli</article-title><source>NeuroImage</source><year>2017</year><volume>161</volume><fpage>67</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.028</pub-id><pub-id pub-id-type="pmid">28807872</pub-id>
</element-citation><mixed-citation id="mc-CR26" publication-type="journal">Gerber, E. M., Golan, T., Knight, R. T. &#x00026; Deouell, L. Y. Cortical representation of persistent visual stimuli. <italic>NeuroImage</italic><bold>161</bold>, 67&#x02013;79, 10.1016/j.neuroimage.2017.08.028 (2017).<pub-id pub-id-type="pmid">28807872</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Broday-Dvir, R., Norman, Y., Harel, M., Mehta, A. D. &#x00026; Malach, R. Perceptual stability reflected in neuronal pattern similarities in human visual cortex. <italic>Cell Rep</italic>. <bold>42</bold>, 10.1016/j.celrep.2023.112614 (2023).</mixed-citation></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Ram&#x000ed;rez</surname><given-names>FM</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Allefeld</surname><given-names>C</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name></person-group><article-title>The Neural Code for Face Orientation in the Human Fusiform Face Area</article-title><source>J. Neurosci.</source><year>2014</year><volume>34</volume><fpage>12155</fpage><lpage>12167</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3156-13.2014</pub-id><pub-id pub-id-type="pmid">25186759</pub-id>
</element-citation><mixed-citation id="mc-CR28" publication-type="journal">Ram&#x000ed;rez, F. M., Cichy, R. M., Allefeld, C. &#x00026; Haynes, J.-D. The Neural Code for Face Orientation in the Human Fusiform Face Area. <italic>J. Neurosci</italic>. <bold>34</bold>, 12155&#x02013;12167, 10.1523/JNEUROSCI.3156-13.2014 (2014).<pub-id pub-id-type="pmid">25186759</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>K</given-names></name><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Denison</surname><given-names>RN</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Arcaro</surname><given-names>DL</given-names></name></person-group><article-title>Tasks and their role in visual neuroscience</article-title><source>Neuron</source><year>2023</year><volume>111</volume><fpage>1697</fpage><lpage>1713</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.03.022</pub-id><pub-id pub-id-type="pmid">37040765</pub-id>
</element-citation><mixed-citation id="mc-CR29" publication-type="journal">Kay, K., Bonnen, K., Denison, R. N., Arcaro, M. J. &#x00026; Barack, D. L. Tasks and their role in visual neuroscience. <italic>Neuron</italic><bold>111</bold>, 1697&#x02013;1713, 10.1016/j.neuron.2023.03.022 (2023).<pub-id pub-id-type="pmid">37040765</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Zheng, Z., Chen, X., Brown, T., Cousijn, H. &#x00026; Melloni, L. A FAIR Workflow Guide for Researchers in Human Cognitive Neuroscience. <italic>Preprint at PsyArXiv</italic>10.31234/osf.io/yhj5c (2024).</mixed-citation></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name><surname>Loring</surname><given-names>DW</given-names></name><etal/></person-group><article-title>Wada memory performance predicts seizure outcome following anterior temporal lobectomy</article-title><source>Neurology</source><year>1994</year><volume>44</volume><fpage>2322</fpage><lpage>2322</lpage><pub-id pub-id-type="doi">10.1212/WNL.44.12.2322</pub-id><pub-id pub-id-type="pmid">7991119</pub-id>
</element-citation><mixed-citation id="mc-CR31" publication-type="journal">Loring, D. W. <italic>et al</italic>. Wada memory performance predicts seizure outcome following anterior temporal lobectomy. <italic>Neurology</italic><bold>44</bold>, 2322&#x02013;2322, 10.1212/WNL.44.12.2322 (1994).<pub-id pub-id-type="pmid">7991119</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Wada</surname><given-names>J</given-names></name><name><surname>Rasmussen</surname><given-names>T</given-names></name></person-group><article-title>Intracarotid Injection of Sodium Amytal for the Lateralization of Cerebral Speech Dominance: Experimental and Clinical Observations</article-title><source>J. Neurosurg.</source><year>1960</year><volume>17</volume><fpage>266</fpage><lpage>282</lpage><pub-id pub-id-type="doi">10.3171/jns.1960.17.2.0266</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Wada, J. &#x00026; Rasmussen, T. Intracarotid Injection of Sodium Amytal for the Lateralization of Cerebral Speech Dominance: Experimental and Clinical Observations. <italic>J. Neurosurg.</italic><bold>17</bold>, 266&#x02013;282, 10.3171/jns.1960.17.2.0266 (1960).</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Tarr, M. J. The Object Databank. <italic>Carnegie Mellon Univ</italic>. (1996).</mixed-citation></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname><given-names>V</given-names></name><etal/></person-group><article-title>Controlling low-level image properties: The SHINE toolbox</article-title><source>Behav. Res. Methods</source><year>2010</year><volume>42</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.3758/BRM.42.3.671</pub-id><pub-id pub-id-type="pmid">20805589</pub-id>
</element-citation><mixed-citation id="mc-CR34" publication-type="journal">Willenbockel, V. <italic>et al</italic>. Controlling low-level image properties: The SHINE toolbox. <italic>Behav. Res. Methods</italic><bold>42</bold>, 671&#x02013;684, 10.3758/BRM.42.3.671 (2010).<pub-id pub-id-type="pmid">20805589</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name><surname>Lepauvre</surname><given-names>A</given-names></name><name><surname>Hirschhorn</surname><given-names>R</given-names></name><name><surname>Bendtz</surname><given-names>K</given-names></name><name><surname>Mudrik</surname><given-names>L</given-names></name><name><surname>Melloni</surname><given-names>L</given-names></name></person-group><article-title>A standardized framework to test event-based experiments</article-title><source>Behav. Res. Methods</source><year>2024</year><volume>56</volume><fpage>8852</fpage><lpage>8868</lpage><pub-id pub-id-type="doi">10.3758/s13428-024-02508-y</pub-id><pub-id pub-id-type="pmid">39285141</pub-id>
</element-citation><mixed-citation id="mc-CR35" publication-type="journal">Lepauvre, A., Hirschhorn, R., Bendtz, K., Mudrik, L. &#x00026; Melloni, L. A standardized framework to test event-based experiments. <italic>Behav. Res. Methods</italic><bold>56</bold>, 8852&#x02013;8868, 10.3758/s13428-024-02508-y (2024).<pub-id pub-id-type="pmid">39285141</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">The MathWorks Inc. MATLAB. The MathWorks Inc (2019).</mixed-citation></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The Psychophysics Toolbox</article-title><source>Spat. Vis.</source><year>2007</year><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Brainard, D. H. The Psychophysics Toolbox. <italic>Spat. Vis.</italic><bold>10</bold>, 433&#x02013;436 (2007).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>A global optimisation method for robust affine registration of brain images</article-title><source>Med. Image Anal.</source><year>2001</year><volume>5</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/S1361-8415(01)00036-6</pub-id><pub-id pub-id-type="pmid">11516708</pub-id>
</element-citation><mixed-citation id="mc-CR38" publication-type="journal">Jenkinson, M. &#x00026; Smith, S. A global optimisation method for robust affine registration of brain images. <italic>Med. Image Anal.</italic><bold>5</bold>, 143&#x02013;156, 10.1016/S1361-8415(01)00036-6 (2001).<pub-id pub-id-type="pmid">11516708</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><etal/></person-group><article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title><source>NeuroImage</source><year>2004</year><volume>23</volume><fpage>S208</fpage><lpage>S219</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id><pub-id pub-id-type="pmid">15501092</pub-id>
</element-citation><mixed-citation id="mc-CR39" publication-type="journal">Smith, S. M. <italic>et al</italic>. Advances in functional and structural MR image analysis and implementation as FSL. <italic>NeuroImage</italic><bold>23</bold>, S208&#x02013;S219, 10.1016/j.neuroimage.2004.07.051 (2004).<pub-id pub-id-type="pmid">15501092</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><article-title>Cortical Surface-Based Analysis: I. Segmentation and Surface Reconstruction</article-title><source>NeuroImage</source><year>1999</year><volume>9</volume><fpage>179</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id><pub-id pub-id-type="pmid">9931268</pub-id>
</element-citation><mixed-citation id="mc-CR40" publication-type="journal">Dale, A. M., Fischl, B. &#x00026; Sereno, M. I. Cortical Surface-Based Analysis: I. Segmentation and Surface Reconstruction. <italic>NeuroImage</italic><bold>9</bold>, 179&#x02013;194, 10.1006/nimg.1998.0395 (1999).<pub-id pub-id-type="pmid">9931268</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>AI</given-names></name><etal/></person-group><article-title>Localization of dense intracranial electrode arrays using magnetic resonance imaging</article-title><source>NeuroImage</source><year>2012</year><volume>63</volume><fpage>157</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.06.039</pub-id><pub-id pub-id-type="pmid">22759995</pub-id>
</element-citation><mixed-citation id="mc-CR41" publication-type="journal">Yang, A. I. <italic>et al</italic>. Localization of dense intracranial electrode arrays using magnetic resonance imaging. <italic>NeuroImage</italic><bold>63</bold>, 157&#x02013;165, 10.1016/j.neuroimage.2012.06.039 (2012).<pub-id pub-id-type="pmid">22759995</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>A</given-names></name><etal/></person-group><article-title>Unified Framework for Development, Deployment and Robust Testing of Neuroimaging Algorithms</article-title><source>Neuroinformatics</source><year>2011</year><volume>9</volume><fpage>69</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1007/s12021-010-9092-8</pub-id><pub-id pub-id-type="pmid">21249532</pub-id>
</element-citation><mixed-citation id="mc-CR42" publication-type="journal">Joshi, A. <italic>et al</italic>. Unified Framework for Development, Deployment and Robust Testing of Neuroimaging Algorithms. <italic>Neuroinformatics</italic><bold>9</bold>, 69&#x02013;84, 10.1007/s12021-010-9092-8 (2011).<pub-id pub-id-type="pmid">21249532</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name><surname>Groppe</surname><given-names>DM</given-names></name><etal/></person-group><article-title>iELVis: An open source MATLAB toolbox for localizing and visualizing human intracranial electrode data</article-title><source>J. Neurosci. Methods</source><year>2017</year><volume>281</volume><fpage>40</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.01.022</pub-id><pub-id pub-id-type="pmid">28192130</pub-id>
</element-citation><mixed-citation id="mc-CR43" publication-type="journal">Groppe, D. M. <italic>et al</italic>. iELVis: An open source MATLAB toolbox for localizing and visualizing human intracranial electrode data. <italic>J. Neurosci. Methods</italic><bold>281</bold>, 40&#x02013;48, 10.1016/j.jneumeth.2017.01.022 (2017).<pub-id pub-id-type="pmid">28192130</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Collins, D. L., Neelin, P., Peters, T. M. &#x00026; Evans, A. C. Automatic 3D Intersubject Registration of MR Volumetric Data in Standardized Talairach Space. <italic>J. Comput. Assist. Tomogr.</italic><bold>18</bold>, 192 (1994)</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Seedat, A. <italic>et al</italic>. Max Planck Institute for Empirical Aesthetics. <italic>Cogitate iEEG Data (Dataset)</italic>. 10.17617/1.CQYN-9A87 (2024).</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Seedat, A. <italic>et al</italic>. Max Planck Institute for Empirical Aesthetics. <italic>Cogitate iEEG BIDS Data (Dataset)</italic>. 10.17617/1.6qpg-gz82 (2024).</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Cogitate Consortium, Ferrante, O. <italic>et al</italic>. <italic>Cogitate Data on XNAT</italic>. 10.17617/1.k278-n152 (2025).</mixed-citation></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name><surname>Appelhoff</surname><given-names>S</given-names></name><etal/></person-group><article-title>MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis</article-title><source>J. Open Source Softw.</source><year>2019</year><volume>4</volume><fpage>1896</fpage><pub-id pub-id-type="doi">10.21105/joss.01896</pub-id><pub-id pub-id-type="pmid">35990374</pub-id>
</element-citation><mixed-citation id="mc-CR48" publication-type="journal">Appelhoff, S. <italic>et al</italic>. MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. <italic>J. Open Source Softw.</italic><bold>4</bold>, 1896, 10.21105/joss.01896 (2019).<pub-id pub-id-type="pmid">35990374</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>J-R</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><etal/></person-group><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><year>2023</year><volume>18(4)</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id></element-citation><mixed-citation id="mc-CR49" publication-type="journal">King, J.-R. &#x00026; Dehaene, S. <italic>et al</italic>. Characterizing the dynamics of mental representations: the temporal generalization method. <italic>Trends in Cognitive Sciences</italic><bold>18</bold>(4), 203-210, 10.1016/j.tics.2014.01.002 (2014).</mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">King, J.-R., Pescetelli, N. &#x00026; Dehaene, S. Brain Mechanisms Underlying the Brief Maintenance of Seen and Unseen Sensory Information. <italic>Neuron</italic><bold>92</bold>, 1122&#x02013;1134, 10.1016/j.neuron.2016.10.051 (2016).</mixed-citation></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="data"><name><surname>Lepauvre</surname><given-names>A</given-names></name><etal/><year>2024</year><data-title>iEEG-data-release</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.13832169</pub-id></element-citation><mixed-citation id="mc-CR51" publication-type="data">Lepauvre, A. <italic>et al</italic>. iEEG-data-release. <italic>Zenodo</italic>10.5281/zenodo.13832169 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Gramfort, A. <italic>et al</italic>. MEG and EEG data analysis with MNE-Python. <italic>Front. Neurosci</italic>. <bold>7</bold>, 10.3389/fnins.2013.00267 (2013).</mixed-citation></ref></ref-list></back></article>