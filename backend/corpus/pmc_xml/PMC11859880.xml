<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006363</article-id><article-id pub-id-type="pmc">PMC11859880</article-id><article-id pub-id-type="doi">10.3390/s25041134</article-id><article-id pub-id-type="publisher-id">sensors-25-01134</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>MR-FuSN: A Multi-Resolution Selective Fusion Approach for Bearing Fault Diagnosis</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sha</surname><given-names>Lin</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-01134" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Shikai</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01134" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Min</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af2-sensors-25-01134" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6922-5986</contrib-id><name><surname>Qiao</surname><given-names>Sibo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af3-sensors-25-01134" ref-type="aff">3</xref><xref rid="c1-sensors-25-01134" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Shihang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af4-sensors-25-01134" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Weixia</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-01134" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Jiaqi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><xref rid="af1-sensors-25-01134" ref-type="aff">1</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Pan</surname><given-names>Haiyang</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Liu</surname><given-names>Xinhua</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Li</surname><given-names>Xin</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01134"><label>1</label>School of Control Science and Engineering, Tiangong University, No. 399 Binshui West Road, Tianjin 300387, China</aff><aff id="af2-sensors-25-01134"><label>2</label>School of Life Sciences, Tiangong University, No. 399 Binshui West Road, Tianjin 300387, China</aff><aff id="af3-sensors-25-01134"><label>3</label>The School of Software, Tiangong University, No. 399 Binshui West Road, Tianjin 300387, China</aff><aff id="af4-sensors-25-01134"><label>4</label>The School of Information and Control Engineering, Qingdao University of Technology, No. 777, Jialingjiang East Road, Qingdao 266520, China</aff><author-notes><corresp id="c1-sensors-25-01134"><label>*</label>Correspondence: <email>siboqiao@tiangong.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>13</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1134</elocation-id><history><date date-type="received"><day>12</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>08</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>11</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Vibration signals serve as the primary data source for bearing fault diagnosis. However, when collected in complex industrial environments, these signals are often contaminated by noise interference, posing significant challenges to fault feature extraction and diagnostic accuracy. To address these issues, this paper proposes a novel bearing fault diagnosis network architecture: the Multi-Resolution Fusion Selection Network (MR-FuSN). The MR-FuSN effectively extracts domain-invariant features from input data through multi-resolution feature extraction and incorporates an adaptive kernel convolution strategy, thereby enhancing its robustness against environmental noise. Experimental results demonstrate that the MR-FuSN achieves outstanding performance in noisy environments with signal-to-noise ratios (SNRs) ranging from &#x02212;5 dB to 10 dB, particularly attaining a diagnostic accuracy of 99.97% under 0 dB conditions. This study provides technical support for practical fault diagnosis applications.</p></abstract><kwd-group><kwd>multi-resolution</kwd><kwd>selective networks</kwd><kwd>deep learning</kwd><kwd>fault diagnosis</kwd><kwd>vibration signal</kwd></kwd-group><funding-group><funding-statement>This research received no external funding.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01134"><title>1. Introduction</title><p>As indispensable components in rotating machinery, bearings are crucial for ensuring the stability and reliability of an entire mechanical system. These components support and guide rotating elements, reduce friction, and transmit loads, which are essential for the smooth operation of machinery [<xref rid="B1-sensors-25-01134" ref-type="bibr">1</xref>]. Owing to their continuous operation under various loads and conditions, bearings are prone to failure in mechanical equipment [<xref rid="B2-sensors-25-01134" ref-type="bibr">2</xref>]. These failures are often caused by excessive loads, inadequate lubrication, or material fatigue [<xref rid="B3-sensors-25-01134" ref-type="bibr">3</xref>]. For instance, excessive load can lead to deformation or cracking, insufficient lubrication can increase friction and wear, and material fatigue may cause gradual degradation of bearing materials. Unexpected bearing failures can result in significant equipment damage and pose safety risks to operators [<xref rid="B4-sensors-25-01134" ref-type="bibr">4</xref>], potentially leading to costly downtimes and hazardous situations [<xref rid="B5-sensors-25-01134" ref-type="bibr">5</xref>].</p><p>In the initial stages of bearing fault diagnosis, traditional signal analysis techniques such as wavelet transform [<xref rid="B6-sensors-25-01134" ref-type="bibr">6</xref>], Fourier analysis [<xref rid="B7-sensors-25-01134" ref-type="bibr">7</xref>], and empirical mode decomposition [<xref rid="B8-sensors-25-01134" ref-type="bibr">8</xref>] have been widely used because of their effectiveness in decomposing complex vibration signals into simpler components for analysis. These methods aid in identifying characteristic frequencies and patterns associated with specific types of faults. For example, wavelet transform, which can analyze signals at multiple scales, has proven to be particularly effective for detecting transient features in vibration signals. Fourier analysis, which is known for its frequency domain representation, can identify periodic components in the signal. Empirical mode decomposition can extract intrinsic mode functions from nonlinear and nonstationary signals, helping detect subtle fault features. For instance, He et al. [<xref rid="B9-sensors-25-01134" ref-type="bibr">9</xref>] developed an innovative method combining wavelet packet transform with convolutional neural networks and optimized it using simulated annealing to improve diagnostic accuracy. This approach leverages the strengths of signal processing and deep learning to enhance fault detection capabilities. Similarly, Sun et al. [<xref rid="B10-sensors-25-01134" ref-type="bibr">10</xref>] improved the wavelet transform technique by adjusting the modal maximum of vibration features to highlight defect-related characteristics, thereby enhancing the sensitivity of the diagnostic process. Qin et al. [<xref rid="B11-sensors-25-01134" ref-type="bibr">11</xref>] proposed a novel m-band flexible wavelet transform specifically designed for fault diagnosis of planetary gear systems, achieving higher diagnostic precision and efficiency. These advances in traditional signal analysis techniques lay the groundwork for more sophisticated diagnostic methods, demonstrating the potential of integrating traditional methods with modern machine learning algorithms to improve fault diagnosis accuracy.</p><p>Despite the practicality of these traditional methods, they have inherent limitations [<xref rid="B12-sensors-25-01134" ref-type="bibr">12</xref>]. First, they rely heavily on the expertise and experience of engineers, requiring in-depth theoretical understanding and practical skills at every stage, from data preprocessing and parameter selection to interpretation [<xref rid="B13-sensors-25-01134" ref-type="bibr">13</xref>]. This dependence on expert knowledge means that the diagnostic process can be time-consuming and prone to human error. Moreover, the necessity of manual feature extraction and parameter tuning can introduce variability and inconsistencies in the results. Second, their effectiveness diminishes when faced with highly complex systems and high-dimensional data, which potentially leads to inaccurate analytical results. Traditional methods often struggle to handle complex nonlinear interactions in data, which may result in missed or incorrect fault diagnosis. These limitations suggest that traditional approaches may not meet the stringent standards of modern equipment fault diagnosis owing to increasing data complexity and volume. These limitations highlight the need for intelligent and robust diagnostic methods that can effectively handle large-scale data and provide accurate and reliable results with minimal human intervention.</p><p>The rapid development of deep learning has resulted in unprecedented technological advancements in the field of bearing fault diagnosis. Advanced deep learning models, such as autoencoders [<xref rid="B14-sensors-25-01134" ref-type="bibr">14</xref>], convolutional neural networks (CNNs) [<xref rid="B15-sensors-25-01134" ref-type="bibr">15</xref>], and residual neural networks [<xref rid="B16-sensors-25-01134" ref-type="bibr">16</xref>], have been increasingly adopted owing to their superior ability to learn complex representations from data. These models can handle extensive datasets autonomously to identify potential fault patterns and trends, significantly reducing their reliance on expert intuition and experience. For instance, autoencoders can effectively compress and reconstruct data, highlighting deviations from normal operating patterns and making them useful for anomaly detection in bearing signals. CNNs capture spatial hierarchies in data, allowing them to automatically extract and learn features from raw vibration signals, thereby circumventing the requirements for manual feature engineering. ResNets address the vanishing gradient problem in deep networks, allowing the construction of deeper models that are capable of capturing more complex fault features. These deep learning models not only improve the accuracy and efficiency of fault diagnosis but also enable the development of more robust and scalable diagnostic systems that can adapt to various operating conditions and datasets.</p><p>For example, Wu et al. [<xref rid="B17-sensors-25-01134" ref-type="bibr">17</xref>] employed a 1D convolutional neural network to automatically extract features from vibration signals, thereby bypassing the need for complex feature extraction techniques. This approach allows raw vibration data to be directly input into the model, facilitating end-to-end learning and reducing the likelihood of human error during feature extraction. Zhang et al. [<xref rid="B18-sensors-25-01134" ref-type="bibr">18</xref>] developed a novel method that transforms raw signals into 2D images by using spatial representations to automatically identify fault features through a network. This transformation simplifies the traditional manual feature extraction process and enhances the ability of the mode to capture complex fault patterns. Li et al. [<xref rid="B19-sensors-25-01134" ref-type="bibr">19</xref>] combined wavelet packet transform with CNNs, further reducing the need for manual intervention and simplifying the fault diagnosis process. This hybrid method integrates the advantages of time&#x02013;frequency analysis and deep learning, providing a more comprehensive diagnostic framework. Jia et al. [<xref rid="B20-sensors-25-01134" ref-type="bibr">20</xref>] used a shallow kernel convolutional network combined with batch normalization and the Adam optimization algorithm to improve the convergence speed of the training process and the generalization ability of their model. This combination enhances network performance by stabilizing the learning process and optimizing weight adjustments. Hao et al. [<xref rid="B21-sensors-25-01134" ref-type="bibr">21</xref>] adopted an innovative approach by replacing the fully connected layers in traditional ResNets with global average pooling, effectively reducing model parameters and improving operational efficiency, thereby enhancing the overall performance of the system. The application of these deep learning techniques not only improves the accuracy and efficiency of fault diagnosis but also opens new avenues for optimizing early fault prediction and maintenance strategies. These advancements highlight the potential of deep learning to revolutionize the field of fault diagnosis, thereby making it more accurate, efficient, and scalable.</p><p>Despite the high effectiveness of deep learning methods in bearing fault diagnosis, there are still some limitations to their application [<xref rid="B22-sensors-25-01134" ref-type="bibr">22</xref>]. One major issue is the insufficient consideration of feature interconnectivity, which can lead to models failing to comprehensively capture crucial feature information, thereby affecting their diagnostic performance and accuracy. Moreover, the complexity and variability of the bearing operating environment poses significant diagnostic challenges. For example, mechanical vibrations and external disturbances from other components can significantly impact data quality and reduce the effectiveness of diagnostic methods [<xref rid="B23-sensors-25-01134" ref-type="bibr">23</xref>]. To address this issue, Chen et al. [<xref rid="B24-sensors-25-01134" ref-type="bibr">24</xref>] proposed a method that combined a 1D denoising convolutional autoencoder (DCAE) with a 1D convolutional neural network. This method effectively removes noise from the data by training noisy input data, thereby significantly improving the accuracy of fault diagnosis. This method enhances the robustness of the diagnostic process by filtering irrelevant noise and retaining necessary fault-related features. Similarly, Xu et al. [<xref rid="B25-sensors-25-01134" ref-type="bibr">25</xref>] developed a multi-receptive field denoising residual convolutional network that utilized a residual structure to extract deeper features, thereby enhancing the adaptability of the model to complex environmental noise.</p><p>However, previous fault diagnosis models typically employed similar network architectures, utilizing stacked convolutional layers for feature extraction followed by the direct input of the extracted features into fully connected networks. The primary method for enhancing noise resistance involves adjusting the hyperparameters (e.g., kernel size, quantity, and hidden layer size). Furthermore, existing models mainly focus on the accuracy performance in low-noise environments. In real-world scenarios, the types and intensities of noise vary significantly. Therefore, it is crucial to develop models that can handle complex noise effectively. Attention mechanisms are particularly adept at handling long-sequence data and noisy inputs, and they can adaptively capture complex dependencies in sequences, making them well suited for tasks such as time series analysis. The robustness of the model was enhanced by dynamically filtering the noise, resulting in a more accurate and reliable intelligent fault diagnosis.</p><p>To improve the diagnostic accuracy and noise resistance of bearing fault diagnosis models for rotating machinery, we propose a novel model called the MR-FuSN. This model adopts a multi-resolution attention mechanism and enhanced residual convolution to optimize the utilization of raw input data.</p><p>This paper presents a new bearing fault diagnosis model, the Multi-Resolution Fusion Selection Network(MR-FuSN). The MR-FuSN does not require complex manual feature extraction processes and demonstrates excellent robustness under noisy conditions, thereby improving the performance of fault diagnosis methods under noise interference.</p><p>The main contributions of this paper can be summarized as follows:<list list-type="order"><list-item><p>A feature extraction module is designed, combining residual convolution, multi-resolution, and attention mechanisms, effectively extracting key features at different scales and improving the detection accuracy of bearing fault diagnosis.</p></list-item><list-item><p>An adaptive dual-kernel channel-focusing module is developed that dynamically adjusts processing strategies based on the characteristics of the input data, thereby enhancing the model&#x02019;s adaptability and diagnostic efficiency in complex data environments.</p></list-item><list-item><p>The model was validated using two bearing datasets and compared to other diagnostic methods, thereby demonstrating its advantages in terms of accuracy and noise resistance. These results confirm the effectiveness and potential value of the proposed model for practical applications.</p></list-item></list></p></sec><sec id="sec2-sensors-25-01134"><title>2. Principle and Model Framework</title><p>In this paper, a novel bearing fault diagnosis framework is proposed, as shown in <xref rid="sensors-25-01134-f001" ref-type="fig">Figure 1</xref>, which integrates multi-resolution processing, Multi-Level Spatial Attention Residual Unit (MSARU) and Adaptive Dual-Core Channel-Focusing Unit (ADCFU). These units not only help to identify features, but also facilitate the model to adaptively adjust its receptive field and optimize feature selection. The construction and implementation of these methods is described in detail in the following section.</p><sec id="sec2dot1-sensors-25-01134"><title>2.1. Multi-Level Spatial Attention Residual Unit</title><p>In the field of bearing fault diagnosis, existing models based on traditional residual networks and attention mechanisms have demonstrated a certain level of effectiveness in terms of feature extraction [<xref rid="B26-sensors-25-01134" ref-type="bibr">26</xref>]. However, their linear structure often limits their capability to process multi-scale fault signals, particularly when capturing critical low-level features [<xref rid="B27-sensors-25-01134" ref-type="bibr">27</xref>]. These models struggle to simultaneously account for both global and local features when dealing with complex and diverse fault signals, which adversely affects overall diagnostic performance and accuracy. To address this issue and improve diagnostic accuracy in noisy environments, we propose a Multi-Scale Spatial Attention Residual Unit (MSARU) module. By combining the strengths of the residual networks and multi-scale attention mechanisms, the MSARU module effectively extracts key features at different scales, thereby significantly enhancing the performance of the model in complex fault signal scenarios.</p><p>The overall structure of the MSARU module is shown in <xref rid="sensors-25-01134-f001" ref-type="fig">Figure 1</xref>. Input feature <italic toggle="yes">X</italic> is first processed through a deep convolution layer [<xref rid="B28-sensors-25-01134" ref-type="bibr">28</xref>]. The depthwise convolution, as the initial operation of the MSARU module, processes each input channel independently, significantly reducing computational complexity. Given input features with dimensions <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the parameter count for depthwise convolution is <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, whereas traditional convolution requires <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, resulting in a computational reduction ratio of <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This design not only optimizes model efficiency but also enhances the extraction capability of critical intra-channel fault features (such as transient impacts) through local convolution kernels. Subsequently, pointwise convolution performs multi-scale fusion of these preprocessed local features across multiple parallel branches, avoiding the parameter explosion and local information loss problems that would occur with direct pointwise convolution [<xref rid="B29-sensors-25-01134" ref-type="bibr">29</xref>]. The operation can be expressed as<disp-formula id="FD1-sensors-25-01134"><label>(1)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mi>c</mml:mi></mml:mfenced></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mfenced open="(" close=")"><mml:mi>c</mml:mi></mml:mfenced></mml:msup><mml:mo>&#x02217;</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mi>c</mml:mi></mml:mfenced></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mi>c</mml:mi></mml:mfenced></mml:msubsup></mml:mrow></mml:math></inline-formula> is the output feature of the cth channel, and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mfenced open="(" close=")"><mml:mi>c</mml:mi></mml:mfenced></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mi>c</mml:mi></mml:mfenced></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the input feature and depth convolution kernel, respectively. This operation reduced the computational complexity of the model. Subsequently, the deep convolution feature <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> was further processed through a 1D convolution layer combined with batch normalization (BatchNorm1d) and a ReLU activation function to improve the stability and quality of the feature representation. This process can be expressed as:<disp-formula id="FD2-sensors-25-01134"><label>(2)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mn>1</mml:mn><mml:mi>d</mml:mi></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02217;</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mfenced></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Subsequently, the processed feature <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> was distributed to three parallel pointwise convolution layers for multi-scale feature extraction. Each pointwise convolution layer decomposes and reconstructs the features using convolution kernels of different sizes to capture information at various scales. The operation of each pointwise convolution layer can be expressed as<disp-formula id="FD3-sensors-25-01134"><label>(3)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02217;</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where (i = 1, 2, 3) represents different scales, and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mi>i</mml:mi></mml:mfenced></mml:msubsup></mml:mrow></mml:math></inline-formula> is the corresponding pointwise convolution kernel.</p><p>After the multi-scale features are extracted through the pointwise convolution layers, they are weighted using a self-attention mechanism (SA) [<xref rid="B30-sensors-25-01134" ref-type="bibr">30</xref>]. In the MSARU module, the different scale features <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mn>1</mml:mn></mml:mfenced></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mn>2</mml:mn></mml:mfenced></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mn>3</mml:mn></mml:mfenced></mml:msubsup></mml:mrow></mml:math></inline-formula> are combined with three trainable weight parameters, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula>, and Value, respectively, which can be represented as<disp-formula id="FD4-sensors-25-01134"><label>(4)</label><mml:math id="mm19" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mn>1</mml:mn></mml:mfenced></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mn>2</mml:mn></mml:mfenced></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mfenced open="(" close=")"><mml:mn>2</mml:mn></mml:mfenced></mml:msubsup></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
Functions <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are learning functions implemented through fully connected layers. These parameters can dynamically learn the relative importance of different features during the final fusion process through a self-attention mechanism, thereby achieving effective integration and precise representation of multi-scale features. The fused multi-scale feature representation is expressed as follows:<disp-formula id="FD5-sensors-25-01134"><label>(5)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula>
Finally, the output feature <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> from the self-attention mechanism is connected to the initial input feature <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> through a residual connection, preserving the original information while enhancing the model&#x02019;s representation capability. The residual connection formula is as follows:<disp-formula id="FD6-sensors-25-01134"><label>(6)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
This multi-scale feature extraction strategy not only improves the model&#x02019;s ability to recognize complex fault signals but also significantly reduces the number of parameters and computational cost, lowering the risk of overfitting.</p><p>During model training, all parameters in the MSARU module, including the weight coefficients <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, are optimized using the backpropagation algorithm. The loss function employs cross-entropy loss to evaluate the classification performance of the model:<disp-formula id="FD7-sensors-25-01134"><label>(7)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mfenced separators="" open="[" close="]"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">N</italic> is the number of samples, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the true label of the <italic toggle="yes">i</italic>-th sample, and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the model&#x02019;s predicted probability. By optimizing the loss function, the model gradually learns the importance of different features in fault diagnosis, thereby improving overall diagnostic accuracy.</p><p>In summary, the MSARU module, through the organic combination of depthwise convolution, pointwise convolution, and the self-attention mechanism, can effectively extract and integrate various feature information in complex multi-scale fault signal environments, significantly enhancing the model&#x02019;s diagnostic performance and robustness.</p></sec><sec id="sec2dot2-sensors-25-01134"><title>2.2. Adaptive Dual-Core Channel-Focusing Unit</title><p>To enhance the model&#x02019;s sensitivity to domain-invariant features and adaptively adjust the receptive field range, this section proposes an Adaptive Dual-Kernel Channel-Focusing Unit (ADCFU) based on channel attention. As shown in <xref rid="sensors-25-01134-f002" ref-type="fig">Figure 2</xref>, this module dynamically adjusts processing strategies according to the characteristics of the input data, strengthens the representation of domain-invariant features, and effectively suppresses irrelevant or redundant information, thereby improving the model&#x02019;s adaptability and diagnostic efficiency in complex data environments [<xref rid="B31-sensors-25-01134" ref-type="bibr">31</xref>].</p><p>The ADCFU module consists of three main components: multi-resolution feature extraction, multi-channel adaptive focusing, and feature fusion. First, the input feature <italic toggle="yes">X</italic> undergoes convolution, normalization, and activation operations to generate the initial feature <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, which is then processed using two convolutional kernels of different sizes. Specifically, <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with a kernel size of 5 was used to capture short-range features, whereas <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>9</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with a kernel size of 9 was chosen to integrate long-range features. This dual-kernel design enables the model to understand and represent input data from both local and global perspectives, thereby capturing multi-scale information more effectively. Smaller kernels help detect short-term variations and fine details, while larger kernels capture broader patterns and trends, providing a comprehensive understanding of the multi-level features in the input data [<xref rid="B32-sensors-25-01134" ref-type="bibr">32</xref>].</p><p>After obtaining the multi-resolution features <italic toggle="yes">M</italic> and <italic toggle="yes">N</italic>, a channel attention [<xref rid="B33-sensors-25-01134" ref-type="bibr">33</xref>] module is applied for adaptive channel focusing. First, the two features <italic toggle="yes">M</italic> and <italic toggle="yes">N</italic> are summed and passed through a shared convolutional layer <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to extract the initial fused features. Subsequently, global average pooling (GAP) is applied to <italic toggle="yes">T</italic> to generate the global feature representation <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The pooled feature representation is passed through a fully connected layer and a Softmax activation function to generate the attention weights for each channel:<disp-formula id="FD8-sensors-25-01134"><label>(8)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are trainable weight matrices for learning inter-channel dependencies, and <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c3;</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Sigmoid function that compresses attention weights to <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, dynamically enhancing key channels while suppressing noise-related features. This attention mechanism automatically adjusts the weights of different channels based on the context information of the input data, highlighting critical features with diagnostic value while suppressing interference from redundant information [<xref rid="B34-sensors-25-01134" ref-type="bibr">34</xref>]. The generated weights were then applied to <italic toggle="yes">M</italic> and <italic toggle="yes">N</italic> to obtain the following adaptively adjusted feature representations:<disp-formula id="FD9-sensors-25-01134"><label>(9)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:msup><mml:mi>N</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>After feature focusing, the adjusted features <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:msup><mml:mi>M</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are further fused to generate the final output features. This fusion process is achieved by adding attention-adjusted features to the initial multi-resolution features as follows:<disp-formula id="FD10-sensors-25-01134"><label>(10)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This fusion strategy not only ensures the effective combination of multi-scale information but also enhances the precision of feature representation through the channel attention mechanism, thereby improving the model&#x02019;s diagnostic capability and robustness in complex environments.</p><p>With this design, the ADCFU module can adaptively adjust the receptive field range and flexibly control the importance of different channel features, thereby enabling the model to exhibit stronger feature extraction capabilities and higher diagnostic efficiency when dealing with complex and variable fault signals.</p></sec><sec id="sec2dot3-sensors-25-01134"><title>2.3. Multi-Resolution Fusion Strategy</title><p>The fusion of multi-resolution features is a critical aspect of bearing fault diagnosis [<xref rid="B35-sensors-25-01134" ref-type="bibr">35</xref>]. Compared to single-resolution features, multi-resolution features provide a more comprehensive and representative insight into the model. By integrating features at various resolutions, the model can more effectively identify fault signals of different frequencies, thereby enhancing diagnostic accuracy and robustness.</p><p>First, the fusion of multi-resolution features significantly improves the model&#x02019;s capability to recognize fault signals at different frequencies [<xref rid="B36-sensors-25-01134" ref-type="bibr">36</xref>]. At lower resolution levels, the model can capture the low-frequency components of the fault signals, which are typically associated with initial symptoms or subtle changes in the system. At higher resolution levels, the model can detect high-frequency components that often reflect the severity of faults or abrupt changes. By considering both low-frequency and high-frequency features simultaneously, the model provides a more comprehensive understanding of the fault signals, significantly improving the fault identification accuracy and sensitivity.</p><p>Second, multi-resolution feature fusion enhances the model&#x02019;s robustness and generalization capability. In complex industrial environments, the fault signals are often affected by noise and interference. Combining features of different resolutions can effectively reduce the impact of noise and improve the model&#x02019;s ability to capture essential features. For example, low-resolution features help the model ignore some high-frequency noise, whereas high-resolution features emphasize subtle changes in the fault signals [<xref rid="B37-sensors-25-01134" ref-type="bibr">37</xref>]. This fusion strategy of multi-resolution features enhances the model&#x02019;s ability to represent features across multiple scales. Given that bearing fault signals inherently possess multi-scale characteristics, where different scales reveal different aspects of faults, the model can capture a complete view of the fault signals at various scales, thereby improving its ability to identify and locate the fault source.</p><p>In the model&#x02019;s multi-resolution feature fusion component, features at different resolutions are processed through separate channels. Initially, the features are processed at each resolution level and fused using a weighted aggregation mechanism. Specifically, let the low-resolution, medium-resolution, and high-resolution features be represented as <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:msub><mml:mi>feature</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:msub><mml:mi>feature</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:msub><mml:mi>feature</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. The fusion process can be expressed as<disp-formula id="FD11-sensors-25-01134"><label>(11)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02295;</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02295;</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>&#x02295;</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the weight matching functions for different features, and &#x02295; denotes the feature concatenation operation. Through this weighted aggregation process, the model can capture and integrate the fault information from different resolutions at various scales.</p><p>Fused feature <italic toggle="yes">F</italic> is then fed into the classifier module, which consists of a series of convolutional and fully connected layers. First, the fused features are processed through two convolutional operations (Conv-BN-ACON), which further extract features and compress the data dimensions during the convolution process. The convolution operation is expressed as<disp-formula id="FD12-sensors-25-01134"><label>(12)</label><mml:math id="mm52" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mfenced separators="" open="(" close=")"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:mfenced></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the convolution operation, <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the batch normalization layer, and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>O</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the activation function. The features processed through convolution were further extracted and dimensionally reduced using Maxpool and AdaptiveAvgPool layers, preparing the features for subsequent classification.</p><p>The pooled features are then fed into fully connected layers, where a multi-layer perceptron (MLP) learns the high-level representation of fault signals. The final classification layer maps the features to specific fault categories and outputs the corresponding classification results, which can be expressed as<disp-formula id="FD13-sensors-25-01134"><label>(13)</label><mml:math id="mm56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:mi>Y</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">W</italic> is the weight matrix of the fully connected layer, <italic toggle="yes">b</italic> is the bias term, <italic toggle="yes">Y</italic> is the pooled feature representation, and <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is used to normalize the output into a probability distribution. The final classification output includes multiple class labels corresponding to different fault types.</p><p>By effectively combining multi-resolution feature fusion and the classifier module, the model can simultaneously capture feature information at various scales and accurately identify fault types in a high-dimensional feature space. This multi-resolution feature fusion strategy significantly improves the fault identification accuracy and robustness of the model, especially in complex industrial environments and noise interference, demonstrating stronger adaptability and generalization capability.</p><p>In conclusion, the model structure combines multi-resolution feature fusion, and the classifier module enables comprehensive capture of diverse characteristics of bearing fault signals across different scales, providing an efficient and reliable solution for bearing fault diagnosis.</p></sec></sec><sec id="sec3-sensors-25-01134"><title>3. Fault Diagnosis Process Based on the MR-FuSN Model</title><p>The fault diagnosis process based on the MR-FuSN model is shown in <xref rid="sensors-25-01134-f001" ref-type="fig">Figure 1</xref> and mainly includes data preprocessing, model training and prediction, and result analysis. First, the vibration signals under different working conditions were collected through the experimental platform, and the raw data were cleaned and processed to ensure the integrity and validity of the data. Next, the vibration signals were divided into samples of fixed lengths to construct the datasets, which were then divided into training and testing sets. In the model training and prediction stages, the training set was used to train the model until its accuracy reached the expected standard. Subsequently, the test set was input into the trained model for prediction to evaluate its performance on unknown data. Finally, the prediction results were analyzed in detail, and the model performance was quantitatively evaluated by calculating the accuracy, standard deviation, and other metrics to verify the effectiveness and robustness of the model in fault diagnosis tasks.</p></sec><sec id="sec4-sensors-25-01134"><title>4. Experimental Verification</title><p>To evaluate the performance of the MR-FuSN model, we conducted experiments on two distinct datasets and assessed the classification accuracy of four comparative methods through benchmark testing. Hyperparameter optimization via grid search determined the configuration: a batch size of 32 combined with a learning rate of 0.001, strategically chosen to balance training efficiency and model convergence. The training duration was ultimately fixed at 200 epochs based on stabilization patterns observed in the validation loss curve. Adam optimizer was adopted as the core training algorithm.</p><sec id="sec4dot1-sensors-25-01134"><title>4.1. Description of Experimental Datasets</title><p>Dataset <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> was sourced from the Case Western Reserve University Bearing Data Center and included various bearing fault modes. The experimental setup is shown in <xref rid="sensors-25-01134-f003" ref-type="fig">Figure 3</xref>. In this setup, bearing defects were artificially induced using an electrical discharge machining process, creating three defect sizes with diameters of 7, 14, and 21 mils. Vibration data were collected using an accelerometer mounted on the housing with a magnetic base. These vibration signals were sampled at a rate of 12,000 samples to analyze the failures of the drive-end bearing. <xref rid="sensors-25-01134-t001" ref-type="table">Table 1</xref> presents the raw vibration signals under ten different operating conditions. Each class contains 400 samples (1-s segments). Each fault type and severity combination contains 400 samples, where each sample corresponds to a 1-s signal segment (12,000 data points). The dataset was split into training and testing sets with an 7:3 ratio (280 training samples and 120 testing samples per class), ensuring stratified sampling to maintain class distribution consistency.</p><p>The bearing dataset <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> was sourced from Paderborn University, Germany. As shown in <xref rid="sensors-25-01134-f004" ref-type="fig">Figure 4</xref>, the test rig consists of the following components: an electric motor, a torque measurement shaft, bearing test unit, flywheel, and load motor. The data were sampled at rates up to 64 kHz. The dataset includes 32 types of faulty bearings, comprising 6 healthy bearings, 14 naturally accelerated damaged bearings, and 12 artificially damaged bearings. The <inline-formula><mml:math id="mm60" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> subset used in this paper primarily consists of ten states, including one healthy bearing, five naturally accelerated damaged bearings, and four artificially damaged bearings, as shown in <xref rid="sensors-25-01134-t002" ref-type="table">Table 2</xref>. The training&#x02013;test split followed a 7:3 ratio (280 training and 120 testing samples per class), with temporal continuity preserved to avoid overlapping segments between sets.</p></sec><sec id="sec4dot2-sensors-25-01134"><title>4.2. Accuracy Comparison with Other Methods</title><p>In the experimental section, ResNet06, TDSMAE, WDCNN [<xref rid="B38-sensors-25-01134" ref-type="bibr">38</xref>], and GRU-WDCNN [<xref rid="B39-sensors-25-01134" ref-type="bibr">39</xref>] were selected as baseline models for comparison. This selection was based on three research considerations: First, ResNet06, as a classic residual network, establishes a performance benchmark for spatial feature extraction; TDSMAE achieves multi-scale feature enhancement through variable-width convolution kernels, representing the latest advances in subtle feature extraction; WDCNN employs wide-kernel convolution and stride optimization strategies, demonstrating advantages in feature capture under strong noise environments; GRU-WDCNN extends temporal modeling capabilities through gated recurrent units. These four models correspond to four major technical directions in fault diagnosis: spatial feature learning, multi-scale analysis, noise-resistant architecture, and temporal dependency modeling. Second, regarding the core challenges specific to bearing fault diagnosis, including weak impact signal detection, operational noise interference, and vibration signal temporal correlation, the selected models have demonstrated superiority in specific scenarios in their original literature, constituting a comprehensive performance evaluation benchmark.</p><p>In the experimental setup, the step size was set to 64, and the sample shape was 2048 &#x000d7; 1. The training process used the Adam optimization algorithm for gradient updates, and the hyperparameters were optimized through grid search. The model was trained for a total of 200 epochs with a batch size of 32 and an initial learning rate of 0.001. All experiments were conducted on a computer equipped with an Intel Core i7-12700K processor (Intel, Santa Clara, CA, USA) and an NVIDIA GeForce RTX 3090 GPU (NVIDIA, Santa Clara, CA, USA) using the PyTorch (v2.4.0) deep learning framework [<xref rid="B40-sensors-25-01134" ref-type="bibr">40</xref>] for model construction and experimental simulation.</p><p>A five-fold cross-validation was performed for each method, and the final results were averaged over the five experiments, which was done to mitigate the local optimal error that may occur in a single experiment. The results are presented in <xref rid="sensors-25-01134-t003" ref-type="table">Table 3</xref>. The effectiveness and reliability of the models were evaluated using &#x0201c;accuracy&#x0201d; and &#x0201c;standard deviation&#x0201d; as metrics. For the <inline-formula><mml:math id="mm61" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> training set, all the models achieved 100% accuracy during the five training runs. On the D1 test set, MR-FuSN achieved the best performance, with an accuracy of 99.98% and a standard deviation of 0.02. The accuracy of the other four methods ranged from 99.91% to 99.96%, with standard deviations between 0.03% and 0.06%, which were slightly lower than that of MR-FuSN, with a difference in accuracy of approximately 0.03%. Because the <inline-formula><mml:math id="mm62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> dataset exhibited significant differences between categories, all methods performed well during testing.</p><p>The MR-FuSN also demonstrated excellent performance during the training and testing phases in the <inline-formula><mml:math id="mm63" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> dataset. As shown in <xref rid="sensors-25-01134-t003" ref-type="table">Table 3</xref>, the accuracy of the training set was 99.98%, with a standard deviation of 0.19, and the accuracy of the test set was 99.92%, with a standard deviation of 0.09. Compared to the other models, the MR-FuSN shows greater flexibility and robustness in handling complex features owing to the functionality of the ADCFU module, which adaptively adjusts feature extraction, enhances the representation of invariant features, and suppresses irrelevant information. Consequently, the MR-FuSN maintained a strong performance even on the <inline-formula><mml:math id="mm64" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> dataset.</p><p>The convergence curves of the five methods based on the best models selected from the five-fold cross-validation are shown in <xref rid="sensors-25-01134-f005" ref-type="fig">Figure 5</xref> and <xref rid="sensors-25-01134-f006" ref-type="fig">Figure 6</xref>. During training, the training accuracy of each epoch was calculated as the average accuracy of all batches. As shown in <xref rid="sensors-25-01134-f005" ref-type="fig">Figure 5</xref>, the five methods converged relatively quickly on the <inline-formula><mml:math id="mm65" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> dataset. Moreover, the MR-FuSN exhibited the smallest fluctuation in its convergence curve compared whit the other models, indicating that the MSARU and ADCFU modules contributed to more stable gradient updates. As shown in <xref rid="sensors-25-01134-f006" ref-type="fig">Figure 6</xref>, the performance of the five methods on the other datasets showed a similar trend. The MR-FuSN maintained the fastest convergence speed and after reaching convergence and sustained a high level of accuracy with minimal fluctuations, demonstrating optimal performance. This indicates that the MR-FuSN can achieve a stable state more quickly while maintaining a high performance when dealing with complex data.</p></sec><sec id="sec4dot3-sensors-25-01134"><title>4.3. Noise Interference Experiment</title><p>Vibration signals in real industrial environments are frequently contaminated by complex noise, which poses significant challenges to fault feature extraction and diagnosis. Noise not only obscures subtle characteristics of fault signals (such as periodic impulses and harmonic components) but can also lead to feature confusion (e.g., spectral overlap between high-frequency noise and fault-induced impulses), thereby substantially degrading the diagnostic accuracy and robustness of models. Consequently, research on bearing fault diagnosis methods under noisy conditions holds crucial theoretical significance and practical application value.</p><p>To simulate the effects of noise on fault signals under actual operating conditions, this experiment introduced Gaussian white noise to contaminate original vibration signals. This approach enables systematic investigation of how different noise intensities affect model performance while validating the robustness of the proposed method in noisy environments. Through this methodology, we can more authentically assess a model&#x02019;s fault diagnosis capabilities under noise interference. A lower SNR (signal-to-noise ratio) indicates a higher level of noise contamination. The SNR is defined as<disp-formula id="FD14-sensors-25-01134"><label>(14)</label><mml:math id="mm66" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>SNR</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mo form="prefix">log</mml:mo><mml:mn>10</mml:mn></mml:msub><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msub><mml:mi>E</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:msub><mml:mi>E</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represent the signal and noise powers, respectively.</p><p>To validate the superiority of the MR-FuSN in noisy environments, we compared its performance with that of ResNet06, TDSMAE, WDCNN, and GRU-WDCNN under the same experimental settings as described above. The results are presented in <xref rid="sensors-25-01134-t004" ref-type="table">Table 4</xref>. On noisy datasets <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, the MR-FuSN demonstrated the highest diagnostic accuracy across all SNR levels. At SNR = 0 dB, the MR-FuSN achieved 99.97% accuracy on <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and 99.85% on <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, surpassing WDCNN (90.58% and 88.45%) and GRU-WDCNN (92.33% and 90.12%). The experiments demonstrate that wide convolutional kernels and attention mechanisms are effective for extracting useful fault features, even from noisy data. For the <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> dataset, the MR-FuSN, WDCNN, and GRU-WDCNN generally outperformed ResNet06 in terms of accuracy, proving that wide convolutional kernels have a significant advantage in extracting fault features in noisy environments. Their primary function is to capture a broader range of feature information, allowing them to effectively retain key features in the signal, even under complex noise backgrounds. Compared to narrow convolutional kernels, wide convolutional kernels cover a larger receptive field, enhancing the ability of the model to perceive low-frequency fault signals and global features. This characteristic enables the model to extract stable fault features despite noise interference, thereby improving noise resistance and diagnostic accuracy. Compared to ResNet06, WDCNN, and GRU-WDCNN, the MR-FuSN achieved higher accuracy owing to the combination of the ADCFU and MSARU modules. Unlike traditional CNNs, ResNets, and attention mechanisms, the MR-FuSN is more flexible in capturing long-term fault information and dealing with noise. The ADCFU module adaptively adjusts the receptive field to extract features at different scales, effectively filtering out noise and enhancing the expression of the key features. The MSARU module further refines the localization of fault features through multi-scale attention mechanisms, allowing the MR-FuSN to maintain a high level of diagnostic capability and robustness even in complex environments.</p></sec><sec id="sec4dot4-sensors-25-01134"><title>4.4. Ablation Experiment</title><p>To verify the effectiveness of each module within the MR-FuSN and the appropriateness of parameter selection, an analysis was conducted under a signal-to-noise ratio of 0 dB. Based on datasets <inline-formula><mml:math id="mm74" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm75" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, the fault diagnosis accuracy of the MR-FuSN under different ablation scenarios was investigated.</p><sec id="sec4dot4dot1-sensors-25-01134"><title>4.4.1. Comparison of Different Network Architectures</title><p>To evaluate the performance of the MDARU, ADCFU, and multi-resolution fusion strategy within the model, this section employed an experimental design based on three different network architectures. These designs aimed to validate the effectiveness of each key component within the overall framework independently.</p><p>Scheme A: To investigate the role of MDARU in fault diagnosis, a modified version of the model was designed with the MDARU unit removed, while the rest of the model remained unchanged.</p><p>Scheme B: To assess the impact of the ADCFU module on fault diagnosis performance, the ADCFU part was removed, keeping the rest of the model intact.</p><p>Scheme C: To evaluate the effectiveness of the multi-resolution fusion strategy, a simplified model was designed with feature fusion performed by concatenation along the channel dimension, while other components remained unchanged.</p><p>Through these three experimental schemes, we not only independently evaluated the contribution of each module to the overall fault diagnosis performance but also further validated the comprehensive advantages of the MR-FuSN in bearing fault diagnosis. As shown in <xref rid="sensors-25-01134-f007" ref-type="fig">Figure 7</xref> and <xref rid="sensors-25-01134-f008" ref-type="fig">Figure 8</xref>, in Scheme A, after removing the MDARU module, the diagnostic accuracy for both datasets decreased. This indicates the critical role of the MDARU in feature extraction and accuracy improvement for fault diagnosis. The MDARU module effectively identifies and enhances the global dependencies between the input features through a self-attention mechanism. This mechanism not only helps the model distinguish and emphasize features crucial in fault diagnosis, but also suppresses irrelevant noise features, thereby optimizing the feature extraction process. In Scheme B, after removing the ADCFU module, the accuracy of the model declined, demonstrating the contribution of this part of the model to fault diagnosis. The ADCFU module combines the two convolutional kernels of different sizes, allowing the model to extract various local details while effectively suppressing noise. This dual-kernel design ensures high accuracy in noisy environments, especially under complex and variable operating conditions. In Scheme C, using a simplified fusion strategy resulted in a significant decline in accuracy compared to the complete MR-FuSN model, clearly illustrating the importance of the multi-resolution fusion strategy in capturing multi-scale information from fault signals. The multi-resolution feature extraction module effectively captures multi-scale information of fault signals by utilizing feature maps of different resolutions, thereby improving diagnostic accuracy. The ablation experiment results show that removing this module leads to a noticeable drop in accuracy, further proving the superior performance of the MR-FuSN.</p><p>In summary, the MDARU, ADCFU, and multi-resolution fusion strategies in the MR-FuSN significantly enhance the model&#x02019;s stability and accuracy, as well as play a crucial role in bearing fault diagnosis tasks under complex noise backgrounds and various fault scenarios.</p></sec><sec id="sec4dot4dot2-sensors-25-01134"><title>4.4.2. Network Width Parameter Selection</title><p>This section aims to validate the appropriateness of the multi-resolution parameter selection in the MR-FuSN network. A series of comprehensive experiments were conducted in the context of bearing fault diagnosis to verify the multi-resolution capability of the MR-FuSN network. The results indicate that optimizing the network resolution parameters can improve model performance. Selecting an appropriate network multi-resolution is crucial for bearing fault diagnosis, as it directly impacts the model&#x02019;s ability to identify fault-related features. In our experiments, we used four different network widths: 4, 6, 8, and 10.</p><p><xref rid="sensors-25-01134-f009" ref-type="fig">Figure 9</xref> shows the variation in the model accuracy for different network widths on the <inline-formula><mml:math id="mm76" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> dataset. As illustrated, when the network width increased from 4 to 8, the diagnostic accuracy of the model improved significantly. For example, when the network width was <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the model accuracy initially increased rapidly but then stabilized, ultimately reaching approximately 96.12%. This suggests that the basic network structure can effectively learn fundamental fault patterns but struggles to capture complex features. When the network width was <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the model accuracy gradually increased during training and eventually stabilized at approximately 96.25%. Compared to <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the network width of <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> demonstrates better performance in capturing complex features. At <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the model achieved the highest accuracy of approximately 99.98%. This indicates that with this parameter setting, the model attains an optimal balance between the feature extraction and generalization capability. However, when the network width was further increased to <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the model accuracy initially increased more rapidly in the early training stages, but then stabilized at approximately 94%. This suggests that an excessively wide network may lead to overfitting, resulting in reduced generalization performance on the test set, despite improved performance on the training set.</p><p><xref rid="sensors-25-01134-f010" ref-type="fig">Figure 10</xref> presents the accuracy trends of the model with different network widths on the <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> dataset, which shows a similar overall pattern to the results on the <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> dataset. As the network width increased, the diagnostic accuracy of the model initially increased and then decreased, indicating that an appropriate network width can enhance the feature extraction capability and diagnostic accuracy. However, an excessiverly large network width may cause the model to overfit, thereby impairing its generalization capability. At <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, the model again demonstrated the best diagnostic performance on the <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> dataset, further confirming the effectiveness of this network structure parameter in balancing feature extraction and model complexity.</p><p>In summary, the network width significantly influences the fault diagnosis performance of the model. For both datasets <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm88" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, as the network width increased from 4 to 8, the diagnostic accuracy of the model improved noticeably, suggesting that a wider network structure can better capture complex features. However, when the network width is increased further to 10, the model performance exhibited a downward trend, primarily owing to overfitting, which reduces the generalization capability. Therefore, a moderate network width achieves an ideal balance between feature extraction and model complexity, effectively enhancing the diagnostic accuracy and robustness of the model under various noise environments. By carefully selecting the network width, it is possible to maintain efficient feature extraction capabilities while avoiding overfitting.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01134"><title>5. Conclusions</title><p>This paper proposes a novel fault diagnosis model, the MR-FuSN, consisting of two modules: the ADCFU and MSARU. The ADCFU module adaptively extracts multi-scale fault features from input data to effectively suppress noise interference. The MSARU module integrates multi-resolution analysis, residual convolution, and attention mechanisms, enhancing the model&#x02019;s flexibility and robustness when processing complex signals. Experimental results demonstrate that MR-FuSN significantly outperformed other baseline models in diagnostic accuracy across two datasets, particularly exhibiting superior anti-interference capability under noisy conditions. Overall, the MR-FuSN design combining the ADCFU and MSARU modules exhibited significant advantages in fault feature extraction and model generalization capability. However, the model still has applicability limitations: the MR-FuSN relies on annotated data, while industrial scenarios typically suffer from scarce fault samples and high annotation costs. Additionally, the model&#x02019;s performance under variable rotational speeds or sudden load change conditions remains unverified. Future work will focus on time&#x02013;frequency joint feature enhancement and dynamic operating condition adaptation strategies to facilitate practical applications of this model in complex industrial scenarios.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, L.S. and S.T.; methodology, L.S. and S.T.; software, L.S. and S.T.; validation, S.T. and M.W.; investigation, M.W., S.T., S.Y., S.Q., J.L., and W.L.; resources, S.T. and M.W.; data curation, L.S., S.Y., S.Q., W.L., and J.L.; writing&#x02014;original draft preparation, L.S.; writing&#x02014;review and editing, L.S.; visualization, S.T.; supervision, M.W.; project administration, S.T. and M.W. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The authors do not have permission to share the data.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01134"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Izagirre</surname><given-names>U.</given-names></name>
<name><surname>Andonegui</surname><given-names>I.</given-names></name>
<name><surname>Landa-Torres</surname><given-names>I.</given-names></name>
<name><surname>Zurutuza</surname><given-names>U.</given-names></name>
</person-group><article-title>A practical and synchronized data acquisition network architecture for industrial robot predictive maintenance in manufacturing assembly lines</article-title><source>Robot. Comput.-Integr. Manuf.</source><year>2022</year><volume>74</volume><fpage>102287</fpage><pub-id pub-id-type="doi">10.1016/j.rcim.2021.102287</pub-id></element-citation></ref><ref id="B2-sensors-25-01134"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>M.</given-names></name>
<name><surname>Jia</surname><given-names>X.</given-names></name>
</person-group><article-title>A novel strategy for signal denoising using reweighted SVD and its applications to weak fault feature enhancement of rotating machinery</article-title><source>Mech. Syst. Signal Process.</source><year>2017</year><volume>94</volume><fpage>129</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2017.02.036</pub-id></element-citation></ref><ref id="B3-sensors-25-01134"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>T.</given-names></name>
<name><surname>Zhang</surname><given-names>A.</given-names></name>
<name><surname>Wu</surname><given-names>W.</given-names></name>
<name><surname>Cheng</surname><given-names>L.</given-names></name>
</person-group><article-title>Compound fault diagnosis for industrial robots based on dual-transformer networks</article-title><source>J. Manuf. Syst.</source><year>2023</year><volume>66</volume><fpage>163</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1016/j.jmsy.2022.12.006</pub-id></element-citation></ref><ref id="B4-sensors-25-01134"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
<name><surname>Zhang</surname><given-names>W.-a.</given-names></name>
</person-group><article-title>MSWR-LRCN: A new deep learning approach to remaining useful life estimation of bearings</article-title><source>Control Eng. Pract.</source><year>2022</year><volume>118</volume><fpage>104969</fpage><pub-id pub-id-type="doi">10.1016/j.conengprac.2021.104969</pub-id></element-citation></ref><ref id="B5-sensors-25-01134"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>Q.</given-names></name>
<name><surname>Jiang</surname><given-names>X.</given-names></name>
<name><surname>Du</surname><given-names>G.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Zhu</surname><given-names>Z.</given-names></name>
</person-group><article-title>Smart multichannel mode extraction for enhanced bearing fault diagnosis</article-title><source>Mech. Syst. Signal Process.</source><year>2023</year><volume>189</volume><fpage>110107</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2023.110107</pub-id></element-citation></ref><ref id="B6-sensors-25-01134"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Tian</surname><given-names>J.</given-names></name>
<name><surname>Liang</surname><given-names>P.</given-names></name>
<name><surname>Xu</surname><given-names>X.</given-names></name>
<name><surname>Yu</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>D.</given-names></name>
</person-group><article-title>Single and simultaneous fault diagnosis of gearbox via wavelet transform and improved deep residual network under imbalanced data</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>133</volume><fpage>108146</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.108146</pub-id></element-citation></ref><ref id="B7-sensors-25-01134"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rai</surname><given-names>V.</given-names></name>
<name><surname>Mohanty</surname><given-names>A.</given-names></name>
</person-group><article-title>Bearing fault diagnosis using FFT of intrinsic mode functions in Hilbert&#x02013;Huang transform</article-title><source>Mech. Syst. Signal Process.</source><year>2007</year><volume>21</volume><fpage>2607</fpage><lpage>2615</lpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2006.12.004</pub-id></element-citation></ref><ref id="B8-sensors-25-01134"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>Y.</given-names></name>
<name><surname>Yu</surname><given-names>D.</given-names></name>
<name><surname>Cheng</surname><given-names>J.</given-names></name>
</person-group><article-title>A roller bearing fault diagnosis method based on EMD energy entropy and ANN</article-title><source>J. Sound Vib.</source><year>2006</year><volume>294</volume><fpage>269</fpage><lpage>277</lpage></element-citation></ref><ref id="B9-sensors-25-01134"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>He</surname><given-names>F.</given-names></name>
<name><surname>Ye</surname><given-names>Q.</given-names></name>
</person-group><article-title>A bearing fault diagnosis method based on wavelet packet transform and convolutional neural network optimized by simulated annealing algorithm</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>1410</elocation-id><pub-id pub-id-type="doi">10.3390/s22041410</pub-id><pub-id pub-id-type="pmid">35214312</pub-id>
</element-citation></ref><ref id="B10-sensors-25-01134"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sun</surname><given-names>Q.</given-names></name>
<name><surname>Tang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Singularity analysis using continuous wavelet transform for bearing fault diagnosis</article-title><source>Mech. Syst. Signal Process.</source><year>2002</year><volume>16</volume><fpage>1025</fpage><lpage>1041</lpage><pub-id pub-id-type="doi">10.1006/mssp.2002.1474</pub-id></element-citation></ref><ref id="B11-sensors-25-01134"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qin</surname><given-names>Y.</given-names></name>
<name><surname>Mao</surname><given-names>Y.</given-names></name>
<name><surname>Tang</surname><given-names>B.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>H.</given-names></name>
</person-group><article-title>M-band flexible wavelet transform and its application to the fault diagnosis of planetary gear transmission systems</article-title><source>Mech. Syst. Signal Process.</source><year>2019</year><volume>134</volume><fpage>106298</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2019.106298</pub-id></element-citation></ref><ref id="B12-sensors-25-01134"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kumar</surname><given-names>A.</given-names></name>
<name><surname>Vashishtha</surname><given-names>G.</given-names></name>
<name><surname>Gandhi</surname><given-names>C.</given-names></name>
<name><surname>Tang</surname><given-names>H.</given-names></name>
<name><surname>Xiang</surname><given-names>J.</given-names></name>
</person-group><article-title>Sparse transfer learning for identifying rotor and gear defects in the mechanical machinery</article-title><source>Measurement</source><year>2021</year><volume>179</volume><fpage>109494</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2021.109494</pub-id></element-citation></ref><ref id="B13-sensors-25-01134"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jia</surname><given-names>F.</given-names></name>
<name><surname>Lei</surname><given-names>Y.</given-names></name>
<name><surname>Lin</surname><given-names>J.</given-names></name>
<name><surname>Zhou</surname><given-names>X.</given-names></name>
<name><surname>Lu</surname><given-names>N.</given-names></name>
</person-group><article-title>Deep neural networks: A promising tool for fault characteristic mining and intelligent diagnosis of rotating machinery with massive data</article-title><source>Mech. Syst. Signal Process.</source><year>2016</year><volume>72</volume><fpage>303</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2015.10.025</pub-id></element-citation></ref><ref id="B14-sensors-25-01134"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>R.</given-names></name>
<name><surname>He</surname><given-names>G.</given-names></name>
<name><surname>Liao</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>W.</given-names></name>
</person-group><article-title>A two-stage transfer adversarial network for intelligent fault diagnosis of rotating machinery with multiple new faults</article-title><source>IEEE/ASME Trans. Mechatron.</source><year>2020</year><volume>26</volume><fpage>1591</fpage><lpage>1601</lpage><pub-id pub-id-type="doi">10.1109/TMECH.2020.3025615</pub-id></element-citation></ref><ref id="B15-sensors-25-01134"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tang</surname><given-names>S.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Yuan</surname><given-names>S.</given-names></name>
</person-group><article-title>A novel adaptive convolutional neural network for fault diagnosis of hydraulic piston pump with acoustic images</article-title><source>Adv. Eng. Inform.</source><year>2022</year><volume>52</volume><fpage>101554</fpage><pub-id pub-id-type="doi">10.1016/j.aei.2022.101554</pub-id></element-citation></ref><ref id="B16-sensors-25-01134"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>M.</given-names></name>
<name><surname>Pang</surname><given-names>S.</given-names></name>
<name><surname>Song</surname><given-names>L.</given-names></name>
<name><surname>Qiao</surname><given-names>S.</given-names></name>
</person-group><article-title>Intelligent fault diagnosis and visual interpretability of rotating machinery based on residual neural network</article-title><source>Measurement</source><year>2022</year><volume>196</volume><fpage>111228</fpage><pub-id pub-id-type="doi">10.1016/j.measurement.2022.111228</pub-id></element-citation></ref><ref id="B17-sensors-25-01134"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>C.</given-names></name>
<name><surname>Jiang</surname><given-names>P.</given-names></name>
<name><surname>Ding</surname><given-names>C.</given-names></name>
<name><surname>Feng</surname><given-names>F.</given-names></name>
<name><surname>Chen</surname><given-names>T.</given-names></name>
</person-group><article-title>Intelligent fault diagnosis of rotating machinery based on one-dimensional convolutional neural network</article-title><source>Comput. Ind.</source><year>2019</year><volume>108</volume><fpage>53</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/j.compind.2018.12.001</pub-id></element-citation></ref><ref id="B18-sensors-25-01134"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Yi</surname><given-names>S.</given-names></name>
<name><surname>Liang</surname><given-names>G.</given-names></name>
<name><surname>Hongli</surname><given-names>G.</given-names></name>
<name><surname>Xin</surname><given-names>H.</given-names></name>
<name><surname>Hongliang</surname><given-names>S.</given-names></name>
</person-group><article-title>A new bearing fault diagnosis method based on modified convolutional neural networks</article-title><source>Chin. J. Aeronaut.</source><year>2020</year><volume>33</volume><fpage>439</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1016/j.cja.2019.07.011</pub-id></element-citation></ref><ref id="B19-sensors-25-01134"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>G.</given-names></name>
<name><surname>Deng</surname><given-names>C.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Xu</surname><given-names>X.</given-names></name>
</person-group><article-title>Rolling bearing fault diagnosis based on wavelet packet transform and convolutional neural network</article-title><source>Appl. Sci.</source><year>2020</year><volume>10</volume><elocation-id>770</elocation-id><pub-id pub-id-type="doi">10.3390/app10030770</pub-id></element-citation></ref><ref id="B20-sensors-25-01134"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jia</surname><given-names>X.</given-names></name>
<name><surname>Xiao</surname><given-names>B.</given-names></name>
<name><surname>Zhao</surname><given-names>Z.</given-names></name>
<name><surname>Ma</surname><given-names>L.</given-names></name>
<name><surname>Wang</surname><given-names>N.</given-names></name>
</person-group><article-title>Bearing fault diagnosis method based on CNN-LightGBM</article-title><source>IOP Conf. Ser. Mater. Sci. Eng.</source><year>2021</year><volume>1043</volume><fpage>022066</fpage><pub-id pub-id-type="doi">10.1088/1757-899X/1043/2/022066</pub-id></element-citation></ref><ref id="B21-sensors-25-01134"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hao</surname><given-names>X.</given-names></name>
<name><surname>Zheng</surname><given-names>Y.</given-names></name>
<name><surname>Lu</surname><given-names>L.</given-names></name>
<name><surname>Pan</surname><given-names>H.</given-names></name>
</person-group><article-title>Research on intelligent fault diagnosis of rolling bearing based on improved deep residual network</article-title><source>Appl. Sci.</source><year>2021</year><volume>11</volume><elocation-id>10889</elocation-id><pub-id pub-id-type="doi">10.3390/app112210889</pub-id></element-citation></ref><ref id="B22-sensors-25-01134"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Jin</surname><given-names>Y.</given-names></name>
<name><surname>Bai</surname><given-names>G.</given-names></name>
</person-group><article-title>Selective kernel convolution deep residual network based on channel-spatial attention mechanism and feature fusion for mechanical fault diagnosis</article-title><source>ISA Trans.</source><year>2023</year><volume>133</volume><fpage>369</fpage><lpage>383</lpage><pub-id pub-id-type="doi">10.1016/j.isatra.2022.06.035</pub-id><pub-id pub-id-type="pmid">35798589</pub-id>
</element-citation></ref><ref id="B23-sensors-25-01134"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>Q.</given-names></name>
<name><surname>Yi</surname><given-names>C.</given-names></name>
<name><surname>Yan</surname><given-names>L.</given-names></name>
<name><surname>Song</surname><given-names>X.</given-names></name>
<name><surname>Xu</surname><given-names>D.</given-names></name>
<name><surname>Huang</surname><given-names>C.</given-names></name>
<name><surname>Zhou</surname><given-names>L.</given-names></name>
<name><surname>Lin</surname><given-names>J.</given-names></name>
</person-group><article-title>Multi-objective sparsity maximum mode de-composition: A new method for rotating machine fault diagnosis on high-speed train axle box</article-title><source>IEEE Trans. Veh. Technol.</source><year>2023</year><volume>72</volume><fpage>12744</fpage><lpage>12756</lpage><pub-id pub-id-type="doi">10.1109/TVT.2023.3271588</pub-id></element-citation></ref><ref id="B24-sensors-25-01134"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Pan</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>G.</given-names></name>
<name><surname>Zi</surname><given-names>Y.</given-names></name>
<name><surname>Yuan</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>He</surname><given-names>Z.</given-names></name>
</person-group><article-title>Wavelet transform based on inner product in fault diagnosis of rotating machinery: A review</article-title><source>Mech. Syst. Signal Process.</source><year>2016</year><volume>70</volume><fpage>1</fpage><lpage>35</lpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2015.08.023</pub-id></element-citation></ref><ref id="B25-sensors-25-01134"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Yan</surname><given-names>X.</given-names></name>
<name><surname>Sun</surname><given-names>B.</given-names></name>
<name><surname>Zhai</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
</person-group><article-title>Multireceptive field denoising residual convolutional networks for fault diagnosis</article-title><source>IEEE Trans. Ind. Electron.</source><year>2021</year><volume>69</volume><fpage>11686</fpage><lpage>11696</lpage><pub-id pub-id-type="doi">10.1109/TIE.2021.3125666</pub-id></element-citation></ref><ref id="B26-sensors-25-01134"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>S.</given-names></name>
<name><surname>Pang</surname><given-names>S.</given-names></name>
<name><surname>Ning</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>M.</given-names></name>
<name><surname>Song</surname><given-names>L.</given-names></name>
</person-group><article-title>ANC-Net: A novel multi-scale active noise cancellation network for rotating machinery fault diagnosis based on discrete wavelet transform</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>265</volume><fpage>125937</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.125937</pub-id></element-citation></ref><ref id="B27-sensors-25-01134"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B28-sensors-25-01134"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>Q.</given-names></name>
<name><surname>Gu</surname><given-names>F.</given-names></name>
</person-group><article-title>DNOCNet: A Novel End-to-End Network for Induction Motor Drive Systems Fault Diagnosis Under Speed Fluctuation Condition</article-title><source>IEEE Trans. Ind. Inform.</source><year>2024</year><volume>20</volume><fpage>8284</fpage><lpage>8293</lpage><pub-id pub-id-type="doi">10.1109/TII.2024.3369239</pub-id></element-citation></ref><ref id="B29-sensors-25-01134"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jardine</surname><given-names>A.K.</given-names></name>
<name><surname>Lin</surname><given-names>D.</given-names></name>
<name><surname>Banjevic</surname><given-names>D.</given-names></name>
</person-group><article-title>A review on machinery diagnostics and prognostics implementing condition-based maintenance</article-title><source>Mech. Syst. Signal Process.</source><year>2006</year><volume>20</volume><fpage>1483</fpage><lpage>1510</lpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2005.09.012</pub-id></element-citation></ref><ref id="B30-sensors-25-01134"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cui</surname><given-names>L.</given-names></name>
<name><surname>Tian</surname><given-names>X.</given-names></name>
<name><surname>Wei</surname><given-names>Q.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
</person-group><article-title>A self-attention based contrastive learning method for bearing fault diagnosis</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>238</volume><fpage>121645</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2023.121645</pub-id></element-citation></ref><ref id="B31-sensors-25-01134"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Han</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>L.</given-names></name>
<name><surname>Luo</surname><given-names>Z.</given-names></name>
</person-group><article-title>Remote Sensing Teacher: Cross-Domain Detection Transformer with Learnable Frequency-Enhanced Feature Alignment in Remote Sensing Imagery</article-title><source>IEEE Trans. Geosci. Remote. Sens.</source><year>2024</year><volume>62</volume><fpage>5619814</fpage><pub-id pub-id-type="doi">10.1109/TGRS.2024.3378284</pub-id></element-citation></ref><ref id="B32-sensors-25-01134"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yan</surname><given-names>S.</given-names></name>
<name><surname>Shao</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Zheng</surname><given-names>X.</given-names></name>
<name><surname>Liu</surname><given-names>B.</given-names></name>
</person-group><article-title>LiConvFormer: A lightweight fault diagnosis framework using separable multiscale convolution and broadcast self-attention</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>237</volume><fpage>121338</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2023.121338</pub-id></element-citation></ref><ref id="B33-sensors-25-01134"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yao</surname><given-names>Y.</given-names></name>
<name><surname>Gui</surname><given-names>G.</given-names></name>
<name><surname>Yang</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
</person-group><article-title>A recursive multi-head self-attention learning for acoustic-based gear fault diagnosis in real-industrial noise condition</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>133</volume><fpage>108240</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.108240</pub-id></element-citation></ref><ref id="B34-sensors-25-01134"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Noman</surname><given-names>K.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Feng</surname><given-names>K.</given-names></name>
<name><surname>Liu</surname><given-names>Z.</given-names></name>
<name><surname>Deng</surname><given-names>Z.</given-names></name>
</person-group><article-title>Multivariate multiscale dispersion Lempel&#x02013;Ziv complexity for fault diagnosis of machinery with multiple channels</article-title><source>Inf. Fusion</source><year>2024</year><volume>104</volume><fpage>102152</fpage><pub-id pub-id-type="doi">10.1016/j.inffus.2023.102152</pub-id></element-citation></ref><ref id="B35-sensors-25-01134"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dong</surname><given-names>Y.</given-names></name>
<name><surname>Jiang</surname><given-names>H.</given-names></name>
<name><surname>Yao</surname><given-names>R.</given-names></name>
<name><surname>Mu</surname><given-names>M.</given-names></name>
<name><surname>Yang</surname><given-names>Q.</given-names></name>
</person-group><article-title>Rolling bearing intelligent fault diagnosis towards variable speed and imbalanced samples using multiscale dynamic supervised contrast learning</article-title><source>Reliab. Eng. Syst. Saf.</source><year>2024</year><volume>243</volume><fpage>109805</fpage><pub-id pub-id-type="doi">10.1016/j.ress.2023.109805</pub-id></element-citation></ref><ref id="B36-sensors-25-01134"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>B.</given-names></name>
<name><surname>Qiao</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>N.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>F.</given-names></name>
<name><surname>Peng</surname><given-names>Q.</given-names></name>
</person-group><article-title>Attention-based ConvNeXt with a parallel multiscale dilated convolution residual module for fault diagnosis of rotating machinery</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>249</volume><fpage>123764</fpage><pub-id pub-id-type="doi">10.1016/j.eswa.2024.123764</pub-id></element-citation></ref><ref id="B37-sensors-25-01134"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gong</surname><given-names>B.</given-names></name>
<name><surname>An</surname><given-names>A.</given-names></name>
<name><surname>Shi</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
</person-group><article-title>Fast fault detection method for photovoltaic arrays with adaptive deep multiscale feature enhancement</article-title><source>Appl. Energy</source><year>2024</year><volume>353</volume><fpage>122071</fpage><pub-id pub-id-type="doi">10.1016/j.apenergy.2023.122071</pub-id></element-citation></ref><ref id="B38-sensors-25-01134"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Song</surname><given-names>X.</given-names></name>
<name><surname>Cong</surname><given-names>Y.</given-names></name>
<name><surname>Song</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Liang</surname><given-names>P.</given-names></name>
</person-group><article-title>A bearing fault diagnosis model based on CNN with wide convolution kernels</article-title><source>J. Ambient. Intell. Humaniz. Comput.</source><year>2022</year><volume>13</volume><fpage>4041</fpage><lpage>4056</lpage><pub-id pub-id-type="doi">10.1007/s12652-021-03177-x</pub-id></element-citation></ref><ref id="B39-sensors-25-01134"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Shenfield</surname><given-names>A.</given-names></name>
<name><surname>Howarth</surname><given-names>M.</given-names></name>
</person-group><article-title>A novel deep learning model for the detection and identification of rolling element-bearing faults</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>5112</elocation-id><pub-id pub-id-type="doi">10.3390/s20185112</pub-id><pub-id pub-id-type="pmid">32911771</pub-id>
</element-citation></ref><ref id="B40-sensors-25-01134"><label>40.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Imambi</surname><given-names>S.</given-names></name>
<name><surname>Prakash</surname><given-names>K.B.</given-names></name>
<name><surname>Kanagachidambaresan</surname><given-names>G.</given-names></name>
</person-group><source>PyTorch</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2021</year><fpage>87</fpage><lpage>104</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01134-f001"><label>Figure 1</label><caption><p>The overall architecture of MR-FuSN. SA: self-attention module; <inline-formula><mml:math id="mm89" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula>, Value: query, key and value vectors for self-attention calculation; &#x02295;: feature fusion operation; <inline-formula><mml:math id="mm91" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, ..., <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: different network widths.</p></caption><graphic xlink:href="sensors-25-01134-g001" position="float"/></fig><fig position="float" id="sensors-25-01134-f002"><label>Figure 2</label><caption><p>Adaptive Dual-Core Channel-Focusing Unit. &#x02297;: element-wise multiplication; &#x02295;: feature fusion.</p></caption><graphic xlink:href="sensors-25-01134-g002" position="float"/></fig><fig position="float" id="sensors-25-01134-f003"><label>Figure 3</label><caption><p>The test rig of <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01134-g003" position="float"/></fig><fig position="float" id="sensors-25-01134-f004"><label>Figure 4</label><caption><p>The test rig of <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01134-g004" position="float"/></fig><fig position="float" id="sensors-25-01134-f005"><label>Figure 5</label><caption><p>The convergence curve of <inline-formula><mml:math id="mm96" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01134-g005" position="float"/></fig><fig position="float" id="sensors-25-01134-f006"><label>Figure 6</label><caption><p>The convergence curve of <inline-formula><mml:math id="mm97" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01134-g006" position="float"/></fig><fig position="float" id="sensors-25-01134-f007"><label>Figure 7</label><caption><p>Accuracy of different network architectures under dataset <inline-formula><mml:math id="mm98" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01134-g007" position="float"/></fig><fig position="float" id="sensors-25-01134-f008"><label>Figure 8</label><caption><p>Accuracy of different network architectures under dataset <inline-formula><mml:math id="mm99" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01134-g008" position="float"/></fig><fig position="float" id="sensors-25-01134-f009"><label>Figure 9</label><caption><p>Fault diagnosis accuracy for different network parameters under dataset <inline-formula><mml:math id="mm100" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01134-g009" position="float"/></fig><fig position="float" id="sensors-25-01134-f010"><label>Figure 10</label><caption><p>Fault diagnosis accuracy for different network parameters under dataset <inline-formula><mml:math id="mm101" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="sensors-25-01134-g010" position="float"/></fig><table-wrap position="float" id="sensors-25-01134-t001"><object-id pub-id-type="pii">sensors-25-01134-t001_Table 1</object-id><label>Table 1</label><caption><p>Description of datasets <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Class Label</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Fault Diameters</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number of Samples</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Fault Types</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
0</td><td align="center" valign="middle" rowspan="1" colspan="1">0</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">Normal</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">0.007</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">IF</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">0.007</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">BF</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.007</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">OF</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">0.014</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">IF</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.014</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">BF</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">0.014</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">OF</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">0.021</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">IF</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">0.021</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">BF</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.021</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">400</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">OF</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01134-t002"><object-id pub-id-type="pii">sensors-25-01134-t002_Table 2</object-id><label>Table 2</label><caption><p>Description of dataset <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Class Label</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Bearing Code</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number of Samples</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Fault Types</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
0</td><td align="center" valign="middle" rowspan="1" colspan="1">K003</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">Normal</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">KA04</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">OR</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">KA16</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">OR</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">KI04</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">IR</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">KI16</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">IR</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">KI18</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">IR</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">KA01</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">OR</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">KA05</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">OR</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">KI01</td><td align="center" valign="middle" rowspan="1" colspan="1">400</td><td align="center" valign="middle" rowspan="1" colspan="1">IR</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KI05</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">400</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IR</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01134-t003"><object-id pub-id-type="pii">sensors-25-01134-t003_Table 3</object-id><label>Table 3</label><caption><p>Mean precision and standard deviation on different datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula> (Train)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula> (Test)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula> (Train)</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mstyle></mml:mrow></mml:math></inline-formula> (Test)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">MR-FuSN</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm108" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>100.00</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.01</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm109" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.98</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.02</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm110" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.98</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.19</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm111" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.92</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.09</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm112" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>100.00</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.03</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm113" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.91</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.05</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm114" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.83</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.31</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm115" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.75</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.26</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">TDSAE</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm116" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>100.00</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.01</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm117" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.93</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.03</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm118" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.89</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.26</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.65</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.21</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">WDCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm120" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>100.00</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.02</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm121" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.95</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.04</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm122" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.91</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.13</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm123" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.77</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.15</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GRU-WDCNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>100.00</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.01</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.96</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.06</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.93</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.21</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm127" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>99.80</mml:mn><mml:mo>&#x000b1;</mml:mo><mml:mn>0.19</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01134-t004"><object-id pub-id-type="pii">sensors-25-01134-t004_Table 4</object-id><label>Table 4</label><caption><p>Diagnostic accuracy of different models on noisy datasets <inline-formula><mml:math id="mm128" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm129" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">SNR = &#x02212;5 dB</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">SNR = 0 dB</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">SNR = 10 dB</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">SNR = &#x02212;5 dB</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">SNR = 0 dB</th><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">SNR = 10 dB</th></tr><tr><th align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<inline-formula><mml:math id="mm130" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<inline-formula><mml:math id="mm131" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<inline-formula><mml:math id="mm133" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">(<inline-formula><mml:math id="mm135" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>)</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">MR-FuSN</td><td align="center" valign="middle" rowspan="1" colspan="1">99.33%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.97%</td><td align="center" valign="middle" rowspan="1" colspan="1">100.00%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.45%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.85%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.98%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;1.24%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.12%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.01%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;1.18%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.15%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.02%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">ResNet</td><td align="center" valign="middle" rowspan="1" colspan="1">49.49%</td><td align="center" valign="middle" rowspan="1" colspan="1">66.75%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.10%</td><td align="center" valign="middle" rowspan="1" colspan="1">48.12%</td><td align="center" valign="middle" rowspan="1" colspan="1">62.34%</td><td align="center" valign="middle" rowspan="1" colspan="1">97.89%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;7.52%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;5.25%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;2.00%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;6.33%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;4.78%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;1.45%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TDSMAE</td><td align="center" valign="middle" rowspan="1" colspan="1">50.36%</td><td align="center" valign="middle" rowspan="1" colspan="1">68.77%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.47%</td><td align="center" valign="middle" rowspan="1" colspan="1">52.11%</td><td align="center" valign="middle" rowspan="1" colspan="1">65.23%</td><td align="center" valign="middle" rowspan="1" colspan="1">98.56%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;3.91%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;1.67%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.55%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;4.02%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;2.89%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.67%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">WDCNN</td><td align="center" valign="middle" rowspan="1" colspan="1">80.71%</td><td align="center" valign="middle" rowspan="1" colspan="1">90.58%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.81%</td><td align="center" valign="middle" rowspan="1" colspan="1">78.92%</td><td align="center" valign="middle" rowspan="1" colspan="1">88.45%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.45%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;3.42%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.32%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.27%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;3.15%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;1.12%</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x000b1;0.25%</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GRU-</td><td align="center" valign="middle" rowspan="1" colspan="1">87.90%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.33%</td><td align="center" valign="middle" rowspan="1" colspan="1">100.00%</td><td align="center" valign="middle" rowspan="1" colspan="1">85.67%</td><td align="center" valign="middle" rowspan="1" colspan="1">90.12%</td><td align="center" valign="middle" rowspan="1" colspan="1">99.92%</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">WDCNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000b1;1.62%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000b1;0.42%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000b1;0.02%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000b1;2.01%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000b1;0.98%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x000b1;0.03%</td></tr></tbody></table></table-wrap></floats-group></article>