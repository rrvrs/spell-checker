<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006320</article-id><article-id pub-id-type="pmc">PMC11859712</article-id><article-id pub-id-type="doi">10.3390/s25041091</article-id><article-id pub-id-type="publisher-id">sensors-25-01091</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Impact Localization System of CFRP Structure Based on EFPI Sensors</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Junsong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-01091" ref-type="aff">1</xref><xref rid="af2-sensors-25-01091" ref-type="aff">2</xref><xref rid="c1-sensors-25-01091" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-4636-5091</contrib-id><name><surname>Peng</surname><given-names>Zipeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-sensors-25-01091" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Gan</surname><given-names>Linghui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af1-sensors-25-01091" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Jun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><xref rid="af1-sensors-25-01091" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Bai</surname><given-names>Yufang</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><xref rid="af3-sensors-25-01091" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Wan</surname><given-names>Shengpeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><xref rid="af1-sensors-25-01091" ref-type="aff">1</xref><xref rid="af2-sensors-25-01091" ref-type="aff">2</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Alberto</surname><given-names>N&#x000e9;lia J.</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01091"><label>1</label>Key Laboratory of Nondestructive Testing, Ministry of Education, Nanchang Hangkong University, Nanchang 330063, China; <email>2308085408014@stu.nchu.edu.cn</email> (Z.P.); <email>2308080300007@stu.nchu.edu.cn</email> (L.G.); <email>j132004@163.com</email> (J.L.); </aff><aff id="af2-sensors-25-01091"><label>2</label>Key Laboratory of Opto-Electronic Information Science and Technology of Jiangxi Province, Nanchang Hangkong University, Nanchang 330063, China</aff><aff id="af3-sensors-25-01091"><label>3</label>School of Electrical Engineering, Shanghai Dianji University, Shanghai 201306, China</aff><author-notes><corresp id="c1-sensors-25-01091"><label>*</label>Correspondence: <email>18070283782@163.com</email></corresp></author-notes><pub-date pub-type="epub"><day>12</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1091</elocation-id><history><date date-type="received"><day>29</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>06</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>10</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Carbon fiber composites (CFRPs) are prone to impact loads during their production, transportation, and service life. These impacts can induce microscopic damage that is always undetectable to the naked eye, thereby posing a significant safety risk to the structural integrity of CFRP structures. In this study, we developed an impact localization system for CFRP structures using extrinsic Fabry&#x02013;Perot interferometric (EFPI) sensors. The impact signals detected by EFPI sensors are demodulated at high speeds using an intensity modulation method. An impact localization method for the CFRP structure based on the energy&#x02013;entropy ratio endpoint detection and CNN-BIGRU-Attention is proposed. The time difference of arrival (TDOA) between signals from different EFPI sensors is collected to characterize the impact location. The attention mechanism is integrated into the CNN-BIGRU model to enhance the significance of the TDOA of impact signals detected by proximal EFPI sensors. The model is trained using the training set, with its parameters optimized using the sand cat swarm optimization algorithm and validation set. The localization performance of different models is then evaluated and compared using the test set. The impact localization system based on the CNN-BIGRU-Attention model using EFPI sensors was validated on a CFRP plate with an experimental area of 400 mm &#x000d7; 400 mm. The average error in impact localization is 8.14 mm, and the experimental results demonstrate the effectiveness and satisfactory performance of the proposed method.</p></abstract><kwd-group><kwd>structural health monitoring</kwd><kwd>impact localization</kwd><kwd>EFPI</kwd><kwd>energy&#x02013;entropy ratio</kwd><kwd>CNN-BIGRU-Attention</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>62105139</award-id></award-group><award-group><funding-source>Aviation Science Foundation of China</funding-source><award-id>2022Z057056003</award-id></award-group><funding-statement>This research was funded by [National Natural Science Foundation of China] grant number [62105139] and [Aviation Science Foundation of China] grant number [2022Z057056003].</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01091"><title>1. Introduction</title><p>Compared with traditional materials, CFRP offers advantages such as low mass, high specific strength and modulus, low creep, and excellent corrosion resistance. These characteristics make it widely used in fields such as aerospace and rail transit [<xref rid="B1-sensors-25-01091" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01091" ref-type="bibr">2</xref>]. However, during the manufacturing, transportation, and service processes, CFRP structures are prone to damage from collisions with other objects. Such damage can significantly compromise the structural integrity of CFRP, reducing its bearing capacity and reliability. Moreover, these damages are often undetectable to the naked eye [<xref rid="B3-sensors-25-01091" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01091" ref-type="bibr">4</xref>]. Therefore, developing health monitoring technology with impact localization ability is of significant engineering practical importance for the maintenance of CFRP structures.</p><p>Numerous studies explored the use of PZT and FBG sensors for impact localization in composite and metallic structures [<xref rid="B5-sensors-25-01091" ref-type="bibr">5</xref>]. Yan et al. introduced a technique that involves comparing time-frequency domain features between the impact point and reference impact point using the Pearson correlation coefficient to achieve impact localization [<xref rid="B6-sensors-25-01091" ref-type="bibr">6</xref>]. Qi et al. developed a methodology that extracts wavelet packet features from impact signals and determines the impact location by minimizing the disparity between the impact signals and reference signal through the application of a search algorithm [<xref rid="B7-sensors-25-01091" ref-type="bibr">7</xref>]. However, methods relying on signal difference comparisons exhibit limited resilience to signal interference. Consequently, machine learning has been widely adopted in impact localization to improve generalization capabilities. Shenoy et al. extracted impact signal features from the frequency spectrum and power spectral density using principal component analysis and applied these features for impact localization on composite sandwich plates through densely based neural networks [<xref rid="B8-sensors-25-01091" ref-type="bibr">8</xref>]. Li et al. analyzed the time and frequency domain characteristics of impact signals, subsequently employing these features as input variables for support vector regression and artificial neural networks to predict the impact locations [<xref rid="B9-sensors-25-01091" ref-type="bibr">9</xref>]. The effectiveness of machine learning-based methods depends on the sensitivity of the extracted eigenvalues to the impact location. Additionally, impact localization techniques that utilize the arrival time of impact signals are frequently employed for localizing the impact point. Zhang et al. utilized the time difference matrix matching method to achieve impact localization on composite material structures and improved localization accuracy through edge precision optimization [<xref rid="B10-sensors-25-01091" ref-type="bibr">10</xref>]. Jang et al. adopted the triangulation method using time-of-arrival data derived from amplitude thresholds to determine the impact location [<xref rid="B11-sensors-25-01091" ref-type="bibr">11</xref>]. However, these methods necessitate a high-speed FBG interrogator to capture the arrival time differences between signals. The precision of determining these time differences depends on the chosen method for extracting the signal arrival times.</p><p>Fiber optic sensors, in contrast to conventional PZT sensors, present several advantages such as small size, light weight, high sensitivity, and resistance to electromagnetic interference. Meanwhile, FBG sensors and their high-speed demodulation systems entail high costs and have different sensitivities to Lamb waves caused by impact loads in different directions [<xref rid="B12-sensors-25-01091" ref-type="bibr">12</xref>]. Due to the advantages of fiber optic sensors, as well as the symmetry of sensor structures and the simplicity and low cost of sensor manufacturing and demodulation systems, EFPI sensors are widely used in pipeline corrosion detection [<xref rid="B13-sensors-25-01091" ref-type="bibr">13</xref>], ocean monitoring [<xref rid="B14-sensors-25-01091" ref-type="bibr">14</xref>], biomedical purposes [<xref rid="B15-sensors-25-01091" ref-type="bibr">15</xref>], and sound source localization [<xref rid="B16-sensors-25-01091" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01091" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01091" ref-type="bibr">18</xref>,<xref rid="B19-sensors-25-01091" ref-type="bibr">19</xref>].</p><p>Therefore, this paper presents an impact localization method for CFRP structures based on EFPI sensors and CNN-BIGRU-Attention model. Impact signals are detected by EFPI sensors and collected via high-speed demodulation technology. This method involves extracting the arrival time of impact signals measured by EFPI sensors at different locations using the energy&#x02013;entropy ratio endpoint detection algorithm. Then, the time difference between the impact signals arriving at different sensors are considered as signal features. The feature dataset is divided into training, validation, and test sets. The models for impact localization are trained using the training set. The validation set is used to optimize model parameters, while the performance of the trained models are evaluated by test set. By comparing the localization results of different models, it is found that the localization results of CNN-BIGRU-Attention model is satisfactory.</p></sec><sec id="sec2-sensors-25-01091"><title>2. Impact Localization Algorithm</title><sec id="sec2dot1-sensors-25-01091"><title>2.1. Endpoint Detection Methods</title><sec id="sec2dot1dot1-sensors-25-01091"><title>2.1.1. Short Time Energy Method</title><p>Assuming <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mfenced><mml:mi>n</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the original signal and the i-th frame signal is <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>n</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, the short time energy <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>n</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as follows:<disp-formula id="FD1-sensors-25-01091"><label>(1)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mfenced><mml:mi>n</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formula, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> represents the frame length.</p><p>A complete impact signal includes noise segments and impact event segments, and the short time energy of the noise segment is less than that of the impact event segment, so the endpoint of impact event can be detected by this characteristic.</p></sec><sec id="sec2dot1dot2-sensors-25-01091"><title>2.1.2. Spectral Entropy Method</title><p>The concept of entropy was initially derived from thermodynamics to quantify the degree of disorder within a system and was later introduced into information theory to measure the uncertainty of random events. With the intersection and integration of disciplines, the application of entropy in signal detection and recognition has gained increasing significance. The disorder within a signal is positively correlated with its entropy value. Typically, noise exhibits a high entropy value.</p><p><inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mfenced><mml:mi>n</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as the original signal, and Fourier transform was performed on it. The spectral value of the <italic toggle="yes">k</italic>-th spectral line of the i-th frame signal after Fourier transform is represented as <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, and its normalized spectral density value <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is represented as follows:<disp-formula id="FD2-sensors-25-01091"><label>(2)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>l</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the above formula, <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> is the FFT length. Consequently, the short time spectral entropy value <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the i-th frame signal is defined as follows:<disp-formula id="FD3-sensors-25-01091"><label>(3)</label><mml:math id="mm13" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:mfenced><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:mi>log</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mi>k</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Compared to impact signals, the normalized spectral probability density function of noise has a more uniform distribution, so the spectral entropy of noise is larger than that of impact signals. Through this characteristic, we can detect impact signals and noise.</p></sec><sec id="sec2dot1dot3-sensors-25-01091"><title>2.1.3. Energy&#x02013;Entropy Ratio Method</title><p>Compared with the traditional peak method and short time energy method, the energy&#x02013;entropy ratio method combines the advantages of short time energy method and short time spectral entropy method. Although peak method and short time energy method are simple to calculate but sensitive to noise and lack robustness in non-stationary signals [<xref rid="B20-sensors-25-01091" ref-type="bibr">20</xref>]. The short time spectral entropy method performs better in noisy environments but has high computational complexity and limited adaptability to periodic noise [<xref rid="B21-sensors-25-01091" ref-type="bibr">21</xref>]. The energy&#x02013;entropy ratio method effectively highlights the significant differences between useful signal segments and noise segments by calculating the ratio of energy envelope to entropy envelope, thereby improving the accuracy and robustness of signal endpoint detection [<xref rid="B22-sensors-25-01091" ref-type="bibr">22</xref>]. In this study, the method was applied to provide a more reliable basis for extracting the arrival time of impact signals detected by EFPI sensors.</p><p>According to the principles of short time energy and spectral entropy methods, the energy envelope and entropy envelope of a signal segment exhibit opposite trends within the same interval. By calculating the energy&#x02013;entropy ratio, significant features and differences between the useful signal segment and the noise segment can be effectively highlighted. Specifically, the short time energy&#x02013;entropy ratio of a signal can be defined as follows:<disp-formula id="FD4-sensors-25-01091"><label>(4)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>E</mml:mi><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>E</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the formula, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is short time energy and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is short time spectral entropy. After calculating the energy&#x02013;entropy ratio, the endpoint is identified by selecting an appropriate threshold. The threshold calculation formula is expressed as follows:<disp-formula id="FD5-sensors-25-01091"><label>(5)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></inline-formula> is a constant, <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum value of the energy&#x02013;entropy ratio, and <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the average value of the energy&#x02013;entropy ratio.</p></sec></sec><sec id="sec2dot2-sensors-25-01091"><title>2.2. CNN-BIGRU-Attention Model</title><sec id="sec2dot2dot1-sensors-25-01091"><title>2.2.1. Convolutional Neural Network</title><p>As a widely used network model in the field of deep learning, convolutional neural network (CNN) is a feedforward neural network with a depth structure [<xref rid="B23-sensors-25-01091" ref-type="bibr">23</xref>]. As shown in <xref rid="sensors-25-01091-f001" ref-type="fig">Figure 1</xref>, conventional CNN typically consists of input layer, convolution layer, pooling layer, full connection layer, and output layer [<xref rid="B24-sensors-25-01091" ref-type="bibr">24</xref>].</p><p>The input layer is responsible for receiving and inputting the original data into the CNN. Generally, in order to optimize the calculation time, the data will be preprocessed. As the most critical component of a CNN, the convolutional layer convolves the input data to extract local features. It employs a convolution kernel to perform convolution operations on the data in a sliding window manner until all data have been processed. Additionally, the convolutional layer can effectively reduce the size of the feature map and achieve dimensionality reduction by setting appropriate strides and padding. The expression for the convolution operation is expressed as follows [<xref rid="B25-sensors-25-01091" ref-type="bibr">25</xref>]:<disp-formula id="FD6-sensors-25-01091"><label>(6)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>&#x000d7;</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In the above equation, <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the local area output value of layer <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the <italic toggle="yes">b</italic>-th channel in layer <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the output value of the <italic toggle="yes">i</italic>-th channel of the <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> layer (i.e., the input value of the n-layer), <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:math></inline-formula> signifies one-dimensional convolution operations, <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the weight coefficient of the <italic toggle="yes">i</italic>-th channel of the n-layer, and <inline-formula><mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the bias of the i-th channel of the n-layer.</p><p>The convolution layer, as the most critical component of a convolutional neural network (CNN), convolves the input data to extract local features. It selects a convolution kernel to perform the convolution operation on the data through a sliding window approach until all data have been processed. By setting the appropriate stride length and padding, the size of the feature map can be effectively reduced, and the purpose of dimension reduction can be achieved.</p><p>Following the convolution layer, the pooling layer plays a crucial role in refining the feature vectors obtained from the convolution operation through down-sampling. This process reduces the feature dimension and enhances the operational speed of the CNN. Commonly used pooling operations include maximum pooling and average pooling [<xref rid="B26-sensors-25-01091" ref-type="bibr">26</xref>]. Maximum pooling preserves important local features by selecting the maximum value in the pooled region, while average pooling preserves the overall information of features by calculating the average value of the pooled region. Therefore, maximum pooling emphasizes salient features, while average pooling maintains smoothness and a global perspective of the information. <xref rid="sensors-25-01091-f002" ref-type="fig">Figure 2</xref> below shows a schematic diagram illustrating both pooling operations:</p><p>The main function of the fully connected layer is to recombine the features obtained after convolution and pooling operations, thereby reducing the dimension of features. Subsequently, these features are expanded into feature vectors.</p><p>The output layer is located behind the full connection layer, which is responsible for generating the final classification predictions and processing input values through a specified activation function to produce the required results. CNN has powerful feature extraction ability. By using its local perception and weight sharing characteristics, it can effectively reduce redundant information, learning parameters, and computational time, ultimately simplifying the complexity of the model.</p></sec><sec id="sec2dot2dot2-sensors-25-01091"><title>2.2.2. Bidirectional Gated Recurrent Unit (BIGRU)</title><p>The Gated Recurrent Unit (GRU) is an improved version of the recurrent neural network (RNN) [<xref rid="B27-sensors-25-01091" ref-type="bibr">27</xref>], which aims to solve the gradient vanishing problem encountered by traditional RNNs when processing long sequences. By introducing a gating mechanism, GRU optimizes information flow, enabling it to retain crucial information and discard unnecessary details more effectively. Compared to standard RNN, GRU is generally easier to train, particularly with long sequence data. Furthermore, due to its reduced parameter count, GRU offers higher computational efficiency.</p><p>However, GRU has certain limitations. Its current output is solely dependent on the previous hidden state and the current input, making it challenging to capture future information within the sequence [<xref rid="B28-sensors-25-01091" ref-type="bibr">28</xref>]. To overcome this, the Bidirectional Gated Recurrent Unit (BIGRU) incorporates a reverse GRU structure [<xref rid="B29-sensors-25-01091" ref-type="bibr">29</xref>]. Compared to another variant of RNN that incorporates memory cells and gating mechanisms, known as long short-term memory (LSTM), while LSTM can effectively capture both long-term and short-term dependencies, BIGRU enhances model predictive capability by comprehensively capturing feature relationships in time series through simultaneous processing of forward and backward information [<xref rid="B30-sensors-25-01091" ref-type="bibr">30</xref>,<xref rid="B31-sensors-25-01091" ref-type="bibr">31</xref>]. The expanded model structure is illustrated in <xref rid="sensors-25-01091-f003" ref-type="fig">Figure 3</xref>:</p><p>As shown in <xref rid="sensors-25-01091-f003" ref-type="fig">Figure 3</xref>, the output layer H of BIGRU is determined by three parts: forward hidden information, reverse hidden information, and input <italic toggle="yes">X</italic>. C denotes the hidden information at time t, <italic toggle="yes">S</italic> denotes the activation value of neurons at time t, and &#x02192; and &#x02190; denote forward propagation and backward propagation, respectively. The specific formula is as follows:<disp-formula id="FD7-sensors-25-01091"><label>(7)</label><mml:math id="mm31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced close="" open="{"><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>R</mml:mi><mml:mi>U</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>R</mml:mi><mml:mi>U</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mover><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the output weight of the forward GRU, <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>&#x02190;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> represents the output weight of the reverse GRU, and <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the offset corresponding to the hidden layer state.</p></sec><sec id="sec2dot2dot3-sensors-25-01091"><title>2.2.3. Attention Mechanism</title><p>The attention mechanism is a technology that assigns different weights to various parts according to their importance when processing input data. By calculating the correlation between elements within the input sequence, the model is able to prioritize more critical information, thereby enhancing its performance. Specifically, the attention mechanism optimizes the sequence generation process by calculating the attention weight and integrating the hidden state of the encoder with the hidden state of the decoder. This mechanism enables the model to dynamically focus on the most pertinent parts of the input sequence, improving both the accuracy and fluency of the generated results [<xref rid="B32-sensors-25-01091" ref-type="bibr">32</xref>,<xref rid="B33-sensors-25-01091" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-01091" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-01091" ref-type="bibr">35</xref>].</p><p>The BIGRU model enhances its localization prediction ability by simultaneously processing forward and backward data, enabling it to capture more comprehensive trends. In addition, the attention mechanism further enhances the performance of the model by selectively emphasizing the most relevant parts of the data. At each time step, the attention mechanism assigns higher weights to key features that may indicate significant changes in the impact location, ensuring that the model focuses on the most important information. The specific formula is as follows [<xref rid="B36-sensors-25-01091" ref-type="bibr">36</xref>,<xref rid="B37-sensors-25-01091" ref-type="bibr">37</xref>]:<disp-formula id="FD8-sensors-25-01091"><label>(8)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>tanh</mml:mi><mml:mfenced><mml:mrow><mml:mi>w</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD9-sensors-25-01091"><label>(9)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD11-sensors-25-01091"><label>(10)</label><mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the value of the attention probability distribution determined by the BIGRU network layer output vector <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> at moment <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:math></inline-formula> are weight coefficients, <inline-formula><mml:math id="mm43" overflow="scroll"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> is the bias coefficient, <inline-formula><mml:math id="mm44" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the attention probability distribution of the attention mechanism output to the BIGRU hidden layer, and <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the output of the attention layer at time <inline-formula><mml:math id="mm46" overflow="scroll"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="sec2dot2dot4-sensors-25-01091"><title>2.2.4. Sand Cat Swarm Optimization</title><p>Sand cat swarm optimization (SCSO) is a swarm intelligence optimization algorithm proposed to simulate the survival behavior of sand cats preying on prey [<xref rid="B38-sensors-25-01091" ref-type="bibr">38</xref>]. The algorithm can effectively explore and develop the solution space by working together with multiple &#x0201c;sand cat&#x0201d; individuals in the search space. The algorithm can also flexibly adjust the search strategy according to the change in environment, so as to improve the efficiency of finding the optimal solution. At the same time, keeping the diversity of individuals is helpful to avoid falling into local optimal solution, thus enhancing the global search ability. Compared with genetic algorithm and particle swarm optimization algorithm, SCSO has a simpler calculation process and robust optimization performance.</p><p>The first step of SCSO algorithm is population initialization; after initialization, the search for prey begins to find the optimal solution. In the SCSO algorithm, the parameter <inline-formula><mml:math id="mm47" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> decreases linearly from 2 to 0, as defined by Equation (11), where <inline-formula><mml:math id="mm48" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is assumed to be 2, <inline-formula><mml:math id="mm49" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the current iteration number, and <inline-formula><mml:math id="mm50" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum number of iterations. Thus, in the initial iterations, sand cats move quickly; after half of the iterations, their movements become more intelligent. As with other metaheuristic algorithms, balancing exploration and exploitation phases is crucial. The SCSO uses the parameter <inline-formula><mml:math id="mm51" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> to achieve this balance. According to Equation (12), the transition between the two phases is balanced. Additionally, the formulation of Equation (13) aims to prevent the algorithm from falling into local optima. Parameter <inline-formula><mml:math id="mm52" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> determines the sensitivity range of each search agent. The core step of SCSO is updating the position of each search agent. Based on Equation (14), the position update of each search agent in every iteration depends on the best candidate position and its current position outside the sensitivity range. In Equation (14), <inline-formula><mml:math id="mm53" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm54" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm55" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> represent the best candidate position, current position, and sensitivity range, respectively. After the exploration stage (searching for prey), SCSO enters the exploitation stage (attacking prey). Equation (15) is used to calculate the distance between the best position and the current position of each search agent during the corresponding iteration. Assuming the sensitivity range is circular, the direction of each move is determined by a random angle <inline-formula><mml:math id="mm56" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula> selected by SCSO through a roulette wheel mechanism. The randomly chosen angle <inline-formula><mml:math id="mm57" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula>, ranging from 0 to 360 degrees, produces a cosine value between &#x02212;1 and 1, thus achieving circular motion. In Equation (15), <inline-formula><mml:math id="mm58" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm59" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> represent the best position and random position, respectively [<xref rid="B39-sensors-25-01091" ref-type="bibr">39</xref>].<disp-formula id="FD13-sensors-25-01091"><label>(11)</label><mml:math id="mm60" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mfenced><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi>I</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD14-sensors-25-01091"><label>(12)</label><mml:math id="mm61" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover><mml:mo>&#x000d7;</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD15-sensors-25-01091"><label>(13)</label><mml:math id="mm62" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover><mml:mo>&#x000d7;</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD16-sensors-25-01091"><label>(14)</label><mml:math id="mm63" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo>&#x000d7;</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD18-sensors-25-01091"><label>(15)</label><mml:math id="mm64" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo>&#x000d7;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover><mml:mo>&#x000d7;</mml:mo><mml:mi>cos</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD19-sensors-25-01091"><label>(16)</label><mml:math id="mm65" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mrow><mml:mfenced><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="" open="{"><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover><mml:mo>&#x000d7;</mml:mo><mml:mi>cos</mml:mi><mml:mi>&#x003b8;</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>R</mml:mi></mml:mfenced><mml:mo>&#x02264;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo>&#x000d7;</mml:mo><mml:mfenced><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mfenced><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfenced><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>R</mml:mi></mml:mfenced><mml:mo>&#x0003e;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec></sec></sec><sec id="sec3-sensors-25-01091"><title>3. Experiment</title><sec id="sec3dot1-sensors-25-01091"><title>3.1. F-P Cavity Interference Principle</title><p>The fiber Fabry&#x02013;Perot cavity sensor is developed from optical Fabry&#x02013;Perot (F-P) interferometer [<xref rid="B40-sensors-25-01091" ref-type="bibr">40</xref>], which consists of two high reflectivity, strictly parallel optical plates spaced by a distance <inline-formula><mml:math id="mm66" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula>. As shown in <xref rid="sensors-25-01091-f004" ref-type="fig">Figure 4</xref>, the reflectivities of the two optical plates is <inline-formula><mml:math id="mm67" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm68" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, while the refractive indices of the media inside and outside the F-P cavity are <inline-formula><mml:math id="mm69" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm70" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. The incident light intensity is <inline-formula><mml:math id="mm71" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the wavelength is <inline-formula><mml:math id="mm72" overflow="scroll"><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:math></inline-formula>, and the incident angle is <inline-formula><mml:math id="mm73" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>For multi-beam interference generated by parallel flat plates, the optical path difference between adjacent reflected or transmitted light is calculated as follows:<disp-formula id="FD20-sensors-25-01091"><label>(17)</label><mml:math id="mm74" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mi>L</mml:mi><mml:mi>cos</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>And the corresponding phase difference is<disp-formula id="FD21-sensors-25-01091"><label>(18)</label><mml:math id="mm75" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>L</mml:mi><mml:mi>cos</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The principle of multi-beam interference can be applied to obtain the synthesized amplitude of reflected light, thereby obtaining the reflected light intensity:<disp-formula id="FD22-sensors-25-01091"><label>(19)</label><mml:math id="mm76" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msqrt><mml:mi>cos</mml:mi><mml:mi>&#x003b4;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msqrt><mml:mi>cos</mml:mi><mml:mi>&#x003b4;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Analysis of the Formulas (18) and (19) reveals that variations in the refractive index <inline-formula><mml:math id="mm77" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>, the incident angle <inline-formula><mml:math id="mm78" overflow="scroll"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:math></inline-formula>, and the F-P cavity length <inline-formula><mml:math id="mm79" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> result in corresponding changes in the reflected light output <inline-formula><mml:math id="mm80" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Modulation can be achieved by altering these three parameters. Given that modulating the cavity length <inline-formula><mml:math id="mm81" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> of the F-P cavity is the simplest and most straightforward method, the EFPI sensor employed in this study is a F-P cavity based on the cavity length <inline-formula><mml:math id="mm82" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> modulation. As shown in <xref rid="sensors-25-01091-f005" ref-type="fig">Figure 5</xref>, the end face of optical fiber and aluminum film form an F-P microcavity. External impacts cause the aluminum film to vibrate, which will lead to a change in cavity length <inline-formula><mml:math id="mm83" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula>. From Formulas (18) and (19), it can be seen that the reflected light intensity <inline-formula><mml:math id="mm84" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> will also change. Therefore, by detecting the change in the intensity of the reflected light <inline-formula><mml:math id="mm85" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, the detected impact signal can be demodulated.</p></sec><sec id="sec3dot2-sensors-25-01091"><title>3.2. Experimental Setup</title><p>The CFRP structure impact monitoring system is shown in <xref rid="sensors-25-01091-f006" ref-type="fig">Figure 6</xref>, comprising a broadband light source (Maxray Optoelectronics Technology Co., Ltd., Hefei, China), coupler (Shanze Jiye Technology Co., Ltd., Shenzhen, China), eight circulators (Qianhai Xunka Technology Co., Ltd., Shenzhen, China), four EFPI sensors, four fiber Bragg gratings (FBGs), four photodetectors (Hongyi Optoelectronics Technology Co., Ltd., Beijing, China), CFRP structural test pieces, a 1 J spring impact hammer (Qigong Instrument Equipment Co., Ltd., Shanghai, China), and a data acquisition system (Art Technology Co., Ltd., Shijiazhuang, China). The monitoring system is based on intensity demodulation. Taking Channel 1 as an example, the broadband light source emits light that is split into four channels by the coupler. One channel proceeds through optical circulator 1 to reach EFPI sensor 1. The reflected light from EFPI sensor 1 passes through optical circulator 5 and enters the FBG. The FBG 1 filters the interference signal reflected by EFPI sensor 1, and the narrowband reflected light <inline-formula><mml:math id="mm86" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> obtained by FBG reflection will be received by the photodetector PD1. The working principle of intensity demodulation method is relatively simple, easy to implement, and has the advantages of low cost and fast dynamic response.</p><p>After establishing the impact monitoring system for the CFRP structure as depicted in <xref rid="sensors-25-01091-f006" ref-type="fig">Figure 6</xref>, an impact localization experiment was conducted on a 600 mm &#x000d7; 600 mm &#x000d7; 3 mm CFRP structure, with a 400 mm &#x000d7; 400 mm &#x000d7; 3 mm experimental area to minimize edge effects. As shown in <xref rid="sensors-25-01091-f007" ref-type="fig">Figure 7</xref>, the lower left corner of the monitored area was designated as the coordinate origin Q1 (0 mm, 0 mm), with the horizontal axis as the <italic toggle="yes">X</italic>-axis and the vertical axis as the <italic toggle="yes">Y</italic>-axis, forming a two-dimensional rectangular coordinate system.</p><p>Four EFPI sensors were adhered to the four corners of the monitored area using epoxy resin glue. The entire impact monitoring area was divided into 17 &#x000d7; 17 grids, and each small grid is 2.5 cm &#x000d7; 2.5 cm in size. The intersections of these grids represented impact points, totaling 285 (excluding sensor positions). The impact test was performed using a spring hammer with an impact energy of 1 J, with the sampling rate set to <inline-formula><mml:math id="mm87" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>&#x000a0;</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and three impacts were performed at each position, resulting in a total of 855 datasets.</p><p>According to <xref rid="sensors-25-01091-f006" ref-type="fig">Figure 6</xref>, an EFPI signal acquisition system based on intensity demodulation is built, as shown in <xref rid="sensors-25-01091-f008" ref-type="fig">Figure 8</xref>. Among them, 1 is amplified spontaneous emission (ASE) broadband light source; 2 is 1 &#x000d7; 4 coupler; 3~10 are circulators; 11 is a multi-channel voltage stabilized source, which provides a constant voltage of 12 V for photoelectric detectors; 12&#x02013;15 are EFPI sensors; 16&#x02013;19 are FBGs; 20~23 are photoelectric detectors; 24 is a data acquisition card; 25 is a computer.</p></sec></sec><sec id="sec4-sensors-25-01091"><title>4. Feature Extraction</title><sec id="sec4dot1-sensors-25-01091"><title>4.1. Data Preprocessing</title><p>When the impact hammer strikes the CFRP structure, it causes vibrations that generate impact stress waves. These waves propagate through the surface of the CFRP structure, and the closer the EFPI sensor is to the impact point, the shorter their transmission time. Consequently, the time of the impact signal received by the EFPI sensor is closely related to the relative location between the impact point and the monitoring sensor. Each EFPI sensor is employed to capture the time domain response signal characteristics at various locations following the impact.</p><p>Taking the response signals collected at impact points B3 (50 mm, 375 mm) as an example, the Daubechies wavelet function (abbreviated as DbN) is utilized due to its advantages of good regularity, smooth vanishing moment, strong localization ability in the frequency domain, and effective frequency band decomposition. Specifically, the Db8 wavelet function is employed to decompose the impact signal into eight layers [<xref rid="B41-sensors-25-01091" ref-type="bibr">41</xref>]. Subsequently, the low-frequency scale a8 is used to reconstruct the narrow-band components generated by Lamb waves. Energy&#x02013;entropy ratio endpoint detection is then performed on the reconstructed impact signal to determine the arrival time of the response signal at the monitoring sensor. The impact signals from four EFPI sensors are shown in <xref rid="sensors-25-01091-f009" ref-type="fig">Figure 9</xref>.</p><p>The energy&#x02013;entropy ratio sequence of the impact signals detected by four EFPI sensors and the corresponding extraction process of the arrival time of the impact point are shown in <xref rid="sensors-25-01091-f010" ref-type="fig">Figure 10</xref>. The energy&#x02013;entropy ratio method can highlight the difference between the impulse signal and noise, making the identification of the endpoint position of the impact signal less sensitive to the selection of threshold. In this study, the length of the short time window was set to 200, and the threshold was set to 0.1 mV. The endpoint positions of the response signals detected by the four EFPI sensors using the energy&#x02013;entropy ratio method were 0.04941 s, 0.04973 s, 0.04995 s, and 0.04971 s, respectively, and are marked with red lines. It can be easily observed that the impact signal first reaches EFPI 1, almost simultaneously reaches EFPI 2 and EFPI 4, and finally reaches EFPI 3. This is consistent with the distance from the impact point to the sensors, confirming that the arrival times extracted by the energy&#x02013;entropy ratio method can serve as features for impact localization.</p></sec><sec id="sec4dot2-sensors-25-01091"><title>4.2. Sensitivity Analysis</title><p>The contour map of the normalized arrival time differences for impact signals between EFPI 1 and the other EFPI sensors is presented in <xref rid="sensors-25-01091-f011" ref-type="fig">Figure 11</xref>. In order to mitigate discrepancies in sensor arrival times caused by location or other factors, we standardize these time differences for a unified comparison across sensors. This normalization process reflects the relative delay in signal propagation and considers the standardized time difference as the characteristic quantity of the impact signals. The specific formula is as follows:<disp-formula id="FD23-sensors-25-01091"><label>(20)</label><mml:math id="mm88" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The normalized arrival time difference is expressed as follows:<disp-formula id="FD24-sensors-25-01091"><label>(21)</label><mml:math id="mm89" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>min</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>min</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x00394;</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><xref rid="sensors-25-01091-f011" ref-type="fig">Figure 11</xref> illustrates that the time difference between impact signals varies significantly with the distance from the impact point to the monitoring EFPI sensors. When the impact point is close to one sensor and distant from another, the time difference is considerable. Conversely, if the impact point is equidistant from two sensors, the time difference is negligible. This demonstrates that EFPI sensors at different locations exhibit high sensitivity to the location of impact point. Consequently, the time difference extracted using the energy&#x02013;entropy ratio method be used as the feature for machine learning models to locate the impact on CFRP structures.</p></sec></sec><sec id="sec5-sensors-25-01091"><title>5. Impact Localization</title><sec id="sec5dot1-sensors-25-01091"><title>5.1. Establishment of Localization Model</title><p>In the experiment, 855 sets of data were obtained and divided into the training set, verification set, and test set. During the model training process, the difference between the arrival time of EFPI1 and the other three EFPI sensors <inline-formula><mml:math id="mm90" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is taken as the input, and the actual impact coordinates are taken as the output.</p><p>The CNN-BIGRU-Attention model is established with 800 sets of data for training, 40 sets of data for verification, and 15 sets of data for testing. The accuracy of the model is evaluated by calculating the average distance error between the actual and predicted impact point locations. The main steps to establish the prediction model are as follows:</p><p>Step 1: Partition and standardization of data</p><p>A random selection of 800 samples is used to form the training set. From the remaining 55 samples, another 40 are randomly chosen for the validation set, leaving the final 15 samples to serve as the test set. Then, the input features of training set and test set are normalized to the range of [0, 1]. This step eliminates the dimensional differences between different features, enhancing model stability and accelerating convergence during training.</p><p>Step 2: Establish the deep learning model</p><p>The flowchart of the CNN-BIGRU-Attention model employed in this paper is shown in <xref rid="sensors-25-01091-f012" ref-type="fig">Figure 12</xref>.</p><p>The CNN module comprises two feature extraction layers, a Flatten layer, and a Fully Connected (FC) layer. Each feature extraction layer consists of four modules: Batch Normalization (Batch Norm), Rectified Linear Unit (ReLU), and Max Pooling. The convolution kernels of the two CONV layers have a size of 3, with a step size of 1. The Batch Norm layer standardizes the input of each layer, accelerating training and enhancing stability. The ReLU layer introduces nonlinearity, aiding the model in learning complex relationships and mitigating gradient disappearance. Max Pooling reduces the dimensionality of features by downsampling the output of the convolution layer, thereby improving training efficiency.</p><p>The Flatten layer converts the output of the convolution layer into one-dimensional vectors, preparing the data for input into the subsequent FC layer. The FC layer integrates the previously extracted features to improve the nonlinear expression ability of features. The BIGRU module, paralleled with the CNN, contains two opposing GRU modules with identical parameters and 35 hidden units.</p><p>The concatenation layer integrates features from the CNN and BIGRU, facilitating feature fusion and information integration. This enhances model performance and expressiveness. Additionally, the attention mechanism allows the model to learn more intricate function mappings by dynamically focusing on various parts of the input, thereby improving its expressive capacity. Simultaneously, it effectively captures relationships between different positions in the input sequence, enabling the model to prioritize information that is distant from the current unit, thus better handling long sequence data.</p><p>The CNN-BIGRU-Attention model works as follows:<list list-type="order"><list-item><p>The normalized dataset undergoes format conversion to form a three-dimensional matrix;</p></list-item><list-item><p>The convolution layer extracts local features of the input sequence data through multi-layer convolution operations;</p></list-item><list-item><p>The pooling layer reduces dimensionality and compresses the data to lower computational complexity;</p></list-item><list-item><p>BIGRU captures long-term dependencies in time series by processing bidirectional sequence information;</p></list-item><list-item><p>The concatenation layer combines the outputs of the convolutional neural network and the Bidirectional Gated Recurrent Unit (BIGRU);</p></list-item><list-item><p>The attention mechanism module assigns different weights to each position by calculating the similarity between positions;</p></list-item><list-item><p>Finally, after processing by the fully connected (FC) layer and an activation function, the output is passed to the regression layer. This regression layer maps the final output of the neural network to the range required by the regression task, generating the predicted localization coordinates.</p></list-item></list></p><p>Step 3: Optimize the model by using sand cat swarm optimization algorithm</p><p>The specific steps for optimizing the model using the SCSO algorithm are as follows:<list list-type="order"><list-item><p>Set initial parameters: initial population size is set to 20, and maximum evolutionary generation is set to 20. The parameters to be optimized include learning rate, number of neurons in BIGRU, attention mechanism key value, and convolution kernel size. The initial learning rate is set to 0.005, and the upper and lower bounds are 0.01 and 0.001, respectively. The initial number of neurons in BIGRU is set to 35, and the upper and lower bounds are 50 and 10, respectively. The initial attention mechanism key value is set to 30, and the upper and lower bounds are 50 and 2, respectively. Initial convolution kernel size is set to 3, and the upper and lower bounds are 10 and 2, respectively.</p></list-item><list-item><p>Calculate the fitness of each individual in the population;</p></list-item><list-item><p>Record the current best individual, select the individual with higher fitness for evolution, and generate the next generation;</p></list-item><list-item><p>Renew the population by replacing poorly performing individuals with newly generated ones, ensuring gradual convergence to the optimal solution;</p></list-item><list-item><p>Iterate steps 2&#x02013;4 until the termination condition is met.</p></list-item><list-item><p>Retrieve the best parameters and output the optimal solution.</p></list-item></list></p><p>Step 4: Prediction of impact point</p><p>The impact samples from the test set were fed into the CNN-BIGRU-Attention model, which was optimized using the sand cat swarm optimization algorithm and the impact samples in the validation set. Subsequently, the model generates predicted coordinates for the impact points in the test set. Ultimately, the average coordinate error was calculated using the obtained results.</p><p>The formula for calculating the average distance error between predicted and actual locations is expressed as follows:<disp-formula id="FD25-sensors-25-01091"><label>(22)</label><mml:math id="mm91" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>&#x02211;</mml:mo></mml:mstyle><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:msubsup><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm92" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the actual abscissa value, <inline-formula><mml:math id="mm93" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the predicted abscissa value, <inline-formula><mml:math id="mm94" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the actual ordinate value, <inline-formula><mml:math id="mm95" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the predicted ordinate value, and n represents the number of predicted coordinates.</p></sec><sec id="sec5dot2-sensors-25-01091"><title>5.2. Analysis of Localization Results</title><p>In order to validate the reliability and precision of the impact localization method based on CNN-BIGRU-Attention model, <xref rid="sensors-25-01091-f013" ref-type="fig">Figure 13</xref> and <xref rid="sensors-25-01091-t001" ref-type="table">Table 1</xref> illustrate the impact localization results alongside prediction errors across various models. According to <xref rid="sensors-25-01091-t001" ref-type="table">Table 1</xref>, the maximum localization errors of CNN, CNN-GRU, CNN-BIGRU, and CNN-BIGRU-Attention models are 19.92 mm, 17.25 mm, 16.78 mm, and 11.82 mm, respectively. Correspondingly, their minimum localization errors based on the four models are 11.67 mm, 8.89 mm, 8.25 mm, and 5.01 mm. The average errors are 15.73 mm, 12.66 mm, 11.30 mm, and 8.03 mm, respectively. The results collectively demonstrate that the CNN-BIGRU-Attention model-based impact localization method outperforms in terms of localization accuracy. Additionally, as the structural complexity of the model rises, the average running time for impact localization using CNN, CNN-GRU, CNN-BIGRU, and CNN-BIGLU Attention models is 2.659 s, 3.253 s, 3.701 s, and 4.426 s, respectively. Compared with the localization results of impact on CFRP plate structures using FBG sensors, the localization accuracy of this system based on EFPI sensors is also higher [<xref rid="B42-sensors-25-01091" ref-type="bibr">42</xref>].</p></sec></sec><sec sec-type="conclusions" id="sec6-sensors-25-01091"><title>6. Conclusions</title><p>In this paper, an impact localization system for CFRP structures by using EFPI sensors demodulated at high speed was established, and an impact localization method based on CNN-BIGRU-Attention model for the CFRP structure is proposed. Firstly, the low-frequency narrowband signal component of the impact signal generated by Lamb waves is extracted using wavelet packet decomposition to mitigate dispersion effects. Then, an endpoint detection method based on the energy&#x02013;entropy ratio algorithm is used to extract the time at which the impact signal reaches each EFPI sensor and verify its sensitivity. Secondly, the arrival time differences in impact signals detected by different EFPI sensors were collected and used as features to train and optimize the CNN-BIGRU-Attention model for impact localization. Finally, the impact localization methods based on different models were validated on a 400 mm &#x000d7; 400 mm &#x000d7; 3 mm CFRP plate. By comparing the prediction results of different models, the proposed impact localization method for the CFRP structure based on CNN-BIGRU-Attention was proved to achieve superior localization accuracy.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.Y. and S.W.; methodology, J.Y.; software, Z.P.; validation, Z.P.; formal analysis, J.L.; investigation, L.G.; resources, J.Y., Y.B. and S.W.; data curation, Z.P.; writing&#x02014;original draft preparation, J.Y. and Z.P.; writing&#x02014;review and editing, J.Y. and Y.B.; visualization, Z.P.; supervision, J.Y. and S.W.; project administration, J.Y.; funding acquisition, J.Y. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data are available from the corresponding author on reasonable request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01091"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Collinson</surname><given-names>M.</given-names></name>
<name><surname>Swait</surname><given-names>T.</given-names></name>
<name><surname>Bower</surname><given-names>M.</given-names></name>
<name><surname>Nuhiji</surname><given-names>B.</given-names></name>
<name><surname>Hayes</surname><given-names>S.</given-names></name>
</person-group><article-title>Development and implementation of direct electric cure of plain weave CFRP composites for aerospace</article-title><source>Compos. Part A Appl. Sci. Manuf.</source><year>2023</year><volume>172</volume><fpage>107615</fpage><pub-id pub-id-type="doi">10.1016/j.compositesa.2023.107615</pub-id></element-citation></ref><ref id="B2-sensors-25-01091"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ding</surname><given-names>G.</given-names></name>
<name><surname>Hou</surname><given-names>S.</given-names></name>
</person-group><article-title>CFRP drive shaft damage identification and localization based on FBG sensing network and GWO-BP neural networks</article-title><source>Opt. Fiber Technol.</source><year>2023</year><volume>82</volume><fpage>103631</fpage><pub-id pub-id-type="doi">10.1016/j.yofte.2023.103631</pub-id></element-citation></ref><ref id="B3-sensors-25-01091"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>L.</given-names></name>
<name><surname>Saito</surname><given-names>O.</given-names></name>
<name><surname>Yu</surname><given-names>F.</given-names></name>
<name><surname>Okabe</surname><given-names>Y.</given-names></name>
<name><surname>Kondoh</surname><given-names>T.</given-names></name>
<name><surname>Tezuka</surname><given-names>S.</given-names></name>
<name><surname>Chiba</surname><given-names>A.</given-names></name>
</person-group><article-title>Impact damage detection using chirp ultrasonic guided waves for development of health monitoring system for cfrp mobility structures</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>789</elocation-id><pub-id pub-id-type="doi">10.3390/s22030789</pub-id><pub-id pub-id-type="pmid">35161538</pub-id>
</element-citation></ref><ref id="B4-sensors-25-01091"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>I.Y.</given-names></name>
<name><surname>Jang</surname><given-names>J.</given-names></name>
<name><surname>Park</surname><given-names>Y.B.</given-names></name>
</person-group><article-title>Advanced structural health monitoring in carbon fiber-reinforced plastic using real-time self-sensing data and convolutional neural network architectures</article-title><source>Mater. Des.</source><year>2022</year><volume>224</volume><fpage>111348</fpage><pub-id pub-id-type="doi">10.1016/j.matdes.2022.111348</pub-id></element-citation></ref><ref id="B5-sensors-25-01091"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ding</surname><given-names>G.</given-names></name>
<name><surname>Chang</surname><given-names>L.</given-names></name>
<name><surname>Zhou</surname><given-names>N.</given-names></name>
<name><surname>Gao</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>M.</given-names></name>
</person-group><article-title>Localization of low velocity impact on CFRP laminate using normalized error outlier-based algorithm cooperating with Db3-wavelet threshold noise reduction and FBG sensors</article-title><source>Opt. Fiber Technol.</source><year>2023</year><volume>80</volume><fpage>103455</fpage><pub-id pub-id-type="doi">10.1016/j.yofte.2023.103455</pub-id></element-citation></ref><ref id="B6-sensors-25-01091"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yan</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>D.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Qing</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
</person-group><article-title>A low-velocity impact localization method for composite stiffened plate based on AIC and WPT</article-title><source>IEEE Sens. J.</source><year>2024</year><volume>24</volume><fpage>10993</fpage><lpage>11002</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2024.3368563</pub-id></element-citation></ref><ref id="B7-sensors-25-01091"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>F.</given-names></name>
<name><surname>Liu</surname><given-names>M.</given-names></name>
<name><surname>Xiao</surname><given-names>W.</given-names></name>
</person-group><article-title>A two-step localization method using wavelet packet energy characteristics for low-velocity im-pacts on composite plate structures</article-title><source>Mech. Syst. Signal Process.</source><year>2023</year><volume>188</volume><fpage>110061</fpage><pub-id pub-id-type="doi">10.1016/j.ymssp.2022.110061</pub-id></element-citation></ref><ref id="B8-sensors-25-01091"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Shenoy</surname><given-names>S.</given-names></name>
<name><surname>Sharma</surname><given-names>R.S.</given-names></name>
</person-group><article-title>Impact Damage Estimation and Localization in Composite Sandwich Plates Using Deep Learning</article-title><source>Proceedings of the 2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)</source><conf-loc>Hamburg, Germany</conf-loc><conf-date>7&#x02013;8 October 2023</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2023</year><fpage>14</fpage><lpage>20</lpage></element-citation></ref><ref id="B9-sensors-25-01091"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Peng</surname><given-names>G.</given-names></name>
<name><surname>Yuan</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Cheng</surname><given-names>F.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
</person-group><article-title>Robust data-driven structural impact localization with multi-sensor real-time monitoring</article-title><source>IEEE Sens. J.</source><year>2023</year><volume>23</volume><fpage>1644</fpage><lpage>1654</lpage></element-citation></ref><ref id="B10-sensors-25-01091"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Tian</surname><given-names>K.</given-names></name>
<name><surname>Rui</surname><given-names>X.</given-names></name>
<name><surname>Xu</surname><given-names>L.</given-names></name>
<name><surname>Qi</surname><given-names>L.</given-names></name>
<name><surname>Feng</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>W.</given-names></name>
</person-group><article-title>Impact localization in anisotropic composites with time difference matrix matching and edge accuracy optimization</article-title><source>IEEE Trans. Instrum. Meas.</source><year>2023</year><volume>72</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1109/TIM.2023.3269113</pub-id><pub-id pub-id-type="pmid">37323850</pub-id>
</element-citation></ref><ref id="B11-sensors-25-01091"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jang</surname><given-names>B.W.</given-names></name>
<name><surname>Kim</surname><given-names>C.G.</given-names></name>
</person-group><article-title>Acoustic emission source localization in composite stiffened plate using triangulation method with signal magnitudes and arrival times</article-title><source>Adv. Compos. Mater.</source><year>2021</year><volume>30</volume><fpage>149</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1080/09243046.2020.1786903</pub-id></element-citation></ref><ref id="B12-sensors-25-01091"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>J.</given-names></name>
<name><surname>Zeng</surname><given-names>J.</given-names></name>
<name><surname>Bai</surname><given-names>Y.</given-names></name>
<name><surname>Cheng</surname><given-names>Z.</given-names></name>
<name><surname>Feng</surname><given-names>Z.</given-names></name>
<name><surname>Qi</surname><given-names>L.</given-names></name>
<name><surname>Liang</surname><given-names>D.</given-names></name>
</person-group><article-title>Layout optimization of fiber Bragg grating strain sensor network based on modified artificial fish swarm algorithm</article-title><source>Opt. Fiber Technol.</source><year>2021</year><volume>65</volume><fpage>102583</fpage><pub-id pub-id-type="doi">10.1016/j.yofte.2021.102583</pub-id></element-citation></ref><ref id="B13-sensors-25-01091"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fan</surname><given-names>L.</given-names></name>
<name><surname>Le</surname><given-names>K.</given-names></name>
<name><surname>Guo</surname><given-names>C.</given-names></name>
<name><surname>Sun</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>G.</given-names></name>
</person-group><article-title>Magnet-assisted hybrid EFPI/FBG sensor for internal corrosion monitoring of steel pipelines</article-title><source>Opt. Fiber Technol.</source><year>2022</year><volume>73</volume><fpage>103064</fpage><pub-id pub-id-type="doi">10.1016/j.yofte.2022.103064</pub-id></element-citation></ref><ref id="B14-sensors-25-01091"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Jing</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Li</surname><given-names>A.</given-names></name>
<name><surname>Lee</surname><given-names>A.</given-names></name>
<name><surname>Cheung</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Peng</surname><given-names>W.</given-names></name>
</person-group><article-title>All-silica fiber-optic temperature-depth-salinity sensor based on cascaded EFPIs and FBG for deep sea exploration</article-title><source>Opt. Express</source><year>2021</year><volume>29</volume><fpage>23953</fpage><lpage>23966</lpage><pub-id pub-id-type="doi">10.1364/OE.432943</pub-id><pub-id pub-id-type="pmid">34614649</pub-id>
</element-citation></ref><ref id="B15-sensors-25-01091"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qiu</surname><given-names>H.</given-names></name>
<name><surname>Yao</surname><given-names>Y.</given-names></name>
<name><surname>Dong</surname><given-names>Y.</given-names></name>
<name><surname>Tian</surname><given-names>J.</given-names></name>
</person-group><article-title>Fiber-optic immunosensor based on a Fabry&#x02013;Perot interferometer for single-molecule detection of biomarkers</article-title><source>Biosens. Bioelectron.</source><year>2024</year><volume>255</volume><elocation-id>116265</elocation-id><pub-id pub-id-type="doi">10.1016/j.bios.2024.116265</pub-id><pub-id pub-id-type="pmid">38569251</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01091"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Guan</surname><given-names>C.</given-names></name>
<name><surname>Qin</surname><given-names>H.</given-names></name>
<name><surname>Huang</surname><given-names>J.</given-names></name>
<name><surname>Chu</surname><given-names>W.</given-names></name>
<name><surname>Chai</surname><given-names>S.</given-names></name>
<name><surname>Lv</surname><given-names>P.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Tong</surname><given-names>Y.</given-names></name>
</person-group><article-title>Three-dimensional sound source localization system based on fiber optic sensor array with an adaptive algorithm</article-title><source>Opt. Commun.</source><year>2024</year><volume>559</volume><fpage>130383</fpage><pub-id pub-id-type="doi">10.1016/j.optcom.2024.130383</pub-id></element-citation></ref><ref id="B17-sensors-25-01091"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Ai</surname><given-names>F.</given-names></name>
<name><surname>Sun</surname><given-names>Q.</given-names></name>
<name><surname>Liu</surname><given-names>T.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Yan</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>D.</given-names></name>
</person-group><article-title>Diaphragm-based optical fiber sensor array for multipoint acoustic detection</article-title><source>Opt. Express</source><year>2018</year><volume>26</volume><fpage>25293</fpage><lpage>25304</lpage><pub-id pub-id-type="doi">10.1364/OE.26.025293</pub-id><pub-id pub-id-type="pmid">30469632</pub-id>
</element-citation></ref><ref id="B18-sensors-25-01091"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Jing</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>A.</given-names></name>
<name><surname>Xia</surname><given-names>Z.</given-names></name>
<name><surname>Peng</surname><given-names>W.</given-names></name>
</person-group><article-title>Multiplexing fiber-optic Fabry&#x02013;Perot acoustic sensors using self-calibrating wavelength shifting interferometry</article-title><source>Opt. Express</source><year>2019</year><volume>27</volume><fpage>38191</fpage><lpage>38203</lpage><pub-id pub-id-type="doi">10.1364/OE.381197</pub-id><pub-id pub-id-type="pmid">31878590</pub-id>
</element-citation></ref><ref id="B19-sensors-25-01091"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Wan</surname><given-names>S.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>B.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Zhong</surname><given-names>H.</given-names></name>
<name><surname>Shi</surname><given-names>J.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>X.</given-names></name>
<name><surname>Wu</surname><given-names>Q.</given-names></name>
</person-group><article-title>2D sound source localization technology based on diaphragm EFPI fiber microphone array</article-title><source>Opt. Commun.</source><year>2022</year><volume>519</volume><fpage>128435</fpage><pub-id pub-id-type="doi">10.1016/j.optcom.2022.128435</pub-id></element-citation></ref><ref id="B20-sensors-25-01091"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zaw</surname><given-names>T.H.</given-names></name>
<name><surname>War</surname><given-names>N.</given-names></name>
</person-group><article-title>The combination of spectral entropy, zero crossing rate, short time energy and linear prediction error for voice activity detection</article-title><source>Proceedings of the 2017 20th International Conference of Computer and Information Technology (ICCIT)</source><conf-loc>Dhaka, Bangladesh</conf-loc><conf-date>22&#x02013;24 December 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2017</year><fpage>1</fpage><lpage>5</lpage></element-citation></ref><ref id="B21-sensors-25-01091"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>T.</given-names></name>
<name><surname>Shao</surname><given-names>Y.</given-names></name>
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Geng</surname><given-names>Y.</given-names></name>
<name><surname>Fan</surname><given-names>L.</given-names></name>
</person-group><article-title>An overview of speech endpoint detection algorithms</article-title><source>Appl. Acoust.</source><year>2019</year><volume>160</volume><fpage>107133</fpage><pub-id pub-id-type="doi">10.1016/j.apacoust.2019.107133</pub-id></element-citation></ref><ref id="B22-sensors-25-01091"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xiuquan</surname><given-names>L.</given-names></name>
<name><surname>Zhen</surname><given-names>W.</given-names></name>
<name><surname>Yeyin</surname><given-names>J.</given-names></name>
<name><surname>Jing</surname><given-names>C.</given-names></name>
<name><surname>Zhenfei</surname><given-names>L.</given-names></name>
</person-group><article-title>Acoustic modulation signal recognition based on endpoint detection</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>19198</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-69934-y</pub-id><pub-id pub-id-type="pmid">39160259</pub-id>
</element-citation></ref><ref id="B23-sensors-25-01091"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nguyen</surname><given-names>T.Q.</given-names></name>
<name><surname>Vu</surname><given-names>T.B.</given-names></name>
<name><surname>Shafiabady</surname><given-names>N.</given-names></name>
<name><surname>Nguyen</surname><given-names>T.T.</given-names></name>
<name><surname>Nguyen</surname><given-names>P.T.</given-names></name>
</person-group><article-title>Real-time structural health monitoring of bridges using convolutional neural net-work-based loss factor analysis for enhanced energy dissipation detection</article-title><source>Structures</source><year>2024</year><volume>70</volume><fpage>107733</fpage><pub-id pub-id-type="doi">10.1016/j.istruc.2024.107733</pub-id></element-citation></ref><ref id="B24-sensors-25-01091"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alzubaidi</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Humaidi</surname><given-names>A.J.</given-names></name>
<name><surname>Al-Dujaili</surname><given-names>A.</given-names></name>
<name><surname>Duan</surname><given-names>Y.</given-names></name>
<name><surname>Al-Shamma</surname><given-names>O.</given-names></name>
<name><surname>Santamar&#x000ed;a</surname><given-names>J.</given-names></name>
<name><surname>Fadhel</surname><given-names>M.A.</given-names></name>
<name><surname>Al-Amidie</surname><given-names>M.</given-names></name>
<name><surname>Farhan</surname><given-names>L.</given-names></name>
</person-group><article-title>Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions</article-title><source>J. Big Data</source><year>2021</year><volume>8</volume><fpage>53</fpage><pub-id pub-id-type="doi">10.1186/s40537-021-00444-8</pub-id><pub-id pub-id-type="pmid">33816053</pub-id>
</element-citation></ref><ref id="B25-sensors-25-01091"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Stankovi&#x00107;</surname><given-names>L.</given-names></name>
<name><surname>Mandic</surname><given-names>D.</given-names></name>
</person-group><article-title>Convolutional neural networks demystified: A matched filtering perspective-based tutorial</article-title><source>IEEE Trans. Syst. Man Cybern. Syst.</source><year>2023</year><volume>53</volume><fpage>3614</fpage><lpage>3628</lpage><pub-id pub-id-type="doi">10.1109/TSMC.2022.3228597</pub-id></element-citation></ref><ref id="B26-sensors-25-01091"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jie</surname><given-names>H.J.</given-names></name>
<name><surname>Wanda</surname><given-names>P.</given-names></name>
</person-group><article-title>RunPool: A dynamic pooling layer for convolution neural network</article-title><source>Int. J. Comput. Intell. Syst.</source><year>2020</year><volume>13</volume><fpage>66</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.2991/ijcis.d.200120.002</pub-id></element-citation></ref><ref id="B27-sensors-25-01091"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mahjoub</surname><given-names>S.</given-names></name>
<name><surname>Chrifi-Alaoui</surname><given-names>L.</given-names></name>
<name><surname>Marhic</surname><given-names>B.</given-names></name>
<name><surname>Delahoche</surname><given-names>L.</given-names></name>
</person-group><article-title>Predicting energy consumption using LSTM, multi-layer GRU and drop-GRU neural networks</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>4062</elocation-id><pub-id pub-id-type="doi">10.3390/s22114062</pub-id><pub-id pub-id-type="pmid">35684681</pub-id>
</element-citation></ref><ref id="B28-sensors-25-01091"><label>28.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>S.</given-names></name>
<name><surname>Yu</surname><given-names>X.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
</person-group><article-title>Lstm and gru neural network performance comparison study: Taking yelp review dataset as an ex-ample</article-title><source>Proceedings of the 2020 International Workshop on Electronic Communication and Artificial Intelligence (IWECAI)</source><conf-loc>Shanghai, China</conf-loc><conf-date>12&#x02013;14 June 2020</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>Piscataway, NJ, USA</publisher-loc><year>2020</year><fpage>98</fpage><lpage>101</lpage></element-citation></ref><ref id="B29-sensors-25-01091"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>S.</given-names></name>
<name><surname>Shi</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>W.</given-names></name>
<name><surname>Yin</surname><given-names>Q.</given-names></name>
</person-group><article-title>High and low frequency wind power prediction based on Transformer and BiGRU-Attention</article-title><source>Energy</source><year>2024</year><volume>288</volume><fpage>129753</fpage><pub-id pub-id-type="doi">10.1016/j.energy.2023.129753</pub-id></element-citation></ref><ref id="B30-sensors-25-01091"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>X.</given-names></name>
<name><surname>Quan</surname><given-names>Z.</given-names></name>
<name><surname>Wang</surname><given-names>Z.-J.</given-names></name>
<name><surname>Huang</surname><given-names>H.</given-names></name>
<name><surname>Zeng</surname><given-names>X.</given-names></name>
</person-group><article-title>A novel molecular representation with BiGRU neural networks for learning atom</article-title><source>Briefings Bioinform.</source><year>2019</year><volume>21</volume><fpage>2099</fpage><lpage>2111</lpage><pub-id pub-id-type="doi">10.1093/bib/bbz125</pub-id><pub-id pub-id-type="pmid">31729524</pub-id>
</element-citation></ref><ref id="B31-sensors-25-01091"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bao</surname><given-names>K.</given-names></name>
<name><surname>Bi</surname><given-names>J.</given-names></name>
<name><surname>Ma</surname><given-names>R.</given-names></name>
<name><surname>Sun</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>W.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
</person-group><article-title>A spatial-reduction attention-based BiGRU network for water level prediction</article-title><source>Water</source><year>2023</year><volume>15</volume><elocation-id>1306</elocation-id><pub-id pub-id-type="doi">10.3390/w15071306</pub-id></element-citation></ref><ref id="B32-sensors-25-01091"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>H.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Kim</surname><given-names>C.</given-names></name>
</person-group><article-title>Semantic and instance segmentation in coastal urban spatial perception: A multi-task learning framework with an attention mechanism</article-title><source>Sustainability</source><year>2024</year><volume>16</volume><elocation-id>833</elocation-id><pub-id pub-id-type="doi">10.3390/su16020833</pub-id></element-citation></ref><ref id="B33-sensors-25-01091"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hassanin</surname><given-names>M.</given-names></name>
<name><surname>Anwar</surname><given-names>S.</given-names></name>
<name><surname>Radwan</surname><given-names>I.</given-names></name>
<name><surname>Khan</surname><given-names>F.S.</given-names></name>
<name><surname>Mian</surname><given-names>A.</given-names></name>
</person-group><article-title>Visual attention methods in deep learning: An in-depth survey</article-title><source>Inf. Fusion</source><year>2024</year><volume>108</volume><fpage>102417</fpage><pub-id pub-id-type="doi">10.1016/j.inffus.2024.102417</pub-id></element-citation></ref><ref id="B34-sensors-25-01091"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Pu</surname><given-names>J.</given-names></name>
<name><surname>Miao</surname><given-names>D.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Du</surname><given-names>X.</given-names></name>
</person-group><article-title>SCGRFuse: An infrared and visible image fusion network based on spatial/channel attention mechanism and gradient aggregation residual dense blocks</article-title><source>Eng. Appl. Artif. Intell.</source><year>2024</year><volume>132</volume><fpage>107898</fpage><pub-id pub-id-type="doi">10.1016/j.engappai.2024.107898</pub-id></element-citation></ref><ref id="B35-sensors-25-01091"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>H.W.</given-names></name>
<name><surname>Qin</surname><given-names>W.</given-names></name>
<name><surname>Sun</surname><given-names>Y.N.</given-names></name>
<name><surname>Lv</surname><given-names>Y.L.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
</person-group><article-title>Attention mechanism-based deep learning for heat load prediction in blast furnace ironmaking process</article-title><source>J. Intell. Manuf.</source><year>2024</year><volume>35</volume><fpage>1207</fpage><lpage>1220</lpage><pub-id pub-id-type="doi">10.1007/s10845-023-02106-3</pub-id></element-citation></ref><ref id="B36-sensors-25-01091"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>M.</given-names></name>
<name><surname>Yu</surname><given-names>X.</given-names></name>
<name><surname>Tan</surname><given-names>H.</given-names></name>
<name><surname>Yuan</surname><given-names>J.</given-names></name>
<name><surname>Chen</surname><given-names>K.</given-names></name>
<name><surname>Xie</surname><given-names>S.</given-names></name>
<name><surname>Han</surname><given-names>Y.</given-names></name>
<name><surname>Long</surname><given-names>W.</given-names></name>
</person-group><article-title>High-precision monitoring and prediction of mining area surface subsidence using SBAS-InSAR and CNN-BiGRU-attention model</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>28968</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-024-80446-7</pub-id><pub-id pub-id-type="pmid">39578598</pub-id>
</element-citation></ref><ref id="B37-sensors-25-01091"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Niu</surname><given-names>D.</given-names></name>
<name><surname>Yu</surname><given-names>M.</given-names></name>
<name><surname>Sun</surname><given-names>L.</given-names></name>
<name><surname>Gao</surname><given-names>T.</given-names></name>
<name><surname>Wang</surname><given-names>K.</given-names></name>
</person-group><article-title>Short-term multi-energy load forecasting for integrated energy systems based on CNN-BiGRU op-timized by attention mechanism</article-title><source>Appl. Energy</source><year>2022</year><volume>313</volume><fpage>118801</fpage><pub-id pub-id-type="doi">10.1016/j.apenergy.2022.118801</pub-id></element-citation></ref><ref id="B38-sensors-25-01091"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Adegboye</surname><given-names>O.R.</given-names></name>
<name><surname>Feda</surname><given-names>A.K.</given-names></name>
<name><surname>Ojekemi</surname><given-names>O.R.</given-names></name>
<name><surname>Agyekum</surname><given-names>E.B.</given-names></name>
<name><surname>Khan</surname><given-names>B.</given-names></name>
<name><surname>Kamel</surname><given-names>S.</given-names></name>
</person-group><article-title>DGS-SCSO: Enhancing sand cat swarm optimization with dynamic pinhole imaging and golden sine algorithm for improved numerical optimization performance</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>1491</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-50910-x</pub-id><pub-id pub-id-type="pmid">38233528</pub-id>
</element-citation></ref><ref id="B39-sensors-25-01091"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Seyyedabbasi</surname><given-names>A.</given-names></name>
</person-group><article-title>Binary sand cat swarm optimization algorithm for wrapper feature selection on biological data</article-title><source>Biomimetics</source><year>2023</year><volume>8</volume><elocation-id>310</elocation-id><pub-id pub-id-type="doi">10.3390/biomimetics8030310</pub-id><pub-id pub-id-type="pmid">37504198</pub-id>
</element-citation></ref><ref id="B40-sensors-25-01091"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pfeifer</surname><given-names>H.</given-names></name>
<name><surname>Ratschbacher</surname><given-names>L.</given-names></name>
<name><surname>Gallego</surname><given-names>J.</given-names></name>
<name><surname>Saavedra</surname><given-names>C.</given-names></name>
<name><surname>Fa&#x000df;bender</surname><given-names>A.</given-names></name>
<name><surname>von Haaren</surname><given-names>A.</given-names></name>
<name><surname>Alt</surname><given-names>W.</given-names></name>
<name><surname>Hofferberth</surname><given-names>S.</given-names></name>
<name><surname>K&#x000f6;hl</surname><given-names>M.</given-names></name>
<name><surname>Linden</surname><given-names>S.</given-names></name>
<etal/>
</person-group><article-title>Achievements and perspectives of optical fiber Fabry&#x02013;Perot cavities</article-title><source>Appl. Phys. B Laser Opt.</source><year>2022</year><volume>128</volume><fpage>29</fpage><pub-id pub-id-type="doi">10.1007/s00340-022-07752-8</pub-id></element-citation></ref><ref id="B41-sensors-25-01091"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>G.</given-names></name>
<name><surname>Hu</surname><given-names>H.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Liu</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>K.</given-names></name>
</person-group><article-title>Localization of impact on composite plates based on integrated wavelet transform and hybrid minimization algorithm</article-title><source>Compos. Struct.</source><year>2017</year><volume>176</volume><fpage>234</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1016/j.compstruct.2017.05.017</pub-id></element-citation></ref><ref id="B42-sensors-25-01091"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>J.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Peng</surname><given-names>Z.</given-names></name>
<name><surname>Gan</surname><given-names>L.</given-names></name>
<name><surname>Wan</surname><given-names>S.</given-names></name>
</person-group><article-title>Localization of impact on CFRP structure based on fiber Bragg gratings and CNN-LSTM-Attention</article-title><source>Opt. Fiber Technol.</source><year>2024</year><volume>87</volume><fpage>103943</fpage><pub-id pub-id-type="doi">10.1016/j.yofte.2024.103943</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01091-f001"><label>Figure 1</label><caption><p>CNN structure schematic.</p></caption><graphic xlink:href="sensors-25-01091-g001" position="float"/></fig><fig position="float" id="sensors-25-01091-f002"><label>Figure 2</label><caption><p>Schematic diagram of pooling operations: (<bold>a</bold>) maximum pooling; (<bold>b</bold>) average pooling.</p></caption><graphic xlink:href="sensors-25-01091-g002" position="float"/></fig><fig position="float" id="sensors-25-01091-f003"><label>Figure 3</label><caption><p>BIGRU network structure model.</p></caption><graphic xlink:href="sensors-25-01091-g003" position="float"/></fig><fig position="float" id="sensors-25-01091-f004"><label>Figure 4</label><caption><p>Schematic diagram of F-P cavity interference.</p></caption><graphic xlink:href="sensors-25-01091-g004" position="float"/></fig><fig position="float" id="sensors-25-01091-f005"><label>Figure 5</label><caption><p>Structure diagram of EFPI sensor.</p></caption><graphic xlink:href="sensors-25-01091-g005" position="float"/></fig><fig position="float" id="sensors-25-01091-f006"><label>Figure 6</label><caption><p>EFPI system demodulated based on intensity method.</p></caption><graphic xlink:href="sensors-25-01091-g006" position="float"/></fig><fig position="float" id="sensors-25-01091-f007"><label>Figure 7</label><caption><p>Schematic diagram of impact monitoring area division.</p></caption><graphic xlink:href="sensors-25-01091-g007" position="float"/></fig><fig position="float" id="sensors-25-01091-f008"><label>Figure 8</label><caption><p>Impact localization system of CFRP structure based on EFPI sensors.</p></caption><graphic xlink:href="sensors-25-01091-g008" position="float"/></fig><fig position="float" id="sensors-25-01091-f009"><label>Figure 9</label><caption><p>Impact signals detected by different EFPI sensors.</p></caption><graphic xlink:href="sensors-25-01091-g009" position="float"/></fig><fig position="float" id="sensors-25-01091-f010"><label>Figure 10</label><caption><p>Impact signal and corresponding energy&#x02013;entropy ratio.</p></caption><graphic xlink:href="sensors-25-01091-g010" position="float"/></fig><fig position="float" id="sensors-25-01091-f011"><label>Figure 11</label><caption><p>Normalized time difference in impact signal arrival between different EFPI sensors. (<bold>a</bold>) Normalized time difference in impact signal arrival between EFPI 2 and EFPI 1. (<bold>b</bold>) Normalized time difference in impact signal arrival between EFPI 3 and EFPI 1. (<bold>c</bold>) Normalized time difference in impact signal arrival between EFPI 4 and EFPI 1.</p></caption><graphic xlink:href="sensors-25-01091-g011" position="float"/></fig><fig position="float" id="sensors-25-01091-f012"><label>Figure 12</label><caption><p>Flow chart of impact localization method based on CNN-BIGRU-Attention.</p></caption><graphic xlink:href="sensors-25-01091-g012" position="float"/></fig><fig position="float" id="sensors-25-01091-f013"><label>Figure 13</label><caption><p>(<bold>a</bold>) Impact localization results; (<bold>b</bold>) impact localization errors.</p></caption><graphic xlink:href="sensors-25-01091-g013" position="float"/></fig><table-wrap position="float" id="sensors-25-01091-t001"><object-id pub-id-type="pii">sensors-25-01091-t001_Table 1</object-id><label>Table 1</label><caption><p>Comparison of impact localization result.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Impact</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Impact Coordinate (mm)</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CNN Algorithm</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CNN-GRU Algorithm</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CNN-BIGRU Algorithm</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">CNN-BIGRU-Attention Algorithm</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Predict Coordinates (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Error (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Predict Coordinates (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Error (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Predict Coordinates (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Error (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Predict Coordinates (mm)</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Error (mm)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">(175, 150)</td><td align="center" valign="middle" rowspan="1" colspan="1">(157.106, 141.247)</td><td align="center" valign="middle" rowspan="1" colspan="1">19.920</td><td align="center" valign="middle" rowspan="1" colspan="1">(163.726, 159.847)</td><td align="center" valign="middle" rowspan="1" colspan="1">14.968</td><td align="center" valign="middle" rowspan="1" colspan="1">(185.262, 142.160)</td><td align="center" valign="middle" rowspan="1" colspan="1">12.914</td><td align="center" valign="middle" rowspan="1" colspan="1">(182.861, 152.650)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.296</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">2</td><td align="center" valign="middle" rowspan="1" colspan="1">(50, 200)</td><td align="center" valign="middle" rowspan="1" colspan="1">(38.865, 190.362)</td><td align="center" valign="middle" rowspan="1" colspan="1">14.727</td><td align="center" valign="middle" rowspan="1" colspan="1">(57.655, 193.975)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.741</td><td align="center" valign="middle" rowspan="1" colspan="1">(45.123, 206.653)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.249</td><td align="center" valign="middle" rowspan="1" colspan="1">(52.680, 205.027)</td><td align="center" valign="middle" rowspan="1" colspan="1">5.696</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">3</td><td align="center" valign="middle" rowspan="1" colspan="1">(175, 325)</td><td align="center" valign="middle" rowspan="1" colspan="1">(159.364, 332.655)</td><td align="center" valign="middle" rowspan="1" colspan="1">17.410</td><td align="center" valign="middle" rowspan="1" colspan="1">(185.153, 321.375)</td><td align="center" valign="middle" rowspan="1" colspan="1">10.781</td><td align="center" valign="middle" rowspan="1" colspan="1">(183.257, 329.561)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.433</td><td align="center" valign="middle" rowspan="1" colspan="1">(168.109, 324.683)</td><td align="center" valign="middle" rowspan="1" colspan="1">6.899</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">4</td><td align="center" valign="middle" rowspan="1" colspan="1">(250,50)</td><td align="center" valign="middle" rowspan="1" colspan="1">(258.265, 41.756)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.674</td><td align="center" valign="middle" rowspan="1" colspan="1">(243.764, 42.925)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.431</td><td align="center" valign="middle" rowspan="1" colspan="1">(245.514, 60.133)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.081</td><td align="center" valign="middle" rowspan="1" colspan="1">(255.553, 56.919)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.871</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">5</td><td align="center" valign="middle" rowspan="1" colspan="1">(250,125)</td><td align="center" valign="middle" rowspan="1" colspan="1">(258.462, 110.036)</td><td align="center" valign="middle" rowspan="1" colspan="1">17.191</td><td align="center" valign="middle" rowspan="1" colspan="1">(240.236, 136.217)</td><td align="center" valign="middle" rowspan="1" colspan="1">14.871</td><td align="center" valign="middle" rowspan="1" colspan="1">(242.995, 116.674)</td><td align="center" valign="middle" rowspan="1" colspan="1">10.881</td><td align="center" valign="middle" rowspan="1" colspan="1">(248.995, 135.334)</td><td align="center" valign="middle" rowspan="1" colspan="1">10.383</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">6</td><td align="center" valign="middle" rowspan="1" colspan="1">(75,100)</td><td align="center" valign="middle" rowspan="1" colspan="1">(64.888, 91.546)</td><td align="center" valign="middle" rowspan="1" colspan="1">13.180</td><td align="center" valign="middle" rowspan="1" colspan="1">(81.236, 109.133)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.059</td><td align="center" valign="middle" rowspan="1" colspan="1">(70.053, 108.264)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.631</td><td align="center" valign="middle" rowspan="1" colspan="1">(76.103, 93.481)</td><td align="center" valign="middle" rowspan="1" colspan="1">6.611</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">7</td><td align="center" valign="middle" rowspan="1" colspan="1">(300,175)</td><td align="center" valign="middle" rowspan="1" colspan="1">(313.763, 187.137)</td><td align="center" valign="middle" rowspan="1" colspan="1">18.350</td><td align="center" valign="middle" rowspan="1" colspan="1">(293.237, 160.783)</td><td align="center" valign="middle" rowspan="1" colspan="1">15.744</td><td align="center" valign="middle" rowspan="1" colspan="1">(303.265, 164.797)</td><td align="center" valign="middle" rowspan="1" colspan="1">10.713</td><td align="center" valign="middle" rowspan="1" colspan="1">(297.923, 183.815)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.057</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">8</td><td align="center" valign="middle" rowspan="1" colspan="1">(50,300)</td><td align="center" valign="middle" rowspan="1" colspan="1">(54.160, 285.846)</td><td align="center" valign="middle" rowspan="1" colspan="1">14.752</td><td align="center" valign="middle" rowspan="1" colspan="1">(39.237, 286.522)</td><td align="center" valign="middle" rowspan="1" colspan="1">17.249</td><td align="center" valign="middle" rowspan="1" colspan="1">(61.236, 287.543)</td><td align="center" valign="middle" rowspan="1" colspan="1">16.776</td><td align="center" valign="middle" rowspan="1" colspan="1">(55.365, 307.191)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.972</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">9</td><td align="center" valign="middle" rowspan="1" colspan="1">(75,350)</td><td align="center" valign="middle" rowspan="1" colspan="1">(63.196, 344.456)</td><td align="center" valign="middle" rowspan="1" colspan="1">13.041</td><td align="center" valign="middle" rowspan="1" colspan="1">(71.736, 341.729)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.891</td><td align="center" valign="middle" rowspan="1" colspan="1">(82.154, 341.133)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.393</td><td align="center" valign="middle" rowspan="1" colspan="1">(77.321, 354.439)</td><td align="center" valign="middle" rowspan="1" colspan="1">5.009</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">10</td><td align="center" valign="middle" rowspan="1" colspan="1">(225, 250)</td><td align="center" valign="middle" rowspan="1" colspan="1">(213.654, 261.861)</td><td align="center" valign="middle" rowspan="1" colspan="1">16.414</td><td align="center" valign="middle" rowspan="1" colspan="1">(220.237, 235.165)</td><td align="center" valign="middle" rowspan="1" colspan="1">15.581</td><td align="center" valign="middle" rowspan="1" colspan="1">(214.824, 242.237)</td><td align="center" valign="middle" rowspan="1" colspan="1">12.800</td><td align="center" valign="middle" rowspan="1" colspan="1">(234.860, 243.484)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.819</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">11</td><td align="center" valign="middle" rowspan="1" colspan="1">(125,50)</td><td align="center" valign="middle" rowspan="1" colspan="1">(135.985, 57.841)</td><td align="center" valign="middle" rowspan="1" colspan="1">13.497</td><td align="center" valign="middle" rowspan="1" colspan="1">(131.137, 40.137)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.532</td><td align="center" valign="middle" rowspan="1" colspan="1">(120.232, 41.279)</td><td align="center" valign="middle" rowspan="1" colspan="1">9.940</td><td align="center" valign="middle" rowspan="1" colspan="1">(121.593, 56.255)</td><td align="center" valign="middle" rowspan="1" colspan="1">7.122</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">12</td><td align="center" valign="middle" rowspan="1" colspan="1">(325,75)</td><td align="center" valign="middle" rowspan="1" colspan="1">(333.824, 57.924)</td><td align="center" valign="middle" rowspan="1" colspan="1">19.221</td><td align="center" valign="middle" rowspan="1" colspan="1">(330.863, 87.116)</td><td align="center" valign="middle" rowspan="1" colspan="1">13.460</td><td align="center" valign="middle" rowspan="1" colspan="1">(317.265, 83.198)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.271</td><td align="center" valign="middle" rowspan="1" colspan="1">(319.220, 69.517)</td><td align="center" valign="middle" rowspan="1" colspan="1">7.966</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">13</td><td align="center" valign="middle" rowspan="1" colspan="1">(100, 175)</td><td align="center" valign="middle" rowspan="1" colspan="1">(110.541, 163.388)</td><td align="center" valign="middle" rowspan="1" colspan="1">15.683</td><td align="center" valign="middle" rowspan="1" colspan="1">(93.133, 184.896)</td><td align="center" valign="middle" rowspan="1" colspan="1">12.045</td><td align="center" valign="middle" rowspan="1" colspan="1">(105.270, 185.091)</td><td align="center" valign="middle" rowspan="1" colspan="1">11.385</td><td align="center" valign="middle" rowspan="1" colspan="1">(93.431, 171.093)</td><td align="center" valign="middle" rowspan="1" colspan="1">7.643</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">14</td><td align="center" valign="middle" rowspan="1" colspan="1">(150, 225)</td><td align="center" valign="middle" rowspan="1" colspan="1">(138.256, 216.463)</td><td align="center" valign="middle" rowspan="1" colspan="1">14.519</td><td align="center" valign="middle" rowspan="1" colspan="1">(159.165, 233.467)</td><td align="center" valign="middle" rowspan="1" colspan="1">12.478</td><td align="center" valign="middle" rowspan="1" colspan="1">(156.880, 214.053)</td><td align="center" valign="middle" rowspan="1" colspan="1">12.930</td><td align="center" valign="middle" rowspan="1" colspan="1">(145.224, 231.053)</td><td align="center" valign="middle" rowspan="1" colspan="1">7.710</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">15</td><td align="center" valign="middle" rowspan="1" colspan="1">(300, 300)</td><td align="center" valign="middle" rowspan="1" colspan="1">(310.563, 287.538)</td><td align="center" valign="middle" rowspan="1" colspan="1">16.336</td><td align="center" valign="middle" rowspan="1" colspan="1">(309.169, 307.790)</td><td align="center" valign="middle" rowspan="1" colspan="1">12.031</td><td align="center" valign="middle" rowspan="1" colspan="1">(292.685, 293.132)</td><td align="center" valign="middle" rowspan="1" colspan="1">10.034</td><td align="center" valign="middle" rowspan="1" colspan="1">(293.622, 305.496)</td><td align="center" valign="middle" rowspan="1" colspan="1">8.389</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Max. error</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">19.920</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">17.249</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">16.776</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">11.819</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Min. error</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">11.674</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">8.891</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">8.249</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">5.001</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Avg. error</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">15.728</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.656</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.295</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8.030</td></tr></tbody></table></table-wrap></floats-group></article>