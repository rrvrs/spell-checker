<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Med Oral Patol Oral Cir Bucal</journal-id><journal-id journal-id-type="iso-abbrev">Med Oral Patol Oral Cir Bucal</journal-id><journal-id journal-id-type="publisher-id">Medicina Oral S.L.</journal-id><journal-title-group><journal-title>Medicina Oral, Patolog&#x000ed;a Oral y Cirug&#x000ed;a Bucal</journal-title></journal-title-group><issn pub-type="ppub">1698-4447</issn><issn pub-type="epub">1698-6946</issn><publisher><publisher-name>Medicina Oral S.L.</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39954282</article-id><article-id pub-id-type="pmc">PMC11972646</article-id>
<article-id pub-id-type="publisher-id">26937</article-id><article-id pub-id-type="doi">10.4317/medoral.26937</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject><subj-group><subject>Periodontology</subject></subj-group></subj-group></article-categories><title-group><article-title>Intelligent biofilm detection with ensemble of deep learning networks</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sobrinho</surname><given-names>Brenda Pereira Pinheiro</given-names></name><xref rid="A1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Silva</surname><given-names>Bernardo Peters Menezes</given-names></name><xref rid="A2" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>de Andrade</surname><given-names>Katia Montanha</given-names></name><xref rid="A1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Sobrinho</surname><given-names>Bruna Pereira Pinheiro</given-names></name><xref rid="A3" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Ribeiro</surname><given-names>Daniel Araki</given-names></name><xref rid="A4" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><name><surname>dos Santos</surname><given-names>Jean Nunes</given-names></name><xref rid="A1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>de Oliveira</surname><given-names>Luciano Rebou&#x000e7;as</given-names></name><xref rid="A2" ref-type="aff">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Cury</surname><given-names>Patricia Ramos</given-names></name><xref rid="A1" ref-type="aff">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>PosGraduate Program in Dentistry and Health, School of Dentistry, Federal University of Bahia, Salvador, Brazil</aff><aff id="A2"><label>2</label>Intelligent Vision Research Lab, Institute of Computing, Federal University of Bahia, Salvador, Brazil</aff><aff id="A3"><label>3</label>Undergraduate Program in Dentistry and Health, School of Dentistry, Federal University of Bahia, Salvador, Brazil</aff><aff id="A4"><label>4</label>Department of Biosciences, Institute of Health and Society, Federal University of S&#x000e3;o Paulo, UNIFESP, Santos, SP, Brazil</aff><author-notes><corresp> Faculdade de Odontologia
Avenida Ara&#x000fa;jo Pinho, 62, Canela 
40110-150, Salvador/Bahia, Brazil
, E-mail: <email>patcury@yahoo.com; patricia.cury@ufba.br</email></corresp></author-notes><pub-date pub-type="ppub"><month>3</month><year>2025</year></pub-date><pub-date pub-type="epub"><day>15</day><month>2</month><year>2025</year></pub-date><volume>30</volume><issue>2</issue><fpage>e282</fpage><lpage>e287</lpage><history><date date-type="accepted"><day>7</day><month>1</month><year>2025</year></date><date date-type="received"><day>23</day><month>9</month><year>2024</year></date></history><permissions><copyright-statement>Copyright: &#x000a9; 2025 Medicina Oral S.L.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/2.5/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><sec><title>Background </title><p> Dental biofilm is traditionally identified visually, which can be challenging and time-consuming due to its color similarity with the tooth. The aim of this study was to evaluate the performance of U-Net neural networks for the automatic detection of dental biofilm without disclosing agents on intraoral photographs of deciduous and permanent teeth using an ensemble strategy.</p></sec><sec><title>Material and Methods </title><p> This retrospective exploratory study was conducted on two datasets of intraoral images obtained from deciduous and permanent dentitions. The first dataset was used to validate dental biofilm annotations by an expert with disclosing agents. The second dataset, without disclosing agents, was employed to train and evaluate the U-Net neural network in the identification of dental biofilms using an ensemble strategy.</p></sec><sec><title>Results </title><p> The performance of the ensemble method was assessed using a cross-validation procedure, with six groups dedicated to training, one group for validation, and one group exclusively taken as a test set for the final evaluation of the ensemble. The performance of the neural network was evaluated using accuracy, F1 score, sensitivity, and specificity. The U-Net neural network achieved an accuracy of 93.1%, sensitivity of 65.1%, specificity of 95.9%, and an F1 score of 63.0%.</p></sec><sec><title>Conclusions </title><p> The U-Net neural network using the ensemble strategy was able to automatically identify visually detecTable dental biofilms on intraoral photographs. The application of this new knowledge will soon be available in clinical practice.</p><p><bold> Key words:</bold>Artificial intelligence, dental biofilm, ensemble method, photography, preventive dentistry.</p></sec></abstract></article-meta></front><body><sec><title>Introduction</title><p>Dental surfaces are exposed to colonization by different microorganisms that form the dental biofilm, the main etiological factor of oral diseases such as dental caries, gingivitis, and periodontitis. However, detecting dental biofilm is difficult for both the patient and the dentist or hygienist because of its similar color to the tooth surface (<xref rid="B1" ref-type="bibr">1</xref>). Dental biofilms can be identified using a disclosing solution. Such solutions use dyes that modify the color of the biofilm so that it contrasts with the surface of the tooth (<xref rid="B2" ref-type="bibr">2</xref>). However, this procedure has disadvantages, such as the temporary staining of mucous membranes and restorations (<xref rid="B3" ref-type="bibr">3</xref>-<xref rid="B5" ref-type="bibr">5</xref>), as well as the unpleasant taste of the disclosing agent. Thus, further investigation is needed to develop a convenient method for the detection and quantification of dental biofilms.</p><p>Artificial intelligence (AI) has gained visibility in dentistry in recent years because it can be used to replicate dentists actions to perform tasks automatically (<xref rid="B6" ref-type="bibr">6</xref>). Machine learning (ML) is a sub-area of AI that uses algorithms to learn from data, adjusting its own internal parameters to automatically improve performance in specific tasks (<xref rid="B7" ref-type="bibr">7</xref>). Deep learning is a ML technique designed to automate tasks that focuses on the creation and training of deep neural network algorithms for complex data processing tasks and high-level pattern recognition and that can generate information such as image classification, detection, and segmentation (<xref rid="B8" ref-type="bibr">8</xref>). In dentistry, examples that highlight the application of deep learning are its use for automatic dental biofilm detection on intraoral photographs (<xref rid="B3" ref-type="bibr">3</xref>,<xref rid="B9" ref-type="bibr">9</xref>,<xref rid="B10" ref-type="bibr">10</xref>). However, the model&#x02019;s performance is related to the availability of large datasets for training the neural network, which can limit the effectiveness of the technique (<xref rid="B10" ref-type="bibr">10</xref>).</p><p>The ensemble strategy is used in AI to deal with challenges such as small sample sizes and irregular and discrepant distribution of classes and data (<xref rid="B11" ref-type="bibr">11</xref>). The neural network receives a labeled database as input and generates masks as output. Using the ensemble strategy, the average output score is calculated by the combination of multiple algorithms to produce more reliable and accurate results (<xref rid="B12" ref-type="bibr">12</xref>). By using the generated model, cohesive predictions can be made for new unlabeled examples (<xref rid="B13" ref-type="bibr">13</xref>). In short, several individual models are trained independently and their predictions are then combined to generate a final prediction. The aim of the present study was to assess the performance of U-Net neural networks in the automatic detection of dental biofilm without disclosing agents on intraoral photographs of deciduous and permanent teeth using an ensemble strategy.</p></sec><sec><title>Material and Methods</title><p>Study design and data collection</p><p>The present study is a retrospective exploratory study in which two databases of randomly chosen intraoral photographs were obtained from a private clinic in Salvador, Bahia, between 10 September 2019 and 6 March 2023. The project was approved by the Research Ethics Committee of the School of Dentistry, Federal University of Bahia (protocol number: 4.434.730), and was conducted in accordance with the Declaration of Helsinki. This study is described following the Checklist for Artificial Intelligence in Medical Imaging.</p><p>The initial dataset consisted of 96 intraoral photographs from 16 participants captured before and after the application of a dental biofilm disclosing agent (Eviplac Tablets, Biodinamicas&#x02122;). This dataset was used to assess the intraexaminer agreement in biofilm detection. Consistency of the examiner annotations was showed previously (<xref rid="B10" ref-type="bibr">10</xref>). The examiner&#x02019;s annotation was tested using the intraclass correlation coefficient (ICC), comparing the dental biofilm area on each tooth based on digital images without and with the disclosing agent. The ICC was 0.93 (95% CI, 0.92-0.94). The dental biofilm labeled by the dentist (without disclosing agent) was thus defined as the ground truth.</p><p>The second dataset, the experimental one, was composed of 480 intraoral images obtained without disclosing agents from 160 people with or without orthodontic appliances. This dataset was used to train and evaluate a neural network for segmentation of the dental biofilm. The database was randomly subdivided into eight groups: training (360 images), validation (60 images), and testing (60 images). The training and validation groups alternated, with each group taking turns being used for training and validation in a cross-validation process. The eighth group was used as a test subset with 60 images selected for the ensemble strategy (Fig. <xref rid="F1" ref-type="fig">1</xref>).</p><p>Eligibility criteria of the images were the absence of clinically visible caries or tetracycline stain, high-resolution and focused images, and frontal and lateral photographs (left and right) taken in the perpendicular plane in buccal views. Low-light images and images containing saliva bubbles or blurred areas were excluded from this study. Furthermore, the patient identification was removed from all captured images and each participant was identified by randomly assigning letters.</p><p>During image acquisition, each participant used a plastic lip retractor. The intraoral photographs were obtained with a Canon EOS Rebel T3 camera (4272 x 2848 pixels; approximately 3 Megabytes), a Canon Macro 100 lens, and a Canon MR-14 flash ring and were stored in JPEG format. The default camera settings were aperture f/29, ISO 200, shutter speed 1/160, and automatic white balance. The flash was set to ETTL mode.</p><p>- Biofilm annotation of the experimental dataset</p><p>Yellowish areas, loss of shine, and a rough or granular appearance were considered dental biofilm. The annotions were made using frontal view photographs to segment the dental biofilm of incisors and lateral view photographs for canines, premolars, and first molars.Two annotations were created for the experimental dataset using the polygon tool of the COCO Annotator 0.11.1 open-source image segmentation software. The first annotation consisted of marking the dental biofilm on all teeth of the image, which provided binary masks to separate the biofilm into background pixels. These binary masks were used in the training and validation procedures. The second annotation consisted of tooth instance segmentation, with 60 images being used exclusively for evaluation of the neural network and for statistical analysis. The buccal surface of each tooth was defined as the region of interest.</p><p>- Cross-validation</p><p>In the cross-validation procedure, the dataset was partitioned into eight distinct subsets, each consisting of 60 images from 20 patients (Fig. <xref rid="F1" ref-type="fig">1</xref>). During each iteration, six of these subsets were allocated to the model&#x02019;s training phase (360 images), while one subset was assigned to the validation phase (60 images), alternating between groups in each subsequent iteration. This procedure ensures that each subset acts as both a training group and a validation group. During the iterations, performance metrics such as sensitivity, specificity, accuracy, and F1 score were recorded, which provide a comprehensive view of the model&#x02019;s behavior throughout training and validation.</p><p>The eighth subset was defined exclusively as the test group (60 images), which remained unchanged during the cross-validation iterations. This approach was adopted in order to later use this last group as a test set for evaluation of the model&#x02019;s performance using the ensemble strategy.</p><p>- Ensemble strategy</p><p>Each base model, trained in the cross-validation iterations, generated its own prediction. These preditions were then combined to form a final prediction. After completion of the iterative process and analysis of performance of the models, the eighth group was strategically kept aside to serve as an independent and unbiased test set, enabling rigorous and unbiased assessment of the ensemble model performance.</p><p>- Model</p><p>The RGB image was taken as input and the binary masks as output. Instead of entering the full high-resolution images into the model, fixed-size patches of 384 x 384 pixels were used. The patch size was chosen based on preliminary tests following the original inputs from the EfficientNet encoders. A U-Net architecture was used for image segmentation (Fig. <xref rid="F2" ref-type="fig">2</xref>), employing a publicly available implementation written in the PyTorch framework (<xref rid="B10" ref-type="bibr">10</xref>). Furthermore, an EfficientNet-B4 trained on the ImageNet (transfer learning) dataset was chosen as the base structure of the U-Net architecture. The EfficientNet-B4 architecture was selected because this conFiguration showed the best results (<xref rid="B10" ref-type="bibr">10</xref>).</p><p>- Pre-processing and data augmentation</p><p>All images were pre-processed by reducing their width and height by 50%, resulting in a resolution of 2136 x 1424 (Fig. <xref rid="F2" ref-type="fig">2</xref>). During training, the data were artificially augmented. Fig. <xref rid="F2" ref-type="fig">2</xref> illustrates the data augmentation procedure: (i) a horizontal flip 50% of the time, and (ii) a random zoom from 80% to 120% and a random rotation from -15 to 15 degrees at 90% of the time. After each augmentation step, four random 384 x 384 patches from each image were selected to train the U-Nets in a supervised manner (Fig. <xref rid="F2" ref-type="fig">2</xref>).</p><p>
<fig position="float" id="F1"><label>Figure 1</label><caption><p>Splitting of the dataset into eight folds for the training, validation, and test sets.</p></caption><graphic xlink:href="medoral-30-e282-g001" position="float"/></fig>
</p><p>
<fig position="float" id="F2"><label>Figure 2</label><caption><p>Pre-processing, data augmentation, training, and evaluation procedures repeated seven times in a cross-validation manner. A: Pre-processing and data augmentation - The photographs and labels of the experimental dataset were resized, and data augmentations were applied during each step. At the end of each step, 4 random patches were selected. B: Training - The training procedure employed supervised learning, in which the model outputs were compared with the ground truth to adjust the Convolutional Neural Network. Transfer learning was applied using an EfficientNet B4 trained on the ImageNet dataset chosen as the backbone of the U-Net. C: The model&#x02019;s inference procedure used a sliding-window approach employing only each window center.</p></caption><graphic xlink:href="medoral-30-e282-g002" position="float"/></fig>
</p><p>- Data training</p><p>The training procedure was run without time concerns on 8 Tesla V100 16GB GPUs using a supervised learning approach. The validation procedure and final evaluation of the model (Fig. <xref rid="F2" ref-type="fig">2</xref>) were carried out at the end of each epoch using the inference method: each image was filled with black pixels around the edges until a size of 2496 x 1728 pixels was reached. Predictions on 384 x 384 patches were computed with 96-pixel steps using the sliding-window approach. Only the central 96 x 96 square of the network predictions was used. The confidence score ranged from 0 to 1, with 0 corresponding to the lowest confidence level and 1 to the highest level, using a sigmoid function (<xref rid="B10" ref-type="bibr">10</xref>). For dichotomous categorization, dental biofilm detection was defined when the confidence scores produced by the network exceeded the midpoint of 0.5 (<xref rid="B14" ref-type="bibr">14</xref>). The model that obtained the highest F1 score for the validation dataset was maintained. The training procedure was repeated for all combinations of cross-validation groups, resulting in seven trained models to be later combined in the ensemble strategy.</p><p>- Evaluation of the U-net neural networks</p><p>Quantitative and qualitative analyses were applied to evaluate whether the proposed deep learning solutions could correctly segment dental biofilms. First, quantitative analysis was performed using dichotomous categorization in which the dental biofilm detected by the model was compared with the ground truth labeled by the dentist. For the ensemble strategy, the output scores of all seven models were averaged to obtain the final scores. Next, qualitative analyses of the most and least satisfactory network results per patient were carried out, comparing the binary ground-truth masks with the dichotomous predictions and the output confidence scores.</p><p>- Statistical analysis</p><p>The results of the deep learning model were compared to the ground truth, providing counts of true positives, true negatives, false positives, and false negatives for each pixel. The following metrics were calculated per tooth: accuracy, sensitivity, specificity, and F1 score. Accuracy was calculated as the proportion of true positives and negatives in all cases evaluated. The statistics were computed using Python and the SciPy 3.7.7 package.</p><p>Results&#x000a0; </p><p>A total of 480 intraoral images from 160 participants aged 5 to 73 years with all teeth present and without caries or periodontal diseases were analyzed. In the initial dataset, 3 images from one participant were excluded because they were out of focus.</p><p>- Quantitative analysis</p><p><xref rid="T1" ref-type="table">Table 1</xref> shows the mean metrics according to the presence of orthodontic appliances and image type (front or lateral view). The performance of the ensemble strategy was adequate, with a mean accuracy of 93.1%, sensitivity of 65.1%, specificity of 95.9%, and F1 score of 63%.</p><p>- Qualitative analysis</p><p>The worst results were obtained with images that contained a lower amount of dental biofilm. Otherwise, the best results were obtained with images with more dental biofilm.</p></sec><sec><title>Discussion</title><p>The present study evaluated U-Net neural networks using an ensemble strategy to identify dental biofilms on intraoral photographs of deciduous and permanent teeth of individuals with or without orthodontic appliances. Analysis of this neural network showed a satisfactory performance, with a mean accuracy of 93.1%, sensitivity of 65.1%, specificity of 95.9%, and F1 score of 63.0%. These results indicate that the trained network employing the ensemble strategy can be used for the automatic identification of dental biofilm on intraoral digital images.</p><p>Studies have highlighted different approaches and obstacles in dental biofilm identification that range from specific methodological limitations to restrictions related to sample size, emphasizing the diverse challenges faced by research in this field. Compared to the U-Net model of a previous study that did not use the ensemble method and achieved an accuracy of 91.8% (<xref rid="B10" ref-type="bibr">10</xref>), sensitivity of 67.2%, specificity of 94.4% and F1 score of 60.6%, the present study applying the ensemble strategy obtained better network performance. One survey on dental biofilm identification without the use of the ensemble method was restricted to a single primary tooth per intraoral image (<xref rid="B3" ref-type="bibr">3</xref>). Other studies used small datasets consisting of intraoral photographs from only 25 and 20 participants (<xref rid="B9" ref-type="bibr">9</xref>,<xref rid="B15" ref-type="bibr">15</xref>). In contrast, the present study included 480 intraoral images from individuals aged 5 to 73 years with primary and permanent teeth, providing the network with a more extensive set of comprehensive, diverse, and valuable data.</p><p>Accurate and comprehensive annotation by accurately delineating the dental biofilm not only provides the essential basis for training the base models but also significantly affects the robustness and reliability of the ensemble strategy. However, the characteristics of the dental biofilm such as a color similar to the tooth and poorly defined and irregular contours compromised the segmentation, labeling and classification of the biofilm, resulting in relatively low F1 scores. To overcome these limitations, the expert annotated the second database using the polygon tool of COCO Annotator, clicking on the limits of the dental biofilm area. This approach reduced interactions and allowed for more accurate segmentation compared to having to paint the entire biofilm area with a brush tool (<xref rid="B10" ref-type="bibr">10</xref>).</p><p>Qualitative analysis, the neural network revealed more satisfactory performance for intraoral images with more significant dental biofilm, in agreement with a previous study (<xref rid="B10" ref-type="bibr">10</xref>). Performance was superior in the cervical third of the teeth, especially in canines, premolars, and first molars, and in cases with braces and orthodontic wires because of greater biofilm accumulation (<xref rid="B13" ref-type="bibr">13</xref>,<xref rid="B16" ref-type="bibr">16</xref>). This fact highlights the continued need for persistence of the challenge improving the ability of neural networks to effectively deal with situations of less dental biofilm accumulation.</p><p>One limitation of this study, with implications for its reproducibility, is that only photographs taken by a professional camera were included. In addition, only one expert annotated the dental biofilm. It would be important that more professionals participate in the annotation process to mitigate the risk of bias. Another limitation was the exclusion of second and third molars from this analysis because of the frequent difficulty in obtaining adequate visualization of these teeth on photographs. The same applies to the palatal, lingual, and occlusal surfaces of all teeth in the oral cavity. One study used a scanner, a device capable of capturing all sides of the teeth, to label the biofilm throughout the dental arch (<xref rid="B15" ref-type="bibr">15</xref>). However, the image quality was compromised, showing pixelation and blurring especially in the proximal areas, which can impair visual detection of dental biofilm.</p><p>Developing a tool for dental biofilm identification can have a significant impact on the prevention, treatment and maintenance of oral health, benefiting both patients and professionals in clinical practice. A tool capable of assisting in dental biofilm detection may guide the patient in order to improve oral hygiene based on immediate feedback about areas where brushing and flossing could be improved. In addition, it would be possible for the professional to monitor the patient&#x02019;s progress throughout treatment, ensuring that biofilm self-control is adequate. Within this context, the results of the present study highlight the feasibility of biofilm identification using deep learning techniques, paving the way for the development of a platform that carries out this recognition in an automated manner.</p><p>A broader database is recommended for future research to improve the performance of the neural network. This can be achieved by incorporating more images from different devices, including cell phone and Tablet cameras. These images must be captured under varying lighting conditions. Furthermore, the annotation task should be performed by multiple examiners. Considering that specific oral conditions can affect dental biofilm distribution, the database of future projects should include patients with fixed and implant-mounted partial dentures, different crowding conditions, distinct patterns of malocclusion, cases of carious lesions, variable roughness of the enamel and dentin surfaces, and teeth with varied colors. These factors would render the dataset more diverse and thus allow more effective neural network training, improving its biofilm detection ability.</p><p>In conclusion, the ensemble method of U-Net neural networks was able to automatically identify visually detecTable dental biofilms on intraoral photographs; it is therefore an automated, objective, and quantitative option for dental biofilm detection. As more data are incorporated into the neural network for labeling and segmentation training, there will be significant potential for improvement of the network.</p></sec></body><back><ack><title>Acknowledgement</title><p>Declared none.</p></ack><sec><title>Funding</title><p>The authors are supported by the Brazilian National Council for Scientific and Technological Development, Grant/Award Numbers: 404088/2023-6, 403344/2022-0 and 306780/2022-40; Coordena&#x000e7;&#x000e3;o de Aperfei&#x000e7;oamento de Pessoal de N&#x000ed;vel Superior (BOL0569/2020); and Funda&#x000e7;&#x000e3;o de Amparo &#x000e0; Pesquisa do Estado da Bahia.</p></sec><sec sec-type="COI-statement"><title>Conflict of interest</title><p>The authors declare no conflict of interest, financial or otherwise.</p></sec><notes><title>Institutional Review Board Statement</title><p>The project was approved by the Research Ethics Committee of the School of Dentistry, Federal University of Bahia (protocol number: 4.434.730).
</p></notes><notes><title>Author Contributions</title><p>Bernardo Peters Menezes Silva, Luciano Rebou&#x000e7;as de Oliveira, Patricia Ramos Cury conceived the ideas; Brenda Pereira Pinheiro Sobrinho, Katia Montanha de Andrade, Bruna Pereira Pinheiro Sobrinho, Daniel Araki Ribeiro collected the data; Bernardo Peters Menezes Silva and Luciano Rebou&#x000e7;as de Oliveira analysed the data; and Daniel Araki Ribeiro, Jean Nunes dos Santos, Luciano Rebou&#x000e7;as de Oliveira and Patricia Ramos Cury led the writing.</p></notes><ref-list><ref id="B1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theilade</surname><given-names>E</given-names></name></person-group><article-title>The non-specific theory in microbial etiology of inflammatory periodontal diseases</article-title><source>J Clin Periodontol</source><year>1986</year><volume>13</volume><fpage>905</fpage><lpage>11</lpage><pub-id pub-id-type="pmid">3540019</pub-id>
</element-citation></ref><ref id="B2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillings</surname><given-names>BR</given-names></name></person-group><article-title>Recent developments in dental plaque disclosants</article-title><source>Aust Dent J</source><year>1977</year><volume>22</volume><fpage>260</fpage><lpage>6</lpage><pub-id pub-id-type="pmid">277142</pub-id>
</element-citation></ref><ref id="B3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>You</surname><given-names>W</given-names></name><name><surname>Hao</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Xia</surname><given-names>B</given-names></name></person-group><article-title>Deep learning-based dental plaque detection on primary teeth: a comparison with clinical assessments</article-title><source>BMC Oral Health</source><year>2020</year><volume>20</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">32404094</pub-id>
</element-citation></ref><ref id="B4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>NP</given-names></name><name><surname>Ostergaard</surname><given-names>E</given-names></name><name><surname>Loe</surname><given-names>H</given-names></name></person-group><article-title>A fluorescent plaque disclosing agent</article-title><source>J Periodontal Res</source><year>1972</year><volume>7</volume><fpage>59</fpage><lpage>67</lpage><pub-id pub-id-type="pmid">4272033</pub-id>
</element-citation></ref><ref id="B5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shrivastava</surname><given-names>D</given-names></name><name><surname>Natoli</surname><given-names>V</given-names></name><name><surname>Srivastava</surname><given-names>KC</given-names></name><name><surname>Alzoubi</surname><given-names>IA</given-names></name><name><surname>Nagy</surname><given-names>AI</given-names></name><name><surname>Hamza</surname><given-names>MO</given-names></name></person-group><article-title>Novel Approach to Dental Biofilm Management through Guided Biofilm Therapy (GBT): A Review</article-title><source>Microorganisms</source><year>2021</year><volume>9</volume><fpage>1966</fpage><pub-id pub-id-type="pmid">34576863</pub-id>
</element-citation></ref><ref id="B6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatt</surname><given-names>FP</given-names></name><name><surname>Rocha</surname><given-names>AO</given-names></name><name><surname>Anjos</surname><given-names>LMD</given-names></name><name><surname>Caldas</surname><given-names>RA</given-names></name><name><surname>Cardoso</surname><given-names>M</given-names></name><name><surname>Rabelo</surname><given-names>GD</given-names></name></person-group><article-title>Artificial intelligence applications in dentistry: A bibliometric review with an emphasis on computational research trends within the field</article-title><source>J Am Dent Assoc</source><year>2024</year><volume>155</volume><fpage>755</fpage><lpage>64</lpage><pub-id pub-id-type="pmid">39093229</pub-id>
</element-citation></ref><ref id="B7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarker</surname><given-names>IH</given-names></name></person-group><article-title>Machine Learning: Algorithms, Real-World Applications and Research Directions</article-title><source>SN Comput Sci</source><year>2021</year><volume>2</volume><fpage>160</fpage><pub-id pub-id-type="pmid">33778771</pub-id>
</element-citation></ref><ref id="B8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>RY</given-names></name><name><surname>Coyner</surname><given-names>AS</given-names></name><name><surname>Kalpathy-Cramer</surname><given-names>J</given-names></name><name><surname>Chiang</surname><given-names>MF</given-names></name><name><surname>Campbell</surname><given-names>JP</given-names></name></person-group><article-title>Introduction to machine learning, neural networks, and deep learning</article-title><source>Transl Vis Sci Technol</source><year>2020</year><volume>9</volume><fpage>14</fpage><pub-id pub-id-type="pmid">32704420</pub-id>
</element-citation></ref><ref id="B9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname><given-names>K</given-names></name><name><surname>Landini</surname><given-names>G</given-names></name><name><surname>Walmsley</surname><given-names>AD</given-names></name></person-group><article-title>Automated quantification of dental plaque accumulation using digital imaging</article-title><source>J Dent</source><year>2004</year><volume>32</volume><fpage>623</fpage><lpage>8</lpage><pub-id pub-id-type="pmid">15476956</pub-id>
</element-citation></ref><ref id="B10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrade</surname><given-names>KM</given-names></name><name><surname>Silva</surname><given-names>BP</given-names></name><name><surname>de Oliveira</surname><given-names>LR</given-names></name><name><surname>Cury</surname><given-names>PR</given-names></name></person-group><article-title>Automatic dental biofilm detection based on deep learning</article-title><source>J Clin Periodontol</source><year>2023</year><volume>50</volume><fpage>571</fpage><lpage>81</lpage><pub-id pub-id-type="pmid">36635042</pub-id>
</element-citation></ref><ref id="B11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RS</given-names></name><name><surname>Shivanna</surname><given-names>DB</given-names></name><name><surname>Lakshminarayana</surname><given-names>S</given-names></name><name><surname>Mahadevpur</surname><given-names>KS</given-names></name><name><surname>Alhazmi</surname><given-names>YA</given-names></name><name><surname>Bakri</surname><given-names>MMH</given-names></name></person-group><article-title>Ensemble Deep-Learning-Based Prognostic and Prediction for Recurrence of Sporadic Odontogenic Keratocysts on Hematoxylin and Eosin Stained Pathological Images of Incisional Biopsies</article-title><source>J Pers Med</source><year>2022</year><volume>12</volume><fpage>1220</fpage><pub-id pub-id-type="pmid">35893314</pub-id>
</element-citation></ref><ref id="B12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>E</given-names></name><name><surname>Sim</surname><given-names>K</given-names></name></person-group><article-title>On Constructing Ensembles for Combinatorial Optimisation</article-title><source>Evol Comput</source><year>2018</year><volume>26</volume><fpage>67</fpage><lpage>87</lpage><pub-id pub-id-type="pmid">28072555</pub-id>
</element-citation></ref><ref id="B13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeganeh</surname><given-names>S</given-names></name><name><surname>Lynch</surname><given-names>E</given-names></name><name><surname>Jovanovski</surname><given-names>V</given-names></name><name><surname>Zou</surname><given-names>L</given-names></name></person-group><article-title>Quantification of root surface plaque using a new 3-D laser scanning method</article-title><source>J Clin Periodontol</source><year>1999</year><volume>26</volume><fpage>692</fpage><lpage>7</lpage><pub-id pub-id-type="pmid">10522781</pub-id>
</element-citation></ref><ref id="B14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esposito</surname><given-names>C</given-names></name><name><surname>Landrum</surname><given-names>GA</given-names></name><name><surname>Schneider</surname><given-names>N</given-names></name><name><surname>Stiefl</surname><given-names>N</given-names></name><name><surname>Riniker</surname><given-names>S</given-names></name></person-group><article-title>GHOST: adjusting the decision threshold to handle imbalanced data in machine learning</article-title><source>J Chem Inf Model</source><year>2021</year><volume>61</volume><fpage>2623</fpage><lpage>40</lpage><pub-id pub-id-type="pmid">34100609</pub-id>
</element-citation></ref><ref id="B15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giese-Kraft</surname><given-names>K</given-names></name><name><surname>Jung</surname><given-names>K</given-names></name><name><surname>Schlueter</surname><given-names>N</given-names></name><name><surname>Vach</surname><given-names>K</given-names></name><name><surname>Ganss</surname><given-names>C</given-names></name></person-group><article-title>Detecting and monitoring dental plaque levels with digital 2D and 3D imaging techniques</article-title><source>PLoS One</source><year>2022</year><volume>17</volume><fpage>e0263722</fpage><pub-id pub-id-type="pmid">35167618</pub-id>
</element-citation></ref><ref id="B16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>S&#x000f6;der</surname><given-names>B</given-names></name><name><surname>Johannsen</surname><given-names>A</given-names></name><name><surname>Lagerl&#x000f6;f</surname><given-names>F</given-names></name></person-group><article-title>Percent of plaque on individual tooth surfaces and differences in plaque area between adjacent teeth in healthy adults</article-title><source>Int J Dent Hyg</source><year>2003</year><volume>1</volume><fpage>23</fpage><lpage>8</lpage><pub-id pub-id-type="pmid">16451543</pub-id>
</element-citation></ref></ref-list></back><floats-group><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Performance metrics (accuracy, sensitivity, specificity and F1 score) of the ensemble model for the entire dataset and subgroups (with and without orthodontic appliances or frontal and lateral views).</p></caption><table frame="border" rules="all" width="100%"><thead><tr><th rowspan="1" colspan="1">Image type</th><th rowspan="1" colspan="1">Accuracy
(%)</th><th rowspan="1" colspan="1">Sensitivity
(%)</th><th rowspan="1" colspan="1">Specificity
(%)</th><th rowspan="1" colspan="1">F1 score
(%)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">No orthodontic appliance</td><td align="center" valign="middle" rowspan="1" colspan="1">92.4</td><td align="center" valign="middle" rowspan="1" colspan="1">71.7</td><td align="center" valign="middle" rowspan="1" colspan="1">94.5</td><td align="center" valign="middle" rowspan="1" colspan="1">63.1</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Orthodontic appliance</td><td align="center" valign="middle" rowspan="1" colspan="1">93.7</td><td align="center" valign="middle" rowspan="1" colspan="1">58.8</td><td align="center" valign="middle" rowspan="1" colspan="1">97.2</td><td align="center" valign="middle" rowspan="1" colspan="1">62.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Frontal view</td><td align="center" valign="middle" rowspan="1" colspan="1">91.9</td><td align="center" valign="middle" rowspan="1" colspan="1">67.9</td><td align="center" valign="middle" rowspan="1" colspan="1">94.0</td><td align="center" valign="middle" rowspan="1" colspan="1">57.3</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lateral view</td><td align="center" valign="middle" rowspan="1" colspan="1">92.6</td><td align="center" valign="middle" rowspan="1" colspan="1">73.3</td><td align="center" valign="middle" rowspan="1" colspan="1">94.7</td><td align="center" valign="middle" rowspan="1" colspan="1">65.8</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" rowspan="1" colspan="1">93.1</td><td align="center" valign="middle" rowspan="1" colspan="1">65.1</td><td align="center" valign="middle" rowspan="1" colspan="1">95.9</td><td align="center" valign="middle" rowspan="1" colspan="1">63.0</td></tr></tbody></table></table-wrap></floats-group></article>