<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408612</article-id><article-id pub-id-type="pmc">PMC12101846</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0324347</article-id><article-id pub-id-type="publisher-id">PONE-D-24-46508</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Plant Science</subject><subj-group><subject>Plant Pathology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject><subj-group><subject>Machine Learning</subject><subj-group><subject>Deep Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Crop Science</subject><subj-group><subject>Crops</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Agriculture</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Professions</subject><subj-group><subject>Agricultural Workers</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Agrochemicals</subject><subj-group><subject>Fertilizers</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Artificial Intelligence</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Agriculture</subject><subj-group><subject>Agricultural Methods</subject><subj-group><subject>Sustainable Agriculture</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Ecology and Environmental Sciences</subject><subj-group><subject>Sustainability Science</subject><subj-group><subject>Sustainable Agriculture</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>An intelligent framework for crop health surveillance and disease management</article-title><alt-title alt-title-type="running-head">An intelligent framework for crop health</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ayid</surname><given-names>Yasser M.</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0001-2802-080X</contrib-id><name><surname>Fouad</surname><given-names>Yasser</given-names></name><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5614-6744</contrib-id><name><surname>Kaddes</surname><given-names>Mourad</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>El-Hoseny</surname><given-names>Heba M.</given-names></name><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff004" ref-type="aff">
<sup>4</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Mathematics Department, Applied Collage Al-Kamil Branch, University of Jeddah, Jeddah, Saudi Arabia</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Department of Computer Science, Faculty of Computers and Information, Suez University, Suez, Egypt</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>Department of Information Systems, College of Computing &#x00026; Information Technology at Khulais, University of Jeddah, Jeddah, Saudi Arabia</addr-line></aff><aff id="aff004"><label>4</label>
<addr-line>Faculty of Computer Studies, Arab Open University, Riyadh, Saudi Arabia</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Parimala</surname><given-names>Venkata Krishna</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Sri Padmavati Mahila Visvavidyalayam, INDIA</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>Yasser.ramadan@suezuni.edu.eg</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0324347</elocation-id><history><date date-type="received"><day>15</day><month>10</month><year>2024</year></date><date date-type="accepted"><day>23</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Ayid et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Ayid et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0324347.pdf"/><abstract><p>The agricultural sector faces critical challenges, including significant crop losses due to undetected plant diseases, inefficient monitoring systems, and delays in disease management, all of which threaten food security worldwide. Traditional approaches to disease detection are often labor-intensive, time-consuming, and prone to errors, making early intervention difficult. This paper proposes an intelligent framework for automated crop health monitoring and early disease detection to overcome these limitations. The system leverages deep learning, cloud computing, embedded devices, and the Internet of Things (IoT) to provide real-time insights into plant health over large agricultural areas. The primary goal is to enhance early detection accuracy and recommend effective disease management strategies, including crop rotation and targeted treatment. Additionally, environmental parameters such as temperature, humidity, and water levels are continuously monitored to aid in informed decision-making. The proposed framework incorporates Convolutional Neural Network (CNN), MobileNet-1, MobileNet-2, Residual Network (ResNet-50), and ResNet-50 with InceptionV3 to ensure precise disease identification and improved agricultural productivity.</p></abstract><funding-group><award-group id="award001"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100015624</institution-id><institution>University of Jeddah</institution></institution-wrap>
</funding-source><award-id>UJ-24-DRK-20761-1</award-id><principal-award-recipient>
<name><surname>Ayid</surname><given-names>Yasser M.</given-names></name>
</principal-award-recipient></award-group><funding-statement>This work was funded by the University of Jeddah, Jeddah, Saudi Arabia, under grant No. (UJ-24-DRK-20761-1). Therefore, the authors thank the University of Jeddah for its technical and financial support. The funders provided technical and financial support for this research but had no role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="11"/><table-count count="5"/><page-count count="18"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data for this study are publicly available from the Kaggle repository (<ext-link xlink:href="https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset" ext-link-type="uri">https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset</ext-link>).</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data for this study are publicly available from the Kaggle repository (<ext-link xlink:href="https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset" ext-link-type="uri">https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset</ext-link>).</p></notes></front><body><sec sec-type="intro" id="sec001"><title>I. Introduction</title><p>Agriculture is vital to global food security and economic stability, but plant diseases reduce crop output and quality. Maintaining agricultural sustainability requires early and precise disease diagnosis. New research in AI technology and deep learning variant techniques have shown significant promise in automating the identification approaches of plant diseases, facilitating prompt responses, and minimizing the utilization of harmful chemicals [<xref rid="pone.0324347.ref001" ref-type="bibr">1</xref>].</p><p>Farmers and agronomists have always relied on hands-on examinations to detect plant diseases. This method consumes a lot of time and is also lead to mistakes, frequently resulting in late detection, which in turn leads to more usage of pesticides, a decrease in crops, and economic losses. AI-based disease detection allows sustainable farming with less chemical use [<xref rid="pone.0324347.ref002" ref-type="bibr">2</xref>].</p><p>Smart agriculture is increasingly applying technology to achieve sustainability, boost productivity, and makes less costs. In smart irrigation, a scheduling tool has been proposed based on cloud computing technologies [<xref rid="pone.0324347.ref002" ref-type="bibr">2</xref>&#x02013;<xref rid="pone.0324347.ref004" ref-type="bibr">4</xref>]. By tracking and analyzing environmental options, farmers can enhance practices, make valuable decisions regarding usage of resources, and detect yield issues at the near stage. This technique enhances sustainability and fosters a deeper understanding of the relationship between farming and the land [<xref rid="pone.0324347.ref001" ref-type="bibr">1</xref>]. Smart diagnosis for plant diseases leverages new technologies like artificial intelligence, machine learning, and image recognition to quickly and accurately identify plant health issues. These systems process and handle data from sensors, cameras, and other tools to detect diseases early, enabling farmers to take timely action. By automating diagnosis, smart systems reduce reliance on manual inspection, improve crop management, and improve sustainability by minimizing the use of chemicals and resources. This technology is revolutionizing agricultural practices, making them more efficient and precise [<xref rid="pone.0324347.ref005" ref-type="bibr">5</xref>].</p><p>Different researchers have introduced solutions for plant disease detection to improve crop productivity and enhance food security. Araujo et al. [<xref rid="pone.0324347.ref006" ref-type="bibr">6</xref>] developed a technique for plant diagnosis based on an enhanced feature-extracting Support Vector Machine (SVM) using a Bag of Visual Words (BoVW) and Local Binary Patterns (LBPs) methods that are used for processing as well as feature extraction. Sh. and P. [<xref rid="pone.0324347.ref007" ref-type="bibr">7</xref>] introduced a rice disease classification algorithm based on color features and SVM classifier. A total of 172 different color channels were implemented for feature extraction classified four different types of rice diseases and achieved 94.68% accuracy.</p><p>Chowdhury et al. [<xref rid="pone.0324347.ref008" ref-type="bibr">8</xref>] employ a deep learning architecture with EfficientNet to categorize tomato illnesses in 18,161 tomato leaf pictures. The effectiveness of the two segmentation models, Modified U-net and U-net, is compared for binary, six-class, and ten-class classifications. With remarkable accuracy, Intersection over Union (IoU), and Dice scores of 98.66%, 98.5%, and 98.73%, respectively, the improved U-net model was achieved. EfficientNet-B7 demonstrated exceptional performance in binary and six-class classifications, with accuracies of 99.95% and 99.12%, respectively. The experimental experiments surpass the current literature.</p><p>Sharma et al. [<xref rid="pone.0324347.ref009" ref-type="bibr">9</xref>] introduced a CNN model for classifying illnesses affecting rice and potato plant leaves. The model, trained on datasets of 5,932 and 1,500 photos, attained an accuracy of 99.58% for rice images and 97.66% for potato leaves. It outperformed several machine learning image classifiers, such as Random Forest, K-Nearest Neighbors, Decision Tree, and Support Vector Machine.</p><p>Computer-based program developed by Haridasan et al. [<xref rid="pone.0324347.ref010" ref-type="bibr">10</xref>] to identify and diagnose rice plant illnesses from photos. This system that was developed by them employs image processing, machine learning, and deep learning methodologies to detect and classify infection based on visual data. SVM classifiers and convolutional neural networks diagnose disease. Deep learning-based strategies had the greatest validation accuracy of 0.9145, and a predictive cure is advised for agriculture-related companies and individuals.</p><p>S and H [<xref rid="pone.0324347.ref011" ref-type="bibr">11</xref>] used photos of tomato leaves along with six diseases and healthy samples to diagnose tomato plant leaf illnesses using Fuzzy-SVM, CNN, and Region-based Convolutional Neural Network (R-CNN). The type of disease is identified using R-CNN classifiers. Fuzzy SVM and CNN classification algorithms are evaluated to determine which plant disease prediction model produces the best results; the R-CNN-based classifier achieves the maximum accuracy, measuring 96.735%.</p><p>In smart agriculture, IoT-based fertilizer management is a key application that leverages IoT technology to optimize fertilizer usage. This system provides farmers with real-time data on soil conditions and nutrient levels, enabling them to adjust the type and amount of fertilizer applied to their crops for more efficient and targeted fertilization [<xref rid="pone.0324347.ref008" ref-type="bibr">8</xref>,<xref rid="pone.0324347.ref009" ref-type="bibr">9</xref>].</p><p>Agriculture faces multifaceted challenges that hinder its sustainability and productivity [<xref rid="pone.0324347.ref010" ref-type="bibr">10</xref>]. These challenges include climate change-induced uncertainty, water scarcity, soil degradation, pest and disease outbreaks, and limited technology access. Addressing these issues is crucial for ensuring food security, supporting rural livelihoods, and promoting sustainable farming practices. Effective solutions involve climate-resilient agriculture, improved water management, soil conservation, disease-resistant crop varieties, and technology dissemination to empower farmers. Additionally, supportive policies and investments in infrastructure are essential to tackle these challenges effectively.</p><p>This paper presents a smart agricultural system that manages the farming process, including crop monitoring, fertilizer administration, climate condition monitoring, and automatic irrigation. The system, powered by a Node Microcontroller Unit (Node-MCU), includes sensors like DHT11, Moisture, LDR, Water Pump, and 12V LED strip. The IoT-based system collects data on soil moisture, temperature, humidity, and temperature, sending it to a cloud for live monitoring. This system optimizes resource utilization, increases productivity, and improves decision-making in farming operations. Critical data on weather, soil moisture, temperature, humidity, and other environmental parameters is gathered via IoT sensors. The data is stored, processed, and analyzed on a centralized cloud platform. The mobile application allows farmers to remotely access the system, adjust irrigation settings, monitor sensor readings, receive alerts, and automate processes. This integration brings benefits such as precise resource management, reduced water consumption, fertilizer usage, and reduced environmental impact. <xref rid="pone.0324347.t001" ref-type="table">Table 1</xref> displays some related studies to the proposed work.</p><table-wrap position="float" id="pone.0324347.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.t001</object-id><label>Table 1</label><caption><title>Some related studies to the proposed work.</title></caption><alternatives><graphic xlink:href="pone.0324347.t001" id="pone.0324347.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Ref.</th><th align="left" rowspan="1" colspan="1">Methodology</th><th align="left" rowspan="1" colspan="1">Contribution</th><th align="left" rowspan="1" colspan="1">Accuracy</th><th align="left" rowspan="1" colspan="1">Limitation</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Araujo et al. [<xref rid="pone.0324347.ref006" ref-type="bibr">6</xref>]</td><td align="left" rowspan="1" colspan="1">SVM with BoVW and LBPs for plant diagnosis</td><td align="left" rowspan="1" colspan="1">BoVW, LBPs, SVM for feature extraction and diagnosis</td><td align="left" rowspan="1" colspan="1">75.8%</td><td align="left" rowspan="1" colspan="1">Low accuracy, traditional feature extraction, limited generalization</td></tr><tr><td align="left" rowspan="1" colspan="1">Sh. and P. [<xref rid="pone.0324347.ref007" ref-type="bibr">7</xref>]</td><td align="left" rowspan="1" colspan="1">Classify Rice leaves diseases using SVM and 172 color channels</td><td align="left" rowspan="1" colspan="1">Color features and SVM; 172 color channels</td><td align="left" rowspan="1" colspan="1">94.68%</td><td align="left" rowspan="1" colspan="1">Relies only on color features, which can be affected by lighting conditions</td></tr><tr><td align="left" rowspan="1" colspan="1">Chowdhury et al. [<xref rid="pone.0324347.ref008" ref-type="bibr">8</xref>]</td><td align="left" rowspan="1" colspan="1">Deep learning with EfficientNet for tomato disease classification</td><td align="left" rowspan="1" colspan="1">EfficientNet-B7, U-net; high accuracy in binary and multiclass</td><td align="left" rowspan="1" colspan="1">98.66% (binary), 99.95% (EfficientNet-B7)</td><td align="left" rowspan="1" colspan="1">Requires high computational power, limited to specific crops</td></tr><tr><td align="left" rowspan="1" colspan="1">Sharma et al. [<xref rid="pone.0324347.ref009" ref-type="bibr">9</xref>]</td><td align="left" rowspan="1" colspan="1">CNN model for rice and potato leaf disease classification</td><td align="left" rowspan="1" colspan="1">CNN; comparison with SVM, KNN, Decision Tree, Random Forest</td><td align="left" rowspan="1" colspan="1">99.58% (rice), 97.66% (potato)</td><td align="left" rowspan="1" colspan="1">No explainability in the decision-making process</td></tr><tr><td align="left" rowspan="1" colspan="1">Haridasan et al. [<xref rid="pone.0324347.ref010" ref-type="bibr">10</xref>]</td><td align="left" rowspan="1" colspan="1">Image processing, Machine Learning, and Deep Learning for rice plant disease identification</td><td align="left" rowspan="1" colspan="1">SVM, CNN; validation accuracy of 0.9145</td><td align="left" rowspan="1" colspan="1">0.9145</td><td align="left" rowspan="1" colspan="1">Limited dataset, potential overfitting</td></tr><tr><td align="left" rowspan="1" colspan="1">S and H [<xref rid="pone.0324347.ref011" ref-type="bibr">11</xref>]</td><td align="left" rowspan="1" colspan="1">Fuzzy-SVM, CNN, R-CNN for tomato disease classification</td><td align="left" rowspan="1" colspan="1">Fuzzy-SVM, CNN, R-CNN; highest accuracy of 96.735%</td><td align="left" rowspan="1" colspan="1">96.735</td><td align="left" rowspan="1" colspan="1">Computational complexity, high training time</td></tr></tbody></table></alternatives></table-wrap><p>The proposed framework addresses the limitations of previous studies, which mainly focus on image-based plant disease classification without considering environmental factors. In contrast, this system integrates real-time monitoring of temperature, soil moisture, and plant health, providing instant alerts for early intervention. To enhance model generalization, the framework has been trained on a large dataset of 87,000 images across 38 plant classes from Kaggle, ensuring higher accuracy across diverse conditions. Unlike prior approaches that lack decision support, this framework not only detects diseases using deep learning but also offers personalized recommendations for pesticides, fertilizers, and optimal farming practices. This makes it a more comprehensive, efficient, and practical solution for modern agriculture.</p><p>The main contribution is the proposal of an intelligent framework rather than just a mobile application that offers a variety of services designed to assist farmers in improving crop management.</p><p>The system continuously monitors key environmental parameters, such as temperature, soil humidity, and plant health, while providing real-time alerts about any significant changes or emergencies. This allows farmers to respond promptly to issues like plant diseases, extreme weather, or soil conditions. Additionally, the framework employs deep learning techniques for automatic plant disease detection, enabling accurate identification of various diseases. This helps farmers quickly diagnose plant health issues and take appropriate action, thus minimizing crop damage and enhancing overall productivity.</p><p>Moreover, the framework provides tailored recommendations to improve plant health, such as suggesting suitable pesticides for different plant diseases or advising on fertilizers that enhance crop growth. The system also enables continuous monitoring empowering farmers to make data-driven decisions to maximize crop yield. By incorporating Deep learning algorithms, the system&#x02019;s recommendations become more precise over time, offering even better guidance.</p><p>By automating much of the monitoring and decision-making process, the framework reduces manual effort and increases agricultural efficiency. This intelligent framework provides a comprehensive solution that addresses several critical aspects of modern farming, simplifying the process and improving productivity.</p><p>The primary motivations for proposing this framework include the need for efficient crop management through continuous monitoring and real-time data, the importance of early detection of plant diseases using deep learning techniques to provide faster and more accurate diagnoses, and the goal of improving agricultural productivity by offering tailored recommendations for pesticides and fertilizers. Additionally, the framework aims to reduce manual labor by automating various farming processes, thus enabling farmers to make data-driven decisions for enhanced crop management.</p><p>This paper&#x02019;s remaining sections are organized as follows. In Preliminaries section, the IoT-based agricultural systems preliminary aspects are examined and clarified. The suggested framework, including mobile application and deployment, is examined in the proposed algorithm and model deployment sections. The assessment metrics analysis and its numerical outcomes are examined in evaluation metrics section. The simulation analysis is examined and discussed in simulations and discussions section. The conclusions section concludes the paper.</p></sec><sec id="sec002"><title>II. Preliminaries</title><sec id="sec003"><title>1. Agriculture systems powered by IoT</title><p>To improve farming techniques and boost productivity, IoT-based agricultural systems make use of a variety of cutting-edge frameworks and technology. <xref rid="pone.0324347.g001" ref-type="fig">Fig 1</xref> demonstrates the IoT architecture for the smart farm system. IoT-based agricultural systems incorporate a wide range of technologies and frameworks, and A brief overview of them can be introduced [<xref rid="pone.0324347.ref012" ref-type="bibr">12</xref>&#x02013;<xref rid="pone.0324347.ref017" ref-type="bibr">17</xref>].</p><fig position="float" id="pone.0324347.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g001</object-id><label>Fig 1</label><caption><title>The smart agriculture system&#x02019;s IoT architecture.</title></caption><graphic xlink:href="pone.0324347.g001" position="float"/></fig><p><bold>Wireless Sensor Networks:</bold> low-cost, low-cost sensor nodes that monitor environmental factors that impact on industry like temperature, humidity, soil moisture, and light intensity, providing real-time data for agriculture decision-making [<xref rid="pone.0324347.ref001" ref-type="bibr">1</xref>,<xref rid="pone.0324347.ref013" ref-type="bibr">13</xref>].</p><p><bold>IoT Platforms:</bold> IoT platforms, such as Microsoft Azure IoT, AWS IoT, Google Cloud IoT, and IBM Watson IoT, are essential for managing and connecting IoT devices and data, offering features like device management, data visualization, analytics, and integration [<xref rid="pone.0324347.ref014" ref-type="bibr">14</xref>&#x02013;<xref rid="pone.0324347.ref017" ref-type="bibr">17</xref>].</p><p><bold>Edge Computing:</bold> Edge computing in agricultural IoT systems allows on-time data processing and analysis at the field level, minimizing latency and bandwidth usage, quicker decision-making, and reducing reliance on cloud resources.</p><p><bold>Artificial intelligence (AI) and machine learning (ML):</bold> These two fields use sensor data to provide insights and forecasts that help with pest and disease diagnosis, yield prediction, and resource management for fertilizer and water use.</p><p><bold>Geographic Information Systems:</bold> combines spatial and agricultural data, enabling farmers to make location-based decisions for soil mapping, agronomic monitoring, and high-tech farming.</p><p><bold>Drones and UAVs:</bold> UAVs, equipped with cameras and sensors, are utilized for aerial imaging, crop monitoring, surveillance, and providing high-resolution data for crop health assessment and yield estimation.</p><p><bold>Smart Irrigation Systems:</bold> IoT-enabled smart irrigation systems employ sensors to assess soil moisture and meteorological conditions, allowing accurate and efficient watering, water saving, and enhancement of agricultural yields [<xref rid="pone.0324347.ref018" ref-type="bibr">18</xref>].</p><p><bold>Reduced-Power Wide-Area Networks:</bold> depicts technologies such as LoRaWAN and NB-IoT have extensive communication capabilities with little power usage, rendering them suitable for linking IoT devices in isolated agricultural regions.</p><p><bold>Open-source frameworks and Libraries:</bold> Arduino, Raspberry Pi, TensorFlow, and OpenCV enable developers to create customized IoT solutions for agricultural needs.</p><p>IoT-based agricultural systems enable farmers with real-time insights, automation, and information-driven choices by integrating various technologies and frameworks, which eventually enhance farming operations&#x02019; efficiency, sustainability, and profitability [<xref rid="pone.0324347.ref019" ref-type="bibr">19</xref>,<xref rid="pone.0324347.ref020" ref-type="bibr">20</xref>].</p></sec><sec id="sec004"><title>2. Crop monitoring IoT systems</title><p>Crop monitoring IoT systems are crucial for modern agriculture, offering real-time insights into crop health, growth conditions, and environmental factors [<xref rid="pone.0324347.ref021" ref-type="bibr">21</xref>&#x02013;<xref rid="pone.0324347.ref023" ref-type="bibr">23</xref>]. Sensors continuously collect data on environmental conditions and crop parameters. This data includes temperature, humidity levels, soil moisture content, nutrient levels, and more. Sensor nodes use protocols such as Wi-Fi, Bluetooth, Zigbee, or LoRaWAN to establish wireless communication with a central hub or gateway. Over vast agricultural areas, data transmission is made possible with ease by this link. Either locally at the edge or remotely to the cloud, the acquired data is processed for analysis. To find trends, spot abnormalities, and forecast crop growth, insect infestations, or the spread of diseases, machine learning algorithms can examine the data. Farmers can access the processed data through web or mobile applications. Visualization tools present the data in easy-to-understand formats such as charts, graphs, and heatmaps [<xref rid="pone.0324347.ref023" ref-type="bibr">23</xref>,<xref rid="pone.0324347.ref024" ref-type="bibr">24</xref>]. These insights help farmers make informed decisions about irrigation scheduling, fertilization, pest control, and harvesting [<xref rid="pone.0324347.ref025" ref-type="bibr">25</xref>&#x02013;<xref rid="pone.0324347.ref027" ref-type="bibr">27</xref>]. The general framework for plant disease detection can be presented in <xref rid="pone.0324347.g002" ref-type="fig">Fig 2</xref>.</p><fig position="float" id="pone.0324347.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g002</object-id><label>Fig 2</label><caption><title>The general framework for plant disease classification approach.</title></caption><graphic xlink:href="pone.0324347.g002" position="float"/></fig></sec><sec id="sec005"><title>3. Convolutional neural network (CNN) architecture</title><p>Deep learning is an artificial intelligence neural network composed of several layers, analogous to the activities of the brain. It has been utilized in several domains, including social networking, e-commerce, and object identification. CNN, a multi-layered deep neural network, has demonstrated exceptional efficacy in data processing. It has two stages: feature picture extraction and classification [<xref rid="pone.0324347.ref028" ref-type="bibr">28</xref>&#x02013;<xref rid="pone.0324347.ref030" ref-type="bibr">30</xref>].</p><p>The main idea behind CNNs is to take local features from higher layers and move them to lower layers to obtain more complex features. To minimize the dimensions of the input volume, the convolutional layer consists of a group of kernels that connect the entire input using &#x0201c;stride(s)&#x0201d; to find feature mappings. The convolutional layer computation equation is as follows [<xref rid="pone.0324347.ref025" ref-type="bibr">25</xref>]:</p><disp-formula id="pone.0324347.e001">
<alternatives><graphic xlink:href="pone.0324347.e001.jpg" id="pone.0324347.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo>*</mml:mo><mml:mi>K</mml:mi><mml:mi>\rightleft</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label>
</disp-formula><p>where <inline-formula id="pone.0324347.e002"><alternatives><graphic xlink:href="pone.0324347.e002.jpg" id="pone.0324347.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>I</mml:mi><mml:mo>*</mml:mo><mml:mi>K</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> depicts the layer that performs the convolution operation, <inline-formula id="pone.0324347.e003"><alternatives><graphic xlink:href="pone.0324347.e003.jpg" id="pone.0324347.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is the input matrix, 2D filter <inline-formula id="pone.0324347.e004"><alternatives><graphic xlink:href="pone.0324347.e004.jpg" id="pone.0324347.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> of <inline-formula id="pone.0324347.e005"><alternatives><graphic xlink:href="pone.0324347.e005.jpg" id="pone.0324347.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> size and <inline-formula id="pone.0324347.e006"><alternatives><graphic xlink:href="pone.0324347.e006.jpg" id="pone.0324347.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>is a 2D feature output map. By maintaining the threshold input at zero, the ReLU layer computes activation and increases the nonlinearity of the feature maps which can be expressed mathematically as:</p><disp-formula id="pone.0324347.e007">
<alternatives><graphic xlink:href="pone.0324347.e007.jpg" id="pone.0324347.e007g" position="anchor"/><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mi>X</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>max</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label>
</disp-formula><p>The layer of pooling reduces image attributes while retaining important information through Max Pooling, average Pooling, and sum Pooling. Max Pooling is the most common type. The FC layer classifies feature outputs using activation functions like soft-max or sigmoid for final decision-making.</p></sec></sec><sec id="sec006"><title>III. The proposed algorithm</title><p>The Smart Agriculture System aspires to cater to a multitude of user and system requirements to deliver an effective and user-friendly solution. From the user perspective, the system must be easy to use, allowing farmers with various technical backgrounds to access its benefits effortlessly. Real-time monitoring capabilities are paramount to enable timely decisions regarding irrigation, nutrients, and disease control. Security and privacy are the most prevalent concerns, and the system must ensure data protection and comply with privacy regulations. Compatibility with major smartphone platforms, affordability, and accessibility even in regions with limited connectivity are expected features. Customization options empower users to tailor the system to their specific crops and farming practices.</p><p>From the system&#x02019;s viewpoint, scalability is a fundamental requirement to accommodate the project&#x02019;s growth. Advanced data analytics capabilities are essential for processing sensor data, generating valuable insights, and offering recommendations. Reliability is a key aspect of minimizing downtime, while sufficient data storage, seamless integration with IoT sensors, and stringent data security measures are non-negotiable. Environmental compatibility and eco-friendliness are vital in today&#x02019;s context. Technical support must be readily available to assist users, and the system should adhere to industry standards and regulations, including data privacy rules. In addition, the provision of comprehensive documentation and user manuals ensures users can easily set up, operate, and troubleshoot the system. The Smart Agriculture System is driven by a commitment to align these user and system requirements to develop a comprehensive solution that enhances the efficiency, sustainability, and productivity of the agriculture sector. Therefore, <xref rid="pone.0324347.g003" ref-type="fig">Fig 3</xref> shows the major data flow diagram for the suggested model and associated tasks.</p><fig position="float" id="pone.0324347.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g003</object-id><label>Fig 3</label><caption><title>The main data flow diagram for the proposed framework and required tasks.</title></caption><graphic xlink:href="pone.0324347.g003" position="float"/></fig><p>The system employs n-tier architecture, a validated software model, to facilitate enterprise-level that run on clients and servers. It provides scalability, security, fault tolerance, reusability, and maintainability. The Use case of the proposed framework is introduced in <xref rid="pone.0324347.g004" ref-type="fig">Fig 4</xref>. Also, the main block diagram for the proposed framework is presented in <xref rid="pone.0324347.g005" ref-type="fig">Fig 5</xref>.</p><fig position="float" id="pone.0324347.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g004</object-id><label>Fig 4</label><caption><title>The Use case diagram for the implemented mobile application.</title></caption><graphic xlink:href="pone.0324347.g004" position="float"/></fig><fig position="float" id="pone.0324347.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g005</object-id><label>Fig 5</label><caption><title>The main block diagram of the suggested framework.</title></caption><graphic xlink:href="pone.0324347.g005" position="float"/></fig><p>Below is the structured pseudo-code that outlines the core functionality of the proposed Smart Agriculture Framework:</p><p specific-use="line"><bold>Initialize</bold> Sensors, Database, and Deep Learning Model</p><p specific-use="line"><bold>WHILE</bold> the System is Running <bold>DO</bold></p><p specific-use="line"><bold>&#x02003;Read</bold> Sensor Data (temperature, moisture, humidity)</p><p specific-use="line"><bold>&#x02003;Capture</bold> Plant Image</p><p specific-use="line">&#x02003;Store Data</p><p specific-use="line"><bold>&#x02003;IF</bold> Abnormal Conditions are Detected <bold>THEN</bold></p><p specific-use="line"><bold>&#x02003;&#x02003;Send</bold> Alert to Farmer</p><p specific-use="line"><bold>&#x02003;IF</bold> Disease Found in Image, <bold>THEN</bold></p><p specific-use="line"><bold>&#x02003;&#x02003;Identify</bold> Disease and Suggest Treatment</p><p specific-use="line"><bold>&#x02003;Provide</bold> Recommendations (Irrigation, Fertilizer, Pesticide)</p><p specific-use="line"><bold>&#x02003;Update</bold> System with New Data</p><p specific-use="line"><bold>END</bold> WHILE</p><p>The pseudo-code defines input variables such as temperature, soil moisture, humidity, and plant images, and output variables such as alerts, disease identification, and recommended actions (irrigation, fertilization, or pesticide application). This is implemented using Python as the primary programming language, particularly for deep learning and backend processing. Additionally, Sensor data and image data are used separately to make decisions. Sensor data, such as temperature, humidity, and soil moisture, is collected and analyzed in real-time. If abnormal conditions like high temperature or low moisture are detected, alerts are sent to the farmer to take appropriate actions, such as irrigation. On the other hand, plant images are captured and processed using deep learning models to detect diseases. If a disease is identified, it is classified, and recommendations for treatment, such as pesticide application, are provided.</p></sec><sec id="sec007"><title>IV. Evaluation metrics</title><p>The performance of the proposed model is assessed using different metrics such as accuracy, Boundary Recall, Precision, F measure, and Loss [<xref rid="pone.0324347.ref029" ref-type="bibr">29</xref>]. The assessment metrics are established utilizing formulae such as TP, TN, FP, and FN, which denote true positives, true negatives, false positives, and false negatives, respectively. True positives occur when the model accurately identifies the positive class, while false positives arise when the model erroneously predicts the negative class, and these variables are measured to calculate evaluation metrics as <xref rid="pone.0324347.e008" ref-type="disp-formula">Equations 3</xref>&#x02013;<xref rid="pone.0324347.e010" ref-type="disp-formula">5</xref>.</p><p>The accuracy refers to the percentage of correctly classified image pixels (Xiao et al. 2018).</p><disp-formula id="pone.0324347.e008">
<alternatives><graphic xlink:href="pone.0324347.e008.jpg" id="pone.0324347.e008g" position="anchor"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.167em"/><mml:mi>&#x02942;</mml:mi><mml:mi>&#x02942;</mml:mi><mml:mspace width="6mu"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="6mu"/><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.167em"/><mml:mi>&#x02942;</mml:mi><mml:mspace width="6mu"/><mml:mi>&#x02942;</mml:mi><mml:mspace width="0.167em"/><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mspace width="6mu"/><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(3)</label>
</disp-formula><p>Precision refers to the fraction of real positive predictions (properly identified positive cases) among all occurrences projected as positive. It helps to answer the question: How many of the positive examples categorized by the model were truly correct?</p><disp-formula id="pone.0324347.e009">
<alternatives><graphic xlink:href="pone.0324347.e009.jpg" id="pone.0324347.e009g" position="anchor"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(4)</label>
</disp-formula><p>Boundary Recall refers to the proportion of illness pixels in the ground truth that was successfully identified by automated segmentation. The sensitivity can be estimated by [<xref rid="pone.0324347.ref027" ref-type="bibr">27</xref>].</p><disp-formula id="pone.0324347.e010">
<alternatives><graphic xlink:href="pone.0324347.e010.jpg" id="pone.0324347.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.167em"/><mml:mi>&#x02942;</mml:mi><mml:mi>&#x02942;</mml:mi><mml:mspace width="6mu"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="6mu"/><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="6mu"/><mml:mspace width="6mu"/><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.167em"/><mml:mi>&#x02942;</mml:mi><mml:mspace width="6mu"/><mml:mi>&#x02942;</mml:mi><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="6mu"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="6mu"/><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="6mu"/><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="6mu"/><mml:mspace width="6mu"/><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives>
<label>(5)</label>
</disp-formula><p>The F measure evaluates precision and recall, ensuring high values for segmented images align with ground truth in close locations. The F measure can be estimated as [<xref rid="pone.0324347.ref030" ref-type="bibr">30</xref>]:</p><disp-formula id="pone.0324347.e011">
<alternatives><graphic xlink:href="pone.0324347.e011.jpg" id="pone.0324347.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mtable columnspacing="" displaystyle="true" rowspacing="3pt"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mfrac><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02295;</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives>
<label>(6)</label>
</disp-formula></sec><sec id="sec008"><title>V. Simulations and discussions</title><p>The proposed framework utilizes five deep learning algorithms for plant disease classification: MobileNet 1, MobileNet 2, ResNet 50, ResNet 50-Inception V3, and CNN. These models were chosen for their ability to process large-scale image datasets efficiently. MobileNet 1 and MobileNet 2 are lightweight architectures optimized for fast inference, making them ideal for real-time applications. ResNet 50 employs residual learning to facilitate deeper network training and improve accuracy. ResNet 50-Inception V3 integrates ResNet&#x02019;s depth with Inception&#x02019;s parallel convolutional layers, enhancing feature extraction. The CNN model serves as a baseline for comparison, offering a conventional deep-learning approach.</p><p>Training and evaluation were conducted using <bold>87,000 images spanning 38 plant classes</bold>, sourced from Kaggle for both healthy and non-healthy datasets [<xref rid="pone.0324347.ref031" ref-type="bibr">31</xref>]. Performance assessment was based on key metrics, including <bold>accuracy, precision, recall, F1 score, and loss</bold>, ensuring a comprehensive comparison of the models. The implemented technologies and tools that have been used to develop this framework are shown in <xref rid="pone.0324347.t002" ref-type="table">Table 2</xref>. The trained dataset can be presented in <xref rid="pone.0324347.t003" ref-type="table">Table 3</xref>. The subsequent section presents the detailed evaluation results.</p><table-wrap position="float" id="pone.0324347.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.t002</object-id><label>Table 2</label><caption><title>Used technologies and tools.</title></caption><alternatives><graphic xlink:href="pone.0324347.t002" id="pone.0324347.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" colspan="6" rowspan="1">Software Requirements</th><th align="left" rowspan="1" colspan="1">Hardware Requirements</th></tr><tr><th align="left" rowspan="1" colspan="1">Front-end</th><th align="left" rowspan="1" colspan="1">Flutter App</th><th align="left" rowspan="1" colspan="1">Back-end</th><th align="left" rowspan="1" colspan="1">Database</th><th align="left" rowspan="1" colspan="1">Deep Learning</th><th align="left" rowspan="1" colspan="1">IDE</th><th align="left" rowspan="1" colspan="1">Hardware</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Html-CSS<break/>JavaScript<break/>Bootstrap<break/>jQuery<break/>Typescript<break/>Angular 16</td><td align="left" rowspan="1" colspan="1">Dart<break/>Bloc State Management<break/>Firebase<break/>HTTP</td><td align="left" rowspan="1" colspan="1">Nodejs<break/>Postman<break/>Express JS</td><td align="left" rowspan="1" colspan="1">Mongoose</td><td align="left" rowspan="1" colspan="1">Resnet 50<break/>Inception Net<break/>CNN</td><td align="left" rowspan="1" colspan="1">VS-Code<break/>Android Studio<break/>PyCharm<break/>Arduino IDE<break/>Jupyter<break/>Google Colab</td><td align="left" rowspan="1" colspan="1">Node-MCU (ESP-8266)<break/>Arduino UNO<break/>ESP-32<break/>ESP-Cam<break/>Soil Moisture<break/>Water Level<break/>Water Pump<break/>DHT22<break/>NPK<break/>Fan<break/>Rain Drop<break/>Flame Sensor<break/>Resistors<break/>Transistors<break/>Capacitors<break/>Jumper Cable<break/>Relay Module</td></tr></tbody></table></alternatives></table-wrap><table-wrap position="float" id="pone.0324347.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.t003</object-id><label>Table 3</label><caption><title>The trained dataset for 38 different classes of plant leaves.</title></caption><alternatives><graphic xlink:href="pone.0324347.t003" id="pone.0324347.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Plant Type</th><th align="left" rowspan="1" colspan="1">Plant Class</th><th align="left" rowspan="1" colspan="1">Trained Dataset</th><th align="left" rowspan="1" colspan="1">Tested Dataset</th></tr></thead><tbody><tr><td align="left" rowspan="4" colspan="1">Apple</td><td align="left" rowspan="1" colspan="1">Apple_scab</td><td align="left" rowspan="1" colspan="1">2016</td><td align="left" rowspan="1" colspan="1">504</td></tr><tr><td align="left" rowspan="1" colspan="1">Black_rot</td><td align="left" rowspan="1" colspan="1">1987</td><td align="left" rowspan="1" colspan="1">497</td></tr><tr><td align="left" rowspan="1" colspan="1">Cedar_apple_rust</td><td align="left" rowspan="1" colspan="1">1760</td><td align="left" rowspan="1" colspan="1">440</td></tr><tr><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">2008</td><td align="left" rowspan="1" colspan="1">502</td></tr><tr><td align="left" rowspan="1" colspan="1">Blueberry</td><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">1816</td><td align="left" rowspan="1" colspan="1">454</td></tr><tr><td align="left" rowspan="2" colspan="1">Cherry_(including_sour)</td><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">1826</td><td align="left" rowspan="1" colspan="1">456</td></tr><tr><td align="left" rowspan="1" colspan="1">Powdery_mildew</td><td align="left" rowspan="1" colspan="1">1683</td><td align="left" rowspan="1" colspan="1">421</td></tr><tr><td align="left" rowspan="4" colspan="1">Corn_(maize)</td><td align="left" rowspan="1" colspan="1">Cercospora_leaf_spot Gray_leaf_spot</td><td align="left" rowspan="1" colspan="1">1642</td><td align="left" rowspan="1" colspan="1">410</td></tr><tr><td align="left" rowspan="1" colspan="1">Common_rust</td><td align="left" rowspan="1" colspan="1">1907</td><td align="left" rowspan="1" colspan="1">477</td></tr><tr><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">1859</td><td align="left" rowspan="1" colspan="1">465</td></tr><tr><td align="left" rowspan="1" colspan="1">Northern_Leaf_Blight</td><td align="left" rowspan="1" colspan="1">1908</td><td align="left" rowspan="1" colspan="1">477</td></tr><tr><td align="left" rowspan="4" colspan="1">Grape</td><td align="left" rowspan="1" colspan="1">Black_rot</td><td align="left" rowspan="1" colspan="1">1888</td><td align="left" rowspan="1" colspan="1">472</td></tr><tr><td align="left" rowspan="1" colspan="1">Esca_(Black_Measles)</td><td align="left" rowspan="1" colspan="1">1920</td><td align="left" rowspan="1" colspan="1">480</td></tr><tr><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">1692</td><td align="left" rowspan="1" colspan="1">423</td></tr><tr><td align="left" rowspan="1" colspan="1">Leaf_blight_(Isariopsis_Leaf_Spot)</td><td align="left" rowspan="1" colspan="1">1722</td><td align="left" rowspan="1" colspan="1">430</td></tr><tr><td align="left" rowspan="1" colspan="1">Orange</td><td align="left" rowspan="1" colspan="1">Haunglongbing_(Citrus_greening)</td><td align="left" rowspan="1" colspan="1">2010</td><td align="left" rowspan="1" colspan="1">503</td></tr><tr><td align="left" rowspan="2" colspan="1">Peach</td><td align="left" rowspan="1" colspan="1">Bacterial_spot</td><td align="left" rowspan="1" colspan="1">1838</td><td align="left" rowspan="1" colspan="1">459</td></tr><tr><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">1728</td><td align="left" rowspan="1" colspan="1">432</td></tr><tr><td align="left" rowspan="2" colspan="1">Pepper,_bell</td><td align="left" rowspan="1" colspan="1">Bacterial_spot</td><td align="left" rowspan="1" colspan="1">1913</td><td align="left" rowspan="1" colspan="1">478</td></tr><tr><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">1988</td><td align="left" rowspan="1" colspan="1">497</td></tr><tr><td align="left" rowspan="3" colspan="1">Potato</td><td align="left" rowspan="1" colspan="1">Early_blight</td><td align="left" rowspan="1" colspan="1">1939</td><td align="left" rowspan="1" colspan="1">485</td></tr><tr><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">1824</td><td align="left" rowspan="1" colspan="1">456</td></tr><tr><td align="left" rowspan="1" colspan="1">Late_blight</td><td align="left" rowspan="1" colspan="1">1939</td><td align="left" rowspan="1" colspan="1">485</td></tr><tr><td align="left" rowspan="1" colspan="1">Raspberry</td><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">1781</td><td align="left" rowspan="1" colspan="1">445</td></tr><tr><td align="left" rowspan="1" colspan="1">Soybean</td><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">2022</td><td align="left" rowspan="1" colspan="1">505</td></tr><tr><td align="left" rowspan="1" colspan="1">Squash</td><td align="left" rowspan="1" colspan="1">Powdery_mildew</td><td align="left" rowspan="1" colspan="1">1736</td><td align="left" rowspan="1" colspan="1">434</td></tr><tr><td align="left" rowspan="2" colspan="1">Strawberry</td><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">1824</td><td align="left" rowspan="1" colspan="1">456</td></tr><tr><td align="left" rowspan="1" colspan="1">Leaf_scorch</td><td align="left" rowspan="1" colspan="1">1774</td><td align="left" rowspan="1" colspan="1">444</td></tr><tr><td align="left" rowspan="10" colspan="1">Tomato</td><td align="left" rowspan="1" colspan="1">Bacterial_spot</td><td align="left" rowspan="1" colspan="1">1702</td><td align="left" rowspan="1" colspan="1">425</td></tr><tr><td align="left" rowspan="1" colspan="1">Early_blight</td><td align="left" rowspan="1" colspan="1">1920</td><td align="left" rowspan="1" colspan="1">480</td></tr><tr><td align="left" rowspan="1" colspan="1">healthy</td><td align="left" rowspan="1" colspan="1">1926</td><td align="left" rowspan="1" colspan="1">481</td></tr><tr><td align="left" rowspan="1" colspan="1">Late_blight</td><td align="left" rowspan="1" colspan="1">1851</td><td align="left" rowspan="1" colspan="1">463</td></tr><tr><td align="left" rowspan="1" colspan="1">Leaf_Mold</td><td align="left" rowspan="1" colspan="1">1882</td><td align="left" rowspan="1" colspan="1">470</td></tr><tr><td align="left" rowspan="1" colspan="1">Septoria_leaf_spot</td><td align="left" rowspan="1" colspan="1">1745</td><td align="left" rowspan="1" colspan="1">436</td></tr><tr><td align="left" rowspan="1" colspan="1">Spider_mites Two-spotted_spider_mite</td><td align="left" rowspan="1" colspan="1">1741</td><td align="left" rowspan="1" colspan="1">435</td></tr><tr><td align="left" rowspan="1" colspan="1">Target_Spot</td><td align="left" rowspan="1" colspan="1">1827</td><td align="left" rowspan="1" colspan="1">457</td></tr><tr><td align="left" rowspan="1" colspan="1">Tomato_mosaic_virus</td><td align="left" rowspan="1" colspan="1">1790</td><td align="left" rowspan="1" colspan="1">448</td></tr><tr><td align="left" rowspan="1" colspan="1">Tomato_Yellow_Leaf_Curl_Virus</td><td align="left" rowspan="1" colspan="1">1961</td><td align="left" rowspan="1" colspan="1">490</td></tr></tbody></table></alternatives></table-wrap><p>In the proposed framework, five deep learning algorithms have been adopted to achieve the highest performance that can be practically implemented for such a large dataset. These algorithms are MobileNet 1, MobileNet 2, ResNet 50, ResNet 50- Inception V3, and CNN. The input datasets have been trained for all deep learning algorithms and evaluated based on different evaluation metrics such as accuracy, precision, recall, F1 score, and loss. From the obtained results, it has been shown that ResNet 50 with InceptionV3 has introduced the highest performance with the highest accuracy value of 92.9% and precision of 92.96%. The comparison results are introduced in <xref rid="pone.0324347.t004" ref-type="table">Table 4</xref>. Also, the confusion matrix is presented in <xref rid="pone.0324347.g006" ref-type="fig">Fig 6</xref>.</p><table-wrap position="float" id="pone.0324347.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.t004</object-id><label>Table 4</label><caption><title>The performance evaluation for different deep learning depicted approaches.</title></caption><alternatives><graphic xlink:href="pone.0324347.t004" id="pone.0324347.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">Accuracy</th><th align="left" rowspan="1" colspan="1">Precision</th><th align="left" rowspan="1" colspan="1">Recall</th><th align="left" rowspan="1" colspan="1">F1-Score</th><th align="left" rowspan="1" colspan="1">Loss</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<bold>MobileNet 1</bold>
</td><td align="left" rowspan="1" colspan="1">91.31%</td><td align="left" rowspan="1" colspan="1">91.36%</td><td align="left" rowspan="1" colspan="1">91.30%</td><td align="left" rowspan="1" colspan="1">91.39%</td><td align="left" rowspan="1" colspan="1">0.0014</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>MobileNet 2</bold>
</td><td align="left" rowspan="1" colspan="1">92.12%</td><td align="left" rowspan="1" colspan="1">92.23%</td><td align="left" rowspan="1" colspan="1">92.25%</td><td align="left" rowspan="1" colspan="1">92.29%</td><td align="left" rowspan="1" colspan="1">1.32E-04</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>ResNet 50</bold>
</td><td align="left" rowspan="1" colspan="1">92.85%</td><td align="left" rowspan="1" colspan="1">92.91%</td><td align="left" rowspan="1" colspan="1">92.85%</td><td align="left" rowspan="1" colspan="1">92.85%</td><td align="left" rowspan="1" colspan="1">1.88E-04</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>ResNet 50 - InceptionV3</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>92.90%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>92.96%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>92.98%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>92.97%</bold>
</td><td align="left" rowspan="1" colspan="1">
<bold>0.082</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>CNN</bold>
</td><td align="left" rowspan="1" colspan="1">90.23%</td><td align="left" rowspan="1" colspan="1">90.26%</td><td align="left" rowspan="1" colspan="1">90.13%</td><td align="left" rowspan="1" colspan="1">90.30%</td><td align="left" rowspan="1" colspan="1">6.67E-04</td></tr></tbody></table></alternatives></table-wrap><fig position="float" id="pone.0324347.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g006</object-id><label>Fig 6</label><caption><title>Confusion matrix for 38-class plant disease classification based on the proposed ResNet-50-InceptionV3 model.</title></caption><graphic xlink:href="pone.0324347.g006" position="float"/></fig><p>In addition to reporting performance metrics, we conducted statistical significance testing to validate the differences in model performance. A paired t-test between ResNet-50-InceptionV3 and CNN yielded a t-statistic of 13.90 and a p-value&#x02009;&#x0003c;&#x02009;0.001, indicating that the performance difference is statistically significant. The 95% confidence interval of the mean difference was between 225.81% and 313.57%. Furthermore, a one-way ANOVA test across all models confirmed significant differences in their accuracies (F(4,45) = 137.06, p&#x02009;&#x0003c;&#x02009;0.0001), as shown in <xref rid="pone.0324347.t005" ref-type="table">Table 5</xref>.</p><table-wrap position="float" id="pone.0324347.t005"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.t005</object-id><label>Table 5</label><caption><title>Statistical test results.</title></caption><alternatives><graphic xlink:href="pone.0324347.t005" id="pone.0324347.t005g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Test</th><th align="left" rowspan="1" colspan="1">Statistic</th><th align="left" rowspan="1" colspan="1">p-value</th><th align="left" rowspan="1" colspan="1">95% Confidence Interval</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Paired t-test (ResNet-50-InceptionV3 vs CNN)</td><td align="left" rowspan="1" colspan="1">13.9036</td><td align="left" rowspan="1" colspan="1">0.0001</td><td align="left" rowspan="1" colspan="1">225.81% to 313.57%</td></tr><tr><td align="left" rowspan="1" colspan="1">One-way ANOVA (All Models)</td><td align="left" rowspan="1" colspan="1">137.0581</td><td align="left" rowspan="1" colspan="1">0.0001</td><td align="left" rowspan="1" colspan="1"/></tr></tbody></table></alternatives></table-wrap><p>Hyperparameters are essential factors that influence the performance of a deep learning model, such as batch size and epochs. The proposed models have been trained using batch-size&#x02009;=&#x02009;100 and epochs&#x02009;=&#x02009;40 and evaluated using ROC curves to visualize the training process. The selection of hyperparameters is aimed at balancing model training efficiency and performance. A batch size of 100 strikes a balance between computational efficiency and model stability besides processing data without compromising the quality of model updates. Smaller batch sizes might slow down the training process but offer more frequent updates, while larger sizes might reduce update frequency and affect convergence.</p><p>Setting the number of epochs to 40 allows the model sufficient iterations to learn from the data without risking overfitting. Fewer epochs could lead to underfitting, while more could cause overfitting. With 40 epochs, the model is expected to learn effectively, especially when early stopping is used to prevent unnecessary training once performance plateaus.</p><p>The Adam optimizer is used due to its ability to automatically adjust learning rates, which helps in faster convergence and efficient training, particularly with complex deep learning models. Adam is effective at handling varying gradients during training. To reduce overfitting, <bold>dropout</bold> and L2 regularization are employed to encourage the model to generalize better by preventing it from becoming too reliant on any specific feature.</p><p>These hyperparameters and optimization choices are designed to ensure efficient training while maximizing model generalization and accuracy on new data. This can be introduced in <xref rid="pone.0324347.g007" ref-type="fig">Figs 7</xref>&#x02013;<xref rid="pone.0324347.g010" ref-type="fig">10</xref>.</p><fig position="float" id="pone.0324347.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g007</object-id><label>Fig 7</label><caption><title>The Roc for the training process of the proposed ResNet 50 with the InceptionV3 algorithm.</title></caption><graphic xlink:href="pone.0324347.g007" position="float"/></fig><fig position="float" id="pone.0324347.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g008</object-id><label>Fig 8</label><caption><title>The Roc for the training process of the proposed MobileNet-1 algorithm.</title></caption><graphic xlink:href="pone.0324347.g008" position="float"/></fig><fig position="float" id="pone.0324347.g009"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g009</object-id><label>Fig 9</label><caption><title>The Roc for the training process of the proposed MobileNet-2 algorithm.</title></caption><graphic xlink:href="pone.0324347.g009" position="float"/></fig><fig position="float" id="pone.0324347.g010"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g010</object-id><label>Fig 10</label><caption><title>The Roc for the training process of the CNN algorithm.</title></caption><graphic xlink:href="pone.0324347.g010" position="float"/></fig></sec><sec id="sec009"><title>VI. Model deployment</title><p>The main objective of this work is to design and implement an integrated framework for monitoring and management of agricultural systems. The following steps illustrate deploying the proposed deep learning model with an application and uploading it to a Railway server while creating an API with Flask.</p><list list-type="order"><list-item><p><bold>Preparing the Model:</bold> Ensure that the deep learning model is trained and ready for deployment. This includes saving the trained model weights, architecture, and any necessary pre-processing steps.</p></list-item><list-item><p><bold>Setting up the Application</bold>: Create a web application structure that will serve as the interface for the model. This typically includes files for handling HTTP requests, HTML templates for rendering the user interface, and static files like CSS and JavaScript for styling and interactivity.</p></list-item><list-item><p><bold>Flask API Development</bold>: Use Flask, a Python micro-framework, to create a RESTful API that will expose endpoints for interacting with the deep learning model. Define routes that correspond to different functionalities of the model, such as predicting outcomes or providing status updates.</p></list-item><list-item><p><bold>Integration with the Model</bold>: Integrate the proposed deep learning model into the Flask application. This involves loading the model from disk, initializing it, and defining functions or methods to make predictions based on input data received through API requests.</p></list-item><list-item><p><bold>Testing Locally</bold>: Before deploying to a server, the Flask application has been tested locally to ensure that it functions as expected using the Postman tool and curl to send HTTP requests to the API endpoints and verify their response.</p></list-item><list-item><p><bold>Setting up Railway Server:</bold> Sign up for an account on Railway and create a new project. Follow the instructions provided by Railway to set up the project and configure the deployment environment.</p></list-item><list-item><p><bold>Deploying to Railway:</bold> Once the project is set up, use Railway&#x02019;s deployment tools to upload the Flask application and any associated files. The Railway will handle the deployment process, including setting up the server environment and managing dependencies.</p></list-item><list-item><p><bold>Accessing the API:</bold> Once deployed, the Flask API will be accessible via a public URL provided by Railway. This URL can be used to send requests to the API from other applications or services.</p></list-item><list-item><p><bold>Monitoring and Maintenance:</bold> Monitor the performance of the deployed application and make any necessary updates or improvements over time. Railway provides tools for monitoring server metrics and managing deployed applications.</p></list-item></list><p><bold>On AI screen,</bold> the capability to capture images using the device&#x02019;s camera or select photos from a gallery to send to the artificial intelligence system is available. The AI analyzes these images to assess the health status of the plants, identify any diseases present, and recommend the appropriate treatments.</p><p>Upon submission, some details results and information about the plant is presented, including its name, current condition, and recommended actions. Whether the plant is suffering from disease or simply needs fertilizers, the proposed AI model provides valuable insights to help farmers maintain their farms.</p></sec><sec id="sec010"><title>VII. Practical implementations</title><p>This practical implementation utilizes a smart IoT integrated system to enhance sustainable agriculture by automating key processes. Soil moisture sensor monitors water levels, ensuring optimal irrigation while conserving resources. Rain-drop sensor triggers irrigation only when needed, and the heat sensor works alongside fans to maintain ideal temperature conditions. Fire detection systems safeguard crops from potential hazards, while a water pump automates irrigation processes. The ESP32 serves as the central controller, enabling seamless communication among all devices. This comprehensive setup enhances crop productivity, reduces waste, and promotes sustainable farming practices. The implemented prototype can be shown in <xref rid="pone.0324347.g011" ref-type="fig">Fig 11</xref>.</p><fig position="float" id="pone.0324347.g011"><object-id pub-id-type="doi">10.1371/journal.pone.0324347.g011</object-id><label>Fig 11</label><caption><title>The implemented prototype for the proposed integrated IoT system.</title></caption><graphic xlink:href="pone.0324347.g011" position="float"/></fig></sec><sec sec-type="conclusions" id="sec011"><title>VIII. Conclusion</title><p>In conclusion, this research outlines a comprehensive integrated IoT system for smart agriculture that aims to revolutionize farming practices. By employing innovative technologies to monitor plant health, detect diseases, and recommend tailored solutions for pesticides and fertilizers, the system enhances productivity and crop quality. Additionally, smart irrigation solutions promote efficient water usage, while continuous weather and temperature monitoring ensures that farmers can respond proactively to environmental changes. Accessible through a dedicated mobile application, this integrated approach empowers farmers, agricultural engineers, and veterinarians, facilitating effective management and ultimately leading to increased agricultural productivity and sustainability. The proposed deep learning algorithm has been evaluated and introduced excellent performance for training 38 different types of plant diseases and achieved an accuracy of 92.9% and precision of 92.98%. The deployed mobile application has been tested locally to verify its response using the Postman tool. In future work, we plan to enhance the proposed intelligent framework by integrating IoT-based smart irrigation management systems to improve water resource efficiency and agricultural sustainability [<xref rid="pone.0324347.ref032" ref-type="bibr">32</xref>]. Additionally, we aim to incorporate renewable energy sources and precision robotics to optimize energy consumption and automate field operations [<xref rid="pone.0324347.ref033" ref-type="bibr">33</xref>]. The framework can also be extended to include intelligent food waste management strategies to promote sustainable agriculture [<xref rid="pone.0324347.ref034" ref-type="bibr">34</xref>]. Furthermore, advanced machine learning techniques such as hybrid stacked models and feature selection approaches will be explored to improve classification accuracy and decision-making efficiency, following recent studies in water potability classification and food consumption forecasting [<xref rid="pone.0324347.ref035" ref-type="bibr">35</xref>,<xref rid="pone.0324347.ref036" ref-type="bibr">36</xref>].</p></sec></body><back><ref-list><title>References</title><ref id="pone.0324347.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Abba</surname><given-names>S</given-names></name>, <name><surname>Namkusong</surname><given-names>JW</given-names></name>, <name><surname>Lee</surname><given-names>JA</given-names></name>, <name><surname>Crespo</surname><given-names>ML</given-names></name>. <article-title>Design and performance evaluation of a low-cost autonomous sensor interface for a smart iot-based irrigation monitoring and control system</article-title>. <source>Sensors (Basel).</source>
<year>2019</year>;<volume>19</volume>(<issue>17</issue>):<fpage>3643</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/s19173643</pub-id>
<pub-id pub-id-type="pmid">31438597</pub-id>
</mixed-citation></ref><ref id="pone.0324347.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Ahmed</surname><given-names>N</given-names></name>, <name><surname>De</surname><given-names>D</given-names></name>, <name><surname>Hussain</surname><given-names>I</given-names></name>. <article-title>Internet of things (IoT) for smart precision agriculture an`d farming in rural areas</article-title>. <source>IEEE Internet Things J</source>. <year>2018</year>;<volume>5</volume>(<issue>6</issue>):<fpage>4890</fpage>&#x02013;<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jiot.2018.2879579</pub-id></mixed-citation></ref><ref id="pone.0324347.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Pramanik</surname><given-names>M</given-names></name>, <name><surname>Khanna</surname><given-names>M</given-names></name>, <name><surname>Singh</surname><given-names>M</given-names></name>, <name><surname>Singh</surname><given-names>DK</given-names></name>, <name><surname>Sudhishri</surname><given-names>S</given-names></name>, <name><surname>Bhatia</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Automation of soil moisture sensor-based basin irrigation system</article-title>. <source>Smart Agric Technol</source>. <year>2022</year>;<volume>2</volume>:<fpage>100032</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.atech.2021.100032</pub-id></mixed-citation></ref><ref id="pone.0324347.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Rahman</surname><given-names>MW</given-names></name>, <name><surname>Hossain</surname><given-names>ME</given-names></name>, <name><surname>Islam</surname><given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Real-time and low-cost IoT-based farming using Raspberry Pi.</article-title>
<source>Indones J Electr Eng Comput Sci</source>. <year>2020</year>;<volume>17</volume>:<fpage>197</fpage>&#x02013;<lpage>204</lpage>.</mixed-citation></ref><ref id="pone.0324347.ref005"><label>5</label><mixed-citation publication-type="book"><name><surname>Alsakar</surname><given-names>YM</given-names></name>, <name><surname>Sakr</surname><given-names>NA</given-names></name>, <name><surname>Elmogy</surname><given-names>M</given-names></name>. <source>Plant disease detection and classification using machine learning and deep learning techniques: current trends and challenges. In: Green Sustainability: Towards Innovative Digital Transformation, vol. 753</source>. <publisher-loc>Singapore</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2023</year>. p. <fpage>197</fpage>&#x02013;<lpage>217</lpage>.</mixed-citation></ref><ref id="pone.0324347.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Araujo</surname><given-names>JMM</given-names></name>, <name><surname>Peixoto</surname><given-names>ZMA</given-names></name>. <article-title>A new proposal for automatic identification of multiple soybean diseases</article-title>. <source>Comput Electron Agric</source>. <year>2019</year>;<volume>167</volume>:<fpage>105060</fpage>.</mixed-citation></ref><ref id="pone.0324347.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Shrivastava</surname><given-names>VK</given-names></name>, <name><surname>Pradhan</surname><given-names>MK</given-names></name>. <article-title>Rice plant disease classification using color features: A machine learning paradigm</article-title>. <source>J Plant Pathol</source>. <year>2020</year>;<volume>103</volume>(<issue>1</issue>):<fpage>17</fpage>&#x02013;<lpage>26</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s42161-020-00683-3</pub-id></mixed-citation></ref><ref id="pone.0324347.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Chowdhury</surname><given-names>MEH</given-names></name>, <name><surname>Rahman</surname><given-names>T</given-names></name>, <name><surname>Khandakar</surname><given-names>A</given-names></name>, <name><surname>Ayari</surname><given-names>MA</given-names></name>, <name><surname>Khan</surname><given-names>AU</given-names></name>, <name><surname>Khan</surname><given-names>MS</given-names></name>, <etal>et al</etal>. <article-title>Automatic and reliable leaf disease detection using deep learning techniques</article-title>. <source>AgriEngineering</source>. <year>2021</year>;<volume>3</volume>(<issue>2</issue>):<fpage>294</fpage>&#x02013;<lpage>312</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/agriengineering3020020</pub-id></mixed-citation></ref><ref id="pone.0324347.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Sharma</surname><given-names>R</given-names></name>, <name><surname>Singh</surname><given-names>A</given-names></name>, <name><surname>Jhanjhi</surname><given-names>NZ</given-names></name>, <name><surname>Masud</surname><given-names>M</given-names></name>, <name><surname>Jaha</surname><given-names>ES</given-names></name>, <etal>et al</etal>. <article-title>Plant disease diagnosis and image classification using deep learning</article-title>. <source>Comput, Mater Continua</source>. <year>2022</year>;<volume>71</volume>(<issue>2</issue>):<fpage>2125</fpage>&#x02013;<lpage>40</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.32604/cmc.2022.020017</pub-id></mixed-citation></ref><ref id="pone.0324347.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Haridasan</surname><given-names>A</given-names></name>, <name><surname>Thomas</surname><given-names>J</given-names></name>, <name><surname>Raj</surname><given-names>ED</given-names></name>. <article-title>Deep learning system for paddy plant disease detection and classification</article-title>. <source>Environ Monit Assess</source>. <year>2022</year>;<volume>195</volume>(<issue>1</issue>):<fpage>120</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10661-022-10656-x</pub-id>
<pub-id pub-id-type="pmid">36399232</pub-id>
</mixed-citation></ref><ref id="pone.0324347.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Nagamani</surname><given-names>HS</given-names></name>, <name><surname>Sarojadevi</surname><given-names>H</given-names></name>. <article-title>Tomato Leaf Disease Detection using Deep Learning Techniques</article-title>. <source>IJACSA</source>. <year>2022</year>;<volume>13</volume>(<issue>1</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.14569/ijacsa.2022.0130138</pub-id></mixed-citation></ref><ref id="pone.0324347.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>N</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Fertigation management for sustainable precision agriculture based on internet of things</article-title>. <source>J Clean Prod</source>. <year>2020</year>;<issue>277</issue>:<fpage>124119</fpage>.</mixed-citation></ref><ref id="pone.0324347.ref013"><label>13</label><mixed-citation publication-type="book"><name><surname>Raut</surname><given-names>R</given-names></name>, <name><surname>Varma</surname><given-names>H</given-names></name>, <name><surname>Mulla</surname><given-names>C</given-names></name>, <name><surname>Pawar</surname><given-names>VR</given-names></name>. <article-title>Soil monitoring, fertigation, and irrigation system using IoT for agricultural application.</article-title> In: <source>Intelligent communication and computational technologies</source>. <publisher-loc>Singapore</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2018</year>. p. <fpage>67</fpage>&#x02013;<lpage>73</lpage>.</mixed-citation></ref><ref id="pone.0324347.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Morchid</surname><given-names>A</given-names></name>, <name><surname>El Alami</surname><given-names>R</given-names></name>, <name><surname>Raezah</surname><given-names>AA</given-names></name>, <name><surname>Sabbar</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Applications of internet of things (IoT) and sensors technology to increase food security and agricultural sustainability: Benefits and challenges</article-title>. <source>Ain Shams Eng J</source>. <year>2024</year>;<volume>15</volume>(<issue>3</issue>):<fpage>102509</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.asej.2023.102509</pub-id></mixed-citation></ref><ref id="pone.0324347.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Blakeney</surname><given-names>M</given-names></name>. <article-title>Agricultural innovation and sustainable development</article-title>. <source>Sustainability</source>. <year>2022</year>;<volume>14</volume>(<issue>5</issue>):<fpage>2698</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/su14052698</pub-id></mixed-citation></ref><ref id="pone.0324347.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Elijah</surname><given-names>O</given-names></name>, <name><surname>Rahman</surname><given-names>TA</given-names></name>, <name><surname>Orikumhi</surname><given-names>I</given-names></name>, <name><surname>Leow</surname><given-names>CY</given-names></name>, <name><surname>Hindia</surname><given-names>MHDN</given-names></name>. <article-title>An overview of internet of things (IoT) and data analytics in agriculture: Benefits and challenges</article-title>. <source>IEEE Internet Things J</source>. <year>2018</year>;<volume>5</volume>(<issue>5</issue>):<fpage>3758</fpage>&#x02013;<lpage>73</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jiot.2018.2844296</pub-id></mixed-citation></ref><ref id="pone.0324347.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Ayaz</surname><given-names>M</given-names></name>, <name><surname>Ammad-Uddin</surname><given-names>M</given-names></name>, <name><surname>Sharif</surname><given-names>Z</given-names></name>, <name><surname>Mansour</surname><given-names>A</given-names></name>, <name><surname>Aggoune</surname><given-names>EHM</given-names></name>. <article-title>Internet-of-Things (IoT)-based smart agriculture: Toward making the fields talk</article-title>. <source>IEEE Access</source>. <year>2019</year>;<volume>7</volume>:<fpage>129551</fpage>&#x02013;<lpage>83</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/access.2019.2932609</pub-id></mixed-citation></ref><ref id="pone.0324347.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Giller</surname><given-names>KE</given-names></name>, <name><surname>Delaune</surname><given-names>T</given-names></name>, <name><surname>Silva</surname><given-names>JV</given-names></name>, <name><surname>Descheemaeker</surname><given-names>K</given-names></name>, <name><surname>van de Ven</surname><given-names>G</given-names></name>, <name><surname>Schut</surname><given-names>AGT</given-names></name>, <etal>et al</etal>. <article-title>The future of farming: Who will produce our food?</article-title>
<source>Food Sec</source>. <year>2021</year>;<volume>13</volume>(<issue>5</issue>):<fpage>1073</fpage>&#x02013;<lpage>99</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s12571-021-01184-6</pub-id></mixed-citation></ref><ref id="pone.0324347.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Kamienski</surname><given-names>C</given-names></name>, <name><surname>Soininen</surname><given-names>J-P</given-names></name>, <name><surname>Taumberger</surname><given-names>M</given-names></name>, <name><surname>Dantas</surname><given-names>R</given-names></name>, <name><surname>Toscano</surname><given-names>A</given-names></name>, <name><surname>Salmon Cinotti</surname><given-names>T</given-names></name>, <etal>et al</etal>. <article-title>Smart water management platform: IoT-based precision irrigation for agriculture</article-title>. <source>Sensors (Basel)</source>. <year>2019</year>;<volume>19</volume>(<issue>2</issue>):<fpage>276</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/s19020276</pub-id>
<pub-id pub-id-type="pmid">30641960</pub-id>
</mixed-citation></ref><ref id="pone.0324347.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Abbassi</surname><given-names>Y</given-names></name>, <name><surname>Benlahmer</surname><given-names>H</given-names></name>. <article-title>The internet of things at the service of tomorrow&#x02019;s agriculture</article-title>. <source>Procedia Comput Sci</source>. <year>2021</year>;<volume>191</volume>:<fpage>475</fpage>&#x02013;<lpage>80</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.procs.2021.07.060</pub-id></mixed-citation></ref><ref id="pone.0324347.ref021"><label>21</label><mixed-citation publication-type="book"><name><surname>Abraham</surname><given-names>A</given-names></name>, <name><surname>Dash</surname><given-names>S</given-names></name>, <name><surname>Rodrigues</surname><given-names>JJPC</given-names></name>, <etal>et al</etal>. <source>AI, Edge and IoT-based smart agriculture</source>. <publisher-name>Academic Press</publisher-name>; <year>2021</year>.</mixed-citation></ref><ref id="pone.0324347.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Hern&#x000e1;ndez-Morales</surname><given-names>CA</given-names></name>, <name><surname>Luna-Rivera</surname><given-names>JM</given-names></name>, <name><surname>Perez-Jimenez</surname><given-names>R</given-names></name>. <article-title>Design and deployment of a practical IoT-based monitoring system for protected cultivations</article-title>. <source>Comput Commun</source>. <year>2022</year>;<volume>186</volume>:<fpage>51</fpage>&#x02013;<lpage>64</lpage>.</mixed-citation></ref><ref id="pone.0324347.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Chamara</surname><given-names>N</given-names></name>, <name><surname>Islam</surname><given-names>MDD</given-names></name>, <name><surname>Bai</surname><given-names>GF</given-names></name>, <name><surname>Shi</surname><given-names>Y</given-names></name>, <name><surname>Ge</surname><given-names>Y</given-names></name>. <article-title>Ag-IoT for crop and environment monitoring: Past, present, and future</article-title>. <source>Agricultural Systems</source>. <year>2022</year>;<volume>203</volume>:<fpage>103497</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.agsy.2022.103497</pub-id></mixed-citation></ref><ref id="pone.0324347.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Abi Saab</surname><given-names>MT</given-names></name>, <name><surname>Jomaa</surname><given-names>I</given-names></name>, <name><surname>Skaf</surname><given-names>S</given-names></name>, <name><surname>Fahed</surname><given-names>S</given-names></name>, <name><surname>Todorovic</surname><given-names>M</given-names></name>. <article-title>Assessment of a smartphone application for real-time irrigation scheduling in mediterranean environments</article-title>. <source>Water</source>. <year>2019</year>;<volume>11</volume>(<issue>2</issue>):<fpage>252</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/w11020252</pub-id></mixed-citation></ref><ref id="pone.0324347.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Habib</surname><given-names>S</given-names></name>, <name><surname>Alyahya</surname><given-names>S</given-names></name>, <name><surname>Islam</surname><given-names>M</given-names></name>, <name><surname>Alnajim</surname><given-names>AM</given-names></name>, <name><surname>Alabdulatif</surname><given-names>A</given-names></name>, <name><surname>Alabdulatif</surname><given-names>A</given-names></name>. <article-title>Design and implementation: An IoT-framework-based automated waste water irrigation system</article-title>. <source>Electronics</source>. <year>2022</year>;<volume>12</volume>(<issue>1</issue>):<fpage>28</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/electronics12010028</pub-id></mixed-citation></ref><ref id="pone.0324347.ref026"><label>26</label><mixed-citation publication-type="book"><name><surname>Anthonys</surname><given-names>G</given-names></name>, <name><surname>Wickramarachchi</surname><given-names>N</given-names></name>. <article-title>An image recognition system for crop disease identification of paddy fields in Sri Lanka.</article-title> In: <source>2009 Int Conf Ind Inf Syst</source>. <publisher-name>IEEE</publisher-name>. <year>2009</year>. <fpage>403</fpage>&#x02013;<lpage>7</lpage>.</mixed-citation></ref><ref id="pone.0324347.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Rybacki</surname><given-names>P</given-names></name>, <name><surname>Niemann</surname><given-names>J</given-names></name>, <name><surname>Derouiche</surname><given-names>S</given-names></name>, <name><surname>Chetehouna</surname><given-names>S</given-names></name>, <name><surname>Boulaares</surname><given-names>I</given-names></name>, <name><surname>Seghir</surname><given-names>NM</given-names></name>, <etal>et al</etal>. <article-title>Convolutional neural network (cnn) model for the classification of varieties of date palm fruits (Phoenix dactylifera L.)</article-title>. <source>Sensors (Basel)</source>. <year>2024</year>;<volume>24</volume>(<issue>2</issue>):<fpage>558</fpage>. <pub-id pub-id-type="doi">10.3390/s24020558</pub-id>
<pub-id pub-id-type="pmid">38257650</pub-id>
</mixed-citation></ref><ref id="pone.0324347.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <article-title>Deep residual learning for image recognition.</article-title> In: <source>Proc IEEE Conf Comput Vis Pattern Recognit (CVPR)</source>. <year>2016</year>.</mixed-citation></ref><ref id="pone.0324347.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>El-Hoseny</surname><given-names>H</given-names></name>. <article-title>Utilization of artificial intelligence in medical image analysis for COVID-19 patients detection</article-title>. <source>Intell Autom Soft Comput</source>. <year>2021</year>;<volume>30</volume>(<issue>1</issue>):<fpage>97</fpage>&#x02013;<lpage>111</lpage>.</mixed-citation></ref><ref id="pone.0324347.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Weersink</surname><given-names>A</given-names></name>, <name><surname>Fraser</surname><given-names>E</given-names></name>, <name><surname>Pannell</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Opportunities and challenges for big data in agricultural and environmental analysis</article-title>. <source>Annu Rev Resour Econ</source>. <year>2018</year>;<volume>10</volume>:<fpage>19</fpage>&#x02013;<lpage>37</lpage>.</mixed-citation></ref><ref id="pone.0324347.ref031"><label>31</label><mixed-citation publication-type="book"><source>New plant diseases dataset</source>. <publisher-name>Kaggle</publisher-name>. Available from: <ext-link xlink:href="https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset" ext-link-type="uri">https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset</ext-link></mixed-citation></ref><ref id="pone.0324347.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Morchid</surname><given-names>A</given-names></name>, <name><surname>Jebabra</surname><given-names>R</given-names></name>, <name><surname>Khalid</surname><given-names>HM</given-names></name>, <name><surname>El Alami</surname><given-names>R</given-names></name>, <name><surname>Qjidaa</surname><given-names>H</given-names></name>, <name><surname>Ouazzani Jamil</surname><given-names>M</given-names></name>. <article-title>IoT-based smart irrigation management system to enhance agricultural water security using embedded systems, telemetry data, and cloud computing</article-title>. <source>Results Eng</source>. <year>2024</year>;<volume>23</volume>:<fpage>102829</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.rineng.2024.102829</pub-id></mixed-citation></ref><ref id="pone.0324347.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Rehman</surname><given-names>AU</given-names></name>, <name><surname>Alamoudi</surname><given-names>Y</given-names></name>, <name><surname>Khalid</surname><given-names>HM</given-names></name>, <name><surname>Morchid</surname><given-names>A</given-names></name>, <name><surname>Muyeen</surname><given-names>SM</given-names></name>, <name><surname>Abdelaziz</surname><given-names>AY</given-names></name>. <article-title>Smart agriculture technology: An integrated framework of renewable energy resources, IoT-based energy management, and precision robotics</article-title>. <source>Cleaner Energy Syst</source>. <year>2024</year>;<volume>9</volume>:<fpage>100132</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cles.2024.100132</pub-id></mixed-citation></ref><ref id="pone.0324347.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Said</surname><given-names>Z</given-names></name>, <name><surname>Sharma</surname><given-names>P</given-names></name>, <name><surname>Thi Bich Nhuong</surname><given-names>Q</given-names></name>, <name><surname>Bora</surname><given-names>BJ</given-names></name>, <name><surname>Lichtfouse</surname><given-names>E</given-names></name>, <name><surname>Khalid</surname><given-names>HM</given-names></name>, <etal>et al</etal>. <article-title>Intelligent approaches for sustainable management and valorisation of food waste</article-title>. <source>Bioresour Technol</source>. <year>2023</year>;<issue>377</issue>:<fpage>128952</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.biortech.2023.128952</pub-id>
<pub-id pub-id-type="pmid">36965587</pub-id>
</mixed-citation></ref><ref id="pone.0324347.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Elshewey</surname><given-names>AM</given-names></name>, <name><surname>Youssef</surname><given-names>RY</given-names></name>, <name><surname>El-Bakry</surname><given-names>HM</given-names></name>, <name><surname>Osman</surname><given-names>AM</given-names></name>. <article-title>Water potability classification based on hybrid stacked model and feature selection</article-title>. <source>Environ Sci Pollut Res Int</source>. <year>2025</year>;<volume>32</volume>(<issue>13</issue>):<fpage>7933</fpage>&#x02013;<lpage>49</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11356-025-36120-0</pub-id>
<pub-id pub-id-type="pmid">40048059</pub-id>
</mixed-citation></ref><ref id="pone.0324347.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Eed</surname><given-names>M</given-names></name>, <name><surname>Alhussan</surname><given-names>AA</given-names></name>, <name><surname>Qenawy</surname><given-names>A-ST</given-names></name>, <name><surname>Osman</surname><given-names>AM</given-names></name>, <name><surname>Elshewey</surname><given-names>AM</given-names></name>, <name><surname>Arnous</surname><given-names>R</given-names></name>. <article-title>Potato consumption forecasting based on a hybrid stacked deep learning model</article-title>. <source>Potato Res</source>. <year>2024</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11540-024-09764-7</pub-id></mixed-citation></ref></ref-list></back></article>