<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006436</article-id><article-id pub-id-type="pmc">PMC11859526</article-id><article-id pub-id-type="doi">10.3390/s25041207</article-id><article-id pub-id-type="publisher-id">sensors-25-01207</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Mixed-Supervised Learning for Cell Classification</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-9437-1009</contrib-id><name><surname>Sun</surname><given-names>Hao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01207" ref-type="aff">1</xref><xref rid="fn1-sensors-25-01207" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-0896-1702</contrib-id><name><surname>Guo</surname><given-names>Danqi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01207" ref-type="aff">1</xref><xref rid="fn1-sensors-25-01207" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5411-8659</contrib-id><name><surname>Chen</surname><given-names>Zhao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-01207" ref-type="aff">1</xref><xref rid="af2-sensors-25-01207" ref-type="aff">2</xref><xref rid="c1-sensors-25-01207" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Nanni</surname><given-names>Loris</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01207"><label>1</label>School of Computer Science and Technology, Donghua University, Shanghai 201620, China; <email>2232835@mail.dhu.edu.cn</email> (H.S.); <email>2212622@mail.dhu.edu.cn</email> (D.G.)</aff><aff id="af2-sensors-25-01207"><label>2</label>Department of Computer Science, University of Warwick, Coventry CV4 7AL, UK</aff><author-notes><corresp id="c1-sensors-25-01207"><label>*</label>Correspondence: <email>chenzhao@dhu.edu.cn</email></corresp><fn id="fn1-sensors-25-01207"><label>&#x02020;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>16</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1207</elocation-id><history><date date-type="received"><day>19</day><month>12</month><year>2024</year></date><date date-type="rev-recd"><day>10</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>14</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Cell classification based on histopathology images is crucial for tumor recognition and cancer diagnosis. Using deep learning, classification accuracy is hugely improved. Semi-supervised learning is an advanced deep learning approach that uses both labeled and unlabeled data. However, complex datasets that comprise diverse patterns may drive models towards learning harmful features. Therefore, it is useful to involve human guidance during training. Hence, we propose a mixed-supervised method incorporating semi-supervision and &#x0201c;human-in-the-loop&#x0201d; for cell classification. We design a sample selection mechanism that assigns highly confident unlabeled samples to automatic semi-supervised optimization and unreliable ones for online annotation correction. We use prior human annotations to pretrain the backbone and trustworthy pseudo labels and online human annotations to fine-tune the model for accurate cell classification. Experimental results show that the mixed-supervised model reaches overall accuracies as high as 86.56%, 99.33% and 74.12% on LUSC, BloodCell, and PanNuke datasets, respectively.</p></abstract><kwd-group><kwd>cell classification</kwd><kwd>mixed-supervised learning</kwd><kwd>human-in-the-loop</kwd></kwd-group><funding-group><award-group><funding-source>National Natural Science Foundation of China</funding-source><award-id>61702094</award-id></award-group><funding-statement>This work was supported by the National Natural Science Foundation of China under Grant 61702094.</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01207"><title>1. Introduction</title><p>Cell classification plays an important role in areas such as pathology diagnosis, cancer research, and drug development. Precise identification and classification of cells are crucial for comprehending cellular behavior, diagnosing diseases, and developing effective treatments. Traditionally, cell classification has relied heavily on manual observation and expert interpretation, which are not only time-consuming but also susceptible to subjective biases. With the rapid advancement of computer vision technology, numerous methods for automatic analysis of pathology images have been applied.</p><p>Deep learning methods can be broadly categorized into three main types based on their level of supervision: unsupervised [<xref rid="B1-sensors-25-01207" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01207" ref-type="bibr">2</xref>], fully supervised [<xref rid="B3-sensors-25-01207" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01207" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01207" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01207" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01207" ref-type="bibr">7</xref>], and semi-supervised [<xref rid="B8-sensors-25-01207" ref-type="bibr">8</xref>,<xref rid="B9-sensors-25-01207" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01207" ref-type="bibr">10</xref>] approaches. Unsupervised methods are data-driven, yielding results that may not be immediately understandable to human users. Most deep learning methods use large amounts of manually annotated data for training in a fully supervised manner, which is labor-intensive and limits the model&#x02019;s adaptability to new data. As hospitals and pathological diagnostic institutions produce a large number of microscopic pathological slides every day without accurate manual annotations, semi-supervised online learning methods have emerged. Compared to fully supervised models, semi-supervised models simultaneously use labeled data and unlabeled data for training, adapting to the diversity of open data and making predictions on new data during training. Semi-supervised methods mainly consist of three types [<xref rid="B11-sensors-25-01207" ref-type="bibr">11</xref>]: consistency regularization-based methods [<xref rid="B12-sensors-25-01207" ref-type="bibr">12</xref>], generative model-based methods [<xref rid="B13-sensors-25-01207" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01207" ref-type="bibr">14</xref>], and pseudo-labeling-based methods [<xref rid="B15-sensors-25-01207" ref-type="bibr">15</xref>,<xref rid="B16-sensors-25-01207" ref-type="bibr">16</xref>]. For instance, ref. [<xref rid="B12-sensors-25-01207" ref-type="bibr">12</xref>] introduced distance correlation to minimize the correlation between feature representations of different views of the same image.</p><p>However, semi-supervised methods often encounter challenges with complex datasets, as they may learn irrelevant or harmful features. Meanwhile, pathologists not only expect to quickly review slides through algorithms but also need to correct algorithm errors promptly [<xref rid="B17-sensors-25-01207" ref-type="bibr">17</xref>]. To address these issues and align with practical needs, the &#x0201c;human-in-the-loop&#x0201d; approach has been proposed. This method focuses on selecting data based on specific criteria to optimize the model&#x02019;s predictive performance [<xref rid="B18-sensors-25-01207" ref-type="bibr">18</xref>]. Various methods have been developed to quantify the informativeness of samples relative to a given model and its underlying distribution. These methods can generally be grouped into four categories: uncertainty, representativeness, generative adversarial networks (GANs) for informativeness, and learning active learning. The order reflects the level of human interpretability each method offers, ranging from the most interpretable to the least. In <xref rid="sensors-25-01207-t001" ref-type="table">Table 1</xref>, we present several classical algorithms for these methods.</p><p>The introduction of the &#x0201c;human-in-the-loop&#x0201d; approach allows experts to intervene during training by annotating difficult samples, thereby improving model accuracy with minimal additional effort [<xref rid="B18-sensors-25-01207" ref-type="bibr">18</xref>]. For instance, ref. [<xref rid="B19-sensors-25-01207" ref-type="bibr">19</xref>] used concept-based interpretability methods like concept activation vectors (CAV) and concept activation regions (CAR) to identify weaknesses in classifier training. Clinicians can then use these insights to determine which concepts or categories need improvement and adjust the dataset to optimize classifier performance. Similarly, ref. [<xref rid="B20-sensors-25-01207" ref-type="bibr">20</xref>] involved pathologists throughout the entire model prediction process. This included reviewing and correcting automated segmentation results, guiding the classification process, and providing feedback on interpretability. The pathologist&#x02019;s expertise not only helped refine the model&#x02019;s output but also contributed to hyperparameter optimization. Unlike explicit sample selection and processing, ref. [<xref rid="B21-sensors-25-01207" ref-type="bibr">21</xref>] proposed a new deep learning method, TOP-GAN, which combines transfer learning and GANs to accurately classify healthy and cancer cell lines using limited labeled data by generating additional training images from sperm cells.</p><table-wrap position="anchor" id="sensors-25-01207-t001"><object-id pub-id-type="pii">sensors-25-01207-t001_Table 1</object-id><label>Table 1</label><caption><p>Human-in-the-loop for cell classification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Evaluating Informativeness</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Principle</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Examples</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Uncertainty</td><td align="center" valign="middle" rowspan="1" colspan="1">Predictions with higher uncertainty offer greater potential for
improving the model when their ground truth is incorporated into the training set.</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B22-sensors-25-01207" ref-type="bibr">22</xref>,<xref rid="B23-sensors-25-01207" ref-type="bibr">23</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Representativeness</td><td align="center" valign="middle" rowspan="1" colspan="1">Encourage selection strategies to sample from different areas
of the distribution, and to increase the diversity of samples</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B19-sensors-25-01207" ref-type="bibr">19</xref>,<xref rid="B24-sensors-25-01207" ref-type="bibr">24</xref>]</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Generative adversarial networks for informativeness</td><td align="center" valign="middle" rowspan="1" colspan="1">GANs are used for image synthesis and data augmentation, enhancing active learning by expanding limited datasets</td><td align="center" valign="middle" rowspan="1" colspan="1">[<xref rid="B21-sensors-25-01207" ref-type="bibr">21</xref>,<xref rid="B25-sensors-25-01207" ref-type="bibr">25</xref>]</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning active learning</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Methods that learn which samples are most informative based on previous selection outcomes.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">[<xref rid="B20-sensors-25-01207" ref-type="bibr">20</xref>,<xref rid="B26-sensors-25-01207" ref-type="bibr">26</xref>]</td></tr></tbody></table></table-wrap><p>The methods mentioned above have shown effectiveness in the field of medical image processing. However, they still fall short in terms of human interpretability. In contrast, interpreting model prediction probabilities as confidence provides a straightforward and highly interpretable means of measuring sample uncertainty. Additionally, confidence not only effectively selects complex samples to enhance the model&#x02019;s reasoning ability but also reduces the computational cost for processing credible samples, thereby accelerating the training process.</p><p>Therefore, we propose a mixed-supervised tumor cell classification framework (MIX-CC) based on a hard sample selection mechanism. It introduces &#x0201c;human-in-the-loop&#x0201d; for online annotation to semi-supervised cell classification. Our contributions are as follows. (1) The proposed method classifies cells in unlabeled images by a sample selection mechanism that allows for online correction of unreliable predictions and automatic optimization using reliable pseudo-labels of the originally unlabeled images. This mechanism is particularly well-suited for large-scale, open pathology datasets, allowing for broader real-world applications. (2) It improves classification accuracy by pretraining the backbone by prior human annotations and refining the model using reliable pseudo labels and online human annotations. This process facilitates precise interpretation of open data, making it ideal for large-scale, practical applications.</p><p>Compared to baseline algorithms, our proposed MIX-CC achieves the best performance across three different datasets, showing significant improvements over both fully supervised and semi-supervised learning methods. The remainder of this paper is organized as follows. In <xref rid="sec2-sensors-25-01207" ref-type="sec">Section 2</xref>, we provide details of various components in the proposed framework. In <xref rid="sec3-sensors-25-01207" ref-type="sec">Section 3</xref>, we present results demonstrating the effectiveness of the proposed framework. <xref rid="sec4-sensors-25-01207" ref-type="sec">Section 4</xref> presents the discussion, while <xref rid="sec5-sensors-25-01207" ref-type="sec">Section 5</xref> concludes the study.</p></sec><sec id="sec2-sensors-25-01207"><title>2. Materials and Methods</title><p>This section provides a detailed description of the dataset, the proposed framework, and programming environment in which the experiment was conducted.</p><sec id="sec2dot1-sensors-25-01207"><title>2.1. DataSet</title><p>The performance of MIX-CC is evaluated across three pathology image datasets: LUSC, BloodCell, and PanNuke, as shown in <xref rid="sensors-25-01207-f001" ref-type="fig">Figure 1</xref>. The datasets are partitioned into labeled sets, validation sets, and unlabeled sets. The labeled set simulated historical data, while the unlabeled set simulated open data, with the validation set employed for model tuning. The specifics of each dataset are detailed below:</p><p>(1) Lung Squamous Cell Carcinoma Dataset (LUSC) [<xref rid="B27-sensors-25-01207" ref-type="bibr">27</xref>]: LUSC is a private dataset, provided by Histo Pathology Diagnostic Center, Shanghai, China (HISTO). It contains 44 immunostained PD-L1 images with 1000 &#x000d7; 1000 pixels obtained from 4 whole slide images scanned by KF-PRO-120 (0.2481 &#x003bc;m/pixel, 40&#x000d7; objective magnification), as shown in <xref rid="sensors-25-01207-f001" ref-type="fig">Figure 1</xref>a. Multiple experts manually labeled the cell nuclei, including two types of tumor cells: positive cells (Positive) and negative cells (Negative). LUSC is randomly divided in a ratio of 7:1:2, comprising 35 labeled images and 9 unlabeled images. Cell image patches in LUSC are set to dimensions of 76 &#x000d7; 76.</p><p>(2) BloodCell Dataset [<xref rid="B28-sensors-25-01207" ref-type="bibr">28</xref>]: Originating from the Ruijin Hospital affiliated with Shanghai Jiao Tong University School of Medicine, it consists of 1326 color images of blood smears, each with dimensions of 360 &#x000d7; 360 pixels, as shown in <xref rid="sensors-25-01207-f001" ref-type="fig">Figure 1</xref>b. These include three types of cells: blast cells (Blast), lymphocytes (Lymphocyte), and lymphoma cells (Lymphoma). BloodCell is randomly split at a ratio of 6:1:3, with 795 images in the labeled set and 399 images in the unlabeled set. The cell image patches in the BloodCell dataset are set to dimensions of 200 &#x000d7; 200.</p><p>(3) Pan-Cancer Histology Dataset (PanNuke) [<xref rid="B29-sensors-25-01207" ref-type="bibr">29</xref>,<xref rid="B30-sensors-25-01207" ref-type="bibr">30</xref>]: PanNuke is published by the University of Warwick. It is a complex, multi-class cell dataset containing 7501 H&#x00026;E stained images with 256 &#x000d7; 256 pixels from 19 different tissue types, as shown in <xref rid="sensors-25-01207-f001" ref-type="fig">Figure 1</xref>c. Each image has pixel level labels for cell segmentation and classification. These include five types of cells: neoplastic cells (Neoplastic), inflammatory cells (Inflammatory), connective tissue cells (Connective Tissue), dead cells (Dead), and non-neoplastic cells (Non-neoplastic). It is divided in a ratio of 5:1:4, comprising 3950 labeled images and 3161 unlabeled images. The size of cell image patches in PanNuke are set to 60 &#x000d7; 60.</p></sec><sec id="sec2dot2-sensors-25-01207"><title>2.2. MIX-CC Framework</title><p>The proposed MIX-CC framework, depicted in <xref rid="sensors-25-01207-f002" ref-type="fig">Figure 2</xref>, is designed with two stages: the fully supervised pretraining stage and the mixed-supervised online learning stage. During the fully supervised pretraining phase, a classification network is trained with labeled data, establishing a strong preliminary feature representation. In the subsequent mixed-supervised learning stage, the &#x0201c;human-in-the-loop&#x0201d; mechanism is employed to dynamically annotate challenging and ambiguous samples from the unlabeled set. Coupled with the credible sample selection and iterative training set updating mechanisms, MIX-CC continuously optimizes the model, enhancing its accuracy and robustness while making refined category predictions for cells in new and diverse images. This approach ensures high adaptability and precision in cell classification tasks.</p><p>Let <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> denote the input cell image patch obtained from the data preprocessing stage, where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the number of images in the training set, <italic toggle="yes">H</italic>, <italic toggle="yes">W</italic>, and <italic toggle="yes">B</italic> represents width, height, and number of channels, respectively, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. As the iterations of the mixed-supervised mechanism progress, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> will gradually increase. The dataset is divided into a labeled set <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:msubsup></mml:mrow></mml:math></inline-formula> and an unlabeled set <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the labeled image, <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the unlabeled image, <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the ground truth of labeled image, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the initial number of labeled and unlabeled samples. In the hard sample selection mechanism, we define the number of unlabeled samples randomly selected in each semi-supervised iteration as <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the number of &#x0201c;credible&#x0201d; unlabeled samples selected in each as <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Conf</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, the number of &#x0201c;hard&#x0201d; unlabeled samples selected in each as <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Hard</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and the remaining unlabeled samples as <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Rest</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Finally, output the prediction obtained from online classification of the selected unlabeled sample <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> as result <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:msubsup></mml:mrow></mml:math></inline-formula> and the classes of the remaining unlabeled samples obtained from offline classification as result <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Rest</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:msubsup></mml:mrow></mml:math></inline-formula>. In this context, <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:msubsup></mml:mrow></mml:math></inline-formula> serves as choosing samples and their pseudo-labels are incorporated into the current training set through an update mechanism, contributing to model optimization in the next iteration cycle. The symbols and notations used in this description are summarized in <xref rid="sensors-25-01207-t002" ref-type="table">Table 2</xref> for better clarity and reference.</p></sec><sec id="sec2dot3-sensors-25-01207"><title>2.3. Backbone</title><p>Selecting an appropriate backbone is crucial for improving cell classification performance, as its design and pretraining are essential for the model&#x02019;s generalization on complex datasets. The proposed MIX-CC framework exhibits a certain degree of flexibility, allowing the utilization of any classification model as the backbone. In this paper, ResNet [<xref rid="B31-sensors-25-01207" ref-type="bibr">31</xref>] is employed as the backbone. It is utilized for both the fully supervised pretraining stage and the mixed-supervised training stage. To fully exploit the potential of MIX-CC, different ResNet variants pretrained on the ImageNet dataset [<xref rid="B32-sensors-25-01207" ref-type="bibr">32</xref>], including ResNet18 [<xref rid="B31-sensors-25-01207" ref-type="bibr">31</xref>] and ResNet50 [<xref rid="B31-sensors-25-01207" ref-type="bibr">31</xref>], are employed for different datasets based on empirical observations. Specifically, ResNet18 yields better performance on certain datasets, while ResNet50 performs more effectively on others. As illustrated by <xref rid="sensors-25-01207-f002" ref-type="fig">Figure 2</xref>, the backbone for both fully supervised pretraining and mixed-supervised training stages of MIX-CC remains consistent.</p></sec><sec id="sec2dot4-sensors-25-01207"><title>2.4. Fully Supervised Pretraining</title><p>To enhance the reliability of pseudo-labels and improve the efficiency and effectiveness of mixed-supervised training, MIX-CC first utilizes the labeled set <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:msubsup></mml:mrow></mml:math></inline-formula> for initialization, feeding it into the ResNet18 or ResNet50 backbones to produce pretrained classification networks, as shown in <xref rid="sensors-25-01207-f003" ref-type="fig">Figure 3</xref>. At this stage, we use the ground truth and model predictions to compute the cross-entropy loss for model optimization, as shown in Equation (<xref rid="FD1-sensors-25-01207" ref-type="disp-formula">1</xref>).<disp-formula id="FD1-sensors-25-01207"><label>(1)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mo>&#x02113;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>fully</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>These pretrained models serve as a high-quality foundation for generating pseudo-labels in the subsequent mixed-supervised phase.</p></sec><sec id="sec2dot5-sensors-25-01207"><title>2.5. Mixed-Supervised Online Learning</title><p>After the fully supervised pretraining stage, the classification networks gain initial feature representation capabilities. However, the presence of challenging samples in the open data can interfere with the model, limiting its generalization. Therefore, the framework initiates mixed-supervised training after fully supervised pretraining, as shown in <xref rid="sensors-25-01207-f004" ref-type="fig">Figure 4</xref>. The mixed-supervised online learning stage comprises three mechanisms: sample selection mechanism, dynamic training set updating mechanism and model iterative training mechanism. The framework takes the labeled set <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:msubsup></mml:mrow></mml:math></inline-formula> and unlabeled set <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:msubsup></mml:mrow></mml:math></inline-formula> as inputs, assuming that pathologists annotate &#x0201c;hard&#x0201d; samples identified by the model in each iteration. The model automatically identifies &#x0201c;credible&#x0201d; and &#x0201c;hard&#x0201d; samples by evaluating the uncertainty in its predictions, which is derived from prediction confidence. &#x0201c;Credible&#x0201d; samples are those with higher confidence, while &#x0201c;hard&#x0201d; samples are typically those that the model finds difficult to classify, often due to factors such as unseen variations, ambiguous boundaries, or underrepresented categories in the training data. These samples tend to have low confidence scores and may represent rare or complex cases that challenge the model&#x02019;s existing knowledge. By incorporating pseudo-labeled samples and hard samples annotated by pathologists into the training set, the model iteratively refines its ability to handle difficult samples and enhance feature representation. This iterative process not only improves the model&#x02019;s performance on hard-to-classify cases but also facilitates the simultaneous prediction of categories for unlabeled samples during training. Consequently, the framework minimizes manual annotation costs while ensuring annotation accuracy.</p><p>In the mixed-supervised stage, we use the cross-entropy loss to optimize the function, just as in the pretraining stage, as shown in Equation (<xref rid="FD2-sensors-25-01207" ref-type="disp-formula">2</xref>). It is important to note that, at this stage, <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents not only the labels of the labeled data but also includes pseudo-labels for &#x0201c;credible&#x0201d; samples and online annotations for &#x0201c;hard&#x0201d; samples, both selected from the unlabeled data through the sample selection mechanism.<disp-formula id="FD2-sensors-25-01207"><label>(2)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mo>&#x02113;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>mixed</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mstyle><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo form="prefix">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>After each iteration of training, MIX-CC conducts online testing on the unlabeled samples selected to update the training set and records the results as the classification results for this portion of the samples. Such a human&#x02013;computer interaction mechanism enhances the model&#x02019;s predictive capabilities.</p></sec><sec id="sec2dot6-sensors-25-01207"><title>2.6. Algorithm Description</title><p>The MIX-CC framework, detailed in Algorithm&#x000a0;1, consists of two stages: fully supervised pretraining and mixed-supervised online learning. During the pretraining and mixed-supervised stages, the model is optimized using gradient descent to update its parameters. The sample selection and online labeling mechanism used in the mixed-supervised learning stage are described in Algorithm&#x000a0;2.
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1:</bold> Mixed-supervised cell classification.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-25-01207-i001.jpg"/></td></tr></tbody></array>
<array><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2:</bold> Sample Selection and Online Labeling.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-25-01207-i002.jpg"/></td></tr></tbody></array></p></sec><sec id="sec2dot7-sensors-25-01207"><title>2.7. Setup</title><sec id="sec2dot7dot1-sensors-25-01207"><title>2.7.1. Competing Algorithm Settings</title><p>This study employs simulated expert annotations to mimic pathology expert online annotating, comparing the performance of ResNet18, SimCLR [<xref rid="B33-sensors-25-01207" ref-type="bibr">33</xref>] and ResNet50 in fully supervised for classification tasks on different datasets. For both BloodCell and LUSC, UNeXt [<xref rid="B34-sensors-25-01207" ref-type="bibr">34</xref>] is utilized for cell segmentation map prediction, employing ResNet18 for both fully supervised and semi-supervised learning. Conversely, for the PanNuke dataset, HoVer-Net [<xref rid="B3-sensors-25-01207" ref-type="bibr">3</xref>] is employed for cell segmentation map prediction, utilizing ResNet50 for both fully supervised and semi-supervised learning stages. <xref rid="sensors-25-01207-t003" ref-type="table">Table 3</xref> provides a summary of the competing algorithm settings.</p></sec><sec id="sec2dot7dot2-sensors-25-01207"><title>2.7.2. Hyperparameter Setting</title><p>The experiments mainly consist of hyperparameters for both the fully supervised pre-training stage and the mixed-supervised learning stage. Different datasets require adjustments in hyperparameters. The details of the semi-supervised stage are as follows.</p><p>For LUSC, the initial learning rate for the semi-supervised stage is 0.0003. Every 5 training epochs, the learning rate is reduced to 0.6 of its current value. Batch size is 64, <inline-formula><mml:math id="mm102" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is 300. <inline-formula><mml:math id="mm103" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Conf</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm104" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Hard</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm105" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Rest</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is 20, 20 and 0, respectively.</p><p>For BloodCell, the initial learning rate for the semi-supervised stage is 0.0002. Every 5 training epochs, the learning rate is decreased to 0.8 of its current value. Batch size is set to 64. The number of candidate unlabeled samples randomly selected per iteration <inline-formula><mml:math id="mm106" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is 200. The preset number of credible samples per iteration <inline-formula><mml:math id="mm107" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Conf</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, the number of hard samples annotated online per iteration <inline-formula><mml:math id="mm108" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Hard</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and the remaining unlabeled samples <inline-formula><mml:math id="mm109" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Rest</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are set to 20, 10, and 0, respectively.</p><p>For PanNuke, the initial learning rate for the semi-supervised stage is 0.0002. Every 5 training epochs, the learning rate is decreased to 0.6 of its current value. Batch size is set to 64, with <inline-formula><mml:math id="mm110" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> set to 1500, <inline-formula><mml:math id="mm111" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Conf</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> set to 300, <inline-formula><mml:math id="mm112" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Hard</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> set to 300, and <inline-formula><mml:math id="mm113" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Rest</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> set to 0.</p><p>Each iteration of the mixed-supervised stage comprises 20 training epochs. The weight_decay parameter is set to 8 &#x000d7; 10<sup>&#x02212;5</sup>, and Adam optimizer [<xref rid="B35-sensors-25-01207" ref-type="bibr">35</xref>] is utilized to optimize model parameters via gradient descent.</p><p>The specific parameter settings for each dataset can be found in <xref rid="sensors-25-01207-t004" ref-type="table">Table 4</xref>.</p></sec><sec id="sec2dot7dot3-sensors-25-01207"><title>2.7.3. Evaluation Metrics</title><p>In the experiment, the overall accuracy (OA) and class-specific accuracy are utilized to evaluate the performance of tumor cell classification. The specific calculation equations are as follows:<disp-formula id="FD3-sensors-25-01207"><label>(3)</label><mml:math id="mm115" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>OA</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>correct</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Hard</mml:mi><mml:mo>,</mml:mo><mml:mi>all</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-01207"><label>(4)</label><mml:math id="mm116" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>correct</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">U</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Hard</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where OA represents the overall classification accuracy, <inline-formula><mml:math id="mm117" overflow="scroll"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>correct</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the number of correctly classified samples among all open data, <inline-formula><mml:math id="mm118" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the total number of samples in all open data. <inline-formula><mml:math id="mm119" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="normal">A</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">c</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> indicates the classification accuracy of cells belonging to class c in the open data, <inline-formula><mml:math id="mm120" overflow="scroll"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>correct</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represents the number of correctly classified cells of class c, and <inline-formula><mml:math id="mm121" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">U</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> represents the total number of cells in class c. <inline-formula><mml:math id="mm122" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Hard</mml:mi><mml:mo>,</mml:mo><mml:mi>all</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represents the number of samples annotated through human-computer interaction online, and <inline-formula><mml:math id="mm123" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Hard</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the number of cells of class c annotated through human&#x02013;computer interaction online. As real-time online annotation can be considered as samples with labels, this part of the samples is not included in the calculation of the metrics.</p></sec><sec id="sec2dot7dot4-sensors-25-01207"><title>2.7.4. Environment Configuration</title><p>All algorithms are implemented using PyTorch 2.5.1 and run on a machine with 43GB of RAM and RTX 2080Ti GPU.</p></sec></sec></sec><sec sec-type="results" id="sec3-sensors-25-01207"><title>3. Results</title><sec id="sec3dot1-sensors-25-01207"><title>3.1. Analysis</title><p>The performance of the MIX-CC framework is evaluated against baseline algorithms using quantitative metrics across three distinct datasets, as summarized in <xref rid="sensors-25-01207-t005" ref-type="table">Table 5</xref>, <xref rid="sensors-25-01207-t006" ref-type="table">Table 6</xref> and <xref rid="sensors-25-01207-t007" ref-type="table">Table 7</xref>. The results reveal that MIX-CC consistently surpasses other methods in all three datasets, showing substantial improvements over both fully supervised and semi-supervised learning approaches. This consistent outperformance underscores the effectiveness and robustness of the MIX-CC framework in various classification scenarios.</p><p>In LUSC, MIX-CC reaches 86.56% for OA, as indicated in <xref rid="sensors-25-01207-t005" ref-type="table">Table 5</xref>. The confusion matrix for LUSC (<xref rid="sensors-25-01207-f005" ref-type="fig">Figure 5</xref>a) reveals a noticeable drop in the accuracy of negative cells, while the accuracy of positive tumor cells improves. This could be attributed to the higher difficulty in distinguishing tumor cells, leading the model to misclassify negative tumor cells as positive ones with high confidence. The number of such hard samples recognized by the model is minimal, thus the online annotation fails to sufficiently correct the model.</p><p>In BloodCell, MIX-CC achieves a classification accuracy of 99.33%, with the highest accuracy observed for each individual class. Particularly, it achieves perfect performance on lymphocyte and lymphoma cells. The confusion matrix for BloodCell (<xref rid="sensors-25-01207-f005" ref-type="fig">Figure 5</xref>b) highlights the model&#x02019;s excellent performance in distinguishing all classes, with no significant misclassifications. The superior performance on this dataset can be attributed to the relatively simple background, the small number of cells, and the significant difference between cells and the background, with minimal cell occlusion. The results are shown in <xref rid="sensors-25-01207-t006" ref-type="table">Table 6</xref>.</p><p>In PanNuke, MIX-CC reaches 74.12% for OA, which is nearly 17% higher than all networks in the baseline algorithms. However, the relatively lower performance in this case can be attributed to the larger dataset size, more complex backgrounds, greater cell occlusion, and less distinction between cells and the background. The confusion matrix for PanNuke (<xref rid="sensors-25-01207-f005" ref-type="fig">Figure 5</xref>c) shows that while the model performs well overall, there is a higher rate of misclassification between certain cell types, indicating that the complexity of the dataset impacts the model&#x02019;s performance. The results are presented in <xref rid="sensors-25-01207-t007" ref-type="table">Table 7</xref>.</p><p>The online annotation mechanism introduced by MIX-CC addresses the issue of unreliable pseudo-labels for &#x0201c;hard&#x0201d; samples. Moreover, the workload of human&#x02013;computer interactive online annotation is significantly reduced compared to manually annotating all open data by experts. The model can automatically recognize cells and select &#x0201c;hard&#x0201d; samples for human annotation. The results on three datasets demonstrate the effectiveness of the MIX-CC framework.</p><sec><title>Convergence Analysis</title><p>Convergence analysis was performed on the MIX-CC model training across various iterations for the three datasets. For illustration, the convergence curves for the first six iterations are presented in <xref rid="sensors-25-01207-f006" ref-type="fig">Figure 6</xref>. From the figure, it can be seen that the MIX-CC loss decreases steadily as the training epochs increase, with this smooth decline observed across all three datasets. This gradual reduction in loss over iterations highlights the model&#x02019;s stability and convergence.</p></sec></sec><sec id="sec3dot2-sensors-25-01207"><title>3.2. Ablation Study</title><p>To investigate the impact of the &#x0201c;human-in-the-loop&#x0201d; mechanism on cell classification tasks, we conducted ablation experiments on three datasets, referred to as SEMI-CC, as presented in <xref rid="sensors-25-01207-t008" ref-type="table">Table 8</xref>, <xref rid="sensors-25-01207-t009" ref-type="table">Table 9</xref> and <xref rid="sensors-25-01207-t010" ref-type="table">Table 10</xref>. In this experiment, the &#x0201c;human-in-the-loop&#x0201d; mechanism was deliberately removed, meaning that the selection and processing of &#x0201c;hard&#x0201d; samples, which are those for which the model&#x02019;s prediction is least confident, was not taken into account. In the absence of this mechanism, only high-confidence unlabeled samples were selected and considered as reliable for inclusion in the training set. These high-confidence samples, along with their corresponding pseudo-labels, were merged with the existing labeled samples, forming an updated training set. This new training set was then used to train the next iteration of the real-time classification model. The omission of the &#x0201c;hard&#x0201d; sample selection process allowed us to examine the performance of the model in a more simplified setup, without the added complexity of integrating challenging, uncertain samples into the learning process.</p><p>For LUSC, MIX-CC achieves 86.56% for OA, showing a notable enhancement of 14.69% compared to SEMI-CC. For BloodCell, MIX-CC achieves 99.33% for OA, which represents a 5.02% improvement over SEMI-CC. Similarly, for PanNuke, MIX-CC demonstrated a relative improvement of 15.5% over SEMI-CC, with enhanced classification accuracy across all categories. These results clearly demonstrate the effectiveness of incorporating the &#x0201c;hard&#x0201d; sample selection mechanism, which has significantly enhanced the model&#x02019;s ability to identify and classify challenging instances. By focusing on these hard samples, the model has become more adept at handling edge cases and has thereby improved its overall classification performance. The inclusion of this mechanism enables the model to not only refine its understanding of rare or complex cases but also to generalize better across a variety of data distributions, making it a more robust and reliable classifier.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-01207"><title>4. Discussion</title><p>The MIX-CC framework demonstrates significant improvements in cell classification accuracy by leveraging both labeled and unlabeled data through semi-supervised learning and integrating human-in-the-loop annotation. This approach yields high overall accuracy across datasets, such as BloodCell (99.33%), LUSC (86.56%), and PanNuke (74.12%), showcasing its effectiveness in diverse histopathology applications.</p><p>A key strength of MIX-CC is its improved classification accuracy, achieved by optimizing the model with confident pseudo-labels and expert corrections. The &#x0201c;human-in-the-loop&#x0201d; mechanism ensures that misclassifications are corrected by pathologists, allowing the model to adapt to complex datasets.</p><p>Clinically, MIX-CC has great potential for improving diagnostic accuracy and streamlining workflows, reducing the need for manual annotation and allowing pathologists to focus on more complex cases. This mechanism is particularly well-suited for large-scale, open pathology datasets, enabling broader real-world applications. By leveraging this mechanism, the model can effectively handle the challenges posed by vast amounts of unlabeled data, optimizing its predictions iteratively and reducing the need for exhaustive manual annotations. Furthermore, the method enhances classification accuracy by pretraining the backbone using prior human annotations and refining the model through reliable pseudo-labels and online human annotations. This dual-phase process facilitates the precise interpretation of open data, making the approach ideal for large-scale, practical applications in clinical and research settings. By continuously refining the model with real-time annotations, MIX-CC ensures that the classification performance remains robust even in the presence of diverse, complex data from different sources.</p><p>Our algorithm has some limitations, such as the restricted feature representation capacity of the backbone. In near future, we plan to explore more advanced deep learning architectures for the backbone to enhance the overall generalizability and adaptability of the algorithm across diverse datasets. Apart from that, we could develop some explainability tools to improve the adoption of our method, MIX-CC, in clinical practice.</p></sec><sec sec-type="conclusions" id="sec5-sensors-25-01207"><title>5. Conclusions</title><p>To overcome the limitations of traditional supervised learning methods in the complex task of cell classification, this paper proposes MIX-CC, a mixed-supervised approach that combines semi-supervised learning with &#x0201c;human-in-the-loop&#x0201d; guidance for cell classification in histopathology images. This method introduces an advanced sample selection mechanism which assigns highly confident unlabeled samples for semi-supervised optimization while unreliable samples are sent for online annotation correction.</p><p>The experimental results show that our algorithm performs well across different types of pathological stained slides and various cell types. Furthermore, the &#x0201c;human-in-the-loop&#x0201d; combined with semi-supervised learning in the mixed-supervision mechanism outperforms the pure semi-supervised approach. The proposed model is expected to gain abilities if more powerful backbone is applied. In clinical practice, our algorithm can assist pathologists in rapidly interpreting the continuously collected pathological slides.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, D.G. and Z.C.; methodology, H.S. and D.G.; software, H.S. and D.G.; validation, H.S.; formal analysis, D.G.; investigation, D.G.; resources, Z.C.; data curation, H.S. and D.G.; writing&#x02014;original draft preparation, D.G.; writing&#x02014;review and editing, H.S.; visualization, H.S. and D.G.; supervision, Z.C.; project administration, Z.C.; funding acquisition, Z.C. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are unavailable due to privacy restrictions.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01207"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kuko</surname><given-names>M.</given-names></name>
<name><surname>Pourhomayoun</surname><given-names>M.</given-names></name>
</person-group><article-title>Single and clustered cervical cell classification with ensemble and deep learning methods</article-title><source>Inf. Syst. Front.</source><year>2020</year><volume>22</volume><fpage>1039</fpage><lpage>1051</lpage><pub-id pub-id-type="doi">10.1007/s10796-020-10028-1</pub-id></element-citation></ref><ref id="B2-sensors-25-01207"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zamanighomi</surname><given-names>M.</given-names></name>
<name><surname>Lin</surname><given-names>Z.</given-names></name>
<name><surname>Daley</surname><given-names>T.</given-names></name>
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Duren</surname><given-names>Z.</given-names></name>
<name><surname>Schep</surname><given-names>A.</given-names></name>
<name><surname>Greenleaf</surname><given-names>W.J.</given-names></name>
<name><surname>Wong</surname><given-names>W.H.</given-names></name>
</person-group><article-title>Unsupervised clustering and epigenetic classification of single cells</article-title><source>Nat. Commun.</source><year>2018</year><volume>9</volume><fpage>2410</fpage><pub-id pub-id-type="doi">10.1038/s41467-018-04629-3</pub-id><pub-id pub-id-type="pmid">29925875</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01207"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Graham</surname><given-names>S.</given-names></name>
<name><surname>Vu</surname><given-names>Q.D.</given-names></name>
<name><surname>Raza</surname><given-names>S.E.A.</given-names></name>
<name><surname>Azam</surname><given-names>A.</given-names></name>
<name><surname>Tsang</surname><given-names>Y.W.</given-names></name>
<name><surname>Kwak</surname><given-names>J.T.</given-names></name>
<name><surname>Rajpoot</surname><given-names>N.</given-names></name>
</person-group><article-title>Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images</article-title><source>Med. Image Anal.</source><year>2019</year><volume>58</volume><fpage>101563</fpage><pub-id pub-id-type="doi">10.1016/j.media.2019.101563</pub-id><pub-id pub-id-type="pmid">31561183</pub-id>
</element-citation></ref><ref id="B4-sensors-25-01207"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ilyas</surname><given-names>T.</given-names></name>
<name><surname>Mannan</surname><given-names>Z.I.</given-names></name>
<name><surname>Khan</surname><given-names>A.</given-names></name>
<name><surname>Azam</surname><given-names>S.</given-names></name>
<name><surname>Kim</surname><given-names>H.</given-names></name>
<name><surname>De Boer</surname><given-names>F.</given-names></name>
</person-group><article-title>TSFD-Net: Tissue specific feature distillation network for nuclei segmentation and classification</article-title><source>Neural Netw.</source><year>2022</year><volume>151</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2022.02.020</pub-id><pub-id pub-id-type="pmid">35367734</pub-id>
</element-citation></ref><ref id="B5-sensors-25-01207"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Dogar</surname><given-names>G.M.</given-names></name>
<name><surname>Shahzad</surname><given-names>M.</given-names></name>
<name><surname>Fraz</surname><given-names>M.M.</given-names></name>
</person-group><article-title>Attention augmented distance regression and classification network for nuclei instance segmentation and type classification in histology images</article-title><source>Biomed. Signal Process. Control</source><year>2023</year><volume>79</volume><elocation-id>104199</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2022.104199</pub-id></element-citation></ref><ref id="B6-sensors-25-01207"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Lu</surname><given-names>L.</given-names></name>
<name><surname>Nogues</surname><given-names>I.</given-names></name>
<name><surname>Summers</surname><given-names>R.M.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Yao</surname><given-names>J.</given-names></name>
</person-group><article-title>DeepPap: Deep convolutional networks for cervical cell classification</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2017</year><volume>21</volume><fpage>1633</fpage><lpage>1643</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2017.2705583</pub-id><pub-id pub-id-type="pmid">28541229</pub-id>
</element-citation></ref><ref id="B7-sensors-25-01207"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Qin</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Ge</surname><given-names>J.</given-names></name>
<name><surname>Liang</surname><given-names>Y.</given-names></name>
</person-group><article-title>A multi-task feature fusion model for cervical cell classification</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2022</year><volume>26</volume><fpage>4668</fpage><lpage>4678</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2022.3180989</pub-id><pub-id pub-id-type="pmid">35671309</pub-id>
</element-citation></ref><ref id="B8-sensors-25-01207"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ha</surname><given-names>Y.</given-names></name>
<name><surname>Du</surname><given-names>Z.</given-names></name>
<name><surname>Tian</surname><given-names>J.</given-names></name>
</person-group><article-title>Fine-grained interactive attention learning for semi-supervised white blood cell classification</article-title><source>Biomed. Signal Process. Control</source><year>2022</year><volume>75</volume><elocation-id>103611</elocation-id><pub-id pub-id-type="doi">10.1016/j.bspc.2022.103611</pub-id></element-citation></ref><ref id="B9-sensors-25-01207"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pasupa</surname><given-names>K.</given-names></name>
<name><surname>Tungjitnob</surname><given-names>S.</given-names></name>
<name><surname>Vatathanavaro</surname><given-names>S.</given-names></name>
</person-group><article-title>Semi-supervised learning with deep convolutional generative adversarial networks for canine red blood cells morphology classification</article-title><source>Multimed. Tools Appl.</source><year>2020</year><volume>79</volume><fpage>34209</fpage><lpage>34226</lpage><pub-id pub-id-type="doi">10.1007/s11042-020-08767-z</pub-id></element-citation></ref><ref id="B10-sensors-25-01207"><label>10.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Su</surname><given-names>H.</given-names></name>
<name><surname>Shi</surname><given-names>X.</given-names></name>
<name><surname>Cai</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>L.</given-names></name>
</person-group><article-title>Local and global consistency regularized mean teacher for semi-supervised nuclei classification</article-title><source>Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</source><conf-loc>Shenzhen, China</conf-loc><conf-date>13&#x02013;17 October 2019</conf-date><fpage>123</fpage><lpage>130</lpage></element-citation></ref><ref id="B11-sensors-25-01207"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>K.</given-names></name>
<name><surname>Fung</surname><given-names>K.M.</given-names></name>
<name><surname>Thai</surname><given-names>T.C.</given-names></name>
<name><surname>Moore</surname><given-names>K.</given-names></name>
<name><surname>Mannel</surname><given-names>R.S.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Zheng</surname><given-names>B.</given-names></name>
<name><surname>Qiu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Recent advances and clinical applications of deep learning in medical image analysis</article-title><source>Med. Image Anal.</source><year>2022</year><volume>79</volume><fpage>102444</fpage><pub-id pub-id-type="doi">10.1016/j.media.2022.102444</pub-id><pub-id pub-id-type="pmid">35472844</pub-id>
</element-citation></ref><ref id="B12-sensors-25-01207"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ren</surname><given-names>Z.</given-names></name>
<name><surname>Kong</surname><given-names>X.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
</person-group><article-title>UKSSL: Underlying knowledge based semi-supervised learning for medical image classification</article-title><source>IEEE Open J. Eng. Med. Biol.</source><year>2023</year><volume>5</volume><fpage>459</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1109/OJEMB.2023.3305190</pub-id><pub-id pub-id-type="pmid">38899016</pub-id>
</element-citation></ref><ref id="B13-sensors-25-01207"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ren</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
</person-group><article-title>LCDAE: Data augmented ensemble framework for lung cancer classification</article-title><source>Technol. Cancer Res. Treat.</source><year>2022</year><volume>21</volume><fpage>15330338221124372</fpage><pub-id pub-id-type="doi">10.1177/15330338221124372</pub-id><pub-id pub-id-type="pmid">36148908</pub-id>
</element-citation></ref><ref id="B14-sensors-25-01207"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ren</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>S.</given-names></name>
</person-group><article-title>A hybrid framework for lung cancer classification</article-title><source>Electronics</source><year>2022</year><volume>11</volume><elocation-id>1614</elocation-id><pub-id pub-id-type="doi">10.3390/electronics11101614</pub-id><pub-id pub-id-type="pmid">36568860</pub-id>
</element-citation></ref><ref id="B15-sensors-25-01207"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Xie</surname><given-names>Q.</given-names></name>
<name><surname>Luong</surname><given-names>M.T.</given-names></name>
<name><surname>Hovy</surname><given-names>E.</given-names></name>
<name><surname>Le</surname><given-names>Q.V.</given-names></name>
</person-group><article-title>Self-training with noisy student improves ImageNet classification</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date><fpage>10687</fpage><lpage>10698</lpage></element-citation></ref><ref id="B16-sensors-25-01207"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Meng</surname><given-names>W.</given-names></name>
<name><surname>Au</surname><given-names>M.H.</given-names></name>
</person-group><article-title>Enhancing collaborative intrusion detection via disagreement-based semi-supervised learning in IoT environments</article-title><source>J. Netw. Comput. Appl.</source><year>2020</year><volume>161</volume><fpage>102631</fpage><pub-id pub-id-type="doi">10.1016/j.jnca.2020.102631</pub-id></element-citation></ref><ref id="B17-sensors-25-01207"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Colling</surname><given-names>R.</given-names></name>
<name><surname>Pitman</surname><given-names>H.</given-names></name>
<name><surname>Oien</surname><given-names>K.</given-names></name>
<name><surname>Rajpoot</surname><given-names>N.</given-names></name>
<name><surname>Macklin</surname><given-names>P.</given-names></name>
<collab>CM-Path AI in Histopathology Working Group</collab>
<name><surname>Bachtiar</surname><given-names>V.</given-names></name>
<name><surname>Booth</surname><given-names>R.</given-names></name>
<name><surname>Bryant</surname><given-names>A.</given-names></name>
<name><surname>Bull</surname><given-names>J.</given-names></name>
<etal/>
</person-group><article-title>Artificial intelligence in digital pathology: A roadmap to routine use in clinical practice</article-title><source>J. Pathol.</source><year>2019</year><volume>249</volume><fpage>143</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.1002/path.5310</pub-id><pub-id pub-id-type="pmid">31144302</pub-id>
</element-citation></ref><ref id="B18-sensors-25-01207"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Budd</surname><given-names>S.</given-names></name>
<name><surname>Robinson</surname><given-names>E.C.</given-names></name>
<name><surname>Kainz</surname><given-names>B.</given-names></name>
</person-group><article-title>A survey on active learning and human-in-the-loop deep learning for medical image analysis</article-title><source>Med. Image Anal.</source><year>2021</year><volume>71</volume><fpage>102062</fpage><pub-id pub-id-type="doi">10.1016/j.media.2021.102062</pub-id><pub-id pub-id-type="pmid">33901992</pub-id>
</element-citation></ref><ref id="B19-sensors-25-01207"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Santos</surname><given-names>J.C.</given-names></name>
<name><surname>Santos</surname><given-names>M.S.</given-names></name>
<name><surname>Abreu</surname><given-names>P.H.</given-names></name>
</person-group><article-title>An Interpretable Human-in-the-Loop Process to Improve Medical Image Classification</article-title><source>Proceedings of the 22nd International Symposium on Intelligent Data Analysis</source><conf-loc>Stockholm, Sweden</conf-loc><conf-date>24&#x02013;26 April 2024</conf-date><publisher-name>Springer Nature</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2024</year><fpage>179</fpage><lpage>190</lpage></element-citation></ref><ref id="B20-sensors-25-01207"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>V&#x000e1;zquez-Lema</surname><given-names>D.</given-names></name>
<name><surname>Mosqueira-Rey</surname><given-names>E.</given-names></name>
<name><surname>Hern&#x000e1;ndez-Pereira</surname><given-names>E.</given-names></name>
<name><surname>Fern&#x000e1;ndez-Lozano</surname><given-names>E.</given-names></name>
<name><surname>Seara-Romera</surname><given-names>C.F.</given-names></name>
<name><surname>Pombo-Otero</surname><given-names>J.</given-names></name>
</person-group><article-title>Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2403.20112</pub-id><pub-id pub-id-type="doi">10.1007/s00521-024-10799-7</pub-id></element-citation></ref><ref id="B21-sensors-25-01207"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Rubin</surname><given-names>M.</given-names></name>
<name><surname>Stein</surname><given-names>O.</given-names></name>
<name><surname>Turko</surname><given-names>N.A.</given-names></name>
<name><surname>Nygate</surname><given-names>Y.</given-names></name>
<name><surname>Roitshtain</surname><given-names>D.</given-names></name>
<name><surname>Karako</surname><given-names>L.</given-names></name>
<name><surname>Barnea</surname><given-names>I.</given-names></name>
<name><surname>Giryes</surname><given-names>R.</given-names></name>
<name><surname>Shaked</surname><given-names>N.T.</given-names></name>
</person-group><article-title>TOP-GAN: Stain-free cancer cell classification using deep learning with a small training set</article-title><source>Med. Image Anal.</source><year>2019</year><volume>57</volume><fpage>176</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/j.media.2019.06.014</pub-id><pub-id pub-id-type="pmid">31325721</pub-id>
</element-citation></ref><ref id="B22-sensors-25-01207"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Engelmann</surname><given-names>J.</given-names></name>
<name><surname>Hetzel</surname><given-names>L.</given-names></name>
<name><surname>Palla</surname><given-names>G.</given-names></name>
<name><surname>Sikkema</surname><given-names>L.</given-names></name>
<name><surname>Luecken</surname><given-names>M.</given-names></name>
<name><surname>Theis</surname><given-names>F.</given-names></name>
</person-group><article-title>Uncertainty quantification for atlas-level cell type transfer</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="arxiv">2211.03793</pub-id></element-citation></ref><ref id="B23-sensors-25-01207"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wen</surname><given-names>S.</given-names></name>
<name><surname>Kurc</surname><given-names>T.M.</given-names></name>
<name><surname>Hou</surname><given-names>L.</given-names></name>
<name><surname>Saltz</surname><given-names>J.H.</given-names></name>
<name><surname>Gupta</surname><given-names>R.R.</given-names></name>
<name><surname>Batiste</surname><given-names>R.</given-names></name>
<name><surname>Zhao</surname><given-names>T.</given-names></name>
<name><surname>Nguyen</surname><given-names>V.</given-names></name>
<name><surname>Samaras</surname><given-names>D.</given-names></name>
<name><surname>Zhu</surname><given-names>W.</given-names></name>
</person-group><article-title>Comparison of different classifiers with active learning to support quality control in nucleus segmentation in pathology images</article-title><source>Amia Summits Transl. Sci. Proc.</source><year>2018</year><volume>2018</volume><fpage>227</fpage></element-citation></ref><ref id="B24-sensors-25-01207"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Chen</surname><given-names>D.Z.</given-names></name>
</person-group><article-title>Suggestive annotation: A deep active learning framework for biomedical image segmentation</article-title><source>Proceedings of the Medical Image Computing and Computer-Assisted Intervention</source><conf-loc>Quebec City, QC, Canada</conf-loc><conf-date>11&#x02013;13 September 2017</conf-date><fpage>20</fpage></element-citation></ref><ref id="B25-sensors-25-01207"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>A.</given-names></name>
<name><surname>Balakrishnan</surname><given-names>G.</given-names></name>
<name><surname>Durand</surname><given-names>F.</given-names></name>
<name><surname>Guttag</surname><given-names>J.V.</given-names></name>
<name><surname>Dalca</surname><given-names>A.V.</given-names></name>
</person-group><article-title>Data augmentation using learned transformations for one-shot medical image segmentation</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>26&#x02013;20 June 2019</conf-date><fpage>8543</fpage><lpage>8553</lpage></element-citation></ref><ref id="B26-sensors-25-01207"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fang</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Cohn</surname><given-names>T.</given-names></name>
</person-group><article-title>Learning how to active learn: A deep reinforcement learning approach</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1708.02383</pub-id></element-citation></ref><ref id="B27-sensors-25-01207"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Zheng</surname><given-names>Q.</given-names></name>
<name><surname>Zhu</surname><given-names>Y.</given-names></name>
<name><surname>Zuo</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Guan</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group><article-title>Weakly supervised histopathology image segmentation with sparse point annotations</article-title><source>IEEE J. Biomed. Health Inform.</source><year>2020</year><volume>25</volume><fpage>1673</fpage><lpage>1685</lpage><pub-id pub-id-type="doi">10.1109/JBHI.2020.3024262</pub-id><pub-id pub-id-type="pmid">32931437</pub-id>
</element-citation></ref><ref id="B28-sensors-25-01207"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sheng</surname><given-names>B.</given-names></name>
<name><surname>Zhou</surname><given-names>M.</given-names></name>
<name><surname>Hu</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>Q.</given-names></name>
<name><surname>Sun</surname><given-names>L.</given-names></name>
<name><surname>Wen</surname><given-names>Y.</given-names></name>
</person-group><article-title>A blood cell dataset for lymphoma classification using faster R-CNN</article-title><source>Biotechnol. Biotechnol. Equip.</source><year>2020</year><volume>34</volume><fpage>413</fpage><lpage>420</lpage><pub-id pub-id-type="doi">10.1080/13102818.2020.1765871</pub-id></element-citation></ref><ref id="B29-sensors-25-01207"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Budd</surname><given-names>S.</given-names></name>
<name><surname>Robinson</surname><given-names>E.C.</given-names></name>
<name><surname>Kainz</surname><given-names>B.</given-names></name>
</person-group><article-title>Pannuke dataset extension, insights and baselines</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="arxiv">1412.6980</pub-id></element-citation></ref><ref id="B30-sensors-25-01207"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Gamper</surname><given-names>J.</given-names></name>
<name><surname>Alemi Koohbanani</surname><given-names>N.</given-names></name>
<name><surname>Benet</surname><given-names>K.</given-names></name>
<name><surname>Khuram</surname><given-names>A.</given-names></name>
<name><surname>Rajpoot</surname><given-names>N.</given-names></name>
</person-group><article-title>Pannuke: An open pan-cancer histology dataset for nuclei instance segmentation and classification</article-title><source>Proceedings of the Digital Pathology: 15th European Congress</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B31-sensors-25-01207"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Deep residual learning for image recognition</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="B32-sensors-25-01207"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Russakovsky</surname><given-names>O.</given-names></name>
<name><surname>Deng</surname><given-names>J.</given-names></name>
<name><surname>Su</surname><given-names>H.</given-names></name>
<name><surname>Krause</surname><given-names>J.</given-names></name>
<name><surname>Satheesh</surname><given-names>S.</given-names></name>
<name><surname>Ma</surname><given-names>S.</given-names></name>
<name><surname>Huang</surname><given-names>Z.</given-names></name>
<name><surname>Karpathy</surname><given-names>A.</given-names></name>
<name><surname>Khosla</surname><given-names>A.</given-names></name>
<name><surname>Bernstein</surname><given-names>M.</given-names></name>
</person-group><article-title>Imagenet large scale visual recognition challenge</article-title><source>Int. J. Comput. Vis.</source><year>2015</year><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="B33-sensors-25-01207"><label>33.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>T.</given-names></name>
<name><surname>Kornblith</surname><given-names>S.</given-names></name>
<name><surname>Norouzi</surname><given-names>M.</given-names></name>
<name><surname>Hinton</surname><given-names>G.</given-names></name>
</person-group><article-title>A simple framework for contrastive learning of visual representations</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2020</conf-date><fpage>1597</fpage><lpage>1607</lpage></element-citation></ref><ref id="B34-sensors-25-01207"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Valanarasu</surname><given-names>J.M.J.</given-names></name>
<name><surname>Patel</surname><given-names>V.M.</given-names></name>
</person-group><article-title>Unext: Mlp-based rapid medical image segmentation network</article-title><source>Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</source><conf-loc>Singapore</conf-loc><conf-date>18&#x02013;22 September 2022</conf-date><fpage>23</fpage><lpage>33</lpage></element-citation></ref><ref id="B35-sensors-25-01207"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Diederik</surname><given-names>P.K.</given-names></name>
</person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1412.6980</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01207-f001"><label>Figure 1</label><caption><p>Samples from (<bold>a</bold>) LUSC, (<bold>b</bold>) BloodCell, and (<bold>c</bold>) PanNuke Datasets.</p></caption><graphic xlink:href="sensors-25-01207-g001" position="float"/></fig><fig position="float" id="sensors-25-01207-f002"><label>Figure 2</label><caption><p>Mixed-supervised tumor cell classification framework based on difficult sample screening mechanism (MIX-CC).</p></caption><graphic xlink:href="sensors-25-01207-g002" position="float"/></fig><fig position="float" id="sensors-25-01207-f003"><label>Figure 3</label><caption><p>Conceptual diagram of the fully supervised pretraining phase. (<bold>a</bold>) is the framework diagram of the fully supervised pretraining phase, while (<bold>b</bold>) and (<bold>c</bold>) are the structure diagrams of the ResNet18 and ResNet50 backbones that we use, respectively.</p></caption><graphic xlink:href="sensors-25-01207-g003" position="float"/></fig><fig position="float" id="sensors-25-01207-f004"><label>Figure 4</label><caption><p>The framework diagram of the mixed-supervised online learning phase.</p></caption><graphic xlink:href="sensors-25-01207-g004" position="float"/></fig><fig position="float" id="sensors-25-01207-f005"><label>Figure 5</label><caption><p>Confusion matrices of MIX-CC for (<bold>a</bold>) LUSC, (<bold>b</bold>) BloodCell, and (<bold>c</bold>) PanNuke datasets.</p></caption><graphic xlink:href="sensors-25-01207-g005" position="float"/></fig><fig position="float" id="sensors-25-01207-f006"><label>Figure 6</label><caption><p>Training convergence curves of MIX-CC for (<bold>a</bold>) LUSC, (<bold>b</bold>) BloodCell, and (<bold>c</bold>) PanNuke datasets in the first six iterations.</p></caption><graphic xlink:href="sensors-25-01207-g006" position="float"/></fig><table-wrap position="float" id="sensors-25-01207-t002"><object-id pub-id-type="pii">sensors-25-01207-t002_Table 2</object-id><label>Table 2</label><caption><p>Nomenclature.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Symbols</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Notations</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm124" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">height of the image</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm125" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">width of the image</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<italic toggle="yes">B</italic>
</td><td align="center" valign="middle" rowspan="1" colspan="1">number of channels in the image, where <inline-formula><mml:math id="mm126" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm127" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the input cell image patch</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm128" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the number of images in the training set</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm129" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the initial number of labeled samples</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm130" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>n</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the initial number of unlabeled samples</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm131" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the <inline-formula><mml:math id="mm132" overflow="scroll"><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mspace width="4.pt"/><mml:mi>th</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> image in the labeled dataset</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm133" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:munder><mml:mi mathvariant="normal">X</mml:mi><mml:mo>&#x0005f;</mml:mo></mml:munder><mml:mi>j</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the <inline-formula><mml:math id="mm134" overflow="scroll"><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mspace width="4.pt"/><mml:mi>th</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> image in the labeled dataset</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm135" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the label of the <inline-formula><mml:math id="mm136" overflow="scroll"><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mspace width="4.pt"/><mml:mi>th</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> image in the labeled dataset</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm137" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the prediction of the <inline-formula><mml:math id="mm138" overflow="scroll"><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mspace width="4.pt"/><mml:mi>th</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> image</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm139" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Conf</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the number of &#x0201c;credible&#x0201d; unlabeled samples selected</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm140" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Hard</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the number of &#x0201c;hard&#x0201d; unlabeled samples selected</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm141" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>rest</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the remaining unlabeled samples</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm142" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the number of
unlabeled samples randomly selected in the <inline-formula><mml:math id="mm143" overflow="scroll"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mspace width="4.pt"/><mml:mi>th</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> iteration</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm144" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">the total number of unlabeled samples selected in the <inline-formula><mml:math id="mm145" overflow="scroll"><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mspace width="4.pt"/><mml:mi>th</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> iteration</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm146" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">U</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">all selected unlabeled credible sample set</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01207-t003"><object-id pub-id-type="pii">sensors-25-01207-t003_Table 3</object-id><label>Table 3</label><caption><p>Model and Algorithm Settings for Cell Segmentation and Classification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Cell Segmentation Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Fully Supervised
Learning Model</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Semi-Supervised Learning Model</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">LUSC</td><td align="center" valign="middle" rowspan="1" colspan="1">UNeXt</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet18, SimCLR</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet18, SimCLR</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">BloodCell</td><td align="center" valign="middle" rowspan="1" colspan="1">UNeXt</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet18, SimCLR</td><td align="center" valign="middle" rowspan="1" colspan="1">ResNet18, SimCLR</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PanNuke</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">HoVer-Net</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50, SimCLR</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ResNet50, SimCLR</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01207-t004"><object-id pub-id-type="pii">sensors-25-01207-t004_Table 4</object-id><label>Table 4</label><caption><p>Parameter configuration for model training.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">DataSet</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">LUSC</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">BloodCell</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">PanNuke</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Epoch</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td><td align="center" valign="middle" rowspan="1" colspan="1">80</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Batch Size</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Learning Rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0003</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0002</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0002</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LR Decay Factor/Epoch</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6/5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.8/5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.6/5</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Weight Decay</td><td align="center" valign="middle" rowspan="1" colspan="1">8 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">8 &#x000d7; 10<sup>&#x02212;5</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">8 &#x000d7; 10<sup>&#x02212;5</sup></td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm150" overflow="scroll"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">300</td><td align="center" valign="middle" rowspan="1" colspan="1">300</td><td align="center" valign="middle" rowspan="1" colspan="1">1500</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm151" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Conf</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">300</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm152" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Hard</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">20</td><td align="center" valign="middle" rowspan="1" colspan="1">300</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm153" overflow="scroll"><mml:mrow><mml:msup><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Rest</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01207-t005"><object-id pub-id-type="pii">sensors-25-01207-t005_Table 5</object-id><label>Table 5</label><caption><p>Overall classification accuracy (OA) on LUSC dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Positive</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Negative</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SimCLR [<xref rid="B33-sensors-25-01207" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">70.09%</td><td align="center" valign="middle" rowspan="1" colspan="1">55.51%</td><td align="center" valign="middle" rowspan="1" colspan="1">67.38%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50 [<xref rid="B31-sensors-25-01207" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">73.38%</td><td align="center" valign="middle" rowspan="1" colspan="1">53.81%</td><td align="center" valign="middle" rowspan="1" colspan="1">69.74%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet18 [<xref rid="B31-sensors-25-01207" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">74.25%</td><td align="center" valign="middle" rowspan="1" colspan="1">51.69%</td><td align="center" valign="middle" rowspan="1" colspan="1">70.06%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIX-CC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.47%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.31%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.56%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01207-t006"><object-id pub-id-type="pii">sensors-25-01207-t006_Table 6</object-id><label>Table 6</label><caption><p>Overall classification accuracy (OA) on BloodCell dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Blast</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Lymphocyte</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Lymphoma</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SimCLR [<xref rid="B33-sensors-25-01207" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">95.89%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.75%</td><td align="center" valign="middle" rowspan="1" colspan="1">88.39%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.26%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50 [<xref rid="B31-sensors-25-01207" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">94.52%</td><td align="center" valign="middle" rowspan="1" colspan="1">96.38%</td><td align="center" valign="middle" rowspan="1" colspan="1">86.45%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.26%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet18 [<xref rid="B31-sensors-25-01207" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">96.58%</td><td align="center" valign="middle" rowspan="1" colspan="1">94.20%</td><td align="center" valign="middle" rowspan="1" colspan="1">90.97%</td><td align="center" valign="middle" rowspan="1" colspan="1">93.85%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIX-CC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.33%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100.00%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100.00%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.33%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01207-t007"><object-id pub-id-type="pii">sensors-25-01207-t007_Table 7</object-id><label>Table 7</label><caption><p>Overall classification accuracy (OA) on PanNuke dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Neoplastic</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Inflammatory</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Connective Tissue</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dead</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Non-Neoplastic</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SimCLR [<xref rid="B33-sensors-25-01207" ref-type="bibr">33</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">73.07%</td><td align="center" valign="middle" rowspan="1" colspan="1">50.96%</td><td align="center" valign="middle" rowspan="1" colspan="1">46.91%</td><td align="center" valign="middle" rowspan="1" colspan="1">41.69%</td><td align="center" valign="middle" rowspan="1" colspan="1">27.28%</td><td align="center" valign="middle" rowspan="1" colspan="1">55.69%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet50 [<xref rid="B31-sensors-25-01207" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">72.43%</td><td align="center" valign="middle" rowspan="1" colspan="1">54.24%</td><td align="center" valign="middle" rowspan="1" colspan="1">48.30%</td><td align="center" valign="middle" rowspan="1" colspan="1">46.34%</td><td align="center" valign="middle" rowspan="1" colspan="1">36.13%</td><td align="center" valign="middle" rowspan="1" colspan="1">57.57%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet18 [<xref rid="B31-sensors-25-01207" ref-type="bibr">31</xref>]</td><td align="center" valign="middle" rowspan="1" colspan="1">73.60%</td><td align="center" valign="middle" rowspan="1" colspan="1">51.63%</td><td align="center" valign="middle" rowspan="1" colspan="1">44.84%</td><td align="center" valign="middle" rowspan="1" colspan="1">46.78%</td><td align="center" valign="middle" rowspan="1" colspan="1">33.46%</td><td align="center" valign="middle" rowspan="1" colspan="1">56.27%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIX-CC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.12%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.75%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.09%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.56%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.86%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.12%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01207-t008"><object-id pub-id-type="pii">sensors-25-01207-t008_Table 8</object-id><label>Table 8</label><caption><p>Overall classification accuracy (OA) on LUSC dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Positive</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Negative</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SEMI-CC</td><td align="center" valign="middle" rowspan="1" colspan="1">78.61%</td><td align="center" valign="middle" rowspan="1" colspan="1">42.37%</td><td align="center" valign="middle" rowspan="1" colspan="1">71.87%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIX-CC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.47%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">34.31%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">86.56%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01207-t009"><object-id pub-id-type="pii">sensors-25-01207-t009_Table 9</object-id><label>Table 9</label><caption><p>Overall classification accuracy (OA) on BloodCell dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Blast</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Lymphocyte</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Lymphoma</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SEMI-CC</td><td align="center" valign="middle" rowspan="1" colspan="1">95.21%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.65%</td><td align="center" valign="middle" rowspan="1" colspan="1">92.26%</td><td align="center" valign="middle" rowspan="1" colspan="1">94.31%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIX-CC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">98.33%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100.00%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100.00%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.33%</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01207-t010"><object-id pub-id-type="pii">sensors-25-01207-t010_Table 10</object-id><label>Table 10</label><caption><p>Overall classification accuracy (OA) on PanNuke dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Neoplastic</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Inflammatory</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Connective Tissue</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Dead</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Non-Neoplastic</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">SEMI-CC</td><td align="center" valign="middle" rowspan="1" colspan="1">71.43%</td><td align="center" valign="middle" rowspan="1" colspan="1">51.11%</td><td align="center" valign="middle" rowspan="1" colspan="1">52.18%</td><td align="center" valign="middle" rowspan="1" colspan="1">45.23%</td><td align="center" valign="middle" rowspan="1" colspan="1">43.60%</td><td align="center" valign="middle" rowspan="1" colspan="1">58.62%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MIX-CC</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.12%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">66.75%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61.09%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">45.56%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">58.86%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.12%</td></tr></tbody></table></table-wrap></floats-group></article>