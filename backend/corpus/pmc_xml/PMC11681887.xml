<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id><journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id><journal-title-group><journal-title>Computational and Structural Biotechnology Journal</journal-title></journal-title-group><issn pub-type="epub">2001-0370</issn><publisher><publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11681887</article-id><article-id pub-id-type="pii">S2001-0370(24)00416-1</article-id><article-id pub-id-type="doi">10.1016/j.csbj.2024.11.045</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Detection and prioritization of COVID-19 infected patients from CXR images: Analysis of AI-assisted diagnosis in clinical settings</article-title></title-group><contrib-group><contrib contrib-type="author" id="au0010"><name><surname>Barbano</surname><given-names>Carlo Alberto</given-names></name><email>carlo.barbano@unito.it</email><xref rid="aff0010" ref-type="aff">a</xref><xref rid="cr0010" ref-type="corresp">&#x0204e;</xref></contrib><contrib contrib-type="author" id="au0020"><name><surname>Berton</surname><given-names>Luca</given-names></name><xref rid="aff0040" ref-type="aff">d</xref></contrib><contrib contrib-type="author" id="au0030"><name><surname>Renzulli</surname><given-names>Riccardo</given-names></name><xref rid="aff0010" ref-type="aff">a</xref></contrib><contrib contrib-type="author" id="au0040"><name><surname>Tricarico</surname><given-names>Davide</given-names></name><xref rid="aff0030" ref-type="aff">c</xref></contrib><contrib contrib-type="author" id="au0050"><name><surname>Rampado</surname><given-names>Osvaldo</given-names></name><xref rid="aff0040" ref-type="aff">d</xref></contrib><contrib contrib-type="author" id="au0060"><name><surname>Basile</surname><given-names>Domenico</given-names></name><xref rid="aff0020" ref-type="aff">b</xref></contrib><contrib contrib-type="author" id="au0070"><name><surname>Busso</surname><given-names>Marco</given-names></name><xref rid="aff0020" ref-type="aff">b</xref></contrib><contrib contrib-type="author" id="au0080"><name><surname>Grosso</surname><given-names>Marco</given-names></name><xref rid="aff0020" ref-type="aff">b</xref></contrib><contrib contrib-type="author" id="au0090"><name><surname>Grangetto</surname><given-names>Marco</given-names></name><xref rid="aff0010" ref-type="aff">a</xref></contrib><aff id="aff0010"><label>a</label>Computer Science Dept., University of Turin, Italy</aff><aff id="aff0020"><label>b</label>Azienda Sanitaria Locale TO3, Italy</aff><aff id="aff0030"><label>c</label>AITEM Solutions s.r.l., Italy</aff><aff id="aff0040"><label>d</label>Medical Physics Department, A.O.U. Citt&#x000e0; della Salute e della Scienza di Torino, Turin, Italy</aff></contrib-group><author-notes><corresp id="cr0010"><label>&#x0204e;</label>Corresponding author. <email>carlo.barbano@unito.it</email></corresp></author-notes><pub-date pub-type="pmc-release"><day>05</day><month>12</month><year>2024</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="collection"><month>12</month><year>2024</year></pub-date><pub-date pub-type="epub"><day>05</day><month>12</month><year>2024</year></pub-date><volume>24</volume><fpage>754</fpage><lpage>761</lpage><history><date date-type="received"><day>31</day><month>8</month><year>2024</year></date><date date-type="rev-recd"><day>28</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>29</day><month>11</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 The Author(s)</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p></license></permissions><abstract id="ab0010"><p>In this paper, we present the significant results from the Covid Radiographic imaging System based on AI (Co.R.S.A.) project, which took place in Italy. This project aims to develop a state-of-the-art AI-based system for diagnosing Covid-19 pneumonia from Chest X-ray (CXR) images. The contributions of this work are manifold: the release of the public CORDA dataset, a deep learning pipeline for Covid-19 detection and prioritization, the clinical validation of the developed solution by expert radiologists, and an in-depth analysis of possible biases embedded in the data and in the models, in order to build more trust in our AI-based pipeline. The proposed detection model is based on a two-step approach that provides reliable results based on objective radiological findings. Our prioritization scheme ensures the ordering of the patients so that severe cases are presented first. We showcase the impact of our pipeline on radiologists' workflow with a clinical study, allowing us to assess the real benefits in terms of accuracy and time efficiency. Project homepage: <ext-link ext-link-type="uri" xlink:href="https://corsa.di.unito.it/" id="inf0010">https://corsa.di.unito.it/</ext-link>.</p></abstract><abstract abstract-type="graphical" id="ab0020"><title>Graphical abstract</title><fig id="fg0070" position="anchor"><graphic xlink:href="gr001" id="ln0070"/></fig></abstract><abstract abstract-type="author-highlights" id="ab0030"><title>Highlights</title><p><list list-type="simple" id="ls0010"><list-item id="li0010"><label>&#x02022;</label><p id="pr0010">Presentation of the Co.R.S.A. research project, from the data collection (publicly available) to the clinical validation.</p></list-item><list-item id="li0020"><label>&#x02022;</label><p id="pr0020">Deep learning pipeline for COVID-19 detection.</p></list-item><list-item id="li0030"><label>&#x02022;</label><p id="pr0030">Deep learning pipeline for COVID-19 prioritization.</p></list-item><list-item id="li0040"><label>&#x02022;</label><p id="pr0040">Impact of AI-assisted tools in clinical settings for COVID-19.</p></list-item><list-item id="li0050"><label>&#x02022;</label><p id="pr0050">In-depth analysis of possible biases embedded in the data and in the models.</p></list-item></list></p></abstract></article-meta></front><body><sec id="se0010"><label>1</label><title>Introduction</title><p id="pr0060">COVID-19 is an infection of the respiratory system that mainly affects the lungs. Common symptoms include fever, fatigue, cough, breathing difficulties, loss of smell, and loss of taste. COVID-19 is very contagious and, starting from the beginning of 2020, quickly spread worldwide, resulting in the COVID-19 pandemic <xref rid="br0010" ref-type="bibr">[1]</xref>. By August 2024, the global tally surpassed 770 million confirmed cases, resulting in over 7 million deaths.<xref rid="fn0010" ref-type="fn">1</xref> Radiological imaging has since become recognized as a valuable tool in detecting COVID-19, even at its early stages. Both chest X-rays (CXR) and computed tomography (CT) scans play a significant role in diagnosis. Clinical studies indicate that CT scans are the most accurate imaging modality for diagnosing COVID-19 <xref rid="br0020" ref-type="bibr">[2]</xref>. However, logistical challenges such as the need for sanitizing rooms, ensuring proper protective gear, radioprotection, and the high costs associated with CT make its routine use difficult in early patient management. In contrast, CXR is often the preferred method in emergency settings due to its ease of use, lower radiation exposure, and comparatively lower costs.</p><p id="pr0070">In 2022, the Piedmont Region financed the Covid Radiographic Imaging System based on AI (Co.R.S.A.) initiative. This collaborative effort between the University of Turin, two hospital radiology units (A.O.U Citt&#x000e1; della Salute e della Scienza, Azienda Sanitaria ASL TO3), and the company REGOLA aimed to create and field-validate an advanced AI system to assist in diagnosing COVID-19 pneumonia from CXR images.</p><p id="pr0080">The main achievements and contributions of this work include:<list list-type="simple" id="ls0020"><list-item id="li0060"><label>1.</label><p id="pr0090">The creation and public release of the CORDA dataset, along with a thorough assessment of the potential stratification factors (e.g., location, gender, imaging techniques) that could affect AI tools;</p></list-item><list-item id="li0070"><label>2.</label><p id="pr0100">The development of a robust deep learning framework for prioritizing and detecting COVID-19 cases;</p></list-item><list-item id="li0080"><label>3.</label><p id="pr0110">Clinical validation of the system by a panel of expert radiologists.</p></list-item></list></p><p id="pr0120">Our validation protocol stands out as the most significant among these contributions, and it also includes the evaluation of possible biases in the CORDA dataset. Prior to integrating an AI tool into clinical practice for diagnostic purposes, it must undergo a thorough evaluation to uncover any limitations or potential biases in its classification outcomes. We consider this analysis essential for establishing trust in these systems and minimizing the likelihood of errors in their application. By incorporating expert assessment, this project bridges that gap, greatly enhancing the clinical relevance of its findings. Understanding the limitations of AI tools and potential biases of data is especially relevant in clinical settings. The importance of such a topic is also noted by <xref rid="br0030" ref-type="bibr">[3]</xref>, which highlights how discovering these limitations enables managers and medical personnel to better understand the risks and benefits of each tool. If not addressed, opacity in the behavior of AI tools may lead to more uncertainty in professionals <xref rid="br0040" ref-type="bibr">[4]</xref>. For this reason, it is important to design user-centered systems which are based on human-in-the-loop, especially when dealing with AI tools <xref rid="br0050" ref-type="bibr">[5]</xref>.</p><p id="pr0130">Although the peak of the COVID-19 pandemic has subsided, the outcomes of this project lay a solid foundation for the quick deployment of similar diagnostic systems in future epidemic responses, ensuring preparedness for emerging public health threats.</p></sec><sec id="se0020"><label>2</label><title>Results</title><p id="pr0140">In this section, we present our findings on the impact of our AI-assisted pipeline in clinical settings. We also perform an in-depth analysis of potential biases in the data. First, in Sec. <xref rid="se0030" ref-type="sec">2.1</xref>, we focus on COVID-19 detection; then, in Sec. <xref rid="se0080" ref-type="sec">2.2</xref>, we analyze potential sources of biases either in the data or in the models; lastly, in Sec. <xref rid="se0130" ref-type="sec">2.3</xref> we present the results of our prioritization pipeline.</p><sec id="se0030"><label>2.1</label><title>Impact of COVID-19 CXR screening</title><p id="pr0150">This section focuses on assessing the impact of AI-aided diagnosis in the radiologists' workflow, one of the key aspects of our project. To this end, the clinical validation focuses on measuring two Key Performance Indicators (KPIs): <italic>i)</italic> the accuracy of the radiologists' diagnosis <italic>ii.)</italic> the time needed to formulate it.</p><sec id="se0040"><label>2.1.1</label><title>Materials and tools</title><sec id="se0050"><title>Training data</title><p id="pr0160">For training the proposed deep learning models, we employ the CORDA dataset <xref rid="br0060" ref-type="bibr">[6]</xref>, comprising 1601 CXR images of both COVID-19 positive and negative patients. We made the dataset publicly available for download in January 2023 <xref rid="br0060" ref-type="bibr">[6]</xref>. This dataset aims to provide a multi-center collection of radiographic images for COVID-19 detection, in order to build more robust machine learning algorithms and models. The dataset curation is part of the ongoing project Co.R.S.A. CORDA comprises four different Italian hospitals:<list list-type="simple" id="ls0030"><list-item id="li0090"><label>1.</label><p id="pr0170">A.O.U. Citt&#x000e0; della Salute e delle Scienza (Molinette), Torino (CDSS);</p></list-item><list-item id="li0100"><label>2.</label><p id="pr0180">A.O.U. San Luigi Gonzata, Orbassano (SLG);</p></list-item><list-item id="li0110"><label>3.</label><p id="pr0190">A.O. Mauriziano, Torino (MRZ);</p></list-item><list-item id="li0120"><label>4.</label><p id="pr0200">Centro Cardiologico Monzino, Milano (MNZ).</p></list-item></list></p><p id="pr0210">The dataset composition is summarized in <xref rid="tbl0010" ref-type="table">Table 1</xref>. We report the composition of the data in terms of imaging modality (either Computed radiography - CR or Digital radiography - DR) and patients' sex. The ground truth in the dataset for COVID-19 is provided as a binary label (positive or negative). It was obtained by using nasopharyngeal swabs, the gold standard for COVID-19 testing.<table-wrap position="float" id="tbl0010"><label>Table 1</label><caption><p>CORDA dataset composition: Covid-19 positive and negative cases, imaging modality and patient sex and age divided by institution.</p></caption><alt-text id="at0070">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="2" align="left">COVID<hr/></th><th colspan="2" align="left">Modality<hr/></th><th colspan="3" align="left">Sex<hr/></th><th colspan="3" align="left">Age<hr/></th></tr><tr><th>Institution</th><th>Neg.</th><th>Pos.</th><th>CR</th><th>DR</th><th>M</th><th>F</th><th>N/A</th><th>Min.</th><th>Mean/Median</th><th>Max.</th></tr></thead><tbody><tr><td>CDSS</td><td>183</td><td>362</td><td>400</td><td>145</td><td>331</td><td>193</td><td>21</td><td>16</td><td>60.66/61</td><td>100</td></tr><tr><td>SLG</td><td>477</td><td>250</td><td>97</td><td>630</td><td>1</td><td>1</td><td>725</td><td>&#x02013;</td><td>&#x02013;</td><td>&#x02013;</td></tr><tr><td>MRZ</td><td>35</td><td>138</td><td>12</td><td>161</td><td>115</td><td>58</td><td/><td>22</td><td>63.52/65</td><td>95</td></tr><tr><td>MNZ</td><td>0</td><td>156</td><td>0</td><td>156</td><td>105</td><td>51</td><td/><td>28</td><td>73.49/72</td><td>94</td></tr><tr><td colspan="11" align="left">&#x02028;&#x02028;</td></tr><tr><td>Total</td><td>695</td><td>906</td><td>509</td><td>1092</td><td>552</td><td>303</td><td>746</td><td>16</td><td>70.7/69</td><td>100</td></tr></tbody></table></table-wrap></p><p id="pr0220"><bold>External Validation data</bold> As validation data, 100 external images were collected at ASL TO3, 50 of which are from COVID-19 patients and 50 are control cases. These images are not part of the CORDA dataset and were not used for previous phases. This external validation also allows us to assess whether the proposed method can generalize to novel data and sites.</p><p id="pr0230"><bold>Dicom Viewer</bold> For this validation, we developed a custom DICOM image viewer, which allows deep neural network (DNN) predictions to be shown alongside the analyzed image. Our software automatically collects the diagnosis made by the radiologist for each image and the time taken to make it. Primary functionalities of a standard DICOM viewer are included, such as image manipulation (e.g. zoom, translation) and adjustable windowing (VOI LUT). <xref rid="fg0010" ref-type="fig">Fig. 1</xref>c depicts an example of DNN predictions as shown to the evaluating radiologist in our viewer. Our DICOM viewer also collects other information such as the time to diagnose each patient.<fig id="fg0010"><label>Fig. 1</label><caption><p>AI-assisted diagnosis helps radiologists make more accurate and faster diagnoses. In <bold>(a)</bold>, we report the average ROC curve of a pool of radiologists during external validation in a blind setting (no AI) and in an AI-assisted setting. In <bold>(b)</bold>, we compare the average diagnosis time (per image) in a blind vs. assisted setting. We notice that the average AUC increases in the AI-assisted setting while the diagnosis is quicker on average. <bold>(c)</bold> Example of AI report in our custom DICOM viewer. This report is shown to the evaluating radiologist alongside the CXR image. The information includes the predicted probability of COVID-19 infection and other relevant lung pathologies. For easiness of readability, probabilities &#x0003e;0.5 are marked in red (except for the &#x0201c;No Finding&#x0201d; class).</p></caption><alt-text id="at0010">Fig. 1</alt-text><graphic xlink:href="gr002" id="ln0010"/></fig></p><p id="pr0590"><bold>Neural Network Training</bold> We employ a two-step approach to train our deep neural networks (DNNs), presented in-depth in <xref rid="br0070" ref-type="bibr">[7]</xref>, <xref rid="br0080" ref-type="bibr">[8]</xref>. In short, we first train a DenseNet-121 <xref rid="br0090" ref-type="bibr">[9]</xref> based model on the large-scale dataset CheXpert <xref rid="br0100" ref-type="bibr">[10]</xref>. CheXpert provides labels for 14 different radiological findings, namely <italic>No Finding, Enlarged Cardiomediastinum, Cardiomegaly, Lung Lesion, Lung Opacity, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Pleural Effusion, Pleural Other, Fracture, Support Devices.</italic> Additional details about the pre-training protocol are presented in the supplementary material. After pre-training on CheXpert, we perform a transfer learning step on the CORDA dataset <xref rid="br0060" ref-type="bibr">[6]</xref>, employing debiasing regularization <xref rid="br0190" ref-type="bibr">[19]</xref>.</p></sec></sec><sec id="se0060"><label>2.1.2</label><title>Validation protocol</title><p id="pr0250">To perform the clinical evaluation, we employ a pool of 6 radiologists from Ospedale di Rivoli and ASL TO3, with different experience levels. They are summarized in <xref rid="tbl0020" ref-type="table">Table 2</xref>. For each image, a severity score <xref rid="br0110" ref-type="bibr">[11]</xref> is assigned by each radiologist upon visual inspection, ranging from 0 (healthy) to 18 (maximum severity). During diagnosis, the radiologists do not know the correct label for a given patient. The experiment is repeated in two settings, with a wash-out period in between:<list list-type="simple" id="ls0040"><list-item id="li0130"><label>&#x02022;</label><p id="pr0260"><italic>blind</italic> setting: radiologists can only leverage the image to make a diagnosis.</p></list-item><list-item id="li0140"><label>&#x02022;</label><p id="pr0270"><italic>AI-assisted</italic> setting: the predicted probabilities of our DNNs are shown alongside the image.</p></list-item></list><table-wrap position="float" id="tbl0020"><label>Table 2</label><caption><p>List of the radiologists involved in the clinical validation study.</p></caption><alt-text id="at0080">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Initials and ID</th><th>Affiliation</th><th>Years of Experience</th></tr></thead><tbody><tr><td>S. N. (1)</td><td>ASL TO3 Rivoli</td><td>1</td></tr><tr><td>V. N. (2)</td><td>ASL TO3 Rivoli</td><td>1</td></tr><tr><td>D. C. (3)</td><td>ASL TO3 Rivoli</td><td>2</td></tr><tr><td>M. B. (4)</td><td>ASL TO3 Rivoli</td><td>5</td></tr><tr><td>F. L. (5)</td><td>ASL TO3 Rivoli</td><td>10</td></tr><tr><td>P. M. (6)</td><td>ASL TO3 Rivoli</td><td>15</td></tr></tbody></table></table-wrap></p></sec><sec id="se0070"><label>2.1.3</label><title>Validation results</title><p id="pr0280"><xref rid="fg0010" ref-type="fig">Fig. 1</xref> shows the overall results of the validation in terms of prediction AUC and diagnosis time. In <xref rid="fg0010" ref-type="fig">Fig. 1</xref>a, the AUC is computed on the radiologists' average severity score assigned for each patient. We observe an improvement in the assisted setting compared to the blind evaluation. In <xref rid="fg0010" ref-type="fig">Fig. 1</xref>b, we plot the diagnosis time in the two settings: on average (blue line), the time required for diagnosis decreases in the assisted setting. In <xref rid="fg0020" ref-type="fig">Fig. 2</xref>, we show a breakout of the results for each participating radiologist for prediction AUC and diagnosis time. In most cases, we observe an improved performance in the assisted setting, with some significant improvement (e.g. AUC increases from 0.85 to 0.96 for radiologist #1). These results show that more accurate and faster diagnosis can be achieved by employing AI systems in a real-world clinical setting.<fig id="fg0020"><label>Fig. 2</label><caption><p>Break-out of diagnosis performance by radiologist. (a) shows the AUC score of blind and AI-assisted evaluation. While for some radiologists, especially those with higher blind scores, we notice a slight decrease in AUC, the improvement is consistent in those who achieve a lower base score. On average, the AUC increases from 0.85 to 0.88, as shown in <xref rid="fg0010" ref-type="fig">Fig. 1</xref>. (b) compares the blind diagnosis time with the AI-assisted diagnosis time. The blue line represents a linear regression line, and the dashed black line represents the identity. We can see that, on average, the diagnosis time decreases as the fitted line lies below the identity for almost all radiologists.</p></caption><alt-text id="at0020">Fig. 2</alt-text><graphic xlink:href="gr003" id="ln0020"/></fig></p></sec></sec><sec id="se0080"><label>2.2</label><title>Evaluation of possible biases in the CORDA dataset</title><p id="pr0290">In this section, we evaluate possible sources of bias in the training dataset which can influence the final model. Before using an AI tool for diagnosis in the clinical routine, it should be rigorously evaluated to identify its limitations and possible biases in the classification output. We believe this analysis is fundamental to building trust in such tools and reducing the risk of errors in their usage. We focus our analysis on the differences in the distribution of imaging modality (CR or DR), the number of COVID cases, and patients' sex and age in the different institutions. We also evaluate the differences in the distributions of exposure parameters that influence the image quality of the CXR between positive and negative cases. Furthermore, we analyze whether our model shows significant performance differences among the centers, image modalities, patient sex, and projection type.</p><sec id="se0090"><label>2.2.1</label><title>Dataset imbalance</title><p id="pr0300">As shown in <xref rid="tbl0010" ref-type="table">Table 1</xref>, the dataset is imbalanced among the different institutions concerning the number of positive and negative cases, imaging modality, and patient sex. The number of images from the different institutions varies widely, mostly from CDSS and SLG. It should be noted that for some patients from CDSS and most of those from SLG, the information regarding the patient's sex (and age) was not available in the DICOM tags, and the sex was labeled as NA. There is also a significant difference in the number of CR and DR images among the different institutions since they have an evident prevalence of one modality over the other; for example, CDSS provides most CR images. Furthermore, the DR images are twice as many as CR images. Moreover, there is an imbalance in the number of positive and negative cases among the different institutions if divided by imaging modality, as seen in <xref rid="tbl0030" ref-type="table">Table 3</xref>.<table-wrap position="float" id="tbl0030"><label>Table 3</label><caption><p>Number of positive and negative cases divided by institution and imaging modality.</p></caption><alt-text id="at0090">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Institution</th><th>COVID-19</th><th>DR Images</th><th>CR Images</th></tr></thead><tbody><tr><td rowspan="2">CDSS</td><td>0</td><td>16</td><td>167</td></tr><tr><td>1</td><td>129</td><td>233</td></tr><tr><td colspan="4" align="left">&#x02028;&#x02028;</td></tr><tr><td rowspan="2">SLG</td><td>0</td><td>419</td><td>58</td></tr><tr><td>1</td><td>211</td><td>39</td></tr><tr><td colspan="4" align="left">&#x02028;&#x02028;</td></tr><tr><td rowspan="2">MRZ</td><td>0</td><td>32</td><td>4</td></tr><tr><td>1</td><td>129</td><td>8</td></tr><tr><td colspan="4" align="left">&#x02028;&#x02028;</td></tr><tr><td rowspan="2">MNZ</td><td>0</td><td>0</td><td>0</td></tr><tr><td>1</td><td>156</td><td>0</td></tr><tr><td colspan="4" align="left">&#x02028;&#x02028;</td></tr><tr><td rowspan="2">Total</td><td>0</td><td>467</td><td>229</td></tr><tr><td>1</td><td>625</td><td>280</td></tr></tbody></table></table-wrap></p></sec><sec id="se0100"><label>2.2.2</label><title>Differences of the distributions of acquisition parameters</title><p id="pr0310">We use the Mann-Whitney U-test, also known as the Wilcoxon test, to assess if there are differences between positive and negative images in the distributions of parameters that may influence image quality. Such differences between the two groups may play a role in the final classification performed by our model. They could introduce a source of bias when the model is applied to external data. The Mann-Whitney U-test is a non-parametric statistical test to compare two distributions and has the advantage of not assuming a specific distribution <xref rid="br0120" ref-type="bibr">[12]</xref>
<xref rid="br0130" ref-type="bibr">[13]</xref>.</p><p id="pr0320">The parameters' distributions are shown in <xref rid="fg0030" ref-type="fig">Fig. 3</xref>. We notice that the dose-area product (DAP) is significantly higher for negative patients (p-value=<inline-formula><mml:math id="M1" altimg="si1.svg"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo linebreak="badbreak" linebreakstyle="after">&#x02212;</mml:mo><mml:mn>59</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>), the exposure (in <italic>&#x003bc;A</italic>) is higher for negative ones (p-value=<inline-formula><mml:math id="M2" altimg="si2.svg"><mml:mn>6.5</mml:mn><mml:mo>&#x022c5;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo linebreak="badbreak" linebreakstyle="after">&#x02212;</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). We also noted a significant difference in the distributions of the exposure time (p-value=<inline-formula><mml:math id="M3" altimg="si3.svg"><mml:mn>9</mml:mn><mml:mo>&#x022c5;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo linebreak="badbreak" linebreakstyle="after">&#x02212;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). These differences might be caused by the fact that the CR images of the dataset, which have a higher fraction of negative patients than DR images, usually have higher exposure parameters.<fig id="fg0030"><label>Fig. 3</label><caption><p>Distributions of the exposure parameters, age, and average image intensity, with the p-value of the Mann-Whitney U-test of each comparison.</p></caption><alt-text id="at0030">Fig. 3</alt-text><graphic xlink:href="gr004" id="ln0030"/></fig></p><p id="pr0330">Finally, the value of kVp (which influences the contrast of the image) is significantly lower for the positive cases (p-value=<inline-formula><mml:math id="M4" altimg="si4.svg"><mml:mn>2</mml:mn><mml:mo>&#x022c5;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo linebreak="badbreak" linebreakstyle="after">&#x02212;</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). Moreover, we found a significant difference in the average image intensity between CR and DR images, with DR images showing lower values (p-value=<inline-formula><mml:math id="M5" altimg="si5.svg"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo linebreak="badbreak" linebreakstyle="after">&#x02212;</mml:mo><mml:mn>34</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). This may reflect the intrinsic physical differences between the two imaging modalities. We also compared the age distribution of positive and negative patients, finding that positive patients are significantly older (p-value=<inline-formula><mml:math id="M6" altimg="si6.svg"><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo linebreak="badbreak" linebreakstyle="after">&#x02212;</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>). This reflects the fact that hospitalization due to COVID-19 is more frequent for older people.</p></sec><sec id="se0110"><label>2.2.3</label><title>Differences in classification performance</title><p id="pr0340">Our bias analysis ends with focusing on the model's performance on different significant subgroups of the dataset to highlight if they influence the classification. We perform this analysis by calculating the AUC of our model for each subgroup, with a 95% confidence interval (C.I.). The confidence intervals are evaluated using the pROC package for R <xref rid="br0140" ref-type="bibr">[14]</xref>, which uses the DeLong statistics to calculate them <xref rid="br0150" ref-type="bibr">[15]</xref>. The results are presented in <xref rid="tbl0040" ref-type="table">Table 4</xref>, <xref rid="tbl0070" ref-type="table">Table 5</xref>.<table-wrap position="float" id="tbl0040"><label>Table 4</label><caption><p>Model performance by institution and image modality.</p></caption><alt-text id="at0100">Table 4</alt-text><table frame="hsides" rules="groups"><tbody><tr><td><p><table-wrap position="float" id="tbl0050"><table frame="hsides" rules="groups"><thead><tr><th>Institution</th><th>AUC (95% C.I.)</th></tr></thead><tbody><tr><td>MRZ</td><td>0.82 (0.75-0.89)</td></tr><tr><td>CDSS</td><td>0.75 (0.71-0.79)</td></tr><tr><td>SLG</td><td>0.89 (0.87-0.92)</td></tr><tr><td>MNZ</td><td>NA</td></tr></tbody></table></table-wrap></p></td><td><p><table-wrap position="float" id="tbl0060"><table frame="hsides" rules="groups"><thead><tr><th>Modality</th><th>AUC (95% C.I.)</th></tr></thead><tbody><tr><td>CR</td><td>0,72 (0,68-0,77)</td></tr><tr><td>DR</td><td>0,91 (0,90-0,93)</td></tr></tbody></table></table-wrap></p></td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl0070"><label>Table 5</label><caption><p>Model performance by patient's sex and projection type.</p></caption><alt-text id="at0110">Table 5</alt-text><table frame="hsides" rules="groups"><tbody><tr><td><p><table-wrap position="float" id="tbl0080"><table frame="hsides" rules="groups"><thead><tr><th>Sex</th><th>AUC (95% C.I.)</th></tr></thead><tbody><tr><td>M</td><td>0,79 (0,75-0,84)</td></tr><tr><td>F</td><td>0,74 (0,69-0,80)</td></tr><tr><td>NA</td><td>0,89 (0,86-0,92)</td></tr></tbody></table></table-wrap></p></td><td><p><table-wrap position="float" id="tbl0090"><table frame="hsides" rules="groups"><thead><tr><th>Projection</th><th>AUC (95% C.I.)</th></tr></thead><tbody><tr><td>AP</td><td>0,88 (0,84-0,92)</td></tr><tr><td>PA</td><td>0,86 (0,80-0,92)</td></tr><tr><td>LAT</td><td>0,90 (0,83-0,96)</td></tr></tbody></table></table-wrap></p></td></tr></tbody></table></table-wrap></p><p id="pr0350">First, we separate the data by institution. For MNZ, it is impossible to compute an AUC score since all images are positive cases. Our analysis shows that the model has different classification capabilities for different institutions: SLG has the highest AUC, while CDSS has the lowest.</p><p id="pr0360">Then, we analyze the data by imaging modality, and we find a significant difference in the classification performance: for CR images, the model achieves an AUC=0.72 (95% C.I.=0.68-0.77), while for DR images, it achieves an AUC=0.91 (95% C.I.=0.90-0.93). This may be due to the different number of images of the two modalities. Still, the intrinsic differences in image quality between CR and DR images may play an essential role in this difference, with DR images having higher contrast and resolution. This may also partially explain the difference in performance among the different institutions, considering the different distribution of CR and DR images: CDSS has the highest fraction of CR images, while SLG has the most DR images.</p><p id="pr0370">We also observe that there is a difference in the classification capability considering the sex of the patients: female and male patients have a lower AUC compared to patients with no data regarding their sex, with respectively females with an AUC=0.74 (95% C.I.=0.69-0.80), males with AUC=0.79 (95% C.I.=0.75-0.84) and NA with AUC=0.89 (95% C.I.=0.86-0.92). This may be partially explained by the fact that most NA patients are from institution SLG, which is the one where our model achieves the best classification accuracy. Lastly, we divided the images by the type of projection, according to the DICOM tag &#x0201c;view position&#x0201d;: anterior-posterior (AP), posterior-anterior (PA), or lateral (LAT). The performance of the model does not show a significant difference among the different projections: AP images have AUC=0.88 (95% C.I.=0.84-0.92), PA images have AUC=0.86 (95% C.I.=0.80-0.92) and LAT images have AUC=0.90 (95% C.I.=0.83-0.96).</p></sec><sec id="se0120"><label>2.2.4</label><title>Takeaways</title><p id="pr0380">Our model exhibits different results in different population groups, even though some precautions, such as site regularization, were employed. This is an essential factor to consider when deploying such systems in real-world scenarios, as these differences can ultimately influence the model's predictions. Even with these issues, however, our system can still significantly improve the quality and speed of the diagnosis, as shown by our clinical validation.</p></sec></sec><sec id="se0130"><label>2.3</label><title>Results of the prioritization system</title><p id="pr0390">This section presents the results of our AI-based prioritization model for COVID-19 patients. In the context of an emergency department, implementing a tool capable of prioritizing the most suspicious CXRs would be of significant value. Such a tool could facilitate the rapid identification of potential COVID-19 cases and minimize exposure risks in waiting areas. To address this need, we evaluated the effectiveness of a CNN-based prioritization model designed to identify potentially suspicious CXRs and position them at the top of the radiologists' workflow. To evaluate the system's efficiency in prioritizing cases with a higher likelihood of being positive, we analyzed the rankings generated using the Mean Average Precision (mAP) metric <xref rid="br0160" ref-type="bibr">[16]</xref>. A higher mAP score indicates a greater prioritization of positive cases over negative ones. In the <italic>blind</italic> setting, the mAP was 47%, whereas in the <italic>AI-assisted</italic> setting, we achieved an mAP of 78%. These results demonstrate the effectiveness of the proposed system in assigning higher priority to positive cases compared to negative ones.</p><p id="pr0400">A more in-depth analysis was conducted by examining the distribution of the prioritization index between positive and negative cases. This distribution is represented in <xref rid="fg0040" ref-type="fig">Fig. 4</xref>a, highlighting that negative cases are assigned a significantly lower priority than positive ones. Specifically, most non-COVID-19 patients had a prioritization value close to or equal to zero. The median prioritization for negative patients was 11%, while for positive patients it reached 46%, resulting in a substantial gap. Over 60% of negative cases exhibited a prioritization index below 20%, in contrast to only 20% of positive cases. A comparison between the physicians' evaluation in terms of severity score and the prioritization index indicates a general increase in the latter for more severe infection cases, with a correlation of 54%. <xref rid="fg0040" ref-type="fig">Fig. 4</xref>b demonstrates how the distribution of prioritization values shifts towards higher levels as case severity increases, highlighting a marked prioritization of more severe infections.<fig id="fg0040"><label>Fig. 4</label><caption><p>(a) Distribution of prioritization index in the <italic>AI-assisted</italic> setting. (b) Distribution of priority index for different ranges of severity scores.</p></caption><alt-text id="at0040">Fig. 4</alt-text><graphic xlink:href="gr005" id="ln0040"/></fig></p><p id="pr0410">To further illustrate the impact of our prioritization pipeline, we conduct a Monte Carlo simulation where each patient's waiting time (time to diagnosis) is computed as the sum of the total time radiologists take to produce a diagnosis for all the previous patients. During the external validation, we tracked the time taken by each radiologist to decide on each patient with our custom DICOM viewer (see Sec. <xref rid="se0150" ref-type="sec">3.1</xref>). For each patient, we compute the normal distribution statistics using the six different measures of diagnosis time (one for each radiologist), and we use them to sample N=10 values. We then employ three different ordering schemes:<list list-type="simple" id="ls0050"><list-item id="li0150"><label>1.</label><p id="pr0420">A random order, where each patient can randomly appear after any other patient.</p></list-item><list-item id="li0160"><label>2.</label><p id="pr0430">A <italic>oracle</italic> order, where patients are ordered based on the average severity score assigned by radiologists.</p></list-item><list-item id="li0170"><label>3.</label><p id="pr0440">An order based on the prioritization score of our neural network.</p></list-item></list></p><p id="pr0450">For each scheme, we repeat the simulation using all the possible values sampled before to obtain an average prioritization index, that we assign to each patient. Intuitively, if the ordering computed by the neural network is close to the ordering obtained by the real scores by radiologists, then our pipeline will successfully prioritize severe patients. The results are illustrated in <xref rid="fg0050" ref-type="fig">Fig. 5</xref>. The advantage in waiting times for positive patients, between the <italic>prioritization</italic> and <italic>random baseline</italic> settings, is evident, as the waiting time tends to decrease with higher parenchyma scores. In contrast, the random baseline remains nearly constant. Furthermore, looking at the <italic>prioritization</italic> and <italic>radiologist</italic> curves, we can clearly observe how they follow the same trend, meaning that the prioritization is actually working as intended. Notably, for the most severe cases, we can observe a reduction of waiting times of a factor of almost x10.<fig id="fg0050"><label>Fig. 5</label><caption><p>Average waiting time for patients with different severity scores assigned by radiologists with different settings: <italic>blind</italic>, radiologist based and <italic>AI-assisted</italic>.</p></caption><alt-text id="at0050">Fig. 5</alt-text><graphic xlink:href="gr006" id="ln0050"/></fig></p></sec></sec><sec id="se0140"><label>3</label><title>Methods</title><p id="pr0460">In this section we present the methods that we employed for training our neural models.</p><sec id="se0150"><label>3.1</label><title>COVID-19 detection</title><p id="pr0470">The method that we propose consists of two steps: first, we pre-train a deep neural network on a large-scale CXR dataset, with the aim of detecting objective radiological findings, then we apply transfer learning to train a Covid-19 classifier on the CORDA dataset. As previously shown in <xref rid="br0080" ref-type="bibr">[8]</xref>, this approach provides reliable results. Additionally, as shown in <xref rid="br0070" ref-type="bibr">[7]</xref>, we also employ the FairKL regularization technique <xref rid="br0070" ref-type="bibr">[7]</xref> to partially mitigate the site effect of medical data <xref rid="br0170" ref-type="bibr">[17]</xref>.</p><p id="pr0480"><bold>Pre-training on objective radiological findings</bold> We leverage a large-scale dataset, <italic>CheXpert</italic>
<xref rid="br0100" ref-type="bibr">[10]</xref>, which contains annotation for different kinds of common radiological findings that can be observed in CXR images (like opacity, pleural effusion, cardiomegaly, etc.). This large dataset is well suited for multi-label classification tasks; in fact, more than one finding can be commonly observed simultaneously in ill patients' lungs. CheXpert provides 14 different types of observations for each image in the dataset. We use the setup described in <xref rid="br0080" ref-type="bibr">[8]</xref>, consisting of a slightly modified DenseNet-121 <xref rid="br0090" ref-type="bibr">[9]</xref> architecture. More details about pre-training can be found in the Appendix.</p><p id="pr0490"><bold>Covid-19 Prediction</bold> To obtain a COVID-19 classifier, we employ the DNN encoder pre-trained on CheXpert as a frozen feature extractor on the CORDA dataset. We train a fully connected binary classifier for the final prediction, using the standard binary cross-entropy loss (BCE). More details about pre-training can be found in the Appendix.</p></sec><sec id="se0160"><label>3.2</label><title>Prioritization</title><p id="pr0500">The prioritization system is built upon a deep neural network based on the <italic>DenseNet-121</italic> architecture <xref rid="br0090" ref-type="bibr">[9]</xref>. This network leverages metric learning to assess the similarity between images from the CORDA dataset <xref rid="br0060" ref-type="bibr">[6]</xref>. The weights for the deep neural network were initialized using those obtained from training the same architecture on the <italic>ImageNet</italic> classification task. No fine-tuning was applied, ensuring that the performance of the CNN was evaluated using the pre-trained <italic>ImageNet</italic> weights. The structure of the system is depicted in <xref rid="fg0060" ref-type="fig">Fig. 6</xref> and is organized as follows:<fig id="fg0060"><label>Fig. 6</label><caption><p>The prioritization system architecture. The image (a) is pre-processed by algorithm (b) into the new image (c) which is elaborated by the deep neural network (d) to extract features (e). Computer features are compared by the distance metric (i) with the previously extracted features (g) stored in the database (f) with their labels (h). The features (g) have been calculated on past known cases. Using the similarity information elaborated by (i) and labels (h) a COVID-19 score (m) is computed by stage (l). The set of most similar cases from the past (n), used for the computation of score (m), is returned to support doctor's diagnosis and to provide an interpretation of the result.</p></caption><alt-text id="at0060">Fig. 6</alt-text><graphic xlink:href="gr007" id="ln0060"/></fig></p><p id="pr0510"><bold>Preprocessing</bold>: after resize, U-Net lung segmentation model <xref rid="br0180" ref-type="bibr">[18]</xref> crops down CXRs producing a final image that just includes the lungs, excluding parts of the image that can add details unrelated to Covid-19. Picture pixel's intensity is finally normalized by subtracting its average and dividing by its standard deviation.</p><p id="pr0520"><bold>Feature extraction</bold>: preprocessed images are projected into the target feature space using the deep convolutional neural network <italic>DenseNet-121</italic>. In this case, the original architecture has been truncated before the output classification layer, resulting in a convolutional feature extractor that yields a 1024-dimensional projection of the image provided as input. Consider a pre-processed picture <italic>x</italic> from the set of all possible pictures <italic>X</italic>, and a deep neural network model <inline-formula><mml:math id="M7" altimg="si7.svg"><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, we have:<disp-formula id="fm0010"><label>(1)</label><mml:math id="M8" altimg="si8.svg"><mml:mo>&#x02200;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">&#x02192;</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mi>z</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo>&#x02286;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1024</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula> where <italic>z</italic> is the feature vector corresponding to the image <italic>x</italic> and belongs to the target feature space <inline-formula><mml:math id="M9" altimg="si9.svg"><mml:mi mathvariant="script">F</mml:mi></mml:math></inline-formula>. With the notation <inline-formula><mml:math id="M10" altimg="si10.svg"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the <italic>i</italic>-th sample in <italic>X</italic> and <inline-formula><mml:math id="M11" altimg="si11.svg"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, consider <inline-formula><mml:math id="M12" altimg="si12.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo>&#x02286;</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></inline-formula>, the label associated with <inline-formula><mml:math id="M13" altimg="si10.svg"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the goal is to design <inline-formula><mml:math id="M14" altimg="si7.svg"><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> such that:<disp-formula id="fm0020"><label>(2)</label><mml:math id="M15" altimg="si13.svg"><mml:mo>&#x02200;</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="double-struck">N</mml:mi><mml:mo>:</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo linebreak="badbreak" linebreakstyle="after">&#x0003c;</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">&#x02192;</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo linebreak="badbreak" linebreakstyle="after">&#x0003c;</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">}</mml:mo></mml:math></disp-formula> where <inline-formula><mml:math id="M16" altimg="si14.svg"><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:math></inline-formula> indicate three samples in set <italic>X</italic> and <inline-formula><mml:math id="M17" altimg="si15.svg"><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:math></inline-formula> is the distance between points <italic>a</italic> and <italic>b</italic>. The meaning of Eq. <xref rid="fm0020" ref-type="disp-formula">(2)</xref> is that if the distance between <inline-formula><mml:math id="M18" altimg="si16.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M19" altimg="si17.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is less than the distance between <inline-formula><mml:math id="M20" altimg="si16.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="M21" altimg="si18.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, then the distance between the embedding <inline-formula><mml:math id="M22" altimg="si19.svg"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M23" altimg="si20.svg"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> should be less than the distance between <inline-formula><mml:math id="M24" altimg="si19.svg"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M25" altimg="si21.svg"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. This distance metric can be chosen conveniently to maximize the performance, using an iterative process, among the ones presented in the literature.</p><p id="pr0530"><bold>Distance-based regression</bold>: To compute the final prediction, the method exploits the distance in the target feature space between the query image and the reference pictures for which the label is known. Consider a set <inline-formula><mml:math id="M26" altimg="si22.svg"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> of <inline-formula><mml:math id="M27" altimg="si23.svg"><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> reference pictures from the CORDA dataset with relative label <inline-formula><mml:math id="M28" altimg="si24.svg"><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, such that given a <inline-formula><mml:math id="M29" altimg="si25.svg"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> its label is <inline-formula><mml:math id="M30" altimg="si26.svg"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="M31" altimg="si27.svg"><mml:mo>&#x02200;</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula>. The variable <inline-formula><mml:math id="M32" altimg="si26.svg"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> takes value 1 if picture <inline-formula><mml:math id="M33" altimg="si28.svg"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is related to Covid-19 positive patient, 0 otherwise. The methodology computes <italic>feature reference set</italic>
<inline-formula><mml:math id="M34" altimg="si29.svg"><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> as:<disp-formula id="fm0030"><label>(3)</label><mml:math id="M35" altimg="si30.svg"><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mo>&#x02200;</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">}</mml:mo></mml:math></disp-formula> Whenever it is requested to predict the prioritization score for a query picture <inline-formula><mml:math id="M36" altimg="si31.svg"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, its projection in the target feature space is computed as:<disp-formula id="fm0040"><label>(4)</label><mml:math id="M37" altimg="si32.svg"><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula> The distance between the resulting 1024-dimensional vector <inline-formula><mml:math id="M38" altimg="si33.svg"><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and the set of reference vectors <inline-formula><mml:math id="M39" altimg="si29.svg"><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is then computed. The elements in the feature reference set are sorted from the nearest to the most distant and related elements in <inline-formula><mml:math id="M40" altimg="si24.svg"><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> are stored in the sorted list <inline-formula><mml:math id="M41" altimg="si34.svg"><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. To compute the final prediction <inline-formula><mml:math id="M42" altimg="si35.svg"><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>, the proposed algorithm computes:<disp-formula id="fm0050"><label>(5)</label><mml:math id="M43" altimg="si36.svg"><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>&#x002c6;</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mspace width="1em"/><mml:mo>&#x02200;</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula> where <inline-formula><mml:math id="M44" altimg="si37.svg"><mml:mi>m</mml:mi><mml:mo>&#x02264;</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a parameter of the algorithm, regulating the number of nearest neighbors considered in the score estimation, and <inline-formula><mml:math id="M45" altimg="si38.svg"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a weight coefficient used to regulate the contribution of each sample. To tune the first, an iterative process has to be put in place: for increasing values of <italic>m</italic>, the performance of the method is evaluated, looking for the best trade-off. In our case, we tuned <italic>m</italic> on the CORDA dataset, using a cross-validation approach. The best results were achieved with <inline-formula><mml:math id="M46" altimg="si39.svg"><mml:mi>m</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>10</mml:mn></mml:math></inline-formula>. The definition of the weight coefficients can be performed empirically, depending on their distance in the target feature space or their position in the ordered list <inline-formula><mml:math id="M47" altimg="si34.svg"><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. The coefficients <inline-formula><mml:math id="M48" altimg="si38.svg"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can be used to incorporate distance into the final score computation. The idea is that closer samples should have a greater weight in the average computation, while less similar samples should weigh less. We experimented with different formulations and we found that a logarithmic decay of <inline-formula><mml:math id="M49" altimg="si38.svg"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> produced the most favorable outcome:<disp-formula id="fm0060"><label>(6)</label><mml:math id="M50" altimg="si40.svg"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo linebreak="badbreak" linebreakstyle="after">&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>&#x003b1;</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>&#x003b2;</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak" linebreakstyle="after">&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula> where <italic>N</italic> is the number of samples. Note that the reference set <inline-formula><mml:math id="M51" altimg="si41.svg"><mml:msup><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> contains both positive and negative samples: this is needed because, given a query image <inline-formula><mml:math id="M52" altimg="si31.svg"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> we aim to determine whether <inline-formula><mml:math id="M53" altimg="si31.svg"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is positive or negative, by computing the similarity with both classes in the reference set. The higher <inline-formula><mml:math id="M54" altimg="si42.svg"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">&#x002c6;</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> will be, the more priority we will assign to <inline-formula><mml:math id="M55" altimg="si31.svg"><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>.</p><p id="pr0540"><bold>Diagnostic workflow prioritization</bold>: The described process is applied to each CXR in the diagnostic queue, assigning a prioritization score for each case. The workflow is then reorganized by arranging the cases in descending order of priority, from highest to lowest.</p></sec></sec><sec id="se0170"><label>4</label><title>Conclusions</title><p id="pr0550">In this work, we have shared the outcomes of the Co.R.S.A. project, which focused on <italic>i)</italic> creating an open-access dataset for Covid-19 diagnosis using CXR images, along with a thorough assessment of the potential biases <italic>ii)</italic> developing a robust deep learning pipeline for automatic classification and prioritization, and <italic>iii)</italic> conducting clinical validation in a real-world setting with expert radiologists. This project encapsulates our extensive efforts in COVID-19 detection over the past few years. Although the most critical phase of the pandemic has passed, the groundwork laid by this initiative provides a strong foundation for rapidly responding to future epidemics, supported by the collaborations established with key hospitals and radiology units.</p><p id="pr0560">Finally, we addressed some potential limitations of the dataset used to train our model. Identifying and accounting for such limitations is crucial before any AI tool can be applied in a clinical setting, particularly given the challenges of acquiring a diverse and balanced collection of clinical images.</p></sec><sec id="se0180"><title>Compliance with ethical standards</title><p id="pr0620">This study was performed in line with the principles of the Declaration of Helsinki. Approval was granted by Comitato Etico Interaziendale A.O.U. San Luigi di Orbassano, AA.SS.LL. TO3, TO4, TO5 (10/24/2022/ No. 153/2022).</p></sec><sec id="se0190"><title>CRediT authorship contribution statement</title><p id="pr0600"><bold>Carlo Alberto Barbano:</bold> Writing &#x02013; review &#x00026; editing, Writing &#x02013; original draft, Software, Methodology, Data curation, Conceptualization. <bold>Luca Berton:</bold> Writing &#x02013; original draft, Visualization, Validation, Formal analysis. <bold>Riccardo Renzulli:</bold> Writing &#x02013; review &#x00026; editing, Writing &#x02013; original draft, Investigation. <bold>Davide Tricarico:</bold> Writing &#x02013; original draft, Visualization, Methodology. <bold>Osvaldo Rampado:</bold> Supervision. <bold>Domenico Basile:</bold> Data curation. <bold>Marco Busso:</bold> Supervision, Data curation. <bold>Marco Grosso:</bold> Validation, Supervision, Project administration, Data curation. <bold>Marco Grangetto:</bold> Writing &#x02013; review &#x00026; editing, Writing &#x02013; original draft, Validation, Supervision, Project administration, Methodology, Investigation, Funding acquisition, Conceptualization.</p></sec><sec sec-type="COI-statement"><title>Declaration of Competing Interest</title><p id="pr0630">The authors declare the following financial interests/personal relationships which may be considered as potential competing interests: Marco Grangetto reports financial support was provided by <funding-source id="gsp0010"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100009885</institution-id><institution>Piedmont Region</institution></institution-wrap></funding-source>. If there are other authors, they declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></sec></body><back><ref-list id="bl0010"><title>References</title><ref id="br0010"><label>1</label><element-citation publication-type="journal" id="bibF4C4C7BEBE2244FAD44561D7133FD969s1"><person-group person-group-type="author"><name><surname>Zu</surname><given-names>Zi Yue</given-names></name><name><surname>Jiang</surname><given-names>Meng Di</given-names></name><name><surname>Xu</surname><given-names>Peng Peng</given-names></name><name><surname>Chen</surname><given-names>Wen</given-names></name><name><surname>Ni</surname><given-names>Qian Qian</given-names></name><name><surname>Lu</surname><given-names>Guang Ming</given-names></name><etal/></person-group><article-title>Coronavirus disease 2019 (covid-19): a perspective from China</article-title><source>Radiology</source><volume>296</volume><issue>2</issue><year>2020</year><fpage>E15</fpage><lpage>E25</lpage><pub-id pub-id-type="pmid">32083985</pub-id>
</element-citation></ref><ref id="br0020"><label>2</label><element-citation publication-type="journal" id="bib443019842087931D8F1E423CF576AA36s1"><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Heshui</given-names></name><name><surname>Han</surname><given-names>Xiaoyu</given-names></name><name><surname>Jiang</surname><given-names>Nanchuan</given-names></name><name><surname>Cao</surname><given-names>Yukun</given-names></name><name><surname>Alwalid</surname><given-names>Osamah</given-names></name><name><surname>Gu</surname><given-names>Jin</given-names></name><etal/></person-group><article-title>Radiological findings from 81 patients with covid-19 pneumonia in Wuhan, China: a descriptive study</article-title><source>Lancet Infect Dis</source><volume>20</volume><issue>4</issue><year>2020</year><fpage>425</fpage><lpage>434</lpage><pub-id pub-id-type="pmid">32105637</pub-id>
</element-citation></ref><ref id="br0030"><label>3</label><element-citation publication-type="journal" id="bib32A73C0C8DA0CB01C7E781B13950F41Cs1"><person-group person-group-type="author"><name><surname>Lebovitz</surname><given-names>Sarah</given-names></name><name><surname>Levina</surname><given-names>Natalia</given-names></name><name><surname>Lifshitz-Assaf</surname><given-names>Hila</given-names></name></person-group><article-title>Is ai ground truth really true? The dangers of training and evaluating ai tools based on experts' know-what</article-title><source>MIS Q</source><volume>45</volume><issue>3</issue><year>2021</year></element-citation></ref><ref id="br0040"><label>4</label><element-citation publication-type="journal" id="bibB40A8F3B4166E9C33D6EB8575E9A2568s1"><person-group person-group-type="author"><name><surname>Roggema</surname><given-names>Rob</given-names></name><name><surname>Chamski</surname><given-names>Robert</given-names></name></person-group><article-title>The new urban profession: entering the age of uncertainty</article-title><source>Urban Sci</source><volume>6</volume><issue>1</issue><year>2022</year><fpage>10</fpage></element-citation></ref><ref id="br0050"><label>5</label><element-citation publication-type="journal" id="bib294BBCE1111FBFF421F93A40AC3C3A23s1"><person-group person-group-type="author"><name><surname>Gr&#x000f8;nsund</surname><given-names>Tor</given-names></name><name><surname>Aanestad</surname><given-names>Margunn</given-names></name></person-group><article-title>Augmenting the algorithm: emerging human-in-the-loop work configurations</article-title><source>J Strateg Inf Syst</source><volume>29</volume><issue>2</issue><year>2020</year><object-id pub-id-type="publisher-id">101614</object-id></element-citation></ref><ref id="br0060"><label>6</label><element-citation publication-type="other" id="bibF052AA04236B66C3A64048A8A387456As1"><person-group person-group-type="author"><name><surname>Alesina</surname><given-names>Marta</given-names></name><name><surname>Barbano</surname><given-names>Carlo Alberto</given-names></name><name><surname>Berzovini</surname><given-names>Claudio</given-names></name><name><surname>Busso</surname><given-names>Marco</given-names></name><name><surname>Calandri</surname><given-names>Marco</given-names></name><name><surname>De Pascale</surname><given-names>Agostino</given-names></name><etal/></person-group><article-title>CORDA dataset</article-title><ext-link ext-link-type="doi" xlink:href="10.5281/zenodo.7821611" id="inf0050">https://doi.org/10.5281/zenodo.7821611</ext-link><year>Jan. 2023</year></element-citation></ref><ref id="br0070"><label>7</label><element-citation publication-type="book" id="bib8E7245ED66CAD8454D93A2EF296B02B8s1"><person-group person-group-type="author"><name><surname>Barbano</surname><given-names>Carlo Alberto</given-names></name><name><surname>Renzulli</surname><given-names>Riccardo</given-names></name><name><surname>Grosso</surname><given-names>Marco</given-names></name><name><surname>Basile</surname><given-names>Domenico</given-names></name><name><surname>Busso</surname><given-names>Marco</given-names></name><name><surname>Grangetto</surname><given-names>Marco</given-names></name></person-group><part-title>AI-assisted diagnosis for Covid-19 CXR screening: from data collection to clinical validation</part-title><source>2024 IEEE international symposium on biomedical imaging (ISBI)</source><year>2024</year><fpage>1</fpage><lpage>4</lpage></element-citation></ref><ref id="br0080"><label>8</label><element-citation publication-type="book" id="bibB1EF707BA3568F23C67A8E6A26C349BCs1"><person-group person-group-type="author"><name><surname>Barbano</surname><given-names>Carlo Alberto</given-names></name><name><surname>Tartaglione</surname><given-names>Enzo</given-names></name><name><surname>Berzovini</surname><given-names>Claudio</given-names></name><name><surname>Calandri</surname><given-names>Marco</given-names></name><name><surname>Grangetto</surname><given-names>Marco</given-names></name></person-group><part-title>A two-step radiologist-like approach for covid-19 computer-aided diagnosis from chest x-ray images</part-title><source>Image analysis and processing &#x02013; ICIAP 2022</source><year>2022</year><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc><fpage>173</fpage><lpage>184</lpage></element-citation></ref><ref id="br0090"><label>9</label><element-citation publication-type="book" id="bib2A9791DE66930F65D1ACB6338E4320EEs1"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>G.</given-names></name><name><surname>Liu</surname><given-names>Z.</given-names></name><name><surname>Van Der Maaten</surname><given-names>L.</given-names></name><name><surname>Weinberger</surname><given-names>K.Q.</given-names></name></person-group><part-title>Densely connected convolutional networks</part-title><source>2017 IEEE conference on computer vision and pattern recognition (CVPR)</source><year>jul 2017</year><publisher-name>IEEE Computer Society</publisher-name><publisher-loc>Los Alamitos, CA, USA</publisher-loc><fpage>2261</fpage><lpage>2269</lpage></element-citation></ref><ref id="br0100"><label>10</label><element-citation publication-type="journal" id="bibF3017587418D59D82D2FD87DD06C5D53s1"><person-group person-group-type="author"><name><surname>Irvin</surname><given-names>Jeremy</given-names></name><name><surname>Rajpurkar</surname><given-names>Pranav</given-names></name><name><surname>Ko</surname><given-names>Michael</given-names></name><name><surname>Yu</surname><given-names>Yifan</given-names></name><name><surname>Ciurea-Ilcus</surname><given-names>Silviana</given-names></name><name><surname>Chute</surname><given-names>Chris</given-names></name><etal/></person-group><article-title>Chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison</article-title><source>Proc AAAI Conf Artif Intell</source><volume>33</volume><issue>01</issue><year>Jul. 2019</year><fpage>590</fpage><lpage>597</lpage></element-citation></ref><ref id="br0110"><label>11</label><element-citation publication-type="journal" id="bib12360C51CAD7A62AE5518A869C55830Es1"><person-group person-group-type="author"><name><surname>Borghesi</surname><given-names>Andrea</given-names></name><name><surname>Maroldi</surname><given-names>Roberto</given-names></name></person-group><article-title>Covid-19 outbreak in Italy: experimental chest x-ray scoring system for quantifying and monitoring disease progression</article-title><source>Radiol Med</source><volume>125</volume><issue>5</issue><year>2020</year><fpage>509</fpage><lpage>513</lpage><pub-id pub-id-type="pmid">32358689</pub-id>
</element-citation></ref><ref id="br0120"><label>12</label><element-citation publication-type="journal" id="bib25B1773A49029656167FB2FC992F71BBs1"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>Henry B.</given-names></name><name><surname>Whitney</surname><given-names>Donald R.</given-names></name></person-group><article-title>On a test of whether one of two random variables is stochastically larger than the other</article-title><source>Ann Math Stat</source><volume>18</volume><issue>1</issue><year>1947</year><fpage>50</fpage><lpage>60</lpage></element-citation></ref><ref id="br0130"><label>13</label><element-citation publication-type="journal" id="bibF274BC717E3399E3E4C2DAAF1D951E6As1"><person-group person-group-type="author"><name><surname>Wilcoxon</surname><given-names>Frank</given-names></name></person-group><article-title>Individual comparisons by ranking methods</article-title><source>Biom Bull</source><volume>1</volume><issue>6</issue><year>1945</year><fpage>80</fpage><lpage>83</lpage></element-citation></ref><ref id="br0140"><label>14</label><element-citation publication-type="journal" id="bibC473824245458A9D7149CB1158E6D25Cs1"><person-group person-group-type="author"><name><surname>Robin</surname><given-names>Xavier</given-names></name><name><surname>Turck</surname><given-names>Natacha</given-names></name><name><surname>Hainard</surname><given-names>Alexandre</given-names></name><etal/></person-group><article-title>Proc: an open-source package for r and s+ to analyze and compare roc curves</article-title><source>BMC Bioinform</source><year>2011</year></element-citation></ref><ref id="br0150"><label>15</label><element-citation publication-type="journal" id="bibDB3898DDD037A64AD0658DC364E296BCs1"><person-group person-group-type="author"><name><surname>DeLong</surname><given-names>Elizabeth R.</given-names></name><name><surname>DeLong</surname><given-names>David M.</given-names></name><name><surname>Clarke-Pearson</surname><given-names>Daniel L.</given-names></name></person-group><article-title>Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach</article-title><source>Biometrics</source><volume>44</volume><issue>3</issue><year>1988</year><fpage>837</fpage><lpage>845</lpage><pub-id pub-id-type="pmid">3203132</pub-id>
</element-citation></ref><ref id="br0160"><label>16</label><element-citation publication-type="journal" id="bib0BDB342D31864EA186FBBB42BAE4CA35s1"><person-group person-group-type="author"><name><surname>Tricarico</surname><given-names>Davide</given-names></name><name><surname>Calandri</surname><given-names>Marco</given-names></name><name><surname>Barba</surname><given-names>Matteo</given-names></name><name><surname>Piatti</surname><given-names>Clara</given-names></name><name><surname>Geninatti</surname><given-names>Carlotta</given-names></name><name><surname>Basile</surname><given-names>Domenico</given-names></name><etal/></person-group><article-title>Convolutional neural network-based automatic analysis of chest radiographs for the detection of covid-19 pneumonia: a prioritizing tool in the emergency department, phase i study and preliminary &#x0201c;real life&#x0201d; results</article-title><source>Diagnostics</source><volume>12</volume><issue>3</issue><year>2022</year></element-citation></ref><ref id="br0170"><label>17</label><element-citation publication-type="other" id="bibC9F1FEC4D9D62522C5A9161892A41BCEs1"><person-group person-group-type="author"><name><surname>Glocker</surname><given-names>Ben</given-names></name><name><surname>Robinson</surname><given-names>Robert</given-names></name><name><surname>Castro</surname><given-names>Daniel C.</given-names></name><name><surname>Dou</surname><given-names>Qi</given-names></name><name><surname>Konukoglu</surname><given-names>Ender</given-names></name></person-group><article-title>Machine learning with multi-site imaging data: an empirical study on the impact of scanner effects</article-title><comment>arXiv preprint</comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1910.04597" id="inf0060">arXiv:1910.04597</ext-link><year>2019</year></element-citation></ref><ref id="br0180"><label>18</label><element-citation publication-type="book" id="bib07C553A021EEFC876017ABB5C3682151s1"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>Olaf</given-names></name><name><surname>Fischer</surname><given-names>Philipp</given-names></name><name><surname>Brox</surname><given-names>Thomas</given-names></name></person-group><part-title>U-net: convolutional networks for biomedical image segmentation</part-title><source>Medical image computing and computer-assisted intervention&#x02013;MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</source><year>2015</year><publisher-name>Springer</publisher-name><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="br0190"><label>19</label><element-citation publication-type="book" id="bib4FF285C196B0C035A84A3C2D448C07ABs1"><person-group person-group-type="author"><name><surname>Barbano</surname><given-names>Carlo Alberto</given-names></name><name><surname>Dufumier</surname><given-names>Benoit</given-names></name><name><surname>Tartaglione</surname><given-names>Enzo</given-names></name><name><surname>Grangetto</surname><given-names>Marco</given-names></name><name><surname>Gori</surname><given-names>Pietro</given-names></name></person-group><part-title>Unbiased supervised contrastive learning</part-title><source>The eleventh international conference on learning representations</source><year>2023</year></element-citation></ref></ref-list><sec id="se0200" sec-type="supplementary-material"><label>Appendix A</label><title>Supplementary material</title><p id="pr0610">The following is the Supplementary material related to this article.<supplementary-material content-type="local-data" id="ec0010"><caption><title>MMC</title><p>Supplementary material for &#x0201c;Detection and Prioritization of COVID-19 Infected Patients from CXR Images: Analysis of AI-assisted diagnosis in clinical settings.&#x0201d; Here we report additional details about the methods and training protocol presented in the main text.</p></caption><media xlink:href="mmc1.pdf"><alt-text>MMC</alt-text></media></supplementary-material></p></sec><ack id="ac0010"><title>Acknowledgements</title><p id="pr0640">The project was funded through the &#x0201c;<award-id award-type="grant" rid="gsp0010">INFRA-P2</award-id> - Potenziamento di laboratori di prova ed infrastrutture di ricerca gi&#x000e0; esistenti nella disponibilit&#x000e0; di organismi di ricerca pubblici e progetti di ricerca e sviluppo finalizzati al contrasto della pandemia Covid-19&#x0201d; call for proposals. A special acknowledgment goes to the other partners of the Co.R.S.A. project: Citt&#x000e0; della Scienza e della Salute and REGOLA s.r.l. We wish to thank the valuable support of all the radiology units that contributed to the CORDA dataset.</p></ack><fn-group><fn id="se0210" fn-type="supplementary-material"><label>Appendix A</label><p id="pr0650">Supplementary material related to this article can be found online at <ext-link ext-link-type="doi" xlink:href="10.1016/j.csbj.2024.11.045" id="inf0040">https://doi.org/10.1016/j.csbj.2024.11.045</ext-link>.</p></fn></fn-group><fn-group><fn id="fn0010"><label>1</label><p id="np0010"><ext-link ext-link-type="uri" xlink:href="https://covid19.who.int/" id="inf0020">https://covid19.who.int/</ext-link>.</p></fn></fn-group></back></article>