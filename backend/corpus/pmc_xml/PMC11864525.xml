<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40009630</article-id><article-id pub-id-type="pmc">PMC11864525</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0318992</article-id><article-id pub-id-type="publisher-id">PONE-D-24-26390</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Diagnostic Medicine</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Magnetic Resonance Imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Magnetic Resonance Imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Radiology and Imaging</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Magnetic Resonance Imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Plant Science</subject><subj-group><subject>Plant Anatomy</subject><subj-group><subject>Plant Roots</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Cardiology</subject><subj-group><subject>Cardiovascular Medicine</subject><subj-group><subject>Cardiovascular Imaging</subject><subj-group><subject>Angiography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Diagnostic Medicine</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Cardiovascular Imaging</subject><subj-group><subject>Angiography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Cardiovascular Imaging</subject><subj-group><subject>Angiography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Radiology and Imaging</subject><subj-group><subject>Diagnostic Radiology</subject><subj-group><subject>Cardiovascular Imaging</subject><subj-group><subject>Angiography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Network Analysis</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Signal to Noise Ratio</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Plant Science</subject><subj-group><subject>Plant Anatomy</subject><subj-group><subject>Plant Roots</subject><subj-group><subject>Fine Roots</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Physiological Parameters</subject><subj-group><subject>Body Weight</subject><subj-group><subject>Weight Loss</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Untrained perceptual loss for image denoising of line-like structures in MR
images</article-title><alt-title alt-title-type="running-head">Untrained perceptual loss for MR denoising</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6160-3011</contrib-id><name><surname>Pfaehler</surname><given-names>Elisabeth</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Pflugfelder</surname><given-names>Daniel</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Scharr</surname><given-names>Hanno</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Institute for Advanced Simulation: Data Analytics and Machine Learning (IAS-8), Forschungszentrum J&#x000fc;lich GmbH, Germany, J&#x000fc;lich, North-Rhine-Westfalia, Germany</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Institute of Bio- and Geosciences: Plant Sciences (IBG-2), Forschungszentrum J&#x000fc;lich GmbH, Germany, J&#x000fc;lich, North-Rhine-Westfalia, Germany</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Khan</surname><given-names>Khan Bahadar</given-names></name><aff id="edit1"><addr-line>Islamia University of Bahawalpur: The Islamia University of BahawalpurPakistan, PAKISTAN</addr-line>
</aff><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>e.pfaehler@fz-juelich.de</email></corresp></author-notes><pub-date pub-type="epub"><day>26</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>2</issue><elocation-id>e0318992</elocation-id><history><date date-type="received"><day>19</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>26</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Pfaehler et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Pfaehler et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0318992.pdf">
</self-uri><abstract><p>In the acquisition of Magnetic Resonance (MR) images shorter scan times lead to higher image noise. Therefore, automatic image denoising using deep learning methods is of high interest. In this work, we concentrate on image denoising of MR images containing line-like structures such as roots or vessels. In particular, we investigate if the special characteristics of these datasets (connectivity, sparsity) benefit from the use of special loss functions for network training. We hereby translate the Perceptual Loss to 3D data by comparing feature maps of untrained networks in the loss function. We tested the performance of untrained Perceptual Loss (uPL) on 3D image denoising of MR images displaying brain vessels (MR angiograms - MRA) and images of plant roots in soil. In this study, 536 MR images of plant roots in soil and 450 MRA images are included. The plant root dataset is split to 380, 80, and 76 images for training, validation, and testing. The MRA dataset is split to 300, 50, and 100 images for training, validation, and testing. We investigate the impact of various uPL characteristics such as weight initialization, network depth, kernel size, and pooling operations on the results. We tested the performance of the uPL loss on four Rician noise levels (1%, 5%, 10%, and 20%) using evaluation metrics such as the Structural Similarity Index Metric (SSIM). Our results are compared with the frequently used L1 loss for different network architectures. We observe, that our uPL outperforms conventional loss functions such as the L1 loss or a loss based on the Structural Similarity Index Metric (SSIM). For MRA images the uPL leads to SSIM values of 0.93 while L1 and SSIM loss led to SSIM values of 0.81 and 0.88, respectively. The uPL network&#x02019;s initialization is not important (e.g. for MR root images SSIM differences of 0.01 occur across initializations, while network depth and pooling operations impact denoising performance slightly more (SSIM of 0.83 for 5 convolutional layers and kernel size 3 vs. 0.86 for 5 convolutional layers and kernel size 5 for the root dataset). We also find that small uPL networks led to better or comparable results than using large networks such as VGG (e.g. SSIM values of 0.93 and 0.90 for a small and a VGG19 uPL network in the MRA dataset). In summary, we demonstrate superior performance of our loss for both datasets, all noise levels, and three network architectures. In conclusion, for images containing line-like structures, uPL is an alternative to other loss functions for 3D image denoising. We observe that small uPL networks have better or equal performance than very large network architectures while requiring lower computational costs and should therefore be preferred.</p></abstract><funding-group><award-group id="award001"><funding-source>
<institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001656</institution-id><institution>Helmholtz-Gemeinschaft</institution></institution-wrap>
</funding-source><award-id>HighLine ZT-I-PF-4-042)</award-id><principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6160-3011</contrib-id>
<name><surname>Pfaehler</surname><given-names>Elisabeth</given-names></name>
</principal-award-recipient></award-group><funding-statement>This work was supported by the Networking Funds of the Helmholtz Association of German Research Centres 276 (HighLine ZT-I-PF-4-042).</funding-statement></funding-group><counts><fig-count count="5"/><table-count count="5"/><page-count count="18"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>Not all roots data can be shared publicly because some of the images were acquired within industrial projects that do not allow for free data sharing. However, around 60% of the roots images will be shared what will be enough to reproduce the study findings. We will share this data in the following weeks. We will start now with preparing the images and select the ones that can be shared. We will upload the data on J&#x000fc;lich data: <ext-link xlink:href="https://data.fz-juelich.de/" ext-link-type="uri">https://data.fz-juelich.de/</ext-link>. All implemented python code is now being prepared and will be made publicly in the next two/three weeks on github.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>Not all roots data can be shared publicly because some of the images were acquired within industrial projects that do not allow for free data sharing. However, around 60% of the roots images will be shared what will be enough to reproduce the study findings. We will share this data in the following weeks. We will start now with preparing the images and select the ones that can be shared. We will upload the data on J&#x000fc;lich data: <ext-link xlink:href="https://data.fz-juelich.de/" ext-link-type="uri">https://data.fz-juelich.de/</ext-link>. All implemented python code is now being prepared and will be made publicly in the next two/three weeks on github.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Enhancing image quality of 3-dimensional (3D) images by suppressing image noise is important in bio-medical applications. Conventional methods such as spatial filtering or other, edge-preserving techniques (e.g.&#x000a0;bilateral filtering [<xref rid="pone.0318992.ref001" ref-type="bibr">1</xref>,&#x000a0;<xref rid="pone.0318992.ref002" ref-type="bibr">2</xref>]) are often used for image denoising [<xref rid="pone.0318992.ref003" ref-type="bibr">3</xref>,&#x000a0;<xref rid="pone.0318992.ref004" ref-type="bibr">4</xref>]. However, these conventional filtering methods tend to lead to blurry results. In recent years, convolutional neural networks (CNNs) overcame the limitations of conventional denoising methods and showed strongly improved results for both 2D and 3D images [<xref rid="pone.0318992.ref005" ref-type="bibr">5</xref>,&#x000a0;<xref rid="pone.0318992.ref006" ref-type="bibr">6</xref>]. For image denoising using deep learning, several network architectures have been proposed [<xref rid="pone.0318992.ref005" ref-type="bibr">5</xref>,&#x000a0;<xref rid="pone.0318992.ref007" ref-type="bibr">7</xref>,&#x000a0;<xref rid="pone.0318992.ref008" ref-type="bibr">8</xref>]. The majority of works reporting on image denoising concentrate on the optimal network architecture. However, not only network structure but also selecting an appropriate loss function is important for network performance. [<xref rid="pone.0318992.ref009" ref-type="bibr">9</xref>,&#x000a0;<xref rid="pone.0318992.ref010" ref-type="bibr">10</xref>] As the loss function is a crucial part of neural network training, the focus of this work lies in finding the optimal loss function for 3D image denoising of images containing line-like structures.</p><p>For Magnetic Resonance (MR) images, image quality is related to scan time, i.e.&#x000a0;shorter scan times result in images with higher image noise [<xref rid="pone.0318992.ref011" ref-type="bibr">11</xref>]. While causing higher noise levels, shorter scan times lead to better patient comfort and higher daily patient throughput. Image denoising can transform these shorter scans to images with similar image quality as scans acquired during a longer time. Please note that in contrast to other works, this paper focus on image-image denoising and is not considering image reconstruction [<xref rid="pone.0318992.ref012" ref-type="bibr">12</xref>,&#x000a0;<xref rid="pone.0318992.ref013" ref-type="bibr">13</xref>].</p><p>For 3D MR image enhancement, two approaches are frequently used: (1) In the majority of the works, the images are denoised slice by slice, i.e.&#x000a0;using a 2D network [<xref rid="pone.0318992.ref014" ref-type="bibr">14</xref>&#x02013;<xref rid="pone.0318992.ref017" ref-type="bibr">17</xref>]. (2) Other works denoise the whole 3D volume (3D networks) [<xref rid="pone.0318992.ref018" ref-type="bibr">18</xref>&#x02013;<xref rid="pone.0318992.ref020" ref-type="bibr">20</xref>]. In [<xref rid="pone.0318992.ref021" ref-type="bibr">21</xref>,&#x000a0;<xref rid="pone.0318992.ref022" ref-type="bibr">22</xref>] the authors compare the 2D slice-by-slice with the pure 3D approach using various denoising networks. They demonstrate that using the 3D volume and 3D networks leads to consistently better results than the 2D approach as 2D denoising methods ignore relevant 3D information. Therefore, in this work, we concentrate on 3D networks.</p><p>For 2D images, the &#x02018;Perceptual Loss&#x02019; [<xref rid="pone.0318992.ref023" ref-type="bibr">23</xref>&#x02013;<xref rid="pone.0318992.ref029" ref-type="bibr">29</xref>] (also called &#x02018;content representation&#x02019; [<xref rid="pone.0318992.ref030" ref-type="bibr">30</xref>]) is frequently used as loss function for image enhancement and outperformed traditional pixel-by-pixel losses. The Perceptual Loss is also often used in combination with L1- or L2-loss where it also leads to performance improvements [<xref rid="pone.0318992.ref031" ref-type="bibr">31</xref>]. The standard Perceptual Loss (sPL) minimizes differences between features extracted from reconstructed and ground truth image using a pre-trained neural network. The success of the sPL has been thought to lie in the fact that the loss network extracts meaningful features, as it has been trained on a large variety of images [<xref rid="pone.0318992.ref024" ref-type="bibr">24</xref>,&#x000a0;<xref rid="pone.0318992.ref028" ref-type="bibr">28</xref>]. Such pretrained networks are unavailable in 3D hampering the application of sPL for 3D MRI enhancement. However, in recent works an untrained Perceptual Loss (uPL) was used in 2D image super-resolution and compared features from untrained, randomly initialized networks [<xref rid="pone.0318992.ref032" ref-type="bibr">32</xref>,&#x000a0;<xref rid="pone.0318992.ref033" ref-type="bibr">33</xref>]. The randomly initialized loss networks were exclusively used to extract feature maps from high-quality images and remain constant during the super-resolution training process and performed similarly to sPL [<xref rid="pone.0318992.ref033" ref-type="bibr">33</xref>]. This opens a door towards using Perceptual Losses also in 3D.</p><p>For 3D image enhancement, the most frequently used loss functions are L1 or L2 loss which compare generated and ground truth images voxel-by-voxel and thereby ignoring voxel neighborhoods [<xref rid="pone.0318992.ref021" ref-type="bibr">21</xref>,&#x000a0;<xref rid="pone.0318992.ref034" ref-type="bibr">34</xref>]. As displayed in <xref rid="pone.0318992.t001" ref-type="table">Table 1</xref>, most recent studies adressing image denoising of MR brain or MRA images focus on the most adequate network architecture while using conventional loss functions such as L1 and/or SSIM-loss. Therefore, the aim of this work is to investigate if the uPL can be used beneficially for 3D images containing line-like structures. The research objective is to assess the impact of uPL network parameters on the results. We are especially interested if also small, simple networks can be used in the uPL.</p><table-wrap position="float" id="pone.0318992.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0313772.t001</object-id><label>Table 1</label><caption><title>Recent networks used for MR image denoising and MRA data.</title><p>As displayed, all listed works use either conventional loss-functions such as L1 and/or SSIM loss or a 2D version of the Perceptual Loss.</p></caption><alternatives><graphic xlink:href="pone.0318992.t001" id="pone.0318992.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Authors</th><th align="left" rowspan="1" colspan="1">Dataset type</th><th align="left" rowspan="1" colspan="1">Purpose</th><th align="left" rowspan="1" colspan="1">Network architecture</th><th align="left" rowspan="1" colspan="1">Loss function</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Tian et al.</td><td align="left" rowspan="1" colspan="1">MR Brain (T1)</td><td align="left" rowspan="1" colspan="1">Denoising</td><td align="left" rowspan="1" colspan="1">Modified U-Net</td><td align="left" rowspan="1" colspan="1">L1</td></tr><tr><td align="left" rowspan="1" colspan="1">Haetsam et al.</td><td align="left" rowspan="1" colspan="1">MR Brain (T1, T2)</td><td align="left" rowspan="1" colspan="1">Denoising</td><td align="left" rowspan="1" colspan="1">Modified U-Net</td><td align="left" rowspan="1" colspan="1">L1</td></tr><tr><td align="left" rowspan="1" colspan="1">Ran et al.</td><td align="left" rowspan="1" colspan="1">MR Brain</td><td align="left" rowspan="1" colspan="1">Denoising</td><td align="left" rowspan="1" colspan="1">2D Generative Adversarial Network</td><td align="left" rowspan="1" colspan="1">2D Perc. Loss</td></tr><tr><td align="left" rowspan="1" colspan="1">Wu et al.</td><td align="left" rowspan="1" colspan="1">MR Brain</td><td align="left" rowspan="1" colspan="1">Denoising</td><td align="left" rowspan="1" colspan="1">Deep Network (no downsampling)</td><td align="left" rowspan="1" colspan="1">L2</td></tr><tr><td align="left" rowspan="1" colspan="1">Li [<xref rid="pone.0318992.ref035" ref-type="bibr">35</xref>] et al.</td><td align="left" rowspan="1" colspan="1">MR Brain</td><td align="left" rowspan="1" colspan="1">Denoising</td><td align="left" rowspan="1" colspan="1">ParallelNet (no downsampling)</td><td align="left" rowspan="1" colspan="1">L1</td></tr><tr><td align="left" rowspan="1" colspan="1">Gurrola et al. [<xref rid="pone.0318992.ref036" ref-type="bibr">36</xref>]</td><td align="left" rowspan="1" colspan="1">MR Brain</td><td align="left" rowspan="1" colspan="1">Denoising</td><td align="left" rowspan="1" colspan="1">Modified Autoencoder</td><td align="left" rowspan="1" colspan="1">L1</td></tr><tr><td align="left" rowspan="1" colspan="1">Lee et al. [<xref rid="pone.0318992.ref037" ref-type="bibr">37</xref>]</td><td align="left" rowspan="1" colspan="1">MR</td><td align="left" rowspan="1" colspan="1">Denoising</td><td align="left" rowspan="1" colspan="1">Diffusion Probabilistic</td><td align="left" rowspan="1" colspan="1">L1</td></tr><tr><td align="left" rowspan="1" colspan="1">[<xref rid="pone.0318992.ref037" ref-type="bibr">37</xref>]</td><td align="left" rowspan="1" colspan="1">Animal images</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">Model</td><td align="left" rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">Fiasam et al. [<xref rid="pone.0318992.ref038" ref-type="bibr">38</xref>]</td><td align="left" rowspan="1" colspan="1">MR Brain</td><td align="left" rowspan="1" colspan="1">Denoising</td><td align="left" rowspan="1" colspan="1">Encoder/Decoder with attention modules</td><td align="left" rowspan="1" colspan="1">L1</td></tr><tr><td align="left" rowspan="1" colspan="1">You et al. [<xref rid="pone.0318992.ref039" ref-type="bibr">39</xref>]</td><td align="left" rowspan="1" colspan="1">MR Brain</td><td align="left" rowspan="1" colspan="1">Denoising</td><td align="left" rowspan="1" colspan="1">CNN</td><td align="left" rowspan="1" colspan="1">L1</td></tr><tr><td align="left" rowspan="1" colspan="1">Kuestner et al. [<xref rid="pone.0318992.ref040" ref-type="bibr">40</xref>]</td><td align="left" rowspan="1" colspan="1">MRA</td><td align="left" rowspan="1" colspan="1">Super-Resolution</td><td align="left" rowspan="1" colspan="1">GAN</td><td align="left" rowspan="1" colspan="1">L1+SSIM</td></tr><tr><td align="left" rowspan="1" colspan="1">Wicaksono et al. [<xref rid="pone.0318992.ref041" ref-type="bibr">41</xref>]</td><td align="left" rowspan="1" colspan="1">MRA</td><td align="left" rowspan="1" colspan="1">Super-resolution</td><td align="left" rowspan="1" colspan="1">GAN</td><td align="left" rowspan="1" colspan="1">L1+SSIM</td></tr></tbody></table></alternatives></table-wrap><p><bold>Our contribution:</bold> In summary, our contributions are as follows:</p><list list-type="bullet"><list-item><p>We propose 3D uPL with small 3D convolutional networks in the loss function for 3D denoising - a straightforward translation from the 2D case.</p></list-item><list-item><p>We demonstrate the suitability of uPL for 3D denoising of MR images containing line-like structures outperforming competing losses by a large margin.</p></list-item><list-item><p>We investigate and discuss the influence of network hyper-parameters like network depth, layer thickness, kernel size, and pooling operations.</p></list-item><list-item><p>We investigate the suitability of the uPL for three denoising network architectures and demonstrate its superiority for all the three architectures, where we also provide a novel 3D implementation of the Restormer [<xref rid="pone.0318992.ref005" ref-type="bibr">5</xref>] network.</p></list-item></list></sec><sec sec-type="materials|methods" id="sec002"><title>Materials and methods</title><sec id="sec003"><title>Datasets</title><p>We included two datasets in this study. The first dataset consists of 536 MR images of plant roots in soil with voxel size 0.5 mm &#x000d7; 0.5 mm &#x000d7; 0.99 mm and a typical image size of <inline-formula id="pone.0318992.e003"><alternatives><graphic xlink:href="pone.0318992.e003.jpg" id="pone.0318992.e003g" position="anchor"/><mml:math id="m3" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>9</mml:mn></mml:mrow><mml:mn>2</mml:mn><mml:mspace class="thinspace" width="0.17em"/><mml:mo stretchy="false">&#x000d7;</mml:mo><mml:mspace class="thinspace" width="0.17em"/><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>9</mml:mn></mml:mrow><mml:mn>2</mml:mn><mml:mspace class="thinspace" width="0.17em"/><mml:mo stretchy="false">&#x000d7;</mml:mo><mml:mspace class="thinspace" width="0.17em"/><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>0</mml:mn><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>. For the root dataset, 380 images were used for training, 80 images for validation, and 76 for testing.</p><p>The MRA data consists of MR images of brain vessels. This dataset is part of the publicly available IXI datasets <xref rid="n1" ref-type="fn"><sup>1</sup></xref>. We had no access to any information that could identify the scanned individuals. The typical MRA image size of this dataset is <inline-formula id="pone.0318992.e004"><alternatives><graphic xlink:href="pone.0318992.e004.jpg" id="pone.0318992.e004g" position="anchor"/><mml:math id="m4" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">&#x000d7;</mml:mo><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">&#x000d7;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mstyle class="text"><mml:mtext>dim</mml:mtext></mml:mstyle></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. The <inline-formula id="pone.0318992.e005"><alternatives><graphic xlink:href="pone.0318992.e005.jpg" id="pone.0318992.e005g" position="anchor"/><mml:math id="m5" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mstyle class="text"><mml:mtext>dim</mml:mtext></mml:mstyle></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> varies from patient to patient. The training set was randomly split into 300 training, 50 validation, and 100 test images.</p><p>An example image for each dataset is displayed in <xref rid="pone.0318992.g001" ref-type="fig">Fig 1</xref>. For both datasets, four levels of Rician noise were artificially added to assess the impact of the loss functions for different signal-to-noise ratios (1%, 5%, 10%, and 20% noise added). An example root and MRA image with different noise levels is displayed in <xref rid="pone.0318992.g002" ref-type="fig">Fig 2</xref>. Experiments regarding loss network structure were performed for noise level 3, i.e.&#x000a0;10%.</p><fig position="float" id="pone.0318992.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0318992.g001</object-id><label>Fig 1</label><caption><title>Example 3D MR images of plant roots and brain vessels.</title><p>First column: Plant scanned in the MR system, second column: maximum intensity projections (MIP) of MR root image (top: axial plane, mid.: sagittal plane) and a single slice of the 3D image marked yellow in the sagittal plane (bot.). Next columns, top to bot.: Axial, coronal, and sagittal MIP of an MRA image.</p></caption><graphic xlink:href="pone.0318992.g001" position="float"/></fig><fig position="float" id="pone.0318992.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0318992.g002</object-id><label>Fig 2</label><caption><title>Maximum Intensity Projections of original and noisy images.</title><p>From left to right: Original image, cropped part of original image, cropped part of image with 1%, 5%, 10%, and 20% added noise. </p></caption><graphic xlink:href="pone.0318992.g002" position="float"/></fig></sec><sec id="sec004"><title>Denoising networks</title><p>Three network architectures are compared in this study: a DnCNN [<xref rid="pone.0318992.ref042" ref-type="bibr">42</xref>], a ResNet architecure [<xref rid="pone.0318992.ref028" ref-type="bibr">28</xref>] and a Transformer network as proposed for 2D by Zamir et al.&#x000a0;[<xref rid="pone.0318992.ref005" ref-type="bibr">5</xref>] translated to 3D. Please note that we also tested U-Net like architectures. However, especially for our very fine root data, these architectures generally led to much worse results than the shown methods, for pixel-wise losses as well as the better but still bad performing uPL. Therefore, we concentrate in this work on the three mentioned architectures.</p><p>The DnCNN consists of one convolutional layer with 64 feature maps and kernel size 3 followed by a ReLU activation function. Next, are three blocks consisting of a convolutional layer with 64 feature maps and kernel size 3, a ReLU activation, and a batch normalization layer. The last layer is a convolutional layer with kernel size 3 and 1 output channel.</p><p>The ResNet consists of one convolutional layer followed by a PreLU activation function. The first convolutional block is followed by five residual blocks, where each residual block contains one convolutional layer, a batch normalization layer, and a PreLU activation function. The residual blocks are connected via a skip connection. The residual blocks are followed by a convolutional layer, a batch normalization, and another PreLu activation.</p><p>The Transformer network [<xref rid="pone.0318992.ref005" ref-type="bibr">5</xref>] is specially designed such that it can be used for large images while modeling global connectivity. In this network, multi-head &#x02018;transposed&#x02019; attention (MDTA) blocks are introduced applying attention across feature dimensions rather than across spatial dimensions. The used Transformer networks starts with a convolutional layer with kernel size 3 and 8 output feature maps. The convolutional layer is followed by a MDTA block. Before feeding the data to the MDTA blocks, the images are resized to the size (image height &#x022c5; image width &#x022c5; image depth) &#x000d7; number of channels. After the MDTA blocks follow feed-forward blocks consisting of two convolutional layers (with kernel size 3 and 16, and 32 output feature maps) and a gating layer. As for our fine structures, UNet-like architectures lead to performance drops, the Transformer blocks were in our study combined sequentially, in contrast to the original 2D Restormer [<xref rid="pone.0318992.ref005" ref-type="bibr">5</xref>]. A graphical overview of all network structures is displayed in the (S1, S2, and S3 Figs).</p><p>The performance of the proposed uPL is compared with the L1 loss across the mentioned network architectures and different noise levels. For the evaluation of characteristics of the uPL network, the DnCNN is used as example network.</p></sec><sec id="sec005"><title>Loss functions</title><p>In the next paragraphs, we first define the uPL. We then compare the denoising results using an uPL with a small loss network with conventional loss functions used in the literature. This experiment serves as a simple demonstration of the suitability of the uPL for image denoising tasks. In the next section, we then investigate the impact of different characteristics of the uPL network on the denoising results. All experiments were performed with five different random seeds.</p><sec id="sec006"><title>Untrained Perceptual Loss</title><p>Perceptual Loss (PL) minimizes the differences between feature maps of neural networks of generated and ground truth images. PL is defined in 1</p><disp-formula id="pone.0318992.e301">
<alternatives><graphic xlink:href="pone.0318992.e301.jpg" id="pone.0318992.e301g" position="anchor"/><mml:math id="m01" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mi>L</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:munder class="msub"><mml:mrow><mml:mo> &#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x00177;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">-</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives>
<label>(1)</label>
</disp-formula><p>where <inline-formula id="pone.0318992.e009"><alternatives><graphic xlink:href="pone.0318992.e009.jpg" id="pone.0318992.e009g" position="anchor"/><mml:math id="m9" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> refer to the number of channels, the image height, the image width, and the image depth in layer <italic toggle="yes">j</italic>, respectively. <italic toggle="yes">&#x00177;</italic> refers to the generated image and <italic toggle="yes">y</italic> to the corresponding ground truth image, and <inline-formula id="pone.0318992.e011"><alternatives><graphic xlink:href="pone.0318992.e011.jpg" id="pone.0318992.e011g" position="anchor"/><mml:math id="m11" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> outputs the activation of the <inline-formula id="pone.0318992.e012"><alternatives><graphic xlink:href="pone.0318992.e012.jpg" id="pone.0318992.e012g" position="anchor"/><mml:math id="m12" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mstyle class="text"><mml:mtext>th</mml:mtext></mml:mstyle></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> layer in the loss network.</p></sec><sec id="sec007"><title>Proof of concept - Comparison with conventional loss functions</title><p>As proof of concept, we compared the results when using a small uPL network in the loss function with the most frequently used loss functions for image enhancement. We compare (1) the L1 loss [<xref rid="pone.0318992.ref021" ref-type="bibr">21</xref>,&#x000a0;<xref rid="pone.0318992.ref034" ref-type="bibr">34</xref>,&#x000a0;<xref rid="pone.0318992.ref043" ref-type="bibr">43</xref>], (2) a loss maximizing SSIM [<xref rid="pone.0318992.ref044" ref-type="bibr">44</xref>,&#x000a0;<xref rid="pone.0318992.ref045" ref-type="bibr">45</xref>], (3) uPL with the untrained 3D version of VGG19, (4) uPL with the untrained 3D AlexNet, and (5) uPL with a simple loss network with three convolutional layers (each with 32 features and kernel size 3, ReLU activation).</p><p>We use this proof of concept study and the small network as a starting point to investigate which impact different characteristics of an uPL network have on the results. In particular, we concentrate on the following questions: <italic toggle="yes">Can small and simple networks be successfully used in uPL? What are the important architectural parameters for successful use in uPL? What impact has the weight initialization? Is there a network that works best for all datasets or does the best network structure depend on image content? Is the impact of the uPL dependent on the denoising architecture? </italic> As there are more hyper-parameters than we can reasonably consider in the uPL, we concentrate on a few prominent ones. Our goal is to find an easy working solution rather than infeasibly covering the entire parameter space. We tested the performance of different uPL network characteristics for 10% added noise and the DnCNN architecture. If a network characteristic leads to a clear improvement, this network characteristic was used as the default characteristic. If performance across similar characteristics was similar, we used the simplest solution (i.e. the smallest network) in the consecutive tests.</p></sec></sec><sec id="sec008"><title>Characteristics of networks used in uPL</title><sec id="sec009"><title>Initialization of network used in uPL</title><p>As the networks in the uPL are not trained, different initializations may have considerable impact on the performance. We investigate the impact of weight initialization in the uPL network by analyzing the results using five different random seeds for weight initialization in PyTorch. Weights were drawn from a uniform distribution <inline-formula id="pone.0318992.e013"><alternatives><graphic xlink:href="pone.0318992.e013.jpg" id="pone.0318992.e013g" position="anchor"/><mml:math id="m13" display="inline" overflow="scroll"><mml:mstyle mathvariant="script"><mml:mi>U</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">-</mml:mo><mml:msqrt><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msqrt><mml:mo stretchy="false">,</mml:mo><mml:msqrt><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula> where <inline-formula id="pone.0318992.e014"><alternatives><graphic xlink:href="pone.0318992.e014.jpg" id="pone.0318992.e014g" position="anchor"/><mml:math id="m14" display="inline" overflow="scroll"><mml:mi>k</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mspace class="thinspace" width="0.17em"/><mml:mo stretchy="false">*</mml:mo><mml:mspace class="thinspace" width="0.17em"/><mml:msub><mml:mrow><mml:mi class="MathClass-op">&#x0220f;</mml:mi><mml:mo> &#x02061;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0318992.e015"><alternatives><graphic xlink:href="pone.0318992.e015.jpg" id="pone.0318992.e015g" position="anchor"/><mml:math id="m15" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> is the size of the <inline-formula id="pone.0318992.e016"><alternatives><graphic xlink:href="pone.0318992.e016.jpg" id="pone.0318992.e016g" position="anchor"/><mml:math id="m16" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mstyle class="text"><mml:mtext>th</mml:mtext></mml:mstyle></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> kernel. The loss network was a simple loss network with three convolutional layers, kernel size 3, and 32 feature maps.</p><p>Regarding the initialization method, five initialization methods were tested: (1) Kaiming uniform, (2) Kaiming normal, (3) Xavier uniform, (4) Xavier normal, and (5) the default uniform initialization of PyTorch (i.e.&#x000a0;weights are drawn from a uniform distribution) [<xref rid="pone.0318992.ref046" ref-type="bibr">46</xref>,&#x000a0;<xref rid="pone.0318992.ref047" ref-type="bibr">47</xref>].</p></sec><sec id="sec010"><title>Depth of network used in uPL</title><p>Previous works demonstrated that for 2D images deeper loss architectures resulted in better performance for image segmentation and super-resolution tasks [<xref rid="pone.0318992.ref028" ref-type="bibr">28</xref>,&#x000a0;<xref rid="pone.0318992.ref032" ref-type="bibr">32</xref>]. We investigate if this holds also for 3D data, by varying uPL network depth. We tested networks with depths 3, 5, 7, 9, and 13 convolutional layers connected with a ReLU activation function. Additionally, we tested kernel sizes of 3, 5, 7, and 9. As default, each convolutional layer created 32 feature maps.</p></sec><sec id="sec011"><title>Impact of pooling operations of network used in uPL</title><p>Down-sampling a signal creates aliasing and potential loss of information when fine details or other high spatial frequency signals are present. This is why down-sampling or pooling is recommended to be applied after sufficient smoothing only (see e.g.&#x000a0;[<xref rid="pone.0318992.ref048" ref-type="bibr">48</xref>]). As noise-initialized kernels typically have poor smoothing behavior the use of pooling layers in the uPL might lead to a loss in information and therefore to a performance drop. To assess the impact of pooling layers, we use an uPL network with five convolutional layers and added max-pooling operations. We were interested if the number of pooling operations impacts the results. The number of pooling operations varied from 1 to 3. We started with only one pooling operation after the first convolutional layer. Next, we added a second pooling layer after the second convolutional layer and a third pooling operation after the third convolutional layer.</p></sec></sec><sec id="sec012"><title>Implementation and training details</title><p>All experiments were implemented in Pytorch 1.10.0 and training was performed using PyTorch lightning. Networks were trained for 30.000 iterations with batch size 16, Adam optimizer [<xref rid="pone.0318992.ref049" ref-type="bibr">49</xref>] and learning rate 0.001. The training parameters were chosen as they lead to the overall best performance in the validation sets. For this purpose, networks were trained for 10.000, 15.000, 20.000, ... 50.000 iterations, batch size was set to 4, 8, 16, 24, and 32, and the learning rate was set to 0.001,0.002, ... 0.005. All possible combinations of these parameters were investigated and the parameters leading to the best performance in the above defined validation datasets were selected. For training, images were cropped randomly to a size of <inline-formula id="pone.0318992.e017"><alternatives><graphic xlink:href="pone.0318992.e017.jpg" id="pone.0318992.e017g" position="anchor"/><mml:math id="m17" display="inline" overflow="scroll"><mml:mrow><mml:mn>9</mml:mn></mml:mrow><mml:mn>6</mml:mn><mml:mo stretchy="false">&#x000d7;</mml:mo><mml:mrow><mml:mn>9</mml:mn></mml:mrow><mml:mn>6</mml:mn><mml:mo stretchy="false">&#x000d7;</mml:mo><mml:mrow><mml:mn>9</mml:mn></mml:mrow><mml:mn>6</mml:mn></mml:math></alternatives></inline-formula>. All codes will be available online after paper publication.</p></sec><sec id="sec013"><title>Evaluation metrics</title><p>For evaluation, we calculate the structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), and mean squared error (MSE) between generated and ground truth images. Mean and standard deviation (std) values over the datasets and random seeds are reported if not stated differently. As both datasets contain fine structures, the evaluation metrics were additionally calculated on the parts of the image containing important information. A cube of size 52 &#x000d7; 52 &#x000d7; 52 was cropped from the root data. As the roots grow from top to bottom and because the plant is always placed in the center of the scanner, the image was cropped such that the upper, middle image part was always present in the evaluation. I.e. the image was cropped from z-slices 0 - 52, and x-, and y-slices from 70-132. A cube of size 68 &#x000d7; 68 &#x000d7; 68 is cropped from the middle of the MRA images as these images contain important information mainly in the image center i.e. the center of the image was determined and a cube with the mentioned size was cropped such that the center of the cube aligned with the center of the image. An example illustration is displayed in S4 Fig.</p></sec></sec><sec sec-type="results" id="sec014"><title>Results</title><sec id="sec015"><title>Proof of concept - Comparison with conventional loss functions</title><p>In this experiment, the performance of different loss functions frequently used for image denoising are compared with an uPL containing a small network (three convolutional layers, kernel size 3). In this experiment, we observe consistent results for both datasets see <xref rid="pone.0318992.t002" ref-type="table">Table 2</xref>. For both datasets, uPL using a small network leads to the best evaluation metrics compared with the other loss functions included in this study. For example, uPL results in an SSIM of 0.86 &#x000b1; 0.01 and a PSNR of 38.1 &#x000b1; 0.4 for the roots dataset. For comparison, L1 (SSIM) loss yields a mean SSIM of 0.79 &#x000b1; 0.02 (0.63 &#x000b1; 0.03) and a PSNR of 37.4 &#x000b1; 0.5 (31.7 &#x000b1; 0.8). For the MRA dataset improvements are similar. uPL yields an SSIM of 0.9 &#x000b1; 0.01 while the L1 loss yields an SSIM of 0.79 &#x000b1; 0.02 and SSIM as loss function leads to an SSIM of 0.86 &#x000b1; 0.04.</p><table-wrap position="float" id="pone.0318992.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0313772.t002</object-id><label>Table 2</label><caption><title>Mean and standard deviation values for the five random seeds used for network training for MRA images and MR root images for the loss functions included in this study.</title><p>Best results are underlined. Mean and standard deviation values for the evaluation metrics calculated exclusively on the center image parts as well as std values across test sets are given in the Supporting information S2 and S3 Tables.</p></caption><alternatives><graphic xlink:href="pone.0318992.t002" id="pone.0318992.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="3" rowspan="1">MRA</th><th align="left" colspan="3" rowspan="1">MR root</th></tr><tr><th align="left" rowspan="1" colspan="1">Loss</th><th align="left" rowspan="1" colspan="1">SSIM</th><th align="left" rowspan="1" colspan="1">PSNR</th><th align="left" rowspan="1" colspan="1">MSE</th><th align="left" rowspan="1" colspan="1">SSIM</th><th align="left" rowspan="1" colspan="1">PSNR</th><th align="left" rowspan="1" colspan="1">MSE (roots)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">L1</td><td align="left" rowspan="1" colspan="1">0.79 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">31.4 &#x000b1; 0.41</td><td align="left" rowspan="1" colspan="1">0.0047 &#x000b1; 0.004</td><td align="left" rowspan="1" colspan="1">0.79 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">34.4 &#x000b1; 0.5</td><td align="left" rowspan="1" colspan="1">0.038 &#x000b1; 0.006</td></tr><tr><td align="left" rowspan="1" colspan="1">SSIM loss</td><td align="left" rowspan="1" colspan="1">0.86 &#x000b1; 0.04</td><td align="left" rowspan="1" colspan="1">35.6 &#x000b1; 1.1</td><td align="left" rowspan="1" colspan="1">0.0051 &#x000b1; 0.011</td><td align="left" rowspan="1" colspan="1">0.63 &#x000b1; 0.03</td><td align="left" rowspan="1" colspan="1">31.7 &#x000b1; 0.8</td><td align="left" rowspan="1" colspan="1">0.058 &#x000b1; 0.008</td></tr><tr><td align="left" rowspan="1" colspan="1">VGG19</td><td align="left" rowspan="1" colspan="1">0.87 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">40.2 &#x000b1; 0.3</td><td align="left" rowspan="1" colspan="1">0.0075 &#x000b1; 0.0008</td><td align="left" rowspan="1" colspan="1">0.84 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">37.8 &#x000b1; 0.3</td><td align="left" rowspan="1" colspan="1">0.031 &#x000b1; 0.007</td></tr><tr><td align="left" rowspan="1" colspan="1">AlexNet</td><td align="left" rowspan="1" colspan="1">0.86 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">30.9 &#x000b1; 0.3</td><td align="left" rowspan="1" colspan="1">0.0043 &#x000b1; 0.001</td><td align="left" rowspan="1" colspan="1">0.75 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">28.4 &#x000b1; 0.4</td><td align="left" rowspan="1" colspan="1">0.043 &#x000b1; 0.006</td></tr><tr><td align="left" rowspan="1" colspan="1">Our SimpleNet</td><td align="left" rowspan="1" colspan="1">
<underline>0.90 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>41.3 &#x000b1; 0.4</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.0043 &#x000b1; 0.0004</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.86 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>38.3 &#x000b1; 0.1</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.031 &#x000b1; 0.005</underline>
</td></tr></tbody></table></alternatives></table-wrap><p>The uPL containing a small network also outperforms the straight-forward 3D extension of uPL using AlexNet or VGG. For the MRA data, the performance drops to an SSIM of 0.86 &#x000b1; 0.02 (PSNR 30.9 &#x000b1; 0.3). The same holds for the root images, where the drop in performance is high. In contrast, using VGG19 in the loss leads to a slight performance drop when compared with the small network for both datasets. However, differences are not pronounced and almost not visible when comparing images by eye.</p><p><xref rid="pone.0318992.g003" ref-type="fig">Fig 3</xref> shows denoising results of one MR root and one MRA image for 10% noise added reconstructed with the DnCNN. As displayed, using L1 loss or SSIM loss leads to strong suppression of fine roots and to incorrect reconstruction of finer vessels. Also uPL using AlexNet fails to reconstruct finer roots and vessels accurately. The DnCNN trained with the smaller network (3 convolutional layers and kernel size 3), the VGG19 network creates root and MRA images where also fine details are correctly reconstructed.</p><fig position="float" id="pone.0318992.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0318992.g003</object-id><label>Fig 3</label><caption><title>MIP of example 3D MR images of plant roots and brain vessels: Ground truth image and image parts after denoising the image that was disturbed with 10% noise.</title><p>Upper row: MIP of whole image. In the second and third row, the marked region is zoomed in. Zoomed part reconstructed with different loss functions. Differences are clearly visible and remarkable differences are marked with an arrow.</p></caption><graphic xlink:href="pone.0318992.g003" position="float"/></fig></sec><sec id="sec016"><title>Impact of network initialization</title><p>To demonstrate that the superior performance of our small loss network is not due to chance, we investigated the performance for five different random seeds in the loss network. Additionally, the impact of network initialization on performance metrics is investigated. For both datasets, the differences across weight initialization seeds are small (see Supporting information Table S1). E.g.&#x000a0;for the MRA dataset different seeds resulted in PSNR between 41.2 and 41.4 and SSIM between 0.90 and 0.91. For the root dataset, different seeds resulted in SSIM values between 0.85 and 0.86.</p><p>All but the uniform initialization method lead to comparable results across datasets (see <xref rid="pone.0318992.t003" ref-type="table">Table 3</xref>). For the MRA dataset, uniform initialization lead to SSIM performance drop from e.g.&#x000a0;0.92 for Xavier normal to 0.86 to uniform initialization. For the root dataset, no difference between initialization methods can be observed. We therefore use the Xavier normal initialization in all other experiments.</p><table-wrap position="float" id="pone.0318992.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0313772.t003</object-id><label>Table 3</label><caption><title>Mean and std evaluation metrics for MRA and MR root images for different initializations of the untrained loss network.</title><p>As shown, different initializations have a minor impact on the results.</p></caption><alternatives><graphic xlink:href="pone.0318992.t003" id="pone.0318992.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="3" rowspan="1">MRA</th><th align="left" colspan="3" rowspan="1">MR root</th></tr><tr><th align="left" rowspan="1" colspan="1">uPL Loss initialization method</th><th align="left" rowspan="1" colspan="1">SSIM</th><th align="left" rowspan="1" colspan="1">PSNR</th><th align="left" rowspan="1" colspan="1">MSE</th><th align="left" rowspan="1" colspan="1">SSIM</th><th align="left" rowspan="1" colspan="1">PSNR</th><th align="left" rowspan="1" colspan="1">MSE (roots)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Kaiming uniform</td><td align="left" rowspan="1" colspan="1">0.91 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">40.6 &#x000b1; 0.3</td><td align="left" rowspan="1" colspan="1">4.3e-3 &#x000b1; 2e-4</td><td align="left" rowspan="1" colspan="1">0.86 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">38.8 &#x000b1; 0.2</td><td align="left" rowspan="1" colspan="1">0.034 &#x000b1; 0.001</td></tr><tr><td align="left" rowspan="1" colspan="1">Kaiming normal</td><td align="left" rowspan="1" colspan="1">0.91 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">40.5 &#x000b1; 0.2</td><td align="left" rowspan="1" colspan="1">4.2e-3 &#x000b1; 1e-4</td><td align="left" rowspan="1" colspan="1">0.85 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">38.8 &#x000b1; 0.3</td><td align="left" rowspan="1" colspan="1">0.034 &#x000b1; 0.002</td></tr><tr><td align="left" rowspan="1" colspan="1">Xavier uniform</td><td align="left" rowspan="1" colspan="1">0.92 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">40.2 &#x000b1; 0.2</td><td align="left" rowspan="1" colspan="1">4.0e-3 &#x000b1; 2e-4</td><td align="left" rowspan="1" colspan="1">0.85&#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">38.8 &#x000b1; 0.2</td><td align="left" rowspan="1" colspan="1">0.032 &#x000b1; 0.001</td></tr><tr><td align="left" rowspan="1" colspan="1">Xavier normal</td><td align="left" rowspan="1" colspan="1">0.92 &#x000b1; 0.00</td><td align="left" rowspan="1" colspan="1">40.4 &#x000b1; 0.3</td><td align="left" rowspan="1" colspan="1">4.3e-3 &#x000b1; 4e-4</td><td align="left" rowspan="1" colspan="1">0.86 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">38.9 &#x000b1; 0.2</td><td align="left" rowspan="1" colspan="1">0.041 &#x000b1; 0.001</td></tr><tr><td align="left" rowspan="1" colspan="1">Default uniform</td><td align="left" rowspan="1" colspan="1">0.86 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">36.7 &#x000b1; 0.3</td><td align="left" rowspan="1" colspan="1">4.9e-3&#x000b1; 5e-4</td><td align="left" rowspan="1" colspan="1">0.85 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">38.8 &#x000b1; 0.3</td><td align="left" rowspan="1" colspan="1">0.05 &#x000b1; 0.002</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec017"><title>Number of convolutional layers vs.&#x000a0;kernel size</title><p>Results in <xref rid="pone.0318992.t004" ref-type="table">Table 4</xref> show that the number of convolutional layers and kernel sizes had a minor impact on the results.</p><table-wrap position="float" id="pone.0318992.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0313772.t004</object-id><label>Table 4</label><caption><title>Mean and std SSIM values for different kernel sizes and network depth for MRA (above) and for the MR root dataset (below).</title><p>All other evaluation metrics can be found in the Supporting information S4 and S5 Tables. As displayed, differences across network sizes and kernel sizes are small.</p></caption><alternatives><graphic xlink:href="pone.0318992.t004" id="pone.0318992.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Metric</th><th align="left" colspan="4" rowspan="1">Number of conv. layers</th></tr><tr><th align="left" rowspan="1" colspan="1">Kernel size</th><th align="left" rowspan="1" colspan="1">3 conv</th><th align="left" rowspan="1" colspan="1">5 conv</th><th align="left" rowspan="1" colspan="1">9 conv</th><th align="left" rowspan="1" colspan="1">13 conv</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1"/><td align="left" colspan="4" rowspan="1">SSIM - MRA dataset</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">0.90 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.93&#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.90&#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.90&#x000b1; 0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">0.92 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.93&#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.90&#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.90&#x000b1; 0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">0.93 &#x000b1; 0.00</td><td align="left" rowspan="1" colspan="1">0.86 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.90&#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.90&#x000b1; 0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">0.92 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.91 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.89&#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.89&#x000b1; 0.02</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" colspan="4" rowspan="1">SSIM - Root dataset</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">0.86 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.83 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.84 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.84 &#x000b1; 0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">0.85 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.86 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.84 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.83 &#x000b1; 0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">0.84 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.84 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.84 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.83 &#x000b1; 0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">0.84 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.83 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.85 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.84 &#x000b1; 0.02</td></tr></tbody></table></alternatives></table-wrap><p>For the MRA images, seven convolutional layers with kernel size 3, five convolutional layers with kernel size 3 or 5 lead with an SSIM of 0.93 to the best results. Similarly, for the root dataset, five convolutional layers with kernel size 5 led with an SSIM of 0.86 to the best results, followed by three convolutional layers with kernel size 3. Taking the variance over the five random seeds into account, the difference across numbers of layers and kernel size is small. The kernel size has little impact on the results for both dataset. A smaller network depth is beneficial.</p></sec><sec id="sec018"><title>Impact of pooling layers</title><p>As pooling operations reduce image details, we investigate if the use of pooling operations might have an impact on the results. For the root images, pooling operations hurt performance slightly (e.g. SSIM without pooling 0.87, with one max pooling operation 0.85). For the MRA dataset, max pooling operations increase evaluation metrics for 1 and 2 pooling operations (SSIM 0.96 and 0.94). However, evaluation metrics dropped slightly for 3 pooling operations (SSIM 0.91). This impact is illustrated in <xref rid="pone.0318992.g004" ref-type="fig">Fig 4</xref>.</p><fig position="float" id="pone.0318992.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0318992.g004</object-id><label>Fig 4</label><caption><title>Impact of pooling operations in the uPL on the results (Left: MIP of an example MRA image; and right: MR Root image. The marked region is the region which is displayed in large on the right.)</title><p>First row: GT image and zoomed part of GT image. Second row: Reconstructed image of image disturbed with 10% noise with different number of pooling operations in the uPL network. Third row: Difference image between GT and reconstructed image.</p></caption><graphic xlink:href="pone.0318992.g004" position="float"/></fig><fig position="float" id="pone.0318992.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0318992.g005</object-id><label>Fig 5</label><caption><title>MIP of original image (First row) and denoising results of image disturbed with 10% noise for different denoising networks and the L1-loss (second row) and our uPL.</title><p>Remarkable differences are marked with arrows.</p></caption><graphic xlink:href="pone.0318992.g005" position="float"/></fig></sec><sec id="sec019"><title>Denoising network architecture vs.&#x000a0;loss function for different noise levels</title><p>Lastly, we investigated the impact of the denoising network architecture across noise levels. Hereby, we were interested in the benefits of uPL across network architectures. We investigated the performance for the previously described DnCNN, ResNet, and Transformer architectures across noise levels. We used a network with three convolutional layers and kernel size 3 in the uPL.</p><p><xref rid="pone.0318992.t005" ref-type="table">Table 5</xref> lists the SSIM for both datasets and all noise levels. Example results are displayed in 5 The improvement when using uPL depends on the network architecture and the noise level. Occasionally, uPL performed equal or slightly worse than L1 loss. For the Transformer network, the uPL showed its benefit, especially for higher noise levels (SSIM of 0.78 &#x000b1; 0.02 (0.81 &#x000b1; 0.01) for uPL and 0.68 &#x000b1; 0.03 (0.72 &#x000b1; 0.03) for L1 for the MRA (root) dataset and 20% noise). The benefit of uPL was similar for DnCNN and ResNet architectures.</p><table-wrap position="float" id="pone.0318992.t005"><object-id pub-id-type="doi">10.1371/journal.pone.0313772.t005</object-id><label>Table 5</label><caption><title>Mean and std SSIM values across different random seeds for both datasets calculated.</title><p>Underlined are the values for the best loss function. I.e. if uPL outperformed L1-loss for a network architecture, the uPL values are underlined. All other evaluation metrics, std values across the testsets, and evaluation metrics calculated on image parts only are listed in the Supporting information (S6&#x02013;S10 Tables).</p></caption><alternatives><graphic xlink:href="pone.0318992.t005" id="pone.0318992.t005g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1"/><th align="left" colspan="4" rowspan="1">SSIM - MR root dataset</th></tr><tr><th align="left" rowspan="1" colspan="1">Network/Loss</th><th align="left" rowspan="1" colspan="1">1% noise</th><th align="left" rowspan="1" colspan="1">5% noise</th><th align="left" rowspan="1" colspan="1">10% noise</th><th align="left" rowspan="1" colspan="1">20% noise</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">DnCNN/L1</td><td align="left" rowspan="1" colspan="1">0.83 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.81 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.79 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.78 &#x000b1; 0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">DnCNN/uPL</td><td align="left" rowspan="1" colspan="1">
<underline>0.86 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.86 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.86 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.82 &#x000b1; 0.0</underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1">ResNet/L1</td><td align="left" rowspan="1" colspan="1">0.83 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.83 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.83 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.80 &#x000b1; 0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">ResNet/uPL</td><td align="left" rowspan="1" colspan="1">
<underline>0.86 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.85 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.84 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.82 &#x000b1; 0.0</underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer/L1</td><td align="left" rowspan="1" colspan="1">0.77 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.79 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.78 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.72 &#x000b1; 0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer/uPL</td><td align="left" rowspan="1" colspan="1">0.81 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">
<underline>0.82 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.82 &#x000b1; 0.0</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.81 &#x000b1; 0.01</underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" colspan="4" rowspan="1">SSIM - MRA dataset</td></tr><tr><td align="left" rowspan="1" colspan="1">DnCNN/L1</td><td align="left" rowspan="1" colspan="1">0.86 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.81 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">0.79 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.81 &#x000b1; 0.01</td></tr><tr><td align="left" rowspan="1" colspan="1">DnCNN/uPL</td><td align="left" rowspan="1" colspan="1">
<underline>0.92 &#x000b1; 0.01 </underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.90 &#x000b1; 0.01 </underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.87 &#x000b1; 0.01 </underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.84 &#x000b1; 0.02 </underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1">ResNet/L1</td><td align="left" rowspan="1" colspan="1">
<underline>0.99 &#x000b1; 0.01 </underline>
</td><td align="left" rowspan="1" colspan="1">0.87 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">0.81 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">
<underline>0.84 &#x000b1; 0.01 </underline>
</td></tr><tr><td align="left" rowspan="1" colspan="1">ResNet/uPL</td><td align="left" rowspan="1" colspan="1">0.98 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">
<underline>0.92 &#x000b1; 0.01 </underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.85 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">0.82 &#x000b1; 0.02</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer/L1</td><td align="left" rowspan="1" colspan="1">0.97 &#x000b1; 0.03</td><td align="left" rowspan="1" colspan="1">0.91 &#x000b1; 0.02</td><td align="left" rowspan="1" colspan="1">
<underline>0.85 &#x000b1; 0.02 </underline>
</td><td align="left" rowspan="1" colspan="1">0.68 &#x000b1; 0.03</td></tr><tr><td align="left" rowspan="1" colspan="1">Transformer/uPL</td><td align="left" rowspan="1" colspan="1">
<underline> 0.98 &#x000b1; 0.02</underline>
</td><td align="left" rowspan="1" colspan="1">
<underline>0.93 &#x000b1; 0.01</underline>
</td><td align="left" rowspan="1" colspan="1">0.83 &#x000b1; 0.01</td><td align="left" rowspan="1" colspan="1">
<underline>0.78 &#x000b1; 0.02 </underline>
</td></tr></tbody></table></alternatives></table-wrap><p>As expected, denoising performance was best for the lowest noise levels: The uPL resulted in SSIM values of 0.82 - 0.86 for the roots data and of 0.9 - 0.99 for the MRA data. The L1 loss led to SSIM values between 0.81 - 0.84 for the root and 0.81 - 0.98 for the MRA data. The uPL outperformed especially in higher noise levels the L1 loss although the denoising performance decreases for higher noise levels. For the MRA data, the superiority in performance was not visible for the lowest and highest noise level and the ResNet, and 10% added noise and the Transformer network.</p></sec></sec><sec sec-type="conclusions" id="sec020"><title>Discussion</title><p>In summary, we demonstrated that the untrained Perceptual Loss leads to clear improvement for image denoising of MR images displaying line-like structures.</p><p>We observe that for both datasets, the uPL leads especially with small uPL networks to very good results. We demonstrated that loss network initialization has a minor impact on the results. However, more parameters might be important and future research may find better loss networks. For example, the use of other non-linearities than ReLU might influence the results and introducing layer weights or masks may be pertinent (similar to the &#x02018;diversity loss&#x02019; in [<xref rid="pone.0318992.ref050" ref-type="bibr">50</xref>]). We plan to address these points in future work.</p><p>It is hard to compare our results directly with results from other works due to the special characteristics of our datasets. Especially the root dataset is very sparse and yields very thin structures. As in our experiments, U-Net-like architectures failed for the MR root images, other famous denoising networks were not tested in this study [<xref rid="pone.0318992.ref051" ref-type="bibr">51</xref>,&#x000a0;<xref rid="pone.0318992.ref052" ref-type="bibr">52</xref>]. We hypothesize that the down-sampling part of a U-Net architecture leads to a loss of information as the roots are very fine structures. We found only a small number of works that report on image denoising of MR angiographs. In [<xref rid="pone.0318992.ref053" ref-type="bibr">53</xref>] the authors used also an U-Net-shaped architecture. However, by testing three different denoising networks - one of them a Transformer network, we aimed to demonstrate that the success of the uPL is not network-dependent and can improve the results of different network architectures.</p><p>In this work, we focused on denoising of MR images displaying line-like structures. These images contain very fine details and yield therefore special challenges. In future work, we will investigate if the uPL can also be used for medical images yielding other image characteristics such as MR images of the human brain, PET or CT images.</p><p>Especially for difficult tasks, i.e.&#x000a0;with increasing noise level the uPL performs considerably better than L1 loss for both datasets. This is probably because the uPL is taking neighborhood information into account by using convolutional kernels with sizes larger than 1 when calculating the loss. Therefore, even if one pixel is highly disturbed by noise, information about the ground truth pixel can still be estimated by its neighbors.</p><p>Regarding network architecture, we observed similar or even better performance when using uPL and a DnCNN than when combining L1 loss and one of the other network structures. These results indicate that the use of uPL makes it possible to use computationally less expensive networks without a performance drop. Additionally, our results suggest that the additional value of a Transformer network is not visible for both datasets. This is in contrast to other works that showed superior performance for similar denoising Transformers [<xref rid="pone.0318992.ref054" ref-type="bibr">54</xref>]. The lower performance of the Transformer network is likely due to the sparsity of the MR root and MRA images. Locally, both datasets contain a rather low variety of different features, i.e.&#x000a0;&#x02018;just&#x02019; lines and not rich texture. Therefore, large, computationally expensive, and complex networks such as the tested Transfomer variant may tend to overfit.</p><p>The success of perceptual loss was thought to be due to the features networks learned during training. However, as previous studies demonstrated for 2D images, pre-trained and untrained networks used in the loss function lead to comparable results [<xref rid="pone.0318992.ref032" ref-type="bibr">32</xref>,&#x000a0;<xref rid="pone.0318992.ref033" ref-type="bibr">33</xref>]. This might be because randomly initialized weights can also represent the statistical properties of the training data. The pre-trained networks are usually trained on classification tasks. However, other image characteristics could be more important for image denoising of an entire image. E.g. the display of sharp edges can also be captured by convolutions and untrained feature maps. Additionally, the perceptual distance between two images is equivalent to the maximum mean discrepancy (MMD) distance between local distributions of small patches in the two images [<xref rid="pone.0318992.ref055" ref-type="bibr">55</xref>]. As also demonstrated in this study, different network structures in the loss function have a high impact on the denoising results. These findings suggest that the design of the network architecture used in the loss function can be chosen such that it captures the most important image information. With a best mean SSIM of 0.87 the denoising results for our root dataset still need improvement to allow for reduced measurement time in the targeted application. None of the tested denoising network/loss function combinations allow to recover very fine roots, even though the performance increase using our uPL is considerable. In consequence, as of now, plants containing very fine roots need to be scanned for a longer time than plants with thick roots. Therefore, before implementing the trained networks on a plant root scanner, it needs to be verified for which plants (i.e. which root thickness) reduced image time in combination with a denoising network can be used without losing fine details.</p><p>Results are considerably better for the MRA dataset. With SSIM values between 0.99 and 0.93 for the lower noise levels, the proposed networks might be used for image denoising in a clinical setting. However, to apply the proposed approach in a clinical setting, our denoising framework needs to be verified on MRA data including healthy and unhealthy subjects (i.e. patients with e.g. an aneurysm). In a clinical scenario, it needs to be verified that denoising does not suppress important clinical information. This will be done in future studies based on this work.</p><p>Our study has several limitations. First, all networks were trained and applied on data acquired at the same scanner. In future work, we will train and test our approach on data from different scanners yielding different image characteristics. Second, the MRA dataset used in this study yields exclusively images from healthy subjects. In future work, we will apply the networks also to subjects with vessel abnormalities.</p><p>MR images of human subjects also suffer from motion artifacts. The proposed uPL might also be a promising candidate for training networks for MR motion correction what will be investigated in future work. All in all, our results demonstrate that the untrained Perceptual Loss can be used in image denoising networks. To what extent it can be used for image enhancement in a clinical scenario, needs to be determined in future work. However, the untrained perceptual loss should definitely considered when training neural networks on MR images displaying fine, line-like structures.</p></sec></body><back><ack id="a021"><p>We acknowledge the help of Dagmar van Dusschoten in providing MR images. The authors gratefully acknowledge the computing time granted through JARA on the supercomputer JURECA [<xref rid="pone.0318992.ref056" ref-type="bibr">56</xref>] at Forschungszentrum J&#x000fc;lich.</p></ack><fn-group><fn id="n1"><p><ext-link xlink:href="https://brain-development.org/ixi-dataset/" ext-link-type="uri">https://brain-development.org/ixi-dataset/</ext-link>, assessed on 09/10/2022.</p></fn></fn-group><sec sec-type="supplementary-material" id="sec030"><title>Supporting information</title><supplementary-material id="pone.0318992.s001" position="float" content-type="local-data"><label>S1 Fig</label><caption><title>ResNet architecture.</title><p>Illustration of the denoising network ResNet. The residual blocks are repeated five times. The first convolutional layer yields kernel size 9 with 64 output channels. All other convolutional layers yield kernel size 3 and 64 output channels. The last convolutional layer has kernel size 3 and 1 output channel.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s001.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s002" position="float" content-type="local-data"><label>S2 Fig</label><caption><title>Transformer architecture.</title><p>IIllustration of the denoising Transformer. The Transformer blocks are organized sequentially. Details about the Transformer blocks are explained in the corresponding paper for 2D.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s002.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s003" position="float" content-type="local-data"><label>S3 Fig</label><caption><title>DnCNN architecture.</title><p>Illustration of the denoising network DnCNN.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s003.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s004" position="float" content-type="local-data"><label>S4 Fig</label><caption><title>Illustration of evaluated image parts.</title><p>Illustration of cropped image parts on which evaluation metrics were calculated.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s004.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s005" position="float" content-type="local-data"><label>S1 Table</label><caption><title>Metrics Random Seeds.</title><p>Evaluation metrics for different random seeds for MRA and root dataset.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s005.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s006" position="float" content-type="local-data"><label>S2 Table</label><caption><title>Metrics different loss functions.</title><p>Evaluation metrics for MRA images and MR root images for the loss functions included in this study calculated on image parts only. Given are the mean and standard deviation values for the five random seeds used for network training.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s006.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s007" position="float" content-type="local-data"><label>S3 Table</label><caption><title>Mean and std values one random seed.</title><p>Mean and std values calculated over the testset for one random seed. Values are similar across random seeds.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s007.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s008" position="float" content-type="local-data"><label>S4 Table</label><caption><title>MSE for different kernel sizes/network depths.</title><p>MSE for different kernel sizes and network depth: MSE calculated only for roots regions (above) and MSE for the whole image also for the MR root dataset (below).</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s008.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s009" position="float" content-type="local-data"><label>S5 Table</label><caption><title>Evaluation metrics for kernel sizes/network depths.</title><p>PSNR/MSE for different kernel sizes and network depth for MRA (above) and MSE for the MRA dataset (below).</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s009.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s010" position="float" content-type="local-data"><label>S6 Table</label><caption><title>PSNR values for different network architectures.</title><p>PSNR values for both datasets, network structures, and noise levels.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s010.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s011" position="float" content-type="local-data"><label>S7 Table</label><caption><title>MSE values calculated on image parts for different network architectures.</title><p>MSE values only calculated on the roots part and MSE values for both datasets for all network architectures, and noise levels included in this study.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s011.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s012" position="float" content-type="local-data"><label>S8 Table</label><caption><title>SSIM values calculated on image parts for different network architectures.</title><p>SSIM values for both datasets calculated on the center of the image.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s012.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s013" position="float" content-type="local-data"><label>S9 Table</label><caption><title>PSNR values calculated on image parts for different network architectures.</title><p>PSNR values for both datasets calculated on the center of the image.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s013.pdf"/></supplementary-material><supplementary-material id="pone.0318992.s014" position="float" content-type="local-data"><label>S10 Table</label><caption><title>MSE values calculated on image parts for different network architectures.</title><p>MSE values for both datasets calculated on the center of the image.</p><p>(PDF)</p></caption><media xlink:href="pone.0318992.s014.pdf"/></supplementary-material></sec><ref-list><title>References</title><ref id="pone.0318992.ref001"><label>1</label><mixed-citation publication-type="journal">Tomasi C, Manduchi R. Bilateral filtering for gray and color images. In: Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271); 1998; p. 839&#x02013;46.</mixed-citation></ref><ref id="pone.0318992.ref002"><label>2</label><mixed-citation publication-type="journal">Banterle F, Corsini M, Cignoni P, Scopigno R. A low-memory, straightforward and fast bilateral filter through subsampling in spatial domain. Comput Graph Forum. 2012;31(1) 19&#x02013;32.</mixed-citation></ref><ref id="pone.0318992.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Manjon</surname><given-names>JV</given-names></name>, <name><surname>Carbonell-Caballero</surname><given-names>J</given-names></name>, <name><surname>Lull</surname><given-names>JJ</given-names></name>, <name><surname>Garca-Mart</surname><given-names>G</given-names></name>, <name><surname>Mart-Bonmat</surname><given-names>L</given-names></name>, <name><surname>Robles</surname><given-names>M</given-names></name>. <article-title>MRI denoising using non-local means</article-title>. <source>Med Image Anal.</source>
<year>2008</year>;<volume>12</volume>(<issue>4</issue>):<fpage>514</fpage>&#x02013;<lpage>23</lpage>.<pub-id pub-id-type="pmid">18381247</pub-id>
</mixed-citation></ref><ref id="pone.0318992.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Fan</surname><given-names>J</given-names></name>, <name><surname>Ai</surname><given-names>D</given-names></name>, <name><surname>Zhou</surname><given-names>S</given-names></name>, <name><surname>Tang</surname><given-names>S</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>. <article-title>Brain MR image denoising for Rician noise using pre-smooth non-local means filter</article-title>. <source>Biomed Eng Online.</source>
<year>2015</year>;<volume>14</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>20</lpage>.<pub-id pub-id-type="pmid">25564100</pub-id>
</mixed-citation></ref><ref id="pone.0318992.ref005"><label>5</label><mixed-citation publication-type="book"><name><surname>Zamir</surname><given-names>SW</given-names></name>, <name><surname>Arora</surname><given-names>A</given-names></name>, <name><surname>Khan</surname><given-names>S</given-names></name>, <name><surname>Hayat</surname><given-names>M</given-names></name>, <name><surname>Khan</surname><given-names>FS</given-names></name>, <name><surname>Yang</surname><given-names>MH</given-names></name>. <article-title>Restormer: efficient transformer for high-resolution image restoration. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</article-title>; <year>2022</year>. </mixed-citation></ref><ref id="pone.0318992.ref006"><label>6</label><mixed-citation publication-type="journal">Zhang Y, Li K, Li K, Zhong B, Fu Y. Residual non-local attention networks for image restoration. arXiv preprint 2019. p. 1&#x02013;18. <ext-link xlink:href="https://arxiv.org/abs/1903.10082" ext-link-type="uri">https://arxiv.org/abs/1903.10082</ext-link></mixed-citation></ref><ref id="pone.0318992.ref007"><label>7</label><mixed-citation publication-type="journal">Fan CM, Liu TJ, Liu KH. SUNet: swin transformer UNet for Image Denoising. In: 2022 IEEE International Symposium on Circuits and Systems (ISCAS); 2022; p. 2333&#x02013;7.</mixed-citation></ref><ref id="pone.0318992.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>ZY</given-names></name>, <name><surname>Zhao</surname><given-names>XL</given-names></name>, <name><surname>Lin</surname><given-names>J</given-names></name>, <name><surname>Chen</surname><given-names>Y</given-names></name>. <article-title>Nonlocal-based tensor-average-rank minimization and tensor transform-sparsity for 3D image denoising</article-title>. <source>Knowl-Based Syst</source>. <year>2022</year>;<volume>244</volume>:<fpage>108590</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.knosys.2022.108590</pub-id></mixed-citation></ref><ref id="pone.0318992.ref009"><label>9</label><mixed-citation publication-type="journal">Wang Q, Li TQ, Sun H, Yang H, Li X. An enhanced network for brain MR image denoising. Intell Data Anal. 1&#x02013;13.</mixed-citation></ref><ref id="pone.0318992.ref010"><label>10</label><mixed-citation publication-type="journal">Xue H, Hooper S, Rehman A, Pierce I, Treibel T, Davies R, et al. Imaging transformer for MRI denoising with the SNR unit training: enabling generalization across field-strengths, imaging contrasts, and anatomy. arXiv preprint 2024. <ext-link xlink:href="https://arxiv.org/abs/2404.02382" ext-link-type="uri">https://arxiv.org/abs/2404.02382</ext-link></mixed-citation></ref><ref id="pone.0318992.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Lustig</surname><given-names>M</given-names></name>, <name><surname>Donoho</surname><given-names>D</given-names></name>, <name><surname>Pauly</surname><given-names>JM</given-names></name>. <article-title>Sparse MRI: The application of compressed sensing for rapid MR imaging</article-title>. <source>Magnet Resonance Med: Off J Int Soc Magnet Resonance Med.</source>
<year>2007</year>;<volume>58</volume>(<issue>6</issue>):<fpage>1182</fpage>&#x02013;<lpage>95</lpage>.</mixed-citation></ref><ref id="pone.0318992.ref012"><label>12</label><mixed-citation publication-type="journal">12</mixed-citation></ref><ref id="pone.0318992.ref013"><label>13</label><mixed-citation publication-type="journal">Hirano Y, Fujima N, Kameda H, Ishizaka K, Kwon J, Yoneyama M, et al. High resolution TOF-MRA using compressed sensing-based deep learning image reconstruction for the visualization of lenticulostriate arteries: a preliminary study. Magnet Resonance Med Sci. 2024:mp-2024.</mixed-citation></ref><ref id="pone.0318992.ref014"><label>14</label><mixed-citation publication-type="journal">Chung H, Lee ES, Ye JC. MR image denoising and super-resolution using regularized reverse diffusion. IEEE Trans Med Imaging. 2022; 42(4):922&#x02013;34. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tmi.2022.3220681</pub-id></mixed-citation></ref><ref id="pone.0318992.ref015"><label>15</label><mixed-citation publication-type="journal">Moreno Lopez M, Frederick JM, Ventura J. Evaluation of MRI denoising methods using unsupervised learning. Front Artif Intell. 2021;4:642731.</mixed-citation></ref><ref id="pone.0318992.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Koch</surname><given-names>KM</given-names></name>, <name><surname>Sherafati</surname><given-names>M</given-names></name>, <name><surname>Arpinar</surname><given-names>VE</given-names></name>, <name><surname>Bhave</surname><given-names>S</given-names></name>, <name><surname>Ausman</surname><given-names>R</given-names></name>, <name><surname>Nencka</surname><given-names>AS</given-names></name>, <etal>et al</etal>. <article-title>Analysis and evaluation of a deep learning reconstruction approach with denoising for orthopedic MRI</article-title>. <source>Radiol: Artif Intell.</source>
<year>2021</year>:<volume>3</volume>(<issue>6</issue>);<fpage>e200278</fpage>.<pub-id pub-id-type="pmid">34870214</pub-id>
</mixed-citation></ref><ref id="pone.0318992.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Sharif</surname><given-names>S</given-names></name>, <name><surname>Naqvi</surname><given-names>RA</given-names></name>, <name><surname>Biswas</surname><given-names>M</given-names></name>, <name><surname>Loh</surname><given-names>WK</given-names></name>. <article-title>Deep perceptual enhancement for medical image analysis</article-title>. <source>IEEE J Biomed Health Inf.</source>
<year>2022</year>;<volume>26</volume>(<issue>10</issue>):<fpage>4826</fpage>&#x02013;<lpage>36</lpage>.</mixed-citation></ref><ref id="pone.0318992.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Tian</surname><given-names>Q</given-names></name>, <name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Fan</surname><given-names>Q</given-names></name>, <name><surname>Polimeni</surname><given-names>JR</given-names></name>, <name><surname>Bilgic</surname><given-names>B</given-names></name>, <name><surname>Salat</surname><given-names>DH</given-names></name>, <name><surname>Huang Susie</surname><given-names>Y</given-names></name>. <article-title>SDnDTI: self-supervised deep learning-based denoising for diffusion tensor MRI</article-title>. <source>Neuroimage</source>. <year>2022</year>;<volume>253</volume>:<fpage>119033</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119033</pub-id>
<pub-id pub-id-type="pmid">35240299</pub-id>
</mixed-citation></ref><ref id="pone.0318992.ref019"><label>19</label><mixed-citation publication-type="book"><name><surname>Aetesam</surname><given-names>H</given-names></name>, <name><surname>Maji</surname><given-names>SK</given-names></name>. <article-title>Attention-based noise prior network for magnetic resonance image denoising. In: 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI). IEEE</article-title>; <year>2022</year>. </mixed-citation></ref><ref id="pone.0318992.ref020"><label>20</label><mixed-citation publication-type="journal">Ran M, Hu J, Chen Y, Chen H, Sun H, Zhou J, et al.. Denoising of 3-D magnetic resonance images using a residual encoder-decoder wasserstein generative adversarial network. Med Image Anal 2019;55:165&#x02013;80.</mixed-citation></ref><ref id="pone.0318992.ref021"><label>21</label><mixed-citation publication-type="journal">Wu L, Hu S, Liu C. Denoising of 3D brain MR images with parallel residual learning of convolutional neural network using global and local feature extraction. Comput Intell Neurosci. 2021;2021(1):1&#x02013; 18.</mixed-citation></ref><ref id="pone.0318992.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Shou</surname><given-names>Q</given-names></name>, <name><surname>Zhao</surname><given-names>C</given-names></name>, <name><surname>Shao</surname><given-names>X</given-names></name>, <name><surname>Jann</surname><given-names>K</given-names></name>, <name><surname>Kim</surname><given-names>H</given-names></name>, <name><surname>Helmer</surname><given-names>KG</given-names></name>, <name><surname>Lu</surname><given-names>Hanzhang</given-names></name>, <name><surname>Wang Danny J</surname><given-names>J</given-names></name>. <article-title>Transformer-based deep learning denoising of single and multi-delay 3D arterial spin labeling</article-title>. <source>Magnet Resonance Med</source>
<year>2024</year>;<volume>91</volume>(<issue>2</issue>):<fpage>803</fpage>&#x02013;<lpage>18</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/mrm.29887</pub-id>
<pub-id pub-id-type="pmid">37849048</pub-id>
</mixed-citation></ref><ref id="pone.0318992.ref023"><label>23</label><mixed-citation publication-type="journal">Bruna J, Sprechmann P, LeCun Y. Super-resolution with deep convolutional sufficient statistics. arXiv preprint <ext-link xlink:href="https://arxiv.org/abs/1511.05666" ext-link-type="uri">https://arxiv.org/abs/1511.05666</ext-link></mixed-citation></ref><ref id="pone.0318992.ref024"><label>24</label><mixed-citation publication-type="book"><name><surname>Johnson</surname><given-names>J</given-names></name>, <name><surname>Alahi</surname><given-names>A</given-names></name>, <name><surname>Fei-Fei</surname><given-names>L</given-names></name>. <article-title>Perceptual losses for real-time style transfer and super-resolution. In: European conference on computer vision. Springer</article-title>; <year>2016</year>. </mixed-citation></ref><ref id="pone.0318992.ref025"><label>25</label><mixed-citation publication-type="journal">Dosovitskiy A, Brox T. Generating images with perceptual similarity metrics based on deep networks. In: Lee D, Sugiyama M, Luxburg U, Guyon I, Garnett R, editors. Advances in neural information processing systems, vol. 29.</mixed-citation></ref><ref id="pone.0318992.ref026"><label>26</label><mixed-citation publication-type="journal">Nguyen A, Yosinski J, Bengio Y, Dosovitskiy A, Clune J. Plug &#x00026; play generative networks: conditional iterative generation of images in latent space. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. p. 4467&#x02013;77.</mixed-citation></ref><ref id="pone.0318992.ref027"><label>27</label><mixed-citation publication-type="journal">Nguyen A, Dosovitskiy A, Yosinski J, Brox T, Clune J. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. In: Advances in Neural Information Processing Systems (NIPS); 2016.</mixed-citation></ref><ref id="pone.0318992.ref028"><label>28</label><mixed-citation publication-type="journal">Ledig C, Theis L, Huszar F, Caballero J, Cunningham A, Acosta A, et al. Photo-realistic single image super-resolution using a generative adversarial network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; 2017; p. 4681&#x02013;90.</mixed-citation></ref><ref id="pone.0318992.ref029"><label>29</label><mixed-citation publication-type="journal">Nasser SA, Shamsi S, Bundele V, Garg B, Sethi A. Perceptual cGAN for MRI super-resolution. arXiv preprint 2022. <ext-link xlink:href="https://arxiv.org/abs/2201.09314" ext-link-type="uri">https://arxiv.org/abs/2201.09314</ext-link></mixed-citation></ref><ref id="pone.0318992.ref030"><label>30</label><mixed-citation publication-type="journal">Gatys LA, Ecker AS, Bethge M. Image style transfer using convolutional neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2016.</mixed-citation></ref><ref id="pone.0318992.ref031"><label>31</label><mixed-citation publication-type="book"><name><surname>Shah</surname><given-names>ZH</given-names></name>, <name><surname>M&#x000fc;ller</surname><given-names>M</given-names></name>, <name><surname>Hammer</surname><given-names>B</given-names></name>, <name><surname>Huser</surname><given-names>T</given-names></name>, <name><surname>Schenck</surname><given-names>W</given-names></name>. <article-title>Impact of different loss functions on denoising of microscopic images. In: 2022 International Joint Conference on Neural Networks (IJCNN). IEEE</article-title>; <year>2022</year>. </mixed-citation></ref><ref id="pone.0318992.ref032"><label>32</label><mixed-citation publication-type="journal">Liu Y, Chen H, Chen Y, Yin W, Shen C. Generic perceptual loss for modeling structured output dependencies. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2021; p. 5424&#x02013;32.</mixed-citation></ref><ref id="pone.0318992.ref033"><label>33</label><mixed-citation publication-type="journal">He K, Wang Y, Hopcroft J. A powerful generative model using random weights for the deep image representation. Advances in neural information processing systems. 2016;29.</mixed-citation></ref><ref id="pone.0318992.ref034"><label>34</label><mixed-citation publication-type="journal">Manjon JV, Coupe P. MRI denoising using deep learning and non-local averaging; arXiv preprint 2019. <ext-link xlink:href="https://arxiv.org/abs/1911.04798" ext-link-type="uri">https://arxiv.org/abs/1911.04798</ext-link></mixed-citation></ref><ref id="pone.0318992.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>M</given-names></name>, <name><surname>Yun</surname><given-names>J</given-names></name>, <name><surname>Liu</surname><given-names>D</given-names></name>, <name><surname>Jiang</surname><given-names>D</given-names></name>, <name><surname>Xiong</surname><given-names>H</given-names></name>, <name><surname>Jiang</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Global and local feature extraction based on convolutional neural network residual learning for MR image denoising</article-title>. <source>Phys Med Biol.</source>
<year>2024</year>:<volume>69</volume>(<issue>20</issue>);<fpage>205007</fpage>.</mixed-citation></ref><ref id="pone.0318992.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Gurrola-Ramos</surname><given-names>J</given-names></name>, <name><surname>Alarcon</surname><given-names>T</given-names></name>, <name><surname>Dalmau</surname><given-names>O</given-names></name>, <name><surname>Manj&#x000f3;n</surname><given-names>JV</given-names></name>, <name><surname>Manjon</surname><given-names>JV</given-names></name>. <article-title>MRI Rician noise reduction using recurrent convolutional neural networks</article-title>. <source>IEEE Access</source>. <year>2024</year>;<volume>12</volume>:<fpage>128272</fpage>&#x02013;<lpage>84</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/access.2024.3446791</pub-id></mixed-citation></ref><ref id="pone.0318992.ref037"><label>37</label><mixed-citation publication-type="book"><name><surname>Lee</surname><given-names>JH</given-names></name>, <name><surname>Nam</surname><given-names>Y</given-names></name>, <name><surname>Kim</surname><given-names>DH</given-names></name>, <name><surname>Ryu</surname><given-names>K</given-names></name>. <article-title>Diffusion probabilistic models-based noise reduction for enhancing the quality of medical images. In: 2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN). IEEE</article-title>; <year>2023</year>. </mixed-citation></ref><ref id="pone.0318992.ref038"><label>38</label><mixed-citation publication-type="book"><name><surname>Fiasam</surname><given-names>LD</given-names></name>, <name><surname>Rao</surname><given-names>Y</given-names></name>, <name><surname>Sey</surname><given-names>C</given-names></name>, <name><surname>Klugah-Brown</surname><given-names>B</given-names></name>, <name><surname>Tettey</surname><given-names>ON</given-names></name>, <name><surname>Aggrey</surname><given-names>ESE</given-names></name>, <etal>et al</etal>. <article-title>SCAF-DG: a multi-site medical image denoising with a domain-generalized spatial-channel attention fusion. In: 2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI). IEEE</article-title>; <year>2023</year>. </mixed-citation></ref><ref id="pone.0318992.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>You</surname><given-names>X</given-names></name>, <name><surname>Cao</surname><given-names>N</given-names></name>, <name><surname>Lu</surname><given-names>H</given-names></name>, <name><surname>Mao</surname><given-names>M</given-names></name>, <name><surname>Wanga</surname><given-names>W</given-names></name>. <article-title>Denoising of MR images with Rician noise using a wider neural network and noise range division</article-title>. <source>Magnet Resonance Imaging</source>. <year>2019</year>;<volume>64</volume>:<fpage>154</fpage>&#x02013;<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.mri.2019.05.042</pub-id>
<pub-id pub-id-type="pmid">31220567</pub-id>
</mixed-citation></ref><ref id="pone.0318992.ref040"><label>40</label><mixed-citation publication-type="journal"><name><surname>K&#x000fc;stner</surname><given-names>T</given-names></name>, <name><surname>Munoz</surname><given-names>C</given-names></name>, <name><surname>Psenicny</surname><given-names>A</given-names></name>, <name><surname>Bustin</surname><given-names>A</given-names></name>, <name><surname>Fuin</surname><given-names>N</given-names></name>, <name><surname>Qi</surname><given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Deep-learning based super-resolution for 3D isotropic coronary MR angiography in less than a minute</article-title>. <source>Magnet Resonance Med.</source>
<year>2021</year>;<volume>86</volume>(<issue>5</issue>):<fpage>2837</fpage>&#x02013;<lpage>52</lpage>.</mixed-citation></ref><ref id="pone.0318992.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Wicaksono</surname><given-names>KP</given-names></name>, <name><surname>Fujimoto</surname><given-names>K</given-names></name>, <name><surname>Fushimi</surname><given-names>Y</given-names></name>, <name><surname>Sakata</surname><given-names>A</given-names></name>, <name><surname>Okuchi</surname><given-names>S</given-names></name>, <name><surname>Hinoda</surname><given-names>T</given-names></name>, <etal>et al</etal>. <article-title>Super-resolution application of generative adversarial network on brain time-of-flight MR angiography: image quality and diagnostic utility evaluation</article-title>. <source>Eur Radiol</source>
<year>2023</year>;<volume>33</volume>(<issue>2</issue>):<fpage>936</fpage>&#x02013;<lpage>46</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s00330-022-09103-9</pub-id>
<pub-id pub-id-type="pmid">36006430</pub-id>
</mixed-citation></ref><ref id="pone.0318992.ref042"><label>42</label><mixed-citation publication-type="journal">Zhang K, Zuo W, Chen Y, Meng D, Zhang L. Beyond a Gaussian denoiser: residual learning of deep CNN for image denoising. IEEE Trans Image Process Publ IEEE Signal Process Soc. 2017;26(7):3142&#x02013; 55. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TIP.2017.2662206</pub-id>
<pub-id pub-id-type="pmid">28166495</pub-id>
</mixed-citation></ref><ref id="pone.0318992.ref043"><label>43</label><mixed-citation publication-type="journal">Feng CM, Yan Y, Chen G, Xu Y, Hu Y, Shao L, et al. Multi-modal transformer for accelerated MR imaging. IEEE Trans Med Imaging 2022;42(10):2804-16. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tmi.2022.3180228</pub-id></mixed-citation></ref><ref id="pone.0318992.ref044"><label>44</label><mixed-citation publication-type="journal">Ahn S, Wollner U, McKinnon G, Jansen IH, Brada R, Rettmann D, et al. Deep learning-based reconstruction of highly accelerated 3D MRI. arXiv preprint 2022. <ext-link xlink:href="https://arxiv.org/abs/2203.04674" ext-link-type="uri">https://arxiv.org/abs/2203.04674</ext-link></mixed-citation></ref><ref id="pone.0318992.ref045"><label>45</label><mixed-citation publication-type="journal"><name><surname>Jia</surname><given-names>L</given-names></name>, <name><surname>He</surname><given-names>X</given-names></name>, <name><surname>Huang</surname><given-names>A</given-names></name>, <name><surname>Jia</surname><given-names>B</given-names></name>, <name><surname>Xinfeng</surname><given-names>W</given-names></name>. <article-title>Highly efficient encoder-decoder network based on multi-scale edge enhancement and dense connectivity for LDCT image denoising</article-title>. <source>Signal, image and video processing</source>
<year>2024</year>;<fpage>1</fpage>&#x02013;<lpage>11</lpage>.</mixed-citation></ref><ref id="pone.0318992.ref046"><label>46</label><mixed-citation publication-type="journal"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <article-title>Delving deep into rectifiers: surpassing human-level performance on imagenet classification</article-title>. <source>In: Proceedings of the IEEE international conference on computer vision;</source>
<year>2025</year>;<fpage>1026</fpage>&#x02013;<lpage>34</lpage>.</mixed-citation></ref><ref id="pone.0318992.ref047"><label>47</label><mixed-citation publication-type="journal">Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. In: Teh YW, Titterington M, editors. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. vol. 9. Proceedings of Machine Learning Research. Chia Laguna Resort, Sardinia, Italy: PMLR; 2010; p. 249&#x02013;56.</mixed-citation></ref><ref id="pone.0318992.ref048"><label>48</label><mixed-citation publication-type="journal">Zhang R. Making convolutional networks shift-invariant again. In: International conference on machine learning; PMLR. p. 7324&#x02013;34.</mixed-citation></ref><ref id="pone.0318992.ref049"><label>49</label><mixed-citation publication-type="journal">Kingma DP, Ba J. Adam: a method for stochastic optimization. arXiv preprint <ext-link xlink:href="https://arxiv.org/abs/1412.6980" ext-link-type="uri">https://arxiv.org/abs/1412.6980</ext-link></mixed-citation></ref><ref id="pone.0318992.ref050"><label>50</label><mixed-citation publication-type="journal">Chen Q, Koltun V. Photographic image synthesis with cascaded refinement networks. In: Proceedings of the IEEE International Conference on Computer Vision (ICCV); 2017.</mixed-citation></ref><ref id="pone.0318992.ref051"><label>51</label><mixed-citation publication-type="journal"><name><surname>Gurrola-Ramos</surname><given-names>J</given-names></name>, <name><surname>Dalmau</surname><given-names>O</given-names></name>, <name><surname>Alarcon</surname><given-names>TE</given-names></name>. <article-title>A residual dense u-net neural network for image denoising</article-title>. <source>IEEE Access</source>. <year>2021</year>;<volume>9</volume>:<fpage>1742</fpage>&#x02013;<lpage>54</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/access.2021.3061062</pub-id></mixed-citation></ref><ref id="pone.0318992.ref052"><label>52</label><mixed-citation publication-type="journal">Chen H, Yang G, Zhang H. Hider: A hyperspectral image denoising transformer with spatial&#x02013;spectral constraints for hybrid noise removal. IEEE Trans Neural Netw Learn Syst. 2022.</mixed-citation></ref><ref id="pone.0318992.ref053"><label>53</label><mixed-citation publication-type="journal">Xue Z, Yang F, Gao J, Chen Z, Peng H, Zou C, et al. DARCS: memory-efficient deep compressed sensing reconstruction for acceleration of 3D whole-heart coronary MR angiography. arXiv preprint 2024. <ext-link xlink:href="https://arxiv.org/abs/2402.00320" ext-link-type="uri">https://arxiv.org/abs/2402.00320</ext-link></mixed-citation></ref><ref id="pone.0318992.ref054"><label>54</label><mixed-citation publication-type="journal">Bai C, Han X. MRFormer: Multiscale retractable transformer for medical image progressive denoising via noise level estimation. Image Vision Comput. 2024;144:104974.</mixed-citation></ref><ref id="pone.0318992.ref055"><label>55</label><mixed-citation publication-type="journal">Amir D, Weiss Y. Understanding and simplifying perceptual distances. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2021; p. 12226&#x02013;35.</mixed-citation></ref><ref id="pone.0318992.ref056"><label>56</label><mixed-citation publication-type="journal">Thornig P. JURECA: Data centric and booster modules implementing the modular supercomputing architecture at J&#x000fc;lich supercomputing Centre. J Large-Scale Res Facilities JLSRF. 2021;7:A182.</mixed-citation></ref></ref-list></back><sub-article article-type="author-comment" id="pone.0318992.r001" specific-use="rebutted-decision-letter-unavailable"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0318992.r001</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">20 Jun 2024</named-content>
</p></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0318992.r002" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0318992.r002</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Khan</surname><given-names>Khan Bahadar</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Khan Bahadar Khan</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Khan Bahadar Khan</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0318992" id="rel-obj002" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">9 Sep 2024</named-content>
</p><p>PONE-D-24-26390</p><p>Untrained Perceptual Loss for image denoising of line-like structures in MR images</p><p>PLOS ONE</p><p>Dear Dr. Pfaehler,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by Oct 24 2024 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to https://www.editorialmanager.com/pone/ and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:</p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Khan Bahadar Khan, Ph.D</p><p>Academic Editor</p><p>PLOS ONE</p><p>Journal Requirements:</p><p>When submitting your revision, we need you to address these additional requirements.</p><p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at&#x000a0;</p><p>https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf and&#x000a0;</p><p>https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</p><p>2. Thank you for stating the following financial disclosure:&#x000a0;</p><p>"This work was supported by the Networking Funds of the Helmholtz Association of German Research Centres 276 HighLine ZT-I-PF-4-042)."</p><p>Please state what role the funders took in the study. If the funders had no role, please state: "The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."</p><p>If this statement is not correct you must amend it as needed.&#x000a0;</p><p>Please include this amended Role of Funder statement in your cover letter; we will change the online submission form on your behalf.</p><p>3. Please note that funding information should not appear in the Acknowledgments section or other areas of your manuscript. We will only publish funding information present in the Funding Statement section of the online submission form. Please remove any funding-related text from the manuscript.&#x000a0;</p><p>4. We note that you have indicated that there are restrictions to data sharing for this study. For studies involving human research participant data or other sensitive data, we encourage authors to share de-identified or anonymized data. However, when data cannot be publicly shared for ethical reasons, we allow authors to make their data sets available upon request. For information on unacceptable data access restrictions, please see http://journals.plos.org/plosone/s/data-availability#loc-unacceptable-data-access-restrictions.&#x000a0;</p><p>Before we proceed with your manuscript, please address the following prompts:</p><p>a) If there are ethical or legal restrictions on sharing a de-identified data set, please explain them in detail (e.g., data contain potentially identifying or sensitive patient information, data are owned by a third-party organization, etc.) and who has imposed them (e.g., a Research Ethics Committee or Institutional Review Board, etc.). Please also provide contact information for a data access committee, ethics committee, or other institutional body to which data requests may be sent.</p><p>b) If there are no restrictions, please upload the minimal anonymized data set necessary to replicate your study findings to a stable, public repository and provide us with the relevant URLs, DOIs, or accession numbers. Please see http://www.bmj.com/content/340/bmj.c181.long for guidelines on how to de-identify and prepare clinical data for publication. For a list of recommended repositories, please see https://journals.plos.org/plosone/s/recommended-repositories. You also have the option of uploading the data as Supporting Information files, but we would recommend depositing data directly to a data repository if possible.</p><p>Please update your Data Availability statement in the submission form accordingly.</p><p>5. When completing the data availability statement of the submission form, you indicated that you will make your data available on acceptance. We strongly recommend all authors decide on a data sharing plan before acceptance, as the process can be lengthy and hold up publication timelines. Please note that, though access restrictions are acceptable now, your entire data will need to be made freely accessible if your manuscript is accepted for publication. This policy applies to all data except where public deposition would breach compliance with the protocol approved by your research ethics board. If you are unable to adhere to our open data policy, please kindly revise your statement to explain your reasoning and we will seek the editor's input on an exemption. Please be assured that, once you have provided your new statement, the assessment of your exemption will not hold up the peer review process.</p><p>6. Please ensure that you refer to Figure 1 in your text as, if accepted, production will need this reference to link the reader to the figure.</p><p>7. We notice that your supplementary figures and tables are included in the manuscript file. Please remove them and upload them with the file type 'Supporting Information'. Please ensure that each Supporting Information file has a legend listed in the manuscript after the references list.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p>2. Has the statistical analysis been performed appropriately and rigorously? </p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>**********</p><p>5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p><p>Reviewer #1:&#x000a0;This study investigated the effect of using a perceptual loss function with an untrained network on 3D MRI denoising.</p><p>The authors tested various untrained networks with different initializations and structures, including depth, kernel size, and pooling operations. Since obtaining a trained network to calculate perceptual loss is often challenging, the use of a perceptual loss function with an untrained network appears to have significant utility. The paper is clearly written, and appropriate results are presented to support each claim. However, further analysis of the untrained perceptual loss is needed in addition to the evaluation results. There are also a few additional contents that should be included in the paper.</p><p>1. The evaluation metrics used in this paper are predominantly influenced by large structures in the image. However, the regions that need to be restored through denoising are the fine structures. Therefore, the evaluation results for the fine structure areas (for example, the left half of the image in Fig. 2) should be presented alongside the evaluation results for the entire image.</p><p>2. Line 254: uPL takes neighborhood information into account when calculating loss. What is the basis for this claim?</p><p>3. It is thought that the effectiveness of perceptual loss arises because the trained network extracts meaningful feature maps. There have been papers that trained a network specifically to extract meaningful features for the primary task, and these papers indeed improved the performance of the primary task. Therefore, a discussion is needed on why an untrained network is also effective.</p><p>Reviewer #2:&#x000a0;The manuscript describes a new deep learning approach to the denoising of magnetic resonance (MR) images, especially those depicting fine fiber structures. The core idea is to use an &#x0201c;untrained perceptual loss&#x0201d; (uPL) to drive the training of the deep neural networks (DNN) intended for denoising. This approach is not new, but here applied and benchmarked systematically for 3D MR image stacks for the first time. The results show that the uPL loss is actually superior to other commonly used loss functions in this domain, at least for the chosen datasets and denoising network architectures.</p><p>===============================================</p><p>Overall assessment</p><p>===============================================</p><p>The scientific idea and results are definitely worth publishing because they provide some new unique insights. However, the writing needs some improvement (more on this in the next paragraph), and the presented experiments seem to be carried out only once for every task condition. Thus, the results which we find in the tables are derived from a single training run of a single network. We all know that DNN training is a stochastic process where the outcome can vary from training run to training run. Therefore, I strongly recommend to carry out multiple training runs per task condition and to present the mean outcome in the tables (incl. standard deviation).</p><p>Even with the current data, one should report for every value in the result tables the standard deviations related to the variability in the test set and the number of test samples. In this way it would be possible to estimate if any of the observed differences are significant in a statistical sense. Even better would be proper statistical testing by the authors to convince the reader that the reported differences in the various performance measures are meaningful at all. For now, in my current assessment of the paper, I *assume* that this is the case for most reported differences in SSIM, PSNR, and MSE values.</p><p>The abstract and introduction section are written very well. For the first paragraph of the introduction, the following reference may be a good addition: ZH Shah, M M&#x000fc;ller, B Hammer, T Huser, and W Schenck. Impact of different loss functions on denoising of microscopic images. In 2022 International Joint Conference on Neural Networks (IJCNN), 2022.</p><p>(in this paper, various loss functions for denoising are compared with each other, among them a version of the perceptual loss; to include this reference is just a suggestion because it seems to be a good fit, but it is not a must)</p><p>The section on &#x0201c;Materials and Methods&#x0201d; lacks some clarity, however. Starting at the subsection &#x0201c;Conventional loss functions&#x0201d;, the reader gets easily confused how the different uPL variations are related to each other. First, VGG19, AlexNet and a simple CNN are mentioned. It is not explicitly written that only the simple CNN serves as starting point for the variations of weight initialization, depth, and number of pooling layers. It is also not clear from the writing how the latter three variations interact with each other.</p><p>Regarding the training details, the question is why 30.000 iterations were chosen. Is this the optimum number for the performance measures on the validation sets? No over- or underfitting in any of the experimental variations?</p><p>Also the learning rate of 0.001 needs justification.</p><p>The writing in the &#x0201c;Results&#x0201d; section should also be improved because it is partly confusing. First, I strongly recommend to check and improve grammar and sentence structure, second, to state in the beginning of each subsection what is the goal of the described experiment and what is the experimental configuration (because it does not always fully align with the corresponding part in the &#x0201c;Methods&#x0201d; section).</p><p>Also reg. the &#x0201c;Results&#x0201d; section, I recommend to add more visualizations of the denoising results. The existing figures and possible additional figures could be formatted in a nicer and more compact way.</p><p>The &#x0201c;Discussion&#x0201d; section is appropriate.</p><p>===============================================</p><p>Small remarks</p><p>===============================================</p><p>&#x02022; writing style not consistent reg. &#x0201c;2D&#x0201d; vs. &#x0201c;2d&#x0201d; and &#x0201c;3D&#x0201d; vs. &#x0201c;3d&#x0201d;</p><p>&#x02022; writing style not consistent reg. &#x0201c;transformer&#x0201d; vs. &#x0201c;Transformer&#x0201d;</p><p>&#x02022; p. 4, beginning: No figure for DuCNN in suppl. materials</p><p>&#x02022; p. 4, 3rd paragraph: More in-depth explanation about adaption of transformer architecture to 3D setting would be nice</p><p>&#x02022; p. 6, line 190-192: If uniform init. is worse than Xavier normal, why use default init. in &#x0201c;all other experiments&#x0201d;?</p><p>&#x02022; line 195: &#x0201c;slighlty&#x0201d; --&#x0003e; &#x0201c;slightly&#x0201d;</p><p>&#x02022; caption of table 3: &#x0201c;&#x02026; MRA (above) for&#x02026;&#x0201d; --&#x0003e; &#x0201c;&#x02026; MRA (above) and for&#x02026;&#x0201d;</p><p>&#x02022; line 201: &#x0201c;&#x02026;show the number&#x02026;&#x0201d; --&#x0003e; &#x0201c;&#x02026;show that the number&#x02026;&#x0201d;</p><p>&#x02022; line 204-208: Description of the results for MRA does not reflect the similarly good results for 7 conv. layers</p><p>&#x02022; caption of Fig. 3 reg. &#x0201c;difference images&#x0201d;: Difference to what?</p><p>&#x02022; line 226/227: I don&#x02019;t understand this sentence since the results in table 4 show consistently better results for uPL at the highest noise level with the roots dataset.</p><p>&#x02022; Caption of table 4: There are no other eval. metrics listed in the suppl. materials section reg. table 4 (btw, what is &#x0201c;Section 7&#x0201d;?)</p><p>&#x02022; Figures S2 and S3: Not very helpful for better understanding (esp. S2)</p><p>&#x02022; Figure S4: Subfigure (g) obviously missing</p><p>&#x02022; Caption of Table S2: Very confusing</p><p>&#x02022; Caption of Table S3: Replace &#x0201c;SSIM&#x0201d; with &#x0201c;PSNR/MSE&#x0201d;</p><p>**********</p><p>6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #2:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p></body></sub-article><sub-article article-type="author-comment" id="pone.0318992.r003"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0318992.r003</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0318992" id="rel-obj003" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">16 Oct 2024</named-content>
</p><p>We thank the reviewers for their valuable feedback which helped to increase the quality of the manuscript. We adapted the manuscript accordingly. Our detailed answers are listed below.</p><p>_______________________________________________________________________________________</p><p>Reviewer #1:</p><p>1. The evaluation metrics used in this paper are predominantly influenced by large structures in the image. However, the regions that need to be restored through denoising are the fine structures. Therefore, the evaluation results for the fine structure areas (for example, the left half of the image in Fig. 2) should be presented alongside the evaluation results for the entire image.</p><p>We thank the reviewer for this important point. We now also calculated the image quality metrics for the cropped region of the images. All these regions also contained vessels/roots (i.e. the important parts of the images), we calculated the image quality metrics also on these regions. To avoid confusion, we added the results to the supplemental information. We added the following sentence to the manuscript:</p><p>&#x0201a;As both datasets contain fine structures, the evaluation metrics were additionally calculated on the parts of the image containing important information. A cube of size 52 x 52 x 52 was cropped from the root data. As the roots grow from up to down, the cube was cropped from the upper middle part of the image. A cube of size 68 x 68 x 68 was cropped from the middle of the MRA images as these images contain important information mainly in the image center. An example illustration is displayed in Supplemental Figure S2 . These results are reported in the Supplemental Information.&#x02018;</p><p>We observed different, but consistent results when calculating evaluation metrics on the whole image and on image parts only (i.e. for the root dataset, 10% noise, and DnCNN network: L1 Loss SSIM: 0.73, uPL: SSIM 0.77 when calculated on image parts; fort he MRA dataset, 10% noise and DnCNN network: L1: SSIM 0.83, uPL: 0.87)</p><p>2. Line 254: uPL takes neighborhood information into account when calculating loss. What is the basis for this claim?</p><p>As the uPL compares feature maps that are acquired by applying convolutional kernels with a kernel size &#x0003e; 1, these feature maps also contain information about the voxel neighborhood. We added the following sentence to the manuscript for clarification: &#x0201a;This is probably because the uPL is taking neighborhood information into account by using convolutional kernels with sizes larger than 1 when calculating the loss.&#x02018;</p><p>3. It is thought that the effectiveness of perceptual loss arises because the trained network extracts meaningful feature maps. There have been papers that trained a network specifically to extract meaningful features for the primary task, and these papers indeed improved the performance of the primary task. Therefore, a discussion is needed on why an untrained network is also effective.</p><p>Thank you for this question. We added a part to the discussion responding this question. It reads: &#x0201a; The success of perceptual loss was thought to be due to the features networks learned during training. However, as previous studies demonstrated for 2D images, pre-trained and untrained networks used in the loss function lead to comparable results (Liu et al., Generic perceptual loss for modeling structured output dependencies, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition). This might be because randomly initialized weights can also represent the statistical properties of the training data. The pre-trained networks are usually trained on classification tasks. However, other image characteristics could be more important for image denoising of an entire image. E.g. the display of sharp edges can also be captured by convolutions and untrained feature maps. Additionally, the perceptual distance between two images is equivalent to the maximum mean discrepancy (MMD) distance between local distributions of small patches in the two images (Amir and Weiss, Understanding and Simplifying Perceptual Distances, CVPR 2021). As also demonstrated in this study, different network structures in the loss function have a high impact on the denoising results. These findings suggest that the design of the network architecture used in the loss function can be chosen such that it captures the most important image information. &#x02018;</p><p>Reviewer #2: The manuscript describes a new deep learning approach to the denoising of magnetic resonance (MR) images, especially those depicting fine fiber structures. The core idea is to use an &#x0201c;untrained perceptual loss&#x0201d; (uPL) to drive the training of the deep neural networks (DNN) intended for denoising. This approach is not new, but here applied and benchmarked systematically for 3D MR image stacks for the first time. The results show that the uPL loss is actually superior to other commonly used loss functions in this domain, at least for the chosen datasets and denoising network architectures.</p><p>===============================================</p><p>Overall assessment</p><p>===============================================</p><p>The scientific idea and results are definitely worth publishing because they provide some new unique insights. However, the writing needs some improvement (more on this in the next paragraph), and the presented experiments seem to be carried out only once for every task condition. Thus, the results which we find in the tables are derived from a single training run of a single network. We all know that DNN training is a stochastic process where the outcome can vary from training run to training run. Therefore, I strongly recommend to carry out multiple training runs per task condition and to present the mean outcome in the tables (incl. standard deviation).</p><p>We thank the reviewer for pointing this out. We now ran the networks for each condition five times (with five different random seeds). We report the mean and std values across random seeds in the Tables. We added the following sentence to the Materials and Methods section: &#x0201a;All experiments were performed with five different random seeds.&#x02018;</p><p>Even with the current data, one should report for every value in the result tables the standard deviations related to the variability in the test set and the number of test samples. In this way it would be possible to estimate if any of the observed differences are significant in a statistical sense. Even better would be proper statistical testing by the authors to convince the reader that the reported differences in the various performance measures are meaningful at all. For now, in my current assessment of the paper, I *assume* that this is the case for most reported differences in SSIM, PSNR, and MSE values.</p><p>We thank the reviewer for pointing this out; We added the standard deviation across random seeds to the results. The results for one random seed including the standard deviation in the testset is separately displayed in the supplemental material. However, we hope and believe with the application of five random seeds and the additional evaluation on image parts only (requested by Reviewer 1), our results are convincing.</p><p>The abstract and introduction section are written very well. For the first paragraph of the introduction, the following reference may be a good addition: ZH Shah, M M&#x000fc;ller, B Hammer, T Huser, and W Schenck. Impact of different loss functions on denoising of microscopic images. In 2022 International Joint Conference on Neural Networks (IJCNN), 2022.</p><p>(in this paper, various loss functions for denoising are compared with each other, among them a version of the perceptual loss; to include this reference is just a suggestion because it seems to be a good fit, but it is not a must)</p><p>We thank the reviewer for mentioning this paper which is indeed a very good fit. We added it to the introduction section: &#x0201a;The Perceptual Loss is also often used in combination with L1- or L2-loss where it also leads to performance improvements &#x02018;</p><p>The section on &#x0201c;Materials and Methods&#x0201d; lacks some clarity, however. Starting at the subsection &#x0201c;Conventional loss functions&#x0201d;, the reader gets easily confused how the different uPL variations are related to each other. First, VGG19, AlexNet and a simple CNN are mentioned. It is not explicitly written that only the simple CNN serves as starting point for the variations of weight initialization, depth, and number of pooling layers. It is also not clear from the writing how the latter three variations interact with each other.</p><p>We adapted &#x0201a;Materials and Methods&#x02018; section accordingly. We changed the order of the subsections: We now first introduce the untrained perceptual loss and then introduce a simple &#x0201a;proof of concept&#x02018; study where we compare the performance of a simple uPL network with the performance of other networks. We then elaborate on the impact of different uPL network characteristics on the results.</p><p>Regarding the training details, the question is why 30.000 iterations were chosen. Is this the optimum number for the performance measures on the validation sets? No over- or underfitting in any of the experimental variations?</p><p>Also the learning rate of 0.001 needs justification.</p><p>We experimented with different learning rates and numbers of iterations. The chosen settings led to the overall optimal performance in the validation sets. As performance was similar in the training set, we assume no over- or underfitting occured for this numbers. We added the following part to the manuscript:</p><p>&#x0201a;The training parameters were chosen as they lead to the overall best performance in the validation set.&#x02018;</p><p>The writing in the &#x0201c;Results&#x0201d; section should also be improved because it is partly confusing. First, I strongly recommend to check and improve grammar and sentence structure, second, to state in the beginning of each subsection what is the goal of the described experiment and what is the experimental configuration (because it does not always fully align with the corresponding part in the &#x0201c;Methods&#x0201d; section).</p><p>We thank the reviewer for this important point. We changed the wording and structure of this section accordingly. We made some small changes in each part and added a sentence describing the aim of each section.</p><p>Also reg. the &#x0201c;Results&#x0201d; section, I recommend to add more visualizations of the denoising results. The existing figures and possible additional figures could be formatted in a nicer and more compact way.</p><p>We changed the Figures and added some results.</p><p>The &#x0201c;Discussion&#x0201d; section is appropriate.</p><p>===============================================</p><p>Small remarks</p><p>===============================================</p><p>&#x02022; writing style not consistent reg. &#x0201c;2D&#x0201d; vs. &#x0201c;2d&#x0201d; and &#x0201c;3D&#x0201d; vs. &#x0201c;3d&#x0201d;</p><p>Thank you for pointing this out. We changed all namings now to 2D and 3D.</p><p>&#x02022; writing style not consistent reg. &#x0201c;transformer&#x0201d; vs. &#x0201c;Transformer&#x0201d;</p><p>Done</p><p>&#x02022; p. 4, beginning: No figure for DuCNN in suppl. Materials</p><p>Done</p><p>&#x02022; p. 4, 3rd paragraph: More in-depth explanation about adaption of transformer architecture to 3D setting would be nice</p><p>We added a more detailed explanation. Additionally, we changed the Supplemental Figure and added more details. The section reads now:</p><p>The Transformer network is specially designed such that it can be used for large images while modeling global connectivity. In this network, multi-head &#x02019;transposed&#x02019; attention (MDTA) blocks are introduced applying attention across feature dimensions rather than across spatial dimensions. Before feeding the data to the MDTA blocks, the images are resized to the size (image height &#x000b7; image width) &#x000d7; number of channels. After the MDTA blocks follow feed-forward blocks consisting of two convolutional layers and a gating layer. We adjust the transposition in the attention blocks to the 3D case. In the 3D Transformer network, all 2D convolutions are replaced by 3D convolutions. Before the data is fed to the MDTA blocks, the data is resized to the size (image height&#x000b7; image width &#x000b7; image depth) &#x000d7; number of channels. As for our fine structures, UNet-like architectures lead to performance drops, the Transformer blocks were in our study combined sequentially. A graphical overview of all network structures is displayed in the supplemental material (S1 Fig, S2 Fig, S3 Fig).</p><p>&#x02022; p. 6, line 190-192: If uniform init. is worse than Xavier normal, why use default init. in &#x0201c;all other experiments&#x0201d;?</p><p>We apologize, this was a mistake. We trained all networks with xavier normal initialisation. We changed the sentence in the manuscript accordingly.</p><p>&#x02022; line 195: &#x0201c;slighlty&#x0201d; --&#x0003e; &#x0201c;slightly&#x0201d;</p><p>Done.</p><p>&#x02022; caption of table 3: &#x0201c;&#x02026; MRA (above) for&#x02026;&#x0201d; --&#x0003e; &#x0201c;&#x02026; MRA (above) and for&#x02026;&#x0201d;</p><p>Done</p><p>&#x02022; line 201: &#x0201c;&#x02026;show the number&#x02026;&#x0201d; --&#x0003e; &#x0201c;&#x02026;show that the number&#x02026;&#x0201d;</p><p>Done</p><p>&#x02022; line 204-208: Description of the results for MRA does not reflect the similarly good results for 7 conv. Layers</p><p>Done</p><p>&#x02022; caption of Fig. 3 reg. &#x0201c;difference images&#x0201d;: Difference to what?</p><p>TODO</p><p>&#x02022; line 226/227: I don&#x02019;t understand this sentence since the results in table 4 show consistently better results for uPL at the highest noise level with the roots dataset.</p><p>True. We changed the sentence accordingly to: For the MRA data, the superiority in performance was not visible for the lowest and highest noise level and the ResNet, and 10 % added noise and the Transformer network.</p><p>&#x02022; Caption of table 4: There are no other eval. metrics listed in the suppl. materials section reg. table 4 (btw, what is &#x0201c;Section 7&#x0201d;?)</p><p>Apologize for this mistake. We added the tables to the Supplemental information.</p><p>&#x02022; Figures S2 and S3: Not very helpful for better understanding (esp. S2)</p><p>We changed the corresponding figures accordingly.</p><p>&#x02022; Figure S4: Subfigure (g) obviously missing</p><p>Done</p><p>&#x02022; Caption of Table S2: Very confusing</p><p>We changed the caption to: MSE for different kernel sizes and network depth: MSE calculated only for roots regions (above) and MSE for the whole image also for the MR root dataset (below)</p><p>&#x02022; Caption of Table S3: Replace &#x0201c;SSIM&#x0201d; with &#x0201c;PSNR/MSE&#x0201d;</p><p>Done.</p><p>________________________________________</p><supplementary-material id="pone.0318992.s015" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">AnswerToReviewers_PLOS.docx</named-content></p></caption><media xlink:href="pone.0318992.s015.docx"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0318992.r004" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0318992.r004</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Khan</surname><given-names>Khan Bahadar</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Khan Bahadar Khan</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Khan Bahadar Khan</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0318992" id="rel-obj004" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">27 Nov 2024</named-content>
</p><p>PONE-D-24-26390R1</p><p>Untrained Perceptual Loss for image denoising of line-like structures in MR images</p><p>PLOS ONE</p><p>Dear Dr. Pfaehler,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by Jan 11 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to https://www.editorialmanager.com/pone/ and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:</p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Khan Bahadar Khan, Ph.D</p><p>Academic Editor</p><p>PLOS ONE</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.</p><p>Reviewer #1:&#x000a0;All comments have been addressed</p><p>Reviewer #3:&#x000a0;(No Response)</p><p>Reviewer #4:&#x000a0;(No Response)</p><p>**********</p><p>2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;(No Response)</p><p>**********</p><p>3. Has the statistical analysis been performed appropriately and rigorously? </p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;N/A</p><p>Reviewer #4:&#x000a0;(No Response)</p><p>**********</p><p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;(No Response)</p><p>**********</p><p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;(No Response)</p><p>**********</p><p>6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p><p>Reviewer #1:&#x000a0;This study investigated the effect of using a perceptual loss function with an untrained network on 3D MRI denoising.</p><p>The authors tested various untrained networks with different initializations and structures, including depth, kernel size, and pooling operations. Since obtaining a trained network to calculate perceptual loss is often challenging, the use of a perceptual loss function with an untrained network appears to have significant utility. The paper is clearly written, and appropriate results are presented to support each claim.</p><p>The authors have satisfactorily addressed my comments.</p><p>Reviewer #3:&#x000a0;This is an interesting paper describing the use of uPL in conjunction with deep neural networks for denoising MR images to resolve fine structures. The paper is well written and provides useful information on the effectiveness of the presented technique.</p><p>General:</p><p>I think the figure and table captions need revision. They are not very clear and are missing information.</p><p>The discussion could be revised. It is a bit anemic. I would include a brief discussion of the limitations of your approach. I would also consider mentioning the MRA results, since you mention the root results.</p><p>What is the practical application implication of these results? You mention reduced imaging time, but to what extent would this allow that? You also mention the ability to use less computationally expensive denoising, but there is not an investigation of comparative performance in that regard, so it feels out-of-context. What specific advantage is gained from using this approach?</p><p>Comments:</p><p>Lines 70-72 &#x0201c;For both datasets, four levels of Rician noise were artificially added to assess the impact of the loss functions for different signal-to-noise ratios (1%, 5%, 10%, and 20% noise added)&#x0201d;</p><p>I would be interested to see images with the various noise levels. It may be helpful for contextualizing the quality of denoising. I do not see these in your figures containing the GT images and denoised images.</p><p>Line 181 &#x0201c;The training parameters were chosen as they lead to the overall best performance in the validation sets&#x0201d;</p><p>Could you expand a bit on what you mean by validation sets? What exactly did you do to determine that these were the best parameters? I don&#x02019;t think you need a lot of extra detail here, but it would be nice to understand how you came to these training parameters, since these are fairly important for the overall performance of the network and since you are investigating other parameters in depth.</p><p>Lines 190-192 "A cube of size 52 &#x000d7; 52 &#x000d7; 52 was cropped from the root data. 190 As the roots grow from top to bottom, the cube is cropped from the upper middle part of the image. A cube of size 68 &#x000d7; 68 &#x000d7; 68 is cropped from the middle of the MRA"</p><p>Is there a reason for these cube sizes? What does middle and upper middle mean in this context? Where all sample cubes taken from the same coordinates?</p><p>Figures 2, 3, 4</p><p>I think these figures could use better captions &#x02013; they are missing some key info. For example, in Fig 2, you do not mention the noise level, or what exactly is being displayed in the caption (but then describe it in text). This should be moved to the caption for easier reference. The same holds true for figures 2 &#x00026; 3. I see that this info is included elsewhere, but as a reader, it is difficult to keep track of what is being displayed.</p><p>Figure 4</p><p>There are arrows (presumably pointing to features of interest) but they are not explained. Why have you chosen to highlight these points?</p><p>Tables</p><p>Why are some values underlined? It would be helpful to provide a legend/explanation in the caption for this. In general, I think the table and figure captions could be revised for clarity and content.</p><p>Line 253 &#x0201c;Denoising network architecture vs. loss function for different noise levels&#x0201d;</p><p>Why only use L1 here as a comparison?</p><p>Lines 328-334</p><p>This section of text feels somewhat out of place.</p><p>Lines 335-336 &#x0201c;However, our results demonstrate the benefit of the untrained Perceptual Loss for both 3D datasets and all noise levels.&#x0201d;</p><p>This final sentence should be reworked a bit. It doesn&#x02019;t flow particularly well with the prior paragraph, and I think that a final statement/claim like this should be given more attention and justification.</p><p>Reviewer #4:&#x000a0;(No Response)</p><p>**********</p><p>7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #3:&#x000a0;No</p><p>Reviewer #4:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p><supplementary-material id="pone.0318992.s016" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Reviewer Attachment.docx</named-content></p></caption><media xlink:href="pone.0318992.s016.docx"/></supplementary-material></body></sub-article><sub-article article-type="author-comment" id="pone.0318992.r005"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0318992.r005</article-id><title-group><article-title>Author response to Decision Letter 2</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0318992" id="rel-obj005" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">16 Jan 2025</named-content>
</p><p>We thank the reviewers for their valuable time and feedback which helped to improve the manuscript. Detailed answers are given below.</p><p>_________________________________________________________</p><p>Reviewer 3:</p><p>I think the figure and table captions need revision. They are not very clear and are missing information.</p><p>The discussion could be revised. It is a bit anemic. I would include a brief discussion of the limitations of your approach. I would also consider mentioning the MRA results, since you mention the root results.</p><p>We thank the reviewer for this comment. We added the following part to the discussion: &#x0201a;Results are considerably better for the MRA dataset. With SSIM values between 0.99 and 0.93 for the lower noise levels, the proposed networks might be used for image denoising in a clinical setting. However, to apply the proposed approach in a clinical setting, our denoising framework needs to be verified on MRA data including healthy and unhealthy subjects (i.e. patients with e.g. an aneurysm). In a clinical scenario, it needs to be verified that denoising does not suppress important clinical information. This will be done in future studies based on this work.&#x02019;</p><p>Moreover, we included a paragraph about the limitations of our study. &#x0201a;Our study has several limitations. First, all networks were trained and applied on data acquired at the same scanner. In future work, we will train and test our approach on data from different scanners yielding different image characteristics. Second, the MRA dataset used in this study yields exclusively images from healthy subjects. In future work, we will apply the networks also to subjects with vessel abnormalities.</p><p> MR images of human subjects also suffer from motion artifacts. The proposed uPL might also be a promising candidate for training networks for MR motion correction what will be investigated in future work.&#x02019;</p><p>What is the practical application implication of these results? You mention reduced imaging time, but to what extent would this allow that? You also mention the ability to use less computationally expensive denoising, but there is not an investigation of comparative performance in that regard, so it feels out-of-context. What specific advantage is gained from using this approach?</p><p>The proposed approach can be used for eliminating noise from MR images containing line-like structures. This would e.g. allow shorter scan times. However, to what extent this can be used in the clinic or for plant research, still needs to be investigated. For the use in a clinical setting, the network needs to be applied on a variety of images including data from healthy and sick subjects. Likely, also additional methods indicating e.g. the network uncertainty (i.e. ensemble models) need to be employed before a clinical implementation can be realized. For the use in plant science, it needs to be very carefully investigated for which kind of plants the method can be used. However, these investigations will be performed in future work. The proposed approach demonstrates that the uPL is a promising candidate for MR image denoising of line-like structures and should definitely be considered in a clinical application. We mention this limitation for root data already in the discussion: &#x0201a;With a best mean SSIM of 0.87 the denoising results for our root dataset still need improvement to allow for reduced measurement time in the targeted application.&#x02018; And added a part regarding the MRA dataset: &#x0201a;However, to apply the proposed approach in a clinical setting, our denoising framework needs to be verified on MRA data including healthy and unhealthy subjects (i.e. patients with e.g. an aneurysm). In a clinical scenario, it needs to be verified that denoising does not suppress important clinical information. This will be done in future studies based on this work.&#x02018;</p><p>Comments:</p><p>Lines 70-72 &#x0201c;For both datasets, four levels of Rician noise were artificially added to assess the impact of the loss functions for different signal-to-noise ratios (1%, 5%, 10%, and 20% noise added)&#x0201d;</p><p>I would be interested to see images with the various noise levels. It may be helpful for contextualizing the quality of denoising. I do not see these in your figures containing the GT images and denoised images.</p><p>Thank you for this remark. We added a new Figure (Figure 2) displaying an example root and MRA image with different noise levels.</p><p>Line 181 &#x0201c;The training parameters were chosen as they lead to the overall best performance in the validation sets&#x0201d;</p><p>Could you expand a bit on what you mean by validation sets? What exactly did you do to determine that these were the best parameters? I don&#x02019;t think you need a lot of extra detail here, but it would be nice to understand how you came to these training parameters, since these are fairly important for the overall performance of the network and since you are investigating other parameters in depth.</p><p>We thank the reviewer for this comment. As described in section &#x02018;Datasets&#x02019;, we split datasets in 3 parts, a training, a validation, and a test set. As usual in machine learning experimentation, the validation set was used for validation purposes during training, allowing e.g. for parameter tuning. Tests were run on the test set. We added the following paragraph to the manuscript: &#x0201a;For this purpose, networks were trained for 10.000, 15.000, 20.000, ... 50.000 iterations, batch size was set to 4, 8, 16, 24, and 32, and the learning rate was set to 0.001,0.002, ... 0.005. All possible combinations of these parameters were investigated and the parameters leading to the best performance in the above defined validation datasets were selected.&#x02019;</p><p>Lines 190-192 "A cube of size 52 &#x000d7; 52 &#x000d7; 52 was cropped from the root data. 190 As the roots grow from top to bottom, the cube is cropped from the upper middle part of the image. A cube of size 68 &#x000d7; 68 &#x000d7; 68 is cropped from the middle of the MRA"</p><p>Is there a reason for these cube sizes? What does middle and upper middle mean in this context? Where all sample cubes taken from the same coordinates?</p><p>We thank the reviewer for this comment. The objective of calculating the evaluation metrics on a part from the image was to calculate the metrics on regions only containing important information and no background. Therefore, we chose the cube size such that as little background as possible was present in the image part. We also tested with slightly varying cube sizes and did not observe notable changes. For the MRA images, the cropped part does not contain any background and by placing the cube on the location of the center of the root, as little background as possible is included. As the MRA images contain different numbers of z-slices, the cube was in these cases not placed at the same coordinates. For the MR-root images, the same coordinates were always chosen. We changed the text to make this more clear: &#x0201a;A cube of size 52&#x000d7;52&#x000d7;52 was cropped from the root data. As the roots grow from top to bottom and because the plant is always placed in the center of the scanner, the image was cropped such that the upper, middle image part was always present in the evaluation. I.e. the image was cropped from z-slices 0 - 52, and x-, and y-slices from 70-132. A cube of size 68&#x000d7;68&#x000d7;68 is cropped from the middle of the MRA images as these images contain important information mainly in the image center i.e. the center of the image was determined and a cube with the mentioned size was cropped such that the center of the cube aligned with the center of the image.&#x02018;</p><p>Figures 2, 3, 4</p><p>I think these figures could use better captions &#x02013; they are missing some key info. For example, in Fig 2, you do not mention the noise level, or what exactly is being displayed in the caption (but then describe it in text). This should be moved to the caption for easier reference. The same holds true for figures 2 &#x00026; 3. I see that this info is included elsewhere, but as a reader, it is difficult to keep track of what is being displayed.</p><p>We thank the reviewer for this comment. We changed all captions accordingly.</p><p>Figure 4</p><p>There are arrows (presumably pointing to features of interest) but they are not explained. Why have you chosen to highlight these points?</p><p>Thank you for this remark. We highlighted these points to mark differences across images. We added this to the Figure captions.</p><p>Tables</p><p>Why are some values underlined? It would be helpful to provide a legend/explanation in the caption for this. In general, I think the table and figure captions could be revised for clarity and content.</p><p>We thank the reviewer for pointing this out. We changed the table captions to improve clarity.</p><p>Line 253 &#x0201c;Denoising network architecture vs. loss function for different noise levels&#x0201d;</p><p>Why only use L1 here as a comparison?</p><p>As we investigated a large number of parameters and network architectures, we decided to only include the results of the L1-loss to keep a better overview. The general trend that the uPL loss outperforms other loss networks is already shown in the previous experiments and would not add valuable information here.</p><p>Lines 328-334</p><p>This section of text feels somewhat out of place.</p><p>We changed the text to: &#x0201a;None of the tested denoising network/loss function combinations allow to recover very fine roots, even though the performance increase using our uPL is considerable. In consequence, as of now, plants containing very fine roots need to be scanned for a longer time than plants with thick roots. Therefore, before implementing the trained networks on a plant root scanner, it needs to be verified for which plants (i.e. which root thickness) reduced image time in combination with a denoising network can be used without losing fine details.&#x02019;</p><p>Lines 335-336 &#x0201c;However, our results demonstrate the benefit of the untrained Perceptual Loss for both 3D datasets and all noise levels.&#x0201d;</p><p>This final sentence should be reworked a bit. It doesn&#x02019;t flow particularly well with the prior paragraph, and I think that a final statement/claim like this should be given more attention and justification.</p><p>Thank you for this comment. We changed the text to: &#x0201a;All in all, our results demonstrate that the untrained Perceptual Loss can be used in image denoising networks. To what extent it can be used for image enhancement in a clinical scenario, needs to be determined in future work. However, the untrained perceptual loss should definitely considered when training neural networks on MR images displaying fine, line-like structures.&#x02019;</p><p>Reviewer 4:</p><p>The topic is relevant and may be of interest to a broad range of the journal's readers. However, this reviewer has some major concerns about the paper.</p><p>The abstract does not highlight the specifics of the research or findings but contains too much background information. It is good to provide some specifics (e.g., sample size, dataset size, numbers from results, etc.).</p><p>We thank the reviewer for pointing this out. We changed the abstract accordingly. We added the following parts: &#x0201a;In this work, we concentrate on image denoising of MR images containing line-like structures such as roots or vessels. In particular, we investigate if the special characteristics of these datasets (connectivity, sparsity) benefit from the use of special loss functions for network training.&#x02019;; &#x0201a;In this study, 536 MR images of plant roots in soil and 450 MRA images are included. The plant root dataset is split to 380, 80, and 76 images for training, validation, and testing. The MRA dataset is split to 300, 50, and 100 images for training, validation, and testing.&#x02019;; &#x0201a;Our results are compared with the frequently used L1 loss for different network architectures. We observe, that our uPL outperforms conventional loss functions such as the L1 loss or a loss based on the Structural Similarity Index Metric (SSIM). For MRA images the uPL leads to SSIM values of 0.93 while L1 and SSIM loss led to SSIM values of 0.81 and 0.88, respectively. The uPL network&#x02019;s initialization is not important (e.g. for MR root images SSIM differences of 0.01 occur across initializations, while network depth and pooling operations impact denoising performance slightly more (SSIM of 0.83 for 5 convolutional layers and kernel size 3 vs. 0.86 for 5 convolutional layers and kernel size 5 for the root dataset).&#x02018;</p><p>2) The discussion of the results does not highlight the strengths and weaknesses of the proposed approach.</p><p>Thank you for pointing this out. We changed the discussion accordingly and added the following part: &#x0201a;None of the tested denoising network/loss function combinations allow to recover very fine roots, even though the performance increase using our uPL is considerable. In consequence, as of now, plants containing very fine roots need to be scanned for a longer time than plants with thick roots. Therefore, before implementing the trained networks on a plant root scanner, it needs to be verified for which plants (i.e. which root thickness) reduced image time in combination with a denoising network can be used without losing fine details. Results are considerably better for the MRA dataset. With SSIM values between 0.99 and 0.93 for the lower noise levels, the proposed networks might be used for image denoising in a clinical setting. However, to apply the proposed approach in a clinical setting, our denoising framework needs to be verified on MRA data including healthy and unhealthy subjects (i.e. patients with e.g. an aneurysm). In a clinical scenario, it needs to be verified that denoising does not suppress important clinical information. This will be done in future studies based on this work.</p><p>Our study has several limitations. First, all networks were trained and applied on data acquired at the same scanner. In future work, we will train and test our approach on data from different scanners yielding different image characteristics. Second, the MRA dataset used in this study yields exclusively images from healthy subjects. In future work, we will apply the networks also to subjects with vessel abnormalities. MR images of human subjects also suffer from motion artifacts. The proposed uPL might also be a promising candidate for training networks for MR motion correction what will be investigated in future work. All in all, our results demonstrate that the untrained Perceptual Loss can be used in image denoising networks. To what extent it can be used for image enhancement in a clinical scenario, needs to be determined in future work. However, the untrained perceptual loss should definitely considered when training neural networks on MR images displaying fine, line-like structures.&#x02019;</p><p>3) I suggest adding a clear research objective and research questions in the introduction section and specify what the main research problem or hypothesis is addressed.</p><p>We thank the reviewer for this important point. We added the following part to the introduction: &#x0201a; As displayed in Table 1, most recent studies addressing image denoising of MR brain or MRA images focus on the most adequate network architecture while using conventional loss functions such as L1 and/or SSIM-loss. Therefore, the aim of this work is to investigate if the uPL can be used beneficially for 3D images containing line-like structures. The research objective is to assess the impact of uPL network parameters on the results. We are especially interested if also small, simple networks can be used in the uPL.&#x02019;</p><p>4) Please add more recent references. Certainly, there has been more recent (within the last two years) research on this topic published in information science and/or computer science outlets. An academic search on the topic (using keywords from the manuscript's title) shows that there is recent work in this area. Therefore, authors must update their literature review.</p><p>Thank you for this comment. We added a Table including recent work (Table 1).</p><p>5) Starting from the previous works, I suggest introducing a table to summarize the most recent works and to highlight the novelty of the proposed work.</p><p>Thank you for pointing this out. We added a part to the introduction (see answer to question 3).</p><p>6) The manuscript only provides the structure of network, it is necessary to explain why these networks can work effectively and to show the concrete parameters.</p><supplementary-material id="pone.0318992.s017" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">plos_answer_HS.docx</named-content></p></caption><media xlink:href="pone.0318992.s017.docx"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0318992.r006" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0318992.r006</article-id><title-group><article-title>Decision Letter 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Khan</surname><given-names>Khan Bahadar</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Khan Bahadar Khan</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Khan Bahadar Khan</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0318992" id="rel-obj006" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>2</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">27 Jan 2025</named-content>
</p><p>Untrained Perceptual Loss for image denoising of line-like structures in MR images</p><p>PONE-D-24-26390R2</p><p>Dear Dr. Pfaehler,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">Editorial Manager&#x000ae;</ext-link>&#x000a0;and clicking the &#x02018;Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at authorbilling@plos.org.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>Kind regards,</p><p>Khan Bahadar Khan, Ph.D</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.</p><p>Reviewer #3:&#x000a0;All comments have been addressed</p><p>Reviewer #4:&#x000a0;All comments have been addressed</p><p>**********</p><p>2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p>3. Has the statistical analysis been performed appropriately and rigorously? </p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p>4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.</p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p>5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p><p>Reviewer #3:&#x000a0;Yes</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p>6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p><p>Reviewer #3:&#x000a0;I think the authors have addressed my comments and concerns adequately. The conclusions seem reasonable given the methods and results.</p><p>Reviewer #4:&#x000a0;(No Response)</p><p>**********</p><p>7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p><p>Reviewer #3:&#x000a0;No</p><p>Reviewer #4:&#x000a0;No</p><p>**********</p></body></sub-article><sub-article article-type="editor-report" id="pone.0318992.r007" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0318992.r007</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Khan</surname><given-names>Khan Bahadar</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Khan Bahadar Khan</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Khan Bahadar Khan</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0318992" id="rel-obj007" related-article-type="reviewed-article"/></front-stub><body><p>PONE-D-24-26390R2</p><p>PLOS ONE</p><p>Dear Dr. Pfaehler,</p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p><p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p><p>* All references, tables, and figures are properly cited</p><p>* All relevant supporting information is included in the manuscript submission,</p><p>* There are no issues that prevent the paper from being properly typeset</p><p>If revisions are needed, the production department will contact you directly to resolve them. If no revisions are needed, you will receive an email when the publication date has been set. At this time, we do not offer pre-publication proofs to authors during production of the accepted work. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few weeks to review your paper and let you know the next and final steps.</p><p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>If we can help with anything else, please email us at customercare@plos.org.</p><p>Thank you for submitting your work to PLOS ONE and supporting open access.</p><p>Kind regards,</p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Dr. Khan Bahadar Khan</p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>