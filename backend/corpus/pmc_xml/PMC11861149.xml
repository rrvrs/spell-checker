<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006478</article-id><article-id pub-id-type="pmc">PMC11861149</article-id><article-id pub-id-type="doi">10.3390/s25041249</article-id><article-id pub-id-type="publisher-id">sensors-25-01249</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Prediction of Vertical Ground Reaction Forces Under Different Running Speeds: Integration of Wearable IMU with CNN-xLSTM</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Tianxiao</given-names></name><xref rid="af1-sensors-25-01249" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1918-0756</contrib-id><name><surname>Xu</surname><given-names>Datao</given-names></name><xref rid="af1-sensors-25-01249" ref-type="aff">1</xref><xref rid="af2-sensors-25-01249" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Zhifeng</given-names></name><xref rid="af1-sensors-25-01249" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Huiyu</given-names></name><xref rid="af1-sensors-25-01249" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Shao</surname><given-names>Shirui</given-names></name><xref rid="af1-sensors-25-01249" ref-type="aff">1</xref><xref rid="c1-sensors-25-01249" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2187-9440</contrib-id><name><surname>Gu</surname><given-names>Yaodong</given-names></name><xref rid="af1-sensors-25-01249" ref-type="aff">1</xref><xref rid="af3-sensors-25-01249" ref-type="aff">3</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Hahn</surname><given-names>Michael E.</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Donahue</surname><given-names>Seth</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01249"><label>1</label>Faculty of Sports Science, Ningbo University, Ningbo 315211, China</aff><aff id="af2-sensors-25-01249"><label>2</label>Faculty of Engineering, University of Pannonia, 8200 Veszprem, Hungary</aff><aff id="af3-sensors-25-01249"><label>3</label>Faculty of Engineering, University of Szeged, 6720 Szeged, Hungary</aff><author-notes><corresp id="c1-sensors-25-01249"><label>*</label>Correspondence: <email>shaoshirui@nbu.edu.cn</email>; Tel.: +86-574-87600208</corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1249</elocation-id><history><date date-type="received"><day>06</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>16</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Traditional methods for collecting ground reaction forces (GRFs) mainly use lab force plates. Previous research broke this pattern by predicting GRFs with deep learning and data from IMUs like joint acceleration. Joint angle, as a geometric, is easier to collect than acceleration outdoors with cameras. LSTM is one of the deep learning models that have shown good performance in biomechanical studies. xLSTM, as an optimized version of LSTM, has not been used in biomechanical studies and no research has predicted GRFs during running solely using lower limb joint angles. This study collected lower-limb joint angle and vertical ground reaction force data at five speeds from 12 healthy male runners with Xsens sensors. Datasets including three joints and three planes were set as the inputs of four deep learning models for vertical-GRF prediction. CNN-xLSTM consistently performed best in the four deep learning models when different datasets were input (R<sup>2</sup> = 0.909 &#x000b1; 0.064, MAPE = 2.18 &#x000b1; 0.09, rMSE = 0.061 &#x000b1; 0.008), and the performance was at a relatively high level at the five speeds. The current findings may contribute to a new GRF measurement and provide a reference for future real-time motion detection and sport injury prediction.</p></abstract><kwd-group><kwd>running</kwd><kwd>ground reaction force</kwd><kwd>wearable IMU</kwd><kwd>deep learning</kwd><kwd>biomechanics prediction</kwd><kwd>xLSTM</kwd></kwd-group><funding-group><award-group><funding-source>Zhejiang Provincial Natural Science Foundation of China for Distinguished Young Scholars</funding-source><award-id>LR22A020002</award-id></award-group><award-group><funding-source>Zhejiang Provincial Key Research and Development Program of China</funding-source><award-id>2021C03130</award-id></award-group><award-group><funding-source>Zhejiang Provincial Natural Science Foundation</funding-source><award-id>LTGY23H040003</award-id></award-group><award-group><funding-source>Ningbo Key R&#x00026;D Program</funding-source><award-id>2022Z196</award-id></award-group><award-group><funding-source>Ningbo Natural Science Foundation</funding-source><award-id>20221JCGY010532</award-id><award-id>20221JCGY010607</award-id></award-group><award-group><funding-source>Public Welfare Science &#x00026; Technology Project of Ningbo, China</funding-source><award-id>2021S134</award-id></award-group><award-group><funding-source>Zhejiang Rehabilitation Medical Association Scientific Research Special Fund</funding-source><award-id>ZKKY2023001</award-id></award-group><funding-statement>This study was sponsored by the Zhejiang Provincial Natural Science Foundation of China for Distinguished Young Scholars (LR22A020002), the Zhejiang Provincial Key Research and Development Program of China (2021C03130), the Zhejiang Provincial Natural Science Foundation (LTGY23H040003), the Ningbo Key R&#x00026;D Program (2022Z196), the Ningbo Natural Science Foundation (20221JCGY010532, 20221JCGY010607), the Public Welfare Science &#x00026; Technology Project of Ningbo, China (2021S134), and the Zhejiang Rehabilitation Medical Association Scientific Research Special Fund (ZKKY2023001).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01249"><title>1. Introduction</title><p>Running, as a widely popular form of exercise, has been deeply favored by the masses due to its simplicity, ease of implementation, and significant effects. A substantial amount of research in sport training and biomechanics has been conducted around this fundamental movement form [<xref rid="B1-sensors-25-01249" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01249" ref-type="bibr">2</xref>]. In biomechanics, the gait phase of running is generally divided into the stance phase and the swing phase [<xref rid="B3-sensors-25-01249" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01249" ref-type="bibr">4</xref>]. The ground reaction force (GRF), as a crucial factor that drives runners moving forward, has attracted extensive attention from biomechanics researchers. Accurate measurement of the ground reaction force during running is of great significance. Biomechanical analysis, through measuring the vertical ground reaction force during running, helps to intuitively understand the force distribution during running, thereby optimizing running technique, such as adjusting the running posture and foot strike pattern, reducing the impact force, and improving running efficiency [<xref rid="B5-sensors-25-01249" ref-type="bibr">5</xref>]. At the same time, this measurement also aids in preventing sport injuries, assessing injury risks, and formulating personalized preventive measures. In addition, the vertical ground reaction force is a key parameter in scientific research, providing data support for the development of sport science, and is of great importance for the training guidance of professional athletes and coaches. Ultimately, by optimizing technique, running efficiency can be improved, enhancing the runner&#x02019;s sporting experience and stimulating their enthusiasm for continued participation [<xref rid="B6-sensors-25-01249" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01249" ref-type="bibr">7</xref>]. Compared with GRFs in other directions, the magnitude of the vertical ground reaction force (vertical GRF) directly influences the propulsive efficiency and energy conversion of running [<xref rid="B8-sensors-25-01249" ref-type="bibr">8</xref>]. Previous studies have shown that, under the rear-foot strike running pattern, the vertical-GRF curve typically exhibits a double-peak trend [<xref rid="B9-sensors-25-01249" ref-type="bibr">9</xref>,<xref rid="B10-sensors-25-01249" ref-type="bibr">10</xref>,<xref rid="B11-sensors-25-01249" ref-type="bibr">11</xref>].</p><p>Traditionally, the measurement of GRFs has primarily relied on specialized force plates in laboratories. Although the data from force plates is highly valuable for in-depth analysis of a runner&#x02019;s running posture, assessment of running efficiency, and prediction of sport injury risks, the high equipment costs, spatial limitations, and lack of flexibility in data collection have become limitations in their widespread application [<xref rid="B12-sensors-25-01249" ref-type="bibr">12</xref>]. Therefore, exploring a method that can measure GRFs in non-laboratory environments through convenient and low-cost means is of great significance for advancing sport science research and sport training practice [<xref rid="B13-sensors-25-01249" ref-type="bibr">13</xref>,<xref rid="B14-sensors-25-01249" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01249" ref-type="bibr">15</xref>]. Such a method can not only reduce the cost and improve the flexibility of data collection but also enable runners and coaches to conduct real-time monitoring and analysis in daily training or home environments, thereby better guiding training and preventing sport injuries [<xref rid="B16-sensors-25-01249" ref-type="bibr">16</xref>,<xref rid="B17-sensors-25-01249" ref-type="bibr">17</xref>,<xref rid="B18-sensors-25-01249" ref-type="bibr">18</xref>].</p><p>With the development of wearable inertial measurement units (IMUs) and deep learning, more and more biomechanical studies have used wearable IMUs to collect kinematics and dynamics and then input these data into machine learning and DL models for the classification, recognition, and prediction of athletic performance and sport injuries [<xref rid="B19-sensors-25-01249" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01249" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01249" ref-type="bibr">21</xref>,<xref rid="B22-sensors-25-01249" ref-type="bibr">22</xref>]. Previous studies have directly or indirectly predicted ground reaction forces in running using machine learning and deep learning algorithms, which contributed to breaking the traditional mode of measuring GRFs with force plates in laboratories [<xref rid="B23-sensors-25-01249" ref-type="bibr">23</xref>,<xref rid="B24-sensors-25-01249" ref-type="bibr">24</xref>,<xref rid="B25-sensors-25-01249" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01249" ref-type="bibr">26</xref>].</p><p>Long Short-Term Memory (LSTM), as a special recurrent neural network structure, has been applied to analysis and prediction tasks in the field of biomechanics in previous research. The gating mechanism of LSTM is capable of capturing complex long-term dependencies, which is particularly important when predicting the relationship between the ground reaction force and joint angle changes during the running stance phase. LSTM can handle long-sequence data, avoiding gradient vanishing or exploding problems, making it suitable for processing longer sequences of data. Moreover, LSTM has a good generalization ability and can be combined with other network structures, such as CNN, to improve prediction accuracy and robustness, making it applicable to different runners and environments, thereby broadening the application scope of GRF prediction [<xref rid="B27-sensors-25-01249" ref-type="bibr">27</xref>,<xref rid="B28-sensors-25-01249" ref-type="bibr">28</xref>]. Alcantara et al. [<xref rid="B29-sensors-25-01249" ref-type="bibr">29</xref>] and Donahue et al. [<xref rid="B30-sensors-25-01249" ref-type="bibr">30</xref>] predicted GRFs accurately with an LSTM-based model. The Extended LSTM (xLSTM) network was proposed by M. Beck and his team, the founders of LSTM [<xref rid="B31-sensors-25-01249" ref-type="bibr">31</xref>]. As an extension of LSTM, it encompasses two variants: scalar LSTM (sLSTM) and matrix memory LSTM (mLSTM). The sLSTM block retains LSTM&#x02019;s sequential processing and optimizes gating through fine-grained control, making it suitable for subtle temporal variations. The mLSTM block processes all the token sequences simultaneously, enhancing memory and parallel processing by extending LSTM&#x02019;s vector operations to matrix operations. The two different modules can be flexibly combined within the xLSTM architecture to balance parallelism and sequential modeling [<xref rid="B31-sensors-25-01249" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01249" ref-type="bibr">32</xref>]. Though xLSTM has been used in previous studies for the prediction of time-series data and has demonstrated good performance, there are currently no studies in the field of biomechanics that use xLSTM to analyze kinematics and kinetics [<xref rid="B33-sensors-25-01249" ref-type="bibr">33</xref>,<xref rid="B34-sensors-25-01249" ref-type="bibr">34</xref>,<xref rid="B35-sensors-25-01249" ref-type="bibr">35</xref>].</p><p>Previous studies have mainly used kinematic data, such as acceleration, from IMUs as inputs of deep learning models. However, although these studies have accurately estimated the ground reaction force during running, there have been no studies that have used lower-limb joint angles as a single input to predict the GRF. With the advancement of machine learning and deep learning algorithms, complex models are often used for visual data analysis and real-time recognition [<xref rid="B36-sensors-25-01249" ref-type="bibr">36</xref>]. Joint angle is not only a type of geometric data that can be captured in real time through imaging equipment and algorithms but is also a basic type of kinematic data in sport biomechanics that has attracted much research attention [<xref rid="B37-sensors-25-01249" ref-type="bibr">37</xref>,<xref rid="B38-sensors-25-01249" ref-type="bibr">38</xref>]. Based on the above introduction, the aim of this study was to develop an xLSTM-based deep learning model to predict the vertical ground reaction force during the stance phase of running by inputting the angle data of lower-limb joints (ankle, hip, and knee) on three planes (sagittal, frontal, and transversal) and explore the influence of the angles of different joints and different motion planes on the accuracy of prediction results. This study may provide alternatives to break the traditional pattern of collecting ground reaction forces by using force plates in the laboratory. We assumed that the prediction would work best when all three joint angles on all three planes were input. The main contributions of this study are as follows.</p><p>1. We develop a deep learning model that can accurately predict the vertical ground reaction force during the stance phase of running by inputting the joint angles of the lower limbs.</p><p>2. We explore the impact of different joint angles on different planes on the prediction results of vertical ground reaction forces.</p><p>3. We test the predictive performance of the developed model at five different running speeds.</p></sec><sec id="sec2-sensors-25-01249"><title>2. Procedure</title><p>The study was divided into 3 main parts. First, lower-limb joint angle and ground reaction force data were collected from 12 healthy male runners through a Vicon three-dimensional motion capture system, Kistler force plates, and Xsens sensors during the running stance phase. Second, the collected data were preprocessed and categorized using different joints and different planes. Third, angles from different joints and planes were used to train 4 deep learning models (CNN-xLSTM, CNN-sLSTM, CNN-mLSTM, and CNN-LSTM) to predict the vertical ground reaction forces. The workflow of the study is shown in <xref rid="sensors-25-01249-f001" ref-type="fig">Figure 1</xref>.</p><sec id="sec2dot1-sensors-25-01249"><title>2.1. Data Collection and Preprocessing</title><p>The vertical-GRF data and the lower-limb joint angle data were collected in the Biomechanics Laboratory of Ningbo University. Twelve healthy male runners (age: 22.5 &#x000b1; 0.86 years; body mass: 72.5 &#x000b1; 9.55 kg; height: 1.78 &#x000b1; 0.77 m) were recruited to participate in the study. The subject screening criteria were as follows: (1) participants must have no history of serious lower-limb surgery or any other injury variables in the past six months that would interfere with the study; and (2) there must be no other factors that would affect athletic performance. All participants were informed of the purpose, requirements, and procedures of the experiment and signed a written informed consent form. This study complied with the principles laid down in the Declaration of Helsinki. Ningbo University&#x02019;s Ethics Committee accepted the study protocol (Approval Number: TY2024037), and all subjects supplied and signed a written informed permission form.</p><p>Vertical GRFs during running were collected in this study through a Vicon three-dimensional motion capture system (Vicon Metrics Ltd., version 2.14.0, Oxford, UK) and Kistler force plates. The sampling frequency was set to 200 Hz and 1000 Hz, respectively. Two photoelectric gates were placed on both sides of the Kistler force plates, and the time it took runners to pass the force plate was recorded and converted to the running speed (8 km/h, 10 km/h, 12 km/h, 14 km/h, and 16 km/h) [<xref rid="B39-sensors-25-01249" ref-type="bibr">39</xref>]. Each runner was required to run through the Vicon&#x02013;Kistler&#x02013;photoelectric gate system with Xsens sensors 10 times at each speed. All runners were required to wear Xsens motion capture sensors and to wear designated clothes and running shoes while running to collect the angle of 3 joints (hip, knee, and ankle) on 3 planes (sagittal, frontal, and transversal). The Xsens sensors (Xsens, Henderson, NV, USA) were set on the hip, thigh, shank, and foot of each runner (<xref rid="sensors-25-01249-f001" ref-type="fig">Figure 1</xref>). All runners were required to perform the running task with the rear-foot strike pattern and were allowed adequate rest after each running task.</p><p>The stance phase of running was defined as the period from the initial contact of the right heel with the ground (when the GRF collected by the force platform exceeded 10 N) to the complete liftoff of the right forefoot from the ground [<xref rid="B40-sensors-25-01249" ref-type="bibr">40</xref>]. The phase was divided into a period of 0&#x02013;100% in this study. A fourth-order Butterworth low-pass filter was used to process the collected ground reaction force and joint angle data, with cutoff frequencies set at 10 Hz and 20 Hz, respectively. The filtered data were imported into MATLAB (Visual R2022a, MathWorks, Natick, MA, USA), and we used a MATLAB script to perform an interpolation calculation, expanding the data to 101 points corresponding to 0&#x02013;100% of the running stance phase. Data on missing instances and eliminated outliers were checked through MATLAB scripts to ensure the accuracy of the dataset. After the preprocessing procedure, 530 sets of one-to-one corresponding vertical-GRF and joint angle data were put into the final dataset. To investigate the impact of different input data on the prediction results, the calculated joint angles were classified as follows and used as different inputs:<list list-type="simple"><list-item><label>1.</label><p>M<sub>1 (3<italic toggle="yes">Joints</italic>, 3<italic toggle="yes">Planes</italic>)</sub> = 530 &#x000d7; 909 <sub>(3<italic toggle="yes">joints</italic> &#x000d7; 3<italic toggle="yes">planes</italic> &#x000d7; 101<italic toggle="yes">angles</italic>)</sub>;</p></list-item><list-item><label>2.</label><p>M<sub>2 (<italic toggle="yes">Ankle</italic>, 3<italic toggle="yes">Planes</italic>)</sub> = 530 &#x000d7; 303 <sub>(1<italic toggle="yes">ankle joint</italic> &#x000d7; 3<italic toggle="yes">planes</italic> &#x000d7; 101<italic toggle="yes">angles</italic>)</sub>;</p></list-item><list-item><label>3.</label><p>M<sub>3 (<italic toggle="yes">Hip</italic>, 3<italic toggle="yes">Planes</italic>)</sub> = 530 &#x000d7; 303 <sub>(1<italic toggle="yes">hip joint</italic> &#x000d7; 3<italic toggle="yes">planes</italic> &#x000d7; 101<italic toggle="yes">angles</italic>)</sub>;</p></list-item><list-item><label>4.</label><p>M<sub>4 (<italic toggle="yes">Knee</italic>, 3<italic toggle="yes">Planes</italic>)</sub> = 530 &#x000d7; 303 <sub>(1<italic toggle="yes">knee joint</italic> &#x000d7; 3<italic toggle="yes">planes</italic> &#x000d7; 101<italic toggle="yes">angles</italic>)</sub>;</p></list-item><list-item><label>5.</label><p>M<sub>5 (3Joints, Sagittal)</sub> = 530 &#x000d7; 303 <sub>(3joints &#x000d7; 1sagittal plane &#x000d7; 101angles)</sub>;</p></list-item><list-item><label>6.</label><p>M<sub>6 (3Joints, Frontal)</sub> = 530 &#x000d7; 303 <sub>(3joints &#x000d7; 1frontal plane &#x000d7; 101angles)</sub>;</p></list-item><list-item><label>7.</label><p>M<sub>7 (3Joints, Transversal)</sub> = 530 &#x000d7; 303 <sub>(3joints &#x000d7; 1transversal plane &#x000d7; 101angles)</sub>.</p></list-item></list></p></sec><sec id="sec2dot2-sensors-25-01249"><title>2.2. Deep Learning Models</title><p>A CNN-xLSTM network, a CNN-sLSTM network, a CNN-mLSTM network, and a CNN-LSTM network were developed in this study for vertical-GRF prediction. The development, training, and validation of the 4 deep learning models were conducted in PyCharm (V2024.2.3, JetBrains, Prague, Czech Republic). The structure of the deep learning models is shown in <xref rid="sensors-25-01249-f002" ref-type="fig">Figure 2</xref>.</p><sec id="sec2dot2dot1-sensors-25-01249"><title>2.2.1. Convolutional Neural Networks (CNNs)</title><p>The basic structure of a Convolutional Neural Network (CNN) comprises an input layer, convolutional layers, pooling layers, fully connected layers, and an output layer. The input layer receives the original data, the convolutional layers extract features using multiple convolution kernels to generate feature maps, the pooling layers reduce the dimensionality of the feature, decreasing the computational load and preventing overfitting, the fully connected layers transform the output of the pooling layers into a probability distribution for classification results, and the output layer produces the final classification labels [<xref rid="B41-sensors-25-01249" ref-type="bibr">41</xref>,<xref rid="B42-sensors-25-01249" ref-type="bibr">42</xref>].</p><p>A CNN block with a convolutional kernel size of 3 and a pooled layer size of 2 was set and combined with 4 deep learning model blocks (xLSTM, sLSTM, mLSTM, and LSTM) in this study to extract temporal features of the joint angle and vertical ground reaction forces during the stance phase. The output of the CNN was set as the input for subsequent models in this study.</p></sec><sec id="sec2dot2dot2-sensors-25-01249"><title>2.2.2. Long Short-Term Memory (LSTM)</title><p>The LSTM model achieves the effective capture and memorization of long-sequence messages by introducing a cell status and three logic gates (a forget gate, an input gate, and an output gate) that control message transmission [<xref rid="B28-sensors-25-01249" ref-type="bibr">28</xref>]. The main characteristic of this model lies in its unique &#x0201c;gating mechanism&#x0201d;, with the primary algorithmic formula as follows:
<disp-formula>C<sub>t</sub> = f<sub>t</sub> &#x02219; C<sub>t&#x02212;1</sub> + i<sub>t</sub> &#x02219; z<sub>t</sub><label>(1)</label></disp-formula>
where C<sub>t</sub> represents the cell state at time t; f<sub>t</sub> represents the output of the forget gate at time t; C<sub>t&#x02212;1</sub> represents the cell state at the previous time step; i<sub>t</sub> represents the output of the input gate at time t; and z<sub>t</sub> represents the candidate cell state at time t.</p><p>The nn LSTM class from the PyTorch deep learning framework was utilized to implement this LSTM layer structure. This class provides all the necessary functionalities for constructing layers, including parameter initialization and forward propagation.</p></sec><sec id="sec2dot2dot3-sensors-25-01249"><title>2.2.3. Extended Long Short-Term Memory (xLSTM)</title><p>xLSTM is actually a hybrid model of two variants: scalar LSTM (sLSTM) and matrix LSTM (mLSTM). sLSTM retains the memory mixing function of traditional LSTM and supports state tracking, making it suitable for tasks that require the capture of subtle changes in time-series data, while mLSTM introduces a normalizer state to track the product of the input gate and the future forget gate. Additionally, mLSTM achieves full parallelism, enabling the efficient processing of large-scale data and making it suitable for tasks requiring a fast response and high-performance computing [<xref rid="B31-sensors-25-01249" ref-type="bibr">31</xref>]. The two blocks can be switched and selected by modifying the &#x02018;s&#x02019; or &#x02018;m&#x02019; module in the model definition code in Pycharm.</p><p>sLSTM improves upon the standard LSTM algorithm through its unique exponential gating mechanism. This variant introduces an exponential function as the activation function of the model to control information flow, making the activation of the input gate and forget gate more efficient and stable. Based on this mechanism, the forward propagation algorithm of sLSTM is as follows:
<disp-formula>n<sub>t</sub> = f<sub>t</sub> &#x02219; n<sub>t&#x02212;1</sub> + i<sub>t</sub><label>(2)</label></disp-formula>
where n<sub>t</sub> represents a normalization state, which sums the product of the input gate and all future forget gates; f<sub>t</sub> represents the activation value of the forget gate, which is used to regulate the amount of information inherited from the previous time step t &#x02212; 1; n<sub>t&#x02212;1</sub> represents the state of the previous time step; and i<sub>t</sub> represents the new information added at the current time step.<disp-formula id="FD1-sensors-25-01249"><label>(3)</label><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo mathvariant="normal">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD2-sensors-25-01249"><label>(4)</label><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo mathvariant="normal">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the output state at a time step; <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the activation value of the output gate, which is used to regulate the amount of output from the cell; <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo mathvariant="normal">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the candidate output state used to adjust the activation level; and <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the internal state of the cell or the &#x0201c;memory cell&#x0201d; state, which is typically used to store long-term information.<disp-formula id="FD3-sensors-25-01249"><label>(5)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi>exp</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mo mathvariant="normal">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="FD4-sensors-25-01249"><label>(6)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi>exp</mml:mi></mml:mrow><mml:mo>&#x02061;</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mo mathvariant="normal">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the activation values of the input gate and forget gate, respectively, after being transformed by the exponential function <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>exp</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mo mathvariant="normal">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mo mathvariant="normal">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represent the activation values before the transformation. This process ensures that the output values of the input gate and forget gate are positive and uses these outputs for subsequent nonlinear activation and the regulation of information flow [<xref rid="B31-sensors-25-01249" ref-type="bibr">31</xref>].</p><p>mLSTM extends the vector operations in the original LSTM algorithm to matrix operations, significantly enhancing the model&#x02019;s memory capacity and parallel processing capability. The algorithm for updating the memory cell through matrix operations in mLSTM is as follows:<disp-formula id="FD5-sensors-25-01249"><label>(7)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">f</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02299;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02299;</mml:mo><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mi>tanh</mml:mi><mml:mo>&#x000a0;</mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mo>[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>]</mml:mo><mml:mo>+</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the memory cell matrix at the current time step, <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02299;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes element-wise multiplication, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the input gate matrix, <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>,</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the matrix formed by concatenating the hidden state <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> from the previous time step and the input <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> at the current time step, and <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a bias term. The function of the hidden state in mLSTM is:<disp-formula id="FD6-sensors-25-01249"><label>(8)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo mathvariant="normal">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000a0;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>max</mml:mi><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x022a4;</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="normal">q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the memory cell matrix at the current time step, <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mo mathvariant="normal">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the hidden state at the current time step, and <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">q</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the query input. The parallelization capability of mLSTM significantly improves the computational efficiency when processing long sequences by eliminating memory mixing, enabling the parallel capture and processing of high-dimensional information in tokens and, thus, accelerating the training and inference processes [<xref rid="B31-sensors-25-01249" ref-type="bibr">31</xref>].</p></sec></sec><sec id="sec2dot3-sensors-25-01249"><title>2.3. Model Training and Validation</title><sec id="sec2dot3dot1-sensors-25-01249"><title>2.3.1. Model Training</title><p>The 7 datasets mentioned in <xref rid="sec2dot1-sensors-25-01249" ref-type="sec">Section 2.1</xref> were used as the input of 4 deep learning models. The Min&#x02013;Max normalization technique was employed to normalize the data, and the algorithm formula for this process is as follows:<disp-formula id="FD7-sensors-25-01249"><label>(9)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>min</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the value of a single data point, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the minimum value in the column of data, and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum value in the column of data. This technique scales the original data to a range between 0 and 1. This method not only preserves the original distribution of the data but also unifies the scale of the data, making different features or variables comparable.</p></sec><sec id="sec2dot3dot2-sensors-25-01249"><title>2.3.2. Model Validation</title><p>K-fold cross-validation was chosen in this study for model validation and to overcome the overfitting problem. Through 10-fold cross-validation (K = 10), the shuffled dataset was evenly divided into 10 subsets of equal size, and 10 iterations were performed. In each iteration, 9 subsets were selected as the training set, and the remaining subset was used as the test set. The squared correlation coefficient (R<sup>2</sup>), the Mean Absolute Percentage Error (MAPE), and the root Mean Squared Error (rMSE) between the predicted values and the actual values were calculated by the matplotlib library with Python code to evaluate the performance of the 4 models in the task of vertical-GRF prediction. The formula for R<sup>2</sup> is as follows:<disp-formula id="FD8-sensors-25-01249"><label>(10)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mo>&#x02211;</mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo mathvariant="normal">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo mathvariant="normal">-</mml:mo></mml:mover><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the true value, <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo mathvariant="normal">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the predicted value, <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo mathvariant="normal">-</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> represents the sample mean, <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mo>&#x02211;</mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo mathvariant="normal">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the error generated by the predictions, and <inline-formula><mml:math id="mm35" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo mathvariant="normal">-</mml:mo></mml:mover><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> represents the error generated by the mean. The R<sup>2</sup> is better when it is larger. When the prediction model makes no errors, the R<sup>2</sup> reaches its maximum value of 1.</p><p>The formula for MAPE is as follows:<disp-formula id="FD9-sensors-25-01249"><label>(11)</label><mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>MAPE</mml:mi><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo mathvariant="normal">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the true value and <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo mathvariant="normal">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the predicted value. The range is [0, +&#x0221e;). An MAPE of 0% indicates a perfect model, while an MAPE greater than 100% indicates a poor model.</p><p>The formula for the rMSE is as follows:<disp-formula id="FD10-sensors-25-01249"><label>(12)</label><mml:math id="mm39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>rMSE</mml:mi><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:msqrt><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mrow><mml:munderover><mml:mo stretchy="true">&#x02211;</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>&#x000a0;</mml:mo><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>&#x000a0;</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo mathvariant="normal">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the number of samples, <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the actual value of the data, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mo mathvariant="normal">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the predicted value of the data. A smaller MSE value indicates a smaller difference between the predicted values and the actual values, implying more accurate predictions.</p></sec></sec></sec><sec sec-type="results" id="sec3-sensors-25-01249"><title>3. Results</title><sec id="sec3dot1-sensors-25-01249"><title>3.1. Parameters of Deep Learning Models</title><p>The seven datasets mentioned in the data processing section were used as the input of four deep learning models (CNN-xLSTM, CNN-sLSTM, CNN-mLSTM, and CNN-LSTM). After training and testing multiple combinations of variant structures, a CNN-xLSTM model that integrates an mLSTM module with an sLSTM module was ultimately constructed in this study. <xref rid="sensors-25-01249-t001" ref-type="table">Table 1</xref> shows the optimal parameter configuration of each model. The training loss and testing loss with the configuration are shown in <xref rid="sensors-25-01249-f003" ref-type="fig">Figure 3</xref>.</p></sec><sec id="sec3dot2-sensors-25-01249"><title>3.2. Prediction Results and Model Performance</title><p>The vertical-GRF prediction results of training four deep learning models with data from different joints and planes (M<sub>1 (3<italic toggle="yes">Joints</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>2 (<italic toggle="yes">Ankle</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>3 (<italic toggle="yes">Hip</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>4 (<italic toggle="yes">Knee</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>5 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Sagittal</italic>)</sub>, M<sub>6 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Frontal</italic>),</sub> and M<sub>7 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Transversal</italic>)</sub>) are shown in the <xref rid="sensors-25-01249-f004" ref-type="fig">Figure 4</xref>. <xref rid="sensors-25-01249-t002" ref-type="table">Table 2</xref> shows the R<sup>2</sup>, MAPE, and rMSE of the four models in each prediction task. In <xref rid="sensors-25-01249-f004" ref-type="fig">Figure 4</xref> and <xref rid="sensors-25-01249-t002" ref-type="table">Table 2</xref>, when the dataset M<sub>1 (3<italic toggle="yes">Joints</italic>, 3<italic toggle="yes">Planes</italic>)</sub> was used as the input dataset, the fitting effect of the prediction results of the four models was the best (R<sup>2</sup><sub>xLSTM</sub> = 0.909 &#x000b1; 0.064, R<sup>2</sup><sub>sLSTM</sub> = 0.748 &#x000b1; 0.056, R<sup>2</sup><sub>mLSTM</sub> = 0.791 &#x000b1; 0.077, and R<sup>2</sup><sub>LSTM</sub> = 0.742 &#x000b1; 0.040), which means that the angles of the ankle, hip, and knee joints on all three planes during the running stance phase made the biggest contribution to accurate vertical-GRF prediction results.</p><p>In addition, when the input datasets were M<sub>2 (<italic toggle="yes">Ankle</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>5 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Sagittal</italic>)</sub>, and M<sub>6 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Frontal</italic>)</sub>, the four models also showed a relatively good performance in fitting the vertical-GRF curve. When the input datasets were M<sub>3 (<italic toggle="yes">Hip</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>4 (<italic toggle="yes">Knee</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, and M<sub>7 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Transversal</italic>)</sub>, the fitting effect of the four models on the vertical ground reaction force curve was not ideal. In <xref rid="sensors-25-01249-f004" ref-type="fig">Figure 4</xref>, <xref rid="sensors-25-01249-f005" ref-type="fig">Figure 5</xref>, and <xref rid="sensors-25-01249-t002" ref-type="table">Table 2</xref>, when the seven datasets were input separately, the CNN-xLSTM model consistently showed the best fitting effect for the vertical-GRF curve among the four models, i.e., the highest R<sup>2</sup> value (R<sup>2</sup> = 0.909 &#x000b1; 0.064), the lowest MAPE value (MAPE = 2.18 &#x000b1; 0.09), and the lowest rMSE value (rMSE = 0.061 &#x000b1; 0.008).</p></sec><sec id="sec3dot3-sensors-25-01249"><title>3.3. Result Validation</title><p>Joint angle and vertical ground reaction force data at five different running speeds (8 km/h, 10 km/h, 12 km/h, 14 km/h, and 16 km/h) were collected in this study. The CNN-xLSTM model with an sLSTM block and an mLSTM block showed the best vertical-GRF prediction performance among the four models (as described in <xref rid="sec2dot2-sensors-25-01249" ref-type="sec">Section 2.2</xref>). Datasets at five speeds were input into CNN-xLSTM to validate the accuracy of the predicted vertical-GRF values at different running speeds, and the four cases with better prediction performance (M<sub>1 (3<italic toggle="yes">Joints</italic>, 3<italic toggle="yes">Planes</italic>),</sub> M<sub>2 (<italic toggle="yes">Ankle</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>5 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Sagittal</italic>)</sub>, and M<sub>6 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Frontal</italic>)</sub>) are discussed in this section. The R<sup>2</sup>, MAPE, and rMSE of CNN-xLSTM at five running speeds are shown in <xref rid="sensors-25-01249-t003" ref-type="table">Table 3</xref> and <xref rid="sensors-25-01249-f006" ref-type="fig">Figure 6</xref>. In <xref rid="sensors-25-01249-t003" ref-type="table">Table 3</xref>, the performance of CNN-xLSTM in the vertical-GRF prediction tasks was at a relatively stable level at all five running speeds. Each case performed best at the speed of 12 km/h, and the case of three joints and three planes made the biggest contribution to the prediction results (R<sup>2</sup> = 0.879 &#x000b1; 0.068, MAPE = 2.19 &#x000b1; 0.12, rMSE = 0.063 &#x000b1; 0.010). The performance of the CNN-xLSTM model developed in this study is compared with the studies mentioned in this article in <xref rid="sensors-25-01249-t004" ref-type="table">Table 4</xref>.</p></sec></sec><sec sec-type="discussion" id="sec4-sensors-25-01249"><title>4. Discussion</title><p>The purpose of this study was to develop an xLSTM-based deep learning model to predict the vertical ground reaction force during the stance phase of running by inputting the angles of the lower-limb joints (ankle, hip, and knee) on three planes (sagittal, frontal, and transversal) and to explore the influence of the angles of different joints and different motion planes on the accuracy of prediction results. We first collected lower-limb joint angles and vertical ground reaction forces at five speeds from 12 healthy male runners during the running stance phase with Xsens sensors. The collected data were divided into seven datasets, including three lower-limb joints (ankle, hip, and knee) and three planes (sagittal, frontal, and transversal), which were set as the input datasets for each of the four deep learning models (CNN-xLSTM, CNN-sLSTM, CNN-mLSTM, and CNN-LSTM). The CNN-xLSTM model showed the best performance in the vertical-GRF prediction tasks among the four models (R<sup>2</sup> = 0.909 &#x000b1; 0.064, MAPE = 2.18 &#x000b1; 0.09, rMSE = 0.061 &#x000b1; 0.008).</p><sec id="sec4dot1-sensors-25-01249"><title>4.1. Contribution of Different Joint Angles</title><p>Most previous studies used joint acceleration or angular velocity from a wearable IMU as the input of machine learning or deep learning models to predict the ground reaction force [<xref rid="B25-sensors-25-01249" ref-type="bibr">25</xref>,<xref rid="B26-sensors-25-01249" ref-type="bibr">26</xref>,<xref rid="B29-sensors-25-01249" ref-type="bibr">29</xref>]. However, there are few studies that predicted ground reaction forces with joint angles alone. The Xsens wearable sensors that were used in this study allowed us to collect joint angle data directly, skipping the process of conversion through other variables. As a type of intuitive geometric data, joint angles can be captured directly by the cameras and identified by a specific algorithm then set as the input of subsequent algorithms, which may help to break away from the limitations of the traditional mode of measuring ground reaction forces with force plates in the laboratory [<xref rid="B12-sensors-25-01249" ref-type="bibr">12</xref>]. Lower-limb joint angle data from three joints and three planes were used as the input into deep learning models in this study to predict vertical GRFs during the running stance phase. In the seven inputs mentioned in <xref rid="sec2dot1-sensors-25-01249" ref-type="sec">Section 2.1</xref>, when M<sub>1 (3<italic toggle="yes">Joints</italic>, 3<italic toggle="yes">Planes</italic>)</sub> was input, all models achieved the best prediction performance when the seven datasets were input separately, indicating that the data from the three joints on all three planes contributed the most to accurate vertical ground reaction force prediction results. At the same time, we also found that, among the three joints, the performance of the four models was better than that of the other two joints when the angles of the ankle were entered on the three planes. On the three planes, when the joint angles on the sagittal and frontal planes were input, the prediction performance of the model was also relatively good (<xref rid="sensors-25-01249-f004" ref-type="fig">Figure 4</xref>). These findings not only reveal the influence of different lower-limb joint angles on different planes on the accurate prediction of vertical ground reaction forces, but also provide a reference for the setting of sport analysis equipment and the rehabilitation and training of athletes or sport injury patients [<xref rid="B18-sensors-25-01249" ref-type="bibr">18</xref>,<xref rid="B43-sensors-25-01249" ref-type="bibr">43</xref>]. The joint angles on the sagittal and frontal planes have a significant impact on predicting the ground reaction force during running. Changes in joint angles on the sagittal plane are directly related to lower-limb propulsion movements and the force output, affecting the direction and magnitude of the ground reaction force, whereas the joint angles on the frontal plane, although small, are crucial for maintaining running stability and also have an indirect influence on the ground reaction force. When data from joint angles on all three planes are considered comprehensively, deep learning models can more fully capture the lower-limb movement status and accurately predict the ground reaction force [<xref rid="B44-sensors-25-01249" ref-type="bibr">44</xref>]. At the same time, the joint angles on the sagittal and frontal planes have independent predictive capabilities, reflecting information on lower-limb propulsion and stability, respectively. Understanding these joint angles is of great significance for optimizing running posture and improving running efficiency.</p></sec><sec id="sec4dot2-sensors-25-01249"><title>4.2. Performance of CNN-xLSTM</title><p>As an optimized model of LSTM, xLSTM was proposed by Sepp Hochreiter and his team. xLSTM combines the advantages of the sLSTM and mLSTM variants in dealing with different tasks and can freely select and combine modules to cope with different tasks through the Python language [<xref rid="B31-sensors-25-01249" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01249" ref-type="bibr">32</xref>]. Since its proposal, xLSTM has demonstrated powerful performance in prediction tasks in many fields [<xref rid="B34-sensors-25-01249" ref-type="bibr">34</xref>,<xref rid="B45-sensors-25-01249" ref-type="bibr">45</xref>,<xref rid="B46-sensors-25-01249" ref-type="bibr">46</xref>]. In this study, we developed a CNN-xLSTM model to predict vertical GRFs during running. CNN-LSTM was constructed by connecting a CNN module to an xLSTM containing an sLSTM module and an mLSTM module. The CNN block was used to extract the features of lower-limb joint angles from the time series of the running standing phase and input these features into xLSTM for vertical-GRF prediction. When the seven datasets were input, CNN-xLSTM consistently showed the best performance among the four models used in this study (R<sup>2</sup> = 0.909 &#x000b1; 0.064, MAPE = 2.18 &#x000b1; 0.09, rMSE = 0.061 &#x000b1; 0.008). We also found in the prediction results that CNN-xLSTM has a better fitting effect on the double-peak characteristics of the vertical GRF during running in the rear-foot strike pattern [<xref rid="B3-sensors-25-01249" ref-type="bibr">3</xref>,<xref rid="B9-sensors-25-01249" ref-type="bibr">9</xref>] (<xref rid="sensors-25-01249-f004" ref-type="fig">Figure 4</xref>). In order to validate the robustness, we tested the four datasets with a relatively large contribution to the prediction results at five speeds (8 km/h, 10 km/h, 12 km/h, 14 km/h, and 16 km/h), and the results show that the prediction performance of CNN-xLSTM for the vertical ground reaction force was maintained at a relatively stable level at all five speeds, which indicates that the model was developed with good robustness (<xref rid="sensors-25-01249-f006" ref-type="fig">Figure 6</xref>, <xref rid="sensors-25-01249-t003" ref-type="table">Table 3</xref>). In addition, CNN-xLSTM was compared with the three previous models mentioned in this paper that attempted to predict the vertical GRF, and the prediction performance of CNN-xLSTM was better than most models proposed by other studies that used acceleration as the input (<xref rid="sensors-25-01249-t004" ref-type="table">Table 4</xref>). Although CNN-xLSTM has demonstrated good accuracy in prediction tasks, it should be acknowledged that the training and test datasets used by the models in the comparison are different from those in this study, and there may also be differences in the data collection methods. In previous studies, it may have been difficult to achieve good results in the prediction of the ground reaction force based on a single data point or feature. This study only used the angles of the lower-limb joints from the three planes and accurately predicted the vertical GRF during running. This result may depend on the analysis and judgment of the complex relationship between the joint angle data and the ground reaction force data in the time series by the &#x02018;m&#x02019; and &#x02018;s&#x02019; modules in xLSTM.</p></sec><sec id="sec4dot3-sensors-25-01249"><title>4.3. Prospects and Limitations</title><p>In this study, the vertical ground reaction force during running was predicted by using the angle of lower-limb joints in different motion planes by using deep learning methods. The results of this study provide a reference for future real-time motion detection and sport injury prediction. The joint angle can be used as a type of geometric data that can be collected and calculated by the image capture device in real time. At the same time, athletes or patients with sport injuries can adjust their stride length, cadence, and other sport modes in time according to the predicted results to achieve the purpose of improving sport performance and rehabilitation.</p><p>There are also limitations to this study. The study only collected data on lower-limb joint angles and vertical ground reaction forces during running from 12 healthy adult male runners, without considering the situation of female runners, and there is a lack of sample diversity that may affect the generality of the results. Additionally, when collecting data at five different speeds, due to equipment limitations, we did not strictly determine speed indicators, and there may be errors in the speed conversion results obtained from the photoelectric gates. Furthermore, the deep learning model developed in this study was not trained and tested using a public dataset, and results obtained using a public dataset may deviate from those in this study.</p></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01249"><title>5. Conclusions</title><p>This study developed a CNN-xLSTM model and accurately predicted the vertical ground reaction force during the stance phase of running by inputting the joint angles of the lower limbs. The study also explored the impact of various joint angles on different planes on the prediction results. Additionally, we tested the predictive performance of the developed model across five distinct running speeds. The current findings may not only contribute to alternatives to the traditional mode of measuring the GRF with force plates in a laboratory but provide a reference for the setting of sport analysis equipment and the rehabilitation and training of athletes or sport injury patients.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>T.C. and D.X. collaborated on the design and performance of the experiments; T.C., S.S. and Z.Z. performed the research; S.S. and H.Z. provided help and advice on the research study; T.C., S.S. and Z.Z. analyzed the data; and T.C., D.X., Z.Z. and Y.G. wrote the manuscript. All authors contributed to editorial changes in the manuscript. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>This study complied with the principles laid down in the Declaration of Helsinki. Ningbo University&#x02019;s Ethics Committee accepted the study protocol (Approval Number: TY2024037), and all subjects supplied and signed a written informed permission form.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">GRF</td><td align="left" valign="middle" rowspan="1" colspan="1">Ground Reaction Force</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">IMU</td><td align="left" valign="middle" rowspan="1" colspan="1">Inertial Measurement Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CNN</td><td align="left" valign="middle" rowspan="1" colspan="1">Convolutional Neural Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">LSTM</td><td align="left" valign="middle" rowspan="1" colspan="1">Long Short-Term Memory</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">xLSTM</td><td align="left" valign="middle" rowspan="1" colspan="1">Extended Long Short-Term Memory</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">sLSTM</td><td align="left" valign="middle" rowspan="1" colspan="1">Scalar Long Short-Term Memory</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">mLSTM</td><td align="left" valign="middle" rowspan="1" colspan="1">Matrix Long Short-Term Memory</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">R<sup>2</sup></td><td align="left" valign="middle" rowspan="1" colspan="1">Squared Correlation Coefficient</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">MAPE</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean Absolute Percentage Error</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">rMSE</td><td align="left" valign="middle" rowspan="1" colspan="1">root Mean Squared Error</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-25-01249"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Van Oeveren</surname><given-names>B.T.</given-names></name>
<name><surname>de Ruiter</surname><given-names>C.J.</given-names></name>
<name><surname>Beek</surname><given-names>P.J.</given-names></name>
<name><surname>van Die&#x000eb;n</surname><given-names>J.H.</given-names></name>
</person-group><article-title>The biomechanics of running and running styles: A synthesis</article-title><source>Sports Biomech.</source><year>2024</year><volume>23</volume><fpage>516</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1080/14763141.2021.1873411</pub-id><pub-id pub-id-type="pmid">33663325</pub-id>
</element-citation></ref><ref id="B2-sensors-25-01249"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Souza</surname><given-names>R.B.</given-names></name>
</person-group><article-title>An evidence-based videotaped running biomechanics analysis</article-title><source>Phys. Med. Rehabil. Clin.</source><year>2016</year><volume>27</volume><fpage>217</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1016/j.pmr.2015.08.006</pub-id><pub-id pub-id-type="pmid">26616185</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01249"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Leardini</surname><given-names>A.</given-names></name>
<name><surname>Benedetti</surname><given-names>M.G.</given-names></name>
<name><surname>Berti</surname><given-names>L.</given-names></name>
<name><surname>Bettinelli</surname><given-names>D.</given-names></name>
<name><surname>Nativo</surname><given-names>R.</given-names></name>
<name><surname>Giannini</surname><given-names>S.</given-names></name>
</person-group><article-title>Rear-foot, mid-foot and fore-foot motion during the stance phase of gait</article-title><source>Gait Posture</source><year>2007</year><volume>25</volume><fpage>453</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1016/j.gaitpost.2006.05.017</pub-id><pub-id pub-id-type="pmid">16965916</pub-id>
</element-citation></ref><ref id="B4-sensors-25-01249"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Van Hooren</surname><given-names>B.</given-names></name>
<name><surname>Bosch</surname><given-names>F.</given-names></name>
</person-group><article-title>Is there really an eccentric action of the hamstrings during the swing phase of high-speed running? Part I: A critical review of the literature</article-title><source>J. Sports Sci.</source><year>2017</year><volume>35</volume><fpage>2313</fpage><lpage>2321</lpage><pub-id pub-id-type="doi">10.1080/02640414.2016.1266018</pub-id><pub-id pub-id-type="pmid">27937671</pub-id>
</element-citation></ref><ref id="B5-sensors-25-01249"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Ugbolue</surname><given-names>U.C.</given-names></name>
</person-group><article-title>Is there a relationship between strike pattern and injury during running: A review</article-title><source>Phys. Act. Health</source><year>2019</year><volume>3</volume><fpage>127</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.5334/paah.45</pub-id></element-citation></ref><ref id="B6-sensors-25-01249"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Moore</surname><given-names>I.S.</given-names></name>
</person-group><article-title>Is there an economical running technique? A review of modifiable biomechanical factors affecting running economy</article-title><source>Sports Med.</source><year>2016</year><volume>46</volume><fpage>793</fpage><lpage>807</lpage><pub-id pub-id-type="doi">10.1007/s40279-016-0474-4</pub-id><pub-id pub-id-type="pmid">26816209</pub-id>
</element-citation></ref><ref id="B7-sensors-25-01249"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Clark</surname><given-names>K.P.</given-names></name>
<name><surname>Ryan</surname><given-names>L.J.</given-names></name>
<name><surname>Weyand</surname><given-names>P.G.</given-names></name>
</person-group><article-title>Foot speed, foot-strike and footwear: Linking gait mechanics and running ground reaction forces</article-title><source>J. Exp. Biol.</source><year>2014</year><volume>217</volume><fpage>2037</fpage><lpage>2040</lpage><pub-id pub-id-type="pmid">24737756</pub-id>
</element-citation></ref><ref id="B8-sensors-25-01249"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>P.</given-names></name>
<name><surname>Cen</surname><given-names>X.</given-names></name>
<name><surname>Mei</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>A.</given-names></name>
<name><surname>Gu</surname><given-names>Y.</given-names></name>
<name><surname>Fernandez</surname><given-names>J.</given-names></name>
</person-group><article-title>Differences in intra-foot movement strategies during locomotive tasks among chronic ankle instability, copers and healthy individuals</article-title><source>J. Biomech.</source><year>2024</year><volume>162</volume><elocation-id>111865</elocation-id><pub-id pub-id-type="doi">10.1016/j.jbiomech.2023.111865</pub-id><pub-id pub-id-type="pmid">37976687</pub-id>
</element-citation></ref><ref id="B9-sensors-25-01249"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gruber</surname><given-names>A.H.</given-names></name>
<name><surname>Edwards</surname><given-names>W.B.</given-names></name>
<name><surname>Hamill</surname><given-names>J.</given-names></name>
<name><surname>Derrick</surname><given-names>T.R.</given-names></name>
<name><surname>Boyer</surname><given-names>K.A.</given-names></name>
</person-group><article-title>A comparison of the ground reaction force frequency content during rearfoot and non-rearfoot running patterns</article-title><source>Gait Posture</source><year>2017</year><volume>56</volume><fpage>54</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1016/j.gaitpost.2017.04.037</pub-id><pub-id pub-id-type="pmid">28499137</pub-id>
</element-citation></ref><ref id="B10-sensors-25-01249"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hall</surname><given-names>J.P.</given-names></name>
<name><surname>Barton</surname><given-names>C.</given-names></name>
<name><surname>Jones</surname><given-names>P.R.</given-names></name>
<name><surname>Morrissey</surname><given-names>D.</given-names></name>
</person-group><article-title>The biomechanical differences between barefoot and shod distance running: A systematic review and preliminary meta-analysis</article-title><source>Sports Med.</source><year>2013</year><volume>43</volume><fpage>1335</fpage><lpage>1353</lpage><pub-id pub-id-type="doi">10.1007/s40279-013-0084-3</pub-id><pub-id pub-id-type="pmid">23996137</pub-id>
</element-citation></ref><ref id="B11-sensors-25-01249"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tongen</surname><given-names>A.</given-names></name>
<name><surname>Wunderlich</surname><given-names>R.E.</given-names></name>
</person-group><article-title>Biomechanics of running and walking</article-title><source>Math. Sports</source><year>2010</year><volume>43</volume><fpage>315</fpage><lpage>327</lpage></element-citation></ref><ref id="B12-sensors-25-01249"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mizoguchi</surname><given-names>M.</given-names></name>
<name><surname>Calame</surname><given-names>C.</given-names></name>
</person-group><article-title>Possibilities and limitation of today&#x02019;s force plate technology</article-title><source>Gait Posture</source><year>1995</year><volume>4</volume><fpage>268</fpage><pub-id pub-id-type="doi">10.1016/0966-6362(96)82863-3</pub-id></element-citation></ref><ref id="B13-sensors-25-01249"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Oh</surname><given-names>S.E.</given-names></name>
<name><surname>Choi</surname><given-names>A.</given-names></name>
<name><surname>Mun</surname><given-names>J.H.</given-names></name>
</person-group><article-title>Prediction of ground reaction forces during gait based on kinematics and a neural network model</article-title><source>J. Biomech.</source><year>2013</year><volume>46</volume><fpage>2372</fpage><lpage>2380</lpage><pub-id pub-id-type="doi">10.1016/j.jbiomech.2013.07.036</pub-id><pub-id pub-id-type="pmid">23962528</pub-id>
</element-citation></ref><ref id="B14-sensors-25-01249"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fluit</surname><given-names>R.</given-names></name>
<name><surname>Andersen</surname><given-names>M.S.</given-names></name>
<name><surname>Kolk</surname><given-names>S.</given-names></name>
<name><surname>Verdonschot</surname><given-names>N.</given-names></name>
<name><surname>Koopman</surname><given-names>H.F.</given-names></name>
</person-group><article-title>Prediction of ground reaction forces and moments during various activities of daily living</article-title><source>J. Biomech.</source><year>2014</year><volume>47</volume><fpage>2321</fpage><lpage>2329</lpage><pub-id pub-id-type="doi">10.1016/j.jbiomech.2014.04.030</pub-id><pub-id pub-id-type="pmid">24835471</pub-id>
</element-citation></ref><ref id="B15-sensors-25-01249"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>D.</given-names></name>
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Quan</surname><given-names>W.</given-names></name>
<name><surname>Gusztav</surname><given-names>F.</given-names></name>
<name><surname>Baker</surname><given-names>J.S.</given-names></name>
<name><surname>Gu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Adaptive neuro-fuzzy inference system model driven by the non-negative matrix factorization-extracted muscle synergy patterns to estimate lower limb joint movements</article-title><source>Comput. Methods Programs Biomed.</source><year>2023</year><volume>242</volume><elocation-id>107848</elocation-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2023.107848</pub-id><pub-id pub-id-type="pmid">37863010</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01249"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chaaban</surname><given-names>C.R.</given-names></name>
<name><surname>Berry</surname><given-names>N.T.</given-names></name>
<name><surname>Armitano-Lago</surname><given-names>C.</given-names></name>
<name><surname>Kiefer</surname><given-names>A.W.</given-names></name>
<name><surname>Mazzoleni</surname><given-names>M.J.</given-names></name>
<name><surname>Padua</surname><given-names>D.A.</given-names></name>
</person-group><article-title>Combining inertial sensors and machine learning to predict vGRF and knee biomechanics during a double limb jump landing task</article-title><source>Sensors</source><year>2021</year><volume>21</volume><elocation-id>4383</elocation-id><pub-id pub-id-type="doi">10.3390/s21134383</pub-id><pub-id pub-id-type="pmid">34206782</pub-id>
</element-citation></ref><ref id="B17-sensors-25-01249"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Greenhalgh</surname><given-names>A.</given-names></name>
<name><surname>Sinclair</surname><given-names>J.</given-names></name>
<name><surname>Protheroe</surname><given-names>L.</given-names></name>
<name><surname>Chockalingam</surname><given-names>N.</given-names></name>
</person-group><article-title>Predicting impact shock magnitude: Which ground reaction force variable should we use</article-title><source>Int. J. Sports Sci. Eng.</source><year>2012</year><volume>6</volume><fpage>225</fpage><lpage>231</lpage></element-citation></ref><ref id="B18-sensors-25-01249"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>C.D.</given-names></name>
<name><surname>Tenforde</surname><given-names>A.S.</given-names></name>
<name><surname>Outerleys</surname><given-names>J.</given-names></name>
<name><surname>Reilly</surname><given-names>J.</given-names></name>
<name><surname>Davis</surname><given-names>I.S.</given-names></name>
</person-group><article-title>Impact-related ground reaction forces are more strongly associated with some running injuries than others</article-title><source>Am. J. Sports Med.</source><year>2020</year><volume>48</volume><fpage>3072</fpage><lpage>3080</lpage><pub-id pub-id-type="doi">10.1177/0363546520950731</pub-id><pub-id pub-id-type="pmid">32915664</pub-id>
</element-citation></ref><ref id="B19-sensors-25-01249"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>D.</given-names></name>
<name><surname>Quan</surname><given-names>W.</given-names></name>
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Sun</surname><given-names>D.</given-names></name>
<name><surname>Baker</surname><given-names>J.S.</given-names></name>
<name><surname>Gu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Explaining the differences of gait patterns between high and low-mileage runners with machine learning</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><elocation-id>2981</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-022-07054-1</pub-id><pub-id pub-id-type="pmid">35194121</pub-id>
</element-citation></ref><ref id="B20-sensors-25-01249"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>D.</given-names></name>
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Quan</surname><given-names>W.</given-names></name>
<name><surname>Gusztav</surname><given-names>F.</given-names></name>
<name><surname>Wang</surname><given-names>M.</given-names></name>
<name><surname>Baker</surname><given-names>J.S.</given-names></name>
<name><surname>Gu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Accurately and effectively predict the ACL force: Utilizing biomechanical landing pattern before and after-fatigue</article-title><source>Comput. Methods Programs Biomed.</source><year>2023</year><volume>241</volume><elocation-id>107761</elocation-id><pub-id pub-id-type="doi">10.1016/j.cmpb.2023.107761</pub-id><pub-id pub-id-type="pmid">37579552</pub-id>
</element-citation></ref><ref id="B21-sensors-25-01249"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>D.</given-names></name>
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Quan</surname><given-names>W.</given-names></name>
<name><surname>Jiang</surname><given-names>X.</given-names></name>
<name><surname>Liang</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
<name><surname>Ugbolue</surname><given-names>U.C.</given-names></name>
<name><surname>Baker</surname><given-names>J.S.</given-names></name>
<name><surname>Gusztav</surname><given-names>F.</given-names></name>
<name><surname>Ma</surname><given-names>X.</given-names></name>
</person-group><article-title>A new method proposed for realizing human gait pattern recognition: Inspirations for the application of sports and clinical gait analysis</article-title><source>Gait Posture</source><year>2024</year><volume>107</volume><fpage>293</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1016/j.gaitpost.2023.10.019</pub-id><pub-id pub-id-type="pmid">37926657</pub-id>
</element-citation></ref><ref id="B22-sensors-25-01249"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>D.</given-names></name>
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Quan</surname><given-names>W.</given-names></name>
<name><surname>Ma</surname><given-names>X.</given-names></name>
<name><surname>Chon</surname><given-names>T.-E.</given-names></name>
<name><surname>Fernandez</surname><given-names>J.</given-names></name>
<name><surname>Gusztav</surname><given-names>F.</given-names></name>
<name><surname>Kov&#x000e1;cs</surname><given-names>A.</given-names></name>
<name><surname>Baker</surname><given-names>J.S.</given-names></name>
<name><surname>Gu</surname><given-names>Y.</given-names></name>
</person-group><article-title>New insights optimize landing strategies to reduce lower limb injury risk</article-title><source>Cyborg Bionic Syst.</source><year>2024</year><volume>5</volume><elocation-id>0126</elocation-id><pub-id pub-id-type="doi">10.34133/cbsystems.0126</pub-id><pub-id pub-id-type="pmid">38778877</pub-id>
</element-citation></ref><ref id="B23-sensors-25-01249"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Halilaj</surname><given-names>E.</given-names></name>
<name><surname>Rajagopal</surname><given-names>A.</given-names></name>
<name><surname>Fiterau</surname><given-names>M.</given-names></name>
<name><surname>Hicks</surname><given-names>J.L.</given-names></name>
<name><surname>Hastie</surname><given-names>T.J.</given-names></name>
<name><surname>Delp</surname><given-names>S.L.</given-names></name>
</person-group><article-title>Machine learning in human movement biomechanics: Best practices, common pitfalls, and new opportunities</article-title><source>J. Biomech.</source><year>2018</year><volume>81</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.jbiomech.2018.09.009</pub-id><pub-id pub-id-type="pmid">30279002</pub-id>
</element-citation></ref><ref id="B24-sensors-25-01249"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Scheltinga</surname><given-names>B.L.</given-names></name>
<name><surname>Kok</surname><given-names>J.N.</given-names></name>
<name><surname>Buurke</surname><given-names>J.H.</given-names></name>
<name><surname>Reenalda</surname><given-names>J.</given-names></name>
</person-group><article-title>Estimating 3D ground reaction forces in running using three inertial measurement units</article-title><source>Front. Sports Act. Living</source><year>2023</year><volume>5</volume><elocation-id>1176466</elocation-id><pub-id pub-id-type="doi">10.3389/fspor.2023.1176466</pub-id><pub-id pub-id-type="pmid">37255726</pub-id>
</element-citation></ref><ref id="B25-sensors-25-01249"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Pogson</surname><given-names>M.</given-names></name>
<name><surname>Verheul</surname><given-names>J.</given-names></name>
<name><surname>Robinson</surname><given-names>M.A.</given-names></name>
<name><surname>Vanrenterghem</surname><given-names>J.</given-names></name>
<name><surname>Lisboa</surname><given-names>P.</given-names></name>
</person-group><article-title>A neural network method to predict task-and step-specific ground reaction force magnitudes from trunk accelerations during running activities</article-title><source>Med. Eng. Phys.</source><year>2020</year><volume>78</volume><fpage>82</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1016/j.medengphy.2020.02.002</pub-id><pub-id pub-id-type="pmid">32115354</pub-id>
</element-citation></ref><ref id="B26-sensors-25-01249"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bogaert</surname><given-names>S.</given-names></name>
<name><surname>Davis</surname><given-names>J.</given-names></name>
<name><surname>Vanwanseele</surname><given-names>B.</given-names></name>
</person-group><article-title>Predicting vertical ground reaction force characteristics during running with machine learning</article-title><source>Front. Bioeng. Biotechnol.</source><year>2024</year><volume>12</volume><elocation-id>1440033</elocation-id><pub-id pub-id-type="doi">10.3389/fbioe.2024.1440033</pub-id><pub-id pub-id-type="pmid">39439554</pub-id>
</element-citation></ref><ref id="B27-sensors-25-01249"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Du</surname><given-names>X.</given-names></name>
<name><surname>Vasudevan</surname><given-names>R.</given-names></name>
<name><surname>Johnson-Roberson</surname><given-names>M.</given-names></name>
</person-group><article-title>Bio-lstm: A biomechanically inspired recurrent neural network for 3-d pedestrian pose and gait prediction</article-title><source>IEEE Robot. Autom. Lett.</source><year>2019</year><volume>4</volume><fpage>1501</fpage><lpage>1508</lpage><pub-id pub-id-type="doi">10.1109/LRA.2019.2895266</pub-id></element-citation></ref><ref id="B28-sensors-25-01249"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gers</surname><given-names>F.A.</given-names></name>
<name><surname>Schmidhuber</surname><given-names>J.</given-names></name>
<name><surname>Cummins</surname><given-names>F.</given-names></name>
</person-group><article-title>Learning to forget: Continual prediction with LSTM</article-title><source>Neural Comput.</source><year>2000</year><volume>12</volume><fpage>2451</fpage><lpage>2471</lpage><pub-id pub-id-type="doi">10.1162/089976600300015015</pub-id><pub-id pub-id-type="pmid">11032042</pub-id>
</element-citation></ref><ref id="B29-sensors-25-01249"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alcantara</surname><given-names>R.S.</given-names></name>
<name><surname>Edwards</surname><given-names>W.B.</given-names></name>
<name><surname>Millet</surname><given-names>G.Y.</given-names></name>
<name><surname>Grabowski</surname><given-names>A.M.</given-names></name>
</person-group><article-title>Predicting continuous ground reaction forces from accelerometers during uphill and downhill running: A recurrent neural network solution</article-title><source>PeerJ</source><year>2022</year><volume>10</volume><fpage>e12752</fpage><pub-id pub-id-type="doi">10.7717/peerj.12752</pub-id><pub-id pub-id-type="pmid">35036107</pub-id>
</element-citation></ref><ref id="B30-sensors-25-01249"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Donahue</surname><given-names>S.R.</given-names></name>
<name><surname>Hahn</surname><given-names>M.E.</given-names></name>
</person-group><article-title>Estimation of ground reaction force waveforms during fixed pace running outside the laboratory</article-title><source>Front. Sports Act. Living</source><year>2023</year><volume>5</volume><elocation-id>974186</elocation-id><pub-id pub-id-type="doi">10.3389/fspor.2023.974186</pub-id><pub-id pub-id-type="pmid">36860734</pub-id>
</element-citation></ref><ref id="B31-sensors-25-01249"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Beck</surname><given-names>M.</given-names></name>
<name><surname>P&#x000f6;ppel</surname><given-names>K.</given-names></name>
<name><surname>Spanring</surname><given-names>M.</given-names></name>
<name><surname>Auer</surname><given-names>A.</given-names></name>
<name><surname>Prudnikova</surname><given-names>O.</given-names></name>
<name><surname>Kopp</surname><given-names>M.</given-names></name>
<name><surname>Klambauer</surname><given-names>G.</given-names></name>
<name><surname>Brandstetter</surname><given-names>J.</given-names></name>
<name><surname>Hochreiter</surname><given-names>S.</given-names></name>
</person-group><article-title>xLSTM: Extended Long Short-Term Memory</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2405.04517</pub-id></element-citation></ref><ref id="B32-sensors-25-01249"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alkin</surname><given-names>B.</given-names></name>
<name><surname>Beck</surname><given-names>M.</given-names></name>
<name><surname>P&#x000f6;ppel</surname><given-names>K.</given-names></name>
<name><surname>Hochreiter</surname><given-names>S.</given-names></name>
<name><surname>Brandstetter</surname><given-names>J.</given-names></name>
</person-group><article-title>Vision-LSTM: xLSTM as Generic Vision Backbone</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2406.04303</pub-id></element-citation></ref><ref id="B33-sensors-25-01249"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alharthi</surname><given-names>M.</given-names></name>
<name><surname>Mahmood</surname><given-names>A.</given-names></name>
</person-group><article-title>xlstmtime: Long-term time series forecasting with xlstm</article-title><source>AI</source><year>2024</year><volume>5</volume><fpage>1482</fpage><lpage>1495</lpage><pub-id pub-id-type="doi">10.3390/ai5030071</pub-id></element-citation></ref><ref id="B34-sensors-25-01249"><label>34.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>T.</given-names></name>
<name><surname>Ding</surname><given-names>C.</given-names></name>
<name><surname>Zhu</surname><given-names>L.</given-names></name>
<name><surname>Xu</surname><given-names>T.</given-names></name>
<name><surname>Yan</surname><given-names>W.</given-names></name>
<name><surname>Ji</surname><given-names>D.</given-names></name>
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Zang</surname><given-names>Y.</given-names></name>
</person-group><article-title>xLSTM-UNet can be an Effective Backbone for 2D &#x00026; 3D Biomedical Image Segmentation Better than its Mamba Counterparts</article-title><source>Proceedings of the IEEE-EMBS International Conference on Biomedical and Health Informatics</source><conf-loc>Houston, TX, USA</conf-loc><conf-date>10&#x02013;13 November 2024</conf-date></element-citation></ref><ref id="B35-sensors-25-01249"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Fan</surname><given-names>X.</given-names></name>
<name><surname>Tao</surname><given-names>C.</given-names></name>
<name><surname>Zhao</surname><given-names>J.</given-names></name>
</person-group><article-title>Advanced stock price prediction with xlstm-based models: Improving long-term forecasting</article-title><source>Proceedings of the 2024 11th International Conference on Soft Computing &#x00026; Machine Intelligence (ISCMI)</source><conf-loc>Melbourne, Australia</conf-loc><conf-date>22&#x02013;23 November 2024</conf-date></element-citation></ref><ref id="B36-sensors-25-01249"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>W.</given-names></name>
<name><surname>Lian</surname><given-names>C.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
<name><surname>Zhan</surname><given-names>Z.</given-names></name>
</person-group><article-title>Sensor-Based Gymnastics Action Recognition Using Time-Series Images and a Lightweight Feature Fusion Network</article-title><source>IEEE Sens. J.</source><year>2024</year><volume>24</volume><fpage>42573</fpage><lpage>42583</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2024.3492004</pub-id></element-citation></ref><ref id="B37-sensors-25-01249"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Giarmatzis</surname><given-names>G.</given-names></name>
<name><surname>Zacharaki</surname><given-names>E.I.</given-names></name>
<name><surname>Moustakas</surname><given-names>K.</given-names></name>
</person-group><article-title>Real-time prediction of joint forces by motion capture and machine learning</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>6933</elocation-id><pub-id pub-id-type="doi">10.3390/s20236933</pub-id><pub-id pub-id-type="pmid">33291594</pub-id>
</element-citation></ref><ref id="B38-sensors-25-01249"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Stetter</surname><given-names>B.J.</given-names></name>
<name><surname>Ringhof</surname><given-names>S.</given-names></name>
<name><surname>Krafft</surname><given-names>F.C.</given-names></name>
<name><surname>Sell</surname><given-names>S.</given-names></name>
<name><surname>Stein</surname><given-names>T.</given-names></name>
</person-group><article-title>Estimation of knee joint forces in sport movements using wearable sensors and machine learning</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>3690</elocation-id><pub-id pub-id-type="doi">10.3390/s19173690</pub-id><pub-id pub-id-type="pmid">31450664</pub-id>
</element-citation></ref><ref id="B39-sensors-25-01249"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Z.</given-names></name>
<name><surname>Qiu</surname><given-names>Q.</given-names></name>
<name><surname>Chen</surname><given-names>S.</given-names></name>
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Lv</surname><given-names>X.</given-names></name>
</person-group><article-title>Effects of unstable shoes on lower limbs with different speeds</article-title><source>Phys. Act. Health</source><year>2019</year><volume>3</volume><fpage>82</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.5334/paah.41</pub-id></element-citation></ref><ref id="B40-sensors-25-01249"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>De Wit</surname><given-names>B.</given-names></name>
<name><surname>De Clercq</surname><given-names>D.</given-names></name>
<name><surname>Aerts</surname><given-names>P.</given-names></name>
</person-group><article-title>Biomechanical analysis of the stance phase during barefoot and shod running</article-title><source>J. Biomech.</source><year>2000</year><volume>33</volume><fpage>269</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1016/S0021-9290(99)00192-X</pub-id><pub-id pub-id-type="pmid">10673110</pub-id>
</element-citation></ref><ref id="B41-sensors-25-01249"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Alzubaidi</surname><given-names>L.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Humaidi</surname><given-names>A.J.</given-names></name>
<name><surname>Al-Dujaili</surname><given-names>A.</given-names></name>
<name><surname>Duan</surname><given-names>Y.</given-names></name>
<name><surname>Al-Shamma</surname><given-names>O.</given-names></name>
<name><surname>Santamar&#x000ed;a</surname><given-names>J.</given-names></name>
<name><surname>Fadhel</surname><given-names>M.A.</given-names></name>
<name><surname>Al-Amidie</surname><given-names>M.</given-names></name>
<name><surname>Farhan</surname><given-names>L.</given-names></name>
</person-group><article-title>Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions</article-title><source>J. Big Data</source><year>2021</year><volume>8</volume><fpage>53</fpage><pub-id pub-id-type="doi">10.1186/s40537-021-00444-8</pub-id><pub-id pub-id-type="pmid">33816053</pub-id>
</element-citation></ref><ref id="B42-sensors-25-01249"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Fernandez</surname><given-names>J.</given-names></name>
</person-group><article-title>Randomized Controlled Trial of Gastrocnemius Muscle Analysis Using Surface Electromyography and Ultrasound in Different Striking Patterns of Young Women&#x02019;s Barefoot Running</article-title><source>Phys. Act. Health</source><year>2024</year><volume>8</volume><fpage>223</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.5334/paah.382</pub-id></element-citation></ref><ref id="B43-sensors-25-01249"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chang</surname><given-names>H.</given-names></name>
<name><surname>Cen</surname><given-names>X.</given-names></name>
</person-group><article-title>Can running technique modification benefit patellofemoral pain improvement in runners? A systematic review and meta-analysis</article-title><source>Int. J. Biomed. Eng. Technol.</source><year>2024</year><volume>45</volume><fpage>83</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1504/IJBET.2024.138706</pub-id></element-citation></ref><ref id="B44-sensors-25-01249"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xu</surname><given-names>D.</given-names></name>
<name><surname>Lu</surname><given-names>J.</given-names></name>
<name><surname>Baker</surname><given-names>J.S.</given-names></name>
<name><surname>Fekete</surname><given-names>G.</given-names></name>
<name><surname>Gu</surname><given-names>Y.</given-names></name>
</person-group><article-title>Temporal kinematic and kinetics differences throughout different landing ways following volleyball spike shots</article-title><source>Proc. Inst. Mech. Eng. Part P</source><year>2022</year><volume>236</volume><fpage>200</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1177/17543371211009485</pub-id></element-citation></ref><ref id="B45-sensors-25-01249"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fang</surname><given-names>Z.</given-names></name>
<name><surname>Shi</surname><given-names>K.</given-names></name>
<name><surname>Han</surname><given-names>Q.</given-names></name>
</person-group><article-title>When Mamba Meets xLSTM: An Efficient and Precise Method with the XLSTM-VMUNet Model for Skin lesion Segmentation</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2411.09363</pub-id></element-citation></ref><ref id="B46-sensors-25-01249"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>Q.</given-names></name>
<name><surname>Cai</surname><given-names>Y.</given-names></name>
<name><surname>Fan</surname><given-names>L.</given-names></name>
</person-group><article-title>Seg-LSTM: Performance of xLSTM for Semantic Segmentation of Remotely Sensed Images</article-title><source>arXiv</source><year>2024</year><pub-id pub-id-type="arxiv">2406.14086</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01249-f001"><label>Figure 1</label><caption><p>An illustration of the study&#x02019;s structure. The study was divided into 3 parts. (<bold>A</bold>) The data collection procedure. Xsens sensors and a Vicon 3D motion capture system were used to collect the joint angle and vertical ground reaction force data. (<bold>B</bold>) The data preprocessing procedure. (<bold>C</bold>) The GRF prediction procedure. The datasets were set as the input of 4 deep learning models (CNN-xLSTM, CNN-sLSTM, CNN-mLSTM, and CNN-LSTM) in order to compare and analyze the performance of models in the vertical-GRF prediction tasks. (<bold>D</bold>) The process for each deep learning model. (<bold>E</bold>) The workflow of the entire study.</p></caption><graphic xlink:href="sensors-25-01249-g001" position="float"/></fig><fig position="float" id="sensors-25-01249-f002"><label>Figure 2</label><caption><p>The structure of the 4 deep learning models. (<bold>A</bold>) The CNN block was combined with xLSTM, sLSTM, mLSTM, and LSTM for feature extraction and vertical-GRF prediction. (<bold>B</bold>) The basic unit of xLSTM, which is a hybrid of sLSTM and mLSTM. (<bold>C</bold>) The basic unit of sLSTM. (<bold>D</bold>) The basic unit of mLSTM. (<bold>E</bold>) The basic unit of LSTM.</p></caption><graphic xlink:href="sensors-25-01249-g002" position="float"/></fig><fig position="float" id="sensors-25-01249-f003"><label>Figure 3</label><caption><p>The visualization of the training loss and testing loss curve for each deep learning model in each epoch.</p></caption><graphic xlink:href="sensors-25-01249-g003" position="float"/></fig><fig position="float" id="sensors-25-01249-f004"><label>Figure 4</label><caption><p>Visualization of the vertical-GRF prediction results obtained from different inputs (M<sub>1 (3<italic toggle="yes">Joints</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>2 (<italic toggle="yes">Ankle</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>3 (<italic toggle="yes">Hip</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>4 (<italic toggle="yes">Knee</italic>, 3<italic toggle="yes">Planes</italic>)</sub>, M<sub>5 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Sagittal</italic>)</sub>, M<sub>6 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Frontal</italic>),</sub> and M<sub>7 (3<italic toggle="yes">Joints</italic>, <italic toggle="yes">Transversal</italic>)</sub>). Each diagram shows the prediction results and the R<sup>2</sup> of the 4 different deep learning models.</p></caption><graphic xlink:href="sensors-25-01249-g004" position="float"/></fig><fig position="float" id="sensors-25-01249-f005"><label>Figure 5</label><caption><p>Visualization of the MAPE and rMSE obtained from 4 deep learning models (CNN-xLSTM, CNN-sLSTM, CNN-mLSTM, and CNN-LSTM). The lower the MAPE and rMSE, the better the performance of models in fitting the vertical-GRF curve. (<bold>A</bold>) MAPE of each model with different inputs. (<bold>B</bold>) rMSE of each model with different inputs.</p></caption><graphic xlink:href="sensors-25-01249-g005" position="float"/></fig><fig position="float" id="sensors-25-01249-f006"><label>Figure 6</label><caption><p>The visualization of the R<sup>2</sup>, MAPE, and rMSE of CNN-xLSTM at 5 running speeds in the 4 cases that fitted the vertical-GRF curve better.</p></caption><graphic xlink:href="sensors-25-01249-g006" position="float"/></fig><table-wrap position="float" id="sensors-25-01249-t001"><object-id pub-id-type="pii">sensors-25-01249-t001_Table 1</object-id><label>Table 1</label><caption><p>Optimal parameter configuration of the four deep learning models used in this study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Batch Size</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hidden Size</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Epochs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Stacked Layers</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Module</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN-xLSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">128</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02018;m&#x02019;, &#x02019;s&#x02019;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN-sLSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">128</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02018;s&#x02019;</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN-mLSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">128</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td><td align="center" valign="middle" rowspan="1" colspan="1">25</td><td align="center" valign="middle" rowspan="1" colspan="1">1</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x02018;m&#x02019;</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">CNN-LSTM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">256</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">128</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">55</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01249-t002"><object-id pub-id-type="pii">sensors-25-01249-t002_Table 2</object-id><label>Table 2</label><caption><p>The mean value and standard deviation of the R<sup>2</sup>, rMSE, and MAPE of 4 deep learning models trained by 7 datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Models</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Index</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">M1</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">M2</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">M3</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">M4</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">M5</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">M6</th><th align="center" valign="top" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">M7</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" colspan="1">CNN-xLSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">R<sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.909 &#x000b1; 0.064</td><td align="center" valign="middle" rowspan="1" colspan="1">0.814 &#x000b1; 0.053</td><td align="center" valign="middle" rowspan="1" colspan="1">0.757 &#x000b1; 0.092</td><td align="center" valign="middle" rowspan="1" colspan="1">0.709 &#x000b1; 0.074</td><td align="center" valign="middle" rowspan="1" colspan="1">0.836 &#x000b1; 0.042</td><td align="center" valign="middle" rowspan="1" colspan="1">0.861 &#x000b1; 0.057</td><td align="center" valign="middle" rowspan="1" colspan="1">0.671 &#x000b1; 0.051</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MAPE</td><td align="center" valign="middle" rowspan="1" colspan="1">2.18 &#x000b1; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">4.38 &#x000b1; 0.21</td><td align="center" valign="middle" rowspan="1" colspan="1">9.95 &#x000b1; 0.34</td><td align="center" valign="middle" rowspan="1" colspan="1">10.01 &#x000b1; 0.41</td><td align="center" valign="middle" rowspan="1" colspan="1">3.17 &#x000b1; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">3.09 &#x000b1; 0.18</td><td align="center" valign="middle" rowspan="1" colspan="1">12.82 &#x000b1; 0.33</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.061 &#x000b1; 0.008</td><td align="center" valign="middle" rowspan="1" colspan="1">0.089 &#x000b1; 0.010</td><td align="center" valign="middle" rowspan="1" colspan="1">0.119 &#x000b1; 0.014</td><td align="center" valign="middle" rowspan="1" colspan="1">0.132 &#x000b1; 0.021</td><td align="center" valign="middle" rowspan="1" colspan="1">0.074 &#x000b1; 0.007</td><td align="center" valign="middle" rowspan="1" colspan="1">0.070 &#x000b1; 0.006</td><td align="center" valign="middle" rowspan="1" colspan="1">0.138 &#x000b1; 0.017</td></tr><tr><td rowspan="3" align="center" valign="middle" colspan="1">CNN-sLSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">R<sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.748 &#x000b1; 0.056</td><td align="center" valign="middle" rowspan="1" colspan="1">0.702 &#x000b1; 0.044</td><td align="center" valign="middle" rowspan="1" colspan="1">0.622 &#x000b1; 0.069</td><td align="center" valign="middle" rowspan="1" colspan="1">0.656 &#x000b1; 0.046</td><td align="center" valign="middle" rowspan="1" colspan="1">0.711 &#x000b1; 0.055</td><td align="center" valign="middle" rowspan="1" colspan="1">0.729 &#x000b1; 0.073</td><td align="center" valign="middle" rowspan="1" colspan="1">0.491 &#x000b1; 0.067</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MAPE</td><td align="center" valign="middle" rowspan="1" colspan="1">3.71 &#x000b1; 0.32</td><td align="center" valign="middle" rowspan="1" colspan="1">6.74 &#x000b1; 0.59</td><td align="center" valign="middle" rowspan="1" colspan="1">17.06 &#x000b1; 0.60</td><td align="center" valign="middle" rowspan="1" colspan="1">13.88 &#x000b1; 0.77</td><td align="center" valign="middle" rowspan="1" colspan="1">6.27 &#x000b1; 0.47</td><td align="center" valign="middle" rowspan="1" colspan="1">5.78 &#x000b1; 0.58</td><td align="center" valign="middle" rowspan="1" colspan="1">15.06 &#x000b1; 0.76</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.097 &#x000b1; 0.011</td><td align="center" valign="middle" rowspan="1" colspan="1">0.114 &#x000b1; 0.009</td><td align="center" valign="middle" rowspan="1" colspan="1">0.143 &#x000b1; 0.014</td><td align="center" valign="middle" rowspan="1" colspan="1">0.161 &#x000b1; 0.023</td><td align="center" valign="middle" rowspan="1" colspan="1">0.092 &#x000b1; 0.007</td><td align="center" valign="middle" rowspan="1" colspan="1">0.094 &#x000b1; 0.005</td><td align="center" valign="middle" rowspan="1" colspan="1">0.168 &#x000b1; 0.028</td></tr><tr><td rowspan="3" align="center" valign="middle" colspan="1">CNN-mLSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">R<sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.791 &#x000b1; 0.027</td><td align="center" valign="middle" rowspan="1" colspan="1">0.713 &#x000b1; 0.031</td><td align="center" valign="middle" rowspan="1" colspan="1">0.608 &#x000b1; 0.035</td><td align="center" valign="middle" rowspan="1" colspan="1">0.627 &#x000b1; 0.025</td><td align="center" valign="middle" rowspan="1" colspan="1">0.709 &#x000b1; 0.031</td><td align="center" valign="middle" rowspan="1" colspan="1">0.721 &#x000b1; 0.027</td><td align="center" valign="middle" rowspan="1" colspan="1">0.626 &#x000b1; 0.031</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MAPE</td><td align="center" valign="middle" rowspan="1" colspan="1">5.56 &#x000b1; 0.38</td><td align="center" valign="middle" rowspan="1" colspan="1">8.58 &#x000b1; 1.04</td><td align="center" valign="middle" rowspan="1" colspan="1">19.49 &#x000b1; 1.13</td><td align="center" valign="middle" rowspan="1" colspan="1">17.39 &#x000b1; 0.93</td><td align="center" valign="middle" rowspan="1" colspan="1">9.19 &#x000b1; 0.72</td><td align="center" valign="middle" rowspan="1" colspan="1">10.20 &#x000b1; 0.97</td><td align="center" valign="middle" rowspan="1" colspan="1">16.38 &#x000b1; 1.45</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.092 &#x000b1; 0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.112 &#x000b1; 0.013</td><td align="center" valign="middle" rowspan="1" colspan="1">0.178 &#x000b1; 0.032</td><td align="center" valign="middle" rowspan="1" colspan="1">0.209 &#x000b1; 0.014</td><td align="center" valign="middle" rowspan="1" colspan="1">0.093 &#x000b1; 0.013</td><td align="center" valign="middle" rowspan="1" colspan="1">0.096 &#x000b1; 0.008</td><td align="center" valign="middle" rowspan="1" colspan="1">0.158 &#x000b1; 0.024</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">CNN-LSTM</td><td align="center" valign="middle" rowspan="1" colspan="1">R<sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.742 &#x000b1; 0.040</td><td align="center" valign="middle" rowspan="1" colspan="1">0.683 &#x000b1; 0.032</td><td align="center" valign="middle" rowspan="1" colspan="1">0.619 &#x000b1; 0.034</td><td align="center" valign="middle" rowspan="1" colspan="1">0.628 &#x000b1; 0.041</td><td align="center" valign="middle" rowspan="1" colspan="1">0.728 &#x000b1; 0.039</td><td align="center" valign="middle" rowspan="1" colspan="1">0.717 &#x000b1; 0.041</td><td align="center" valign="middle" rowspan="1" colspan="1">0.653 &#x000b1; 0.042</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MAPE</td><td align="center" valign="middle" rowspan="1" colspan="1">7.17 &#x000b1; 0.45</td><td align="center" valign="middle" rowspan="1" colspan="1">8.12 &#x000b1; 0.53</td><td align="center" valign="middle" rowspan="1" colspan="1">14.44 &#x000b1; 1.05</td><td align="center" valign="middle" rowspan="1" colspan="1">15.07 &#x000b1; 1.50</td><td align="center" valign="middle" rowspan="1" colspan="1">8.50 &#x000b1; 0.80</td><td align="center" valign="middle" rowspan="1" colspan="1">8.89 &#x000b1; 0.49</td><td align="center" valign="middle" rowspan="1" colspan="1">13.54 &#x000b1; 0.98</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">rMSE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.104 &#x000b1; 0.018</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.108 &#x000b1; 0.006</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.135 &#x000b1; 0.029</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.173 &#x000b1; 0.037</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.112 &#x000b1; 0.010</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.105 &#x000b1; 0.008</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.172 &#x000b1; 0.015</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01249-t003"><object-id pub-id-type="pii">sensors-25-01249-t003_Table 3</object-id><label>Table 3</label><caption><p>The mean value and standard deviation of the R<sup>2</sup>, MAPE, and rMSE of CNN-xLSTM at 5 running speeds in the 4 cases that fitted the vertical-GRF curve better.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inputs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Index</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">8 km/h</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">10 km/h</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">12 km/h</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">14 km/h</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">16 km/h</th></tr></thead><tbody><tr><td rowspan="3" align="center" valign="middle" colspan="1">3 Joints and 3 Planes</td><td align="center" valign="middle" rowspan="1" colspan="1">R<sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.842 &#x000b1; 0.047</td><td align="center" valign="middle" rowspan="1" colspan="1">0.854 &#x000b1; 0.043</td><td align="center" valign="middle" rowspan="1" colspan="1">0.879 &#x000b1; 0.068</td><td align="center" valign="middle" rowspan="1" colspan="1">0.861 &#x000b1; 0.054</td><td align="center" valign="middle" rowspan="1" colspan="1">0.836 &#x000b1; 0.032</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MAPE</td><td align="center" valign="middle" rowspan="1" colspan="1">2.45 &#x000b1; 0.18</td><td align="center" valign="middle" rowspan="1" colspan="1">2.33 &#x000b1; 0.09</td><td align="center" valign="middle" rowspan="1" colspan="1">2.19 &#x000b1; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">2.36 &#x000b1; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">2.71 &#x000b1; 0.08</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.069 &#x000b1; 0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.067 &#x000b1; 0.007</td><td align="center" valign="middle" rowspan="1" colspan="1">0.063 &#x000b1; 0.010</td><td align="center" valign="middle" rowspan="1" colspan="1">0.065 &#x000b1; 0.009</td><td align="center" valign="middle" rowspan="1" colspan="1">0.070 &#x000b1; 0.013</td></tr><tr><td rowspan="3" align="center" valign="middle" colspan="1">Ankle and 3 Planes</td><td align="center" valign="middle" rowspan="1" colspan="1">R<sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.801 &#x000b1; 0.037</td><td align="center" valign="middle" rowspan="1" colspan="1">0.804 &#x000b1; 0.039</td><td align="center" valign="middle" rowspan="1" colspan="1">0.811 &#x000b1; 0.036</td><td align="center" valign="middle" rowspan="1" colspan="1">0.807 &#x000b1; 0.044</td><td align="center" valign="middle" rowspan="1" colspan="1">0.793 &#x000b1; 0.043</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MAPE</td><td align="center" valign="middle" rowspan="1" colspan="1">4.69 &#x000b1; 0.22</td><td align="center" valign="middle" rowspan="1" colspan="1">4.63 &#x000b1; 0.26</td><td align="center" valign="middle" rowspan="1" colspan="1">4.45 &#x000b1; 0.19</td><td align="center" valign="middle" rowspan="1" colspan="1">4.59 &#x000b1; 0.18</td><td align="center" valign="middle" rowspan="1" colspan="1">4.78 &#x000b1; 0.20</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.116 &#x000b1; 0.018</td><td align="center" valign="middle" rowspan="1" colspan="1">0.109 &#x000b1; 0.013</td><td align="center" valign="middle" rowspan="1" colspan="1">0.101 &#x000b1; 0.011</td><td align="center" valign="middle" rowspan="1" colspan="1">0.105 &#x000b1; 0.009</td><td align="center" valign="middle" rowspan="1" colspan="1">0.127 &#x000b1; 0.021</td></tr><tr><td rowspan="3" align="center" valign="middle" colspan="1">3 Joints and Sagittal</td><td align="center" valign="middle" rowspan="1" colspan="1">R<sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.819 &#x000b1; 0.62</td><td align="center" valign="middle" rowspan="1" colspan="1">0.821 &#x000b1; 0.058</td><td align="center" valign="middle" rowspan="1" colspan="1">0.831 &#x000b1; 0.055</td><td align="center" valign="middle" rowspan="1" colspan="1">0.829 &#x000b1; 0.060</td><td align="center" valign="middle" rowspan="1" colspan="1">0.813 &#x000b1; 0.052</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MAPE</td><td align="center" valign="middle" rowspan="1" colspan="1">3.73 &#x000b1; 0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">3.39 &#x000b1; 0.15</td><td align="center" valign="middle" rowspan="1" colspan="1">3.24 &#x000b1; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">3.29 &#x000b1; 0.12</td><td align="center" valign="middle" rowspan="1" colspan="1">3.87 &#x000b1; 0.17</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">rMSE</td><td align="center" valign="middle" rowspan="1" colspan="1">0.093 &#x000b1; 0.019</td><td align="center" valign="middle" rowspan="1" colspan="1">0.089 &#x000b1; 0.011</td><td align="center" valign="middle" rowspan="1" colspan="1">0.082 &#x000b1; 0.012</td><td align="center" valign="middle" rowspan="1" colspan="1">0.085 &#x000b1; 0.010</td><td align="center" valign="middle" rowspan="1" colspan="1">0.104 &#x000b1; 0.014</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">3 Joints and Frontal</td><td align="center" valign="middle" rowspan="1" colspan="1">R<sup>2</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.811 &#x000b1; 0.038</td><td align="center" valign="middle" rowspan="1" colspan="1">0.832 &#x000b1; 0.050</td><td align="center" valign="middle" rowspan="1" colspan="1">0.857 &#x000b1; 0.043</td><td align="center" valign="middle" rowspan="1" colspan="1">0.841 &#x000b1; 0.037</td><td align="center" valign="middle" rowspan="1" colspan="1">0.803 &#x000b1; 0.046</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">MAPE</td><td align="center" valign="middle" rowspan="1" colspan="1">3.65 &#x000b1; 0.21</td><td align="center" valign="middle" rowspan="1" colspan="1">3.27 &#x000b1; 0.19</td><td align="center" valign="middle" rowspan="1" colspan="1">3.06 &#x000b1; 0.13</td><td align="center" valign="middle" rowspan="1" colspan="1">3.12 &#x000b1; 0.11</td><td align="center" valign="middle" rowspan="1" colspan="1">3.71 &#x000b1; 0.16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">rMSE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.091 &#x000b1; 0.013</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.085 &#x000b1; 0.017</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.079 &#x000b1; 0.012</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.082 &#x000b1; 0.012</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.093 &#x000b1; 0.011</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01249-t004"><object-id pub-id-type="pii">sensors-25-01249-t004_Table 4</object-id><label>Table 4</label><caption><p>The comparison of performance between CNN-xLSTM and the models mentioned in other studies that aim to predict the vertical GRF.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Studies</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Models</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Inputs</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">MAPE (%)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">rMSE</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Oh et al. (2013) [<xref rid="B13-sensors-25-01249" ref-type="bibr">13</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ANN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Centre of Mass and Acceleration of Segments and Joints</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.982</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.058 &#x000b1; 0.010</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pogson et al. (2020) [<xref rid="B25-sensors-25-01249" ref-type="bibr">25</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">PCA-MLP-ANN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trunk Acceleration</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.900</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Alcantara et al. (2022) [<xref rid="B29-sensors-25-01249" ref-type="bibr">29</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LSTM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Height, Mass, Speed, Slope, and Running Pattern</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.064 &#x000b1; 0.015</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Scheltinga et al. (2023) [<xref rid="B24-sensors-25-01249" ref-type="bibr">24</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ensANN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acceleration of Pelvis and Tibias</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.960</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.066 &#x000b1; 0.001</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Bogaert et al. (2024) [<xref rid="B26-sensors-25-01249" ref-type="bibr">26</xref>]</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lasso</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3-Dimensional Sacral Acceleration</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.870</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.29</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.106</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Donahue et al. (2023) [<xref rid="B30-sensors-25-01249" ref-type="bibr">30</xref>] </td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LSTM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3D accelerations and angular velocities</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">/</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.189</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>This Study</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>CNN-xLSTM</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>Joints Angle of Ankle, Hip, and Knee</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.973</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>2.18 &#x000b1; 0.09</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.061 &#x000b1; 0.008</bold>
</td></tr></tbody></table></table-wrap></floats-group></article>