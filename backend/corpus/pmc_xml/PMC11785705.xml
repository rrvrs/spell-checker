<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Clin Oral Investig</journal-id><journal-id journal-id-type="iso-abbrev">Clin Oral Investig</journal-id><journal-title-group><journal-title>Clinical Oral Investigations</journal-title></journal-title-group><issn pub-type="ppub">1432-6981</issn><issn pub-type="epub">1436-3771</issn><publisher><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin/Heidelberg</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39888441</article-id><article-id pub-id-type="pmc">PMC11785705</article-id>
<article-id pub-id-type="publisher-id">6156</article-id><article-id pub-id-type="doi">10.1007/s00784-025-06156-0</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>DeepLabv3&#x02009;+&#x02009;method for detecting and segmenting apical lesions on panoramic radiography</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9618-1082</contrib-id><name><surname>Ketenci &#x000c7;ay</surname><given-names>Fatmanur</given-names></name><address><email>fatmanur.ketenci@yeditepe.edu.tr</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8961-2673</contrib-id><name><surname>Ye&#x0015f;il</surname><given-names>&#x000c7;a&#x0011f;r&#x00131;</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-4244-6571</contrib-id><name><surname>&#x000c7;ay</surname><given-names>Oktay</given-names></name><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-7316-6708</contrib-id><name><surname>Y&#x00131;lmaz</surname><given-names>B&#x000fc;&#x0015f;ra G&#x000fc;l</given-names></name><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff8">8</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0009-0002-9256-4753</contrib-id><name><surname>&#x000d6;z&#x000e7;ini</surname><given-names>Fatma Hasene</given-names></name><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff9">9</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2050-3425</contrib-id><name><surname>&#x00130;lg&#x000fc;y</surname><given-names>Dilhan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/025mx2575</institution-id><institution-id institution-id-type="GRID">grid.32140.34</institution-id><institution-id institution-id-type="ISNI">0000 0001 0744 4075</institution-id><institution>Department of Dentomaxillofacial Radiology, Faculty of Dentistry, </institution><institution>Yeditepe University, </institution></institution-wrap>Istanbul, Turkey </aff><aff id="Aff2"><label>2</label>SADC Department, Huawei T&#x000fc;rkiye R&#x00026;D Center, Istanbul, Turkey </aff><aff id="Aff3"><label>3</label>&#x0015e;akir G&#x000fc;rkan Family Health Center, Istanbul, Turkey </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03k7bde87</institution-id><institution-id institution-id-type="GRID">grid.488643.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 5894 3909</institution-id><institution>Department of Dentomaxillofacial Radiology, Faculty of Dentistry, </institution><institution>Sa&#x0011f;l&#x00131;k Bilimleri University, </institution></institution-wrap>Istanbul, Turkey </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/025mx2575</institution-id><institution-id institution-id-type="GRID">grid.32140.34</institution-id><institution-id institution-id-type="ISNI">0000 0001 0744 4075</institution-id><institution>Faculty of Dentistry, </institution><institution>Yeditepe University, </institution></institution-wrap>Caddebostan, Ba&#x0011f;dat St. Nu:238, Kad&#x00131;k&#x000f6;y, &#x00130;stanbul, 34728 Turkey </aff><aff id="Aff6"><label>6</label>Huawei T&#x000fc;rkiye, R&#x00026;D Center, &#x000dc;mraniye, Istanbul, Turkey </aff><aff id="Aff7"><label>7</label>F&#x00131;nd&#x00131;kl&#x00131;, Gazi Mustafa Kemal St. Nu:23, Maltepe, &#x00130;stanbul, 34854 Turkey </aff><aff id="Aff8"><label>8</label>Mekteb-i T&#x00131;bbiye-i &#x0015e;ahane (Hamidiye) K&#x000fc;lliyesi Selimiye, T&#x00131;bbiye St. Nu:38, &#x000dc;sk&#x000fc;dar, &#x00130;stanbul, 34668 Turkey </aff><aff id="Aff9"><label>9</label>Mehmet Akif, Tacirler E&#x0011f;itim Vakf&#x00131; Sultanbeyli Devlet Hst., Sultanbeyli, &#x00130;stanbul, 34920 Turkey </aff></contrib-group><pub-date pub-type="epub"><day>31</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>31</day><month>1</month><year>2025</year></pub-date><pub-date pub-type="ppub"><year>2025</year></pub-date><volume>29</volume><issue>2</issue><elocation-id>101</elocation-id><history><date date-type="received"><day>5</day><month>7</month><year>2024</year></date><date date-type="accepted"><day>8</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><sec><title>Objective</title><p id="Par1">This study aimed to apply the DeepLabv3&#x02009;+&#x02009;model and compare it with the U-Net model in terms of detecting and segmenting apical lesions on panoramic radiography.</p></sec><sec><title>Methods</title><p id="Par2">260 panoramic images that contain apical lesions in different regions were collected and randomly divided into training and test datasets. All images were manually annotated for apical lesions using Computer Vision Annotation Tool software by two independent dental radiologists and a master reviewer. The DeepLabv3&#x02009;+&#x02009;model, one of the state-of-the-art deep semantic segmentation models, was utilized using Python programming language and the TensorFlow library and applied to the prepared datasets. The model was compared with the U-Net model applied to apical lesions and other medical image segmentation problems in the literature.</p></sec><sec><title>Results</title><p id="Par3">The DeepLabv3&#x02009;+&#x02009;and U-Net models were applied to the same datasets with the same hyper-parameters. The AUC and recall results of the DeepLabv3&#x02009;+&#x02009;were 29.96% and 61.06% better than the U-Net model. However, the U-Net model gets 69.17% and 25.55% better precision and F1-score results than the DeepLabv3&#x02009;+&#x02009;model. The difference in the IoU results of the models was not statistically significant.</p></sec><sec><title>Conclusions</title><p id="Par4">This paper comprehensively evaluated the DeepLabv3&#x02009;+&#x02009;model and compared it with the U-Net model. Our experimental findings indicated that DeepLabv3&#x02009;+&#x02009;outperforms the U-Net model by a substantial margin for both AUC and recall metrics. According to those results, for detecting apical lesions, we encourage researchers to use and improve the DeepLabv3&#x02009;+&#x02009;model.</p></sec><sec><title>Clinical relevance</title><p id="Par5">The DeepLabv3&#x02009;+&#x02009;model has the poten tial to improve clinical diagnosis and treatment planning and save time in the clinic.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Apical lesions</kwd><kwd>Semantic segmentation</kwd><kwd>Deep learning</kwd><kwd>Artificial intelligence</kwd></kwd-group><funding-group><award-group><funding-source><institution>Yeditepe University</institution></funding-source></award-group><open-access><p>Open access funding provided by the Scientific and Technological Research Council of T&#x000fc;rkiye (T&#x000dc;B&#x00130;TAK).</p></open-access></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer-Verlag GmbH Germany, part of Springer Nature 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par6">Apical periodontitis is characterized by an inflammation that occurs in the apical region of the root of a tooth, primarily due to bacterial infection within the pulp of the tooth, leading to a subsequent inflammatory response in the surrounding bone tissue [<xref ref-type="bibr" rid="CR1">1</xref>]. On radiographs, apical periodontitis can be recognized by the appearance of periapical radiolucency, which may manifest as either a widened periodontal ligament or a lesion [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. These radiolucencies, called apical lesions (AL), can be identified by focused radiographic examinations. Panoramic and periapical radiographs are the most widely used techniques to diagnose and treat apical lesions [<xref ref-type="bibr" rid="CR4">4</xref>].</p><p id="Par7">However, apical lesions can easily be overlooked in dental radiographs in clinics because of the complex anatomy of the jaws and teeth, especially due to superimpositions of the anatomical structures onto the apical lesions. Moreover, dentists often review numerous radiographs daily, which can lead to visual fatigue and reduced concentration over time. This fatigue can decrease the accuracy of lesion segmentation, especially toward the end of the day or after reviewing many images in succession. Also, due to the daily workload of the clinics, radiographs may not be evaluated comprehensively, and therefore an incorrect or incomplete diagnosis may be made. Depending on this, various complications such as abscess formation, cystification, a larger alveolar bone defect, and even tooth loss may occur. Such cases require AI-based computer-aided diagnosis tools that can assist dentists in interpreting medical images.</p><p id="Par8">AI-based segmentation of apical lesions can help differentiate radiolucencies caused by anatomical structures from true apical lesions on panoramic radiographs, thereby reducing the risk of misdiagnosis [<xref ref-type="bibr" rid="CR3">3</xref>]. Segmentation enables the precise measurement of lesion size, which is crucial for diagnosing the extent of apical periodontitis, planning appropriate treatment strategies, and monitoring the disease&#x02019;s progression or resolution over time. Moreover, accurate segmentation can improve the quantification of treatment outcomes, providing objective data to support clinical decisions.</p><p id="Par9">AI applications and segmentation have expanded rapidly in various fields, including medical management and medical imaging [<xref ref-type="bibr" rid="CR5">5</xref>]. For instance, AI models such as OCR-Net, DeepLabv3+ [<xref ref-type="bibr" rid="CR6">6</xref>], Feature Pyramid Network [<xref ref-type="bibr" rid="CR7">7</xref>], YOLOv3, Faster R-CNN, RetinaNet, ResNext, DenseNet, and ShuffleNet [<xref ref-type="bibr" rid="CR8">8</xref>] have been employed for the segmentation and classification of dental caries. In the context of periodontal bone loss detection, AI models such as DeepLabv3+ [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>], DeNTNet, R-CNN, and Faster R-CNN [<xref ref-type="bibr" rid="CR11">11</xref>] have been utilized. Similarly, tooth segmentation has been addressed through models such as U-Net, Faster R-CNN [<xref ref-type="bibr" rid="CR12">12</xref>], U-Net3+ [<xref ref-type="bibr" rid="CR13">13</xref>], U-Net++, DeepLabV3+, MultiResU-Net, ResU-Net++, PraNet, Poly-PVT, TransResU-Net [<xref ref-type="bibr" rid="CR14">14</xref>], and SegNet [<xref ref-type="bibr" rid="CR15">15</xref>]. However, current research on the segmentation of apical lesions has primarily focused on the use of the U-Net model [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR16">16</xref>&#x02013;<xref ref-type="bibr" rid="CR19">19</xref>]. Although only one study [<xref ref-type="bibr" rid="CR16">16</xref>] investigated Lightweight CNN for apical lesion segmentation, this study also used the U-Net model to enhance precision in lesion segmentation.</p><p id="Par10">DeepLabv3+ [<xref ref-type="bibr" rid="CR20">20</xref>] is another pixel-based image segmentation model widely used in medical imaging, capable of segmenting various medical images, including radiographs, MRI, and CT scans, as well as identifying tumors, organs, and jawbone lesions [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR22">22</xref>]. DeepLabv3&#x02009;+&#x02009;and U-Net are powerful tools in segmentation tasks. DeepLabv3&#x02009;+&#x02009;is optimized for more generalized tasks, leveraging atrous convolution to adjust the field of view, which makes it effective across diverse image resolutions. On the other hand, U-Net, specifically designed for medical image segmentation, often delivers better performance in medical scenarios requiring high precision for small-scale segmentations [<xref ref-type="bibr" rid="CR23">23</xref>].</p><p id="Par11">However, DeepLabv3&#x02009;+&#x02009;has not been used before to detect apical lesions, and its performance has not been compared with U-Net, yet. Determining which segmentation method is more successful and utilizing new models such as DeepLabv3&#x02009;+&#x02009;is important for enhancing the reliability and success of artificial intelligence. Thus, apical lesions will be diagnosed promptly and accurately and treated at an early stage. In this way, the patient will be protected from complications that may arise due to delays in treatment, and the treatment procedure will be carried out in a more comfortable process for the patient. Herewith, the time spent in the clinics will be saved, and the treatment cost will be reduced.</p><p id="Par12">This study focuses on the success of the DeepLabV3&#x02009;+&#x02009;model in apical lesion segmentation and compares it with U-Net. This study aims to investigate whether the success of apical lesion segmentation can be improved with DeepLabV3+.</p></sec><sec id="Sec2"><title>Materials and methods</title><p id="Par13">This study is reported following the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) guideline. The study design was authorized by the Non-Interventional Clinical Research Ethics Committee of Yeditepe University (decision date and number: 08.12.2023/ E.83321821-805.02.03-326). The study was conducted following the regulations of the Declaration of Helsinki.</p><p id="Par14">The panoramic radiographs used in this study were randomly selected from the archives of the Faculty of Dentistry at Yeditepe University. The Morita Veraviewepocs (Kyoto, Japan) panoramic imaging system was used to obtain panoramic radiographs with the following parameters: 76 kVp, 7&#x000a0;mA, and 14s. Only the radiographs of patients with mixed dentition were excluded. Additionally, images with issues such as blurriness, distortion, or technician-related errors were excluded. After the exclusion of the radiographs, the records were checked to identify the gender and age of the participants. It was noted that 133 (51.15%) of the radiographs belonged to male patients and 127 (48.85%) belonged to females. Also, the ages of the participants whose panoramic radiographs were included ranged from 17 to 90 years, with a mean age of 48.9 years.</p><p id="Par15">The selected images were cropped to remove the patients&#x02019; names, thereby ensuring patient privacy. A total of 260 anonymized panoramic radiographs were manually annotated using the Computer Vision Annotation Tool (CVAT) software by two independent dental radiologists (B.G.Y. and F.H.&#x000d6;. with five years of experience) and a master reviewer (F.K.&#x000c7;. with ten years of experience). The apical lesions were labeled using polygon labeling tool. An example of an annotated image is given in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>.</p><p id="Par16">All images with different resolutions were collected, resized to the same size (512&#x02009;&#x000d7;&#x02009;512) to be able to be used in the models, and converted to PNG format. No augmentation strategy was applied to the images. The dataset was split into train and test by using the Kfold (k&#x02009;=&#x02009;5) method of the Sklearn library. Therefore, 208 of 260 images were used in training, and 52 instances were used in the test set. In detail, the dataset was split into 5 different subsets randomly. In each trial, one set was used as the test set and others were used in the training.</p><p id="Par17">
<fig id="Fig1"><label>Fig. 1</label><caption><p>Annotation of the apical lesions</p></caption><graphic xlink:href="784_2025_6156_Fig1_HTML" id="d33e366"/></fig>
</p><p id="Par18">This study employed a DeepLabv3&#x02009;+&#x02009;network from the official Keras GitHub repository to segment apical lesions in panoramic images. The model features an encoder-decoder architecture utilizing atrous convolution. The encoder maps input radiographic images to a latent space through parallel convolution blocks with varying sizes, dilation rates, batch normalization, ReLU activation, and added dropout layers to prevent overfitting. Outputs are combined via pooling, concatenation, and 1&#x02009;&#x000d7;&#x02009;1 convolution (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). As the original DeepLabv3&#x02009;+&#x02009;paper suggested [<xref ref-type="bibr" rid="CR20">20</xref>], transfer learning with ResNet50 [<xref ref-type="bibr" rid="CR24">24</xref>], trained on ImageNet, was used to extract low-level features from specific layers for both encoding (conv4_block6_2_relu) and decoding (conv2_block3_2_relu).</p><p id="Par19">
<fig id="Fig2"><label>Fig. 2</label><caption><p>Encoder architecture of DeepLabv3+</p></caption><graphic xlink:href="784_2025_6156_Fig2_HTML" id="d33e387"/></fig>
</p><p id="Par20">The decoder reconstructs the segmentation mask by upsampling, convolving, and combining encoder outputs. Two final convolution layers and an up-convolution operation generate the prediction mask (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). This process reverses the encoder&#x02019;s contraction, translating latent information into the final segmentation mask.</p><p id="Par21">
<fig id="Fig3"><label>Fig. 3</label><caption><p>Decoder architecture of DeepLabv3+</p></caption><graphic xlink:href="784_2025_6156_Fig3_HTML" id="d33e402"/></fig>
</p><p id="Par22">On the other hand, U-Net model used in comparison with DeepLabv3&#x02009;+&#x02009;in this study, features an encoding path for context capture and a symmetric decoding path for precise localization. The encoder uses convolutional and pooling layers to extract features, while the decoder upsamples and combines feature maps to restore spatial details. Transfer learning with ResNet50 was applied to the encoder for a fair comparison. The detailed architecture is shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>.</p><p id="Par23">
<fig id="Fig4"><label>Fig. 4</label><caption><p>Architecture of U-Net. (<bold>A</bold>) U-Net Architecture, (<bold>B</bold>) Decoder Block</p></caption><graphic xlink:href="784_2025_6156_Fig4_HTML" id="d33e425"/></fig>
</p><p id="Par24">The performance of the developed model was evaluated using the test set. Precision, recall, F1-score, and IoU metrics were utilized for evaluation, and they are demonstrated in the following Eqs.&#x000a0;(<xref rid="Equ1" ref-type="disp-formula">1</xref>&#x02013;<xref rid="Equ4" ref-type="disp-formula">4</xref>).<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:Precision=\frac{TP}{TP+FP\:}$$\end{document}</tex-math><graphic xlink:href="784_2025_6156_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:Recall=\frac{TP}{TP+FN\:}$$\end{document}</tex-math><graphic xlink:href="784_2025_6156_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:F1\_Score=2*\frac{Precision*Recall}{Precision+\:Recall\:}$$\end{document}</tex-math><graphic xlink:href="784_2025_6156_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:IOU=\frac{TP}{TP+FN+FP\:}$$\end{document}</tex-math><graphic xlink:href="784_2025_6156_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par25">TP, FP, and FN are the numbers of true positives, false positives, and false negatives, respectively. In addition to these parameters, the ROC-AUC, the area under the ROC curve, was used to evaluate the model.</p></sec><sec id="Sec3"><title>Results</title><p id="Par26">The performance of the DeepLabv3&#x02009;+&#x02009;model for apical lesion segmentation was evaluated using the performance metrics mentioned above. Additionally, its performance was compared with that of the U-Net model, which is commonly used for the segmentation of apical lesions. For a fair comparison, the same hyper-parameters were used for both models, as shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.</p><p id="Par27">The models were optimized using TensorFlow (v2.12.0) with CUDA (v12.2) in Python open-source programming language (v 3.9.16). The training and testing were processed with a computer having NVIDIA GeForce RTX 3060 with 12 GB VRAM, 13th Gen Intel(R) Core (TM) i7-13700&#x000a0;K 3.40&#x000a0;GHz, and 32 GB of DDR4 RAM.</p><p id="Par28">
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Hyper-parameters</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Metric</th><th align="left">Value</th></tr></thead><tbody><tr><td align="left">Batch_size</td><td align="left">16</td></tr><tr><td align="left">Learning_rate</td><td align="left">0.001</td></tr><tr><td align="left">Epoch</td><td align="left">100</td></tr><tr><td align="left">Dropout_rate</td><td align="left">0.3</td></tr><tr><td align="left">Loss Function</td><td align="left">BCE</td></tr><tr><td align="left">Optimizer</td><td align="left">Adam</td></tr></tbody></table></table-wrap>
</p><p id="Par29">The performance results of the DeepLabv3&#x02009;+&#x02009;and U-Net models were obtained by using 5-fold cross-validation. Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> shows the average of the folds with the corresponding standard deviation of DeepLabv3&#x02009;+&#x02009;over U-Net. The better result for each metric is marked in bold. As seen in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, the DeepLabv3&#x02009;+&#x02009;model has 29.96% better AUC and 61.06% better recall results than the U-Net model. However, the U-Net model gets 3.61%, -69.17%, and 25.55% better IoU, precision, and F1-score results than the DeepLabv3&#x02009;+&#x02009;model, respectively.</p><p id="Par30">
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Comparison of DeepLabv3&#x02009;+&#x02009;and U-Net models</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Metric</th><th align="left">DeepLabv3+</th><th align="left">U-Net</th><th align="left">Rel. Imp.</th></tr></thead><tbody><tr><td align="left">AUC</td><td char="&#x000b1;" align="char">
<bold>0.879&#x02009;&#x000b1;&#x02009;0.055</bold>
</td><td char="&#x000b1;" align="char">0.676&#x02009;&#x000b1;&#x02009;0.051</td><td char="." align="char">29.96%</td></tr><tr><td align="left">IoU</td><td char="&#x000b1;" align="char">0.546&#x02009;&#x000b1;&#x02009;0.011</td><td char="&#x000b1;" align="char">
<bold>0.566&#x02009;&#x000b1;&#x02009;0.032</bold>
</td><td char="." align="char">-3.61%</td></tr><tr><td align="left">Precision</td><td char="&#x000b1;" align="char">0.171&#x02009;&#x000b1;&#x02009;0.074</td><td char="&#x000b1;" align="char">
<bold>0.554&#x02009;&#x000b1;&#x02009;0.122</bold>
</td><td char="." align="char">-69.17%</td></tr><tr><td align="left">Recall</td><td char="&#x000b1;" align="char">
<bold>0.248&#x02009;&#x000b1;&#x02009;0.108</bold>
</td><td char="&#x000b1;" align="char">0.154&#x02009;&#x000b1;&#x02009;0.079</td><td char="." align="char">61.06%</td></tr><tr><td align="left">F1-score</td><td char="&#x000b1;" align="char">0.172&#x02009;&#x000b1;&#x02009;0.036</td><td char="&#x000b1;" align="char">
<bold>0.231&#x02009;&#x000b1;&#x02009;0.096</bold>
</td><td char="." align="char">-25.55%</td></tr></tbody></table></table-wrap>
</p><p id="Par31">The results and corresponding standard deviations are shown as error bars in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> to interpret the effect of the standard deviation. Figure&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> shows that the minimum AUC value of the DeepLabv3&#x02009;+&#x02009;model is larger than the maximum AUC value of the U-Net model. On the contrary, the minimum precision value of the U-Net model is larger than the maximum precision value of the DeepLabv3&#x02009;+&#x02009;model. However, according to the error bar of the IoU scores, the difference between the DeepLabv3&#x02009;+&#x02009;and U-Net models is not significant in the IoU metric. When the F1-score of the models is compared according to Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>, it is seen that the minimum and maximum bars of the U-Net and DeepLabv3&#x02009;+&#x02009;models intersect each other, which shows that the DeepLabv3&#x02009;+&#x02009;model has the potential to get better F1-score results than the U-Net model.</p><p id="Par32">
<fig id="Fig5"><label>Fig. 5</label><caption><p>Comparison of results with error bars</p></caption><graphic xlink:href="784_2025_6156_Fig5_HTML" id="d33e634"/></fig>
</p><p id="Par33">We also carried out the t-test analysis by setting the significance threshold at 0.05 to compare the results of the U-Net and DeepLabv3&#x02009;+&#x02009;models. According to the results of the t-test, the p-values were approximately 0.00065 for AUC, 0.26631 for IoU, 0.00125 for precision, 0.20123 for recall, and 0.30324 for F1-score. All p-values of AUC and precision were less than 0.05, meaning that performance differences between models were statistically significant for those metrics. But IoU, recall, and F1-score, p-values were larger than 0.05, which means that performance differences between DeepLabv3&#x02009;+&#x02009;and U-Net were not statistically significant for these metrics.</p><p id="Par34">When we examined the outputs of both models, we observed that while some annotated apical lesions were better segmented by U-Net, others were better segmented by DeepLabv3+. The example outputs of both models from the test set are shown in Figs.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref> and <xref rid="Fig7" ref-type="fig">7</xref>.</p><p id="Par35">
<fig id="Fig6"><label>Fig. 6</label><caption><p>Comparison of U-Net and DeepLabv3&#x02009;+&#x02009;outputs for an image in the test set where DeepLabv3&#x02009;+&#x02009;has better prediction</p></caption><graphic xlink:href="784_2025_6156_Fig6_HTML" id="d33e654"/></fig>
</p><p id="Par36">
<fig id="Fig7"><label>Fig. 7</label><caption><p>Comparison of U-Net and DeepLabv3&#x02009;+&#x02009;outputs for an image in the test set where U-Net has better prediction</p></caption><graphic xlink:href="784_2025_6156_Fig7_HTML" id="d33e665"/></fig>
</p></sec><sec id="Sec4"><title>Discussion</title><p id="Par37">In this study, we focused on the segmentation of apical lesions using the DeepLabv3&#x02009;+&#x02009;artificial intelligence model and compared it with the U-Net model. The DeepLabv3&#x02009;+&#x02009;model outperformed the U-Net model in terms of AUC and recall. However, the U-Net model achieved better results in precision, and F1-score compared to the DeepLabv3&#x02009;+&#x02009;model.</p><p id="Par38">The recall metric also known as true positive rate gives insight into a model&#x02019;s prediction performance of true positives. From the perspective of apical lesion segmentation, a high recall means that the model can capture the region with a lesion more accurately. This increases the chance of early diagnosis which is crucial in the health domain. Moreover, high precision may reduce a model&#x02019;s false positive rate, preventing false diagnosis. Based on our results, DeepLabv3&#x02009;+&#x02009;is preferable in cases where dentists don&#x02019;t want to miss probable apical lesions since it has better recall than U-Net. On the other hand, the U-Net model is preferable in cases where dentists don&#x02019;t want to take the risk of a false diagnosis because of its higher precision score.</p><p id="Par39">Similarly, a model with a higher AUC is likely better at overall classification and distinguishing between lesion and non-lesion regions across varying thresholds but may not perform as well at a specific threshold in terms of the balance between precision and recall. A model with a higher F1-score is optimized for a specific threshold and strikes a better balance between detecting lesions and avoiding false positives at that threshold. If a dentist wants to focus on identifying all potential lesions (e.g., for initial screening), they should consider using the DeepLabV3&#x02009;+&#x02009;model since it has a higher AUC in our experiments. However, if a dentist wants to accurately identify lesions with minimal false positives, they might prefer the U-Net model due to its higher F1-score.</p><p id="Par40">Upon reviewing the literature, we found that some studies have used the U-Net model for apical lesion segmentation [<xref ref-type="bibr" rid="CR2">2</xref>&#x02013;<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR25">25</xref>]. We also found several studies that employed the DeepLabv3&#x02009;+&#x02009;model to detect dental plaque and caries lesions [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>]. However, we did not come across any articles in which the DeepLabv3&#x02009;+&#x02009;model was used for the segmentation of apical lesions.</p><p id="Par41">In studies using U-Net, both cropped and uncropped panoramic images were used. Cropping images enhances the efficiency of the training and prediction process by reducing irrelevant or distracting information, allowing the model to focus on the most relevant areas. However, it is important to consider the potential drawbacks, such as information loss, bias, and reduced robustness. Striking the right balance between cropping and preserving relevant information is crucial for efficiently training machine learning models. Such efficient cropping requires expertise in this domain and can be time-consuming. If the images are cropped to obtain a region of interest (ROI) and train a more robust AI model, the dentists who wish to use such an AI tool would need to crop each image, a process that is prone to errors and can be time-consuming. In this study, we used uncropped images to avoid these disadvantages and to evaluate the performance of AI models on images without cropping.</p><p id="Par42">Among the studies that used U-Net for the segmentation of apical lesions, Bayrakdar et al. cropped the panoramic images into four parts. Their recall, precision, and F1-score values were 0.92, 0.84, and 0.88, respectively, with an IoU value of 0.7 [<xref ref-type="bibr" rid="CR2">2</xref>]. Krois et al. cropped the panoramic images based on the region of interest. They studied two different data sets. One dataset showed an F1-score of 0.54 and a precision of 0.64, while the other showed an F1-score of 0.32 and a precision of 0.63 [<xref ref-type="bibr" rid="CR19">19</xref>]. Endres et al. cropped the panoramic images by 100 pixels on the upper and lower boundaries and 300 pixels on the left and right boundaries. They found a precision of 0.60 and an F1-score of 0.58 [<xref ref-type="bibr" rid="CR25">25</xref>].</p><p id="Par43">All three studies reported better precision and F1-score results than the U-Net and DeepLabv3&#x02009;+&#x02009;results obtained in our study. However, since they did not report them, we were unable to compare our IoU, AUC, and recall metrics with Krois and Endres&#x02019; studies. On the other hand, Bayrakdar&#x02019;s study obtained better results in all metrics than ours. One of the main reasons for such a difference in the results may be attributed to the use of cropping in these studies, which enhances the performance of the models, as previously mentioned. In addition to cropping, the number of images used for machine learning training plays a crucial role in the model performance. 470, 650, and 2900 images were used in the studies of Bayrakdar, Krois, and Endres respectively while in our study, only 260 images were used. Another important point that affects the model performance is the hyper-parameters used in the training. Unfortunately, there is no information about hyper-parameters used in Bayrakdar&#x02019;s study except for the number of epochs which is 95. On the other hand, Krois used a linear combination of binary cross entropy and Dice loss functions to optimize the model with a 0.002 learning rate and 200 epochs while we only used binary cross entropy with a 0.001 learning rate and 100 epochs. Similarly, Endres used the Dice loss function in their training with 25 epochs. Although their learning rate is 0.001 as in our study, they reduced their learning rate with exponential decay. Most importantly, they used a different evaluation strategy by training 10 different models by dividing the training set into 10 subsets and using 9 of them for each model&#x02019;s training. Then, they obtained the results by taking the mean output produced by the 10 constituent models which produced more robust results. Considering all these differences, it is understandable that the studies obtained different results than ours.</p><p id="Par44">On the other hand, Song et al. used U-Net without any cropping, which is similar to our study. They reported precision, recall, F1-score, and IoU values of 0.744, 0.740, 0.742, and 0.5, respectively [<xref ref-type="bibr" rid="CR3">3</xref>], while our U-Net scores were 0.55, 0.15, 0.23, and 0.56, and our DeepLabv3&#x02009;+&#x02009;scores were 0.17, 0.24, 0.17, and 0.54, respectively. Although our precision, recall, and F1-score were lower than Song et al.&#x02019;s results, we obtained better results on the IoU metric, which is the most commonly used evaluation metric in the image segmentation domain.</p><p id="Par45">As stated above, we found no article in which the DeepLabv3&#x02009;+&#x02009;model was used for the segmentation of apical lesions. But when we examined the articles comparing U-Net and DeepLabv3&#x02009;+&#x02009;for caries lesion segmentation, we saw that Zhu et al. found a better F1-score in U-Net and better accuracy, precision, and recall metrics in DeepLabv3+ [<xref ref-type="bibr" rid="CR22">22</xref>]. Zhang et al. reported a better recall in DeepLabv3&#x02009;+&#x02009;and a better IoU with U-Net [<xref ref-type="bibr" rid="CR27">27</xref>]. Chen et al. found better recall, precision, and IoU in U-Net than DeepLabv3+ [<xref ref-type="bibr" rid="CR6">6</xref>]. The results of these studies indicate that, currently, U-Net and DeepLabv3&#x02009;+&#x02009;cannot outperform each other across all metrics. Also, we obtained similar results in our study where DeepLabv3&#x02009;+&#x02009;gave better recall and AUC results but U-Net obtained better IoU and precision results.</p><p id="Par46">However, this study had several limitations, such as the number of patients, the use of a single panoramic radiography machine, and standardized parameters. Furthermore, the success of AI is closely linked to the accuracy of labeling (annotation), which depends on the skill of the practitioners. Although three experienced dental radiologists performed the labeling process, achieving entirely accurate labeling at a pixel level is not feasible. Future work may focus on evaluating the performance of DeepLabv3&#x02009;+&#x02009;on larger, more diverse datasets, as well as using different radiography machines. Additionally, future studies may explore the use of other deep learning architectures for apical lesion segmentation and investigate the impact of cropping strategies on model performance. Moreover, due to the limited number of images, the validation set was not utilized in the training which might result in a low generalization and overfitting on the test set. We had planned to handle this difficulty by increasing images with data augmentation techniques. But, due to the limited computational power, we couldn&#x02019;t achieve successful training. With higher computing devices, it will be possible to obtain more robust results with higher prediction performance both for U-Net and DeepLabV3+.</p></sec><sec id="Sec5"><title>Conclusion</title><p id="Par47">In this paper, we comprehensively evaluated the DeepLabv3&#x02009;+&#x02009;and U-Net models in the context of apical lesion segmentation. Our findings indicate that DeepLabv3&#x02009;+&#x02009;outperforms the U-Net model by a substantial margin in terms of AUC and recall. However, it is behind the U-Net model in precision and F1-score, and there is no significant difference between the models in terms of the IoU metric. Overall, the DeepLabv3&#x02009;+&#x02009;model outperforms U-Net in 2 of the 5 metrics, performs worse in 2 of the 5 metrics, and is equivalent in 1 of the 5 metrics. Although the DeepLabv3&#x02009;+&#x02009;model does not outperform U-Net in all cases, it can still compete with the U-Net model in the segmentation of apical lesions based on these results. However, the current forms of the DeepLabv3&#x02009;+&#x02009;and the U-Net model are not directly applicable in clinical diagnosis since their current results are not high enough due to limitations in the number of images available for training. On the other hand, with a larger dataset and a more detailed training process, these models could be used to improve clinical diagnosis and treatment planning.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>O. &#x000c7; transferred the data to CVAT. F. K.&#x000c7;, B. G. Y and F. H. &#x000d6;. labelled the data on CVAT. &#x000c7;. Y. trained artificial intelligence. F. K.&#x000c7; and &#x000c7;. Y. wrote the manuscript. D.&#x00130;. helped choose the journal. All authors reviewed the manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>Open access funding provided by the Scientific and Technological Research Council of T&#x000fc;rkiye (T&#x000dc;B&#x00130;TAK).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>No datasets were generated or analysed during the current study.</p></notes><notes><title>Declarations</title><notes id="FPar3"><title>Ethical approval</title><p id="Par48">Approval was obtained from the Yeditepe University Non-Interventional Clinical Research Ethics Committee. The procedures used in this study adhere to the tenets of the Declaration of Helsinki (No: E.83321821-805.02.03-326).</p></notes><notes id="FPar2"><title>Informed consent</title><p id="Par49">The informed consent obtained from study participants was written.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par50">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Arias</surname><given-names>Z</given-names></name><name><surname>Nizami</surname><given-names>MZ</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Chai</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Kuang</surname><given-names>C</given-names></name><name><surname>Omori</surname><given-names>K</given-names></name><name><surname>Takashiba</surname><given-names>S</given-names></name></person-group><article-title>Recent advances in apical periodontitis treatment: a narrative review</article-title><source>Bioengineering</source><year>2023</year><volume>10</volume><issue>4</issue><fpage>488</fpage><pub-id pub-id-type="doi">10.3390/bioengineering10040488</pub-id><pub-id pub-id-type="pmid">37106675</pub-id>
</element-citation><mixed-citation id="mc-CR1" publication-type="journal">Arias Z, Nizami MZ, Chen X, Chai X, Xu B, Kuang C, Omori K, Takashiba S (2023) Recent advances in apical periodontitis treatment: a narrative review. Bioengineering 10(4):488. 10.3390/bioengineering10040488<pub-id pub-id-type="pmid">37106675</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Bayrakdar</surname><given-names>IS</given-names></name><name><surname>Orhan</surname><given-names>K</given-names></name><name><surname>&#x000c7;elik</surname><given-names>&#x000d6;</given-names></name><name><surname>Bilgir</surname><given-names>E</given-names></name><name><surname>Sa&#x0011f;lam</surname><given-names>H</given-names></name><name><surname>Kaplan</surname><given-names>FA</given-names></name><name><surname>G&#x000f6;r&#x000fc;r</surname><given-names>SA</given-names></name><name><surname>Odaba&#x0015f;</surname><given-names>A</given-names></name><name><surname>Aslan</surname><given-names>AF</given-names></name><name><surname>R&#x000f3;&#x0017c;y&#x00142;o-Kalinowska</surname><given-names>I</given-names></name></person-group><article-title>A U-net approach to apical lesion segmentation on panoramic radiographs</article-title><source>Biomed Res Int</source><year>2022</year><volume>2022</volume><issue>1</issue><fpage>7035367</fpage><pub-id pub-id-type="doi">10.1155/2022/7035367</pub-id><pub-id pub-id-type="pmid">35075428</pub-id>
</element-citation><mixed-citation id="mc-CR2" publication-type="journal">Bayrakdar IS, Orhan K, &#x000c7;elik &#x000d6;, Bilgir E, Sa&#x0011f;lam H, Kaplan FA, G&#x000f6;r&#x000fc;r SA, Odaba&#x0015f; A, Aslan AF, R&#x000f3;&#x0017c;y&#x00142;o-Kalinowska I (2022) A U-net approach to apical lesion segmentation on panoramic radiographs. Biomed Res Int 2022(1):7035367. 10.1155/2022/7035367<pub-id pub-id-type="pmid">35075428</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>IS</given-names></name><name><surname>Shin</surname><given-names>HK</given-names></name><name><surname>Kang</surname><given-names>JH</given-names></name><name><surname>Kim</surname><given-names>JE</given-names></name><name><surname>Huh</surname><given-names>KH</given-names></name><name><surname>Yi</surname><given-names>WJ</given-names></name><name><surname>Lee</surname><given-names>SS</given-names></name><name><surname>Heo</surname><given-names>MS</given-names></name></person-group><article-title>Deep learning-based apical lesion segmentation from panoramic radiographs</article-title><source>Imaging Sci Dent</source><year>2022</year><volume>52</volume><issue>4</issue><fpage>351</fpage><pub-id pub-id-type="doi">10.5624/isd.20220078</pub-id><pub-id pub-id-type="pmid">36605863</pub-id>
</element-citation><mixed-citation id="mc-CR3" publication-type="journal">Song IS, Shin HK, Kang JH, Kim JE, Huh KH, Yi WJ, Lee SS, Heo MS (2022) Deep learning-based apical lesion segmentation from panoramic radiographs. Imaging Sci Dent 52(4):351. 10.5624/isd.20220078<pub-id pub-id-type="pmid">36605863</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Velvart</surname><given-names>P</given-names></name><name><surname>Hecker</surname><given-names>H</given-names></name><name><surname>Tillinger</surname><given-names>G</given-names></name></person-group><article-title>Detection of the apical lesion and the mandibular canal in conventional radiography and computed tomography</article-title><source>Oral Surg Oral Med Oral Pathol Oral Radiol</source><year>2001</year><volume>92</volume><issue>6</issue><fpage>682</fpage><lpage>688</lpage><pub-id pub-id-type="doi">10.1067/moe.2001.118904</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Velvart P, Hecker H, Tillinger G (2001) Detection of the apical lesion and the mandibular canal in conventional radiography and computed tomography. Oral Surg Oral Med Oral Pathol Oral Radiol 92(6):682&#x02013;688. 10.1067/moe.2001.118904</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>B</given-names><suffix>Jr</suffix></name><name><surname>Seltzer</surname><given-names>SE</given-names></name><name><surname>Langlotz</surname><given-names>CP</given-names></name><name><surname>Dreyer</surname><given-names>KP</given-names></name><name><surname>Summers</surname><given-names>RM</given-names></name><name><surname>Petrick</surname><given-names>N</given-names></name><name><surname>Marinac-Dabic</surname><given-names>D</given-names></name><name><surname>Cruz</surname><given-names>M</given-names></name><name><surname>Alkasab</surname><given-names>TK</given-names></name><name><surname>Hanisch</surname><given-names>RJ</given-names></name><name><surname>Nilsen</surname><given-names>WJ</given-names></name></person-group><article-title>A road map for translational research on artificial intelligence in medical imaging</article-title><source>J Am Coll Radiol</source><year>2019</year><volume>16</volume><issue>9</issue><fpage>1179</fpage><lpage>1189</lpage><pub-id pub-id-type="doi">10.1016/j.jacr.2019.04.014</pub-id><pub-id pub-id-type="pmid">31151893</pub-id>
</element-citation><mixed-citation id="mc-CR5" publication-type="journal">Allen B Jr, Seltzer SE, Langlotz CP, Dreyer KP, Summers RM, Petrick N, Marinac-Dabic D, Cruz M, Alkasab TK, Hanisch RJ, Nilsen WJ (2019) A road map for translational research on artificial intelligence in medical imaging. J Am Coll Radiol 16(9):1179&#x02013;1189. 10.1016/j.jacr.2019.04.014<pub-id pub-id-type="pmid">31151893</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Chen S, Yang Y, Wu W, Wei R, Wang Z, Tay FR, Hu J, Ma J (2024 May) Classification of Caries based on CBCT: a deep Learning Network Interpretability Study. J Imaging Inf Med 28:1&#x02013;4. 10.1007/s10278-024-01143-5</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">M&#x00103;rginean AC, Mure&#x0015f;anu S, Hede&#x0015f;iu M, Dio&#x0015f;an L (2024) Teeth segmentation and carious lesions segmentation in panoramic X-ray images using CariSeg, a networks&#x02019; ensemble. Heliyon 10(10). 10.1016/j.heliyon.2024.e30836</mixed-citation></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Moharrami</surname><given-names>M</given-names></name><name><surname>Farmer</surname><given-names>J</given-names></name><name><surname>Singhal</surname><given-names>S</given-names></name><name><surname>Watson</surname><given-names>E</given-names></name><name><surname>Glogauer</surname><given-names>M</given-names></name><name><surname>Johnson</surname><given-names>AE</given-names></name><name><surname>Schwendicke</surname><given-names>F</given-names></name><name><surname>Quinonez</surname><given-names>C</given-names></name></person-group><article-title>Detecting dental caries on oral photographs using artificial intelligence: a systematic review</article-title><source>Oral Dis</source><year>2024</year><volume>30</volume><issue>4</issue><fpage>1765</fpage><lpage>1783</lpage><pub-id pub-id-type="doi">10.1111/odi.14659</pub-id><pub-id pub-id-type="pmid">37392423</pub-id>
</element-citation><mixed-citation id="mc-CR8" publication-type="journal">Moharrami M, Farmer J, Singhal S, Watson E, Glogauer M, Johnson AE, Schwendicke F, Quinonez C (2024) Detecting dental caries on oral photographs using artificial intelligence: a systematic review. Oral Dis 30(4):1765&#x02013;1783. 10.1111/odi.14659<pub-id pub-id-type="pmid">37392423</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Liu CW, Wu HH Distribution Analysis of Dental Plaque Based on Deep Learning. In 2022 IEEE International Conference on Consumer Electronics-Taiwan 2022 Jul 6 (pp. 185&#x02013;186). IEEE. 10.1109/ICCE-Taiwan55306.2022.9869078</mixed-citation></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>Y&#x000fc;ksel</surname><given-names>B</given-names></name><name><surname>&#x000d6;zveren</surname><given-names>N</given-names></name><name><surname>Ye&#x0015f;il</surname><given-names>&#x000c7;</given-names></name></person-group><article-title>Evaluation of dental plaque area with artificial intelligence model</article-title><source>Niger J Clin Pract</source><year>2024</year><volume>27</volume><issue>6</issue><fpage>759</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.4103/njcp.njcp_862_23</pub-id><pub-id pub-id-type="pmid">38943301</pub-id>
</element-citation><mixed-citation id="mc-CR10" publication-type="journal">Y&#x000fc;ksel B, &#x000d6;zveren N, Ye&#x0015f;il &#x000c7; (2024) Evaluation of dental plaque area with artificial intelligence model. Niger J Clin Pract 27(6):759&#x02013;765. 10.4103/njcp.njcp_862_23<pub-id pub-id-type="pmid">38943301</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Tariq</surname><given-names>A</given-names></name><name><surname>Nakhi</surname><given-names>FB</given-names></name><name><surname>Salah</surname><given-names>F</given-names></name><name><surname>Eltayeb</surname><given-names>G</given-names></name><name><surname>Abdulla</surname><given-names>GJ</given-names></name><name><surname>Najim</surname><given-names>N</given-names></name><name><surname>Khedr</surname><given-names>SA</given-names></name><name><surname>Elkerdasy</surname><given-names>S</given-names></name><name><surname>Al-Rawi</surname><given-names>N</given-names></name><name><surname>Alkawas</surname><given-names>S</given-names></name><name><surname>Mohammed</surname><given-names>M</given-names></name></person-group><article-title>Efficiency and accuracy of artificial intelligence in the radiographic detection of periodontal bone loss: a systematic review</article-title><source>Imaging Sci Dent</source><year>2023</year><volume>53</volume><issue>3</issue><fpage>193</fpage><pub-id pub-id-type="doi">10.5624/isd.20230092</pub-id><pub-id pub-id-type="pmid">37799746</pub-id>
</element-citation><mixed-citation id="mc-CR11" publication-type="journal">Tariq A, Nakhi FB, Salah F, Eltayeb G, Abdulla GJ, Najim N, Khedr SA, Elkerdasy S, Al-Rawi N, Alkawas S, Mohammed M (2023) Efficiency and accuracy of artificial intelligence in the radiographic detection of periodontal bone loss: a systematic review. Imaging Sci Dent 53(3):193. 10.5624/isd.20230092<pub-id pub-id-type="pmid">37799746</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Adnan N, Bin Khalid W, Umer F (2023) An artificial intelligence model for instance segmentation and tooth numbering on orthopantomograms. Int J Comput Dent 26(4). 10.3290/j.ijcd.b3840535</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">&#x0015e;ahin ME, Uluta&#x0015f; H, S&#x000fc;zgen EE Automated Segmentation of Dental Structures in Panoramic Radiographs Using U-Net 3+. In 2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP) 2024 Sep 21 (pp. 1&#x02013;6). IEEE. 10.1109/IDAP64064.2024.10711001</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Alhasson HF (2024) Automatic Segmentation of Teeth from Panoramic X-Ray Images Employing Deep Learning Models. In 4th Interdisciplinary Conference on Electrics and Computer (INTCEC) 2024 Jun 11 (pp. 1&#x02013;6). IEEE. 10.1109/INTCEC61833.2024.10603373</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Kayadibi &#x00130;, K&#x000f6;se U, G&#x000fc;raks&#x00131;n GE Computer-aided tooth segmentation using image Processing techniques and convolutional neural network. Pamukkale Univ J Eng Sci. 1000(1000). 10.5505/pajes.2024.22237</mixed-citation></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Latke</surname><given-names>V</given-names></name><name><surname>Narawade</surname><given-names>V</given-names></name></person-group><article-title>Detection of dental periapical lesions using retinex based image enhancement and lightweight deep learning model</article-title><source>Image Vis Comput</source><year>2024</year><volume>146</volume><fpage>105016</fpage><pub-id pub-id-type="doi">10.1016/j.imavis.2024.105016</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Latke V, Narawade V (2024) Detection of dental periapical lesions using retinex based image enhancement and lightweight deep learning model. Image Vis Comput 146:105016. 10.1016/j.imavis.2024.105016</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>YS</given-names></name><name><surname>Iakubovskii</surname><given-names>P</given-names></name><name><surname>Lim</surname><given-names>LZ</given-names></name><name><surname>Mol</surname><given-names>A</given-names></name><name><surname>Tyndall</surname><given-names>DA</given-names></name></person-group><article-title>Evaluation of deep learning for detecting intraosseous jaw lesions in cone beam computed tomography volumes</article-title><source>Oral Surg Oral Med Oral Pathol Oral Radiol</source><year>2024</year><volume>138</volume><issue>1</issue><fpage>173</fpage><lpage>183</lpage><pub-id pub-id-type="doi">10.1016/j.oooo.2023.09.011</pub-id><pub-id pub-id-type="pmid">38155015</pub-id>
</element-citation><mixed-citation id="mc-CR17" publication-type="journal">Huang YS, Iakubovskii P, Lim LZ, Mol A, Tyndall DA (2024) Evaluation of deep learning for detecting intraosseous jaw lesions in cone beam computed tomography volumes. Oral Surg Oral Med Oral Pathol Oral Radiol 138(1):173&#x02013;183. 10.1016/j.oooo.2023.09.011<pub-id pub-id-type="pmid">38155015</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name><surname>Setzer</surname><given-names>FC</given-names></name><name><surname>Shi</surname><given-names>KJ</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Yan</surname><given-names>H</given-names></name><name><surname>Yoon</surname><given-names>H</given-names></name><name><surname>Mupparapu</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>J</given-names></name></person-group><article-title>Artificial intelligence for the computer-aided detection of periapical lesions in cone-beam computed tomographic images</article-title><source>J Endod</source><year>2020</year><volume>46</volume><issue>7</issue><fpage>987</fpage><lpage>993</lpage><pub-id pub-id-type="doi">10.1016/j.joen.2020.03.025</pub-id><pub-id pub-id-type="pmid">32402466</pub-id>
</element-citation><mixed-citation id="mc-CR18" publication-type="journal">Setzer FC, Shi KJ, Zhang Z, Yan H, Yoon H, Mupparapu M, Li J (2020) Artificial intelligence for the computer-aided detection of periapical lesions in cone-beam computed tomographic images. J Endod 46(7):987&#x02013;993. 10.1016/j.joen.2020.03.025<pub-id pub-id-type="pmid">32402466</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name><surname>Krois</surname><given-names>J</given-names></name><name><surname>Garcia Cantu</surname><given-names>A</given-names></name><name><surname>Chaurasia</surname><given-names>A</given-names></name><name><surname>Patil</surname><given-names>R</given-names></name><name><surname>Chaudhari</surname><given-names>PK</given-names></name><name><surname>Gaudin</surname><given-names>R</given-names></name><name><surname>Gehrung</surname><given-names>S</given-names></name><name><surname>Schwendicke</surname><given-names>F</given-names></name></person-group><article-title>Generalizability of deep learning models for dental image analysis</article-title><source>Sci Rep</source><year>2021</year><volume>11</volume><issue>1</issue><fpage>6102</fpage><pub-id pub-id-type="doi">10.1038/s41598-021-85454-5</pub-id><pub-id pub-id-type="pmid">33731732</pub-id>
</element-citation><mixed-citation id="mc-CR19" publication-type="journal">Krois J, Garcia Cantu A, Chaurasia A, Patil R, Chaudhari PK, Gaudin R, Gehrung S, Schwendicke F (2021) Generalizability of deep learning models for dental image analysis. Sci Rep 11(1):6102. 10.1038/s41598-021-85454-5<pub-id pub-id-type="pmid">33731732</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Chen LC, Zhu Y, Papandreou G, Schroff F, Adam H (2018) (pp. 801&#x02013;818) Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV)</mixed-citation></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Chouai</surname><given-names>M</given-names></name><name><surname>Dolezel</surname><given-names>P</given-names></name><name><surname>Stursa</surname><given-names>D</given-names></name><name><surname>Nemec</surname><given-names>Z</given-names></name></person-group><article-title>New end-to-end strategy based on DeepLabv3&#x02009;+&#x02009;semantic segmentation for human head detection</article-title><source>Sensors</source><year>2021</year><volume>21</volume><issue>17</issue><fpage>5848</fpage><pub-id pub-id-type="doi">10.3390/s21175848</pub-id><pub-id pub-id-type="pmid">34502738</pub-id>
</element-citation><mixed-citation id="mc-CR21" publication-type="journal">Chouai M, Dolezel P, Stursa D, Nemec Z (2021) New end-to-end strategy based on DeepLabv3&#x02009;+&#x02009;semantic segmentation for human head detection. Sensors 21(17):5848. 10.3390/s21175848<pub-id pub-id-type="pmid">34502738</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Zhu H, Cao Z, Lian L, Ye G, Gao H, Wu J CariesNet: a deep learning approach for segmentation of multi-stage caries lesion from oral panoramic X-ray image. Neural Comput Appl 2023 Aug 1:1&#x02013;9. 10.1007/s00521-021-06684-2</mixed-citation></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>&#x0015e;ahin</surname><given-names>GA</given-names></name></person-group><article-title>Advances in Artificial Intelligence-aided Intraoral Imaging Analysis in Periodontics</article-title><source>BSJ Health Sci</source><year>2024</year><volume>7</volume><issue>5</issue><fpage>218</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.19127/bshealthscience.1539717</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">&#x0015e;ahin GA (2024) Advances in Artificial Intelligence-aided Intraoral Imaging Analysis in Periodontics. BSJ Health Sci 7(5):218&#x02013;225. 10.19127/bshealthscience.1539717</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J (2016) (pp. 770&#x02013;778) Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Endres</surname><given-names>MG</given-names></name><name><surname>Hillen</surname><given-names>F</given-names></name><name><surname>Salloumis</surname><given-names>M</given-names></name><name><surname>Sedaghat</surname><given-names>AR</given-names></name><name><surname>Niehues</surname><given-names>SM</given-names></name><name><surname>Quatela</surname><given-names>O</given-names></name><name><surname>Hanken</surname><given-names>H</given-names></name><name><surname>Smeets</surname><given-names>R</given-names></name><name><surname>Beck-Broichsitter</surname><given-names>B</given-names></name><name><surname>Rendenbach</surname><given-names>C</given-names></name><name><surname>Lakhani</surname><given-names>K</given-names></name></person-group><article-title>Development of a deep learning algorithm for periapical disease detection in dental radiographs</article-title><source>Diagnostics</source><year>2020</year><volume>10</volume><issue>6</issue><fpage>430</fpage><pub-id pub-id-type="doi">10.3390/diagnostics10060430</pub-id><pub-id pub-id-type="pmid">32599942</pub-id>
</element-citation><mixed-citation id="mc-CR25" publication-type="journal">Endres MG, Hillen F, Salloumis M, Sedaghat AR, Niehues SM, Quatela O, Hanken H, Smeets R, Beck-Broichsitter B, Rendenbach C, Lakhani K (2020) Development of a deep learning algorithm for periapical disease detection in dental radiographs. Diagnostics 10(6):430. 10.3390/diagnostics10060430<pub-id pub-id-type="pmid">32599942</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>You</surname><given-names>W</given-names></name><name><surname>Hao</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Xia</surname><given-names>B</given-names></name></person-group><article-title>Deep learning-based dental plaque detection on primary teeth: a comparison with clinical assessments</article-title><source>BMC Oral Health</source><year>2020</year><volume>20</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1186/s12903-020-01114-6</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">You W, Hao A, Li S, Wang Y, Xia B (2020) Deep learning-based dental plaque detection on primary teeth: a comparison with clinical assessments. BMC Oral Health 20:1&#x02013;7. 10.1186/s12903-020-01114-6</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Ye</surname><given-names>F</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Xu</surname><given-names>F</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Cao</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>X</given-names></name></person-group><article-title>Children&#x02019;s dental panoramic radiographs dataset for caries segmentation and dental disease detection</article-title><source>Sci Data</source><year>2023</year><volume>10</volume><issue>1</issue><fpage>380</fpage><pub-id pub-id-type="doi">10.1038/s41597-023-02237-5</pub-id><pub-id pub-id-type="pmid">37316638</pub-id>
</element-citation><mixed-citation id="mc-CR27" publication-type="journal">Zhang Y, Ye F, Chen L, Xu F, Chen X, Wu H, Cao M, Li Y, Wang Y, Huang X (2023). Children&#x02019;s dental panoramic radiographs dataset for caries segmentation and dental disease detection. Sci Data 10(1):380. 10.1038/s41597-023-02795-8<pub-id pub-id-type="pmid">37316638</pub-id>
</mixed-citation></citation-alternatives></ref></ref-list></back></article>