<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmc">PMC11679958</article-id><article-id pub-id-type="doi">10.3390/s24248102</article-id><article-id pub-id-type="publisher-id">sensors-24-08102</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Safety After Dark: A Privacy Compliant and Real-Time Edge Computing Intelligent Video Analytics for Safer Public Transportation</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7800-5309</contrib-id><name><surname>Barthelemy</surname><given-names>Johan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-24-08102" ref-type="aff">1</xref><xref rid="c1-sensors-24-08102" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0894-0489</contrib-id><name><surname>Iqbal</surname><given-names>Umair</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af2-sensors-24-08102" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2418-6992</contrib-id><name><surname>Qian</surname><given-names>Yan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-24-08102" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8466-1380</contrib-id><name><surname>Amirghasemi</surname><given-names>Mehrdad</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af3-sensors-24-08102" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6151-6228</contrib-id><name><surname>Perez</surname><given-names>Pascal</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><xref rid="af4-sensors-24-08102" ref-type="aff">4</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Molina L&#x000f3;pez</surname><given-names>Jose Manuel</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-24-08102"><label>1</label>Faculty of Engineering and Information Sciences, University of Wollongong, Wollongong, NSW 2522, Australia; <email>yqian@uow.edu.au</email></aff><aff id="af2-sensors-24-08102"><label>2</label>Centre for Geotechnical Science and Engineering, School of Engineering, University of Newcastle, Newcastle, NSW 2308, Australia; <email>umair.iqbal@newcastle.edu.au</email></aff><aff id="af3-sensors-24-08102"><label>3</label>Faculty of Business and Law, University of Wollongong, Wollongong, NSW 2522, Australia; <email>mehrdad@uow.edu.au</email></aff><aff id="af4-sensors-24-08102"><label>4</label>Australian Urban Research Infrastructure Network (AURIN), University of Melbourne, Melbourne, VIC 3052, Australia; <email>pascal.perez@unimelb.edu.au</email></aff><author-notes><corresp id="c1-sensors-24-08102"><label>*</label>Correspondence: <email>johan@uow.edu.au</email></corresp></author-notes><pub-date pub-type="epub"><day>19</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="collection"><month>12</month><year>2024</year></pub-date><volume>24</volume><issue>24</issue><elocation-id>8102</elocation-id><history><date date-type="received"><day>18</day><month>10</month><year>2024</year></date><date date-type="rev-recd"><day>25</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>17</day><month>12</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; 2024 by the authors.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Public transportation systems play a vital role in modern cities, but they face growing security challenges, particularly related to incidents of violence. Detecting and responding to violence in real time is crucial for ensuring passenger safety and the smooth operation of these transport networks. To address this issue, we propose an advanced artificial intelligence (AI) solution for identifying unsafe behaviours in public transport. The proposed approach employs deep learning action recognition models and utilises technologies like NVIDIA DeepStream SDK, Amazon Web Services (AWS) DirectConnect, local edge computing server, ONNXRuntime and MQTT to accelerate the end-to-end pipeline. The solution captures video streams from remote train stations closed circuit television (CCTV) networks, processes the data in the cloud, applies the action recognition model, and transmits the results to a live web application. A temporal pyramid network (TPN) action recognition model was trained on a newly curated video dataset mixing open-source resources and live simulated trials to identify the unsafe behaviours. The base model was able to achieve a validation accuracy of 93% when trained using open-source dataset samples and was improved to 97% when live simulated dataset was included during the training. The developed AI system was deployed at Wollongong Train Station (NSW, Australia) and showcased impressive accuracy in detecting violence incidents during an 8-week test period, achieving a reliable false-positive (FP) rate of 23%. While the AI correctly identified 30 true-positive incidents, there were 6 cases of false negatives (FNs) where violence incidents were missed during the rainy weather suggesting more data in the training dataset related to bad weather. The AI model&#x02019;s continuous retraining capability ensures its adaptability to various real-world scenarios, making it a valuable tool for enhancing safety and the overall passenger experience in public transport settings.</p></abstract><kwd-group><kwd>safety</kwd><kwd>public transport</kwd><kwd>artificial intelligence (AI)</kwd><kwd>action recognition</kwd><kwd>computer vision</kwd></kwd-group><funding-group><award-group><funding-source>Safety After Dark initiative from Transport For NSW (Australia)</funding-source></award-group><funding-statement>This project was funded by the Safety After Dark initiative from Transport For NSW (Australia).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-24-08102"><title>1. Introduction</title><p>Public transport systems (e.g., buses, trains) are the lifeblood of modern cities, providing a convenient and cost-effective means of transportation for millions of commuters every day [<xref rid="B1-sensors-24-08102" ref-type="bibr">1</xref>,<xref rid="B2-sensors-24-08102" ref-type="bibr">2</xref>]. However, with the growing population density and increasing complexity of urban environments, public transport systems have also become susceptible to safety challenges [<xref rid="B3-sensors-24-08102" ref-type="bibr">3</xref>]. One of the most pressing concerns is the occurrence of violence and abuse within these public spaces, which not only jeopardizes passenger safety but also poses significant operational and reputational risks for transport authorities [<xref rid="B4-sensors-24-08102" ref-type="bibr">4</xref>,<xref rid="B5-sensors-24-08102" ref-type="bibr">5</xref>].</p><p>Acts of violence within public transport settings can range from physical altercations, assaults, and vandalism to more severe incidents like armed attacks and harassment [<xref rid="B6-sensors-24-08102" ref-type="bibr">6</xref>]. Such incidents not only traumatize passengers but also disrupt the smooth functioning of public transport services, leading to potential delays, service interruptions [<xref rid="B7-sensors-24-08102" ref-type="bibr">7</xref>], and increased public fear [<xref rid="B8-sensors-24-08102" ref-type="bibr">8</xref>]. Consequently, addressing and mitigating the risks of violence in public transport has become a critical imperative for ensuring the safety and well-being of passengers and staff.</p><p>Conventional methods for monitoring and addressing security issues in public transport often rely on human-based operator monitoring videos from closed circuit television (CCTV) [<xref rid="B9-sensors-24-08102" ref-type="bibr">9</xref>], which can be labor-intensive, error-prone, and challenging to manage, especially in large and busy transit systems. Human operators may struggle to identify and respond swiftly to incidents in real time, and their effectiveness may be limited by factors such as fatigue and distraction [<xref rid="B10-sensors-24-08102" ref-type="bibr">10</xref>]. It should also be noted that victims do not always report abuse or incidents to the security officers [<xref rid="B11-sensors-24-08102" ref-type="bibr">11</xref>].</p><p>Herein lies the significance of developing an automated end-to-end artificial intelligence (AI) solution for the detection of violence in public transport. AI-based systems offer a transformative approach to public transport security by harnessing the power of computer vision, deep learning, and real-time data analysis. By automating the process of abnormal behaviour detection, AI can enhance the speed, accuracy, and efficiency of security operations, allowing for rapid responses and proactive interventions. The deployment of an AI-driven violence detection system can lead to numerous benefits. First and foremost, it significantly improves passenger safety, fostering an environment where commuters can travel with confidence and peace of mind. Swift detection and response to violent incidents can prevent escalations and potential harm, reducing the overall impact on victims and bystanders. Furthermore, AI-based surveillance systems offer comprehensive coverage, enabling transport authorities to monitor multiple locations and angles simultaneously, thereby supporting the security officers performing the surveillance. The continuous and automated monitoring capabilities ensure that no incident is overlooked, allowing authorities to address security concerns promptly and better understanding where spatio-tempral distributions of the hotspots. Moreover, the implementation of an AI solution for violence detection can optimize resource allocation and operational efficiency. By automating the detection process, transport authorities can allocate their human resources more strategically, focusing on critical intervention and emergency response tasks. To address this problem from an AI perspective, we explore the research questions (RQs) set out in this manuscript:<list list-type="simple"><list-item><label>RQ1:</label><p>How can real-world simulations enhance data generation for AI models, addressing ethical concerns and improving performance in data-scarce scenarios?</p></list-item><list-item><label>RQ2:</label><p>What combination of technologies can be used to develop a real-time AI-driven video analytics pipeline for detecting unsafe behaviours in public transport?</p></list-item></list></p><p>We propose an end-to-end AI solution for detecting inappropriate behaviours in public transportation networks by leveraging an existing CCTV infrastructure and state-of-the-art technologies. The video feed from CCTV cameras is securely streamed to a local edge server using Amazon Web Services (AWS) DirectConnect, ensuring efficient and private data handling. To process the video with privacy compliance in real time, we deploy an action recognition model based on the temporal pyramid network (TPN) [<xref rid="B12-sensors-24-08102" ref-type="bibr">12</xref>] using a local edge computing server (i.e., a graphical processing unit (GPU)-powered edge server at the SMART Infrastructure Facility to perform on-board computing), which excels at capturing dynamic temporal patterns in video streams without any personal identifiers. ONNXRuntime serves as the inference engine, enabling fast and cross-platform model deployment, while NVIDIA GPUs provide the computational power necessary for handling high-resolution video streams with low latency. The real-time video analytics pipeline is developed using DeepStream SDK [<xref rid="B13-sensors-24-08102" ref-type="bibr">13</xref>], optimized for efficient video processing and deep learning integration. For communication between the edge server and the web dashboard, we use MQTT, a lightweight messaging protocol that ensures reliable and timely transmission of inference results. As part of the operational pipeline, the CCTV network captures video, which is securely streamed via AWS DirectConnect to a local edge computing server. There, the trained TPN model is executed for inference using ONNXRuntime, and the resulting outputs are sent to a web dashboard through the MQTT protocol.</p><p>The contributions of the research presented in this work are as follows:<list list-type="order"><list-item><p>Development of a new video dataset from open-source resources and live simulated trials to facilitate the training of TPN action recognition model for unsafe behaviour detection at train stations.</p></list-item><list-item><p>Development, deployment and validation of a privacy-compliant and end-to-end pilot AI solution for the detection of specific behaviours in real time from CCTV cameras.</p></list-item></list></p></sec><sec id="sec2-sensors-24-08102"><title>2. Related Work</title><p>There are very few papers published in regard to violence detection in the transport utility, highlighting the nascent nature of this field. This section reviews the existing solutions where computer vision techniques have been applied to detect violent events in transportation contexts, such as buses and trains. This section aims to provide an overview of the progress made thus far and identify the gaps that remain to be addressed.</p><p>In 2022, Ciampi et al. [<xref rid="B14-sensors-24-08102" ref-type="bibr">14</xref>] introduced the &#x0201c;Bus Violence&#x0201d; dataset, a new benchmark for violence detection in public transport, featuring video clips with simulated violent and non-violent actions under varying conditions. They evaluated several state-of-the-art models pre-trained on general violence datasets and observed that models struggle to generalize to this specific scenario, revealing the domain shift problem. Authors suggested that domain adaptation techniques and unsupervised learning could enhance performance. In 2023, Marteau et al. [<xref rid="B15-sensors-24-08102" ref-type="bibr">15</xref>] used deep learning architectures for violence detection in railway environments. Authors performed a domain adaptation on a state-of-the-art architecture trained on a railway dataset and enhanced it by incorporating a recurrent mechanism to process long-term sequences. The results showed promising performance on the selected datasets (i.e., Hockey Fight, Surveillance Camera Fight), and an evaluation on Suburban Train Fight demonstrated that combining a three-dimensional (3D)-CNN and a graph recurrent unit (GRU) achieved a detection accuracy of 72%.</p><p>In 2023, Kulkarni and Chakraborty [<xref rid="B16-sensors-24-08102" ref-type="bibr">16</xref>] introduced a novel approach to violence detection through a multi-model training framework, leveraging data categorization to address challenges in generalization and model performance. By categorizing violent actions into subsets and training models separately for each, the system used ResNet 3D as the base model to consolidate pre-trained models into a unified network. The proposed method achieved a 2&#x02013;3% accuracy improvement over state-of-the-art approaches. The findings highlighted the potential of data categorization to simplify model development and enhance learning capabilities, with applications beyond violence detection. Most recently, in 2024, Tsiktsiris et al. [<xref rid="B17-sensors-24-08102" ref-type="bibr">17</xref>] proposed a multi-modal abnormal event detection system to enhance passenger safety on public transportation, specifically inside autonomous vehicles. The system integrated RGB, depth, and audio data using a deep learning architecture to detect events like aggression, theft, and vandalism. Experiments, conducted on a custom dataset, showed promising results with an accuracy of 85.1%, outperforming existing methods. The system&#x02019;s real-time capability was validated in autonomous minibuses, demonstrating its practical applicability. However, challenges like computational efficiency and distinguishing similar events remain.</p><p>The reviewed literature highlights the use of deep learning models like 3D-CNN, GRU, ResNet3D and Multimodal approaches for violence detection in public transport systems. While novel datasets such as the Bus Violence dataset and domain adaptation techniques show promise in improving model generalization, the studies reviewed indicate a common limitation: the struggle to apply pre-trained models, often trained on general datasets, to domain-specific scenarios such as public transport. The domain shift problem, evident in the failure of models to generalize across varying conditions, underscores the need for more targeted data collection and domain-adaptation techniques. Moreover, despite the integration of advanced architectures, like 3D-CNN and GRU, issues like occlusions and distinguishing between similar violent actions remain unresolved. While multi-modal systems that combine video, depth, and audio data have shown potential, challenges around real-time performance, computational efficiency, and the accurate differentiation of events persist. Notably, none of the proposed solutions have been fully validated or deployed in real-world, real-time public transport settings.</p></sec><sec id="sec3-sensors-24-08102"><title>3. The Proposed AI Solution for Unsafe Behaviour Detection</title><p>This section presents the details of the proposed AI solution for unsafe behaviour detection, leveraging the power of deep learning action recognition model. The solution aims to continuously monitor the train station platform using CCTV and securely stream the video to a local edge server using AWS DirectConnect, where the video is processed through the deep learning action recognition model (i.e., TPN) using ONNXRuntime inference engine. The model inference results are then transmitted to a web application using MQTT. <xref rid="sensors-24-08102-f001" ref-type="fig">Figure 1</xref> shows the block diagram representation of the proposed solution.</p><p>The high-level architecture of the solution is designed to enable real-time action recognition from CCTV streams, effectively enhancing the security and surveillance capabilities of Sydney Trains. Illustrated in <xref rid="sensors-24-08102-f002" ref-type="fig">Figure 2</xref>, the system seamlessly integrates various technological components including AWS DirectConnect, local edge server, ONNXRuntime, DeepStream SDK and MQTT, ensuring efficient data flow, processing, and secure communication. To begin, the CCTV streams from Sydney Trains, encoded in a H.264/H.265 format, are securely transmitted to the local edge server through AWS DirectConnect. The ingestion and decoding of these video streams are expertly handled using GStreamer [<xref rid="B18-sensors-24-08102" ref-type="bibr">18</xref>], guaranteeing compatibility and optimal video processing. The heart of the system lies in the dedicated local edge server with an AI inference engine, capable of processing each CCTV stream independently. The ONNXRuntime inference engine, powered by the powerful NVIDIA DeepStream 6.0 platform, utilizes NVIDIA GPUs for accelerated AI computations. As the video streams are received, the local edge server performs frame decoding, batching sequences of 32 frames, and initiates AI computations using the efficient ONNXRuntime. When an incident is detected, the system transmits immediate alerts via the secure MQTT protocol to the web application. The technological choices in this solution were carefully selected to ensure efficient, scalable, and secure processing of CCTV streams in real time. AWS DirectConnect was chosen to securely stream the video feeds to the local edge server as it provides a reliable, high-throughput connection with low-latency data transfer, which is crucial for real-time processing. The local edge server was integrated to minimize dependency on cloud-based computing, ensuring faster inference times and reducing bandwidth requirements, as only relevant data (such as alerts) are transmitted. ONNXRuntime was selected as the inference engine due to its cross-platform support and optimization for performance across hardware, enabling the efficient execution of the trained TPN model on the NVIDIA GPUs. Leveraging DeepStream SDK enhances the system&#x02019;s ability to handle high-throughput video streams, offering optimized tools for video decoding, frame batching, and deep learning model integration. The MQTT protocol was employed for its lightweight, reliable, and low-latency communication, ensuring that alerts are sent in real time to the web application. This study assumed uninterrupted video streaming to maintain focus on developing an effective end-to-end AIoT pipeline for real-time unsafe behaviour detection. The system was designed and tested in an ideal operational environment where video feeds are consistently available. This assumption allowed this study to emphasize the accuracy and performance of action recognition and alerting mechanisms under normal conditions.</p><p>The web application consisted of two core elements: a front-end implemented using the React framework, shown in <xref rid="sensors-24-08102-f003" ref-type="fig">Figure 3</xref>, and a Node.js back-end hosted on a dedicated virtual machine. The aim was to facilitate real-time monitoring and exploration of detected incidents, as well as to listen to alerts transmitted through MQTT. The user-friendly web-based interface presents actionable information to security personnel, enabling swift responses to potential threats. Additionally, the web application allows for querying a PostgreSQL database deployed in a container, granting access to historical incident data. This empowers users to conduct in-depth analyses, identify patterns, and draw insights for enhanced situational awareness.</p><p>To address privacy concerns, the system offers an optional module for automatic face blurring, ensuring individual anonymity while preserving the integrity of the surveillance process. This feature enhances the system&#x02019;s ethical compliance and fosters public trust. The solution&#x02019;s modular architecture provides a remarkable advantage, allowing individual components to be easily replaced or relocated without compromising the overall system&#x02019;s performance and robustness. This flexibility eases maintenance efforts and ensures the system&#x02019;s adaptability to evolving requirements. In addition, this study employs the TPN model, which operates solely on a global/image scale without extracting any personal identifiers. The TPN model focuses on the movement patterns of pixels across consecutive video frames to recognize actions, ensuring that the analysis remains at an abstract level, solely interpreting motion data. This design inherently respects individual privacy, as no facial or personally identifiable information is derived or processed during action recognition.</p><p>For secure communication, alerts transmitted from the AI engine to the web application are protected using TLS 1.3, guaranteeing data privacy and integrity during transmission. As the action recognition model is based on the NVIDIA ecosystem, NVIDIA GPUs are instrumental in enabling efficient and accelerated AI computations, both during the model training and inference stages.</p></sec><sec id="sec4-sensors-24-08102"><title>4. Temporal Pyramidal Network (TPN)</title><p>TPN is a framework designed for video action recognition, aiming to capture the dynamics and temporal scale of actions, which are crucial for accurate recognition. TPN operates at the feature-level and can be easily integrated into existing two-dimensional (2D) or 3D backbone networks in a plug-and-play manner. The TPN model consists of two essential components: the feature source and the feature aggregation. The feature source involves collecting hierarchical features from the backbone network, where these features have increasing temporal receptive fields from bottom to top. There are two ways to collect these features: single-depth pyramid and multi-depth pyramid [<xref rid="B12-sensors-24-08102" ref-type="bibr">12</xref>]. <xref rid="sensors-24-08102-f004" ref-type="fig">Figure 4</xref> shows the stature of the TPN network.</p><p>Let us consider a backbone network that produces hierarchical features <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at different depths, where <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the feature at level <italic toggle="yes">i</italic>. These features have increasing temporal receptive fields from bottom to top.</p><list list-type="bullet"><list-item><p>Single-depth pyramid: Choose a base feature <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> at a certain depth and sample along the temporal dimension with different rates <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The TPN consists of <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of sizes <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>T</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>T</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">C</italic> is the number of channels, <italic toggle="yes">T</italic> is the temporal dimension, and <italic toggle="yes">W</italic> and <italic toggle="yes">H</italic> are spatial dimensions.</p></list-item><list-item><p>Multi-depth pyramid: Collect a set of <italic toggle="yes">M</italic> features with increasing depths, resulting in a TPN made of <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of sizes <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>T</mml:mi><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi><mml:mi>M</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>H</mml:mi><mml:mi>M</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where the dimensions satisfy <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This approach captures richer spatial semantics compared to the single-depth pyramid but requires careful feature fusion to ensure correct information flow.</p></list-item></list><p>The TPN captures temporal features at different scales (see <xref rid="sensors-24-08102-f004" ref-type="fig">Figure 4</xref>), which is crucial for handling a wide range of action dynamics, from fine-grained details to global patterns. At the lower levels of the pyramid, the TPN focuses on short-term, high-frequency features that correspond to rapid movements, such as hand gestures or quick body posture changes, ensuring that subtle actions are effectively recognized. At higher levels, it captures long-term features that span larger time windows, enabling the model to understand the global context of an action, such as the full sequence of movements in running or dancing. Additionally, the TPN captures intermediate features that represent transitions between different action phases, like moving from walking to running or standing to sitting, helping recognize actions with smooth and gradual changes. The integration of short-term, intermediate, and long-term features across different temporal scales allows the TPN to build a richer understanding of actions, improving its ability to recognize both rapid movements and slower, continuous motions, making it effective for a wide range of action recognition tasks in real-world environments.</p><p>To align spatial semantics, a spatial semantic modulation is employed. For each but the top-level feature, a stack of convolutions with level-specific stride is applied, matching its spatial shape and receptive field with the top-level feature. Additionally, an auxiliary classification head is appended to it to receive stronger supervision, leading to enhanced semantics. Traditional action recognition methods generally operate with a fixed temporal receptive field, meaning that they only focus on either short-term or long-term features, but not both. This can lead to suboptimal performance when actions contain both rapid, detailed movements and slower, broader patterns. The TPN, in contrast, captures temporal features at multiple scales, allowing it to efficiently process both fast and slow motions. This is accomplished by aggregating features from different temporal scales in a hierarchical manner, ensuring that the model can adapt to the varying duration of actions in real-world environments.</p><p>The overall objective for a backbone network with a TPN becomes
<disp-formula id="FD1-sensors-24-08102"><label>(1)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x02211;</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the original cross-entropy loss, <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the loss for the <italic toggle="yes">i</italic>-th auxiliary head and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents balancing coefficients.</p><p>To control the relative differences of features in terms of temporal scales, a set of hyper-parameters <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is introduced to the TPN for temporal rate modulation. Specifically, <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes that after spatial semantic modulation, the updated feature at the <italic toggle="yes">i</italic>-th level will be temporally downsampled by a factor of <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> using a parametric sub-net.</p><p>For feature aggregation, there are different options (as listed below). The aggregated features from all levels are then rescaled and concatenated for final predictions.</p><list list-type="bullet"><list-item><p>Top-down flow: Aggregating features from higher-level to lower-level in the pyramid.</p></list-item><list-item><p>Bottom-up flow: Aggregating features from lower-level to higher-level in the pyramid.</p></list-item><list-item><p>Lateral flow: Aggregating features at the same level in the pyramid.</p></list-item><list-item><p>Cascade Flow: Combining top-down and bottom-up flows.</p></list-item><list-item><p>Parallel Flow: Simultaneously combining top-down and bottom-up flows.</p></list-item></list></sec><sec id="sec5-sensors-24-08102"><title>5. Experimental Results and Discussions</title><sec id="sec5dot1-sensors-24-08102"><title>5.1. The Safety After Dark (SAD) Dataset</title><p>This section presents the meticulous development of a customized video dataset specifically tailored to facilitate the training of a robust deep learning action recognition model. The dataset comprises a diverse collection of video sources, including publicly available footage and real-world recordings graciously provided by Sydney Trains, ensuring a comprehensive representation of various scenarios encountered in surveillance and video analytics. The dataset was meticulously divided into two distinct categories, each catering to the specific needs of the action recognition model.</p><p>In the first category, we assembled video clips depicting instances of people engaging in physical altercations, sourced from well-established datasets like UCF-Crime [<xref rid="B19-sensors-24-08102" ref-type="bibr">19</xref>] and NTU CCTV-Fights Dataset [<xref rid="B20-sensors-24-08102" ref-type="bibr">20</xref>]. These video clips were further augmented with real-world footage obtained directly from Sydney Trains, capturing authentic scenes of conflicts in different settings. The inclusion of 677 annotated clips in this category enriches the dataset with challenging and critical action classes, reflecting scenarios of vital interest in safety and security applications. The second category of the dataset was thoughtfully curated to encompass routine activities commonly captured by security cameras. Drawing upon the UCF101&#x02014;Action Recognition Dataset [<xref rid="B21-sensors-24-08102" ref-type="bibr">21</xref>]&#x02014;we collected video clips portraying individuals performing everyday actions such as walking, sitting, shaking hands, eating, and children playing. To enhance the diversity of non-violent activities, we further integrated clips from the VIRAT Video Dataset [<xref rid="B22-sensors-24-08102" ref-type="bibr">22</xref>] and NTU RGB+D Dataset [<xref rid="B23-sensors-24-08102" ref-type="bibr">23</xref>,<xref rid="B24-sensors-24-08102" ref-type="bibr">24</xref>]. This meticulous curation resulted in a compilation of 1206 video clips, ensuring a comprehensive coverage of routine actions typically encountered in surveillance settings. In total, in the final version of the SAD dataset, there were 2059 clips, 1353 from &#x0201c;Fighting&#x0201d; class while 706 from the &#x0201c;Normal&#x0201d; class. <xref rid="sensors-24-08102-f005" ref-type="fig">Figure 5</xref> shows the class-wise distribution of the dataset. The class imbalance is evident from the plot with &#x0201c;Fighting&#x0201d; class dominating which may introduce a bias. <xref rid="sensors-24-08102-f006" ref-type="fig">Figure 6</xref> shows the random snapshots from the dataset belonging to both of the action classes.</p><p>To facilitate accurate model training and performance evaluation, each video clip in the dataset was annotated to represent an action class. A standardized format was employed to label each video with its corresponding action class, effectively distinguishing between the &#x0201c;fighting&#x0201d; and &#x0201c;routine activities&#x0201d; categories. This consistent annotation schema ensures a clear understanding of ground truth labels and enables seamless integration into the deep learning action recognition model. Before initiating the model training process, we subjected the video clips to careful preprocessing steps. These steps included resolution normalization and frame rate adjustment to ensure a uniform representation of video data, eliminating potential biases due to variations in video quality. Additionally, the entire dataset was converted to the widely supported MP4 format, enabling easy compatibility with various deep learning frameworks and simplifying the integration process. By providing a meticulously curated and standardized dataset, we contribute significantly to the advancement of action recognition research, fostering the development of more sophisticated video analysis applications across a multitude of domains.</p></sec><sec id="sec5dot2-sensors-24-08102"><title>5.2. Base Model Training</title><p>The base TPN model for fight detection was trained using a patched version of the GluonCV toolkit7 [<xref rid="B25-sensors-24-08102" ref-type="bibr">25</xref>]. The training process involved two phases: initial training and retraining. The model&#x02019;s inference engine was exported and optimized for the ONNXRuntime with the TensorRT backend, utilizing half-precision (FP16) for faster inference. To process the video streams efficiently, the AI inference engine was deployed in a virtual machine equipped with four NVIDIA T4 GPUs, capable of running the model at a constant 20 frames per second (FPS). This hardware setup ensured optimized performance and utilization of the GPUs.</p><p>During the base model training, the model was trained for 50 epochs with a batch size of 10 and a learning rate of 0.001. No warm-up training steps were used. To assess its generalization ability, a validation dataset was created, containing images not seen during training, split using a 75/25 ratio from the complete dataset. Regular evaluation of the model&#x02019;s accuracy was performed on this validation dataset during the training phase. <xref rid="sensors-24-08102-f007" ref-type="fig">Figure 7</xref> depicts the evolution of the validation accuracy over the 50 epochs. At the 38th epoch, the base AI for fight detection achieved an impressive 93% validation accuracy. Throughout training, regular validation on unseen data was carried out to monitor the model&#x02019;s ability to generalize and prevent overfitting.</p></sec><sec id="sec5dot3-sensors-24-08102"><title>5.3. Live Trial and Model Retraining</title><p>The live trial of the AI model was a crucial phase in evaluating its performance in a real-world setting. Sydney Trains provided access to the live video feeds from five CCTVs strategically positioned at the Wollongong Train Station. This trial spanned a period of eight weeks in November/December 2021, during which the AI&#x02019;s ability to detect potential fight incidents was put to the test. Whenever the AI identified a potential fight, the inference engine recorded a five-second video clip of the incident and promptly transmitted an alert. To facilitate the assessment and validation of the alerts generated by the AI, a user-friendly web application was developed. This web-based application provided a comprehensive interface for users to interact with the AI system. Key functionalities of the web application included receiving real-time notifications of incidents, allowing users to confirm or adjust the AI&#x02019;s predictions based on their assessment, tracking the true-positive (TP) and true-negative (TN) rates for performance evaluation, and enabling users to annotate the incidents based on their feedback.</p><p>As actual fight incidents at the Wollongong Train Station were relatively infrequent, the project team planned and conducted two simulation exercises. These simulations took place on the 16th of November and the 6th of December. During these exercises, several University of Wollongong (UOW) staff members played the roles of different actors to enact various fight scenarios in front of the CCTV cameras. The simulation scenarios were designed to encompass different types of fight incidents commonly observed in such public transportation settings. The scenarios included</p><list list-type="bullet"><list-item><p>One-on-one fights: A person walks, then a second person walks towards the first one and attack the first one. The two persons start to fight.</p></list-item><list-item><p>One-against-multiple attacks: A person walks, two persons are standing on the way, and then when the first person comes close, the two persons assault the first one.</p></list-item><list-item><p>Group assaults: A group of persons is chasing and assaulting one individual.</p></list-item><list-item><p>Clashes between groups: Two groups of persons are colliding.</p></list-item></list><p>Each scenario was repeated multiple times with different actors to create a diverse dataset that could represent a wide range of possible real-world scenarios. The first simulation exercise yielded 203 videos, while the second one contributed a larger dataset of 487 videos. This additional data were invaluable for training the AI to recognize and differentiate various fight scenarios effectively.</p><p>Following each simulation exercise, the AI model was retrained using the newly collected data. The retraining process involved incorporating the original training data with the data collected from Simulation 1 and subsequently with the data from both simulation exercises. The additional data from simulations were randomly incorporated into the original dataset, ensuring a balanced representation of scenarios. To prepare for retraining and avoid overfitting, the combined final dataset was then split randomly into 75/25 proportions, with 75% used for model training and 25% held out for validation. The goal was to continuously improve the model&#x02019;s accuracy and performance as more diverse and relevant data became available. The impact of the retraining process was evident in the model&#x02019;s validation accuracy. <xref rid="sensors-24-08102-f008" ref-type="fig">Figure 8</xref> depicted the evolution of accuracy with different training datasets. Augmenting the training data with the simulation data resulted in a noticeable improvement in the model&#x02019;s validation accuracy. The use of a complete dataset that included data from both simulations resulted in higher accuracy compared to using only the dataset collected after Simulation 1 or relying solely on the original data. The iterative retraining approach was instrumental in fine-tuning the AI model&#x02019;s behaviour recognition capabilities, reducing false positives (FPs), and enhancing its overall performance in detecting fight incidents in the transportation network.</p></sec><sec id="sec5dot4-sensors-24-08102"><title>5.4. On-Site Evaluation</title><p>The developed solution was deployed at Wollongong Train Station for its real-world functionality and performance evaluation. In evaluating the performance of our AI models, we used several key evaluation metrics to assess their effectiveness in correctly classifying positive and negative instances. These metrics include accuracy, which measures the proportion of correctly classified instances among all instances; TP rate (Recall), which indicates the model&#x02019;s ability to correctly identify positive cases; TN rate, which reflects the model&#x02019;s success in identifying negative cases; FP rate, which measures the likelihood of incorrectly classifying negative instances as positive; and false-negative (FN) rate, which represents the likelihood of incorrectly classifying positive instances as negative. Additionally, we calculated precision, which evaluates the proportion of true-positive predictions among all positive predictions made by the model, and F1 score, the harmonic mean of precision and recall, providing a balance between the two for cases where both FPs and FNs are critical. These metrics collectively offer a comprehensive view of the model&#x02019;s performance across various aspects of classification.</p><p>During the 8-week test period at Wollongong Train Station, the AI system generated a total of 3600 alerts based on its detection of potential fight incidents. Out of these numerous alerts, only 36 were confirmed as actual fight incidents. Among the confirmed incidents, the AI correctly identified 30 as true positives, indicating instances where fights were accurately detected. However, there were six incidents that were missed by the AI, resulting in FNs, which means the AI failed to identify those fights. The initial FP rate observed during the live trial was relatively high, with the AI generating up to one FP per CCTV second. However, continuous updates and retraining of the AI model led to improvements in the actual FP rate after each iteration. Specifically, the first update of the model, incorporating the original data along with data from the first simulation exercise, demonstrated substantial enhancements in key performance indicators. For instance, accuracy, true negative rate, and FP rate were notably improved in events collected for a week after the deployment of the updated model (<xref rid="sensors-24-08102-t001" ref-type="table">Table 1</xref>). The primary objective was to achieve a low FP rate, essential for ensuring the reliability and operational effectiveness of the solution.</p><p>The base model exhibited relatively poor overall performance, with an accuracy of only 0.16, which indicates that the model struggled to differentiate between the &#x0201c;Fight&#x0201d; and &#x0201c;Normal&#x0201d; classes (see <xref rid="sensors-24-08102-t001" ref-type="table">Table 1</xref>). This low accuracy suggests a substantial misclassification rate, which is further reflected in the extreme imbalance between the TP and TN (see <xref rid="sensors-24-08102-f009" ref-type="fig">Figure 9</xref>). While the TP for the base model is 0.86, indicating a strong ability to identify the positive class (Fight), the TN is alarmingly low at 0.14, which implies that the model failed to recognize the negative class (Normal) effectively. This imbalance is compounded by a FP of 0.86, suggesting that the model frequently misclassified &#x0201c;Normal&#x0201d; instances as &#x0201c;Fight&#x0201d;. The FN, while low at 0.14, does not fully compensate for the high number of FP, which is a critical drawback for practical applications where FP can have significant consequences.</p><p>In contrast, the retrained model demonstrated substantial improvements across most metrics. The accuracy increased to 0.77, which indicates a marked enhancement in the model&#x02019;s ability to correctly classify both positive (Fight) and negative (Normal) instances. The retrained model&#x02019;s improved performance is particularly evident in its TN of 0.77, which indicates a much better recognition of the negative class compared to the base model. This reduction in FP to 0.23 suggests that the retrained model is more reliable and accurate when distinguishing between the two classes. Additionally, the precision for the retrained model improved to 0.76, reflecting a much stronger ability to predict the positive class correctly. This, in turn, resulted in an improved F1 score of 0.74, signalling a better balance between precision and recall. However, it is worth noting that the retrained model&#x02019;s TP decreased to 0.73, slightly lower than the 0.86 observed in the base model. This decrease in recall suggests that while the retrained model is more effective at minimizing FPs, it does so at the cost of missing some true-positive instances. This trade-off highlights a common challenge in machine learning: the balancing act between precision and recall.</p><p>Overall, the retrained model outperformed the base model in several key metrics, including accuracy, precision, and F1 score. While there is a slight sacrifice in recall, the retrained model&#x02019;s overall performance is far more reliable and balanced (see <xref rid="sensors-24-08102-t001" ref-type="table">Table 1</xref> and <xref rid="sensors-24-08102-f009" ref-type="fig">Figure 9</xref>), making it better suited for deployment in practical scenarios where both positive and negative class detection are critical. The improvements in the retrained model reflect the positive impact of additional data and retraining, suggesting that further refinement with additional fine-tuning or more diverse datasets could yield even better performance. This final model, deployed on the 21st of December, successfully lowered the FP rate to between 3 and 12 per hour per CCTV feed, representing a considerable reduction. Notably, this progress was achieved despite conducting only two simulation exercises to gather additional data (see <xref rid="sensors-24-08102-f010" ref-type="fig">Figure 10</xref>).</p><p>An interesting finding during the live test was the impact of rainy weather on the FP rate. The day with the highest number of FPs (152) occurred on the 28 December, coinciding with heavy rainfall of 31 mm, as reported by the Bureau of Meteorology. <xref rid="sensors-24-08102-f011" ref-type="fig">Figure 11</xref> illustrates the distribution of the FP rate per hour for that particular day, revealing that the majority of FPs occurred during the daytime, with a peak between 1 pm and 3 pm.</p><p>The influence of rainy weather on FPs was attributed to the scarcity of training data that depicted incidents occurring during such weather conditions. Rain introduced significant noise and dynamic elements into the captured video feed, which impacted the AI&#x02019;s predictions, leading to a higher FP rate. However, the AI&#x02019;s performance remained unaffected by low light conditions at night, as only a few FPs were detected during night-time. This resilience was attributed to the diverse range of luminosity present in the training data, with some video clips recorded during night time or in black and white.</p><p>The on-site evaluation demonstrated the AI approach&#x02019;s potential for real-time incident detection and its ability to aid transportation operators in responding promptly to incidents. The successful application at Wollongong Train Station indicates the scalability of the solution, making it feasible for extension to other locations and adaptability to different types of behaviours, provided sufficient data are available for retraining the AI. An additional advantage of the deployed model is its ability to be continuously retrained and improved. User assessments of the AI&#x02019;s predictions through the developed web application played a vital role in enhancing its accuracy and fine-tuning its behaviour recognition capabilities in real-world scenarios.</p></sec></sec><sec id="sec6-sensors-24-08102"><title>6. Challenges and Future Research Directions</title><p>The development a smart sensing video-based action recognition pipeline involves several challenges. One of the major hurdles was obtaining ethics clearance and legal permissions to access live video feeds from security cameras at Wollongong Train Station. Ensuring compliance with privacy regulations and obtaining the necessary approvals for data collection was a time-consuming and resource-intensive process. Moreover, the COVID-19 pandemic posed significant impacts on the project. UOW&#x02019;s restrictions on student involvement in simulations and New South Wales (NSW) Health Orders preventing in-person simulations led to delays in the project timeline. Additionally, acquiring sufficient training data to teach the AI to recognize a specific behaviour proved to be a challenge, prompting the project team to organize simulation exercises in collaboration with Transport for NSW and Sydney Trains. Deploying AI also resulted in a high number of FPs, which needs to be addressed before it can be operationalized by Sydney Trains. While the AI showed consistent performance across indoor and outdoor settings and different times of the day, it struggled with rainy weather, indicating the need for more training data in such conditions.</p><p>Several future research directions have been identified to enhance the effectiveness of the developed AI-based surveillance system. First and foremost, improving the accuracy of the current model requires continuous data collection, especially in challenging weather conditions like rainy weather. In real-world scenarios with complex conditions, such as low light, rainy weather, or high crowd density, a higher rate of FPs is expected. While not ideal, this is preferable to having high FNs, as it prioritizes safety by ensuring potentially unsafe behaviours are detected. These FPs will be systematically added to the training dataset, providing valuable data to improve model performance in future deployments. By collecting this automatically annotated data, the model can be retrained periodically, with each iteration enhancing its accuracy and reliability in challenging environments. Overall, continuous training and access to more data are expected to improve the AI&#x02019;s performance over time, gradually reducing FPs and achieving human-level accuracy. The primary aim of this study was to develop a lightweight, operational pipeline capable of real-time monitoring on edge computing infrastructure, rather than to achieve the absolute highest accuracy in action recognition. However, future research may explore integrating more advanced models, such as transformer-based action recognition models (e.g., action transformer (AcT) [<xref rid="B26-sensors-24-08102" ref-type="bibr">26</xref>]) or visual language models (VLMs) (e.g., VILA [<xref rid="B27-sensors-24-08102" ref-type="bibr">27</xref>]). Moreover, exploring a model based on pose estimation, using 3D skeletons to capture subtle behaviours, presents a promising direction. This approach would require a new annotation exercise for the collected dataset and has the potential to enhance the AI&#x02019;s ability to detect more complex actions. To fully leverage GPUs for efficient decoding and AI computations, an updated version of the inference pipeline can be developed using a complete DeepStream implementation instead of the current one based on ONNXRuntime and TensorRT.</p><p>While this study successfully demonstrated the feasibility and effectiveness of a real-time AIoT solution for monitoring unsafe behaviours, it did so under the assumption of continuous video feed availability. Future research could extend the system&#x02019;s resilience to handle real-world operational challenges, such as video stream interruptions or network failures. Potential enhancements include incorporating local buffering on edge devices, on-device action recognition for brief outages, and multi-path streaming to alternate servers for redundancy. Additionally, incorporating automatic reconnection protocols and cloud failover instances could further enhance the robustness and reliability of the system.</p></sec><sec sec-type="conclusions" id="sec7-sensors-24-08102"><title>7. Conclusions</title><p>In this study, we developed an advanced AI solution to detect violence and unsafe behaviours in public transport systems, particularly trains. The TPN action recognition model was trained and deployed to identify fighting and normal behaviours in the CCTV streams from train station. The proposed solution was deployed over local server where AI inference was carried out using the ONNXRuntime and inference results were transmitted to a web application using MQTT. The base model was deployed at Wollongong Train Station, and simulated data were collected which helped in improving the model training performance to 97% from 94%. The final solution with the retrained model was tested for eight weeks as a field testing and was able to achieve the reliable FP rate of 23% and correctly identified 30 true-positive incidents. However, six FNs were observed, indicating the need for further improvement, especially in recognizing incidents during adverse weather conditions. Continuous retraining of the AI model played a vital role in refining its behaviour recognition capabilities and reducing FPs. The success of the deployment showcased the solution&#x02019;s scalability and adaptability, making it applicable to other locations and behaviours with sufficient data for retraining.</p></sec></body><back><ack><title>Acknowledgments</title><p>We acknowledge the support the NVIDIA Applied Research Accelerator program, which provided the GPU resources for training and deploying the models detailed in this work.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.B., U.I., Y.Q., M.A. and P.P.; methodology, J.B., U.I., Y.Q., M.A. and P.P.; software, J.B., U.I., Y.Q. and M.A.; validation, J.B., U.I., Y.Q. and M.A.; formal analysis, J.B., U.I., Y.Q. and M.A.; investigation, J.B., Y.Q. and M.A.; resources, J.B., Y.Q., M.A. and P.P.; data curation, J.B., Y.Q. and M.A.; writing&#x02014;original draft preparation, J.B., U.I., Y.Q. and M.A.; writing&#x02014;review and editing, J.B., U.I., Y.Q. and M.A.; supervision, J.B. and P.P. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>Data are contained within the article.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflicts of interest.</p></notes><glossary><title>Abbreviations</title><p>The following abbreviations are used in this manuscript:
<array><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">AWS</td><td align="left" valign="middle" rowspan="1" colspan="1">Amazon Web Service</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">CCTV</td><td align="left" valign="middle" rowspan="1" colspan="1">Closed Circuit Television</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TPN</td><td align="left" valign="middle" rowspan="1" colspan="1">Temporal Pyramid Network</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">AI</td><td align="left" valign="middle" rowspan="1" colspan="1">Artificial Intelligence</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">RQ</td><td align="left" valign="middle" rowspan="1" colspan="1">Research Question</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GPU</td><td align="left" valign="middle" rowspan="1" colspan="1">Graphical Processing Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">3D</td><td align="left" valign="middle" rowspan="1" colspan="1">Three-Dimensional</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">GRU</td><td align="left" valign="middle" rowspan="1" colspan="1">Graph Recurrent Unit</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">2D</td><td align="left" valign="middle" rowspan="1" colspan="1">Two-Dimensional</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FPS</td><td align="left" valign="middle" rowspan="1" colspan="1">Frames per Second</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TP</td><td align="left" valign="middle" rowspan="1" colspan="1">True Positive</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">TN</td><td align="left" valign="middle" rowspan="1" colspan="1">true negative</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">UOW</td><td align="left" valign="middle" rowspan="1" colspan="1">University of Wollongong</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FP</td><td align="left" valign="middle" rowspan="1" colspan="1">False Positive</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">FN</td><td align="left" valign="middle" rowspan="1" colspan="1">False Negative</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">NSW</td><td align="left" valign="middle" rowspan="1" colspan="1">New South Wales</td></tr></tbody></array></p></glossary><ref-list><title>References</title><ref id="B1-sensors-24-08102"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Saif</surname><given-names>M.A.</given-names></name>
<name><surname>Zefreh</surname><given-names>M.M.</given-names></name>
<name><surname>Torok</surname><given-names>A.</given-names></name>
</person-group><article-title>Public transport accessibility: A literature review</article-title><source>Period. Polytech. Transp. Eng.</source><year>2019</year><volume>47</volume><fpage>36</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.3311/PPtr.12072</pub-id></element-citation></ref><ref id="B2-sensors-24-08102"><label>2.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Tuvikene</surname><given-names>T.</given-names></name>
<name><surname>Keblowski</surname><given-names>W.</given-names></name>
<name><surname>Weicker</surname><given-names>T.</given-names></name>
<name><surname>Finch</surname><given-names>J.</given-names></name>
<name><surname>Sgibnev</surname><given-names>W.</given-names></name>
<name><surname>Strauli</surname><given-names>L.</given-names></name>
<name><surname>Laine</surname><given-names>S.</given-names></name>
<name><surname>Dobruszkes</surname><given-names>F.</given-names></name>
<name><surname>Ianchenko</surname><given-names>A.</given-names></name>
</person-group><source>Public Transport as Public Space in European Cities</source><publisher-name>DEU</publisher-name><publisher-loc>&#x00130;zmir, T&#x000fc;rkiye</publisher-loc><year>2021</year><volume>Volume 39</volume></element-citation></ref><ref id="B3-sensors-24-08102"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Broere</surname><given-names>W.</given-names></name>
</person-group><article-title>Urban underground space: Solving the problems of today&#x02019;s cities</article-title><source>Tunn. Undergr. Space Technol.</source><year>2016</year><volume>55</volume><fpage>245</fpage><lpage>248</lpage><pub-id pub-id-type="doi">10.1016/j.tust.2015.11.012</pub-id></element-citation></ref><ref id="B4-sensors-24-08102"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Sousa</surname><given-names>D.C.B.d.</given-names></name>
<name><surname>Pitombo</surname><given-names>C.S.</given-names></name>
<name><surname>Rocha</surname><given-names>S.S.</given-names></name>
<name><surname>Salgueiro</surname><given-names>A.R.</given-names></name>
<name><surname>Delgado</surname><given-names>J.P.M.</given-names></name>
</person-group><article-title>Violence in public transportation: An approach based on spatial analysis</article-title><source>Rev. Saude Publica</source><year>2017</year><volume>51</volume><fpage>127</fpage><pub-id pub-id-type="doi">10.11606/S1518-8787.2017051007085</pub-id><pub-id pub-id-type="pmid">29236883</pub-id>
</element-citation></ref><ref id="B5-sensors-24-08102"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cardoso</surname><given-names>M.H.S.</given-names></name>
<name><surname>Santos</surname><given-names>T.F.</given-names></name>
<name><surname>Silva</surname><given-names>M.A.V.d.</given-names></name>
</person-group><article-title>Violence in public transport: An analysis of resilience and vulnerability in the city of rio de Janeiro</article-title><source>Urbe. Rev. Bras. Gest&#x000e3;o Urbana</source><year>2021</year><volume>13</volume><fpage>e20200231</fpage><pub-id pub-id-type="doi">10.1590/2175-3369.013.e20200231</pub-id></element-citation></ref><ref id="B6-sensors-24-08102"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kacharo</surname><given-names>D.K.</given-names></name>
<name><surname>Teshome</surname><given-names>E.</given-names></name>
<name><surname>Woltamo</surname><given-names>T.</given-names></name>
</person-group><article-title>Safety and security of women and girls in public transport</article-title><source>Urban Plan. Transp. Res.</source><year>2022</year><volume>10</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1080/21650020.2022.2027268</pub-id></element-citation></ref><ref id="B7-sensors-24-08102"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Smith</surname><given-names>M.J.</given-names></name>
<name><surname>Clarke</surname><given-names>R.V.</given-names></name>
</person-group><article-title>Crime and public transport</article-title><source>Crime Justice</source><year>2000</year><volume>27</volume><fpage>169</fpage><lpage>233</lpage><pub-id pub-id-type="doi">10.1086/652200</pub-id></element-citation></ref><ref id="B8-sensors-24-08102"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Coppola</surname><given-names>P.</given-names></name>
<name><surname>Silvestri</surname><given-names>F.</given-names></name>
</person-group><article-title>Gender inequality in safety and security perceptions in railway stations</article-title><source>Sustainability</source><year>2021</year><volume>13</volume><elocation-id>4007</elocation-id><pub-id pub-id-type="doi">10.3390/su13074007</pub-id></element-citation></ref><ref id="B9-sensors-24-08102"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Velastin</surname><given-names>S.A.</given-names></name>
<name><surname>Vicencio-Silva</surname><given-names>M.A.</given-names></name>
<name><surname>Lo</surname><given-names>B.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
<name><surname>Khoudour</surname><given-names>L.</given-names></name>
</person-group><article-title>A distributed surveillance system for improving security in public transport networks</article-title><source>Meas. Control</source><year>2002</year><volume>35</volume><fpage>209</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1177/002029400203500704</pub-id></element-citation></ref><ref id="B10-sensors-24-08102"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Elhamod</surname><given-names>M.</given-names></name>
<name><surname>Levine</surname><given-names>M.D.</given-names></name>
</person-group><article-title>Automated real-time detection of potentially suspicious behavior in public transport areas</article-title><source>IEEE Trans. Intell. Transp. Syst.</source><year>2012</year><volume>14</volume><fpage>688</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1109/TITS.2012.2228640</pub-id></element-citation></ref><ref id="B11-sensors-24-08102"><label>11.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>McGuire</surname><given-names>J.</given-names></name>
<name><surname>Evans</surname><given-names>E.</given-names></name>
<name><surname>Kane</surname><given-names>E.</given-names></name>
<name><surname>McGuire</surname><given-names>J.</given-names></name>
<name><surname>Evans</surname><given-names>E.</given-names></name>
<name><surname>Kane</surname><given-names>E.</given-names></name>
</person-group><article-title>Victim Counselling and Support: A Review of Police, Community and Therapeutic Interventions</article-title><source>Evidence-Based Policing and Community Crime Prevention</source><publisher-name>Springer</publisher-name><publisher-loc>Cham, Switzerland</publisher-loc><year>2021</year><fpage>361</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-76363-3_10</pub-id></element-citation></ref><ref id="B12-sensors-24-08102"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Yang</surname><given-names>C.</given-names></name>
<name><surname>Xu</surname><given-names>Y.</given-names></name>
<name><surname>Shi</surname><given-names>J.</given-names></name>
<name><surname>Dai</surname><given-names>B.</given-names></name>
<name><surname>Zhou</surname><given-names>B.</given-names></name>
</person-group><article-title>Temporal pyramid network for action recognition</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>13&#x02013;19 June 2020</conf-date><fpage>591</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00067</pub-id></element-citation></ref><ref id="B13-sensors-24-08102"><label>13.</label><element-citation publication-type="webpage"><person-group person-group-type="author">
<collab>NVIDIA</collab>
</person-group><article-title>NVIDIA DeepStream SDK</article-title><comment>Available online: <ext-link xlink:href="https://developer.nvidia.com/deepstream-sdk" ext-link-type="uri">https://developer.nvidia.com/deepstream-sdk</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-09-11">(accessed on 11 September 2024)</date-in-citation></element-citation></ref><ref id="B14-sensors-24-08102"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ciampi</surname><given-names>L.</given-names></name>
<name><surname>Foszner</surname><given-names>P.</given-names></name>
<name><surname>Messina</surname><given-names>N.</given-names></name>
<name><surname>Staniszewski</surname><given-names>M.</given-names></name>
<name><surname>Gennaro</surname><given-names>C.</given-names></name>
<name><surname>Falchi</surname><given-names>F.</given-names></name>
<name><surname>Serao</surname><given-names>G.</given-names></name>
<name><surname>Cogiel</surname><given-names>M.</given-names></name>
<name><surname>Golba</surname><given-names>D.</given-names></name>
<name><surname>Szcz&#x00119;sna</surname><given-names>A.</given-names></name>
<etal/>
</person-group><article-title>Bus violence: An open benchmark for video violence detection on public transport</article-title><source>Sensors</source><year>2022</year><volume>22</volume><elocation-id>8345</elocation-id><pub-id pub-id-type="doi">10.3390/s22218345</pub-id><pub-id pub-id-type="pmid">36366043</pub-id>
</element-citation></ref><ref id="B15-sensors-24-08102"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Marteau</surname><given-names>T.</given-names></name>
<name><surname>Afanon</surname><given-names>S.</given-names></name>
<name><surname>Sodoyer</surname><given-names>D.</given-names></name>
<name><surname>Ambellouis</surname><given-names>S.</given-names></name>
</person-group><article-title>Violence detection in railway environment with modern deep learning approaches and small dataset</article-title><source>Transp. Res. Procedia</source><year>2023</year><volume>72</volume><fpage>87</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1016/j.trpro.2023.11.348</pub-id></element-citation></ref><ref id="B16-sensors-24-08102"><label>16.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Kulkarni</surname><given-names>M.</given-names></name>
<name><surname>Chakraborty</surname><given-names>R.</given-names></name>
</person-group><article-title>Violent Activity Detection on Public Transportation using Surveillance Footage</article-title><source>Proceedings of the 2023 16th International Conference on Developments in eSystems Engineering (DeSE)</source><conf-loc>Istanbul, Turkey</conf-loc><conf-date>18&#x02013;20 December 2023</conf-date><fpage>737</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1109/DeSE60595.2023.10468681</pub-id></element-citation></ref><ref id="B17-sensors-24-08102"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tsiktsiris</surname><given-names>D.</given-names></name>
<name><surname>Lalas</surname><given-names>A.</given-names></name>
<name><surname>Dasygenis</surname><given-names>M.</given-names></name>
<name><surname>Votis</surname><given-names>K.</given-names></name>
</person-group><article-title>Multimodal Abnormal Event Detection in Public Transportation</article-title><source>IEEE Access</source><year>2024</year><volume>12</volume><fpage>133469</fpage><lpage>133480</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2024.3425308</pub-id></element-citation></ref><ref id="B18-sensors-24-08102"><label>18.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Newmarch</surname><given-names>J.</given-names></name>
<name><surname>Newmarch</surname><given-names>J.</given-names></name>
</person-group><article-title>GStreamer</article-title><source>Linux Sound Programming</source><publisher-name>Apress</publisher-name><publisher-loc>Berkeley, CA, USA</publisher-loc><year>2017</year><fpage>211</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1007/978-1-4842-2496-0_10</pub-id></element-citation></ref><ref id="B19-sensors-24-08102"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sultani</surname><given-names>W.</given-names></name>
<name><surname>Chen</surname><given-names>C.</given-names></name>
<name><surname>Shah</surname><given-names>M.</given-names></name>
</person-group><article-title>Real-world anomaly detection in surveillance videos</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Salt Lake City, UT, USA</conf-loc><conf-date>18&#x02013;23 June 2018</conf-date><fpage>6479</fpage><lpage>6488</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00678</pub-id></element-citation></ref><ref id="B20-sensors-24-08102"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Perez</surname><given-names>M.</given-names></name>
<name><surname>Kot</surname><given-names>A.C.</given-names></name>
<name><surname>Rocha</surname><given-names>A.</given-names></name>
</person-group><article-title>Detection of real-world fights in surveillance videos</article-title><source>Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><conf-loc>Brighton, UK</conf-loc><conf-date>12&#x02013;17 May 2019</conf-date><fpage>2662</fpage><lpage>2666</lpage><pub-id pub-id-type="doi">10.1109/ICASSP.2019.8683676</pub-id></element-citation></ref><ref id="B21-sensors-24-08102"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Soomro</surname><given-names>K.</given-names></name>
<name><surname>Zamir</surname><given-names>A.R.</given-names></name>
<name><surname>Shah</surname><given-names>M.</given-names></name>
</person-group><article-title>UCF101: A dataset of 101 human actions classes from videos in the wild</article-title><source>arXiv</source><year>2012</year><pub-id pub-id-type="doi">10.48550/arXiv.1212.0402</pub-id><pub-id pub-id-type="arxiv">1212.0402</pub-id></element-citation></ref><ref id="B22-sensors-24-08102"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Oh</surname><given-names>S.</given-names></name>
<name><surname>Hoogs</surname><given-names>A.</given-names></name>
<name><surname>Perera</surname><given-names>A.</given-names></name>
<name><surname>Cuntoor</surname><given-names>N.</given-names></name>
<name><surname>Chen</surname><given-names>C.C.</given-names></name>
<name><surname>Lee</surname><given-names>J.T.</given-names></name>
<name><surname>Mukherjee</surname><given-names>S.</given-names></name>
<name><surname>Aggarwal</surname><given-names>J.</given-names></name>
<name><surname>Lee</surname><given-names>H.</given-names></name>
<name><surname>Davis</surname><given-names>L.</given-names></name>
<etal/>
</person-group><article-title>A large-scale benchmark dataset for event recognition in surveillance video</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Colorado Springs, CO, USA</conf-loc><conf-date>20&#x02013;25 June 2011</conf-date><fpage>3153</fpage><lpage>3160</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2011.5995586</pub-id></element-citation></ref><ref id="B23-sensors-24-08102"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Shahroudy</surname><given-names>A.</given-names></name>
<name><surname>Perez</surname><given-names>M.</given-names></name>
<name><surname>Wang</surname><given-names>G.</given-names></name>
<name><surname>Duan</surname><given-names>L.Y.</given-names></name>
<name><surname>Kot</surname><given-names>A.C.</given-names></name>
</person-group><article-title>Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2019</year><volume>42</volume><fpage>2684</fpage><lpage>2701</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2019.2916873</pub-id><pub-id pub-id-type="pmid">31095476</pub-id>
</element-citation></ref><ref id="B24-sensors-24-08102"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Shahroudy</surname><given-names>A.</given-names></name>
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Ng</surname><given-names>T.T.</given-names></name>
<name><surname>Wang</surname><given-names>G.</given-names></name>
</person-group><article-title>Ntu rgb+ d: A large scale dataset for 3d human activity analysis</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><conf-loc>Las Vegas, NV, USA</conf-loc><conf-date>27&#x02013;30 June 2016</conf-date><fpage>1010</fpage><lpage>1019</lpage><pub-id pub-id-type="doi">10.48550/arXiv.1604.02808</pub-id></element-citation></ref><ref id="B25-sensors-24-08102"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Guo</surname><given-names>J.</given-names></name>
<name><surname>He</surname><given-names>H.</given-names></name>
<name><surname>He</surname><given-names>T.</given-names></name>
<name><surname>Lausen</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>M.</given-names></name>
<name><surname>Lin</surname><given-names>H.</given-names></name>
<name><surname>Shi</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>C.</given-names></name>
<name><surname>Xie</surname><given-names>J.</given-names></name>
<name><surname>Zha</surname><given-names>S.</given-names></name>
<etal/>
</person-group><article-title>GluonCV and GluonNLP: Deep learning in computer vision and natural language processing</article-title><source>J. Mach. Learn. Res.</source><year>2020</year><volume>21</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.5555/3455716.3455739</pub-id></element-citation></ref><ref id="B26-sensors-24-08102"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mazzia</surname><given-names>V.</given-names></name>
<name><surname>Angarano</surname><given-names>S.</given-names></name>
<name><surname>Salvetti</surname><given-names>F.</given-names></name>
<name><surname>Angelini</surname><given-names>F.</given-names></name>
<name><surname>Chiaberge</surname><given-names>M.</given-names></name>
</person-group><article-title>Action transformer: A self-attention model for short-time pose-based human action recognition</article-title><source>Pattern Recognit.</source><year>2022</year><volume>124</volume><fpage>108487</fpage><pub-id pub-id-type="doi">10.1016/j.patcog.2021.108487</pub-id></element-citation></ref><ref id="B27-sensors-24-08102"><label>27.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lin</surname><given-names>J.</given-names></name>
<name><surname>Yin</surname><given-names>H.</given-names></name>
<name><surname>Ping</surname><given-names>W.</given-names></name>
<name><surname>Molchanov</surname><given-names>P.</given-names></name>
<name><surname>Shoeybi</surname><given-names>M.</given-names></name>
<name><surname>Han</surname><given-names>S.</given-names></name>
</person-group><article-title>Vila: On pre-training for visual language models</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Seattle, WA, USA</conf-loc><conf-date>16&#x02013;22 June 2024</conf-date><fpage>26689</fpage><lpage>26699</lpage></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-24-08102-f001"><label>Figure 1</label><caption><p>The block diagram representation of the proposed AIoT solution for unsafe behaviour detection.</p></caption><graphic xlink:href="sensors-24-08102-g001" position="float"/></fig><fig position="float" id="sensors-24-08102-f002"><label>Figure 2</label><caption><p>The architecture of the proposed AIoT solution for the unsafe behaviour detection.</p></caption><graphic xlink:href="sensors-24-08102-g002" position="float"/></fig><fig position="float" id="sensors-24-08102-f003"><label>Figure 3</label><caption><p>The Safety After Dark web-based front-end.</p></caption><graphic xlink:href="sensors-24-08102-g003" position="float"/></fig><fig position="float" id="sensors-24-08102-f004"><label>Figure 4</label><caption><p>The architecture of the temporal pyramid network (TPN) model for action recognition (image taken from [<xref rid="B12-sensors-24-08102" ref-type="bibr">12</xref>]).</p></caption><graphic xlink:href="sensors-24-08102-g004" position="float"/></fig><fig position="float" id="sensors-24-08102-f005"><label>Figure 5</label><caption><p>Class-wise distribution of Safety After Dark (SAD) dataset.</p></caption><graphic xlink:href="sensors-24-08102-g005" position="float"/></fig><fig position="float" id="sensors-24-08102-f006"><label>Figure 6</label><caption><p>Random dataset snapshots from fighting and normal classes. Top row represent samples from &#x0201c;Fight&#x0201d; class. Bottom row represent samples from &#x0201c;Normal&#x0201d; class with no fighting.</p></caption><graphic xlink:href="sensors-24-08102-g006" position="float"/></fig><fig position="float" id="sensors-24-08102-f007"><label>Figure 7</label><caption><p>Base TPN model training accuracy curve.</p></caption><graphic xlink:href="sensors-24-08102-g007" position="float"/></fig><fig position="float" id="sensors-24-08102-f008"><label>Figure 8</label><caption><p>Model retraining using simulated datasets collected during the live trials.</p></caption><graphic xlink:href="sensors-24-08102-g008" position="float"/></fig><fig position="float" id="sensors-24-08102-f009"><label>Figure 9</label><caption><p>Confusion matrices for the base and retrained models.</p></caption><graphic xlink:href="sensors-24-08102-g009" position="float"/></fig><fig position="float" id="sensors-24-08102-f010"><label>Figure 10</label><caption><p>Field evaluation of the ai model in terms of false positives (FPs) (the green line represents the time when weights of the model were updated with latest training iteration).</p></caption><graphic xlink:href="sensors-24-08102-g010" position="float"/></fig><fig position="float" id="sensors-24-08102-f011"><label>Figure 11</label><caption><p>Field evaluation of the AI model in terms of false positives (FPs) for a rainy day.</p></caption><graphic xlink:href="sensors-24-08102-g011" position="float"/></fig><table-wrap position="float" id="sensors-24-08102-t001"><object-id pub-id-type="pii">sensors-24-08102-t001_Table 1</object-id><label>Table 1</label><caption><p>On-site evaluation of the retrained ai model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">&#x000a0;</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">TP Rate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">TN Rate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FP Rate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">FN Rate</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">F1 Score</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">Base Model</td><td align="center" valign="middle" rowspan="1" colspan="1">0.16</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86</td><td align="center" valign="middle" rowspan="1" colspan="1">0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86</td><td align="center" valign="middle" rowspan="1" colspan="1">0.14</td><td align="center" valign="middle" rowspan="1" colspan="1">0.5</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Retrained with Sim2 Data</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.27</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.76</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.73</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.74</td></tr></tbody></table></table-wrap></floats-group></article>