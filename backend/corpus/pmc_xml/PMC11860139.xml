<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006230</article-id><article-id pub-id-type="pmc">PMC11860139</article-id><article-id pub-id-type="doi">10.3390/s25041002</article-id><article-id pub-id-type="publisher-id">sensors-25-01002</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Real Steps or Not: Auto-Walker Detection in Move-to-Earn Applications</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5216-0266</contrib-id><name><surname>Lee</surname><given-names>Sunwoo</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Billis</surname><given-names>Antonis</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Dominguez-Morales</surname><given-names>Manuel</given-names></name><role>Academic Editor</role></contrib><contrib contrib-type="editor"><name><surname>Civit</surname><given-names>Anton</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01002">Department of Information Security, Seoul Women&#x02019;s University, 621, Hwarang-ro, Nowon-gu, Seoul 01797, Republic of Korea; <email>sun.lee@swu.ac.kr</email></aff><pub-date pub-type="epub"><day>07</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1002</elocation-id><history><date date-type="received"><day>01</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>04</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>06</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the author.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>In recent times, the emergence of Move-to-Earn (M2E) applications has revolutionized the intersection of digital innovation and physical wellness. Unlike their predecessors in the Play-to-Earn (P2E) domain, M2E apps incentivize physical activity, offering rewards for real-world movement such as walking or running. This shift aligns with a growing global focus on health consciousness that is propelled by the widespread adoption of smartphones and an increased awareness of the benefits of maintaining an active lifestyle. However, the rising popularity of these platforms has also brought about new problematic activities, with some users exploiting additional automated devices to simulate physical activity and claim rewards. In response, we propose an AI-based method aimed at distinguishing genuine user engagement from artificially generated auto-walker activity to ensure the integrity of reward distributions in M2E platforms. To demonstrate the generalizability of our model, we use a total of six open gait datasets and auto-walker datasets of automatic walking devices measured with various smartphones. Under unbiased and transparent evaluation, our model shows its ability to effectively discriminate auto-walker and genuine gait data not only on the seen datasets but also on the unseen datasets; it attained an F1-score of 0.997 on the auto-walker datasets and an F1-score of 1.000 on the genuine datasets.</p></abstract><kwd-group><kwd>fraud detection</kwd><kwd>M2E</kwd><kwd>gait</kwd><kwd>MEMS sensor</kwd><kwd>AI</kwd><kwd>signal processing</kwd></kwd-group><funding-group><award-group><funding-source>Seoul Women&#x02019;s University</funding-source><award-id>2024-0006</award-id></award-group><award-group><funding-source>Seoul Business Agency(SBA) funded by Seoul Metropolitan Government</funding-source><award-id>QR240004</award-id></award-group><award-group><funding-source>Korea government(MSIT)</funding-source><award-id>2022-0-00995</award-id></award-group><funding-statement>This work was supported by a research grant from Seoul Women&#x02019;s University (2024-0006). This research was supported by Seoul R&#x00026;BD Program (QR240004) through the Seoul Business Agency (SBA) funded by Seoul Metropolitan Government. This work was supported in part by Institute of Information &#x00026; communications Technology Planning &#x00026; Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00995, Automated Reliable Source Code Generation from Natural Language Description).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01002"><title>1. Introduction</title><p>In recent times, there has been a significant surge in popularity of reward-based applications, particularly against the backdrop of the economic climate defined by the &#x0201c;three highs&#x0201d;: elevated inflation, interest rates, and exchange rates. This period has compelled consumers to adopt a more frugal mindset and actively seek avenues to reduce personal expenses by capitalizing on savings opportunities, however minor. Amid this economic landscape, reward apps have emerged as a promising avenue, as they offer users the opportunity to earn monetary rewards for completing simple tasks through the app interface. These tasks range from watching advertisements and answering quizzes to achieving specific step counts, effectively monetizing everyday activities. Among the myriad of reward-based applications, Move-to-Earn (M2E) apps have become distinguished. Evolving from the Play-to-Earn (P2E) model, which primarily rewarded users within gaming environments, M2E apps shift the focus toward promoting healthier lifestyles. By incentivizing physical activities such as walking or running, these apps aim to reward users who incorporate more active routines into their daily lives. A report by Grand View Research, Inc. projects that the global fitness apps market encompassing M2E applications will reach USD 1.8 billion by 2030, with an expected compound annual growth rate (CAGR) of 18.3% during the forecast period [<xref rid="B1-sensors-25-01002" ref-type="bibr">1</xref>]. This growth is propelled by the widespread adoption of smartphones and a growing consciousness about the importance of maintaining a healthy lifestyle. As the prioritization of well-being becomes more prevalent in society, we can expect that the appeal and uptake of M2E apps will rise significantly as well, posing as integral tools that foster a culture of holistic health and wellness. A notable advantage of these app-based reward systems is the flexibility they offer users to simultaneously engage with multiple platforms. For instance, a user can amplify potential rewards by installing several apps that reward similar activities, such as achieving a specific step count, effectively maximizing their earnings from the platforms.</p><p>However, a concerning trend has grown in parallel with the rising popularity of reward apps: the emergence of users seeking to exploit these platforms for financial gain through deceptive means. One prevalent method involves the use of automatic walking devices, or &#x0201c;auto-walkers&#x0201d;, designed to simulate steps without actual physical movements. While companies operating these apps expect rewards to be distributed based on genuine user engagement and physical exertion, some users utilize automatic walking devices to claim rewards for performing the action. These practices enable users to accumulate rewards at an accelerated rate&#x02014;one that is well beyond what companies intended.</p><p>This paper addresses the challenge of distinguishing genuine human activities from auto-walker-generated movements in Move-to-Earn (M2E) applications. We propose an AI-driven approach that analyzes gait signals&#x02014;capturing differences in movement patterns between actual human walking and artificially generated motion from automatic walking devices as detected by smartphone sensors. While most existing studies on gait analysis have focused on human activity recognition (HAR) [<xref rid="B2-sensors-25-01002" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01002" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01002" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01002" ref-type="bibr">5</xref>] or disease diagnosis [<xref rid="B6-sensors-25-01002" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01002" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01002" ref-type="bibr">8</xref>], anomaly detection in digital platforms has primarily been applied to financial fraud detection, identifying suspicious transactions and behavioral anomalies [<xref rid="B9-sensors-25-01002" ref-type="bibr">9</xref>]. However, these methods are not designed to detect motion-based fraud in M2E applications. Our work is the first to specifically address this issue, introducing a robust fraud detection mechanism that ensures the integrity of reward-based platforms. By accurately identifying fraudulent activity, our method helps prevent exploitation, ensuring that rewards are distributed fairly based on genuine user engagement and effort.</p><p>Our summarized contributions are as follows:<list list-type="bullet"><list-item><p><italic toggle="yes">Development of a Detection Method for M2E Fraud:</italic> We present an AI-based detection method specifically designed to address the unique challenge of detecting fraudulent activities in Move-to-Earn (M2E) platforms. By distinguishing genuine human movements from those generated by automatic walking devices, our approach targets a critical and underexplored issue in the M2E ecosystem. Our method provides a robust solution for ensuring the integrity of M2E platforms. The effectiveness of our approach is demonstrated by achieving an F1-score of 0.997 on datasets containing auto-walker-generated movements and an F1-score of 1.000 on datasets of genuine human gait. To the best of our knowledge, this is the first method explicitly developed to protect M2E platforms from fraudulent behavior, thereby fostering trust and reliability in digital wellness ecosystems.</p></list-item><list-item><p><italic toggle="yes">A Comparative Model Study:</italic> We evaluate and compare the performance of several models, each selected for its potential to accurately discriminate genuine human gait data and those generated by auto-walkers. The models to be compared include a Convolutional Neural Network (CNN), which are designed to capture and analyze spatial and temporal features within the data. Beyond deep learning models, we include traditional machine learning models in our comparative study: Random Forest (RF), Support Vector Machine (SVM), and k-Nearest Neighbor (kNN). This comprehensive model comparison illuminates the most effective techniques for distinguishing between human and auto-walker data patterns.</p></list-item><list-item><p><italic toggle="yes">Evaluation of the Auto-walker Detection Model in M2E Ecosystems:</italic> We systematically evaluate the effectiveness of our proposed method specifically against the prevalent use of simplistic mechanical devices, commonly referred to as auto-walkers or automatic walking devices. These devices are designed to mimic human movements, thereby generating false gait data without physical activity on the part of the user. Our evaluation demonstrates the practical applicability of our model by distinguishing genuine physical activities from those generated by such devices.</p></list-item><list-item><p><italic toggle="yes">Generalizability of Model Across Unseen Data:</italic> Our method has exceptional generalizability, evidenced by its consistent accuracy in identifying problematic activity across both new, unseen gait data from different individuals and auto-walker data generated by various devices. We use a total of six open gait datasets and gait data from automatic walking devices measured with various smartphones. Under unbiased and transparent evaluation, our model has an F1-score of 0.994 on the unseen auto-walker datasets and an F1-score of 1.000 on the unseen genuine datasets. This robustness, tested across diverse datasets, showcases the model&#x02019;s universal applicability and underscores its potential as a reliable tool in real-world scenarios, irrespective of the automatic walking device types and the smartphones.</p></list-item></list></p></sec><sec id="sec2-sensors-25-01002"><title>2. Motivation</title><p>There is a burgeoning class of M2E applications that incentivize physical activity without necessitating high initial investments, such as the purchase of NFT sneakers. Examples of such applications include Charity Miles [<xref rid="B10-sensors-25-01002" ref-type="bibr">10</xref>], Sweatcoin [<xref rid="B11-sensors-25-01002" ref-type="bibr">11</xref>], Evidation [<xref rid="B12-sensors-25-01002" ref-type="bibr">12</xref>], BetterPoints [<xref rid="B13-sensors-25-01002" ref-type="bibr">13</xref>], and Runtopia [<xref rid="B14-sensors-25-01002" ref-type="bibr">14</xref>]. These platforms provide a compelling proposition by rewarding users for engaging in physical activities and eliminating the barrier of costly digital assets. However, this ease of access makes these platforms particularly attractive to actors seeking to maximize rewards through minimal effort. Notably, devices such as automatic walking devices can artificially generate up to 10,000 steps per hour without any actual physical movement, presenting a significant challenge in distinguishing genuine walking activity from auto-walker activity.</p><p>The integrity of M2E platforms is crucial to their success. Auto-walker activity not only undermines trust in these platforms but also artificially inflates the distribution of rewards, thereby jeopardizing the economic model. Consequently, there is an urgent need to develop a sophisticated detection method capable of effectively countering such behavior. This need underpins the motivation for our research. It is imperative to safeguard the long-term viability of M2E applications by ensuring that rewards are distributed appropriately and that user trust and engagement are maintained. By addressing these challenges, we aim to reinforce the foundational principles upon which the M2E ecosystem is built, creating a secure environment, where users can participate in physical activities and legitimately earn rewards.</p></sec><sec id="sec3-sensors-25-01002"><title>3. Related Works</title><p><bold>Human Activity Recognition and Gait Analysis.</bold> Extensive research has been conducted in human activity recognition (HAR) and gait analysis, focusing on classifying various human movements such as walking, running, and sitting, as well as analyzing gait patterns for disease diagnosis [<xref rid="B2-sensors-25-01002" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01002" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01002" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01002" ref-type="bibr">5</xref>,<xref rid="B6-sensors-25-01002" ref-type="bibr">6</xref>,<xref rid="B7-sensors-25-01002" ref-type="bibr">7</xref>,<xref rid="B8-sensors-25-01002" ref-type="bibr">8</xref>]. Many studies have leveraged machine learning and deep learning models to recognize motion patterns or detect abnormalities in human gait.</p><p>For instance, Kaur et al. proposed a machine learning-based method to diagnose multiple sclerosis by analyzing spatiotemporal and kinematic gait features, demonstrating the potential of gait data in clinical applications [<xref rid="B6-sensors-25-01002" ref-type="bibr">6</xref>]. Similarly, Takallou et al. developed a deep learning-based approach to diagnose peripheral arterial disease using accelerometer data collected from a wearable device placed on the waist, emphasizing the role of sensor-based motion analysis in disease detection [<xref rid="B7-sensors-25-01002" ref-type="bibr">7</xref>]. In another study, Del et al. employed a statistical analysis-based method to diagnose Parkinson&#x02019;s disease (PD) using accelerometer-based body-worn monitors, highlighting the effectiveness of accelerometer signals in identifying neurodegenerative disorders [<xref rid="B8-sensors-25-01002" ref-type="bibr">8</xref>].</p><p>Beyond medical applications, several studies have explored HAR using various classification techniques. Raj et al. applied Convolutional Neural Networks (CNNs) to classify different human activities, demonstrating the capability of deep learning in recognizing motion patterns [<xref rid="B2-sensors-25-01002" ref-type="bibr">2</xref>]. Nurwulan et al. utilized Random Forest models for HAR, while Chathuramali et al. proposed a Support Vector Machine (SVM)-based approach, and Mohsen et al. investigated the use of k-Nearest Neighbors (k-NN) for activity classification [<xref rid="B3-sensors-25-01002" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01002" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01002" ref-type="bibr">5</xref>].</p><p>While these studies have significantly advanced HAR and gait analysis, they do not address the fundamental distinction between genuine human gait and artificially generated gait produced by mechanical devices. The existing research primarily focuses on recognizing human movements or diagnosing medical conditions but does not consider cases where motion sensor data are manipulated or artificially induced, such as in fraudulent activities within Move-to-Earn (M2E) applications.</p><p><bold>Fraud Detection in Digital Platforms.</bold> In contrast to HAR studies, research on fraud detection in digital platforms has largely focused on identifying anomalies in financial transactions and behavioral patterns. Traditional fraud detection methods primarily rely on analyzing complex relational patterns within financial networks to flag suspicious transactions or activities [<xref rid="B9-sensors-25-01002" ref-type="bibr">9</xref>]. These approaches have been effective in detecting fraudulent behaviors in financial systems but are not directly applicable to motion-based fraud detection, as they do not account for the sensor-driven manipulation of physical activities. In M2E applications, users are rewarded based on their physical activity data, making motion-based fraud a novel and emerging concern. Fraudsters can exploit low-cost auto-walkers (priced under USD 5) to artificially generate movement and accumulate rewards without engaging in actual physical activity. Despite the growing adoption of M2E platforms, there is currently no dedicated research addressing the detection of motion-based fraud or differentiating between genuine and artificially generated movement.</p><p>This critical research gap underscores the need for novel fraud detection techniques that leverage motion sensor data to ensure the integrity of M2E ecosystems. Unlike traditional fraud detection methods that primarily target financial transactions, effective fraud prevention in M2E applications requires the ability to distinguish genuine human gait from artificially generated movement.</p></sec><sec id="sec4-sensors-25-01002"><title>4. Attack Model</title><p>We consider an actor who seeks to obtain a large reward in M2E ecosystems at a minimal cost: by using simplistic devices commonly referred to as auto-walkers or automatic walking devices. These devices can deceive activity tracking systems with simple, repetitive movements, effectively registering physical activity even though the user has not performed it. Such a strategy enables actors to gain rewards with minimal effort, bypassing the need for direct manipulation of the app&#x02019;s software or its internal data and avoiding the costs associated with genuine physical activity. Accordingly, we do not consider actors who directly manipulate app software or internal data in this paper. In response to these challenges, our goal is to devise a robust model that detects such auto-walker activity facilitated by these simple devices.</p></sec><sec id="sec5-sensors-25-01002"><title>5. Our Method</title><p>Our method is specifically designed to detect and distinguish automated walking patterns from genuine human gait data, ensuring the integrity of M2E platforms. Our approach not only identifies fraudulent walking patterns generated by auto-walkers but also accurately classifies genuine human movements as valid.</p><p>To provide a meaningful comparison, we evaluate and compare the performance of several widely used models in behavior analysis [<xref rid="B2-sensors-25-01002" ref-type="bibr">2</xref>,<xref rid="B3-sensors-25-01002" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01002" ref-type="bibr">4</xref>,<xref rid="B5-sensors-25-01002" ref-type="bibr">5</xref>]. We integrate both deep learning and traditional machine learning techniques to perform a comprehensive evaluation of various model architectures. This includes models like Convolutional Neural Networks (CNNs), which are designed to capture and analyze spatial and temporal features within motion data. Additionally, beyond deep learning approaches, we include traditional machine learning models such as Random Forest (RF), Support Vector Machine (SVM), and k-Nearest Neighbors (kNN) in our comparative study. A brief description of the models used in our evaluation is provided below, while the detailed layer configurations of the deep learning models are illustrated in <xref rid="sensors-25-01002-f001" ref-type="fig">Figure 1</xref>.</p><list list-type="bullet"><list-item><p><bold>CNN Model:</bold> We evaluate a CNN model, which is particularly effective at capturing the spatial and temporal features of the data [<xref rid="B15-sensors-25-01002" ref-type="bibr">15</xref>].</p></list-item><list-item><p><bold>RF Model:</bold> We evaluate a Random Forest model, which is a robust ensemble method that combines multiple decision trees to improve generalizability and reduce overfitting [<xref rid="B16-sensors-25-01002" ref-type="bibr">16</xref>].</p></list-item><list-item><p><bold>SVM Model:</bold> We also evaluate a Support Vector Machine model, which works well with high-dimensional data and employs a kernel trick to transform data into higher dimensions [<xref rid="B17-sensors-25-01002" ref-type="bibr">17</xref>].</p></list-item><list-item><p><bold>kNN Model:</bold> We finally evaluate a k-Nearest Neighbor model which is a simple, intuitive model that classifies data points based on the majority vote of their k-Nearest Neighbors [<xref rid="B18-sensors-25-01002" ref-type="bibr">18</xref>].</p></list-item></list><p>To effectively distinguish genuine gait data from those of auto-walker movements, we implement a hybrid classification approach that combines deep learning models with binary SVM [<xref rid="B19-sensors-25-01002" ref-type="bibr">19</xref>,<xref rid="B20-sensors-25-01002" ref-type="bibr">20</xref>,<xref rid="B21-sensors-25-01002" ref-type="bibr">21</xref>]. Initially, deep learning models are trained for multi-class classification to differentiate between N+1 classes, where N represents the types of auto-walker movements, and the additional class represents genuine gait data. Each deep learning model generates a probability vector indicating the likelihood of the input data belonging to each class. For final classification, these probability vectors are fed into a binary SVM model. The SVM is trained to label genuine gait data as legitimate (class 1) and auto-walker data as illegitimate (class 0), enabling the binary model to make the ultimate decision. This hybrid framework leverages the feature extraction capabilities of deep learning models while enhancing classification robustness with the binary SVM, ensuring the accurate differentiation between genuine and fraudulent movements.</p><p>We construct and evaluate the models in two ways according to model type. First, for deep learning models, since each model acts as a feature extractor for genuine and auto-walker data, we train each model to perform multi-class classification: If the auto-walkers have N movement types, the models are trained to classify N+1 multi-class classification problems, including the gait class (becoming class 0). Our ultimate goal is for the model to determine the genuine gait data as legitimate and auto-walker data as illegitimate data, which becomes a binary classification problem. Accordingly, we combine SVM models to deep learning models. Each deep learning model generates a probability vector indicating the likelihood of each data belonging to a particular class. Subsequently, the input data are classified based on the class corresponding to the highest probability element in the vector. These probability vectors are labeled as class 1 (i.e., True) for the genuine gait data and class 0 (i.e., False) for the auto-walker data, and then they are fed into the binary SVM model. The binary model makes the final classification decision regarding whether the data are genuine or not.</p><p>For the traditional machine learning models, we extract features from the data and then feed them into each model. Similarly, since our goal is to determine if the input data are genuine, we label those gait data (i.e., the extracted features from genuine gait data as class 1 and auto-walker data as class 0), and then we train each machine learning model.</p><p><bold>Feature Extraction.</bold> To effectively distinguish between genuine human gait and the movements generated by automated devices, we develop a comprehensive feature engineering approach specifically tailored to the unique challenges of M2E fraud detection. By extracting meaningful features from raw gait data, we aim to capture the nuanced differences between human and mechanical movement patterns.</p><p>Our approach begins with the extraction of statistical features that represent the overall characteristics of raw gait signals, which are widely used in motion analysis [<xref rid="B22-sensors-25-01002" ref-type="bibr">22</xref>]. These features include integrated absolute value, mean absolute value, variance, root mean square, standard deviation, mean value, and median absolute deviation. Additionally, we calculate simple moving averages (using simple, advanced, absolute simple, and absolute advanced methods), skewness, kurtosis, interquartile range, energy, and entropy (calculated using four different methods). These features provide a comprehensive statistical summary of the data, capturing global trends, variability, and temporal characteristics.</p><p>To improve the model&#x02019;s ability to capture temporal patterns specific to gait data, we additionally extract peak-related features. These include the indices and heights of the three highest peaks in the raw gait signals. Furthermore, we calculate the differences in indices and heights between adjacent peaks, capturing the temporal and amplitude relationships within the gait cycle. For example, if the peak indices and heights are denoted as <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the derived features include <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. These features are particularly effective at capturing periodic and structural differences between human and mechanical movements. Since each gait dataset consists of six distinct signals (three axes from both accelerometer and gyroscope sensors), this feature extraction process results in a total of 174 features per dataset. By systematically incorporating both statistical summaries and temporal peak characteristics, this feature set is uniquely tailored to address the complexities of M2E fraud detection, providing a rich representation of the data that facilitates robust classification.</p><p>We evaluate whether our features are appropriate for gait data. To this end, we employ a scatter method because feature points typically cluster by participant if the features extracted from their gait data are associated. Conversely, a lack of association results in the features being dispersed. It is noted that at least two features are necessary to generate a scatter plot. However, given that the feature dimensions exceed two, we utilize a Random Forest model to classify all 23 participants in the GeoTecINIT dataset [<xref rid="B23-sensors-25-01002" ref-type="bibr">23</xref>]. From the complete set of features, we then select two. The RF model provides a feature importance score, indicating the utility of each in classifying the participants. <xref rid="sensors-25-01002-f002" ref-type="fig">Figure 2</xref> shows a scatter plot of the features extracted from the gait data of 11 participants in the GeoTecINIT dataset, alongside the data from auto-walkers 1 and 2. The clustering observed between features extracted from the 11 participants and those from the auto-walkers suggests that these features effectively capture the characteristics of the gait data. Among the total 174 features, the top 15 in terms of importance are listed in <xref rid="sensors-25-01002-t001" ref-type="table">Table 1</xref>. <xref rid="sensors-25-01002-f003" ref-type="fig">Figure 3</xref> shows the results of classifying all 23 participants in the GeoTecINIT dataset, indicating that our features well reflect the characteristics of each participant&#x02019;s gait data.</p></sec><sec id="sec6-sensors-25-01002"><title>6. Experimental Setup</title><sec id="sec6dot1-sensors-25-01002"><title>6.1. Open Gait Datasets</title><p>To evaluate the performance of the model in identifying genuine human gait as legitimate, we utilize six publicly available gait datasets. These datasets contain extensive gait signals collected from multiple participants engaged in walking activities. The diverse activities recorded under real-world conditions provide a robust foundation for developing and testing the model. However, these datasets do not consistently provide detailed demographic information about participants, such as age, physical condition, or other personal attributes. To address this limitation, we rely on the inherent diversity within the datasets, which are collected in varied environments and contexts. This variability helps ensure a certain level of robustness in the data. Additionally, the use of publicly available datasets ensures an unbiased and transparent evaluation of the model. A summary of the open datasets is provided in <xref rid="sensors-25-01002-t002" ref-type="table">Table 2</xref>.</p><p><bold>ADL Dataset.</bold> The Activities of Daily Living (ADL) dataset [<xref rid="B24-sensors-25-01002" ref-type="bibr">24</xref>] comprises data from the motions performed during five different daily activities: walking, running, standing, walking up the stairs, and walking down the stairs. Data were collected from a group of 25 participants in Portugal, consisting of 15 men and 10 women. The age range of the participants spanned 16 to 60 years old. For the data measurement, each participant was equipped with a BQ Aquaris 5.7 smartphone [<xref rid="B25-sensors-25-01002" ref-type="bibr">25</xref>] placed in a waistband. The built-in accelerometer and gyroscope sensors measured data at a frequency of 100 Hz, and the magnetometer sampled at 50 Hz. The data measurements were performed in different environments, such as in a hall, on a street, and in other places. Since M2E apps generally count walking steps, only the walking data from this dataset were utilized. Additionally, we exclusively used data recorded by accelerometer and gyroscope sensors for our evaluation.</p><p><bold>FLAAP Dataset.</bold> The FLAAP dataset [<xref rid="B26-sensors-25-01002" ref-type="bibr">26</xref>] contains data from ten activities&#x02014;including walking, sitting, standing, jogging, sitting cross-legged, lying down, walking in a circle, going up the stairs up, going down the stairs, and sitting up&#x02014;measured from eight different participants. Participants were all men and had an average of 169.125 cm in height, 66.00 kg in weight, and 29.75 years in age. A smartphone was placed at the center of each participant&#x02019;s body and the built-in accelerometer and gyroscope sensors measured data for each activity at a sampling frequency of 100 Hz. The data measurements were performed at Banaras Hindu University in India. Note that only data for walking and walking in a circle were used for our evaluation.</p><p><bold>GeoTecINIT Dataset.</bold> The GeoTecINIT dataset [<xref rid="B23-sensors-25-01002" ref-type="bibr">23</xref>,<xref rid="B27-sensors-25-01002" ref-type="bibr">27</xref>] contains data pertaining to sequences of specific activities&#x02014;such as sitting, standing up, walking, turning, and sitting down&#x02014;measured from 23 participants. The participants represented diverse age demographics, ranging from 23 to 66 years old, and gender balance, featuring a male-to-female ratio of 56% to 44%. Data were measured using built-in sensors at a sampling rate of 100 Hz while the participants placed a Xiaomi Poco X3 Pro smartphone in their left pocket and wore a TicWatch Pro 3 GPS smartwatch on their left wrist. The data measurements were performed at Jaume I University in Spain. Note that only walking data were used for our evaluation.</p><p><bold>MotionSense Dataset.</bold> The MotionSense dataset [<xref rid="B28-sensors-25-01002" ref-type="bibr">28</xref>,<xref rid="B29-sensors-25-01002" ref-type="bibr">29</xref>] comprises data from six different activities: going down the stairs, going up the stairs, walking, jogging, sitting, and standing. Data were collected from 24 participants, including 14 males and 10 females. The participants&#x02019; weight ranged from 40 kg to 100 kg, their height from 161 cm to 190 cm, and their age from 18 to 35 years. An iPhone 6s, located in the front pocket of the participant&#x02019;s pants, measured data using built-in accelerometer and gyroscope sensors at a sampling rate of 50 Hz. The data measurements were performed around the Queen Mary University of London&#x02019;s Mile End campus. Note that only walking data were used for our evaluation, and the data were upscaled to 100 Hz to match the sampling frequency of other datasets.</p><p><bold>Nazli Dataset.</bold> The Nazli dataset [<xref rid="B30-sensors-25-01002" ref-type="bibr">30</xref>] contains gait signals collected from 93 participants. The participants consisted of 47 males and 46 females, and their weight ranged from 44 kg to 137 kg, their height from 144 cm to 204 cm, and their age from 17 to 53 years. One smartphone was placed on the right thigh and one on the left side of the waist, and gait signals were measured using built-in accelerometer and gyroscope sensors while each participant walked between two endpoints (320 m). The dataset did not include details regarding the locations of the measurements or the sampling rate. Nonetheless, we inferred that the sampling rate was approximately 100 Hz by analyzing the timestamps in each data file.</p><p><bold>USC-HAD Dataset.</bold> The USC-HAD dataset [<xref rid="B31-sensors-25-01002" ref-type="bibr">31</xref>,<xref rid="B32-sensors-25-01002" ref-type="bibr">32</xref>] contains data of twelve different activities: walking forward, walking to the left, walking to the right, walking up the stairs, walking down the stairs, running forward, jumping, sitting, standing, riding the elevator up, and riding the elevator down. Data were collected from 14 participants, including seven males and seven females. The participants&#x02019; weight ranged from 43 kg to 80 kg, their height from 160 cm to 185 cm, and their age from 21 to 49 years. A MotionNode [<xref rid="B33-sensors-25-01002" ref-type="bibr">33</xref>] located in the participant&#x02019;s front right hip pocket measured data at a sampling rate of 100 Hz utilizing its built-in accelerometer and gyroscope sensors. Data collection occurred across a number of days and was conducted in both indoor and outdoor settings. Note that only data for walking forward, walking to the left, and walking to the right were used for our evaluation.</p></sec><sec id="sec6dot2-sensors-25-01002"><title>6.2. Auto-Walking Devices</title><p>To evaluate the performance of models in detecting data generated by auto-walking devices, we employ two types of auto-walkers (c.f., there are only two types on the market). These devices are distinguished by their rotational movements: One type rotates around the X-axis, while the other revolves around the Z-axis. We denote the device rotating around the X-axis as auto-walker 1, and the device rotating around the Z-axis as auto-walker 2. <xref rid="sensors-25-01002-f004" ref-type="fig">Figure 4</xref> shows the orientation axes (X, Y, Z) of a standard smartphone and the two types of auto-walking devices. Each auto-walking device was purchased from AliExpress [<xref rid="B34-sensors-25-01002" ref-type="bibr">34</xref>] at a low cost of approximately USD 1 and USD 5, respectively.</p><p>We utilize six smartphones, namely, the iPhone 7, X, 11, 12, 13, and 15. <xref rid="sensors-25-01002-f005" ref-type="fig">Figure 5</xref>a&#x02013;f illustrate the accelerometer and gyroscope data measured from each smartphone using auto-walker 1, the device that rotates around the X-axis. Despite the identical movement of the auto-walker, each smartphone records distinct sensor data.</p><p>To provide a clearer comparison between auto-walker-generated and genuine human gait, we include accelerometer and gyroscope data from six randomly selected subjects from the ADL open dataset. These genuine gait signals are presented alongside the auto-walker data in <xref rid="sensors-25-01002-f005" ref-type="fig">Figure 5</xref>g&#x02013;l, allowing for a direct comparison of motion patterns. The comparison highlights the distinct characteristics of natural human gait, which exhibits greater variability due to individual biomechanics, in contrast to the periodic and structured oscillations observed in auto-walker-generated movement. Although the difference between generated and genuine gait signals is visually apparent, M2E applications often fail to distinguish them and incorrectly classify artificially generated motion as real human activity. This misclassification occurs because human gait also inherently follows a periodic pattern, making it challenging to differentiate from simulated movement. This limitation further underscores the need for a robust classification model capable of capturing these differences to prevent fraudulent activity in M2E applications.</p><p><bold>Data Measurements.</bold> Movement data from the two types of auto-walking devices were measured using different iPhone models. Data from auto-walker 1 were measured over a total of 3.08 h, while auto-walker 2 data were measured for a total of 1.17 h. Additionally, to evaluate the detection performance of the models over time, movement data were measured for auto-walkers 1 (for a total of 0.41 h) and auto-walker 2 (for a total of 0.42 h) using both the iPhone 7 and iPhone 13 after a period of two weeks. In total, 5.07 h of data were measured for the movements of auto-walkers 1 and 2.</p></sec></sec><sec id="sec7-sensors-25-01002"><title>7. Performance Metrics</title><p>In this section, we define the performance metrics used to evaluate our method. An important factor when evaluating these model is that both must identify genuine gait data as legitimate and gait data generated by the auto-walkers as illegitimate.</p><list list-type="bullet"><list-item><p><bold>False Negative Rate (FNR)</bold>: The rate of data generated by auto-walkers but determined to be genuine (i.e., the rate of false rejections).</p></list-item><list-item><p><bold>False Positive Rate (FPR)</bold>: The rate of genuine gait data but determined to be illegitimate (i.e., the rate of false acceptances).</p></list-item><list-item><p><bold>Equal Error Rate (EER)</bold>: The point at which FNR and FPR are equal, a point that is widely used to measure the performance of a model.</p></list-item><list-item><p><bold>Precision</bold>: The ratio of data correctly classified as belonging to a specific class out of all the data the model predicted to belong to that class, indicating that Precision focuses on reducing false positives.</p></list-item><list-item><p><bold>Recall</bold>: The ratio of data in a class that the model correctly classified out of all the data in that class, indicating that Recall, also known as sensitivity, focuses on reducing false negatives.</p></list-item><list-item><p><bold>F1-score</bold>: The harmonic mean of Precision and Recall, which is widely applied to balance Precision and Recall and is used when evaluating classification models. The F1-score is calculated as<disp-formula><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>Recall</mml:mi></mml:mrow><mml:mrow><mml:mi>Precision</mml:mi><mml:mo>+</mml:mo><mml:mi>Recall</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item></list></sec><sec id="sec8-sensors-25-01002"><title>8. Evaluation</title><p>In this section, we present how to divide datasets into a training dataset and a test dataset, how to train the models, and what the optimized hyperparameters of each model are. The performance of each model is evaluated based on whether it can determine genuine gait data as genuine and detect auto-walker data as illegitimate.</p><sec id="sec8dot1-sensors-25-01002"><title>8.1. Data Configuration for Training and Testing Models</title><p>Our evaluation of the auto-walker detection model encompasses both seen and unseen data to comprehensively evaluate performance. In other words, other gait signals from participants included in the training set (i.e., other data from seen classes) are expected to be accurately classified as legitimate. Also, signals from other participants not included in the training set (i.e., data from unseen classes) should also be correctly identified as genuine data, affirming the model&#x02019;s ability to generalize across different individuals. Similarly, data for the auto-walkers measured with a specific smartphone model included in the training set (i.e., other auto-walker data from seen classes) must be detected as illegitimate, and the data for the auto-walkers measured with other smartphone models not included in the training set (i.e., auto-walker data from unseen classes) must be also detected as illegitimate.</p><p>This subsection thus details how to configure data into training/test sets and additional test sets on unseen data to ensure a thorough evaluation of our model&#x02019;s performance.</p><p><bold>Training/Test Data.</bold> Initially, we preprocess the data from each dataset by segmenting them into arrays with dimensions of (200, 6), representing two seconds of data from a three-axis accelerometer and a three-axis gyroscope. To train and test our model, we utilize complete data from all participants in the FLAAP dataset (eight out of eight participants) and the GeoTecINIT dataset (23 out of 23 participants), along with a subset from the Nazli dataset (59 out of 93 participants). Each participant&#x02019;s data are then split into training and test sets at a ratio of 0.8 to 0.2, respectively. As a result, within the FLAAP dataset, the total shapes for the training and test sets are (1168, 200, 6) and (290, 200, 6), respectively. For the GeoTecINIT dataset, these dimensions are (378, 200, 6) for training data and (70, 200, 6) for the test data. In the case of the Nazli dataset, the training and test sets are shaped as (11948, 200, 6) and (2928, 200, 6), correspondingly. We use data from the remaining 34 out of 93 participants in the Nazli dataset, which are not utilized in the training sets, to evaluate the model&#x02019;s performance on unseen gait data. Additionally, we train and test our model using gait data generated by auto-walkers 1 and 2 measured with the iPhone 7 and 13. The training set includes data from auto-walkers 1 and 2 measured with iPhone 7 and 13, totaling 2.11 h and 0.59 h, respectively. For the test set, the data for auto-walkers 1 and 2 are measured for a total of 0.53 h and 0.15 h, respectively.</p><p>In summary, the training and test sets for the seen gait data have shapes of (13494, 200, 6) and (3288, 200, 6), respectively, sourced from a total of 90 participants. The shapes of the training and test sets for the seen auto-walker data are (4857, 200, 6) and (1213, 200, 6), respectively.</p><p><bold>Unseen Test Data.</bold> For unseen gait data, we use data from participants not included in the training sets of the open datasets. Specifically, the ADL dataset provides gait signals from 25 participants, resulting in a test set shaped as (1393, 200, 6). The MotionSense dataset contributes gait signals from 24 participants, with a dataset shape of (3336, 200, 6). From the Nazli dataset, we include the gait signals of the remaining 34 participants, leading to a shape of (7852, 200, 6). Lastly, the USC-HAD dataset offers data from 14 participants, with a dataset shape of (6295, 200, 6). As for the unseen auto-walker data, data from auto-walkers 1 and 2 were measured with the iPhone X, 11, 12, and 15 models.</p><p>Therefore, the combined test sets for the unseen gait data have a total shape of (18876, 200, 6), sourced from a total of 97 participants, and the shape of the test sets for the unseen auto-walker data is (1574, 200, 6)</p></sec><sec id="sec8dot2-sensors-25-01002"><title>8.2. Model Training and Hyperparameter Optimization</title><p><bold>Deep Learning Models.</bold> As mentioned in <xref rid="sec5-sensors-25-01002" ref-type="sec">Section 5</xref>, we employ a deep learning model, CNN. Initially, for multi-class classification, we set class labels as follows: gait data are labeled as class 0, data for auto-walker 1 as class 1, and data for auto-walker 2 as class 2. Each model is trained using the &#x02018;categorical_crossentropy&#x02019; loss function and the Adaptive Moment Estimation (Adam) function, with parameters set to a batch size of 40, 10 training steps, and 10 epochs.</p><p>For our ultimate goal, which is to determine the gait data or auto-walker data, we combine binary SVM models to the deep learning models. To train and optimize each SVM model, we employ the five-fold cross-validation method along with the &#x02018;gridsearch&#x02019; function to tune its hyperparameters. These hyperparameters include kernels, regularization parameters, and kernel coefficients, which are chosen from predefined sets [&#x02018;linear&#x02019;, &#x02018;rbf&#x02019;, &#x02018;poly&#x02019;], [0.1, 1, 10, 100], and [1, 0.1, 0.01, 0.001], respectively. Through this tuning process, the optimal hyperparameters for the CNN-SVM combination are found to be {&#x02018;kernel&#x02019;: &#x02018;poly&#x02019;, <italic toggle="yes">C</italic>: 1, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></inline-formula>: 0.1}, as the polynomial kernel demonstrates superior performance in capturing non-linear gait variations compared to linear and RBF kernels. The selected <italic toggle="yes">C</italic> and <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></inline-formula> values balance model complexity and classification performance, ensuring robustness across datasets.</p><p><bold>Machine Learning Models.</bold> We evaluate three traditional machine learning models: a binary kNN model, a binary RF model, and a binary SVM model. Each model undergoes hyperparameter optimization via grid search to maximize classification accuracy.</p><p>For the kNN model, we tune the number of neighbors and weighting function, selecting from predefined sets [3, 5, 7, 9, 11] and [&#x02018;uniform&#x02019;, &#x02018;distance&#x02019;], respectively. The best configuration is {&#x02018;n_neighbors&#x02019;: 3, &#x02018;weights&#x02019;: &#x02018;uniform&#x02019;}, as a lower neighbor count preserves fine-grained gait distinctions, while larger values lead to excessive smoothing, reducing classification performance. For the RF model, the optimized hyperparameters are {&#x02018;n_estimators&#x02019;: 200, &#x02018;max_features&#x02019;: &#x02018;sqrt&#x02019;, &#x02018;max_depth&#x02019;: 7, &#x02018;criterion&#x02019;: &#x02018;gini&#x02019;}. Using 200 estimators ensures classification stability, while &#x02018;sqrt&#x02019; for &#x02018;max_features&#x02019; improves generalization. A maximum depth of 7 prevents overfitting, and the &#x02018;gini&#x02019; criterion provides better robustness across different sensor devices. For the SVM model, the best-performing hyperparameters are {&#x02018;kernel&#x02019;: &#x02018;poly&#x02019;, <italic toggle="yes">C</italic>: 0.1, <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></inline-formula>: 1}. Similar to the CNN-SVM model, the polynomial kernel outperforms linear and RBF kernels in capturing non-linear gait variations. A lower <italic toggle="yes">C</italic> value (0.1) prevents overfitting to device-specific noise, while a higher <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:math></inline-formula> value (1) allows for more flexible decision boundaries, improving classification across datasets.</p><p>These optimized hyperparameters provide the best trade-off between classification accuracy and computational efficiency, ensuring reliable distinction between real and artificially generated gaits.</p></sec><sec id="sec8dot3-sensors-25-01002"><title>8.3. Evaluation on Auto-Walker Detection Models</title><p>We use a dataset consisting of 4501 signal data, encompassing both seen gait data and data for auto-walkers 1 and 2, for evaluation purposes. The CNN model combined with binary SVM successfully distinguishes gait data generated by the auto-walkers from genuine gait data. Likewise, the machine learning models for binary classification, which are kNN, RF, and SVM models, distinguish to a high degree of accuracy between auto-walker data and genuine data. <xref rid="sensors-25-01002-f006" ref-type="fig">Figure 6</xref>a,b show the results of classifying seen gait data and seen auto-walkers data for the CNN model combined with SVM, and the binary kNN model. In the diagonal components, all values are given 1. This result implies that models are able to distinguish genuine gait data and auto-walker data with low error rates.</p><p><bold>Evaluation on Model Performance over Time.</bold> Furthermore, we evaluate the performance of each model to maintain accurate auto-walker data detection over time. After a two-week interval, we measure a total of 423 data for auto-walker 1 using the iPhone 7, along with 314 auto-walker data using the iPhone 13. Similarly, for auto-walker 2, 380 data were measured with the iPhone 7, and 369 data with the iPhone 13. The CNN model combined with SVM successfully detects auto-walker data measured after two weeks. Likewise, the machine learning models for binary classification, which are kNN, RF, and SVM, detect the illegitimate data. <xref rid="sensors-25-01002-f007" ref-type="fig">Figure 7</xref>a,b show the results of classifying auto-walker data after two weeks for the CNN model combined with the SVM, and the binary kNN model. In the first diagonal components, all values are given 1. This result implies that the models are able to detect auto-walker data accurately even as time elapses.</p></sec><sec id="sec8dot4-sensors-25-01002"><title>8.4. Evaluation on Model Generalizability Across Unseen Data</title><p>We evaluate the model&#x02019;s ability to distinguish between genuine and fraudulent gait data under two specific unseen conditions. First, we test the model on gait data from users who were not included in the training dataset to assess its capacity to identify genuine gait patterns from previously unencountered individuals. Second, we evaluate the model using auto-walker data collected from smartphone models that are not part of the training data. Since identical movements can produce differing sensor readings depending on the smartphone model, this ensures that the model can generalize across device-specific variations. These evaluations demonstrate the model&#x02019;s robustness and universal applicability in real-world M2E scenarios, where both new users and diverse devices are prevalent.</p><p>Since we use the CNN model as the feature extractor, we first check whether the model satisfactorily extracts features from data of the genuine gait dataset and the auto-walker datasets (i.e., whether it classifies them well). <xref rid="sensors-25-01002-f008" ref-type="fig">Figure 8</xref> shows the result of classifying three unseen datasets (i.e., genuine gait dataset, auto-walker 1 dataset, and auto-walker 2 dataset). All diagonal components are 1.0. This implies that the CNN model distinguishes to a high degree of accuracy between the genuine gait datasets and the auto-walker datasets, even if the data are unseen.</p><p><bold>Decision Values for Unseen Gait Data by Model.</bold> After extracting features from the data, whether using deep learning models or those specified in <xref rid="sec5-sensors-25-01002" ref-type="sec">Section 5</xref>, our goal is to ultimately determine genuine gait data as legitimate and auto-walker data as illegitimate. We analyze decision values for each unseen dataset that indicates in which class unseen data are classified when they are given to the model. The &#x02018;decision_function&#x02019; obtains the decision values for the SVM model, and &#x02018;predict_proba&#x02019; for the kNN and RF models. The &#x02018;decision_function&#x02019; of the SVM model provides a raw decision score for each data point, which is a value that indicates how far the data are from the decision boundary (i.e., which class they belong to). On the other side, the &#x02018;predict_proba&#x02019; estimates the probability that the data belong to each class. The kNN model finds the k-Nearest Neighbors for given data, and count the number of neighbors belonging to each class among these neighbors. The probability of belonging to a class is then calculated by dividing the number of neighbors in each class by the total number of neighbors (k). For the RF model, the probability that the given data belong to each class is calculated independently from all trees, and the final class probability is calculated as the proportion of trees that selected that class out of all trees. Accordingly, for the binary classification, the output of the &#x02018;predict_proba&#x02019; is a vector with two elements (i.e., the probability value of each class), and the sum of each element is 1.0. We consider the second element value of the probability vector as the decision value. <xref rid="sensors-25-01002-f009" ref-type="fig">Figure 9</xref> shows the decision values for each unseen dataset derived from all models. The decision values of the data in the Nazli unseen dataset and the MotionSense dataset are distributed around 1.00, but the decision values of the ADL dataset and USC-HAD dataset have a wide distribution. <xref rid="sensors-25-01002-t003" ref-type="table">Table 3</xref> illustrates the minimum and maximum of the decision values for each unseen gait dataset by model.</p><p><bold>Decision Values for Unseen Auto-walker Data by Model.</bold> Similar to the unseen gait datasets, we analyze the decision values of models for each unseen auto-walker dataset. <xref rid="sensors-25-01002-f009" ref-type="fig">Figure 9</xref> shows all the decision values for each unseen auto-walker datasets derived from all models. For all deep learning models, the decision values of the data in the all unseen auto-walker datasets are distributed around <inline-formula><mml:math id="mm11" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1.00</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The number of unseen gait data with a decision value of <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> or less for each model is as follows: For the CNN model 1 combined with the SVM model, there are four in the ADL dataset and 183 in the USC-HAD dataset, while for the CNN model 2 combined with the SVM model, there are five in the ADL dataset and 12 in USC-HAD dataset. For the CNN-LSTM model combined with the SVM model, there are 14 in the ADL dataset and 5448 in the USC-HAD dataset, and for the CNN-GRU model combined with the SVM model, there are 113 in the ADL dataset and 5890 in the USC-HAD dataset. The decision values of all unseen autor-walker datasets are 0.00 for the binary kNN model, and the number of unseen gait data with a decision value of 0.01 or less is 5688 in the USC-HAD dataset. For the binary RF model, the decision values of all unseen auto-walker datasets range between 0.005 (from auto-walker 1 and iPhone 11) and 0.835 (from auto-walker 2 and iPhone15), and the number of unseen gait data with a decision value of 0.84 or less is 85 in the ADL dataset and 6293 in the USC-HAD dataset. Last is the the binary SVM model, for which the decision values of all unseen auto-walker datasets are distributed from <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>3.42</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>0.54</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and the number of unseen gait data with a decision value of 0 or less is 93 in the ADL dataset and 6245 in the USC-HAD dataset. Even if we simply compare the range values of decision values, we can observe a significant overlap between the unseen gait datasets and the unseen auto-walker datasets, except for the CNN models combined with the SVM models.</p><p><bold>Overall FNR and FPR of All Unseen Data by Model.</bold> To assess the overall performance of each model, we evaluate the False Positive Rate (FPR) and False Negative Rate (FNR) across all unseen datasets as a function of the threshold. Since each model generates decision scores on different scales, the optimal threshold range varies. The CNN + binary SVM model exhibits a threshold range of [&#x02212;2, 2], as the CNN extracts refined, lower-dimensional features that result in a more compact and concentrated decision score distribution. In contrast, when using binary SVM alone, the input feature space remains higher-dimensional and more diverse, causing the decision scores to be more widely distributed and requiring a broader threshold range of [&#x02212;4, 4]. Meanwhile, binary kNN and binary RF output probability-based classification scores, resulting in a threshold range of [&#x02212;0.25, 1.5]. These differences in threshold distributions reflect how each model processes feature variability and classification confidence.</p><p><xref rid="sensors-25-01002-f010" ref-type="fig">Figure 10</xref> presents the FPR and FNR of all unseen datasets based on the threshold for each model. Traditional machine learning models achieve high classification accuracy on seen test datasets and some unseen datasets but struggle with the USC-HAD dataset. In contrast, the CNN model, particularly when combined with SVM, exhibits consistently strong performance across all datasets. Although feature extraction is performed effectively, as discussed in <xref rid="sec5-sensors-25-01002" ref-type="sec">Section 5</xref>, the lower performance of traditional models on the USC-HAD dataset can be attributed to its greater diversity in gait types. Models such as SVM, kNN, and RF rely on manually extracted features, making them less adaptable to highly variable gait patterns, which likely result in reduced classification accuracy. Conversely, the CNN model demonstrates superior feature extraction capabilities, automatically learning spatial and temporal representations from motion sensor data. This ability allows it to capture subtle variations in gait dynamics and generalize more effectively across diverse walking styles. As a result, the CNN model significantly outperforms traditional approaches on the USC-HAD dataset, demonstrating its robustness in handling datasets with high intra-class variability. This adaptability makes CNN more effective in distinguishing genuine gait from auto-walker motion across a wide range of unseen datasets. For better visual clarity, <xref rid="sensors-25-01002-f010" ref-type="fig">Figure 10</xref> presents the overall trends, while detailed individual plots for each model are provided in <xref rid="app1-sensors-25-01002" ref-type="app">Appendix A</xref> to enhance interpretability.</p><p>To evaluate the overall performance of each model, we obtain the FNR and FPR on all unseen datasets. The CNN model with the SVM has an EER of 0.001 based on a threshold of <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>0.990</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. In addition, <xref rid="sensors-25-01002-f011" ref-type="fig">Figure 11</xref> shows the overall FNR and FPR for the machine learning models: the binary kNN model has an EER of 0.243 based on a threshold of 0.379, the binary RF model has an EER of 0.208 based on a threshold of 0.313, and the binary SVM model has an EER of 0.333 based on a threshold of <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1.729</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. As a result, our model, which is a combination of the CNN model and the binary SVM model, has the lowest EER of 0.001 for all unseen datasets. This implies that it can distinguish between genuine gait data and auto-walker data even for data collected from participants or devices not encountered during model training.</p></sec><sec id="sec8dot5-sensors-25-01002"><title>8.5. Comprehensive Analysis of Model Performance in Detecting Auto-Walker Activity</title><p>Each model ultimately determines which class the data belong to based on their decision value. For the SVM model, if the decision value of the data is positive, they belong to the positive class (i.e., gait class); otherwise, they belong to the negative class (i.e, auto-walker class). The RF and kNN models output a probability vector with two elements (i.e., the probability value of each class) indicating which class the data belong to, and classify them into the class at the position of the largest element in the vector. <xref rid="sensors-25-01002-t004" ref-type="table">Table 4</xref> illustrates the Precision, Recall, and F1-scores for all seen and unseen datasets by model. In the case of auto-walker data, the Precision of all models except the CNN model combined with the SVM model is significantly lower than the Recall. A low Precision value indicates that among the results the model predicts as auto-walker data, there are more genuine gait data than auto-walker data. This means that the model does not learn the difference between auto-walker data and genuine gait data well. Additionally, in the case of genuine gait data, the Recall of all models except the CNN model combined with the SVM model is somewhat lower than Precision. A low Recall value means that the model predicts only some of the data that are actually genuine gait data as genuine gait data. In other words, this indicates that genuine gait data are not accurately identified and the completeness of the model is low.</p></sec></sec><sec sec-type="discussion" id="sec9-sensors-25-01002"><title>9. Discussion</title><p><bold>Dataset Limitations and Generalizability.</bold> This study evaluated the proposed method using six publicly available gait datasets comprising data collected from 187 participants. Additionally, we collected auto-walker data using iPhone 7, 13, X, 11, 12, and 15 to assess the model&#x02019;s performance. These diverse datasets underscore the robustness of the model; however, several limitations remain. First, class imbalance in some datasets could bias the model&#x02019;s predictions. For instance, genuine gait data are more prevalent than auto-walker data, potentially leading to a learning bias favoring the majority class. Future research could address this issue by exploring advanced techniques such as data augmentation or cost-sensitive learning. Second, variations in sensor placement, sampling rates, and recording conditions could result in inconsistencies in data distributions. These environmental and device-specific variations present challenges to the model&#x02019;s generalizability. To address this, we divided the data into training, testing, and unseen datasets. Specifically, data from 90 participants were used for training and testing, while data from the remaining 97 participants were reserved exclusively for testing the model&#x02019;s generalization capability. Additionally, auto-walker data collected from iPhone 7 and 13 were used for both training and testing, while data collected from iPhone X, 11, 12, and 15 were used solely for testing. This cross-device evaluation played a crucial role in assessing the model&#x02019;s adaptability to unseen devices.</p><p>These experimental conditions are critical for evaluating the model&#x02019;s performance and applicability in real-world scenarios. While class imbalance and data diversity remain limitations, the results demonstrate the model&#x02019;s effectiveness across diverse scenarios, highlighting its potential for reliable fraud detection in M2E platforms.</p><p><bold>Participant Diversity.</bold> One limitation of this study is the absence of detailed demographic information about the participants in the publicly available datasets used for evaluation. Specific factors such as age, physical condition, or other personal attributes are not consistently provided, which limits the ability to comprehensively analyze the model&#x02019;s applicability across diverse population groups. To mitigate this limitation, we relied on the inherent diversity within the datasets, which were collected in varied environments and contexts. While this provides some degree of variability, it may not fully represent the wide range of potential users in real-world applications. Future research should prioritize datasets that include detailed demographic information to better evaluate the model&#x02019;s performance and applicability across different user groups.</p><p><bold>Other Movement Patterns.</bold> This study focuses on distinguishing genuine walking patterns from auto-walker movements to address fraud detection in M2E applications. Future work could expand the scope by incorporating additional movement patterns, such as running or cycling, to enable multi-class classification. This would enhance the model&#x02019;s applicability to broader activity recognition tasks beyond fraud detection. Furthermore, analyzing walking patterns in the context of more diverse activities, such as distinguishing walking from sitting or jumping, could improve the model&#x02019;s generalizability in real-world scenarios. Such extensions would make the system more robust and versatile for various applications.</p><p><bold>Use of More Complex Automatic Walking Devices.</bold> We propose a model capable of detecting data generated by existing automatic walking devices that move in two patterns. Additional automatic walking devices with different types of movements may also be developed in the future. Our model demonstrates generalizability across various unseen datasets, and therefore, we expect that it will be able to identify auto-walker movements even in the face of more complex devices. Furthermore, such advanced devices will surely come at a higher price, potentially raising the cost for actors to engage in problematic activities.</p><p><bold>Comparison with Existing Methods.</bold> Our study focuses on motion-based fraud detection in M2E applications, and to the best of our knowledge, no existing methods specifically address this issue. While traditional fraud detection techniques often target financial anomalies, they do not account for motion manipulation via mechanical devices. Consequently, we evaluated and compared the performance of several widely used models in anomaly detection and behavioral analysis. These models serve as reasonable baselines given their effectiveness in related domains and allow us to demonstrate the advantages of our proposed approach.</p><p><bold>Potential Application Scenarios.</bold> Our study focuses on detecting fraudulent activities in Move-to-Earn (M2E) applications. Our method distinguishes genuine human movement from artificially generated motion using only motion sensor data (accelerometer and gyroscope), which is readily available in most smartphones and wearable devices. This enables seamless integration into existing M2E frameworks while also offering the potential for expansion into other domains. The ability to differentiate natural human gait from synthetic or artificially induced movements has broad applications in security, healthcare, and fair competition in motion-based systems.</p><p>One key application is wearable device security, particularly in fitness trackers and smartwatches. Many reward-based fitness apps can be exploited by artificially generating motion (e.g., attaching devices to oscillating objects). Our AI-based model enhances fraud detection, ensuring fair use and system integrity. In healthcare, our method aids physical rehabilitation monitoring by distinguishing genuine patient movements from assisted/artificial movements. This would improve the effectiveness of remote rehabilitation programs and ensure that patients receive appropriate feedback. Additionally, our model helps prevent cheating in exergames and virtual sports, where some users exploit automated movement devices for unfair advantages. Integrating our detection model ensures fair competition and gameplay integrity.</p><p>By extending our AI-driven motion fraud detection methodology to these areas, we believe that our approach has the potential to contribute beyond the M2E ecosystem, addressing security concerns, enhancing healthcare applications, and maintaining fairness in motion-based digital environments.</p></sec><sec sec-type="conclusions" id="sec10-sensors-25-01002"><title>10. Conclusions</title><p>As M2E applications gain popularity, the misuse of automatic walking devices to simulate physical activity and unfairly claim rewards has emerged as a critical issue. To address this, we proposed an AI-based method capable of distinguishing genuine human movements from those generated by automatic walking devices. Utilizing six open gait datasets and auto-walker datasets measured across various smartphones, we evaluated the generalizability of our model. By optimizing the model configuration through comparisons with both deep learning and traditional machine learning approaches, our method achieved F1-scores of 0.997 on auto-walker datasets and 1.000 on genuine datasets, demonstrating its robustness and effectiveness. Our system ensures that rewards are fairly allocated based on genuine user engagement, enhancing the integrity and trustworthiness of M2E platforms. By tackling the challenges posed by auto-walker activity, our work contributes to building a more secure and equitable ecosystem for users. This ultimately supports the broader mission of promoting physical wellness through technology by ensuring that rewards reflect genuine effort and participation.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Institutional Review Board Statement</title><p>This study did not involve new data collection from human participants. The research utilized pre-existing, publicly available datasets, which are exempt from IRB approval according to institutional and ethical guidelines.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The data presented in this study are available at the following sources: ADL Dataset: <uri xlink:href="https://data.mendeley.com/datasets/xknhpz5t96/2">https://data.mendeley.com/datasets/xknhpz5t96/2</uri>, FLAAP Dataset: <uri xlink:href="https://data.mendeley.com/datasets/bdng756rgw/1">https://data.mendeley.com/datasets/bdng756rgw/1</uri>, GeoTecINIT Dataset: <uri xlink:href="https://zenodo.org/records/8398688">https://zenodo.org/records/8398688</uri>, MotionSense Dataset: <uri xlink:href="https://github.com/mmalekzadeh/motion-sense">https://github.com/mmalekzadeh/motion-sense</uri>, Nazli Dataset: <uri xlink:href="https://figshare.com/articles/dataset/Gait_Database/20346852">https://figshare.com/articles/dataset/Gait_Database/20346852</uri>, USC-HAD Dataset: <uri xlink:href="https://sipi.usc.edu/had/">https://sipi.usc.edu/had/</uri>, all accessed on 4 February 2025.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The author declares no conflicts of interest.</p></notes><app-group><app id="app1-sensors-25-01002"><title>Appendix A</title><fig position="anchor" id="sensors-25-01002-f0A1"><label>Figure A1</label><caption><p>Individual FNR and FPR for each model and dataset.</p></caption><graphic xlink:href="sensors-25-01002-g0A1" position="float"/></fig></app></app-group><ref-list><title>References</title><ref id="B1-sensors-25-01002"><label>1.</label><element-citation publication-type="webpage"><article-title>Move to Earn Fitness Apps Market Worth $1.8 Billion By 2030</article-title><comment>Available online: <ext-link xlink:href="https://www.grandviewresearch.com/press-release/global-move-to-earn-fitness-apps-market" ext-link-type="uri">https://www.grandviewresearch.com/press-release/global-move-to-earn-fitness-apps-market</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-03-06">(accessed on 6 March 2024)</date-in-citation></element-citation></ref><ref id="B2-sensors-25-01002"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Raj</surname><given-names>R.</given-names></name>
<name><surname>Kos</surname><given-names>A.</given-names></name>
</person-group><article-title>An improved human activity recognition technique based on convolutional neural network</article-title><source>Sci. Rep.</source><year>2023</year><volume>13</volume><elocation-id>22581</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-49739-1</pub-id><pub-id pub-id-type="pmid">38114574</pub-id>
</element-citation></ref><ref id="B3-sensors-25-01002"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nurwulan</surname><given-names>N.R.</given-names></name>
<name><surname>Selamaj</surname><given-names>G.</given-names></name>
</person-group><article-title>Random forest for human daily activity recognition</article-title><source>J. Phys. Conf. Ser.</source><year>2020</year><volume>1655</volume><fpage>012087</fpage><pub-id pub-id-type="doi">10.1088/1742-6596/1655/1/012087</pub-id></element-citation></ref><ref id="B4-sensors-25-01002"><label>4.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chathuramali</surname><given-names>K.M.</given-names></name>
<name><surname>Rodrigo</surname><given-names>R.</given-names></name>
</person-group><article-title>Faster human activity recognition with SVM</article-title><source>Proceedings of the International Conference on Advances in ICT for Emerging Regions (ICTer2012)</source><conf-loc>Colombo, Sri Lanka</conf-loc><conf-date>12&#x02013;15 December 2012</conf-date><fpage>197</fpage><lpage>203</lpage></element-citation></ref><ref id="B5-sensors-25-01002"><label>5.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Mohsen</surname><given-names>S.</given-names></name>
<name><surname>Elkaseer</surname><given-names>A.</given-names></name>
<name><surname>Scholz</surname><given-names>S.G.</given-names></name>
</person-group><article-title>Human Activity Recognition Using K-Nearest Neighbor Machine Learning Algorithm algorithm</article-title><source>Sustainable Design and Manufacturing. KES-SDM 2021. Smart Innovation, Systems and Technologies</source><publisher-name>Springer</publisher-name><publisher-loc>Singapore</publisher-loc><year>2021</year><fpage>304</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1007/978-981-16-6128-0_29</pub-id></element-citation></ref><ref id="B6-sensors-25-01002"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kaur</surname><given-names>R.</given-names></name>
<name><surname>Chen</surname><given-names>Z.</given-names></name>
<name><surname>Motl</surname><given-names>R.</given-names></name>
<name><surname>Hernandez</surname><given-names>M.E.</given-names></name>
<name><surname>Sowers</surname><given-names>R.</given-names></name>
</person-group><article-title>Predicting multiple sclerosis from gait dynamics using an instrumented treadmill: A machine learning approach</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2020</year><volume>68</volume><fpage>2666</fpage><lpage>2677</lpage><pub-id pub-id-type="doi">10.1109/TBME.2020.3048142</pub-id><pub-id pub-id-type="pmid">33378257</pub-id>
</element-citation></ref><ref id="B7-sensors-25-01002"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Takallou</surname><given-names>M.A.</given-names></name>
<name><surname>Fallahtafti</surname><given-names>F.</given-names></name>
<name><surname>Hassan</surname><given-names>M.</given-names></name>
<name><surname>Al-Ramini</surname><given-names>A.</given-names></name>
<name><surname>Qolomany</surname><given-names>B.</given-names></name>
<name><surname>Pipinos</surname><given-names>I.</given-names></name>
<name><surname>Myers</surname><given-names>S.</given-names></name>
<name><surname>Alsaleem</surname><given-names>F.</given-names></name>
</person-group><article-title>Diagnosis of disease affecting gait with a body acceleration-based model using reflected marker data for training and a wearable accelerometer for implementation</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><elocation-id>1075</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-023-50727-8</pub-id><pub-id pub-id-type="pmid">38212467</pub-id>
</element-citation></ref><ref id="B8-sensors-25-01002"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Del Din</surname><given-names>S.</given-names></name>
<name><surname>Godfrey</surname><given-names>A.</given-names></name>
<name><surname>Galna</surname><given-names>B.</given-names></name>
<name><surname>Lord</surname><given-names>S.</given-names></name>
<name><surname>Rochester</surname><given-names>L.</given-names></name>
</person-group><article-title>Free-living gait characteristics in ageing and Parkinson&#x02019;s disease: Impact of environment and ambulatory bout length</article-title><source>J. Neuroeng. Rehabil.</source><year>2016</year><volume>13</volume><fpage>46</fpage><pub-id pub-id-type="doi">10.1186/s12984-016-0154-5</pub-id><pub-id pub-id-type="pmid">27175731</pub-id>
</element-citation></ref><ref id="B9-sensors-25-01002"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>D.</given-names></name>
<name><surname>Zou</surname><given-names>Y.</given-names></name>
<name><surname>Xiang</surname><given-names>S.</given-names></name>
<name><surname>Jiang</surname><given-names>C.</given-names></name>
</person-group><article-title>Graph neural networks for financial fraud detection: A review</article-title><source>Front. Comput. Sci.</source><year>2025</year><volume>19</volume><elocation-id>199609</elocation-id><pub-id pub-id-type="doi">10.1007/s11704-024-40474-y</pub-id></element-citation></ref><ref id="B10-sensors-25-01002"><label>10.</label><element-citation publication-type="webpage"><article-title>Charity Miles</article-title><comment>Available online: <ext-link xlink:href="https://charitymiles.org/" ext-link-type="uri">https://charitymiles.org/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-03-22">(accessed on 22 March 2024)</date-in-citation></element-citation></ref><ref id="B11-sensors-25-01002"><label>11.</label><element-citation publication-type="webpage"><article-title>Sweatcoin</article-title><comment>Available online: <ext-link xlink:href="https://sweatco.in/" ext-link-type="uri">https://sweatco.in/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-03-22">(accessed on 22 March 2024)</date-in-citation></element-citation></ref><ref id="B12-sensors-25-01002"><label>12.</label><element-citation publication-type="webpage"><article-title>Evidation</article-title><comment>Available online: <ext-link xlink:href="https://evidation.com/" ext-link-type="uri">https://evidation.com/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-03-22">(accessed on 22 March 2024)</date-in-citation></element-citation></ref><ref id="B13-sensors-25-01002"><label>13.</label><element-citation publication-type="webpage"><article-title>Betterpoints</article-title><comment>Available online: <ext-link xlink:href="https://betterpoints.app/" ext-link-type="uri">https://betterpoints.app/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-03-22">(accessed on 22 March 2024)</date-in-citation></element-citation></ref><ref id="B14-sensors-25-01002"><label>14.</label><element-citation publication-type="webpage"><article-title>Runtopia</article-title><comment>Available online: <ext-link xlink:href="https://www.runtopia.net/" ext-link-type="uri">https://www.runtopia.net/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-03-22">(accessed on 22 March 2024)</date-in-citation></element-citation></ref><ref id="B15-sensors-25-01002"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Z.</given-names></name>
<name><surname>Liu</surname><given-names>F.</given-names></name>
<name><surname>Yang</surname><given-names>W.</given-names></name>
<name><surname>Peng</surname><given-names>S.</given-names></name>
<name><surname>Zhou</surname><given-names>J.</given-names></name>
</person-group><article-title>A survey of convolutional neural networks: Analysis, applications, and prospects</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2021</year><volume>33</volume><fpage>6999</fpage><lpage>7019</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2021.3084827</pub-id><pub-id pub-id-type="pmid">34111009</pub-id>
</element-citation></ref><ref id="B16-sensors-25-01002"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Breiman</surname><given-names>L.</given-names></name>
</person-group><article-title>Random forests</article-title><source>Mach. Learn.</source><year>2001</year><volume>45</volume><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></element-citation></ref><ref id="B17-sensors-25-01002"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hearst</surname><given-names>M.A.</given-names></name>
<name><surname>Dumais</surname><given-names>S.T.</given-names></name>
<name><surname>Osuna</surname><given-names>E.</given-names></name>
<name><surname>Platt</surname><given-names>J.</given-names></name>
<name><surname>Scholkopf</surname><given-names>B.</given-names></name>
</person-group><article-title>Support vector machines</article-title><source>IEEE Intell. Syst. Their Appl.</source><year>1998</year><volume>13</volume><fpage>18</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1109/5254.708428</pub-id></element-citation></ref><ref id="B18-sensors-25-01002"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Peterson</surname><given-names>L.E.</given-names></name>
</person-group><article-title>K-nearest neighbor</article-title><source>Scholarpedia</source><year>2009</year><volume>4</volume><fpage>1883</fpage><pub-id pub-id-type="doi">10.4249/scholarpedia.1883</pub-id></element-citation></ref><ref id="B19-sensors-25-01002"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>S.</given-names></name>
<name><surname>Choi</surname><given-names>W.</given-names></name>
<name><surname>Lee</surname><given-names>D.H.</given-names></name>
</person-group><article-title>Usable user authentication on a smartwatch using vibration</article-title><source>Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security</source><conf-loc>Virtual Event, Republic of Korea</conf-loc><conf-date>15&#x02013;19 November 2021</conf-date><fpage>304</fpage><lpage>319</lpage></element-citation></ref><ref id="B20-sensors-25-01002"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>S.</given-names></name>
<name><surname>Lee</surname><given-names>D.H.</given-names></name>
</person-group><article-title>From attack to identification: MEMS sensor fingerprinting using acoustic signals</article-title><source>IEEE Internet Things J.</source><year>2022</year><volume>10</volume><fpage>5447</fpage><lpage>5460</lpage><pub-id pub-id-type="doi">10.1109/JIOT.2022.3221930</pub-id></element-citation></ref><ref id="B21-sensors-25-01002"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Lee</surname><given-names>S.</given-names></name>
<name><surname>Choi</surname><given-names>W.</given-names></name>
<name><surname>Lee</surname><given-names>D.H.</given-names></name>
</person-group><article-title>The vibration knows who you are! A further analysis on usable authentication for smartwatch users</article-title><source>Comput. Secur.</source><year>2023</year><volume>125</volume><fpage>103040</fpage><pub-id pub-id-type="doi">10.1016/j.cose.2022.103040</pub-id></element-citation></ref><ref id="B22-sensors-25-01002"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Atalaa</surname><given-names>B.A.</given-names></name>
<name><surname>Ziedan</surname><given-names>I.</given-names></name>
<name><surname>Alenany</surname><given-names>A.</given-names></name>
<name><surname>Helmi</surname><given-names>A.</given-names></name>
</person-group><article-title>Feature Engineering for Human Activity Recognition</article-title><source>Int. J. Adv. Comput. Sci. Appl</source><year>2021</year><volume>12</volume><fpage>160</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.14569/IJACSA.2021.0120221</pub-id></element-citation></ref><ref id="B23-sensors-25-01002"><label>23.</label><element-citation publication-type="webpage"><article-title>Smartphone and Smartwatch Inertial Measurements from Heterogeneous Subjects for Human Activity Recognition</article-title><comment>Available online: <ext-link xlink:href="https://zenodo.org/records/8398688" ext-link-type="uri">https://zenodo.org/records/8398688</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-04">(accessed on 4 April 2024)</date-in-citation></element-citation></ref><ref id="B24-sensors-25-01002"><label>24.</label><element-citation publication-type="webpage"><article-title>Activities of Daily Living with Motion: A Dataset with Accelerometer, Magnetometer and Gyroscope Data from Mobile Devices</article-title><comment>Available online: <ext-link xlink:href="https://www.data-in-brief.com/article/S2352-3409(20)31508-0/fulltext#seccesectitle0005" ext-link-type="uri">https://www.data-in-brief.com/article/S2352-3409(20)31508-0/fulltext#seccesectitle0005</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-03-04">(accessed on 4 March 2024)</date-in-citation></element-citation></ref><ref id="B25-sensors-25-01002"><label>25.</label><element-citation publication-type="webpage"><article-title>Smartphones BQ Aquaris</article-title><comment>Available online: <ext-link xlink:href="https://www.bq.com/pt/smartphones" ext-link-type="uri">https://www.bq.com/pt/smartphones</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2020-08-29">(accessed on 29 August 2020)</date-in-citation></element-citation></ref><ref id="B26-sensors-25-01002"><label>26.</label><element-citation publication-type="webpage"><article-title>FLAAP: An open Human Activity Recognition (HAR) Dataset for Learning and Finding the Associated Activity Patterns</article-title><comment>Available online: <ext-link xlink:href="https://data.mendeley.com/datasets/bdng756rgw/1" ext-link-type="uri">https://data.mendeley.com/datasets/bdng756rgw/1</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-04">(accessed on 4 April 2024)</date-in-citation></element-citation></ref><ref id="B27-sensors-25-01002"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Matey-Sanz</surname><given-names>M.</given-names></name>
<name><surname>Casteleyn</surname><given-names>S.</given-names></name>
<name><surname>Granell</surname><given-names>C.</given-names></name>
</person-group><article-title>Dataset of inertial measurements of smartphones and smartwatches for human activity recognition</article-title><source>Data Brief</source><year>2023</year><volume>51</volume><fpage>109809</fpage><pub-id pub-id-type="doi">10.1016/j.dib.2023.109809</pub-id><pub-id pub-id-type="pmid">38075620</pub-id>
</element-citation></ref><ref id="B28-sensors-25-01002"><label>28.</label><element-citation publication-type="webpage"><article-title>MotionSense Dataset</article-title><comment>Available online: <ext-link xlink:href="https://github.com/mmalekzadeh/motion-sense" ext-link-type="uri">https://github.com/mmalekzadeh/motion-sense</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-04">(accessed on 4 April 2024)</date-in-citation></element-citation></ref><ref id="B29-sensors-25-01002"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Malekzadeh</surname><given-names>M.</given-names></name>
<name><surname>Clegg</surname><given-names>R.G.</given-names></name>
<name><surname>Cavallaro</surname><given-names>A.</given-names></name>
<name><surname>Haddadi</surname><given-names>H.</given-names></name>
</person-group><article-title>Mobile Sensor Data Anonymization</article-title><source>Proceedings of the International Conference on Internet of Things Design and Implementation</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>15&#x02013;18 April 2019</conf-date><fpage>49</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1145/3302505.3310068</pub-id></element-citation></ref><ref id="B30-sensors-25-01002"><label>30.</label><element-citation publication-type="webpage"><article-title>Gait Database</article-title><comment>Available online: <ext-link xlink:href="https://figshare.com/articles/dataset/Gait_Database/20346852" ext-link-type="uri">https://figshare.com/articles/dataset/Gait_Database/20346852</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-04">(accessed on 4 April 2024)</date-in-citation></element-citation></ref><ref id="B31-sensors-25-01002"><label>31.</label><element-citation publication-type="webpage"><article-title>The USC-SIPI Human Activity Dataset</article-title><comment>Available online: <ext-link xlink:href="https://sipi.usc.edu/had/" ext-link-type="uri">https://sipi.usc.edu/had/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-04">(accessed on 4 April 2024)</date-in-citation></element-citation></ref><ref id="B32-sensors-25-01002"><label>32.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Sawchuk</surname><given-names>A.A.</given-names></name>
</person-group><article-title>USC-HAD: A daily activity dataset for ubiquitous activity recognition using wearable sensors</article-title><source>Proceedings of the 2012 ACM Conference on Ubiquitous Computing</source><conf-loc>Pittsburgh, PA, USA</conf-loc><conf-date>5&#x02013;8 September 2012</conf-date><fpage>1036</fpage><lpage>1043</lpage></element-citation></ref><ref id="B33-sensors-25-01002"><label>33.</label><element-citation publication-type="webpage"><article-title>MotionNode Is the Easy to Use Inertial Sensor Module</article-title><comment>Available online: <ext-link xlink:href="http://www.motionnode.com/" ext-link-type="uri">http://www.motionnode.com/</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-04">(accessed on 4 April 2024)</date-in-citation></element-citation></ref><ref id="B34-sensors-25-01002"><label>34.</label><element-citation publication-type="webpage"><article-title>List of Automatic Walking Devices on AliExpress</article-title><comment>Available online: <ext-link xlink:href="https://www.aliexpress.com/w/wholesale-automatic-walking-swing-mobile-phone.html" ext-link-type="uri">https://www.aliexpress.com/w/wholesale-automatic-walking-swing-mobile-phone.html</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2024-04-04">(accessed on 4 April 2024)</date-in-citation></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01002-f001"><label>Figure 1</label><caption><p>The layer configurations of a CNN model.</p></caption><graphic xlink:href="sensors-25-01002-g001" position="float"/></fig><fig position="float" id="sensors-25-01002-f002"><label>Figure 2</label><caption><p>Scatter plot according to the features extracted from the data of 11 participants in the GeoTecINIT dataset and data of auto-walkers 1 and 2.</p></caption><graphic xlink:href="sensors-25-01002-g002" position="float"/></fig><fig position="float" id="sensors-25-01002-f003"><label>Figure 3</label><caption><p>Results of classifying all 23 participants in the GeoTecINIT dataset using our features.</p></caption><graphic xlink:href="sensors-25-01002-g003" position="float"/></fig><fig position="float" id="sensors-25-01002-f004"><label>Figure 4</label><caption><p>The orientation axes of a general smartphone and two types of auto-walkers: (<bold>a</bold>) X, Y, and Z orientation axes, (<bold>b</bold>) auto-walker 1 (rotating around X-axis), and (<bold>c</bold>) auto-walker 2 (rotating around the Z-axis).</p></caption><graphic xlink:href="sensors-25-01002-g004" position="float"/></fig><fig position="float" id="sensors-25-01002-f005"><label>Figure 5</label><caption><p>Comparison of auto-walker-generated and genuine gait data.</p></caption><graphic xlink:href="sensors-25-01002-g005" position="float"/></fig><fig position="float" id="sensors-25-01002-f006"><label>Figure 6</label><caption><p>Classification results of seen gait data and seen auto-walker data for two selected models.</p></caption><graphic xlink:href="sensors-25-01002-g006" position="float"/></fig><fig position="float" id="sensors-25-01002-f007"><label>Figure 7</label><caption><p>Classification results of auto-walker data after two weeks for two selected models.</p></caption><graphic xlink:href="sensors-25-01002-g007" position="float"/></fig><fig position="float" id="sensors-25-01002-f008"><label>Figure 8</label><caption><p>Classification results of unseen gait data, unseen auto-walker 1 data, and unseen auto-walker 2 data by a CNN model.</p></caption><graphic xlink:href="sensors-25-01002-g008" position="float"/></fig><fig position="float" id="sensors-25-01002-f009"><label>Figure 9</label><caption><p>Decision values for each unseen dataset by model.</p></caption><graphic xlink:href="sensors-25-01002-g009" position="float"/></fig><fig position="float" id="sensors-25-01002-f010"><label>Figure 10</label><caption><p>FPR and FNR of each unseen dataset by model.</p></caption><graphic xlink:href="sensors-25-01002-g010" position="float"/></fig><fig position="float" id="sensors-25-01002-f011"><label>Figure 11</label><caption><p>Overall FPR and FNR of total unseen datasets by machine learning model.</p></caption><graphic xlink:href="sensors-25-01002-g011" position="float"/></fig><table-wrap position="float" id="sensors-25-01002-t001"><object-id pub-id-type="pii">sensors-25-01002-t001_Table 1</object-id><label>Table 1</label><caption><p>Top 15 features by importance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Importance</th><th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Feature Description</th></tr></thead><tbody><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.022192</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean value of accelerometer y-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.021644</td><td align="left" valign="middle" rowspan="1" colspan="1">Advanced SMA of accelerometer y-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.020901</td><td align="left" valign="middle" rowspan="1" colspan="1">Advanced SMA of accelerometer z-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.020395</td><td align="left" valign="middle" rowspan="1" colspan="1">Height of the first peak in accelerometer y-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.018841</td><td align="left" valign="middle" rowspan="1" colspan="1">Height of the third peak in accelerometer y-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.017451</td><td align="left" valign="middle" rowspan="1" colspan="1">First difference of indices in accelerometer y-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.017286</td><td align="left" valign="middle" rowspan="1" colspan="1">Simple SMA of accelerometer y-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.016541</td><td align="left" valign="middle" rowspan="1" colspan="1">Simple SMA of accelerometer x-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.016482</td><td align="left" valign="middle" rowspan="1" colspan="1">Absolute-advanced SMA of gyroscope y-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.016093</td><td align="left" valign="middle" rowspan="1" colspan="1">Absolute-simple SMA of gyroscope y-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.016029</td><td align="left" valign="middle" rowspan="1" colspan="1">Root mean square of gyroscope y-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.015632</td><td align="left" valign="middle" rowspan="1" colspan="1">Energy of gyroscope z-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.015478</td><td align="left" valign="middle" rowspan="1" colspan="1">Index of the first peak in accelerometer y-axis signal</td></tr><tr><td align="left" valign="middle" rowspan="1" colspan="1">0.015299</td><td align="left" valign="middle" rowspan="1" colspan="1">Mean value of accelerometer z-axis signal</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.014955</td><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Root mean square of gyroscope z-axis signal</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01002-t002"><object-id pub-id-type="pii">sensors-25-01002-t002_Table 2</object-id><label>Table 2</label><caption><p>Lists of open datasets used in our evaluation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">#1</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">#2</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">#3</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">#4</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">#5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">#6</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ADL</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FLAAP</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GeoTec</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">M.S.</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Nazli</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">USC-HAD</th></tr></thead><tbody><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Subjects</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">25</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">23</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">93</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">14</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">#(data) *</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1393</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1458</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">448</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3336</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22,728</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6295</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Freq.</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100 Hz</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100 Hz</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100 Hz</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50 Hz</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100 Hz</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">100 Hz</td></tr></tbody></table><table-wrap-foot><fn><p>* The number of datasets when one data shape is (200, 6).</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-01002-t003"><object-id pub-id-type="pii">sensors-25-01002-t003_Table 3</object-id><label>Table 3</label><caption><p>Range of decision values for each unseen gait dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">ADL</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">USC-HAD</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Nazli</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MotionSense</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Min</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Min</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Min</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Min</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Max</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN *</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1.00</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.00</td><td align="center" valign="middle" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1.00</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" rowspan="1" colspan="1">1.00</td><td colspan="2" align="center" valign="middle" rowspan="1">around 1.00</td><td colspan="2" align="center" valign="middle" rowspan="1">around 1.00</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">bin. kNN</td><td colspan="2" align="center" valign="middle" rowspan="1">1.00</td><td align="center" valign="middle" rowspan="1" colspan="1">0.00</td><td align="center" valign="middle" rowspan="1" colspan="1">1.00</td><td colspan="2" align="center" valign="middle" rowspan="1">1.00</td><td colspan="2" align="center" valign="middle" rowspan="1">1.00</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">bin. RF</td><td align="center" valign="middle" rowspan="1" colspan="1">0.34</td><td align="center" valign="middle" rowspan="1" colspan="1">1.00</td><td align="center" valign="middle" rowspan="1" colspan="1">0.32</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" rowspan="1" colspan="1">0.85</td><td align="center" valign="middle" rowspan="1" colspan="1">1.00</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86</td><td align="center" valign="middle" rowspan="1" colspan="1">1.00</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">bin. SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>21.38</mml:mn></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.71</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1.68</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math>
</inline-formula>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">107.15</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.00</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.65</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.30</td></tr></tbody></table><table-wrap-foot><fn><p>* Combined with the binary SVM model.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="sensors-25-01002-t004"><object-id pub-id-type="pii">sensors-25-01002-t004_Table 4</object-id><label>Table 4</label><caption><p>Precision, Recall, and F1-scores for all datasets by model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">
</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Auto-Walker Data</th><th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Genuine Gait Data</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recall</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">CNN *</td><td align="center" valign="middle" rowspan="1" colspan="1">0.994</td><td align="center" valign="middle" rowspan="1" colspan="1">1.000</td><td align="center" valign="middle" rowspan="1" colspan="1">0.997</td><td align="center" valign="middle" rowspan="1" colspan="1">1.000</td><td align="center" valign="middle" rowspan="1" colspan="1">0.999</td><td align="center" valign="middle" rowspan="1" colspan="1">1.000</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">bin. kNN</td><td align="center" valign="middle" rowspan="1" colspan="1">0.315</td><td align="center" valign="middle" rowspan="1" colspan="1">1.000</td><td align="center" valign="middle" rowspan="1" colspan="1">0.480</td><td align="center" valign="middle" rowspan="1" colspan="1">1.000</td><td align="center" valign="middle" rowspan="1" colspan="1">0.727</td><td align="center" valign="middle" rowspan="1" colspan="1">0.842</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">bin. RF</td><td align="center" valign="middle" rowspan="1" colspan="1">0.280</td><td align="center" valign="middle" rowspan="1" colspan="1">0.883</td><td align="center" valign="middle" rowspan="1" colspan="1">0.426</td><td align="center" valign="middle" rowspan="1" colspan="1">0.980</td><td align="center" valign="middle" rowspan="1" colspan="1">0.715</td><td align="center" valign="middle" rowspan="1" colspan="1">0.827</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">bin. SVM</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.305</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.468</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.000</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.714</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.833</td></tr></tbody></table><table-wrap-foot><fn><p>* Combined with the binary SVM model.</p></fn></table-wrap-foot></table-wrap></floats-group></article>