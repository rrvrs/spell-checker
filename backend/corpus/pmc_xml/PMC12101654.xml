<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408363</article-id><article-id pub-id-type="pmc">PMC12101654</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0322449</article-id><article-id pub-id-type="publisher-id">PONE-D-24-52958</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Music Cognition</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Music Cognition</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Music Cognition</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Music Perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Hearing</subject><subj-group><subject>Pitch Perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Research Design</subject><subj-group><subject>Experimental Design</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Research Design</subject><subj-group><subject>Survey Research</subject><subj-group><subject>Questionnaires</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Synesthesia</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Synesthesia</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Synesthesia</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory Perception</subject><subj-group><subject>Synesthesia</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Assessment of the role of emotions in audiovisual associations through an enactive approach</article-title><alt-title alt-title-type="running-head">Assessment of the role of emotions in audiovisual associations through an enactive approach</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7765-3008</contrib-id><name><surname>Cenerini</surname><given-names>Costanza</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Vollero</surname><given-names>Luca</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Di Stefano</surname><given-names>Nicola</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff003" ref-type="aff">
<sup>3</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Santonico</surname><given-names>Marco</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><xref rid="aff004" ref-type="aff">
<sup>4</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Pennazza</surname><given-names>Giorgio</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Keller</surname><given-names>Flavio</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff005" ref-type="aff">
<sup>5</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Department of Engineering, Unit of Electronics for Sensor Systems, Universit&#x000e0; Campus Bio-Medico di Roma, Rome, Italy</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Department of Engineering, Unit of Computational Systems and Bioinformatics, Universit&#x000e0; Campus Bio-Medico di Roma, Rome, Italy</addr-line></aff><aff id="aff003"><label>3</label>
<addr-line>National Research Council, Institute of Cognitive Sciences and Technology, Rome, Italy</addr-line></aff><aff id="aff004"><label>4</label>
<addr-line>Department of Science and Technology for Sustainable Development and One Health, Unit of Electronics for Sensor Systems, Universit&#x000e0; Campus Bio-Medico di Roma, Rome, Italy</addr-line></aff><aff id="aff005"><label>5</label>
<addr-line>Department of Medicine, Unit of Developmental Neuroscience, Universit&#x000e0; Campus Bio-Medico di Roma, Rome, Italy</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Megna</surname><given-names>Nicola</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Istituto di Ricerca e di Studi in Ottica e Optometria, ITALY</addr-line>
</aff><author-notes><corresp id="cor001">* E-mail: <email>costanza.cenerini@unicampus.it</email></corresp><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0322449</elocation-id><history><date date-type="received"><day>19</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>23</day><month>3</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Cenerini et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Cenerini et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0322449.pdf"/><abstract><p>In the expanding literature on audiovisual associations, the question of how music influences the way a person actively creates an image has received little attention. To address this gap, our study investigated the effect of music-induced emotions on several key parameters that define visual images. Following the hypothesis that creating an image while listening to emotionally evocative music would result in artwork emotionally coherent with the music, we designed a two-phase experiment. During the first phase, participants listened to ten original songs that were composed to evoke a broad range of emotions and were asked to create an image by manipulating both colour (i.e. Saturation, Brightness, Hue) and Geometric parameters (i.e., Shape, Dimension, Spatial Dispersion, and Numerosity). Participants were also required to describe the emotional responses evoked by the songs, assigning scores to a predefined set of nine musical emotions (Amazement, Solemnity, Tenderness, Nostalgia, Calmness, Power, Joyful Activation, Tension, and Sadness). In the second phase, participants viewed images with one parameter set at an extreme value while others remained at intermediate levels and were asked to describe the emotions elicited by the images. This approach enabled us to collect data on the associations between music and images, music and emotions, and images and emotions, thereby assessing whether paired audiovisual contents share the same emotional meaning. Findings reveal that sadness in music influences Brightness and Spatial Dispersion of objects in image creation. Sadness also influences Saturation. Surprisingly, amazement shared many similarities with sadness in terms of influenced parameters, namely Brightness and Dispersion, and was correlated with the colour green, typically evoking solemnity and tenderness. These findings shed novel light on the role of emotions in the mediation of audiovisual associations.</p></abstract><funding-group><funding-statement>The author(s) received no specific funding for this work.</funding-statement></funding-group><counts><fig-count count="4"/><table-count count="4"/><page-count count="16"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data for this study are publicly available from the Zenodo repository (<ext-link xlink:href="https://doi.org/10.5281/zenodo.14975418" ext-link-type="uri">https://doi.org/10.5281/zenodo.14975418</ext-link>).</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data for this study are publicly available from the Zenodo repository (<ext-link xlink:href="https://doi.org/10.5281/zenodo.14975418" ext-link-type="uri">https://doi.org/10.5281/zenodo.14975418</ext-link>).</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>The term &#x02019;crossmodal associations&#x02019; is typically used to indicate the consistent matching between sensory features in one modality and features in another modality, either physically present or imagined [<xref rid="pone.0322449.ref001" ref-type="bibr">1</xref>]. These correspondences have been observed in normal perceivers and documented between various sensory modalities. Some correspondences have been observed to be valid across cultures and languages [<xref rid="pone.0322449.ref002" ref-type="bibr">2</xref>].</p><p>While crossmodal associations might evoke the phenomenon of synaesthesia, their distinction is evident. Whereas the former are consistently encountered by non-synaesthetes (and, to some degree, by synesthetes as well), the latter are idiosyncratic matchings perceived by synesthetes only [<xref rid="pone.0322449.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0322449.ref004" ref-type="bibr">4</xref>].</p><p>In research on crossmodal associations, audiovisual associations are amongst the most widely investigated correspondences, with studies mostly focusing on the association between colour and sound. Research has demonstrated that several visual features are consistently associated with auditory features, such as hue with timbre [<xref rid="pone.0322449.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0322449.ref006" ref-type="bibr">6</xref>] or pitch [<xref rid="pone.0322449.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0322449.ref008" ref-type="bibr">8</xref>], saturation with loudness and pitch [<xref rid="pone.0322449.ref009" ref-type="bibr">9</xref>], and brightness with pitch and loudness [<xref rid="pone.0322449.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0322449.ref011" ref-type="bibr">11</xref>]. (See [<xref rid="pone.0322449.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0322449.ref013" ref-type="bibr">13</xref>] for reviews).</p><p>In an attempt to understand the mechanisms underlying crossmodal correspondences, researchers have occasionally referred to sensory mechanisms, including associative learning, statistical co-occurrence, and at times, perceptual similarity (see [<xref rid="pone.0322449.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0322449.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0322449.ref015" ref-type="bibr">15</xref>]). Starting from the 1930s, many studies were conducted to test the hypothesis that emotions mediate audiovisual association, leading to the &#x0201c;emotional mediation hypothesis" (see [<xref rid="pone.0322449.ref016" ref-type="bibr">16</xref>]).</p><p>A distinction should be made between perceived and induced emotions in music psychology. Perceived emotions arise from recognizing emotional cues in music without necessarily eliciting emotional responses. Conversely, induced emotions directly evoke emotional reactions in listeners due to musical characteristics. This differentiation, outlined by Gabrielsson [<xref rid="pone.0322449.ref017" ref-type="bibr">17</xref>] and supported by Schubert [<xref rid="pone.0322449.ref018" ref-type="bibr">18</xref>], highlights significant differences, especially when the musician&#x02019;s expressive intent is negative. Studies exploring the role of emotion in crossmodal associations have investigated both types.</p><p>In 1942, Odbert <italic toggle="yes">et al</italic>. [<xref rid="pone.0322449.ref019" ref-type="bibr">19</xref>] asked participants to associate perceived emotions and colours with a series of songs and found that songs that convey a similar emotional response were associated with similar colours, and where the participants disagreed in their emotional evaluation of the song, the colour association was inconsistent. These results were confirmed by many studies: Bresin [<xref rid="pone.0322449.ref020" ref-type="bibr">20</xref>] found a high correlation between the emotional intention of the performer and participants&#x02019; preference for Hue, Saturation, and Brightness: significant correlations were reported for Brightness and expressed emotions like Love, Pride, Tenderness, Contentment, Sadness, and Fear; Anger and Shame were significantly correlated with Saturation. Barbiere <italic toggle="yes">et al</italic>. [<xref rid="pone.0322449.ref021" ref-type="bibr">21</xref>] found that colour-music correspondences are similar among individuals, and they are correlated with perceived emotions, in such a way that sad songs are mainly associated with grey, which is correlated with Sadness, and happy songs are correlated with bright primary colours, which are correlated with positive emotions. Palmer <italic toggle="yes">et al</italic>. [<xref rid="pone.0322449.ref001" ref-type="bibr">1</xref>] conducted a study in which participants from the UK and Mexico were invited to choose colours that were most/least consistent with classical orchestral music by Bach, Mozart, and Brahms. The results showed that faster music in the major mode induced colour choices that were more saturated, lighter, and yellower, while slower music in minor tonality was associated with unsaturated, darker, and bluer colours. Strong correlations were found between the emotional profiles of the music and those of the colours chosen, supporting the emotional mediation hypothesis across cultures. Similar results were obtained also by Cutietta &#x00026; Haggerty [<xref rid="pone.0322449.ref022" ref-type="bibr">22</xref>], Isbilen &#x00026; Krumhansl [<xref rid="pone.0322449.ref023" ref-type="bibr">23</xref>], and Di Stefano <italic toggle="yes">et al</italic>. [<xref rid="pone.0322449.ref002" ref-type="bibr">2</xref>].</p><p>In a study involving complex stimuli, Albertazzi <italic toggle="yes">et al</italic>. [<xref rid="pone.0322449.ref024" ref-type="bibr">24</xref>] showcased the presence of reliable audiovisual associations between paintings and music excerpts from the classical guitar repertoire (or transcriptions, e.g., Villa-Lobos, Albeniz).</p><p>Taken together, the results from these studies suggest that emotion (either expressed or perceived) plays a fundamental role in music-colour association, thus supporting the &#x0201c;emotion mediation hypothesis" [<xref rid="pone.0322449.ref001" ref-type="bibr">1</xref>]. Based on this assumption, one might expect that, if sadness makes us see the world less bright [<xref rid="pone.0322449.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0322449.ref026" ref-type="bibr">26</xref>], listening to sad music while creating an image will likely influence the way we create the image (e.g., the created image will likely be less bright).</p><p>In the empirical literature, emotions are captured through different tools, ranging from discrete labelling, such as Ekman&#x02019;s 7-emotion model [<xref rid="pone.0322449.ref027" ref-type="bibr">27</xref>], to a multidimensional framework like the valence-arousal model by Russell [<xref rid="pone.0322449.ref028" ref-type="bibr">28</xref>], as Eerola &#x00026; Vuoskoski have pointed out in their review of music and emotion studies [<xref rid="pone.0322449.ref029" ref-type="bibr">29</xref>]. Zentner introduced the Geneva Emotional Music Scale (GEMS) in 2008, outperforming traditional representations in describing emotions from Western classical music [<xref rid="pone.0322449.ref030" ref-type="bibr">30</xref>]. Aljanaki [<xref rid="pone.0322449.ref031" ref-type="bibr">31</xref>] employs a simplified version of the GEMS which features nine emotions: Amazement, Solemnity, Tenderness, Nostalgia, Calmness, Power, Joyful Activation, Tension, and Sadness.</p><p>In the existing literature, most studies passively involved subjects by exposing them to visual and musical stimuli and offering a predetermined set of alternatives from which to choose. However, based on an enactive approach to perception and cognition, according to which organisms gain knowledge of the world through dynamic interactions with their environment (see, e.g., [<xref rid="pone.0322449.ref032" ref-type="bibr">32</xref>]) and to an embodied view of human intelligence [<xref rid="pone.0322449.ref033" ref-type="bibr">33</xref>], humans should be actively involved in the experimental paradigm. Furthermore, none of the studies discussed above conducted an in-depth analysis of the role of Shape and Spatial Distribution of the image in the association, focusing only on colour dimensions such as Hue, Saturation, and Brightness (though see [<xref rid="pone.0322449.ref034" ref-type="bibr">34</xref>] for a possible exception).</p><p>To fill this gap, we developed an experimental protocol to investigate how perceived musical emotions influence image creation. Participants were invited to actively create novel images by acting on several parameters, such as Colour, Shapes, and Spatial Distribution of graphic elements upon hearing a song, and they were then asked to describe the emotions elicited by listening using the GEMS reduction by Aljanaki [<xref rid="pone.0322449.ref031" ref-type="bibr">31</xref>]; later they were shown some images and asked to describe again their emotions. The results substantiate the role of emotions in the crossmodal association between music and colour, emphasizing its effects on perceiving Shapes and Spatial Organization of elements in images. Specifically, the results showed that music associated with Amazement influenced the Shape of generated objects, Sadness impacted the Spatial Dispersion of the objects, and Calmness influenced the Number of objects.</p></sec><sec sec-type="materials|methods" id="sec002"><title>Materials and methods</title><p>This study received the approval of Universit&#x000e0; Campus Bio-Medico di Roma&#x02019;s ethical committee on February 16, 2022, with number of clinical studies&#x02019; register 2021.236. We affirm that all methods employed in this study were conducted in strict adherence to the applicable guidelines and regulations.</p><p>The protocol employed in this study underwent initial testing in a pilot trial with the beta version administered to 15 subjects. Early findings and subsequent adjustments, informed by feedback from this trial, were presented at the ACAIN conference in September 2022 [<xref rid="pone.0322449.ref035" ref-type="bibr">35</xref>]. In this paper, we present the results of the modified protocol developed post the pilot trial phase, involving 80 participants.</p><p>The test was conducted remotely through a website hosted on Digital Ocean (<ext-link xlink:href="https://www.digitalocean.com/" ext-link-type="uri">https://www.digitalocean.com/</ext-link>). Results were securely stored in a proprietary database hosted on Firebase (<ext-link xlink:href="https://firebase.google.com/" ext-link-type="uri">https://firebase.google.com/</ext-link>), a service provided by Google.</p><sec id="sec003"><title>Participants</title><p>Over a span of 2 months, from 2nd April to 2nd June 2022, Italian-speaking participants were recruited for the study. The recruitment was based on a predetermined time frame rather than targeting a specific sample size, with the final number reflecting all eligible volunteers who joined during this period. An a priori power analysis indicated that for detecting large effects (f = 0.4) in our planned analyses, approximately 55 participants would be needed. Prior to their involvement in the trial, individuals were required to complete and sign the informed consent form as well as the privacy policy. Enrolment took place in two parts: in the first part, in the month of April, 84 subjects were recruited and then split into two groups, namely Group A and Group B, both comprising 42 subjects; in the second part, in the month of May, 39 subjects were recruited to be added to Group A (which thus increased up to a final number of 81 subjects). The total number of participants was 123 (M=56 (45.53%), F=67 (54.47%), mean age=23.4, SD=4.7). Exclusion criteria included colour blindness and incorrect completion of the key phases of the test. Out of this group, 39 participants did not start the test after the enrolment, and 4 started the test but did not finish it, therefore the final number of subjects analysed was 80 (M=34 (42.5%), F=46 (57.5%), mean age=23.07, SD=5.3). During the experiment, 80% of participants chose to complete the test in one session, while the remaining participants opted to finish it in multiple sessions. For those who chose multiple sessions, no interval exceeded one week between sessions. Group A was composed of 47 subjects (M=22 (46.8%), F=25 (53.2%), mean age=22.34), Group B of 33 subjects (M=15 (45.5%), F=18 (54.5%), mean age=24.12).</p><p>Participants were required to fill out a form that gathered demographic information as well as details on musical and artistic background and ratings, emotional responses to specific colours and shapes, and any previous experiences with synaesthesia. Upon receiving the experiment link via email, participants could complete the test in one or multiple sessions.</p></sec><sec id="sec004"><title>Test protocol</title><sec id="sec005"><title>Pretest.</title><p>The pretest comprises five distinct evaluations designed to assess the visual and musical capabilities of the subjects. The tests included are:</p><list list-type="bullet"><list-item><p>Ishihara Test [<xref rid="pone.0322449.ref036" ref-type="bibr">36</xref>]: this test is a diagnostic examination to assess an individual&#x02019;s ability to perceive colours. Comprising plates of coloured dots, the test is designed to detect the presence of colour blindness and determine the specific type of colour vision deficiency an individual may have.</p></list-item><list-item><p>Perfect Pitch Test: &#x0201c;Perfect pitch," or absolute pitch, is the ability to recognize and identify a musical note immediately without the need for a reference. In the context of an absolute pitch test, individuals are presented with a series of musical notes and asked to identify each note without external assistance. This test aims to assess the accuracy and consistency with which a person can recognize specific sound frequencies, thereby providing a measure of absolute pitch ability.</p></list-item><list-item><p>Melodic Discrimination Test (adapted from Harrison <italic toggle="yes">et al</italic>. [<xref rid="pone.0322449.ref037" ref-type="bibr">37</xref>]).</p></list-item><list-item><p>Mistuning Perception Test (adapted from Larrouy <italic toggle="yes">et al</italic>. [<xref rid="pone.0322449.ref038" ref-type="bibr">38</xref>]).</p></list-item><list-item><p>Beat Alignment Test (adapted from Harrison <italic toggle="yes">et al</italic>. [<xref rid="pone.0322449.ref039" ref-type="bibr">39</xref>]).</p></list-item></list><p>The adaptation made to the last three tests was that, in this case, we presented a subset of 5 stimuli in a random order, instead of the whole set of stimuli in an increasing difficulty order.</p></sec><sec id="sec006"><title>First phase.</title><p>This phase of the study aims to assess the music-induced emotions experienced by the subjects and their ability to generate mental imagery from music. During this phase, participants listened to a specific song while simultaneously viewing an image on the screen. They had the opportunity to modify the image by adjusting 7 sliders that control the colour of the objects, represented using the Munsell colour space [<xref rid="pone.0322449.ref040" ref-type="bibr">40</xref>], a standardized system that delineates Hues, Saturation, and Brightness (HSB) parameters, and graphical parameters of the objects in the image: Number, Dimension, Dispersion, and Shape. They were asked to listen to the music and modify the image they were seeing so that it would match the one forming in their mind.</p><p>Subsequently, participants were able to listen to the same song again and were asked to describe their emotional response using a 9-point emotional scale, which represents a Reduction of the GEMS emotion model [<xref rid="pone.0322449.ref031" ref-type="bibr">31</xref>]. This process was repeated for a total of 10 songs. The songs, composed by a professional musician specifically for the study, represented different musical genres, each lasting approximately 1.30 minutes, with a range from 78 to 106 seconds. They were artificially synthesized using the music software Logic Pro X. The files created were of MIDI type, except for the piano performances and some of the percussion, which were recorded in audio format. The singer&#x02019;s voice is a sample available online. An overview of the features of the songs can be found in <xref rid="pone.0322449.t001" ref-type="table">Table 1</xref>.</p><table-wrap position="float" id="pone.0322449.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0322449.t001</object-id><label>Table 1</label><caption><title>Information about musical tracks presented in the first phase of the test.</title></caption><alternatives><graphic xlink:href="pone.0322449.t001" id="pone.0322449.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Track Number</th><th align="left" rowspan="1" colspan="1">Genre</th><th align="left" rowspan="1" colspan="1">Key</th><th align="left" rowspan="1" colspan="1">BPM</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">Choir</td><td align="left" rowspan="1" colspan="1">Fmin</td><td align="left" rowspan="1" colspan="1">140</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">POP</td><td align="left" rowspan="1" colspan="1">Dmaj</td><td align="left" rowspan="1" colspan="1">140</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">Orchestral Soundtrack</td><td align="left" rowspan="1" colspan="1">Bmin</td><td align="left" rowspan="1" colspan="1">94</td></tr><tr><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">Trap</td><td align="left" rowspan="1" colspan="1">Cmin</td><td align="left" rowspan="1" colspan="1">110</td></tr><tr><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">Electronic</td><td align="left" rowspan="1" colspan="1">Cmin</td><td align="left" rowspan="1" colspan="1">128</td></tr><tr><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">Minimal Soundtrack</td><td align="left" rowspan="1" colspan="1">Emaj</td><td align="left" rowspan="1" colspan="1">90</td></tr><tr><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">Country</td><td align="left" rowspan="1" colspan="1">Gmaj</td><td align="left" rowspan="1" colspan="1">74</td></tr><tr><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">Jazz</td><td align="left" rowspan="1" colspan="1">Dmaj</td><td align="left" rowspan="1" colspan="1">90</td></tr><tr><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">Classical</td><td align="left" rowspan="1" colspan="1">Bmaj</td><td align="left" rowspan="1" colspan="1">100</td></tr><tr><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">Latino</td><td align="left" rowspan="1" colspan="1">Amin</td><td align="left" rowspan="1" colspan="1">90</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec007"><title>Second phase.</title><p>In this phase of the study, participants were presented with 21 images that had been generated using the same parameters that participants were able to modify in the first phase. Each image had neutral values for most parameters, except for one specific parameter that was intentionally exaggerated (e.g., an image with very high Brightness). Upon viewing each image, participants were once again prompted to describe their emotional experience using the same 9 emotions as in the first phase. A summary of the variables involved in the test can be visualized in <xref rid="pone.0322449.g001" ref-type="fig">Fig 1</xref>.</p><fig position="float" id="pone.0322449.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0322449.g001</object-id><label>Fig 1</label><caption><title>Schematic representation of the test phases.</title><p>The test was administered remotely, and subjects could complete the pretest and the two main phases in a single or in separate sessions. Subjects belonging to Group A completed all the phases, whereas subjects from Group B skipped phase one.</p></caption><graphic xlink:href="pone.0322449.g001" position="float"/></fig></sec></sec><sec id="sec008"><title>Familiarity bias assessment</title><p>Familiarity bias can be described as the tendency to seek confirmation of expectations, retaining, or avoiding abandoning favoured hypotheses or choices [<xref rid="pone.0322449.ref041" ref-type="bibr">41</xref>]. During the beta testing of the protocol, some subjects reported that they felt inclined to align their choices in the second phase with their selections in the first phase instead of answering spontaneously, potentially introducing a familiarity bias in the results. To address this concern, a decision was made to divide the subjects into two groups: Group A, which completed the entire protocol, and Group B, which only completed the pretest and the second phase.</p><p>The purpose of this division was to establish a standard reference for a typical emotional response to the images by observing Group B&#x02019;s results. Subsequently, the responses of Group A could be compared to this reference to determine if there were any deviations to be addressed to the completion of phase one, thereby assessing the presence of a familiarity bias.</p><sec id="sec009"><title>Division in Groups A and B.</title><p>To ensure an equal distribution among the groups, participants were divided based on their responses to the enrolment form. Firstly, the k-means clustering algorithm was applied to the form results to verify the absence of any hidden stratifications [<xref rid="pone.0322449.ref042" ref-type="bibr">42</xref>] in the subject population. This highlighted the presence of two significantly different groups according to the results of the Kruskal-Wallis test (p &#x0003c; 0.001). Then, Group A and Group B were formed by selecting equal random proportions from the two clusters (see <xref rid="pone.0322449.g002" ref-type="fig">Fig 2</xref>). The Kruskal-Wallis test was applied to compare these new two groups (Group A and Group B), and the results showed no significant difference (p &#x0003e; 0.05).</p><fig position="float" id="pone.0322449.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0322449.g002</object-id><label>Fig 2</label><caption><title>Process of division into groups A and B.</title><p>The dots in the figure represent individual subjects positioned in the feature space, which has been simplified to 2D for clarity. Process of division into Group A and Group B: We applied the k-means algorithm to the responses provided in the form (black dots) and identified two significantly different groups (yellow vs blue dots), indicating a stratification of the subjects. Consequently, we split (dotted lines) and recombined these groups to obtain Group A and Group B, which exhibit a similar distribution.</p></caption><graphic xlink:href="pone.0322449.g002" position="float"/></fig></sec></sec><sec id="sec010"><title>Analysis of the relevance of the emotion in the whole dataset</title><p>The final analysis focuses on all subjects from Group A, aiming to assess the emotional correspondence between the emotions induced by music during the image generation process and the emotions evoked by viewing the images in phase two. To achieve this, we computed the mean of each graphical parameter for each subject. Subsequently, for each emotion and graphical parameter, the following steps were undertaken:</p><list list-type="order"><list-item><p>Identify the songs from phase one where the graphical parameter of the corresponding generated image is higher than the mean.</p></list-item><list-item><p>Compare the similarity of the emotions indicated for the track corresponding to the image generated in the first phase and the corresponding one from the second phase and average over each emotion:</p></list-item></list><disp-formula id="pone.0322449.e001"><alternatives><graphic xlink:href="pone.0322449.e001.jpg" id="pone.0322449.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>emotional&#x000a0;consistency</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">|</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="true" form="postfix" stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><p>in which: <italic toggle="yes">i = emotion, A = emotional spectra from phase 1, B = emotional spectra from phase 2</italic></p><p>The mean per subject and the standard deviation of the emotional consistency were then computed.</p><p>A low value of emotional correspondence indicates a disagreement between the music-induced emotions of the first phase and those induced by the sight of images with high parameters in the second phase. In such cases, the specific emotion was not considered relevant in the association between the music heard and the generated image. Conversely, a high value suggests that the emotions expressed in the two phases were similar. Some examples of possible values are reported in <xref rid="pone.0322449.t002" ref-type="table">Table 2</xref>.</p><table-wrap position="float" id="pone.0322449.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0322449.t002</object-id><label>Table 2</label><caption><title>Example of possible values given in the two phases and their relative emotional correspondence index.</title></caption><alternatives><graphic xlink:href="pone.0322449.t002" id="pone.0322449.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Emotion in Phase 1</th><th align="left" rowspan="1" colspan="1">Graphical parameter in Phase 1</th><th align="left" rowspan="1" colspan="1">Emotion in Phase 2</th><th align="left" rowspan="1" colspan="1">Emotional correspondence index</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">100</td><td align="left" rowspan="1" colspan="1">70</td><td align="left" rowspan="1" colspan="1">100</td><td align="left" rowspan="1" colspan="1">1</td></tr><tr><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">1</td></tr><tr><td align="left" rowspan="1" colspan="1">50</td><td align="left" rowspan="1" colspan="1">100</td><td align="left" rowspan="1" colspan="1">50</td><td align="left" rowspan="1" colspan="1">1</td></tr><tr><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">20</td><td align="left" rowspan="1" colspan="1">80</td><td align="left" rowspan="1" colspan="1">0.2</td></tr><tr><td align="left" rowspan="1" colspan="1">20</td><td align="left" rowspan="1" colspan="1">80</td><td align="left" rowspan="1" colspan="1">70</td><td align="left" rowspan="1" colspan="1">0.5</td></tr></tbody></table></alternatives></table-wrap><p>After computing the emotional correspondence, the authors applied different techniques to find clusters of subjects with different behaviours but could not find any groups with significant differences.</p></sec><sec id="sec011"><title>Insights on music-induced emotions</title><p>To gain a deeper understanding of the emotions induced by music, we calculated Kendall&#x02019;s correlation coefficient between the features of the songs and the emotions expressed in phase one. Utilizing the MIRToolbox on MATLAB R2020a, 12 loudness, timbral, rhythmic, and tonal features were extracted from each song. None of these correlation values exceeded 0.5.</p><p>We extended this analysis by computing the same parameter for the emotions in phase one and the results from the pretest, aiming to assess how one&#x02019;s musical abilities influence the perception of music-induced emotions. In this case as well, the correlation values were lower than 0.5, indicating that musical abilities do not significantly impact the perception of emotions.</p><p>For additional details on music-induced emotions, please refer to page 1 of the supporting information.</p><sec id="sec012"><title>Groups A and B comparison.</title><p>Once the subjects completed the experiment, the results from Groups A and B were analysed to investigate the potential influence of phase one on phase two. Two main comparisons were conducted.</p><p>The first comparison focused on the emotional response to the neutral image, which served as the baseline for analysis. The second comparison involved all the other images. For each subject, the emotions evoked by the neutral image were subtracted from the emotions evoked by the other images. This approach allowed for the examination of emotional variations rather than absolute emotional states.</p><p>Subsequently, the distribution of each image was compared between the experimental and control groups using the Kruskal-Wallis test.</p><p>In addition to the Kruskal-Wallis test, we calculated the effect size using eta squared (<inline-formula id="pone.0322449.e002"><alternatives><graphic xlink:href="pone.0322449.e002.jpg" id="pone.0322449.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>) to quantify the magnitude of the differences between groups. Eta squared was computed from the chi-square statistic using the formula <inline-formula id="pone.0322449.e003"><alternatives><graphic xlink:href="pone.0322449.e003.jpg" id="pone.0322449.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> = chi-square/(n-1), where n is the total number of observations. To interpret the magnitude of the effect sizes, we followed conventional guidelines: <inline-formula id="pone.0322449.e004"><alternatives><graphic xlink:href="pone.0322449.e004.jpg" id="pone.0322449.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x02248;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> 0.01 indicates a small effect, <inline-formula id="pone.0322449.e005"><alternatives><graphic xlink:href="pone.0322449.e005.jpg" id="pone.0322449.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x02248;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> 0.06 indicates a medium effect, and <inline-formula id="pone.0322449.e006"><alternatives><graphic xlink:href="pone.0322449.e006.jpg" id="pone.0322449.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x02248;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> 0.14 indicates a large effect [<xref rid="pone.0322449.ref043" ref-type="bibr">43</xref>].</p><p>After confirming the absence of familiarity bias, Group B was excluded from further analysis, and only data from Group A were considered for subsequent analysis.</p></sec></sec></sec><sec sec-type="results" id="sec013"><title>Results</title><p>In order to assess the consistency of the observed associations within subjects, we devised an experimental paradigm consisting of two phases: in Phase 1, participants were allowed to actively modify all parameters of an image while listening to musical excerpts and then rate 9 emotions to describe the feeling induced by the song. In Phase 2, they were shown a &#x0201c;neutral" image and a series of images where all image parameters, except one, were set at an average level. They were then asked again to rate the same 9 emotions. Participants were divided into two groups. Group A completed the whole test, while Group B completed only Phase 2. This was necessary to test whether a familiarity bias emerges between the two phases. Analysis of the data from the second phase of both groups indicated there was no statistically significant difference between them, thus making familiarity bias in the test unlikely.</p><sec id="sec014"><title>Robustness and impartiality of the results</title><p>A comparison between the emotional spectra induced by the neutral image in both groups is shown in <xref rid="pone.0322449.g003" ref-type="fig">Fig 3</xref>. The application of the Kruskal-Wallis test to these data revealed a significant difference between the two groups (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.01). This indicates that completion of Phase 1 resulted in an alteration of the emotional state. Emotions such as Tenderness, Calmness, Power, and Joyful Activation were particularly affected, as illustrated in the boxplot in <xref rid="pone.0322449.g003" ref-type="fig">Fig 3</xref>.</p><fig position="float" id="pone.0322449.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0322449.g003</object-id><label>Fig 3</label><caption><title>Initial emotional response differences between groups.</title><p>(A) The neutral image shown to Group A and B as a reference for the emotional state at the beginning of phase 2. The neutral image was generated with parameters including saturation, brightness, dimension, dispersion, shape, and numerosity were set at the median value within its respective range, except for hue, which varies for each object contained in the image, covering the entire 360<inline-formula id="pone.0322449.e007"><alternatives><graphic xlink:href="pone.0322449.e007" id="pone.0322449.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x02000;</mml:mi><mml:mrow><mml:mo>&#x02218;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> range of the colour wheel. (B) Boxplot illustrating the distinct emotional reactions between participants from Group A, who completed the first phase before viewing this neutral image, and participants from Group B, who did not undergo phase one. The scale of ratings in the boxplot ranges from 0 to 100, where 100 represents a strong preference and 0 represents a low preference. This finding suggests that the completion of phase one has modified the emotional baseline of Group A.</p></caption><graphic xlink:href="pone.0322449.g003" position="float"/></fig><p>Conversely, the effect of the first phase on the variations of emotions triggered by all the other images did not reach significance (<italic toggle="yes">p</italic>&#x02009;&#x0003e;&#x02009;0.05), indicating that there is no familiarity bias in the test, but just a modification of the emotional baseline that does not affect individual associations.</p><fig position="float" id="pone.0322449.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0322449.g004</object-id><label>Fig 4</label><caption><title>Emotional response comparison between groups.</title><p>The Kruskal-Wallis test was employed to examine differences in the emotional response between Group A and Group B when exposed to the images during phase 2. In this phase, subjects were shown images where all image parameters were set at a median value, except one parameter that was set at an extreme value. The emotional responses were measured as differences from each subject&#x02019;s response to the neutral image. Bars represent p-values. The analysis revealed that there was not a statistically significant difference between the two groups (<italic toggle="yes">p</italic>&#x02009;&#x0003e;&#x02009;0.05).</p></caption><graphic xlink:href="pone.0322449.g004" position="float"/></fig><p>The calculated effect size for the comparison of emotional responses to the neutral image was substantial (<inline-formula id="pone.0322449.e008"><alternatives><graphic xlink:href="pone.0322449.e008.jpg" id="pone.0322449.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> = 0.2439), indicating a large effect according to conventional guidelines and suggesting that completing Phase 1 had a considerable impact on participants&#x02019; baseline emotional state.</p><p>Conversely, when analysing the normalized emotional responses (variations from neutral image) for all other images, the effect sizes were notably smaller, as detailed in <xref rid="pone.0322449.t003" ref-type="table">Table 3</xref>. Most parameters showed small or negligible effect sizes (<inline-formula id="pone.0322449.e009"><alternatives><graphic xlink:href="pone.0322449.e009.jpg" id="pone.0322449.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x0003c;</mml:mo><mml:mn>0.06</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>), with only &#x0201c;Red" (<inline-formula id="pone.0322449.e010"><alternatives><graphic xlink:href="pone.0322449.e010.jpg" id="pone.0322449.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.0773</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>), &#x0201c;Low Dispersion" (<inline-formula id="pone.0322449.e011"><alternatives><graphic xlink:href="pone.0322449.e011.jpg" id="pone.0322449.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.0903</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>), and &#x0201c;High/Low Number" (<inline-formula id="pone.0322449.e012"><alternatives><graphic xlink:href="pone.0322449.e012.jpg" id="pone.0322449.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x02248;</mml:mo><mml:mn>0.062</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) reaching medium effect sizes. These substantially reduced effect sizes for normalized responses confirm that while Phase 1 altered the emotional baseline, it did not affect the individual patterns of audiovisual associations, thus supporting the absence of a familiarity bias in our experimental design.</p><table-wrap position="float" id="pone.0322449.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0322449.t003</object-id><label>Table 3</label><caption><title>Effect sizes (<inline-formula id="pone.0322449.e013"><alternatives><graphic xlink:href="pone.0322449.e013" id="pone.0322449.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>) for comparisons between Group A and Group B. Effect sizes are color-coded according to magnitude: yellow (<inline-formula id="pone.0322449.e014"><alternatives><graphic xlink:href="pone.0322449.e014" id="pone.0322449.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x0003c;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) indicates negligible effect, orange (<inline-formula id="pone.0322449.e015"><alternatives><graphic xlink:href="pone.0322449.e015" id="pone.0322449.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>&#x02264;</mml:mo><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x0003c;</mml:mo><mml:mn>0.06</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) indicates small effect, red (<inline-formula id="pone.0322449.e016"><alternatives><graphic xlink:href="pone.0322449.e016" id="pone.0322449.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.06</mml:mn><mml:mo>&#x02264;</mml:mo><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x0003c;</mml:mo><mml:mn>0.14</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) indicates medium effect, and red (<inline-formula id="pone.0322449.e017"><alternatives><graphic xlink:href="pone.0322449.e017" id="pone.0322449.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x02265;</mml:mo><mml:mn>0.14</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) indicates large effect.</title></caption><alternatives><graphic xlink:href="pone.0322449.t003" id="pone.0322449.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Comparison</th><th align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0322449.e018">
<alternatives><graphic xlink:href="pone.0322449.e018" id="pone.0322449.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives>
</inline-formula>
</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Emotional response to neutral image</td><td align="left" rowspan="1" colspan="1">0.2439</td></tr><tr><td align="center" colspan="2" rowspan="1">
<bold>Normalized emotional responses by image parameter</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Green</td><td align="left" rowspan="1" colspan="1">0.0148</td></tr><tr><td align="left" rowspan="1" colspan="1">Red</td><td align="left" rowspan="1" colspan="1">0.0773</td></tr><tr><td align="left" rowspan="1" colspan="1">Light Blue</td><td align="left" rowspan="1" colspan="1">0.0001</td></tr><tr><td align="left" rowspan="1" colspan="1">Purple</td><td align="left" rowspan="1" colspan="1">0.0001</td></tr><tr><td align="left" rowspan="1" colspan="1">Big Dimension</td><td align="left" rowspan="1" colspan="1">0.0010</td></tr><tr><td align="left" rowspan="1" colspan="1">Small Dimension</td><td align="left" rowspan="1" colspan="1">0.0338</td></tr><tr><td align="left" rowspan="1" colspan="1">High Dispersion</td><td align="left" rowspan="1" colspan="1">0.0223</td></tr><tr><td align="left" rowspan="1" colspan="1">Low Dispersion</td><td align="left" rowspan="1" colspan="1">0.0903</td></tr><tr><td align="left" rowspan="1" colspan="1">Spikes</td><td align="left" rowspan="1" colspan="1">0.0464</td></tr><tr><td align="left" rowspan="1" colspan="1">Curves</td><td align="left" rowspan="1" colspan="1">0.0225</td></tr><tr><td align="left" rowspan="1" colspan="1">High Brightness</td><td align="left" rowspan="1" colspan="1">0.0163</td></tr><tr><td align="left" rowspan="1" colspan="1">Low Brightness</td><td align="left" rowspan="1" colspan="1">0.0205</td></tr><tr><td align="left" rowspan="1" colspan="1">High Number</td><td align="left" rowspan="1" colspan="1">0.0623</td></tr><tr><td align="left" rowspan="1" colspan="1">Low Number</td><td align="left" rowspan="1" colspan="1">0.0621</td></tr><tr><td align="left" rowspan="1" colspan="1">High Saturation</td><td align="left" rowspan="1" colspan="1">0.0392</td></tr><tr><td align="left" rowspan="1" colspan="1">Low Saturation</td><td align="left" rowspan="1" colspan="1">0.0125</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec015"><title>The influence of music-induced emotions in image creation</title><p>To quantify the influence of emotion on audiovisual association, we computed an &#x0201c;emotional correspondence index" between the emotions expressed in the first and second phase by each subject. This coefficient compares the music-induced emotions that lead to creating images with a high graphical parameter value in the first phase (e.g., Colour Saturation) to the emotions expressed upon seeing the image with the same set of parameters in the second phase (i.e., Colour Saturation is set at high value, and all other parameters are set at a neutral value). Theoretically, the emotional correspondence index can vary from 0 (no correspondence) to 1 (perfect correspondence). A high emotional correspondence index indicates that the ratings given to a given emotion in the two phases are similar, suggesting that the emotion had a relevant role in the association, whereas a low value indicates that they are dissimilar, and thus the emotion did not influence the association.</p><p>The results are shown in <xref rid="pone.0322449.t004" ref-type="table">Table 4</xref>.</p><table-wrap position="float" id="pone.0322449.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0322449.t004</object-id><label>Table 4</label><caption><title>Emotional correspondence index.</title></caption><alternatives><graphic xlink:href="pone.0322449.t004" id="pone.0322449.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Image parameters</th><th align="left" rowspan="1" colspan="1">Solemnity</th><th align="left" rowspan="1" colspan="1">Tension</th><th align="left" rowspan="1" colspan="1">Power</th><th align="left" rowspan="1" colspan="1">Amazement</th><th align="left" rowspan="1" colspan="1">Sadness</th><th align="left" rowspan="1" colspan="1">Nostalgia</th><th align="left" rowspan="1" colspan="1">Tenderness</th><th align="left" rowspan="1" colspan="1">Calmness</th><th align="left" rowspan="1" colspan="1">Joyful Activation</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Green</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.72</td><td align="left" rowspan="1" colspan="1">0.66</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.75</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.62</td></tr><tr><td align="left" rowspan="1" colspan="1">Red</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.62</td><td align="left" rowspan="1" colspan="1">0.65</td><td align="left" rowspan="1" colspan="1">0.66</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.61</td><td align="left" rowspan="1" colspan="1">0.63</td></tr><tr><td align="left" rowspan="1" colspan="1">Light Blue</td><td align="left" rowspan="1" colspan="1">0.64</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.65</td><td align="left" rowspan="1" colspan="1">0.64</td><td align="left" rowspan="1" colspan="1">0.72</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.69</td><td align="left" rowspan="1" colspan="1">0.64</td><td align="left" rowspan="1" colspan="1">0.64</td></tr><tr><td align="left" rowspan="1" colspan="1">Purple</td><td align="left" rowspan="1" colspan="1">0.65</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.66</td><td align="left" rowspan="1" colspan="1">0.64</td><td align="left" rowspan="1" colspan="1">0.66</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.69</td><td align="left" rowspan="1" colspan="1">0.60</td></tr><tr><td align="left" rowspan="1" colspan="1">High Brightness</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.72</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.73</td><td align="left" rowspan="1" colspan="1">0.76</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.66</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.67</td></tr><tr><td align="left" rowspan="1" colspan="1">Low Brightness</td><td align="left" rowspan="1" colspan="1">0.73</td><td align="left" rowspan="1" colspan="1">0.74</td><td align="left" rowspan="1" colspan="1">0.73</td><td align="left" rowspan="1" colspan="1">0.69</td><td align="left" rowspan="1" colspan="1">0.76</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.65</td><td align="left" rowspan="1" colspan="1">0.69</td><td align="left" rowspan="1" colspan="1">0.62</td></tr><tr><td align="left" rowspan="1" colspan="1">High Saturation</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.72</td><td align="left" rowspan="1" colspan="1">0.62</td></tr><tr><td align="left" rowspan="1" colspan="1">Low Saturation</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.73</td><td align="left" rowspan="1" colspan="1">0.75</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.65</td></tr><tr><td align="left" rowspan="1" colspan="1">Big Dimension</td><td align="left" rowspan="1" colspan="1">0.62</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.64</td></tr><tr><td align="left" rowspan="1" colspan="1">Small Dimension</td><td align="left" rowspan="1" colspan="1">0.64</td><td align="left" rowspan="1" colspan="1">0.63</td><td align="left" rowspan="1" colspan="1">0.63</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.74</td><td align="left" rowspan="1" colspan="1">0.72</td><td align="left" rowspan="1" colspan="1">0.69</td><td align="left" rowspan="1" colspan="1">0.63</td><td align="left" rowspan="1" colspan="1">0.65</td></tr><tr><td align="left" rowspan="1" colspan="1">Spikes</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.63</td><td align="left" rowspan="1" colspan="1">0.66</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.64</td><td align="left" rowspan="1" colspan="1">0.58</td></tr><tr><td align="left" rowspan="1" colspan="1">Curves</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.61</td><td align="left" rowspan="1" colspan="1">0.69</td><td align="left" rowspan="1" colspan="1">0.75</td><td align="left" rowspan="1" colspan="1">0.74</td><td align="left" rowspan="1" colspan="1">0.73</td><td align="left" rowspan="1" colspan="1">0.73</td><td align="left" rowspan="1" colspan="1">0.66</td><td align="left" rowspan="1" colspan="1">0.67</td></tr><tr><td align="left" rowspan="1" colspan="1">High Number</td><td align="left" rowspan="1" colspan="1">0.63</td><td align="left" rowspan="1" colspan="1">0.53</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.62</td><td align="left" rowspan="1" colspan="1">0.50</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.61</td><td align="left" rowspan="1" colspan="1">0.50</td></tr><tr><td align="left" rowspan="1" colspan="1">Low Number</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.66</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.68</td></tr><tr><td align="left" rowspan="1" colspan="1">High Dispersion</td><td align="left" rowspan="1" colspan="1">0.67</td><td align="left" rowspan="1" colspan="1">0.69</td><td align="left" rowspan="1" colspan="1">0.64</td><td align="left" rowspan="1" colspan="1">0.69</td><td align="left" rowspan="1" colspan="1">0.71</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.65</td></tr><tr><td align="left" rowspan="1" colspan="1">Low Dispersion</td><td align="left" rowspan="1" colspan="1">0.72</td><td align="left" rowspan="1" colspan="1">0.72</td><td align="left" rowspan="1" colspan="1">0.70</td><td align="left" rowspan="1" colspan="1">0.74</td><td align="left" rowspan="1" colspan="1">0.78</td><td align="left" rowspan="1" colspan="1">0.68</td><td align="left" rowspan="1" colspan="1">0.72</td><td align="left" rowspan="1" colspan="1">0.66</td><td align="left" rowspan="1" colspan="1">0.70</td></tr></tbody></table></alternatives></table-wrap><p>Sadness is the emotion with the highest emotional correspondence indexes: it is perceived similarly in the first and second phase when dealing with images with low Dispersion (0.78), high or low Brightness (0.76 in both cases), and low Saturation (0.75). Second to Sadness, Amazement showed a high emotional consistency in the perception of Curvilinear shapes (0.75). Many other emotion-graphical parameter relationships are worth mentioning:</p><list list-type="bullet"><list-item><p>Sadness influences almost every graphical parameter, except for Hue, Big Dimension, and high Number.</p></list-item><list-item><p>Amazement influences both Curvilinear and Spiked shapes, and parameters like low Dispersion and low Saturation.</p></list-item><list-item><p>Tenderness influences many parameters, with a high score for Green (0.75). The most dichotomic image parameter, i.e. the one that showed the most difference between a low and a high value, was Numerosity: low Numerosity was associated with Tension, Sadness, and Joyful Activation.</p></list-item><list-item><p>Nostalgia influences both Spiked and Curvilinear shapes (0.70 and 0.73) and Dimensions (0.71 and 0.72).</p></list-item><list-item><p>Power and Solemnity are both relevant in both directions of Saturation (0.70 in both directions for Power and 0.71 in both directions for Solemnity) and low Brightness (respectively, 0.73 and 0.74).</p></list-item><list-item><p>Tension influences the perception of Brightness (0.72 and 0.74).</p></list-item></list></sec></sec><sec sec-type="conclusions" id="sec016"><title>Discussion</title><p>This paper presents the results of an innovative music-emotion-image association test conducted with 80 participants. The novelties of this study are the enactive experimental approach, which actively involved participants in the generation of visual images, and the parameters investigated, which included geometrical properties such as Shape, Dimension, Numerosity, and Spatial Dispersion. The experimental design guarantees that the emerging relationships between the three variables (music, emotions, and image parameters) are likely due to spontaneous cross-modal interactions, as completing Phase 1 does not affect the answers given in Phase 2.</p><p>The analysis of effect sizes (<xref rid="pone.0322449.t003" ref-type="table">Table 3</xref>) provides additional insight into our methodological approach. While the Kruskal-Wallis test showed a significant difference between groups in baseline emotional responses to the neutral image (<italic toggle="yes">p</italic>&#x02009;&#x0003c;&#x02009;0.01), the large effect size (<inline-formula id="pone.0322449.e019"><alternatives><graphic xlink:href="pone.0322449.e019.jpg" id="pone.0322449.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003b7;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> = 0.2439) quantifies the magnitude of this difference, confirming that completing Phase 1 substantially altered participants&#x02019; emotional state. Crucially, when examining normalized responses across different parameters, not only did the Kruskal-Wallis test show no significant diffe-rences (<italic toggle="yes">p</italic>&#x02009;&#x0003e;&#x02009;0.05), but the effect sizes were also predominantly small. This combination of non-significant test results and small effect sizes provides robust evidence that the baseline emotional shift did not influence the patterns of audiovisual associations. These quantitative findings validate our experimental design and support our conclusion that the observed relationships between music, emotions, and image parameters reflect genuine cross-modal interactions rather than methodological artifacts.</p><p>Before delving into the results, it&#x02019;s crucial to emphasize the significance of the values presented in <xref rid="pone.0322449.t004" ref-type="table">Table 4</xref>. These values indicate the degree of consistency exhibited by participants in expressing emotions during both Phase 1 and Phase 2. Specifically, they illustrate the extent to which the emotions elicited by the music during Phase 1, which led to the creation of an image, closely aligned with those expressed during Phase 2. Essentially, we assessed how the music influenced the emotional responses that guided image creation, without considering the directionality of this influence. Therefore, it&#x02019;s possible for an emotion to be associated with two opposite ranges of the same parameter (e.g., Sadness with both High and Low Brightness), indicating that Sadness influences Brightness across its entire range.</p><p>The results presented in <xref rid="pone.0322449.t004" ref-type="table">Table 4</xref> align with existing literature on the interplay between emotion and the association between music and colour. Previous studies, such as Barbiere <italic toggle="yes">et al</italic>. [<xref rid="pone.0322449.ref021" ref-type="bibr">21</xref>], have reported correlations between specific colours and emotional states induced by music. For instance, grey has been linked to sadness [<xref rid="pone.0322449.ref021" ref-type="bibr">21</xref>], which is in line with our findings indicating that high and low Brightness are both influenced by Sadness. Similarly, bright colours have been associated with happiness [<xref rid="pone.0322449.ref021" ref-type="bibr">21</xref>], which is supported by our observation of a high emotional correspondence index between high Brightness and emotions such as Amazement, albeit also with Sadness. Furthermore, green has been correlated with happiness [<xref rid="pone.0322449.ref021" ref-type="bibr">21</xref>], consistent with its elevated emotional correspondence values with emotions like Amazement and Tenderness in our study.</p><p>The findings not only confirm existing knowledge regarding the role of emotions in crossmodal connections between music and colour but also yield novel insights into their impact on associations with graphical parameters such as Shape, Dimension, Numerosity, and Dispersion. To our knowledge, this relationship between emotions and geometrical parameters in the context of image creation has never been analysed before in detail. Studies suggest that the geometrical characteristics of spaces and environments can influence emotional states. For example, people tend to prefer objects with curvilinear contours rather than sharp contours [<xref rid="pone.0322449.ref044" ref-type="bibr">44</xref>]. Also, the layout and geometry of a room or building may contribute to feelings of comfort, arousal, or relaxation (see [<xref rid="pone.0322449.ref045" ref-type="bibr">45</xref>]). Our study indicates that this relationship is reciprocal, reinforcing the relevance of the association.</p><p>Several factors should be considered when interpreting the findings of this study. The experimental sample consisted of Italian participants aged between 18 and 40. While this homogeneous sample may limit generalizability, it also provides insights into a specific demographic. Future studies could benefit from including participants of diverse ages and nationalities to enhance generalizability.</p><p>Regarding the musical stimuli, they successfully elicited a wide range of emotions (see Supporting Information), and using original compositions minimized the influence of prior associations on emotional responses. The number and duration of stimuli ensured a good balance between study length and the breadth of emotions elicited, considering the potential for participant fatigue&#x02014;a significant concern in emotion measurement studies&#x02014;over extended testing periods. Despite this, exploring a wider variety of musical sources could enrich the understanding of emotional responses.</p><p>Our findings are relevant to many areas of artistic activity, especially for art forms that make ample use of combinations of visual effects and music to evoke specific emotions in the viewer, such as films, operas, etc. For example, our results show that musical Sadness influences geometrical properties like low Numerosity, low Dispersion, and colour properties like low Saturation and light blue Colour. Such empirical evidence could become the basis for fruitful interactions between researchers and artists like painters, filmmakers, and opera directors. This could provide empirical evidence to common practices in the field of artistic audiovisual creation, such as film or opera. Additionally, these data could be used to try to predict some general features of the image that a viewer may associate with a song based on prior knowledge of the emotions evoked by listening to music. In this way, an expert system could be trained to reproduce audio-visual associations in healthy subjects and then used to &#x0201c;play" images or &#x0201c;show" songs to people with sensory impairments, allowing the development of new tools to help them perceive the emotional content of the affected sense. This extension of the present work will require more extensive data collection with the already tested protocol and platform presented in this paper.</p></sec><sec id="sec017" sec-type="supplementary-material"><title>Supporting information</title><supplementary-material id="pone.0322449.s001" position="float" content-type="local-data"><label>S1 Fig</label><caption><title>Mean and standard deviation of emotional scores across songs.</title><p>Mean and standard deviation of the scores of each emotion for each of the songs. Each score ranges from 0 (center of the plot) to 100 (extremities of the plot), with lines at 33.3 and 66.6, representing intermediate points.</p><p>(TIF)</p></caption><media xlink:href="pone.0322449.s001.tif"/></supplementary-material><supplementary-material id="pone.0322449.s002" position="float" content-type="local-data"><label>S2 Fig</label><caption><title>Correlation between musical features and emotions.</title><p>Correlation coefficient between musical features and emotion induced by hearing the songs in phase one. The musical features are extracted using the MIRToolbox in MATLAB.</p><p>(TIF)</p></caption><media xlink:href="pone.0322449.s002.tif"/></supplementary-material><supplementary-material id="pone.0322449.s003" position="float" content-type="local-data"><label>S3 Fig</label><caption><title>Correlation between pretest scores and emotions.</title><p>Correlation coefficient between pretest scores and emotion induced by hearing the songs in phase one. MPT = Mistuning Perception Test, aiming at evaluating the ability to discerning tune and out of tune songs, BAT = Beat Alignment Test, that evaluates the ability of recognizing on time and out of time percussion, and MDT = Melodic Discrimination Test, that evaluates the ability to discern different melodies.</p><p>(TIF)</p></caption><media xlink:href="pone.0322449.s003.tif"/></supplementary-material><supplementary-material id="pone.0322449.s004" position="float" content-type="local-data"><label>S1 File</label><caption><title>Original questionnaire in Italian.</title><p>Complete questionnaire administered to participants, containing all questions about demographic information, musical/artisticbackground, personal preferences, emotional responses to specific colours and shapes, and prior experiences with synaesthesia.</p><p>(PDF)</p></caption><media xlink:href="pone.0322449.s004.pdf"/></supplementary-material><supplementary-material id="pone.0322449.s005" position="float" content-type="local-data"><label>S2 File</label><caption><title>English translation of the questionnaire.</title><p>English translation of the complete questionnaire, providing the same questions as in S1 File for international readers.</p><p>(PDF)</p></caption><media xlink:href="pone.0322449.s005.pdf"/></supplementary-material><supplementary-material id="pone.0322449.s006" position="float" content-type="local-data"><label>S1 Data</label><caption><title>Musical stimuli.</title><p>The songs utilized as stimuli in the protocol can be accessed at: <ext-link xlink:href="https://drive.google.com/drive/folders/1li5TKqhGgFZxzkdYUdriveink" ext-link-type="uri">https://drive.google.com/drive/folders/1li5TKqhGgFZxzkdYUdriveink</ext-link>.</p><p>(PDF)</p></caption><media xlink:href="pone.0322449.s006.pdf"/></supplementary-material></sec></body><back><ack><p>We extend our sincere appreciation to Andrea Sorbo for composing the musical pieces used in this experiment.</p></ack><ref-list><title>References</title><ref id="pone.0322449.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Palmer</surname><given-names>SE</given-names></name>, <name><surname>Schloss</surname><given-names>KB</given-names></name>, <name><surname>Xu</surname><given-names>Z</given-names></name>, <name><surname>Prado-Le&#x000f3;n</surname><given-names>LR</given-names></name>. <article-title>Music-color associations are mediated by emotion</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2013</year>;<volume>110</volume>(<issue>22</issue>):<fpage>8836</fpage>&#x02013;<lpage>41</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.1212562110</pub-id>
<pub-id pub-id-type="pmid">23671106</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Di Stefano</surname><given-names>N</given-names></name>, <name><surname>Ansani</surname><given-names>A</given-names></name>, <name><surname>Schiavio</surname><given-names>A</given-names></name>, <name><surname>Spence</surname><given-names>C</given-names></name>. <article-title>Prokofiev was (almost) right: a cross-cultural investigation of auditory-conceptual associations in Peter and the Wolf</article-title>. <source>Psychonomic Bullet Rev</source>. <year>2024</year>;<volume>31</volume>(<issue>4</issue>):<fpage>1735</fpage>&#x02013;<lpage>44</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Deroy</surname><given-names>O</given-names></name>, <name><surname>Spence</surname><given-names>C</given-names></name>. <article-title>Why we are not all synesthetes (not even weakly so)</article-title>. <source>Psychon Bull Rev</source>. <year>2013</year>;<volume>20</volume>(<issue>4</issue>):<fpage>643</fpage>&#x02013;<lpage>64</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/s13423-013-0387-2</pub-id>
<pub-id pub-id-type="pmid">23413012</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Di Stefano</surname><given-names>N</given-names></name>, <name><surname>Spence</surname><given-names>C</given-names></name>. <article-title>Perceptual similarity: insights from crossmodal correspondences</article-title>. <source>Rev Philos Psychol</source>. <year>2023</year>:<fpage>1</fpage>&#x02013;<lpage>30</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref005"><label>5</label><mixed-citation publication-type="book"><name><surname>Lavignac</surname><given-names>A</given-names></name>. <source>Music and Musicians. Henry Holt and Company</source>; <year>1899</year>.</mixed-citation></ref><ref id="pone.0322449.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Wallmark</surname><given-names>Z</given-names></name>. <article-title>Semantic crosstalk in timbre perception</article-title>. <source>Music Science</source>
<year>2019</year>;<volume>2</volume>:<fpage>2059204319846617</fpage>.</mixed-citation></ref><ref id="pone.0322449.ref007"><label>7</label><mixed-citation publication-type="book"><name><surname>Von Goethe</surname><given-names>JW</given-names></name>. <source>Theory of colours. John Murray</source>; <year>1840</year>.</mixed-citation></ref><ref id="pone.0322449.ref008"><label>8</label><mixed-citation publication-type="book"><name><surname>Von Helmholtz</surname><given-names>H</given-names></name>.<source>Handbuch der Physiologischen Optik [Handbook of Physiological Optics]. Voss</source>; <year>1867</year>.</mixed-citation></ref><ref id="pone.0322449.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Anikin</surname><given-names>A</given-names></name>, <name><surname>Johansson</surname><given-names>N</given-names></name>. <article-title>Implicit associations between individual properties of color and sound</article-title>. <source>Atten Percept Psychophys</source>. <year>2019</year>;<volume>81</volume>(<issue>3</issue>):<fpage>764</fpage>&#x02013;<lpage>77</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/s13414-018-01639-7</pub-id>
<pub-id pub-id-type="pmid">30547381</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Marks</surname><given-names>LE</given-names></name>. <article-title>On cross-modal similarity: the perceptual structure of pitch, loudness, and brightness</article-title>. <source>J Exp Psychol: Hum Percept Perform</source>. <year>1989</year>;<volume>15</volume>(<issue>3</issue>):<fpage>586</fpage>.<pub-id pub-id-type="pmid">2527964</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Marks</surname><given-names>LE</given-names></name>. <article-title>On associations of light and sound: the mediation of brightness, pitch, and loudness</article-title>. <source>Am J Psychol</source>. <year>1974</year>;<fpage>173</fpage>&#x02013;<lpage>188</lpage>. <pub-id pub-id-type="pmid">4451203</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Spence</surname><given-names>C</given-names></name>, <name><surname>Di Stefano</surname><given-names>N</given-names></name>. <article-title>Coloured hearing, colour music, colour organs, and the search for perceptually meaningful correspondences between colour and sound</article-title>. <source>i-Perception</source>. <year>2022</year>;<volume>13</volume>(<issue>3</issue>):20416695221092802. 2022</mixed-citation></ref><ref id="pone.0322449.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Spence</surname><given-names>C</given-names></name>, <name><surname>Di Stefano</surname><given-names>N</given-names></name>. <article-title>Sensory translation between audition and vision</article-title>. <source>Psychon Bull Rev</source>. <year>2023</year>:<fpage>1</fpage>&#x02013;<lpage>28</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Spence</surname><given-names>C</given-names></name>. <article-title>Crossmodal correspondences: a tutorial review</article-title>. <source>Atten Percept Psychophys</source>. <year>2011</year>;<volume>73</volume>(<issue>4</issue>):<fpage>971</fpage>&#x02013;<lpage>95</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/s13414-010-0073-7</pub-id>
<pub-id pub-id-type="pmid">21264748</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Spence</surname><given-names>C</given-names></name>. <article-title>Assessing the role of emotional mediation in explaining crossmodal correspondences involving musical stimuli</article-title>. <source>Multisens Res</source>. <year>2020</year>;<volume>33</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>29</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1163/22134808-20191469</pub-id>
<pub-id pub-id-type="pmid">31648195</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Karwoski</surname><given-names>TF</given-names></name>, <name><surname>Odbert</surname><given-names>HS</given-names></name>. <article-title>Color-music</article-title>. <source>Psychol Monogr</source>. <year>1938</year>;<volume>50</volume>(<issue>2</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/h0093458</pub-id></mixed-citation></ref><ref id="pone.0322449.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Gabrielsson</surname><given-names>A</given-names></name>. <article-title>Emotion perceived and emotion felt: same or different?</article-title>. <source>Musicae Scientiae</source>. <year>2001</year>;<volume>5</volume>(1 suppl):<fpage>123</fpage>&#x02013;<lpage>47</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/10298649020050s105</pub-id></mixed-citation></ref><ref id="pone.0322449.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Schubert</surname><given-names>E</given-names></name>. <article-title>Emotion felt by the listener and expressed by the music: literature review and theoretical perspectives</article-title>. <source>Front Psychol</source>. <year>2013</year>;<volume>4</volume>:<fpage>54344</fpage>.</mixed-citation></ref><ref id="pone.0322449.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Odbert</surname><given-names>HS</given-names></name>, <name><surname>Karwoski</surname><given-names>TF</given-names></name>, <name><surname>Eckerson</surname><given-names>AB</given-names></name>. <article-title>Studies in synesthetic thinking: I. Musical and verbal associations of color and mood</article-title>. <source>J Gen Psychol</source>. <year>1942</year>;<volume>26</volume>(<issue>1</issue>):<fpage>153</fpage>&#x02013;<lpage>73</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref020"><label>20</label><mixed-citation publication-type="book"><name><surname>Bresin</surname><given-names>R</given-names></name>. <article-title>What is the color of that music performance?</article-title> In: <source>Proceedings of the International Computer Music Conference</source>. <year>2005</year>.</mixed-citation></ref><ref id="pone.0322449.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Barbiere</surname><given-names>JM</given-names></name>, <name><surname>Vidal</surname><given-names>A</given-names></name>, <name><surname>Zellner</surname><given-names>DA</given-names></name>. <source>The color of music: correspondence through emotion. Empiric Stud Arts</source>. <year>2007</year>;<volume>25</volume>(<issue>2</issue>):<fpage>193</fpage>&#x02013;<lpage>208</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.2190/a704-5647-5245-r47p</pub-id></mixed-citation></ref><ref id="pone.0322449.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Cutietta</surname><given-names>RA</given-names></name>, <name><surname>Haggerty</surname><given-names>KJ</given-names></name>. <article-title>A comparative study of color association with music at various age levels</article-title>. <source>J Res Music Educ</source>. <year>1987</year>;<volume>35</volume>(<issue>2</issue>):<fpage>78</fpage>&#x02013;<lpage>91</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Isbilen</surname><given-names>ES</given-names></name>, <name><surname>Krumhansl</surname><given-names>CL</given-names></name>. <article-title>The color of music: emotion-mediated associations to Bach&#x02019;s Well-tempered Clavier</article-title>. <source>Psychomusicol: Music Mind Brain</source>. <year>2016</year>;<volume>26</volume>(<issue>2</issue>):<fpage>149</fpage>&#x02013;<lpage>61</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Albertazzi</surname><given-names>L</given-names></name>, <name><surname>Canal</surname><given-names>L</given-names></name>
<name><surname>Micciolo</surname><given-names>R</given-names></name>. <article-title>Cross-modal associations between materic painting and classical Spanish music</article-title>. <source>Front Psychol</source>. <year>2015</year>;<volume>6</volume>:<fpage>424</fpage>.<pub-id pub-id-type="pmid">25954217</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Li</surname><given-names>Q</given-names></name>, <name><surname>Zuo</surname><given-names>B</given-names></name>. <article-title>Gaze direction and brightness can affect self-reported emotion</article-title>. <source>J Environ Psychol</source>. <year>2014</year>;<volume>40</volume>:<fpage>9</fpage>&#x02013;<lpage>13</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>X</given-names></name>, <etal>et al</etal>. <article-title>Feeling light or dark? Emotions affect perception of brightness</article-title>. <source>J Environ Psychol</source>. <year>2016</year>;<volume>47</volume>:<fpage>107</fpage>&#x02013;<lpage>11</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Ekman</surname><given-names>P</given-names></name>. <article-title>An argument for basic emotions</article-title>. <source>Cognit Emot</source>. <year>1992</year>;<volume>6</volume>(3&#x02013;4):<fpage>169</fpage>&#x02013;<lpage>200</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/02699939208411068</pub-id></mixed-citation></ref><ref id="pone.0322449.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Russell</surname><given-names>JA</given-names></name>. <article-title>A circumplex model of affect</article-title>. <source>J Personal Soc Psychol</source>. <year>1980</year>;<volume>39</volume>(<issue>6</issue>):<fpage>1161</fpage>.</mixed-citation></ref><ref id="pone.0322449.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Eerola</surname><given-names>T</given-names></name>, <name><surname>Vuoskoski</surname><given-names>JK</given-names></name>. <article-title>A review of music and emotion studies: approaches, emotion models, and stimuli</article-title>. <source>Music Perception</source>. <year>2012</year>;<volume>30</volume>(<issue>3</issue>):<fpage>307</fpage>&#x02013;<lpage>40</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1525/mp.2012.30.3.307</pub-id></mixed-citation></ref><ref id="pone.0322449.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Zentner</surname><given-names>M</given-names></name>, <name><surname>Grandjean</surname><given-names>D</given-names></name>, <name><surname>Scherer</surname><given-names>KR</given-names></name>. <article-title>Emotions evoked by the sound of music: characterization, classification, and measurement</article-title>. <source>Emotion</source>. <year>2008</year>;<volume>8</volume>(<issue>4</issue>):<fpage>494</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/1528-3542.8.4.494</pub-id>
<pub-id pub-id-type="pmid">18729581</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref031"><label>31</label><mixed-citation publication-type="book"><name><surname>Aljanaki</surname><given-names>A</given-names></name>, <name><surname>Wiering</surname><given-names>F</given-names></name>, <name><surname>Veltkamp</surname><given-names>R</given-names></name>. <article-title>Computational modeling of induced emotion using GEMS.</article-title> In: <source>Proceedings of the 15th Conference of the International Society for Music Information Retrieval (ISMIR 2014)</source>. <year>2014</year>. p. <fpage>373</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref032"><label>32</label><mixed-citation publication-type="book"><name><surname>Varela</surname><given-names>FJ</given-names></name>, <name><surname>Thompson</surname><given-names>E</given-names></name>, <name><surname>Rosch</surname><given-names>E</given-names></name>, <name><surname>Kabat-Zinn</surname><given-names>J</given-names></name>. <source>The embodied mind</source>. <publisher-name>The MIT Press</publisher-name>; <year>2017</year>.</mixed-citation></ref><ref id="pone.0322449.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Keller</surname><given-names>F</given-names></name>. <article-title>The concept of embodied human intelligence: power and limits</article-title>. <source>Acta Philos</source>. <year>2023</year>;<volume>32</volume>:<fpage>55</fpage>&#x02013;<lpage>74</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Sebba</surname><given-names>R</given-names></name>. <article-title>Structural correspondence between music and color</article-title>. <source>Color Res Appl</source>. <year>1991</year>;<volume>16</volume>(<issue>2</issue>):<fpage>81</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/col.5080160206</pub-id></mixed-citation></ref><ref id="pone.0322449.ref035"><label>35</label><mixed-citation publication-type="book"><name><surname>Cenerini</surname><given-names>C</given-names></name>, <name><surname>Vollero</surname><given-names>L</given-names></name>, <name><surname>Pennazza</surname><given-names>G</given-names></name>, <name><surname>Santonico</surname><given-names>M</given-names></name>, <name><surname>Keller</surname><given-names>F</given-names></name>. <article-title>Audio visual association test in non synesthetic subjects: technological tailoring of the methods.</article-title> In: <source>Machine Learning, Optimization, and Data Science: 8th International Workshop, LOD 2022</source>. <year>2023</year>. p. <fpage>432</fpage>&#x02013;<lpage>7</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>Clark</surname><given-names>JH</given-names></name>. <article-title>The Ishihara test for color blindness</article-title>. <source>Am J Physiol Optics</source>. <year>1924</year>.</mixed-citation></ref><ref id="pone.0322449.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Harrison</surname><given-names>PMC</given-names></name>, <name><surname>Musil</surname><given-names>JJ</given-names></name>, <name><surname>M&#x000fc;llensiefen</surname><given-names>D</given-names></name>. <article-title>Modelling melodic discrimination tests: descriptive and explanatory approaches</article-title>. <source>J New Music Res</source>. <year>2016</year>;<volume>45</volume>(<issue>3</issue>):<fpage>265</fpage>&#x02013;<lpage>80</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/09298215.2016.1197953</pub-id></mixed-citation></ref><ref id="pone.0322449.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Larrouy-Maestri</surname><given-names>P</given-names></name>, <name><surname>Harrison</surname><given-names>P</given-names></name>, <name><surname>M&#x000fc;llensiefen</surname><given-names>D</given-names></name>. <article-title>The mistuning perception test: a new measurement instrument</article-title>. <source>Behav Res Methods</source>. <year>2019</year>;<volume>51</volume>(<issue>2</issue>):<fpage>663</fpage>&#x02013;<lpage>675</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3758/s13428-019-01225-1</pub-id>
<pub-id pub-id-type="pmid">30924106</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Harrison</surname><given-names>P</given-names></name>, <name><surname>M&#x000fc;llensiefen</surname><given-names>D</given-names></name>. <article-title>Development and validation of the computerised adaptive beat alignment test (CA-BAT)</article-title>. <source>Sci Rep</source>. <year>2018</year>;<volume>8</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1038/s41598-018-30318-8</pub-id>
<pub-id pub-id-type="pmid">29311619</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref040"><label>40</label><mixed-citation publication-type="book"><name><surname>Setchell</surname><given-names>JS.</given-names></name>
<article-title>Colour description and communication.</article-title> In: <name><surname>Best</surname><given-names>J</given-names></name>, editor. <source>Colour design</source>
<edition>2nd edn</edition>. <publisher-name>Woodhead Publishing</publisher-name>; <year>2012</year>. p. <fpage>99</fpage>&#x02013;<lpage>129</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Klayman</surname><given-names>J</given-names></name>. <article-title>Varieties of confirmation bias</article-title>. <source>Psychol Learn Motivat</source>. <year>1995</year>;<volume>32</volume>:<fpage>385</fpage>&#x02013;<lpage>418</lpage>.</mixed-citation></ref><ref id="pone.0322449.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Oakden-Rayner</surname><given-names>L</given-names></name>, <name><surname>Dunnmon</surname><given-names>J</given-names></name>, <name><surname>Carneiro</surname><given-names>G</given-names></name>, <name><surname>R&#x000e9;</surname><given-names>C</given-names></name>. <article-title>Hidden stratification causes clinically meaningful failures in machine learning for medical imaging.</article-title>
<source>Proc ACM Conf Health Inference Learn</source>. <year>2020</year>;<volume>2020</volume>:<fpage>151</fpage>&#x02013;<lpage>159</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3368555.3384468</pub-id>
<pub-id pub-id-type="pmid">33196064</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref043"><label>43</label><mixed-citation publication-type="book"><name><surname>Cohen</surname><given-names>J</given-names></name>. <source>Statistical power analysis for the behavioral sciences. Routledge</source>; <year>2013</year>.</mixed-citation></ref><ref id="pone.0322449.ref044"><label>44</label><mixed-citation publication-type="journal"><name><surname>Leder</surname><given-names>H</given-names></name>, <name><surname>Tinio</surname><given-names>PP</given-names></name>, <name><surname>Bar</surname><given-names>M</given-names></name>. <article-title>Emotional valence modulates the preference for curved objects</article-title>. <source>Perception</source>. <year>2011</year>;<volume>40</volume>(<issue>6</issue>):<fpage>649</fpage>&#x02013;<lpage>55</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1068/p6845</pub-id>
<pub-id pub-id-type="pmid">21936294</pub-id>
</mixed-citation></ref><ref id="pone.0322449.ref045"><label>45</label><mixed-citation publication-type="book"><name><surname>Soares</surname><given-names>MM</given-names></name>, <name><surname>Rosenzweig</surname><given-names>E</given-names></name>, <name><surname>Marcus</surname><given-names>A</given-names></name>. <source>Design, user experience, and usability: design for emotion, well-being and health, learning, and culture. Springer Nature</source>; <year>2022</year>.</mixed-citation></ref></ref-list></back></article>