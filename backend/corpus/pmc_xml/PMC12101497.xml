<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Adv</journal-id><journal-id journal-id-type="iso-abbrev">Sci Adv</journal-id><journal-id journal-id-type="publisher-id">sciadv</journal-id><journal-id journal-id-type="hwp">advances</journal-id><journal-title-group><journal-title>Science Advances</journal-title></journal-title-group><issn pub-type="epub">2375-2548</issn><publisher><publisher-name>American Association for the Advancement of Science</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408491</article-id><article-id pub-id-type="pmc">PMC12101497</article-id><article-id pub-id-type="publisher-id">adt3063</article-id><article-id pub-id-type="doi">10.1126/sciadv.adt3063</article-id><article-categories><subj-group subj-group-type="article-type"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="legacy-article-type"><subject>SciAdv r-articles</subject></subj-group><subj-group subj-group-type="field"><subject>Cognitive Neuroscience</subject><subject>Psychological Science</subject></subj-group><subj-group subj-group-type="overline"><subject>Cognitive Neuroscience</subject></subj-group></article-categories><title-group><article-title>Event cache: An independent component in working memory</article-title><alt-title alt-title-type="short">An independent component in working memory</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8467-8676</contrib-id><name><surname>Zhou</surname><given-names>Hui</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Visualization" vocab-term-identifier="https://web.archive.org/web/20180313224017/http://dictionary.credit.niso.org/Contributor_Roles/Visualization">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - original draft" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-original-draft/">Writing - original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-review-editing/">Writing - review &#x00026; editing</role><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><xref rid="afn1" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3056-1127</contrib-id><name><surname>Wu</surname><given-names>Jinglan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Visualization" vocab-term-identifier="https://web.archive.org/web/20180313224017/http://dictionary.credit.niso.org/Contributor_Roles/Visualization">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - original draft" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-original-draft/">Writing - original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-review-editing/">Writing - review &#x00026; editing</role><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="afn1" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Jiaofeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Visualization" vocab-term-identifier="https://web.archive.org/web/20180313224017/http://dictionary.credit.niso.org/Contributor_Roles/Visualization">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-review-editing/">Writing - review &#x00026; editing</role><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4929-5949</contrib-id><name><surname>Pan</surname><given-names>Zhihe</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-investigation/">Investigation</role><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Jinying</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - original draft" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-original-draft/">Writing - original draft</role><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7661-2968</contrib-id><name><surname>Shen</surname><given-names>Mowei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-review-editing/">Writing - review &#x00026; editing</role><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor1" ref-type="corresp">*</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1585-4143</contrib-id><name><surname>Wang</surname><given-names>Tengfei</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Visualization" vocab-term-identifier="https://web.archive.org/web/20180313224017/http://dictionary.credit.niso.org/Contributor_Roles/Visualization">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - original draft" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-original-draft/">Writing - original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-review-editing/">Writing - review &#x00026; editing</role><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor1" ref-type="corresp">*</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3427-0650</contrib-id><name><surname>Hu</surname><given-names>Yuzheng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Visualization" vocab-term-identifier="https://web.archive.org/web/20180313224017/http://dictionary.credit.niso.org/Contributor_Roles/Visualization">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - original draft" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-original-draft/">Writing - original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-review-editing/">Writing - review &#x00026; editing</role><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><xref rid="aff4" ref-type="aff">
<sup>4</sup>
</xref><xref rid="cor1" ref-type="corresp">*</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9727-8524</contrib-id><name><surname>Gao</surname><given-names>Zaifeng</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-funding-acquisition/">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Visualization" vocab-term-identifier="https://web.archive.org/web/20180313224017/http://dictionary.credit.niso.org/Contributor_Roles/Visualization">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - original draft" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-original-draft/">Writing - original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/term/contributor-roles/" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/term/contributor-roles-writing-review-editing/">Writing - review &#x00026; editing</role><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor1" ref-type="corresp">*</xref></contrib><aff id="aff1"><label><sup>1</sup></label>Department of Psychology and Behavioral Sciences, Zhejiang University, Hangzhou 310058, China.</aff><aff id="aff2"><label><sup>2</sup></label>The State Key Lab of Brain-Machine Intelligence, Zhejiang University, Hangzhou 310058, China.</aff><aff id="aff3"><label><sup>3</sup></label>MOE Frontiers Science Center for Brain Science &#x00026; Brain-Machine Integration, Zhejiang University, Hangzhou 310058, China.</aff><aff id="aff4"><label><sup>4</sup></label>Nanhu Brain-Computer Interface Institute, Hangzhou 311121, China.</aff></contrib-group><author-notes><corresp id="cor1"><label>*</label>Corresponding author. Email: <email xlink:href="mwshen@zju.edu.cn">mwshen@zju.edu.cn</email> (M.S.); <email xlink:href="tfwang@zju.edu.cn">tfwang@zju.edu.cn</email> (T.W.); <email xlink:href="huyuzheng@zju.edu.cn">huyuzheng@zju.edu.cn</email> (Y.H.); <email xlink:href="zaifengg@zju.edu.cn">zaifengg@zju.edu.cn</email> (Z.G.)</corresp><fn id="afn1" fn-type="equal"><label>&#x02020;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="collection"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date publication-format="electronic" date-type="pub"><day>23</day><month>5</month><year>2025</year></pub-date><volume>11</volume><issue>21</issue><elocation-id>eadt3063</elocation-id><history>
<date date-type="received"><day>21</day><month>9</month><year>2024</year></date>
<date date-type="accepted"><day>22</day><month>4</month><year>2025</year></date>
</history><permissions><copyright-statement>Copyright &#x000a9; 2025 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution License 4.0 (CC BY).</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>The Authors</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense" start_date="2025-05-23">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution license</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="sciadv.adt3063.pdf"/><abstract><p>Working memory (WM) has been a major focus of cognitive science and neuroscience for the past 50 years. While most WM research has centered on the mechanisms of objects, there has been a lack of investigation into the cognitive and neural mechanisms of events, which are the building blocks of our experience. Using confirmatory factor analysis, psychophysical experiments, and resting-state and task functional magnetic resonance imaging methods, our study demonstrated that events have an independent storage space within WM, named as event cache, with distinct neural correlates compared to object storage in WM. We found the cerebellar network to be the most essential network for event cache, with the left cerebellum Crus I being particularly involved in encoding and maintaining events. Our findings shed critical light on the neuropsychological mechanism of WM by revealing event cache as an independent component of WM and encourage the reconsideration of theoretical models for WM.</p></abstract><abstract abstract-type="teaser"><p>The &#x0201c;event cache,&#x0201d; a working memory component reshaping our understanding of event storage and processing, is discovered.</p></abstract><funding-group><award-group id="award1785120"><funding-source>
<institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012226</institution-id><institution>Fundamental Research Funds for the Central Universities</institution></institution-wrap>
</funding-source><award-id>2022QZJH09</award-id></award-group><award-group id="award1785121"><funding-source>
<institution-wrap><institution-id institution-id-type="FundRef"/><institution>Research of Basic Discipline for the 2.0 Base of Top-notch Students Training Program</institution></institution-wrap>
</funding-source></award-group><award-group id="award1785124"><funding-source>
<institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap>
</funding-source><award-id>32271090</award-id></award-group><award-group id="award1785125"><funding-source>
<institution-wrap><institution-id institution-id-type="FundRef"/><institution>the STI 2030-Major Projects</institution></institution-wrap>
</funding-source><award-id>2021ZD0200409</award-id></award-group><award-group id="award1785126"><funding-source>
<institution-wrap><institution-id institution-id-type="FundRef"/><institution>The MOE Frontiers Science Center for Brain Science &#x00026; Brain-Machine Integration, Zhejiang University</institution></institution-wrap>
</funding-source></award-group><award-group id="award1785128"><funding-source>
<institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap>
</funding-source><award-id>81971245</award-id></award-group><award-group id="award1785131"><funding-source>
<institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap>
</funding-source><award-id>32000761</award-id></award-group><award-group id="award1785134"><funding-source>
<institution-wrap><institution-id institution-id-type="FundRef"/><institution>Key program of Natural Science Foundation of Zhejiang Province</institution></institution-wrap>
</funding-source><award-id>LZ20C090001</award-id></award-group><award-group id="award1964832"><funding-source>
<institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap>
</funding-source><award-id>32071044</award-id></award-group></funding-group></article-meta></front><body><sec sec-type="introduction" disp-level="1"><title>INTRODUCTION</title><p>Working memory (WM) refers to a system that temporarily stores and manipulates a limited set of information for ongoing tasks (<xref rid="R1" ref-type="bibr"><italic toggle="yes">1</italic></xref>&#x02013;<xref rid="R3" ref-type="bibr"><italic toggle="yes">3</italic></xref>). Although WM capacity is limited, it is essential for high-order cognitions, such as fluid intelligence (<xref rid="R4" ref-type="bibr"><italic toggle="yes">4</italic></xref>), reading comprehension (<xref rid="R5" ref-type="bibr"><italic toggle="yes">5</italic></xref>), planning (<xref rid="R6" ref-type="bibr"><italic toggle="yes">6</italic></xref>), and creative thinking (<xref rid="R7" ref-type="bibr"><italic toggle="yes">7</italic></xref>). WM deficits have been reported in various mental disorders such as Alzheimer&#x02019;s disease, attention deficit hyperactivity disorder, autism spectrum disorders, and schizophrenia (<xref rid="R8" ref-type="bibr"><italic toggle="yes">8</italic></xref>&#x02013;<xref rid="R11" ref-type="bibr"><italic toggle="yes">11</italic></xref>). Understanding the mechanisms of WM has substantial educational and clinical implications and has been a key topic in cognitive science and neuroscience over the past 50 years (<xref rid="R12" ref-type="bibr"><italic toggle="yes">12</italic></xref>, <xref rid="R13" ref-type="bibr"><italic toggle="yes">13</italic></xref>).</p><p>Different WM theories have been proposed to elucidate how information is stored and manipulated in WM. Two of the most influential models are the multicomponent model and the embedded-processes model (<xref rid="R1" ref-type="bibr"><italic toggle="yes">1</italic></xref>&#x02013;<xref rid="R3" ref-type="bibr"><italic toggle="yes">3</italic></xref>). The multicomponent model claims that WM consists of four components: central executive (CE), phonological loop, visuospatial sketchpad, and episodic buffer (<xref rid="R2" ref-type="bibr"><italic toggle="yes">2</italic></xref>, <xref rid="R14" ref-type="bibr"><italic toggle="yes">14</italic></xref>). The CE is responsible for coordinating and monitoring ongoing tasks in a top-down manner, while the remaining three are responsible for retaining verbal, visuospatial, and binding information (<xref rid="R14" ref-type="bibr"><italic toggle="yes">14</italic></xref>). Moreover, these distinct components are supported by different neural substrates or networks (<xref rid="R2" ref-type="bibr"><italic toggle="yes">2</italic></xref>). In contrast, the embedded-processes model and its variants suggest that all information is stored in a capacity-limited focus of attention (FoA), which is a subset of information in long-term memory that is temporarily activated (<xref rid="R15" ref-type="bibr"><italic toggle="yes">15</italic></xref>, <xref rid="R16" ref-type="bibr"><italic toggle="yes">16</italic></xref>).</p><p>Because of the reductionism routine, where complex information from the external world is simplified into more fundamental components, and past technical limitations in psychological explorations, the main body of WM studies, including theoretical models, has focused on verbal and visual objects and their constituent features (<xref rid="R2" ref-type="bibr"><italic toggle="yes">2</italic></xref>, <xref rid="R17" ref-type="bibr"><italic toggle="yes">17</italic></xref>). However, in daily life, individuals not only process objects and features (e.g., shapes, colors, and motion) but also attend to what happens to them&#x02014;namely, events. Events represent a distinct psychological construct, separate from objects, and are characterized by their temporal and spatial structures (<xref rid="R18" ref-type="bibr"><italic toggle="yes">18</italic></xref>, <xref rid="R19" ref-type="bibr"><italic toggle="yes">19</italic></xref>). Here, we operationally define an event as a spatiotemporally bound segment with clear initiation and termination points (<xref rid="R20" ref-type="bibr"><italic toggle="yes">20</italic></xref>). While events are typically characterized by spatiotemporal boundaries, not all stimuli with such boundaries qualify as events. A key feature of an event is its perceived coherence&#x02014;it generally maintains an internally structured and interpretable sequence throughout its duration (<xref rid="R20" ref-type="bibr"><italic toggle="yes">20</italic></xref>), with boundaries that naturally align with its intrinsic meaning. For example, biological movements (BMs) such as running or jumping exhibit coherence and thus constitute events, whereas a sequence of random, unstructured movements&#x02014;such as dynamic noise&#x02014;does not. While some theoretical frameworks suggest that static scenarios, such as a glass resting on a table, can be classified as an event because of their functional significance (<xref rid="R21" ref-type="bibr"><italic toggle="yes">21</italic></xref>), the predominant approach in event cognition differentiates static states from dynamic events, emphasizing that events typically involve structured temporal progression. Accordingly, this study focuses on the examination of dynamic events and their cognitive representations.</p><p>Events are fundamental units of our experience, shaping the way we connect and separate various occurrences (<xref rid="R19" ref-type="bibr"><italic toggle="yes">19</italic></xref>, <xref rid="R22" ref-type="bibr"><italic toggle="yes">22</italic></xref>). The human brain has evolved to use temporal structures in events to predict incoming information and enable functional behaviors. Consequently, events are considered to be one of the most important classes of entities in daily psychology (<xref rid="R18" ref-type="bibr"><italic toggle="yes">18</italic></xref>, <xref rid="R19" ref-type="bibr"><italic toggle="yes">19</italic></xref>) and should be a critical unit of analysis for most psychological domains, including WM. Recent studies have revealed that continuous information flow from the environment is segmented into events in the WM (<xref rid="R22" ref-type="bibr"><italic toggle="yes">22</italic></xref>&#x02013;<xref rid="R25" ref-type="bibr"><italic toggle="yes">25</italic></xref>). Existing research on events has been focusing on how an event is perceptually segmented from a continuous information flow and what is the temporal structure of an event (<xref rid="R22" ref-type="bibr"><italic toggle="yes">22</italic></xref>). In addition, an event has been extensively explored from the perspective of episodic memory, according to which the term of &#x0201c;event memory&#x0201d; was created (<xref rid="R22" ref-type="bibr"><italic toggle="yes">22</italic></xref>, <xref rid="R26" ref-type="bibr"><italic toggle="yes">26</italic></xref>). However, neither the psychological construct nor the underlying neural substrates of event processing in WM are well understood. In particular, it remains unknown whether there is a unique cache for events in WM.</p><p>Addressing this question is essential, as it touches on the core mechanisms of how the human brain organizes information over time. While both animate actions (e.g., BMs) and nonanimate changes (e.g., physical object movements) are often studied as representative examples of events in the past half-century (<xref rid="R19" ref-type="bibr"><italic toggle="yes">19</italic></xref>, <xref rid="R20" ref-type="bibr"><italic toggle="yes">20</italic></xref>, <xref rid="R22" ref-type="bibr"><italic toggle="yes">22</italic></xref>), to our knowledge, no study has systematically examined whether event storage constitutes a distinct WM component. Filling this gap requires not only robust empirical evidence but also a multidisciplinary approach capable of disentangling complex cognitive constructs.</p><p>To rigorously address this question, we adopt a multimodal integrative approach transcending traditional methodological boundaries. Specifically, four complementary investigative strands provide converging evidence (<xref rid="F1" ref-type="fig">Fig. 1</xref>). First, psychometric validation through confirmatory factor analysis (CFA) of 14 established WM tasks establishes whether event storage constitutes a statistically separable latent construct within the WM framework. Second, six psychophysical experiments using dual-task paradigms were conducted to provide behavioral evidence for the independence of event storage in WM. Third, machine learning analysis of resting-state functional magnetic resonance imaging (fMRI) connectivity patterns identifies neural signatures predictive of event-specific WM capacity. Last, task-based fMRI examines the spatiotemporal dynamics of cortical activation during the event versus object maintenance, aiming to verify the findings derived from resting-state fMRI. This methodological triangulation&#x02014;spanning psychometrics, psychophysics, and multimodal neuroimaging&#x02014;enables robust cross-validation of findings while mitigating the limitations inherent to any single methodology.</p><fig position="float" id="F1" fig-type="image" specific-use="distribute"><label>Fig. 1.</label><caption><title>Flowchart of the current study.</title><p>First, confirmatory factor analysis (CFA) was used to test candidate models on the behavioral data. Subsequently, six dual-task experiments were conducted to validate the independence of event cache from object cache and confirm the model chosen by CFA. Next, support vector regression (SVR) was conducted on resting-state functional magnetic resonance imaging (fMRI) data to identify neural signatures of working memory (WM) components in the optimal model. The most important network for event cache was identified by evaluating the network-wise connectivity degree. The node degree for each brain region in this network was calculated to find regions most contributing to the prediction of the event cache. Subsequently, virtual lesion analysis was done to assess each network&#x02019;s importance in prediction. Last, region of interest (ROI)-based analysis was performed on task fMRI data to verify the specific functional implications of the event-associated brain regions identified with resting-state data, and multivariate pattern analysis (MVPA) was additionally performed to deeply explore the role that the event-associated brain regions play in event-load and event-content processing. TransDirection, direction of transparent motion; TransVelocity, velocity of transparent motion; CSB, color-shape binding.</p></caption><graphic xlink:href="sciadv.adt3063-f1" position="float"/></fig></sec><sec sec-type="results" disp-level="1"><title>RESULTS</title><sec disp-level="2"><title>Latent variable modeling indicates event cache as an independent component in WM</title><sec disp-level="3"><title>
Descriptive statistics of WM measures used for latent variable modeling
</title><p>To estimate the latent variable models of WM, we used 14 tasks to tap the different components of WM. There were four event storage tasks [point-light display (PLD) BM, solid BM, rectangular movements (RecMoves), and circular movements], three object storage tasks (colors, shapes, and locations), two binding storage tasks (color-location and color-letter bindings), two CE tasks (anti-saccade and <italic toggle="yes">N</italic>-back), and three other tasks [operation span (Ospan), symmetry span (Sspan), and multiple-object tracking (MOT)]. The two complex span tasks (Ospan and Sspan) (<xref rid="R4" ref-type="bibr"><italic toggle="yes">4</italic></xref>, <xref rid="R27" ref-type="bibr"><italic toggle="yes">27</italic></xref>) have been used frequently to tap the capacity of storing information in the presence of distractions, reflecting the joint function of storage and CE of WM (<xref rid="R28" ref-type="bibr"><italic toggle="yes">28</italic></xref>). The common variance shared by the CE tasks and the two complex span tasks therefore mainly represents CE. The MOT, which has dynamic features without persisting higher-order stability, relies on the visuospatial sketchpad to process moving objects for a short interval (<xref rid="R29" ref-type="bibr"><italic toggle="yes">29</italic></xref>). For both event and object storage tasks, we used a simultaneous-display change detection paradigm, which is a standard approach in the object WM research. An eye-tracking experiment confirmed that participants can extract multiple events by sequentially encoding each dynamic item in this simultaneous-display setting (fig. S1; see Supplementary Text S1.1 to S1.3). A detailed rationale for task selection is provided in the Materials and Methods.</p><p>Cowan&#x02019;s <italic toggle="yes">K</italic> (<xref rid="R30" ref-type="bibr"><italic toggle="yes">30</italic></xref>) was used to estimate the WM capacity for each storage task. The formula of Scholl <italic toggle="yes">et&#x000a0;al.</italic> (<xref rid="R31" ref-type="bibr"><italic toggle="yes">31</italic></xref>) was used to estimate the tracked capacity of the MOT, and task accuracy was used to indicate the WM performance for the CE tasks and span tasks. Detailed descriptive statistics for the 14 WM tasks are presented in table S1, and their correlations are presented in table S2. Most measures showed acceptable levels of reliability (&#x02265;0.60) and relatively low skewness and kurtosis values (between &#x02212;2 and 2). The tasks assumed to tap the same WM component showed relatively higher correlations than those of a different WM component, suggesting both the convergent and discriminant validity of these measures. Overall, the WM tasks with strictly controlled experimental settings (the change detection paradigm), appropriate difficulty, good reliability, and validity provided a favorable prerequisite for latent variable modeling.</p></sec><sec disp-level="3"><title>
Latent variable modeling based on Baddeley and Cowan&#x02019;s WM models is suboptimal
</title><p>We initially evaluated three models according to Baddeley and Cowan&#x02019;s WM models (fig. S2). It is worth noting that events in the current testing were probed using visual stimuli, and verbal encoding was suppressed; hence, we did not consider the phonological loop in the multicomponent model of WM.</p><p>Model 1 was constructed to test whether all tasks could fit Cowan&#x02019;s embedded-processes model (<xref rid="R15" ref-type="bibr"><italic toggle="yes">15</italic></xref>), which included CE and FoA.According to this model, different types of stimuli are stored in FoA. Therefore, all tasks entailing the storage of visuospatial stimuli, binding, and events were assumed to load on a common FoA latent variable, while other tasks relating to CE loaded on the CE latent variable. Models 2 and 3 were constructed to test how well the tasks fit Baddeley&#x02019;s multicomponent model (<xref rid="R14" ref-type="bibr"><italic toggle="yes">14</italic></xref>). The only distinction between these two models was that the event storage tasks were loaded on a visuospatial sketchpad (model 2) or episodic buffer (model 3). The fit statistics of all the models are listed in table S3. Models 1 to 3 showed poor fitness according to the comparative fit index (CFI), suggesting that the two well-established models cannot fully account for the underlying structure of WM. In particular, the representations of events are probably not stored in FoA, conventional visuospatial sketchpad, or episodic buffer. It is possible that there is an additional storage space for events.</p></sec><sec disp-level="3"><title>
Models with event tasks loading on an independent component are superior
</title><p>We further tested another set of models considering an independent event cache for holding events in WM (fig. S2). We established model 4 based on Cowan&#x02019;s model, wherein the FoA was divided into event cache and object cache, with the former being responsible for storing events and the latter for static visual stimuli. Alternatively, we established model 5 according to Baddeley&#x02019;s model. In model 5, an event cache was introduced in addition to a visuospatial sketchpad and episodic buffer. Both models 4 and 5 showed an acceptable fit to the data (table S3). These results clearly demonstrate that there is an independent event cache underlying the WM structure.</p><p>The two types of events in this study have distinct properties. BMs of humans contain rich social information, whereas nonbiological movements (NBMs; i.e., RecMoves and circular movements) of physical shape do not (<xref rid="R32" ref-type="bibr"><italic toggle="yes">32</italic></xref>, <xref rid="R33" ref-type="bibr"><italic toggle="yes">33</italic></xref>). To further explore whether there are two different types of event cache owing to the difference in social semantics, we constructed models 6 and 7 based on models 4 and 5, respectively, by separating event cache into BM and NBM latent variables. Both models were acceptable. However, model 6 showed a better fit to the data than the other three models according to the comparisons of CFI and Akaike&#x02019;s information criterion (AIC) (table S3). Furthermore, loading either BM or NBM into a visuospatial sketchpad, episodic buffer, or object cache worsened model fitting (see models 8 to 13 in fig. S3), suggesting that events, including BM and NBM, are represented separately from objects in WM.</p><p>Given that the tasks of BM and NBM both tap the nature of events, we tested whether an underlying common event cache could be captured by a higher-order factor. We constructed model 14 based on model 6 (the best nonhierarchical model), where event cache was conceptualized as a second-order latent variable extracted from BM and NBM factors (<xref rid="F2" ref-type="fig">Fig. 2</xref>). We used the chi-square (&#x003c7;<sup>2</sup>) difference test to compare these two nested models. According to the result (&#x003c7;<sub>diff</sub><sup>2</sup>&#x000a0;=&#x000a0;2.6, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.273), there is no significant difference between the two models. On the basis of the principle of model simplicity, we should choose model 14, which was simpler (with greater degrees of freedom), to be the best description of the WM construct.</p><fig position="float" id="F2" fig-type="image" specific-use="distribute"><label>Fig. 2.</label><caption><title>Hierarchical structural model of WM (model 14).</title><p>SolidBM, solid biological movement; CirMove, circular movement; ColorLoc, color-location binding; ColorLet, color-letter binding; Anti, anti-saccade. The nonbiological movement factor yielded a nonsignificant negative residual variance, which therefore had to be fixed at zero. Asterisks indicate significant paths or loadings (**<italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.01). Ellipses represent latent variables, and rectangles represent observed indicators. The single-headed arrow points from a latent factor to an observed indicator or from a higher-order factor to lower-order factors, and the double-headed arrow indicates the correlation between latent variables. The single-headed arrow on an observed indicator indicates the error term.</p></caption><graphic xlink:href="sciadv.adt3063-f2" position="float"/></fig></sec><sec disp-level="3"><title>
Event cache is not simply driven by the motion features of the event stimuli
</title><p>It is reasonable to conjecture that event cache differed from object cache in the model, mainly because the to-be-remembered stimuli contained motion feature in the event storage tasks but static in the object storage tasks. To rule out this possibility, we constructed four models in which the MOT was loaded on the latent variable of BM (model a), NBM (model b), object cache (model c), and CE (model d) based on model 14. The fit statistics of the models are presented in table S3. The best model (model c) shows that object cache is responsible for storing moving objects. Therefore, the finding indicates that event cache differing from object cache is not due to the motion feature of task stimuli but rather the intrinsically different mechanisms between event storage and object storage in WM.</p><p>In short, our behavioral data suggest that event cache is an independent component of WM, which is distinct from object cache that is responsible for the WM storage of single features and binding information. Moreover, this distinction is not due to the motion feature of the task stimuli. In addition, the averaged WM capacity of event tasks (<italic toggle="yes">K</italic>&#x000a0;=&#x000a0;2.96) did not differ from that of object tasks [<italic toggle="yes">K</italic>&#x000a0;=&#x000a0;3.00; <italic toggle="yes">t</italic> (205)&#x000a0;=&#x000a0;&#x02212;0.837, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.404, Cohen&#x02019;s <italic toggle="yes">d</italic>&#x000a0;=&#x000a0;&#x02212;0.058], suggesting that the separation of event and object latent variables is not driven by task difficulty.</p></sec></sec><sec disp-level="2"><title>Psychophysical experiments further confirm the independence of event cache in WM</title><p>To verify that there are separate storage buffers for events and objects, we conducted six experiments using the dual-task paradigm (<xref rid="F3" ref-type="fig">Fig. 3A</xref>), wherein participants were required to simultaneously memorize two types of randomly presented stimuli (below, we used A and B to denote two distinct types of stimuli). If A and B shared the same storage buffer, they would compete for the limited storage capacity of WM. To be specific, experiments 1 to 5 consisted of five load conditions (2A, 2B, 2A&#x000a0;+&#x000a0;2B, 2A&#x000a0;+&#x000a0;4B, and 4A&#x000a0;+&#x000a0;2B) and experiment 6 contained three conditions (2A&#x000a0;+&#x000a0;2B, 2A&#x000a0;+&#x000a0;4B, and 4A&#x000a0;+&#x000a0;2B), where the numbers indicate how many items are under these conditions. Trials of these conditions were presented randomly, and each type of stimulus was probed with equal probability. Each experiment contained 20 participants, with the sample size determined using the sequential Bayes factor design (<xref rid="R34" ref-type="bibr"><italic toggle="yes">34</italic></xref>). Conditions 2A and 2B served the purpose of introducing lower WM load scenarios and ensuring good WM performance when memorizing two stimuli (accuracy &#x02265;70% for both conditions in all the experiments; see table S4), hence effectively averting the emergence of floor effects under higher-load conditions (i.e., retaining four or six stimuli) in a sequential display. Detailed descriptive statistics of each experiment are presented in table S4.</p><fig position="float" id="F3" fig-type="image" specific-use="distribute"><label>Fig. 3.</label><caption><title>Design of the dual-tasks and behavioral results.</title><p>(<bold>A</bold>) Schematic illustration of a single trial in the dual-task paradigm. (<bold>B</bold> to <bold>F</bold>) Memory performance for different types of stimuli in experiments 1 to 5, where the memorized items were presented sequentially. The first column shows the results of memory accuracy for type A stimuli under different loads (2 or 4) of A while keeping the load of B fixed at 2. The significant decrease in performance under high-load condition indicates the effectiveness of WM load manipulation. The second column shows the results of memory accuracy for fixed-load materials (type A, load 2) under different loads of the other type of material (type B, load 2 or 4) using the load effect to investigate whether the two types of stimuli compete for the storage capacity. (<bold>G</bold>) Memory performance for experiment 6, where four colorized BMs were displayed simultaneously. In the probe, a black BM or a color square was displayed in the screen center. The other aspects were the same as in experiments 1 to 5. The legend in the figure represents the stimulus used as type A in the <italic toggle="yes">x</italic> axis. The black color indicates the stimulus targeting the event component; the pink color indicates the stimulus targeting the object component. The error bar stands for the standard error of the mean. &#x0201c;**&#x0201d; denotes significant load effect <italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.01, &#x0201c;*&#x0201d; denotes significant load effect 0.01&#x000a0;&#x0003c;&#x000a0;<italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.05, and &#x0201c;n.s.&#x0201d; denotes a nonsignificant load effect.</p></caption><graphic xlink:href="sciadv.adt3063-f3" position="float"/></fig><p>Two groups of repeated-measures analysis of variance (ANOVA) on memory accuracy were performed. The first group, which was named as same-type-manipulation ANOVA (abbreviated as STM_ANOVA), was used to verify the effectiveness of memory load manipulation by increasing two items that belong to the same type of material. A significant main effect of load was expected in a 2 (material type: A versus B)&#x02013;by&#x02013;2 (load: 2 and 4) ANOVA, in which the memory accuracy of A under the conditions of 2A&#x000a0;+&#x000a0;2B and 4A&#x000a0;+&#x000a0;2B and the memory accuracy of B under 2A&#x000a0;+&#x000a0;2B and 2A&#x000a0;+&#x000a0;4B were tested. The second group, which was named as cross-type-manipulation ANOVA (abbreviated as CTM_ANOVA), was used to test whether the two types of materials share a common storage in WM. If they do, a significant load effect should be identified by a 2 (material type: A versus B)&#x02013;by&#x02013;2 (load: 2 and 4) ANOVA, in which the memory accuracy of A taken from the conditions of 2A&#x000a0;+&#x000a0;2B and 2A&#x000a0;+&#x000a0;4B and the memory accuracy of B taken from the conditions of 2A&#x000a0;+&#x000a0;2B and 4A&#x000a0;+&#x000a0;2B were tested.</p><sec disp-level="3"><title>
Experiment 1: The storage of BM and NBM events interferes with each other
</title><p>As the BM and NBM (RecMove) were used to establish the &#x0201c;event&#x0201d; component, we first tested the hypothesis that the representations of these two types of stimuli would interfere with each other. Results of the STM_ANOVA showed that the increase in WM load of the stimulus type to be detected significantly decreased the memory accuracy (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 13.171, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.002, <inline-formula><mml:math id="m1" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.409, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;18.851, <xref rid="F3" ref-type="fig">Fig. 3B</xref>), indicating that the load manipulation was effective. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.493, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.491, <inline-formula><mml:math id="m2" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.025, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.321) and the interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.009, <italic toggle="yes">P</italic>&#x000a0;= 0.926, <inline-formula><mml:math id="m3" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 4.675 &#x000d7; 10<sup>&#x02212;4</sup>, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.314) were nonsignificant.</p><p>Meanwhile, results of the CTM_ANOVA showed that the increase in WM load of the stimulus type not to be detected also had a significant load effect on the memory accuracy of the stimulus type being detected (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 22.020, <italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.001, <inline-formula><mml:math id="m4" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.537, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;95.945, <xref rid="F3" ref-type="fig">Fig. 3B</xref>), suggesting that BM and RecMove share a common storage space in WM. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 1.457, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.242, <inline-formula><mml:math id="m5" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.071, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.498) and the interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.265, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.612, <inline-formula><mml:math id="m6" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.014, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.447) were nonsignificant. This result further confirms our CFA result that BM and NBM belong to the same event component.</p></sec><sec disp-level="3"><title>
Experiment 2: The storage of BM events and objects (the direction of transparent motion) does not interfere with each other
</title><p>Then, we tested the possibility that BM and RecMove share one buffer, which may be due to the fact that they both have motion feature instead of the putative event nature. A previous study has shown that the transparent motion is represented in the form of object (<xref rid="R35" ref-type="bibr"><italic toggle="yes">35</italic></xref>). To this end, we replaced the RecMove with transparent motion consisting of dots that continuously move in one direction without beginnings or endings. Participants were required to retain the moving direction of transparent motion.</p><p>The STM_ANOVA showed that the increase in WM load of the stimulus type to be detected significantly decreased the memory accuracy (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 8.418, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.009, <inline-formula><mml:math id="m7" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.307, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;5.040, <xref rid="F3" ref-type="fig">Fig. 3C</xref>), indicating that the load manipulation was effective. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.541, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.471, <inline-formula><mml:math id="m8" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.028, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.387) and the interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.818, <italic toggle="yes">P</italic> = 0.377, <inline-formula><mml:math id="m9" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.041, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.429) were nonsignificant.</p><p>However, results of the CTM_ANOVA showed that the increase in WM load of the stimulus type not to be detected had no significant load effect on the memory accuracy of the stimulus type being detected (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 0.424, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.523, <inline-formula><mml:math id="m10" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.022, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.332, <xref rid="F3" ref-type="fig">Fig. 3C</xref>), suggesting that BM and transparent motion do not share a common storage space. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 1.367, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.257, <inline-formula><mml:math id="m11" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.067, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.425) and the interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 1.103, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.307, <inline-formula><mml:math id="m12" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.055, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.655) were nonsignificant. This result echoes with the CFA results showing that the MOT task did not load onto the event component and excludes the motion feature hypothesis of experiment 1.</p></sec><sec disp-level="3"><title>
Experiment 3: The storage of NBM events and objects (the direction of transparent motion) does not interfere with each other
</title><p>To examine whether the noncompetition between BM event and transparent motion could be generalized to another type of event, we replaced BM in experiment 2 with the NBM (i.e., RecMove). The STM_ANOVA results demonstrated that the increase in WM load of the stimulus type to be detected significantly decreased the memory accuracy (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 10.442, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.004, <inline-formula><mml:math id="m13" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.354, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;3.042, <xref rid="F3" ref-type="fig">Fig. 3D</xref>), indicating that the load manipulation was effective. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 2.787, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.111, <inline-formula><mml:math id="m14" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.128, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;1.101) and the interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.004, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.952, <inline-formula><mml:math id="m15" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 1.946 &#x000d7; 10<sup>&#x02212;4</sup>, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.281) were nonsignificant.</p><p>Critically, results of the CTM_ANOVA showed that the increase in WM load of the stimulus type not to be detected had no significant load effect on memory accuracy (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 0.388, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.541, <inline-formula><mml:math id="m16" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.020, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.326, <xref rid="F3" ref-type="fig">Fig. 3D</xref>), suggesting that RecMoves and transparent motion do not share a common storage space. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 2.012, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.172, <inline-formula><mml:math id="m17" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.096, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.690) and the interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.563, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.462, <inline-formula><mml:math id="m18" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.029, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.410) were nonsignificant. This result extends the finding of experiment 2, supporting the prediction that RecMoves are stored independently from simple motion features.</p></sec><sec disp-level="3"><title>
Experiment 4: The storage of BM events and objects (the velocity of transparent motion) does not interfere with each other
</title><p>To examine whether the key finding of experiment 2 could be generalized to other motion attributes, we replaced the transparent motion direction task with a transparent motion velocity task. Similarly, the STM_ANOVA demonstrated that the increase in WM load of the stimulus type to be detected significantly decreased the memory accuracy (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 10.842, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.004, <inline-formula><mml:math id="m19" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.886, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;9.675, <xref rid="F3" ref-type="fig">Fig. 3E</xref>), indicating that the load manipulation was effective. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 148.056, <italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.001, <inline-formula><mml:math id="m20" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.886, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;5.843 &#x000d7; 10<sup>7</sup>) was significant. The interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.136, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.717, <inline-formula><mml:math id="m21" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.007, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.454) was nonsignificant.</p><p>Critically, results of the CTM_ANOVA showed that the increase in WM load of the stimulus type not to be detected had no significant load effect on memory accuracy (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 0.496, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.490, <inline-formula><mml:math id="m22" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.025, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.326, <xref rid="F3" ref-type="fig">Fig. 3E</xref>), suggesting that BM and the velocity of transparent motion do not share a common storage space. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 146.077, <italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.001, <inline-formula><mml:math id="m23" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.885, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;5.436 &#x000d7; 10<sup>7</sup>) was significant. The interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 1.233, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.281, <inline-formula><mml:math id="m24" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.061, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.558) was nonsignificant. This result extends the finding of experiment 2, supporting the prediction that BMs are stored independently from simple motion features.</p></sec><sec disp-level="3"><title>
Experiment 5: The storage of BM events and objects (human posture) does not interfere with each other
</title><p>Our model predicts that any static visual stimuli (i.e., objects) would not compete for the WM capacity with event. In this experiment, static human posture images extracted from BM were used, in contrast to BM. As both BM and human posture contain rich biological information and activate mirror neurons (<xref rid="R36" ref-type="bibr"><italic toggle="yes">36</italic></xref>), the influence of biological information would be controlled in this experiment.</p><p>The STM_ANOVA results showed that the increase in WM load of the stimulus type to be detected significantly decreased the memory accuracy (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 14.043, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.001, <inline-formula><mml:math id="m25" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.425, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;20.885, <xref rid="F3" ref-type="fig">Fig. 3F</xref>), indicating that the load manipulation was effective. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.192, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.666, <inline-formula><mml:math id="m26" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.010, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.329) and the interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 1.453, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.243, <inline-formula><mml:math id="m27" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.071, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.598) were nonsignificant.</p><p>Critically, results of the CTM_ANOVA showed that the increase in WM load of the stimulus type not to be detected had no significant load effect on memory accuracy (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 0.033, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.858, <inline-formula><mml:math id="m28" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.002, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.301, <xref rid="F3" ref-type="fig">Fig. 3F</xref>), suggesting that BM is stored independently from human posture in WM. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.155, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.698, <inline-formula><mml:math id="m29" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.008, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.327) and the interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 1.641, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.216, <inline-formula><mml:math id="m30" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.080, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.746) were nonsignificant. This result is in line with the prediction that human postures are stored as objects, even though they share similarities with BM.</p></sec><sec disp-level="3"><title>
Experiment 6: The storage of BM events and objects (colors) does not interfere with each other
</title><p>Last, we examined whether the separate retention of events and objects in experiments 2 to 5 stemmed from inherent visual attribute differences (e.g., stimulus offset) among two stimulus categories. We presented four colorized BMs simultaneously, which may contain two distinct BMs and two distinct colors, two distinct BMs and four distinct colors, or four distinct BMs and two distinct colors (fig. S5). Participants were tasked with retaining both sets of information and determining whether the probed BM or color appeared in the memory array.</p><p>The STM_ANOVA results demonstrated that the increase in WM load of the stimulus type to be detected significantly decreased the memory accuracy (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 34.943, <italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.001, <inline-formula><mml:math id="m31" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.648, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;1870.439, <xref rid="F3" ref-type="fig">Fig. 3G</xref>), indicating that the load manipulation was effective. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 17.916, <italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.001, <inline-formula><mml:math id="m32" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.485, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;25.399) was significant. The interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 1.114, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.304, <inline-formula><mml:math id="m33" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.055, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.517) was not significant.</p><p>Critically, results of the CTM_ANOVA showed that the increase in WM load of the stimulus type not to be detected had no significant load effect on the memory accuracy of the stimulus type being detected (main effect of load: <italic toggle="yes">F</italic><sub>1,19</sub> = 0.362, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.554, <inline-formula><mml:math id="m34" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.019, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.331, <xref rid="F3" ref-type="fig">Fig. 3G</xref>), suggesting that BM and color are stored independently in WM. The main effect of material type (<italic toggle="yes">F</italic><sub>1,19</sub> = 19.542, <italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.001, <inline-formula><mml:math id="m35" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.507, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;21.991) was significant. The interaction (<italic toggle="yes">F</italic><sub>1,19</sub> = 0.083, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.776, <inline-formula><mml:math id="m36" display="inline" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">&#x003b7;</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.004, <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;=&#x000a0;0.342) was nonsignificant. This result echoes with the CFA results showing that the BM and color loaded onto distinct components and excludes the visual attribute difference hypothesis of experiments2 to 5.</p><p>Together, the results of these experiments provide strong behavioral evidence for the existence of a separate event cache in WM. To further verify and extend these results, we conducted an extra experiment comparing transparent motion versus human posture (fig. S6). Our results confirmed that the dynamic stimulus (transparent motion in experiment 2) interfered with the static object stimulus (human posture in experiment 5), both belonging to object cache (fig. S7).</p></sec></sec><sec disp-level="2"><title>Event cache has independent neural signatures revealed by resting-state fMRI data</title><sec disp-level="3"><title>
Support vector regression predicts event cache in addition to classical CE and object components of WM
</title><p>Based on model 14, we further applied support vector regression (SVR) on resting-state functional connectivity to identify the neural signatures of each WM component [i.e., event cache (EVENT), object cache (OBJECT), and CE]. We computed the functional connectivity matrix for each participant using a brain atlas that consists of 268 areas (nodes) covering the whole brain (<xref rid="R37" ref-type="bibr"><italic toggle="yes">37</italic></xref>), which were assigned to 10 brain networks (<xref rid="R38" ref-type="bibr"><italic toggle="yes">38</italic></xref>) to facilitate network-based analysis. Specifically, the time course of each node was extracted and a Pearson correlation coefficient was calculated between each pair of nodes, which was further transformed to Fisher&#x02019;s <italic toggle="yes">Z</italic> score to ensure normality. An upper triangular connectivity matrix (35,778 edges) was used in the prediction analyses. On the basis of previous studies that conducted SVR to reveal brain-behavior associations, a prediction model with a feature number of ~200 achieved good performance (<xref rid="R39" ref-type="bibr"><italic toggle="yes">39</italic></xref>, <xref rid="R40" ref-type="bibr"><italic toggle="yes">40</italic></xref>). Thus, we selected several subsets of features with ~200 features to investigate the prediction performance. Specifically, five subsets of features ranking from the top 1 to 9&#x02030;, with an increase step of 2&#x02030;, were used to predict the individual differences in WM components (with feature numbers ranging from 35 to 322, more subsets were also tested; see fig. S8).</p><p>Prediction analyses revealed that all three WM components could be predicted by resting-state functional connectivity patterns (<xref rid="F4" ref-type="fig">Fig. 4, A and B</xref>, and fig. S8). In particular, the EVENT component could be predicted by a relatively small set of edges (with thresholds of 1 and 3&#x02030;). With a threshold of 1&#x02030;, the prediction model achieved the best performance, and the predicted EVENT scores were positively correlated with the actual values (<italic toggle="yes">r</italic>&#x000a0;=&#x000a0;0.36, permutation <italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.001; <xref rid="F4" ref-type="fig">Fig. 4, B and C</xref>). The model for OBJECT prediction achieved the best performance under the threshold of 9&#x02030;, and the predicted OBJECT scores were positively correlated with the actual values (<italic toggle="yes">r</italic>&#x000a0;=&#x000a0;0.36, permutation <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.001; <xref rid="F4" ref-type="fig">Fig. 4, B and C</xref>). For the CE component, the prediction model could successfully predict individuals&#x02019; CE scores under thresholds of 5, 7, and 9&#x02030;, and the model performed the best under the threshold of 5&#x02030;. The correlation between predicted and actual values was significant (<italic toggle="yes">r</italic>&#x000a0;=&#x000a0;0.28, permutation <italic toggle="yes">P</italic>&#x000a0;&#x0003c;&#x000a0;0.001, <xref rid="F4" ref-type="fig">Fig. 4, B and C</xref>). The characteristics of the selected features in the prediction models are visualized under the corresponding thresholds in the &#x0201c;Event WM capacity can be predicted by resting-state brain networks distinct from those of other WM components&#x0201d; and &#x0201c;Brain regions predicting event WM capacity are different from those for other WM components&#x0201d; sections.</p><fig position="float" id="F4" fig-type="image" specific-use="distribute"><label>Fig. 4.</label><caption><title>Prediction performance of support vector regression (SVR) models and the correlation between predicted and actual values.</title><p>(<bold>A</bold>) Prediction performance of the SVR model for each threshold. Note that &#x0201c;**&#x0201d; indicates that the permuted <italic toggle="yes">P</italic> value was considered significant after FDR correction for 15 comparisons (5 sets of feature thresholds &#x000d7; 3 WM components). (<bold>B</bold>) Scatter plots show significant positive correlation between predicted and actual values. The dash line indicates 95% confidence interval. (<bold>C</bold>) Bar plots show the permutation results, with the red line indicating the location of the correlation coefficient between actual and predicted WM scores.</p></caption><graphic xlink:href="sciadv.adt3063-f4" position="float"/></fig></sec><sec disp-level="3"><title>
Event WM capacity can be predicted by resting-state brain networks distinct from those of other WM components
</title><p>To evaluate the importance of different brain networks in the prediction of these three WM components, the 268 nodes were grouped into 10 networks (<xref rid="F5" ref-type="fig">Fig. 5</xref> and table S5) (<xref rid="R38" ref-type="bibr"><italic toggle="yes">38</italic></xref>), and the edges selected as features in the prediction models were plotted (<xref rid="F5" ref-type="fig">Fig. 5A</xref>). As different features were selected in each iteration, only the features selected in all iterations (103) were considered, and the absolute weights of these features were summed to represent the connectivity between networks (<xref rid="F5" ref-type="fig">Fig. 5A</xref>) with the calculation formula as follows: <inline-formula><mml:math id="m37" display="inline" overflow="scroll"><mml:msub><mml:mtext mathvariant="italic">Edge</mml:mtext><mml:mi mathvariant="italic">IJ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>J</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:mspace width="-.15em"/><mml:msub><mml:mtext mathvariant="italic">edge</mml:mtext><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mspace width="-.15em"/><mml:mo>&#x02223;</mml:mo></mml:math></inline-formula>. Capital letters &#x0201c;<italic toggle="yes">I</italic>&#x0201d; and &#x0201c;<italic toggle="yes">J</italic>&#x0201d; refer to network <italic toggle="yes">I</italic> and network <italic toggle="yes">J</italic>, and &#x0201c;<italic toggle="yes">Edge</italic>&#x0201d; refers to the connectivity strength between two networks. Lowercase letters &#x0201c;<italic toggle="yes">i</italic>&#x0201d; and &#x0201c;<italic toggle="yes">j</italic>&#x0201d; refer to node <italic toggle="yes">i</italic> and node <italic toggle="yes">j</italic>, and &#x0201c;<italic toggle="yes">edge</italic>&#x0201d; refers to the feature weight between the two nodes. To quantitatively illustrate the relative contribution of each network, the relative degree (<italic toggle="yes">RD</italic>) of each network was calculated. The degree of each network (number of edges belonging to a network) was normalized by the total number of existing network degrees using the following formula: <inline-formula><mml:math id="m38" display="inline" overflow="scroll"><mml:msub><mml:mtext mathvariant="italic">RD</mml:mtext><mml:mi>I</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>J</mml:mi></mml:msub><mml:mtext>binarize</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mtext mathvariant="italic">Edge</mml:mtext><mml:mi mathvariant="italic">IJ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>I</mml:mi></mml:msub><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>J</mml:mi></mml:msub><mml:mtext>binarize</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mtext mathvariant="italic">Edge</mml:mtext><mml:mi mathvariant="italic">IJ</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mfrac></mml:math></inline-formula>. The capital letters refer to networks, and the &#x0201c;binarize&#x0201d; function transfers a nonzero value to a 1 and keeps a zero value unchanged. The <italic toggle="yes">RD</italic> values are shown in the radar graphs (<xref rid="F5" ref-type="fig">Fig. 5B</xref>). According to the <italic toggle="yes">RD</italic> values (<xref rid="F5" ref-type="fig">Fig. 5B</xref> and table S6), the cerebellum network had the highest <italic toggle="yes">RD</italic> among the 10 networks in the EVENT prediction model and was considered the most important network for predicting the EVENT component, whereas the medial frontal, frontal-parietal-temporal associative, salience, and visual association networks were the top four networks with similar <italic toggle="yes">RD</italic> values for OBJECT prediction. The frontoparietal network and the default mode network played the most important roles in predicting the CE component.</p><fig position="float" id="F5" fig-type="image" specific-use="distribute"><label>Fig. 5.</label><caption><title>Chord charts of selected features in three prediction models and the relative importance of brain networks for each component.</title><p>(<bold>A</bold>) Chord charts of between-network Edges show distinct neural signatures for the three WM components. The between-network Edge was the sum of feature weights between nodes that constitute two networks, and the thickness of lines within a chart represents the strength of between-network Edge. (<bold>B</bold>) Radar graphs show successful prediction of the three WM components&#x02019; weighting on different networks. FPT associative, frontal-parietal-temporal associative.</p></caption><graphic xlink:href="sciadv.adt3063-f5" position="float"/></fig></sec><sec disp-level="3"><title>
Brain regions predicting event WM capacity are different from those for other WM components
</title><p>Although we identified the most important networks for each WM component, which brain regions within these networks contribute the most remained unclear. To this end, we calculated the degree of nodes (also referred to as node degree) within these networks and ranked nodes according to their degrees to identify the most important nodes within each network (fig. S9 and table S7; for the node degree of all networks, please refer to fig. S10). The results displayed that the node with the largest degree was in the left posterior cerebellum, mainly in Crus I for the EVENT component. As for the OBJECT component, the nodes consisting of the left middle/superior frontal gyrus, right middle/superior temporal gyrus, left inferior/middle occipital gyrus, right postcentral gyrus, superior parietal lobule, and middle cingulate cortex contribute the most in prediction. In contrast, for the CE component, the right middle frontal gyrus within the frontoparietal network and the node covering the left middle occipital gyrus, middle temporal gyrus, angular gyrus, and precuneus within the default mode network had the largest degree.</p></sec><sec disp-level="3"><title>
Virtual lesion study on resting-state fMRI data shows that the cerebellar network plays a critical role in predicting event WM
</title><p>To further validate the essential role of the cerebellar network for the EVENT component, the prediction procedure was performed after removing nodes belonging to the cerebellar network (<xref rid="R41" ref-type="bibr"><italic toggle="yes">41</italic></xref>). Permutation tests were performed to confirm the significance, and the false discovery rate (FDR) method was applied to correct for multiple comparisons. The results showed that without the cerebellum, the EVENT component could not be successfully predicted (<xref rid="F6" ref-type="fig">Fig. 6A</xref>), whereas the EVENT component could still be predicted when removing any other network (<xref rid="F6" ref-type="fig">Fig. 6B</xref>). To further investigate whether the role of the cerebellar network is specific for EVENT prediction, we also performed virtual lesion analyses for CE and OBJECT, which showed that without the cerebellar network, CE and OBJECT could still be predicted (<xref rid="F6" ref-type="fig">Fig. 6A</xref>), indicating that the cerebellar network was only essential in predicting the EVENT component. In addition, no single network played a decisive role in predicting OBJECT because the OBJECT component could be successfully predicted regardless of which network was removed (fig. S11). For the CE component, the prediction model failed when the frontoparietal network, medial frontal network, or visual B network was removed (fig. S12).</p><fig position="float" id="F6" fig-type="image" specific-use="distribute"><label>Fig. 6.</label><caption><title>Prediction performance after removing one brain network.</title><p>(<bold>A</bold>) Prediction performance of EVENT, CE, and OBJECT components after removing the cerebellar network. CE and OBJECT components could still be predicted, but the EVENT component could not be predicted after the cerebellar network was removed. Note that &#x0201c;**&#x0201d; indicates that the permuted <italic toggle="yes">P</italic> value was considered significant after FDR correction for 15 comparisons (5 sets of feature thresholds &#x000d7; 3 WM components). (<bold>B</bold>) The EVENT component could be predicted after removing any of the other nine networks. Note that &#x0201c;**&#x0201d; indicates that the permuted <italic toggle="yes">P</italic> value, was considered significant after FDR correction for 45 comparisons (5 sets of feature thresholds &#x000d7; 9 networks).</p></caption><graphic xlink:href="sciadv.adt3063-f6" position="float"/></fig></sec></sec><sec disp-level="2"><title>The key region of the cerebellar network identified from resting-state fMRI prediction analysis is activated specifically by event tasks</title><sec disp-level="3"><title>
Region of interest&#x02013;based analysis on task fMRI data reveals load-dependent activation of the left posterior cerebellum in event WM representations
</title><p>Both the prediction analyses and virtual lesion analyses indicated that the resting-state connectivity of the cerebellum played an important role in predicting the EVENT component. In addition, the assessment of nodal contribution to the prediction indicated that the node of the left posterior cerebellum Crus I was the most stable feature for prediction (fig. S9). However, whether this region, identified by resting-state connectivity analysis, is functionally involved in WM processing is unclear. To address this issue, we further examined the activations (i.e., &#x003b2; values) of this cerebellar node (<xref rid="F7" ref-type="fig">Fig. 7A</xref>) using general linear modeling (GLM) analysis on an event <italic toggle="yes">N</italic>-back and a change detection fMRI task.</p><fig position="float" id="F7" fig-type="image" specific-use="distribute"><label>Fig. 7.</label><caption><title>Region of interest (ROI) analyses for WM tasks.</title><p>(<bold>A</bold>) Anatomical location of the cerebellar node. (<bold>B</bold>) The cerebellar node was significantly activated in the <italic toggle="yes">N</italic>-back task in a load-dependent manner under both BM and NBM conditions. (<bold>C</bold>) The cerebellar node showed similar load-dependent activation (4-sets&#x000a0;&#x0003e;&#x000a0;2-sets) during the delay phase of BM and NBM conditions on the event change detection task. The horizontal line in the middle of the violin plots depicts the median, and the lines above and below depict 25th and 75th quartiles. (<bold>D</bold>) Time course&#x02013;based analysis revealed significant load-dependent activation during the delay phase under both BM and NBM conditions on the event change detection task. The black stars represent the significant load effect under BM conditions, whereas the red stars represent the significant load effect under NBM conditions. The error bar represents the standard error of the mean. (<bold>E</bold>) The cerebellar node was significantly activated during the delay period, but no load-dependent pattern was found on the object (color-shape binding) change detection task. (<bold>F</bold>) The WM performance of BM condition and (<bold>G</bold>) that of NBM condition were significantly correlated with the responses of the left cerebellar node during the delay period. (<bold>H</bold>) The performance of the object task was not significantly correlated with the cerebellar response during the delay period of this task. The dash line in the correlation plot indicates 95% confidence interval. The pink color indicates the stimulus targeting the object component. Note that (A) to (E) illustrate the WM load effect on the cerebellar ROI, and &#x0201c;**&#x0201d; indicates that the <italic toggle="yes">P</italic> value was considered significant after FDR correction for 21 comparisons (10 one-sample <italic toggle="yes">t</italic> tests and 11 paired <italic toggle="yes">t</italic> tests); (F) to (H) depict the brain-behavior correlations, and &#x0201c;**&#x0201d; indicates that the <italic toggle="yes">P</italic> value was considered significant after FDR correction for three comparisons.</p></caption><graphic xlink:href="sciadv.adt3063-f7" position="float"/></fig><p>The behavioral results demonstrated that accuracies of all task conditions were above 0.6 and the accuracy of high-load condition was significantly lower than that of low-load condition (table S8). Brain activation results suggested that Crus I in the left posterior cerebellum was significantly activated in the <italic toggle="yes">N</italic>-back task (both at low and high loads for BM/NBM conditions; <xref rid="F7" ref-type="fig">Fig. 7B</xref> and table S9). Furthermore, significantly higher activation for the high-load condition (2-back) relative to the low-load condition (0-back) was found for both BM and NBM conditions (table S9). In the event change detection task, a significantly higher activation in high load (4-sets) relative to the low load (2-sets) during the delay phase was found under both BM and NBM conditions (table S9 and <xref rid="F7" ref-type="fig">Fig. 7C</xref>). A time-series analysis also confirmed significantly higher activations for the high-load condition (4-sets) relative to the low-load condition (2-sets) during the delay phase under both BM and NBM conditions (<xref rid="F7" ref-type="fig">Fig. 7D</xref> and table S10).</p><p>To examine the specificity of the cerebellum region for event cache, we further included an object-based binding change detection task (color-shape binding; fig. S13). This task was selected because the loadings of the binding tasks on the object cache factor were higher than the other object tasks in model 14. The behavioral results revealed that accuracies of all task conditions were above 0.6 and the accuracy of high-load condition was significantly lower than that of low-load condition (table S8). We extracted the &#x003b2; value of the left cerebellum region from the delay period. The results showed that the left cerebellum was significantly activated (both at low and high loads for delay period; <xref rid="F7" ref-type="fig">Fig. 7E</xref> and table S9). However, the load effect (4-sets versus 2-sets) was not significant during the delay period (<xref rid="F7" ref-type="fig">Fig. 7E</xref> and table S9).</p></sec><sec disp-level="3"><title>
The activation of the left posterior cerebellum correlates with behavioral performance in event tasks but not object tasks
</title><p>We further examined the relationship between brain response (the average of <italic toggle="yes">z</italic>-transformed activation for two WM loads) and behavioral performance (the average of <italic toggle="yes">z</italic>-transformed accuracies for two WM loads) on the WM storage tasks (i.e., change detection task). The results showed significant correlations between the cerebellum response during the delay period and performance in the two event tasks (BM: <italic toggle="yes">r</italic>&#x000a0;=&#x000a0;0.41, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.017, <xref rid="F7" ref-type="fig">Fig. 7F</xref>; NBM: <italic toggle="yes">r</italic>&#x000a0;=&#x000a0;0.37, <italic toggle="yes">P</italic>&#x000a0;=&#x000a0;0.030, <xref rid="F7" ref-type="fig">Fig. 7G</xref>). However, for the color-shape binding task, no significant correlations were found between cerebellum activation during the delay period and accuracy (<xref rid="F7" ref-type="fig">Fig. 7H</xref>).</p><p>In short, using two different event stimulus materials with two different WM paradigms, our data showed that the cerebellar node with the highest predictive power was consistently activated in a load-dependent manner during WM tasks requiring the involvement of the event component, and the cerebellum activation tracks with the WM ability, whereas such a relationship is nonsignificant in the object-based binding task, indicating that the left cerebellum was specific for event cache.</p></sec><sec disp-level="3"><title>
The left posterior cerebellum distinguishes event from object processing and tracks WM load in event tasks
</title><p>Last, to further verify the specific role of the cerebellar region of interest (ROI) in event processing, we conducted multivariate pattern analyses (MVPAs) and found that a classifier trained on activation values from BM low/high loads could distinguish between NBM low and high loads but not between object low and high loads. In addition, a classifier trained on the event (BM) and object conditions could distinguish between these two categories but not between BM and NBM conditions. These results remained consistent when the NBM condition was used as the training set. These results indicate that the activity of this cerebellum region not only tracks load manipulation in the event tasks but also processes event-specific content information. For details in the methodology and results of MVPA, please refer to Supplementary Text S2 (figs. S14 to 16).</p></sec></sec></sec><sec sec-type="discussion" disp-level="1"><title>DISCUSSION</title><sec disp-level="2"><title>CEO model of WM</title><p>By using BM and NBM as the representative stimuli of events in daily life, our study offers evidence supporting an independent storage component (i.e., event cache) in WM. Event cache, as a psychological construct, is distinct from the WM storage space holding objects and related features (i.e., object cache in the current study; <xref rid="F8" ref-type="fig">Fig. 8</xref>). The independence of event cache is further confirmed by psychophysical experiments with dual-task paradigm. Moreover, SVR prediction analyses based on resting-state functional connectivity successfully predicted event cache, object cache, and CE components with distinct patterns. Critically, the cerebellar network emerged as the key component of the event cache, with the left cerebellum Crus I exhibiting the highest node degree in the prediction model. Its activation during event tasks correlated with behavioral performance and was sensitive to event-related load and content. These results provided convergent evidence from behavioral and neural data to support event cache as an independent component of WM.</p><fig position="float" id="F8" fig-type="image" specific-use="distribute"><label>Fig. 8.</label><caption><title>Illustrations of different working memory models with distinct predictions.</title><p>Both the embedded-processes model and multiple-component model predict that BM and NBM share one storage buffer with objects (including the constituent attributes), while the CEO model predicts that BM and NBM adopt a separate representation format, resulting in independent storage distinct from objects.</p></caption><graphic xlink:href="sciadv.adt3063-f8" position="float"/></fig><p>On the basis of current empirical insights, we have cautiously introduced a model of WM, termed the CEO (central executive-event cache-object cache) model, as a framework to elucidate the conceptual architecture of WM (<xref rid="F8" ref-type="fig">Fig. 8</xref>). In this model, CE assumes the role of overseeing and directing attention across multiple concurrent tasks, while two subordinate components, object cache and event cache, are entrusted with the maintenance of objects (including their constituent attributes) and events, respectively. Object cache and event cache own distinct neural substrates and cognitive processing, leading to resource competition within cache yet no competition between caches. Diverging from the embedded-processes model and the multiple-component model, the CEO model distinctively captures the discrete storage of objects and events. Below, we will discuss the implications of the components embedded in this model.</p></sec><sec disp-level="2"><title>Beyond event perception and event memory: Event in WM</title><p>The present study provides fresh insights by revealing the existence of a distinct storage space and neural mechanisms dedicated to the representation of events in WM, thereby addressing a crucial gap in our understanding of how the brain processes events. While prior research has predominantly focused on event processing within the domains of perception and episodic memory (<xref rid="R19" ref-type="bibr"><italic toggle="yes">19</italic></xref>, <xref rid="R22" ref-type="bibr"><italic toggle="yes">22</italic></xref>, <xref rid="R26" ref-type="bibr"><italic toggle="yes">26</italic></xref>), the investigation conducted here extends beyond these boundaries. Event perception research has delved into the segmentation of continuous experiences into discrete events, as well as the characterization of these events (<xref rid="R20" ref-type="bibr"><italic toggle="yes">20</italic></xref>, <xref rid="R42" ref-type="bibr"><italic toggle="yes">42</italic></xref>). In parallel, studies concerning event memory have probed the formation, consolidation, and subsequent retrieval of event memory in the brain (<xref rid="R43" ref-type="bibr"><italic toggle="yes">43</italic></xref>, <xref rid="R44" ref-type="bibr"><italic toggle="yes">44</italic></xref>). Notably, the interplay between event segmentation and event memory has been subject to exploration (<xref rid="R22" ref-type="bibr"><italic toggle="yes">22</italic></xref>, <xref rid="R26" ref-type="bibr"><italic toggle="yes">26</italic></xref>). Intriguingly, these inquiries suggest that WM acts as a temporary repository for the products of event segmentation derived from perception, which eventually traverse into episodic memory and undergo intricate processing orchestrated by the hippocampus and prefrontal cortex (<xref rid="R22" ref-type="bibr"><italic toggle="yes">22</italic></xref>). Existing event-related investigations have alluded to the retention of segmented events within a multimodal episodic buffer (<xref rid="R19" ref-type="bibr"><italic toggle="yes">19</italic></xref>). Nevertheless, despite these insights, the precise nature of the storage location for these postsegmentation products within WM remains uncharted territory, marking a notable gap in the existing literature.</p><p>Our findings suggest that event cache is responsible for the storage space for the product of event segmentation. Although all stimuli were visually presented in the tasks, we found that visual events had an independent storage space in WM in addition to object cache. Unexpectedly, our investigation did not yield supportive evidence for the presence of an independent episodic buffer in WM. This underscores the likelihood of visual events finding their storage within event cache in WM. It is important to note that our current inquiry represents an initial stride toward unveiling a dedicated buffer for event representation in WM.</p></sec><sec disp-level="2"><title>Implications of event cache as an independent psychological construct of WM</title><p>The establishment of an independent event component in WM offers critical insights into the development of WM theories. First, while the identification of an event cache supports Baddeley&#x02019;s speculation (<xref rid="R2" ref-type="bibr"><italic toggle="yes">2</italic></xref>) that there might be an independent store holding human movements in visual WM, we should not grant human movements a special status in WM. Instead, BM and NBM share one buffer, and we need to take human movements as a representative of event representation, which is distinct from object and related features. Although we have known much about WM mechanisms of objects in the past 40 years, the event representation has long been overlooked in the exploration of WM studies. Our study strongly suggests the necessity and importance of extending the scope of WM exploration from objects to events. For example, it remains unclear how events are rehearsed in WM and how events in WM guide external attention.</p><p>Second, integrating event cache into the model of WM offers a representation-based explanation to a set of previous empirical findings that human movements are stored independently from colors, shapes, locations, and binding representations (<xref rid="R45" ref-type="bibr"><italic toggle="yes">45</italic></xref>&#x02013;<xref rid="R47" ref-type="bibr"><italic toggle="yes">47</italic></xref>). Particularly, previous WM studies exploring the storage of human movement have implicitly or explicitly taken human movement as a special stimulus containing rich social information in the environment or as a special form of spatiotemporal information. They argue that human movements own an independent social WM buffer (<xref rid="R32" ref-type="bibr"><italic toggle="yes">32</italic></xref>, <xref rid="R48" ref-type="bibr"><italic toggle="yes">48</italic></xref>) or correspond to a core knowledge system in visual WM (<xref rid="R46" ref-type="bibr"><italic toggle="yes">46</italic></xref>). The current study did not support either view. Our comprehensive evidence from CFA, multimodal neuroimaging, and psychophysical experiments collectively demonstrates that human movements share the same event cache with NBMs, which have a lack of social information, yet were stored independently from human postures.</p><p>Third, the differentiation between events and motions suggests that motion features alone are not sufficient to define an event. The MOT task, in which the spatial location of objects needs to be dynamically updated in WM, was found to load on object cache, which aligns with a previous study (<xref rid="R29" ref-type="bibr"><italic toggle="yes">29</italic></xref>). When dealing with moving stimuli that lack clear beginnings and endings, participants did not categorize them as events. They were able to hold both event stimuli (such as human movements or RecMoves) and transparent motion stimuli in WM without capacity competition (experiments 2 to 5). However, there was storage competition between static posture stimuli and transparent motion stimuli (experiment S1). Collectively, these findings suggest that the embedded perceived coherence over time is more critical for defining an event.</p><p>Last, event cache may be a hierarchical construct consisting of two subcomponents: BM and NBM. This further dissociation is essentially congruent with recent findings that only WM of BM has an intimate relationship with empathy and theory of mind (<xref rid="R32" ref-type="bibr"><italic toggle="yes">32</italic></xref>, <xref rid="R33" ref-type="bibr"><italic toggle="yes">33</italic></xref>) and exhibited certain distinct neural substrates from NBM (<xref rid="R49" ref-type="bibr"><italic toggle="yes">49</italic></xref>, <xref rid="R50" ref-type="bibr"><italic toggle="yes">50</italic></xref>). From this perspective, the previous claim of social WM indexed by human movements tapped a subcomponent (or social component) of event cache.</p></sec><sec disp-level="2"><title>Both visual features and bindings are retained in object cache</title><p>Our best model supports the existence of an object cache that is mainly responsible for the storage of visual objects and their constituent features. In contrast to our object cache, Baddeley&#x02019;s model suggests that the information of bindings and their constituent features are stored in episodic buffer and visuospatial sketchpad, respectively. The episodic buffer was initially assumed to actively bind information from different sources into one episode with the help of CE (<xref rid="R14" ref-type="bibr"><italic toggle="yes">14</italic></xref>). However, few studies have directly examined whether an independent component exists to handle binding (<xref rid="R51" ref-type="bibr"><italic toggle="yes">51</italic></xref>, <xref rid="R52" ref-type="bibr"><italic toggle="yes">52</italic></xref>). Our findings suggest that although the to-be-remembered representations in the binding and feature tasks are different, they are processed by the same object cache and hence may share similar mechanisms. Recent empirical findings suggest that object-based attention plays a key role in retaining both bindings and single-featured objects in WM (<xref rid="R35" ref-type="bibr"><italic toggle="yes">35</italic></xref>, <xref rid="R53" ref-type="bibr"><italic toggle="yes">53</italic></xref>). Therefore, we argue that episodic buffer as an independent buffer for handling bindings should be reconsidered. Note that although color-letter binding theoretically tapped the binding between visual and verbal codes, participants may process the letter both visually and verbally in a short maintenance period (<xref rid="R54" ref-type="bibr"><italic toggle="yes">54</italic></xref>); hence, color-letter bindings in our setting have visual-related representations. Because all bindings in our study contained visual information, future studies need to test the generalization of the current finding using bindings of other modalities (e.g., auditory).</p></sec><sec disp-level="2"><title>Event cache, object cache, and CE have distinct neural correlates</title><p>The predictions of WM components from resting-state fMRI data suggest that the event cache has a unique neural signature, with the cerebellar network playing a critical role. Traditionally, the cerebellum is considered important for motor coordination and learning (<xref rid="R55" ref-type="bibr"><italic toggle="yes">55</italic></xref>, <xref rid="R56" ref-type="bibr"><italic toggle="yes">56</italic></xref>). However, accumulating evidence supports its involvement in various high-level cognitive functions, including WM (<xref rid="R57" ref-type="bibr"><italic toggle="yes">57</italic></xref>). Evidence from lesion studies has shown that deficits of cerebellum regions or inhibition of cerebellar neuronal excitability significantly reduces the performance of both verbal and visual WM (<xref rid="R58" ref-type="bibr"><italic toggle="yes">58</italic></xref>&#x02013;<xref rid="R60" ref-type="bibr"><italic toggle="yes">60</italic></xref>). Our task fMRI data showed that the left Crus I of the cerebellum displayed significant activation in response to both BM and NBM. Notably, a positive correlation between Crus I activation and behavioral performance was found in event storage tasks but not in object storage tasks, highlighting its specific role in event WM. Lesion studies have shown that patients with cerebellar damage struggle to detect object movement sequences (a type of event) (<xref rid="R61" ref-type="bibr"><italic toggle="yes">61</italic></xref>, <xref rid="R62" ref-type="bibr"><italic toggle="yes">62</italic></xref>), and the posterior part (especially in Crus I and II) was found to be closely associated with the ability to reconstruct action sequences in a chronological order (<xref rid="R62" ref-type="bibr"><italic toggle="yes">62</italic></xref>, <xref rid="R63" ref-type="bibr"><italic toggle="yes">63</italic></xref>). According to a hierarchical processing perspective of WM (<xref rid="R17" ref-type="bibr"><italic toggle="yes">17</italic></xref>, <xref rid="R64" ref-type="bibr"><italic toggle="yes">64</italic></xref>, <xref rid="R65" ref-type="bibr"><italic toggle="yes">65</italic></xref>), it could be speculated that the cerebellum is involved in constructing high-order representations (referred to as events in our study) of what happened to person(s) or object(s) from dynamic stimuli. Our MVPA results demonstrated that the activation of this cerebellum region not only differentiates event WM from object WM but also can track event WM load. Overall, these convergent empirical neural imaging findings indicate that the cerebellum Crus I may serve as one of the primary neural underpinnings of event cache.</p><p>While the left Crus I was examined in detail in the current study, we also recognize that the left Crus I does not function alone to process events. Instead, our data show that the cerebellar connectivity with networks such as the default mode and visual networks also contributes to event cache prediction. This suggests that when individuals conceptualize movements as personally meaningful events, an integration between visual input and intrinsic self-referential processing mechanisms was also required. The primary objective of our task-based fMRI analysis was to uncover the distinct neural bases of event cache and object cache, providing evidence that event cache should be considered as an independent component from object cache. Future studies could build on our findings by using whole-brain or search-light methods to investigate the involvement of broader brain regions in the processing of event WM, thus extending our understanding of the neural basis of event WM.</p><p>The neural correlates of CE and object cache revealed by the prediction analysis in the current study are highly consistent with previous studies. Our finding that no single brain network dominates object maintenance aligns with the distributed nature of the cortical regions involved in the maintenance of object features (<xref rid="R17" ref-type="bibr"><italic toggle="yes">17</italic></xref>). As for the CE component, we found that the frontoparietal network and default mode network contributed most to its prediction, with their connectivity also playing a significant role. These results align with previous studies showing the frontoparietal network as the core substrate for tasks requiring executive control (<xref rid="R65" ref-type="bibr"><italic toggle="yes">65</italic></xref>, <xref rid="R66" ref-type="bibr"><italic toggle="yes">66</italic></xref>) and the strength of the functional interaction between these networks correlating with performance on the <italic toggle="yes">N</italic>-back task (<xref rid="R67" ref-type="bibr"><italic toggle="yes">67</italic></xref>), suggesting that the resting-state connectivity&#x02013;based prediction method can recapture these neural pathways reported by studies that only examined a few sets of connections. These results demonstrate the feasibility of identifying important neural correlates of the event cache and object cache components using the same approach.</p></sec><sec disp-level="2"><title>Limitations and future directions</title><p>Recognizing the inherent limitations of a single study in covering all event materials, our research builds on the evolutionary history of the event concept and related studies (<xref rid="R18" ref-type="bibr"><italic toggle="yes">18</italic></xref>, <xref rid="R19" ref-type="bibr"><italic toggle="yes">19</italic></xref>, <xref rid="R22" ref-type="bibr"><italic toggle="yes">22</italic></xref>). We selected BM and NBM&#x02014;two of the most commonly used materials in event research&#x02014;to investigate the independent storage of events in WM. Our findings provide preliminary evidence for distinct storage spaces for events in WM. However, these results should be considered preliminary because of the limited scope of event materials used, leaving the broader generalizability of the proposed event cache and its functional sensitivity to be explored in future research.</p><p>For instance, while our study suggests the presence of a single event cache for various event types (e.g., visual, verbal, and multimodal), the possibility of modality-specific buffers cannot be ruled out. Given the potential relevance of a multimodal event cache, future research could examine how WM handles different types of stimuli beyond the visual modality, such as auditory or verbal events. Furthermore, our study exclusively focused on events perceived from external stimuli. We did not investigate events based on internal mental simulations, such as imagining an event triggered by an auditory cue. Future studies should explore this type of event to better understand how WM processes both externally perceived and internally generated events, which may involve distinct cognitive mechanisms and neural substrates. In addition, our sample consisted of relatively homogeneous young adult participants. The generalizability of our findings across different age groups should be further tested.</p><p>It is worth noting that our use of fine-grained tokens for both events and objects aligns with previous studies on the storage spaces of visual objects (<xref rid="R4" ref-type="bibr"><italic toggle="yes">4</italic></xref>, <xref rid="R15" ref-type="bibr"><italic toggle="yes">15</italic></xref>, <xref rid="R46" ref-type="bibr"><italic toggle="yes">46</italic></xref>, <xref rid="R51" ref-type="bibr"><italic toggle="yes">51</italic></xref>, <xref rid="R52" ref-type="bibr"><italic toggle="yes">52</italic></xref>). However, event segmentation studies have unveiled a hierarchical relationship between coarser-grained and finer-grained events, with multiple fine-grained events collectively constituting a coarse-grained event (<xref rid="R20" ref-type="bibr"><italic toggle="yes">20</italic></xref>, <xref rid="R22" ref-type="bibr"><italic toggle="yes">22</italic></xref>, <xref rid="R42" ref-type="bibr"><italic toggle="yes">42</italic></xref>). Future research should investigate how WM processes both coarse-grained and fine-grained events, especially considering that the encoding of such events may involve distinct cognitive strategies. In addition, we used a change detection paradigm, which is commonly used in object cache explorations but differs from the paradigms traditionally used in event research. While this approach has its strengths, future studies should examine whether the storage of events in WM is modulated by task type, as different tasks might tap into different aspects of WM storage and retrieval.</p><p>In summary, the event cache proposed in our study paves the way for exploring the multifaceted nature of WM. Further research addressing these limitations and exploring the functional specificity of event storage across populations and modalities will advance our understanding of cognitive flexibility and capacity in various contexts.</p></sec></sec><sec sec-type="materials|methods" disp-level="1"><title>MATERIALS AND METHODS</title><sec disp-level="2"><title>Participants</title><p>A total of 208 participants (150 females and 58 males) was recruited from the Zhejiang University and Hangzhou Normal University for the CFA. Two participants were excluded because of incomplete data. The final sample that completed behavioral tests (including 7 questionnaires and 14 WM tasks) consisted of 206 participants (148 females and 58 males) aged 17 to 25 years (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;20.4, <italic toggle="yes">SD</italic>&#x000a0;=&#x000a0;1.4).</p><p>For the psychophysical experiments, 120 participants were recruited through online advertisement [experiment 1, 12 females and 8 males aged 18 to 26 years (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;21.4, <italic toggle="yes">SD</italic>&#x000a0;=&#x000a0;2.4); experiment 2, 14 females and 6 males aged 18 to 24 years (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;21.3, <italic toggle="yes">SD</italic>&#x000a0;=&#x000a0;1.7); experiment 3, 14 females and 6 males aged 18 to 22 years (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;19.5, SD&#x000a0;=&#x000a0;1.2); experiment 4, 10 females and 10 males aged 18 to 29 years (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;21.9, <italic toggle="yes">SD</italic>&#x000a0;=&#x000a0;3.0); experiment 5, 16 females and 4 males aged 18 to 48 years (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;22.7, <italic toggle="yes">SD</italic>&#x000a0;=&#x000a0;6.3); experiment 6, 8 females and 12 males aged 17 to 27 years (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;21.7, <italic toggle="yes">SD</italic>&#x000a0;=&#x000a0;2.7)]. The sample size of the psychophysical experiments was determined on the basis of the sequential Bayes factor design (<xref rid="R34" ref-type="bibr"><italic toggle="yes">34</italic></xref>). The Bayes factor (<italic toggle="yes">BF</italic><sub>10</sub>) value indicates the ratio of the likelihood of supporting an alternative versus a null hypothesis. Following the sequential Bayes factor design, the <italic toggle="yes">BF</italic><sub>10</sub> was repeatedly calculated after reaching the minimum sample size. If the predetermined evidence threshold was not reached, more participants were recruited until the Bayes factors reached this threshold. The minimum sample size was set to 20 before data collection. The threshold for stopping sampling was set to <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;&#x0003e;&#x000a0;3 (moderate evidence for <italic toggle="yes">H</italic><sub>1</sub>) or <italic toggle="yes">BF</italic><sub>10</sub>&#x000a0;&#x0003c;&#x000a0;1/3 (moderate evidence for <italic toggle="yes">H</italic><sub>0</sub>) for the main effect of memory load in the cross-type-manipulation ANOVA, which examined whether the two target types of materials share a common storage in WM.</p><p>For the prediction analysis, 107 of 206 participants underwent MRI experiments, including high-resolution <italic toggle="yes">T</italic><sub>1</sub>-weighted, resting-state fMRI and task fMRI scanning [part of task fMRI data were reported elsewhere (<xref rid="R65" ref-type="bibr"><italic toggle="yes">65</italic></xref>)]. In the resting-state fMRI analysis, four participants were excluded because of excessive head motion (maximum displacement &#x0003e;3 mm or 3&#x000b0;, or &#x0003e;15% of time points with framewise displacement &#x0003e;0.5 mm), leaving 103 participants in the final fMRI sample (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;19.5, <italic toggle="yes">SD</italic>&#x000a0;=&#x000a0;1.3; 61 females and 42 males). For the ROI analysis, a subgroup of 47 participants (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;19.9, <italic toggle="yes">SD</italic>&#x000a0;=&#x000a0;0.8; 29 females and 18 males) among 107 participants was included to perform the event <italic toggle="yes">N</italic>-back task and another 34 participants (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;23.3, <italic toggle="yes">SD</italic>&#x000a0;=&#x000a0;2.6; 16 females and 18 males) were recruited to perform the event change detection task independently [for details of <italic toggle="yes">N</italic>-back and change detection task, please refer to the study of Zhou <italic toggle="yes">et&#x000a0;al.</italic> (<xref rid="R65" ref-type="bibr"><italic toggle="yes">65</italic></xref>)]. For the color-shape binding change detection task, 31 participants were recruited. Four participants were excluded for chance-level performance, two for incomplete fMRI data, and one for excessive head motion (maximum displacement &#x0003e;3 mm or 3&#x000b0;, or &#x0003e;15% of time points with framewise displacement &#x0003e;0.5 mm), resulting in 24 participants (<italic toggle="yes">M</italic>&#x000a0;=&#x000a0;23.56, <italic toggle="yes">SD</italic>&#x000a0;=&#x000a0;2.01; 18 females and 6 males) in the final analysis.</p><p>All participants had normal color vision and normal or corrected-to-normal visual acuity. They provided written informed consent before the experiments and received payment or course credit for their participation. This study was approved by the ethics committee of Zhejiang University [Ethics Approval of Zhejiang University Psychology (2021) no. 051, (2023) no. 081].</p><p>The behavioral experiments (for CFA) lasted ~210 min. To control for potential fatigue effects, we divided the testing into three sessions, which were conducted separately for 3 days. In session 1, participants completed seven questionnaires that were irrelevant to our study and were not reported here. In sessions 2 and 3, they completed a series of WM tasks in a dark lab compartment. Resting-state fMRI data and <italic toggle="yes">N</italic>-back task fMRI data were collected after the participants completed all behavioral tests. An independent group performed an event change detection task in the fMRI scanner [preliminary results have been reported in a previous study (<xref rid="R65" ref-type="bibr"><italic toggle="yes">65</italic></xref>)]. The color-shape binding fMRI task was conducted on a separate group of participants, lasting for 40 min. The three psychophysical experiments, each about 50 min long, were conducted on three other independent samples.</p></sec><sec disp-level="2"><title>WM measurements for CFA</title><p>The stimuli in the 14 WM tasks were generated using MATLAB Psychophysics Toolbox and presented on a black (0, 0, 0) background. Participants sat in a dark room, 60 cm in front of a 17-inch (43.18 cm) cathode ray tube monitor with a resolution of 1024 by 768.</p><p>For storage tasks (event, object, and binding storage), the change detection paradigm was adopted, and the number of memory items was set to two, four, and six (for the location storage task, it was four, six, and eight). There were 26 trials under each memory load and 78 trials in each task. Participants could take a break every 26 trials. An eight-trial practice preceded the formal experiment.</p><p>To tap CE, we selected four tasks, including the <italic toggle="yes">N</italic>-back, anti-saccade, Ospan, and Sspan tasks. Both the <italic toggle="yes">N</italic>-back and anti-saccade tasks are well-established measures of CE, encompassing updating and inhibition (<xref rid="R68" ref-type="bibr"><italic toggle="yes">68</italic></xref>, <xref rid="R69" ref-type="bibr"><italic toggle="yes">69</italic></xref>). Specifically, the <italic toggle="yes">N</italic>-back task serves to capture the updating component of WM. When the value of <italic toggle="yes">N</italic> is 2 or greater, simply keeping recently presented items in memory is not sufficient to complete the task. Continuous updates to the WM buffer are required to track the current stimulus and facilitate comparisons. In contrast, to complete the anti-saccade task, participants need to inhibit the automatic eye movement toward the cue side of the screen and redirect their gaze in the opposite direction to locate and identify the target. This task taps proponent response inhibition (<xref rid="R70" ref-type="bibr"><italic toggle="yes">70</italic></xref>). The complex span tasks, including Ospan and Sspan, tap both the storage and CE (<xref rid="R4" ref-type="bibr"><italic toggle="yes">4</italic></xref>), and there are studies suggesting that attention control primarily underlies the performance on the complex span tasks (<xref rid="R28" ref-type="bibr"><italic toggle="yes">28</italic></xref>, <xref rid="R71" ref-type="bibr"><italic toggle="yes">71</italic></xref>). The CFA method was used to extract the common variance shared by these four tasks, which mainly reflected CE.</p><sec disp-level="3"><title>
Event storage tasks
</title><p>Event storage tasks included BM and NBM tasks. Following previous studies, we adopted PLDs (<xref rid="R47" ref-type="bibr"><italic toggle="yes">47</italic></xref>) and solid agents (<xref rid="R45" ref-type="bibr"><italic toggle="yes">45</italic></xref>) as two types of BM stimuli and rectangular and circular movements (<xref rid="R33" ref-type="bibr"><italic toggle="yes">33</italic></xref>, <xref rid="R49" ref-type="bibr"><italic toggle="yes">49</italic></xref>) as two types of NBM stimuli.</p><sec disp-level="4"><title>
PLD BM
</title><p>For each PLD movement, 13 light points were placed at the distinct joints of a moving human body to form a coherent and meaningful movement. We selected nine movements from the database of Vanrie and Verfaillie (<xref rid="R72" ref-type="bibr"><italic toggle="yes">72</italic></xref>): cycling, jumping, painting, spading, walking, waving, chopping, paddling, and saluting (see fig. S4B). Every animation consisted of 30 distinct frames, with each frame displayed twice, leading to a 1-s PLD (refresh rate, 60 Hz). Each stimulus subtended a visual angle of ~1.43&#x000b0; (width) by 3.62&#x000b0; (height). The spatial locations of the PLDs were randomly selected from eight evenly distributed spots on an invisible circle with a radius of 4.88&#x000b0; from the screen center.</p><p>The procedure of a single trial is shown in fig. S4A. Each trial began with the word &#x0201c;Coca-Cola&#x0201d; (in Chinese) displayed for 500 ms, prompting participants to rehearse it aloud during the whole task. This was to prevent the participants from verbally coding the BM. After an interval of 150 to 350 ms, the memory array appeared for 4 s. Participants were required to remember these stimuli. After a 900-ms retention interval, a red BM was presented at the screen center. Participants were required to determine within 3 s whether the red BM had appeared in the memory array. They pressed the &#x0201c;J&#x0201d; key if it had appeared in the memory array and pressed the &#x0201c;F&#x0201d; key if it had not. The probed red BM remained the same in 50% of the trials and altered to a different action in the remaining 50% of the trials. After the response, there was an interval of 500 to 700 ms between trials.</p></sec><sec disp-level="4"><title>
SolidBM
</title><p>Solid agents were used as the other type of BM stimulus. In this task, BMs were presented in the form of solid agents, which were created using Poser software. We selected nine movements: arm-moving, stretching, jumping, squatting, walking, waving, saluting, turning, and bowing (see fig. S4C). The other settings were the same as those in the PLD BM task.</p></sec><sec disp-level="4"><title>
Rectangular movement (RecMove)
</title><p>The movements of the rectangles were created using a 12-dotted PLD (<xref rid="R33" ref-type="bibr"><italic toggle="yes">33</italic></xref>). In line with the PLD BM stimuli, the dotted rectangle showed nine distinct movements (see fig. S4D). (i) The left and right sides moved downward by 60&#x000b0; relative to their vertical positions and then returned. (ii) The left side moved upward by 45&#x000b0; relative to its vertical position and then returned, while the right half moved upward by 45&#x000b0; relative to its vertical position and then returned. (iii) The left and right halves moved downward by 90&#x000b0; relative to their vertical positions and then returned, while the top side rotated around the middle dot once. (iv) The top side moved upward by 90&#x000b0; relative to its horizontal position and then returned. (v) The top half of the rectangle rotated 90&#x000b0; clockwise relative to its vertical position, while the left bottom half moved upward by 90&#x000b0;. (vi) The top side rotated around the top dot on the right side by 180&#x000b0;, while both the left and bottom sides moved downward by 90&#x000b0;. (vii) The right side moved downward by 45&#x000b0; and then returned. (viii) The left side rotated 180&#x000b0; clockwise around the middle dot, while the right side moved downward by 90&#x000b0; relative to its vertical position. (ix) The top side moved downward by 90&#x000b0; relative to its horizontal position and then returned, while the right side moved 180&#x000b0; clockwise relative to its horizontal position and then returned. The other settings were the same as those in the PLD BM task.</p></sec><sec disp-level="4"><title>
Circular movement (CirMove)
</title><p>We used 13-dotted PLDs of circular movement, containing nine distinct movements (<xref rid="R47" ref-type="bibr"><italic toggle="yes">47</italic></xref>): moving up, moving down, rolling right, rolling left, shrinking, inflating, moving diagonally, splitting horizontally, and splitting vertically (see fig. S4E). The initial radius of each circle is ~1.15&#x000b0;. Each movement lasted for 1 s. The other settings were the same as those in the PLD BM task.</p></sec></sec><sec disp-level="3"><title>
Object storage tasks
</title><sec disp-level="4"><title>
Color
</title><p>For distinct colors (fig. S4F), we used white (255, 255, 255, in RGB value), yellow (255, 255, 0), lime (0, 255, 0), gray (128, 128, 128), light pink (255, 178, 193), aqua (0, 255, 255), blue (0, 0, 255), red (255, 0, 0), and magenta (255, 0, 255). The spatial locations of the colored squares (1.1&#x000b0; by 1.1&#x000b0;) were distributed at eight spots on an invisible circle with a radius of 4&#x000b0; from the screen center. After a 300- to 400-ms interval, the memory array was displayed for 200 ms. The other settings were the same as those in the PLD BM task.</p></sec><sec disp-level="4"><title>
Shape
</title><p>We replaced the nine colored squares with nine distinct shapes (see fig. S4G; 1.6&#x000b0; by 1.6&#x000b0;). The other aspects were the same as those of the color WM task.</p></sec><sec disp-level="4"><title>
Location
</title><p>Participants were required to remember the locations of four, six, or eight white squares (0.2&#x000b0; by 0.2&#x000b0;) to avoid the ceiling effect. They judged whether the probe (red square) appeared at one of the memorized locations. The stimuli appeared inside an invisible circle with a radius of 5&#x000b0; from the screen center. These squares were evenly distributed in the four quadrants and did not overlap. A fixation (&#x0201c;+&#x0201d;) remained at the screen center from the blank interval after the Coca-Cola presentation until the end of the detection phase. The other settings were the same as those in the color task.</p></sec></sec><sec disp-level="3"><title>
Binding tasks
</title><p>Binding tasks were designed to assess WM capacity when two different features were presented within the visuospatial domains (color-location binding) or across the verbal and visual WM domains (color-letter binding). Participants had to hold them together in WM to respond correctly to the task. The probe could be one of the old items in the memory array in 50% of trials. In the remaining trials, the probe was formed by combining the different dimensions of the two items in the memory array.</p><sec disp-level="4"><title>
Color-location binding
</title><p>Participants were required to remember the bindings between the color and location of the 0.8&#x000b0;-by-0.8&#x000b0; squares. The same colors were chosen from the color task, except that light pink was replaced with green (0, 128, 0). The squares appeared for 200 ms in an area of 10&#x000b0; by 10&#x000b0; in the screen center and were nonoverlapping. Subsequently, a probe appeared at a certain location on the screen, and participants were required to report whether its color matched the memory item at the corresponding location. The fixation settings were the same as those used for the location task.</p></sec><sec disp-level="4"><title>
Color-letter binding
</title><p>Participants were required to remember the bindings between color and capital letter (A, B, C, D, E, H, M, J, and K). Each letter subtended a visual angle of ~1.3&#x000b0; by 1.3&#x000b0; and was presented on an invisible circle with a radius of 4&#x000b0; from the screen center for 200 ms. A colored capital letter then appeared at the screen center. Participants were asked to determine whether any memory item matching both letter and color appeared. No fixations were demonstrated during the task.</p></sec></sec><sec disp-level="3"><title>
CE tasks
</title><sec disp-level="4"><title>
Anti-saccade (Anti)
</title><p>This task was adapted from the task used by Unsworth and Spillers (<xref rid="R73" ref-type="bibr"><italic toggle="yes">73</italic></xref>). Each trial started with a fixation point displayed for 200, 600, 1000, 1400, or 1800 ms. A cue was then displayed either to the left or right of the fixation point for 100 ms, followed by a 50-ms interval. This procedure was repeated a second time. After the disappearance of the cue, a target letter (B, P, or R) was presented on the opposite side of the second cue for 100 ms, succeeded by a masking stimulus (an H for 50 ms and then an &#x0201c;8&#x0201d; that remained on screen until a response). Participants were required to identify the target letter (B, P, or R) by pressing a corresponding key (1, 2, or 3). There were 15 practice and 40 formal trials. The proportion of correct responses was then recorded.</p></sec><sec disp-level="4"><title>N<italic toggle="yes">-back</italic></title><p>The task began with a 1000-ms fixation. A sequence of black squares then appeared in the eight outer squares of a three-by-three grid (excluding the middle one). Participants had to decide whether each square&#x02019;s location matched the one showing three items (3-back) before. Each square was presented for 500 ms, with interstimulus fixations of 2000 ms. Participants pressed &#x0201c;F&#x0201d; for a match and &#x0201c;J&#x0201d; otherwise. The task included two 40-trial blocks, preceded by eight practice trials. The proportion of correct responses was then recorded.</p></sec></sec><sec disp-level="3"><title>
Other WM tasks
</title><sec disp-level="4"><title>
Operation span (Ospan)
</title><p>Participants remembered a set of consonants (F, H, J, K, L, N, P, Q, R, S, T, and Y) while solving arithmetic problems. Three practice sessions preceded the formal trials to familiarize participants with the procedure. In the first session, a simple letter-span task required participants to recall presented consonants in the same order. Each letter was presented in the screen center for 1000 ms. During recall, a 4- by 3-letter matrix was displayed, and participants had to select the letters in the correct order by clicking the box beside the letter. In the second session, the participants were asked to solve 15 arithmetic problems [e.g., (3&#x000a0;&#x000d7;&#x000a0;2)&#x000a0;+&#x000a0;6&#x000a0;= ?]. The processing time for each problem was recorded, and the program calculated the mean and SD of the time each participant took after completion. The third session combined the simple span and arithmetic tasks. A mathematical operation was presented first, followed by a letter. The time limit for each operation was fixed to the average time plus 2.5 SD obtained in the second session so that participants had little time to rehearse the letters. After all three practice sessions, the program proceeded to the formal trials, which were similar to the trials in the third practice session. The list length varied randomly from four to eight letters. Three trials were performed for each set size, resulting in a total of 15 trials. The dependent variable was the number of correct lists that recalled all letters in the correct order. Participants repeated the task if the overall accuracy of the math portion was lower than 80%.</p></sec><sec disp-level="4"><title>
Symmetry span (Sspan)
</title><p>Participants had to remember the location of sequences of red squares (1.1&#x000b0; by 1.1&#x000b0;) presented within a 3-by-3 matrix while performing a symmetry judgment task. In line with Ospan, the task started with three practice sessions: two trials of the simple square-span task, 15 symmetry judgment tasks, and three trials of the combined task. In the formal trials, participants first saw a black-and-white 8-by-8 grid pattern and decided whether it was symmetrical along the vertical axis. The time limit was calculated in the same manner as for Ospan. Then, participants were presented with a 4-by-4 matrix with one of the cells filled in red for 650 ms. During recall, a blank 4-by-4 matrix appeared, and participants had to recall the red squares in the same order as they were presented by clicking on the corresponding locations. There were three trials for each set size with a list length ranging from three to six. The same scoring procedure was used as with the Ospan task.</p></sec><sec disp-level="4"><title>
Multiple-object tracking (MOT)
</title><p>Twelve white disks with a viewing angle of 0.25&#x000b0; were presented in an area of 8.5&#x000b0; by 8.5&#x000b0; at the screen center. Disk movement had three stages: marking, tracking, and detection. The marking phase lasted for 3000 ms, during which all disks remained still. However, two, four, or six disks turned red and solid for 2500 ms, prompting the participants to track the position of these disks. Subsequently, all disks became hollow and appeared for 500 ms. The tracking phase lasted for 4500 ms. All discs moved in random directions at speeds ranging from &#x02212;0.00058&#x000b0;/ms to 0.0064&#x000b0;/ms, and they did not overlap during the movement. The detection phase lasted for a maximum of 1500 ms. All discs stopped moving, and one of the discs turned solid green. Participants were required to determine whether the disc was one of several discs marked in the marking phase. There were 26 trials under each tracking load and 78 trials in total. This task did not require verbal suppression, and the other aspects were the same as those in the PLD BM task.</p></sec></sec></sec><sec disp-level="2"><title>Latent variable modeling</title><p>For WM storage tasks, the WM capacity for each type of stimulus was estimated using Cowan&#x02019;s formula (<xref rid="R30" ref-type="bibr"><italic toggle="yes">30</italic></xref>): <italic toggle="yes">K</italic> = <italic toggle="yes">S</italic> &#x000d7; (<italic toggle="yes">H</italic>&#x000a0;&#x02212;&#x000a0;<italic toggle="yes">F</italic>), where <italic toggle="yes">K</italic> is the WM capacity, <italic toggle="yes">S</italic> is the number of to-be-memorized stimuli, <italic toggle="yes">H</italic> is the hit rate, and <italic toggle="yes">F</italic> is the false alarm rate. We calculated <italic toggle="yes">K</italic> for each set size for each participant and considered <italic toggle="yes">K</italic><sub>max</sub> among the three load conditions as one&#x02019;s WM capacity (<xref rid="R47" ref-type="bibr"><italic toggle="yes">47</italic></xref>, <xref rid="R49" ref-type="bibr"><italic toggle="yes">49</italic></xref>). For all tasks, univariate outliers were defined as individual scores exceeding 3 SDs from the respective grand mean. Of 2884 observations, 5 met this criterion and were replaced with corresponding cutoff values (<italic toggle="yes">M</italic>&#x000a0;&#x000b1;&#x000a0;3 SD). For the MOT task, we used the formula of Scholl <italic toggle="yes">et&#x000a0;al.</italic> (<xref rid="R31" ref-type="bibr"><italic toggle="yes">31</italic></xref>) to derive the effective number of objects tracked: <italic toggle="yes">K</italic>&#x000a0;=&#x000a0;<italic toggle="yes">S</italic> &#x000d7; (2<italic toggle="yes">P</italic>&#x000a0;&#x02212;&#x000a0;1), where <italic toggle="yes">K</italic> is the effective number of objects tracked, <italic toggle="yes">S</italic> is the number of targets, and <italic toggle="yes">P</italic> is the tracked accuracy under a tracked-load condition.</p><p>All structural equation models were estimated using Mplus 8 (<ext-link xlink:href="http://www.statmodel.com/" ext-link-type="uri">www.statmodel.com/</ext-link>). The robust maximum likelihood estimation method that has been developed for nonnormal data was used in modeling estimation (<xref rid="R74" ref-type="bibr"><italic toggle="yes">74</italic></xref>). The evaluation of the fit statistics was based on the criteria recommended by Kline (<xref rid="R75" ref-type="bibr"><italic toggle="yes">75</italic></xref>) and DiStefano (<xref rid="R76" ref-type="bibr"><italic toggle="yes">76</italic></xref>). Specifically, the fit of a model was considered good (or acceptable) if normed &#x003c7;<sup>2</sup> (&#x003c7;<sup>2</sup>/<italic toggle="yes">df</italic>)&#x000a0;&#x02264;&#x000a0;2 (3), root mean square error of approximation (RMSEA) &#x02264;0.05 (0.08), standardized root mean square residual (SRMR) &#x02264;0.05 (0.10), and CFI&#x000a0;&#x02265;&#x000a0;0.95 (0.90). We made model comparisons by considering changes in CFI and AIC. A difference of 0.01 or larger in CFI was considered a substantial difference (<xref rid="R77" ref-type="bibr"><italic toggle="yes">77</italic></xref>). AIC was used to compare nonnested models, in which a model with a smaller AIC was preferred (<xref rid="R76" ref-type="bibr"><italic toggle="yes">76</italic></xref>). The chi-square (&#x003c7;<sup>2</sup>) difference test was used to compare nested models (<xref rid="R74" ref-type="bibr"><italic toggle="yes">74</italic></xref>).</p></sec><sec disp-level="2"><title>Psychophysical experiments</title><sec disp-level="3"><title>
Apparatus and stimuli
</title><p>The experiments were implemented using MATLAB and Psychophysics Toolbox. Stimuli were presented against a gray (128, 128, 128, RGB) background on a 17-inch (43.18 cm) cathode ray tube monitor with a resolution of 1024 by 768 pixels at a 60-Hz refresh rate. Participants were seated in a dark room ~60 cm from the screen. The fixation point subtended a 0.27&#x000b0;-by-0.27&#x000b0; visual angle. Both the memory items and probe items were presented at the screen center.</p><sec disp-level="4"><title>
BM
</title><p>The first seven movements from the PLD BM task were selected. Each action consisted of 15 frames, with each frame displayed twice in succession, leading to a 500-ms duration. In experiment 1, we connected the dots with white line segments to build line-formed BM, where the line segments represent the structure of the human skeleton. The remaining settings remained consistent with those used in the previously mentioned PLD BM task for CFA.</p></sec><sec disp-level="4"><title>
RecMove
</title><p>The first seven movements from the RecMove task were selected. Each comprised 15 frames and was presented for 500 ms. The other settings remained consistent with those in the RecMove task for CFA.</p></sec><sec disp-level="4"><title>
Human posture
</title><p>Seven static body postures were created through Poser software: waving, saluting, pointing, squatting, rejecting, bowing, and stretching (see fig. S6). Each had an approximate size of 2.5&#x000b0; (width) by 4.0&#x000b0; (height). Four of these postures were chosen from the actions in the preceding solid BM task, while the remaining three were additional postures. This selection ensures that each posture has a clear meaning and is distinguishable from each other.</p></sec><sec disp-level="4"><title>
Transparent motion
</title><p>The task in experiments 2 and 3 was adapted from the task used by Valdes-Sosa <italic toggle="yes">et&#x000a0;al.</italic> (<xref rid="R78" ref-type="bibr"><italic toggle="yes">78</italic></xref>). The moving dots consisted of 25 red (255, 0, 0) target dots and 10 white (255, 255, 255) distractor dots. The target dots moved together at 4.2&#x000b0;/s in one of seven fixed directions (0&#x000b0;, 51.4&#x000b0;, 102.9&#x000b0;, 154.3&#x000b0;, 205.7&#x000b0;, 257.1&#x000b0;, or 308.6&#x000b0;), while the distractor dots moved together at the same velocity in a different direction from the seven (38.6&#x000b0;, 90.0&#x000b0;, 141.4&#x000b0;, 192.9&#x000b0;, 244.3&#x000b0;, 295.7&#x000b0;, or 347.1&#x000b0;). All dots were randomly generated within a 4.8&#x000b0;-radius circular area at the visual field center. Once a dot moved beyond the circular area, it reappeared at the corresponding position on the opposite side of the circular area. Participants had to retain the moving direction of target dots. Probe items contained only target dots. In experiment 4 of the velocity of transparent motion task, we used 100% motion saliency memory array to reduce the task difficulty. The moving dots consisted of 35 black (0, 0, 0) target dots, whose velocity was set to 0.3&#x000b0;, 0.6&#x000b0;, 1.2&#x000b0;, 2.4&#x000b0;, 4.8&#x000b0;, 9.6&#x000b0;, or 19.2&#x000b0;/s. In each trial, all dots moved in the same direction, which was randomly selected from the seven directions (0&#x000b0;, 51.4&#x000b0;, 102.9&#x000b0;, 154.3&#x000b0;, 205.7&#x000b0;, 257.1&#x000b0;, or 308.6&#x000b0;).</p></sec><sec disp-level="4"><title>
Colorized BM
</title><p>To strictly control the visual attributes, we used colorized BMs as stimuli in experiment 6. We used the same nine PLD BMs as in the event storage task. For distinct colors, we selected brown (124, 77, 37), yellow (255, 255, 0), lime (0, 255, 0), orange (255, 127, 0), pink (255, 178, 193), aqua (0, 255, 255), blue (0, 0, 255), red (255, 0, 0), and magenta (255, 0, 255).</p></sec></sec><sec disp-level="3"><title>
Design and procedure
</title><p>Participants had to maintain stimuli A and B simultaneously in WM (e.g., in experiment 1, A represents BM and B represents RecMove; for experiment 6, A represents BM and B represents the color of BM). Experiments 1 to 5 consisted of five load conditions (2A, 2B, 2A&#x000a0;+&#x000a0;2B, 2A&#x000a0;+&#x000a0;4B, and 4A&#x000a0;+&#x000a0;2B), and experiment 6 contained three conditions (2A&#x000a0;+&#x000a0;2B, 2A&#x000a0;+&#x000a0;4B, and 4A&#x000a0;+&#x000a0;2B), with numbers denoting item counts. Conditions 2A and 2B served the purpose of adding lower WM load conditions and ensuring good WM performance when holding two stimuli. These actions collectively diminish the overall memory task difficulty, boost participants&#x02019; confidence in handling higher WM loads (retaining four or six stimuli), and hence avert the emergence of floor effects under such demanding conditions. Each condition included 48 trials, resulting in 240 trials for experiments 1 to 5 and 144 trials for experiment 6.</p><p>In Experiments 1 to 5, each trial (see <xref rid="F3" ref-type="fig">Fig. 3A</xref>) began with a 500-ms fixation point in the screen center, followed by a 200-ms blank screen. Then, stimuli were sequentially presented in a random order at the screen center. Each memory item was presented for 500 ms, with a 500-ms interstimulus interval between items. The random presentation ensured that the order of memory item presentation was unpredictable, preventing the formation of chunks in WM. After all memory items, a blank screen of 1500 ms was displayed. Then, a probe item marked with a white frame was presented for 3000 ms. Participants were required to indicate whether this probe had appeared in the memory array by pressing &#x0201c;J&#x0201d; for yes or &#x0201c;F&#x0201d; for no. Except for the conditions where only A or B was present, the detection probability of A and B was 50% each, and the probe had a 50% chance of being in the array. The interval between the two trials varied between 1500 and 2000 ms. Throughout the entire experiment, participants were asked to continuously repeat the phrase &#x0201c;Coca-Cola&#x0201d; to prevent verbal encoding of the memory stimuli.</p><p>In experiment 6, we presented four colorized BMs simultaneously for 3 s and asked the participants to memorize both colors and BMs. Both the presented BMs and colors can be either four different ones or two sets of identical pairs, leading to three memory load conditions: two colors and two BMs, two colors and four BMs, and four colors and two BMs. The probe item is either a black BM or a color square in the screen center, which was detected with equal probability.</p><p>Participants were given a minimum of 20 practice trials. If they achieved an accuracy of 75% or higher, or completed 40 practice trials, they proceeded to the formal experiment. Participants were allowed to take a break every 30 trials. Accuracy instead of reaction time was emphasized.</p></sec><sec disp-level="3"><title>
Analysis
</title><p>First, a 2 (material type: A versus B)&#x02013;by&#x02013;2 (load: 2 and 4) repeated-measures ANOVA (STM_ANOVA) was conducted to investigate the effectiveness of manipulating the memory load of certain memory material, and the load effect was the primary interest to check the effectiveness of load manipulation. Second, a 2 (material type: A versus B)&#x02013;by&#x02013;2 (load: 2 and 4) repeated-measures ANOVA (CTM_ANOVA) was conducted to examine whether the memory performance of the fixed-load material was modulated by the load variations of the other material. To be specific, we compared the performance of A under 2A&#x000a0;+&#x000a0;2B and 2A&#x000a0;+&#x000a0;4B conditions, as well as the performance of B under 2A&#x000a0;+&#x000a0;2B and 4A&#x000a0;+&#x000a0;2B conditions.</p></sec></sec><sec disp-level="2"><title>MRI imaging parameters and preprocessing</title><sec disp-level="3"><title>
Resting-state and event-related task fMRI
</title><p>MRI data for the resting-state and <italic toggle="yes">N</italic>-back tasks were acquired on a 3T Siemens Prisma scanner using a 20-channel coil at the Center for Brain Imaging Science and Technology, Zhejiang University. A <italic toggle="yes">T</italic><sub>2</sub>*-weighted single-shot echo-planar imaging sequence with multiband acceleration (multiband factor, 4) was used.</p><p>The participants were instructed to look at a fixation and remain stationary during the resting-state scanning. The acquisition parameters were as follows: repetition time (TR)/echo time (TE), 1000/34 ms; flip angle, 50&#x000b0;; field of view (FOV), 230 mm by 230 mm; matrix, 92 by 92; voxel size, 2.5 mm by 2.5 mm by 2.5 mm; slice number, 52. High-resolution <italic toggle="yes">T</italic><sub>1</sub>-weighted anatomical images were collected using a <italic toggle="yes">T</italic><sub>1</sub>-weighted three-dimensional magnetization-prepared rapid gradient echo sequence with the following parameters: TR/TE, 2300/2.32 ms; flip angle, 8&#x000b0;; FOV, 240 mm by 240 mm; matrix, 256 by 256; voxel size, 0.94 mm by 0.94 mm by 0.9 mm; 208 slices in the sagittal panel. The resting-state scan lasted for 8 min, a duration that is widely accepted in neuroscience research, with their reliability and validity well documented in numerous studies (<xref rid="R39" ref-type="bibr"><italic toggle="yes">39</italic></xref>, <xref rid="R79" ref-type="bibr"><italic toggle="yes">79</italic></xref>, <xref rid="R80" ref-type="bibr"><italic toggle="yes">80</italic></xref>).</p><p>MRI data for the event change detection task were acquired on two different sites. Please see our previous study for details of collection procedure and preprocessing steps of both the <italic toggle="yes">N</italic>-back and event change detection tasks (<xref rid="R65" ref-type="bibr"><italic toggle="yes">65</italic></xref>).</p></sec><sec disp-level="3"><title>
Object-related task fMRI
</title><p>A color-shape binding change detection task was adopted as an object-related task. During the task, the to-be-memorized colors and shapes were randomly selected from a pool of six distinct values of each type (fig. S13). Each memory array contained two or four color-shape bindings (1.4&#x000b0; by 1.4&#x000b0;), which were constructed by randomly combining distinct values of two dimensions. The positions of the bindings were randomly selected from 45&#x000b0;, 135&#x000b0;, 225&#x000b0;, and 315&#x000b0; clockwise on a virtual circle 1.7&#x000b0; from the screen center. A one-way (memory load: 2-sets versus 4-sets) within-subject design was adopted, and the order of the two loads was counterbalanced across participants. Each load consisted of two runs, and each run had 24 trials, resulting in a total of 96 trials. Each trial began with a black fixation presented at the screen center for 900 ms, followed by the memory array for 100 ms, a blank interval (randomly chosen from 2, 4, 6, and 8 s), and a probe to identify if it was in the memory array (pressing &#x0201c;4&#x0201d; for yes or &#x0201c;1&#x0201d; for no). After pressing the key, the probe did not disappear immediately and remained present until 2&#x000a0;s. If participants did not respond within 2 s, the response for the trial would be recorded as wrong. In half of the trials, the probe was not included in the memory array.</p><p>To mitigate the risk of participants depending solely on memorizing partial bindings to accomplish the task, the introduction of bindings took two different approaches during probe-absent trials. Specifically, in two-thirds of these trials, the binding was generated by randomly combining two features from two distinct bindings within the memory array. In the remaining one-third of probe-absent trials, the binding emerged through the replacement of a feature within an existing binding from the memory array with a previously unused feature. The intertrial interval was randomly chosen from 5, 9, 13, and 17 s with equal probability. Each run lasted 456 s, and the whole experiment lasted 40 min. Before the formal experiment, participants completed a practice session (10 trials) outside the scanner to become familiar with the task.</p><p>MRI data for the color-shape binding change detection task were acquired on a 3T Siemens Prisma scanner using a 20-channel coil at the Center for Brain Imaging Science and Technology, Zhejiang University. A <italic toggle="yes">T</italic><sub>2</sub>*-weighted single-shot echo-planar imaging sequence with multiband acceleration (multiband factor, 4) was used. The acquisition parameters were as follows: TR/TE, 1000/34 ms; flip angle, 62&#x000b0;; FOV, 230 mm by 230 mm; matrix, =&#x000a0;92 by 92; voxel size, =&#x000a0;2.5 mm by 2.5 mm by 2.5 mm; slice number, 52. High-resolution <italic toggle="yes">T</italic><sub>1</sub>-weighted anatomical images were collected using a <italic toggle="yes">T</italic><sub>1</sub>-weighted three-dimensional magnetization-prepared rapid gradient echo sequence with the following parameters: TR/TE, 2300/2.32 ms; flip angle, 8&#x000b0;; FOV, 240 mm by 240 mm; matrix, 256&#x000a0;&#x000d7;&#x000a0;256; voxel size, 0.94 mm by 0.94 mm by 0.9 mm; 192 slices in the sagittal panel.</p></sec><sec disp-level="3"><title>
Preprocessing of resting-state and object-related task fMRI data
</title><p>Resting-state and color-shape binding task fMRI data were preprocessed using AFNI (<ext-link xlink:href="https://afni.nimh.nih.gov/" ext-link-type="uri">https://afni.nimh.nih.gov/</ext-link>), ANTs (<ext-link xlink:href="http://stnava.github.io/ANTs/" ext-link-type="uri">http://stnava.github.io/ANTs/</ext-link>), SPM12 (<ext-link xlink:href="http://www.fil.ion.ucl.ac.uk/spm/" ext-link-type="uri">www.fil.ion.ucl.ac.uk/spm/</ext-link>), and DPABI (<ext-link xlink:href="https://rfmri.org/DPABI" ext-link-type="uri">https://rfmri.org/DPABI</ext-link>). The first five volumes of the resting-state fMRI data were discarded to allow for signal equilibrium, and the remaining volumes were first corrected for slice acquisition timing and then for head motion using rigid-body transformation. Spatial smoothing was conducted with a 5-mm full-width-at-half-maximum Gaussian kernel after spatial normalization. To further control for head motion influence, the six motion parameters and their temporal derivatives, as well as the first five principal components extracted from the white matter and CSF regions, were regressed out from the smoothed data, and last, band pass filtering (0.01 to 0.1 Hz) was applied.</p></sec></sec><sec disp-level="2"><title>Brain-behavior prediction analyses</title><p>We calculated the scores of all latent variables to investigate the neural correlates underlying different WM components. We first transformed the scores (accuracies for CE tasks and <italic toggle="yes">K</italic><sub>max</sub>&#x02019;s for storage tasks) of each WM task into <italic toggle="yes">z</italic>-scores. According to the structure of model 14, we then used the average of the <italic toggle="yes">z</italic>-scores of the corresponding tasks to attain the score of each first-order factor. For example, BM&#x000a0;=&#x000a0;0.5 * (<italic toggle="yes">z</italic>PLD BM + <italic toggle="yes">z</italic>Solid BM). The score of the second-order factor is averaged over the scores of the first-order factors; for example, EVENT&#x000a0;=&#x000a0;0.5 * (BM&#x000a0;+&#x000a0;NBM). Thus, we obtained scores for the EVENT, OBJECT, and CE components. SVR models were trained using the leave-one-out cross-validation method to predict the scores of the CE, EVENT, and OBJECT components derived from factor analysis. The prediction procedure was performed using the MVPANI toolbox (<ext-link xlink:href="https://github.com/pymnn/MVPANI" ext-link-type="uri">https://github.com/pymnn/MVPANI</ext-link>), which was programmed on the basis of the LIBSVM toolbox (<ext-link xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" ext-link-type="uri">www.csie.ntu.edu.tw/~cjlin/libsvm/</ext-link>).</p><p>Feature ranking and selection were performed using a training dataset. For each iteration, one participant was excluded as the testing sample, with the rest forming the training dataset. Within the training dataset, the predictive weight of each edge was determined through SVR model training, and the absolute values of the weights were sorted in descending order. As we used whole-brain connectivity matrices as input features and the number of features is huge, we limited the number of the selected features to be around 200 based on previous studies (<xref rid="R39" ref-type="bibr"><italic toggle="yes">39</italic></xref>, <xref rid="R40" ref-type="bibr"><italic toggle="yes">40</italic></xref>). Thus, five subsets of features ranking from the top 1 to 9&#x02030;, with an increase step of 2&#x02030;, were used to predict the WM score of the testing sample in the current study (with feature numbers ranging from 35 to 322, more subsets were also tested; see the Supplementary Materials). The correlation coefficient between the predicted WM score and the actual score was calculated to quantify the prediction accuracy with permutation to test the significance of this correlation. For each permutation, the WM scores were shuffled and the same leave-one-out cross-validation prediction procedure was performed to generate a correlation coefficient between the predicted and shuffled WM scores. After 1000 permutations, the significance <italic toggle="yes">P</italic> value was calculated as the percentage of correlation coefficients exceeding the one calculated without shuffling WM scores. We applied FDR correction to account for multiple comparisons relevant to each research question, and the number of comparisons corrected for is explicitly stated in the figure captions.</p></sec><sec disp-level="2"><title>ROI analyses of event WM tasks</title><p>Both the <italic toggle="yes">N</italic>-back and event change detection event tasks were used to investigate whether the brain regions identified as important by the prediction models participated in WM processing. Both tasks used PLD BM and NBM as stimuli and consisted of two WM loads (0-back and 2-back for the <italic toggle="yes">N</italic>-back task and 2-sets and 4-sets for the event change detection task). For the <italic toggle="yes">N</italic>-back task, BM 0-back, BM 2-back, NBM 0-back, and NBM 2-back were regarded as separate regressors in the GLM analyses. For the change detection task, BM 2-sets, BM 4-sets, NBM 2-sets, and NBM 4-sets regressors were constructed for both encoding and delay periods [for task and GLM details, please refer to the study of Zhou <italic toggle="yes">et&#x000a0;al.</italic> (<xref rid="R65" ref-type="bibr"><italic toggle="yes">65</italic></xref>)]. &#x003b2; values were extracted from each load for each stimulus type of the event WM task.</p><p>The color-shape binding task was adopted to further confirm the specific role of the left cerebellar node in event cache. For each memory load condition, six regressors were defined: encoding (0.1 s of the memory array), delay (2/4/6/8 s of the delay after the memory array), response (2 s) for trials that were correctly responded, encoding, delay, and response for trials with wrong response.</p><p>We conducted two-sided one-sample <italic toggle="yes">t</italic> tests to test the significance of activation and two-sided paired <italic toggle="yes">t</italic> tests to compare activation differences between low and high loads for each stimulus type (BM and NBM, as well as the binding task), and the effect size (Cohen&#x02019;s <italic toggle="yes">d</italic>) was also estimated by using JASP 0.17.2.1 (<ext-link xlink:href="https://jasp-stats.org/" ext-link-type="uri">https://jasp-stats.org/</ext-link>).</p><p>In addition, we conducted time-series analyses for the event change detection task to investigate the BOLD signal change percentage during the task. Specifically, for each participant, we extracted the average time course of the ROI and segmented it according to the onset time of each encoding period. We then averaged the time-course segments of each load for each stimulus type. The resultant time courses were further converted to the percent signal change for each condition by subtracting the value of the corresponding time points of the baseline trial and then dividing by the baseline trial value. We observed two peaks in the time series, which corresponded to the encoding and probe periods. Accordingly, we selected signals between 13 and 15 s for the delay phase (<xref rid="R50" ref-type="bibr"><italic toggle="yes">50</italic></xref>). Last, we conducted two-sided paired <italic toggle="yes">t</italic> tests to test the load effects (4-sets&#x000a0;&#x0003e;&#x000a0;2-sets) for each stimulus type.</p><p>The correlation between brain activation and behavioral performance on the WM storage tasks was also examined. To reduce the number of multiple comparisons, we calculated a combined behavioral index for WM performance and a combined index for brain response in the change detection tasks (including the event task and color-shape binding task). We first did <italic toggle="yes">z</italic>-transform for the accuracy and brain activation for each WM load of each stimulus type. Then, we calculated the average of <italic toggle="yes">z</italic>-transformed accuracies for two WM loads of each stimulus type as WM performance, as well as the average of <italic toggle="yes">z</italic>-transformed activation for two WM loads of each stimulus type as brain activation. The Pearson&#x02019;s correlation efficient was calculated between brain activation and WM performance for each stimulus type. Given that information maintenance is the key feature in WM, we only considered the delay period when performing statistical comparisons.</p></sec></sec></body><back><ack><title>Acknowledgments</title><p>We would like to acknowledge the contribution of Y. Shen, Y. Zhao, and Y. Huang to data collection of psychophysical experiments.</p><sec><p><bold>Funding:</bold> This work was supported by the grants from the National Natural Science Foundation of China (32271090 to Z.G., 32000761 to T.W., 81971245 to Y.H., and 32071044 to M.S.), grants from the Key Program of Natural Science Foundation of Zhejiang Province (LZ20C090001 to Z.G.), grants from the Fundamental Research Funds for the Central Universities (no. 2022QZJH09 to Z.G.), grants from the Research of Basic Discipline for the 2.0 Base of Top-notch Students Training Program (to Z.G.), grants from the MOE Frontiers Science Center for Brain Science &#x00026; Brain-Machine Integration, Zhejiang University (to Y.H.), and grants from the STI 2030-Major Projects (no. 2021ZD0200409 to Y.H.).</p></sec><sec><p><bold>Author contributions:</bold> Conceptualization: M.S., T.W., Y.H., and Z.G. Methodology: H.Z., J.W., T.W., Y.H., and Z.G. Investigation: H.Z., J.W., J.Li, J.Lu, Z.P., T.W., Y.H., and Z.G. Supervision: M.S., T.W., Y.H., and Z.G. Writing&#x02014;original draft: H.Z., J.W., T.W., Y.H., and Z.G. Writing&#x02014;review and editing: H.Z., J.W., M.S., T.W., Y.H., and Z.G.</p></sec><sec><p><bold>Competing interests:</bold> The authors declare that they have no competing interests.</p></sec><sec><p><bold>Data and materials availability:</bold> All data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials. All of the necessary data and codes are available from <ext-link xlink:href="https://osf.io/ntfjz/" ext-link-type="uri">https://osf.io/ntfjz/</ext-link>.</p></sec></ack><sec sec-type="supplementary-material"><title>Supplementary Materials</title><sec><title>The PDF file includes:</title><supplementary-material position="float" content-type="local-data"><caption><p>Supplementary Text</p><p>Figs. S1 to S16</p><p>Legends for tables S1 to S10</p></caption><media xlink:href="sciadv.adt3063_sm.pdf"/></supplementary-material></sec><sec><title>Other Supplementary Material for this manuscript includes the following:</title><supplementary-material position="float" content-type="local-data"><caption><p>Tables S1 to S10</p></caption><media xlink:href="sciadv.adt3063_tables_s1_to_s10.zip"/></supplementary-material></sec></sec><ref-list><title>REFERENCES AND NOTES</title><ref id="R1"><label>1</label><mixed-citation publication-type="book">A. D. Baddeley, G. Hitch, &#x0201c;Working memory&#x0201d; in <italic toggle="yes">Psychology of Learning and Motivation</italic> (Elsevier, 1974), vol. 8, pp. 47&#x02013;89.</mixed-citation></ref><ref id="R2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Baddeley</surname></string-name></person-group>, 
<article-title>Working memory: Theories, models, and controversies</article-title>. <source>Annu. Rev. Psychol.</source>
<volume>63</volume>, 
<fpage>1</fpage>&#x02013;<lpage>29</lpage> (<year>2012</year>).<pub-id pub-id-type="pmid">21961947</pub-id>
</mixed-citation></ref><ref id="R3"><label>3</label><mixed-citation publication-type="book">N. Cowan, &#x0201c;An embedded-processes model of working memory&#x0201d; in <italic toggle="yes">Models of Working Memory: Mechanisms of Active Maintenance and Executive Control</italic>, A. Miyake, P. Shah, Eds. (Cambridge Univ. Press, 1999), pp. 62&#x02013;101.</mixed-citation></ref><ref id="R4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names>
<surname>Unsworth</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Fukuda</surname></string-name>, <string-name><given-names>E.</given-names>
<surname>Awh</surname></string-name>, <string-name><given-names>E. K.</given-names>
<surname>Vogel</surname></string-name></person-group>, 
<article-title>Working memory and fluid intelligence: Capacity, attention control, and secondary memory retrieval</article-title>. <source>Cogn. Psychol.</source>
<volume>71</volume>, 
<fpage>1</fpage>&#x02013;<lpage>26</lpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24531497</pub-id>
</mixed-citation></ref><ref id="R5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. C.</given-names>
<surname>McVay</surname></string-name>, <string-name><given-names>M. J.</given-names>
<surname>Kane</surname></string-name></person-group>, 
<article-title>Why does working memory capacity predict variation in reading comprehension? On the influence of mind wandering and executive attention</article-title>. <source>J. Exp. Psychol. Gen.</source>
<volume>141</volume>, 
<fpage>302</fpage>&#x02013;<lpage>320</lpage> (<year>2012</year>).<pub-id pub-id-type="pmid">21875246</pub-id>
</mixed-citation></ref><ref id="R6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. T.</given-names>
<surname>Kellogg</surname></string-name>, <string-name><given-names>C. E.</given-names>
<surname>Turner</surname></string-name>, <string-name><given-names>A. P.</given-names>
<surname>Whiteford</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Mertens</surname></string-name></person-group>, 
<article-title>The role of working memory in planning and generating written sentences</article-title>. <source>J. Writ. Res.</source>
<volume>7</volume>, 
<fpage>397</fpage>&#x02013;<lpage>416</lpage> (<year>2016</year>).</mixed-citation></ref><ref id="R7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. S.</given-names>
<surname>Lee</surname></string-name>, <string-name><given-names>D. J.</given-names>
<surname>Therriault</surname></string-name></person-group>, 
<article-title>The cognitive underpinnings of creative thought: A latent variable analysis exploring the roles of intelligence and working memory in three creative thinking processes</article-title>. <source>Dermatol. Int.</source>
<volume>41</volume>, 
<fpage>306</fpage>&#x02013;<lpage>320</lpage> (<year>2013</year>).</mixed-citation></ref><ref id="R8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names>
<surname>Kumar</surname></string-name>, <string-name><given-names>R.</given-names>
<surname>Zomorrodi</surname></string-name>, <string-name><given-names>Z.</given-names>
<surname>Ghazala</surname></string-name>, <string-name><given-names>M. S.</given-names>
<surname>Goodman</surname></string-name>, <string-name><given-names>D. M.</given-names>
<surname>Blumberger</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Cheam</surname></string-name>, <string-name><given-names>C.</given-names>
<surname>Fischer</surname></string-name>, <string-name><given-names>Z. J.</given-names>
<surname>Daskalakis</surname></string-name>, <string-name><given-names>B. H.</given-names>
<surname>Mulsant</surname></string-name>, <string-name><given-names>B. G.</given-names>
<surname>Pollock</surname></string-name>, <string-name><given-names>T. K.</given-names>
<surname>Rajji</surname></string-name></person-group>, 
<article-title>Extent of dorsolateral prefrontal cortex plasticity and its association with working memory in patients with Alzheimer disease</article-title>. <source>JAMA Psychiatry</source>
<volume>74</volume>, 
<fpage>1266</fpage>&#x02013;<lpage>1274</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">29071355</pub-id>
</mixed-citation></ref><ref id="R9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Anticevic</surname></string-name>, <string-name><given-names>G.</given-names>
<surname>Repovs</surname></string-name>, <string-name><given-names>D. M.</given-names>
<surname>Barch</surname></string-name></person-group>, 
<article-title>Working memory encoding and maintenance deficits in schizophrenia: Neural evidence for activation and deactivation abnormalities</article-title>. <source>Schizophr. Bull.</source>
<volume>39</volume>, 
<fpage>168</fpage>&#x02013;<lpage>178</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">21914644</pub-id>
</mixed-citation></ref><ref id="R10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names>
<surname>Duan</surname></string-name>, <string-name><given-names>W.</given-names>
<surname>Jiang</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Rootes-Murdy</surname></string-name>, <string-name><given-names>G. H.</given-names>
<surname>Schoenmacker</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Arias-Vasquez</surname></string-name>, <string-name><given-names>J. K.</given-names>
<surname>Buitelaar</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Hoogman</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Oosterlaan</surname></string-name>, <string-name><given-names>P. J.</given-names>
<surname>Hoekstra</surname></string-name>, <string-name><given-names>D. J.</given-names>
<surname>Heslenfeld</surname></string-name>, <string-name><given-names>C. A.</given-names>
<surname>Hartman</surname></string-name>, <string-name><given-names>V. D.</given-names>
<surname>Calhoun</surname></string-name>, <string-name><given-names>J. A.</given-names>
<surname>Turner</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Liu</surname></string-name></person-group>, 
<article-title>Gray matter networks associated with attention and working memory deficit in ADHD across adolescence and adulthood</article-title>. <source>Transl. Psychiatry</source>
<volume>11</volume>, 
<fpage>184</fpage> (<year>2021</year>).<pub-id pub-id-type="pmid">33767139</pub-id>
</mixed-citation></ref><ref id="R11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Y.</given-names>
<surname>Wang</surname></string-name>, <string-name><given-names>Y.-B.</given-names>
<surname>Zhang</surname></string-name>, <string-name><given-names>L.-L.</given-names>
<surname>Liu</surname></string-name>, <string-name><given-names>J.-F.</given-names>
<surname>Cui</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Wang</surname></string-name>, <string-name><given-names>D. H.</given-names>
<surname>Shum</surname></string-name>, <string-name><given-names>T.</given-names>
<surname>van Amelsvoort</surname></string-name>, <string-name><given-names>R. C.</given-names>
<surname>Chan</surname></string-name></person-group>, 
<article-title>A meta-analysis of working memory impairments in autism spectrum disorders</article-title>. <source>Neuropsychol. Rev.</source>
<volume>27</volume>, 
<fpage>46</fpage>&#x02013;<lpage>61</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28102493</pub-id>
</mixed-citation></ref><ref id="R12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names>
<surname>Cowan</surname></string-name></person-group>, 
<article-title>Working memory development: A 50-year assessment of research and underlying theories</article-title>. <source>Cognition</source>
<volume>224</volume>, 
<fpage>105075</fpage> (<year>2022</year>).<pub-id pub-id-type="pmid">35247864</pub-id>
</mixed-citation></ref><ref id="R13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>W.</given-names>
<surname>Xie</surname></string-name>, <string-name><given-names>J. I.</given-names>
<surname>Chapeton</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Bhasin</surname></string-name>, <string-name><given-names>C.</given-names>
<surname>Zawora</surname></string-name>, <string-name><given-names>J. H.</given-names>
<surname>Wittig</surname>
<suffix>Jr.</suffix></string-name>, <string-name><given-names>S. K.</given-names>
<surname>Inati</surname></string-name>, <string-name><given-names>W.</given-names>
<surname>Zhang</surname></string-name>, <string-name><given-names>K. A.</given-names>
<surname>Zaghloul</surname></string-name></person-group>, 
<article-title>The medial temporal lobe supports the quality of visual short-term memory representation</article-title>. <source>Nat. Hum. Behav.</source>
<volume>7</volume>, 
<fpage>627</fpage>&#x02013;<lpage>641</lpage> (<year>2023</year>).<pub-id pub-id-type="pmid">36864132</pub-id>
</mixed-citation></ref><ref id="R14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Baddeley</surname></string-name></person-group>, 
<article-title>The episodic buffer: A new component of working memory?</article-title>
<source>Trends Cogn. Sci.</source>
<volume>4</volume>, 
<fpage>417</fpage>&#x02013;<lpage>423</lpage> (<year>2000</year>).<pub-id pub-id-type="pmid">11058819</pub-id>
</mixed-citation></ref><ref id="R15"><label>15</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names>
<surname>Cowan</surname></string-name></person-group>, 
<article-title>Evolving conceptions of memory storage, selective attention, and their mutual constraints within the human information-processing system</article-title>. <source>Psychol. Bull.</source>
<volume>104</volume>, 
<fpage>163</fpage>&#x02013;<lpage>191</lpage> (<year>1988</year>).<pub-id pub-id-type="pmid">3054993</pub-id>
</mixed-citation></ref><ref id="R16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K.</given-names>
<surname>Oberauer</surname></string-name></person-group>, 
<article-title>Access to information in working memory: Exploring the focus of attention</article-title>. <source>J. Exp. Psychol. Learn. Mem. Cogn.</source>
<volume>28</volume>, 
<fpage>411</fpage>&#x02013;<lpage>421</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">12018494</pub-id>
</mixed-citation></ref><ref id="R17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. B.</given-names>
<surname>Christophel</surname></string-name>, <string-name><given-names>P. C.</given-names>
<surname>Klink</surname></string-name>, <string-name><given-names>B.</given-names>
<surname>Spitzer</surname></string-name>, <string-name><given-names>P. R.</given-names>
<surname>Roelfsema</surname></string-name>, <string-name><given-names>J. D.</given-names>
<surname>Haynes</surname></string-name></person-group>, 
<article-title>The distributed nature of working memory</article-title>. <source>Trends Cogn. Sci.</source>
<volume>21</volume>, 
<fpage>111</fpage>&#x02013;<lpage>124</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28063661</pub-id>
</mixed-citation></ref><ref id="R18"><label>18</label><mixed-citation publication-type="book">T. F. Shipley, J. M. Zacks, <italic toggle="yes">Understanding Events: From Perception to Action</italic> (Oxford Univ. Press, 2008).</mixed-citation></ref><ref id="R19"><label>19</label><mixed-citation publication-type="book">G. A. Radvansky, J. M. Zacks, <italic toggle="yes">Event Cognition</italic> (Oxford Univ. Press, 2014).</mixed-citation></ref><ref id="R20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. M.</given-names>
<surname>Zacks</surname></string-name>, <string-name><given-names>B.</given-names>
<surname>Tversky</surname></string-name></person-group>, 
<article-title>Event structure in perception and conception</article-title>. <source>Psychol. Bull.</source>
<volume>127</volume>, 
<fpage>3</fpage>&#x02013;<lpage>21</lpage> (<year>2001</year>).<pub-id pub-id-type="pmid">11271755</pub-id>
</mixed-citation></ref><ref id="R21"><label>21</label><mixed-citation publication-type="book">J. Barwise, J. Perry, <italic toggle="yes">Situations and Attitudes</italic> (MIT Press, 1983).</mixed-citation></ref><ref id="R22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. M.</given-names>
<surname>Zacks</surname></string-name></person-group>, 
<article-title>Event perception and memory</article-title>. <source>Annu. Rev. Psychol.</source>
<volume>71</volume>, 
<fpage>165</fpage>&#x02013;<lpage>191</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">31905113</pub-id>
</mixed-citation></ref><ref id="R23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G. A.</given-names>
<surname>Radvansky</surname></string-name></person-group>, 
<article-title>Event segmentation as a working memory process</article-title>. <source>Mem. Cogn.</source>
<volume>6</volume>, 
<fpage>121</fpage>&#x02013;<lpage>123</lpage> (<year>2017</year>).</mixed-citation></ref><ref id="R24"><label>24</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L. L.</given-names>
<surname>Richmond</surname></string-name>, <string-name><given-names>D. A.</given-names>
<surname>Gold</surname></string-name>, <string-name><given-names>J. M.</given-names>
<surname>Zacks</surname></string-name></person-group>, 
<article-title>Event perception: Translations and applications</article-title>. <source>J. Appl. Res. Mem. Cogn.</source>
<volume>6</volume>, 
<fpage>111</fpage>&#x02013;<lpage>120</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">28936393</pub-id>
</mixed-citation></ref><ref id="R25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G. A.</given-names>
<surname>Radvansky</surname></string-name>, <string-name><given-names>J. M.</given-names>
<surname>Zacks</surname></string-name></person-group>, 
<article-title>Event boundaries in memory and cognition</article-title>. <source>Curr. Opin. Behav. Sci.</source>
<volume>17</volume>, 
<fpage>133</fpage>&#x02013;<lpage>140</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">29270446</pub-id>
</mixed-citation></ref><ref id="R26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N. T.</given-names>
<surname>Franklin</surname></string-name>, <string-name><given-names>K. A.</given-names>
<surname>Norman</surname></string-name>, <string-name><given-names>C.</given-names>
<surname>Ranganath</surname></string-name>, <string-name><given-names>J. M.</given-names>
<surname>Zacks</surname></string-name>, <string-name><given-names>S. J.</given-names>
<surname>Gershman</surname></string-name></person-group>, 
<article-title>Structured Event Memory: A neuro-symbolic model of event cognition</article-title>. <source>Psychol. Rev.</source>
<volume>127</volume>, 
<fpage>327</fpage>&#x02013;<lpage>361</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32223284</pub-id>
</mixed-citation></ref><ref id="R27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names>
<surname>Li</surname></string-name>, <string-name><given-names>X.</given-names>
<surname>Ren</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Schweizer</surname></string-name>, <string-name><given-names>T.</given-names>
<surname>Wang</surname></string-name></person-group>, 
<article-title>Strategy use moderates the relation between working memory capacity and fluid intelligence: A combined approach</article-title>. <source>Dermatol. Int.</source>
<volume>91</volume>, 
<fpage>101627</fpage> (<year>2022</year>).</mixed-citation></ref><ref id="R28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. W.</given-names>
<surname>Engle</surname></string-name></person-group>, 
<article-title>Working memory capacity as executive attention</article-title>. <source>Curr. Dir. Psychol. Sci.</source>
<volume>11</volume>, 
<fpage>19</fpage>&#x02013;<lpage>23</lpage> (<year>2002</year>).</mixed-citation></ref><ref id="R29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names>
<surname>Drew</surname></string-name>, <string-name><given-names>E. K.</given-names>
<surname>Vogel</surname></string-name></person-group>, 
<article-title>Neural measures of individual differences in selecting and tracking multiple moving objects</article-title>. <source>J. Neurosci.</source>
<volume>28</volume>, 
<fpage>4183</fpage>&#x02013;<lpage>4191</lpage> (<year>2008</year>).<pub-id pub-id-type="pmid">18417697</pub-id>
</mixed-citation></ref><ref id="R30"><label>30</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names>
<surname>Cowan</surname></string-name></person-group>, 
<article-title>The magical number 4 in short-term memory: A reconsideration of mental storage capacity</article-title>. <source>Behav. Brain</source>
<volume>24</volume>, 
<fpage>87</fpage>&#x02013;<lpage>114</lpage> (<year>2001</year>).</mixed-citation></ref><ref id="R31"><label>31</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B. J.</given-names>
<surname>Scholl</surname></string-name>, <string-name><given-names>Z. W.</given-names>
<surname>Pylyshyn</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Feldman</surname></string-name></person-group>, 
<article-title>What is a visual object? Evidence from target merging in multiple object tracking</article-title>. <source>Cognition</source>
<volume>80</volume>, 
<fpage>159</fpage>&#x02013;<lpage>177</lpage> (<year>2001</year>).<pub-id pub-id-type="pmid">11245843</pub-id>
</mixed-citation></ref><ref id="R32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names>
<surname>He</surname></string-name>, <string-name><given-names>D.</given-names>
<surname>Guo</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Zhai</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Shen</surname></string-name>, <string-name><given-names>Z.</given-names>
<surname>Gao</surname></string-name></person-group>, 
<article-title>Development of social working memory in preschoolers and its relation to theory of mind</article-title>. <source>Child Dev.</source>
<volume>90</volume>, 
<fpage>1319</fpage>&#x02013;<lpage>1332</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">29292501</pub-id>
</mixed-citation></ref><ref id="R33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Z.</given-names>
<surname>Gao</surname></string-name>, <string-name><given-names>T.</given-names>
<surname>Ye</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Shen</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Perry</surname></string-name></person-group>, 
<article-title>Working memory capacity of biological movements predicts empathy traits</article-title>. <source>Psychon. Bull. Rev.</source>
<volume>23</volume>, 
<fpage>468</fpage>&#x02013;<lpage>475</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">26174575</pub-id>
</mixed-citation></ref><ref id="R34"><label>34</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F. D.</given-names>
<surname>Sch&#x000f6;nbrodt</surname></string-name>, <string-name><given-names>E.-J.</given-names>
<surname>Wagenmakers</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Zehetleitner</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Perugini</surname></string-name></person-group>, 
<article-title>Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences</article-title>. <source>Psychol. Methods</source>
<volume>22</volume>, 
<fpage>322</fpage>&#x02013;<lpage>339</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">26651986</pub-id>
</mixed-citation></ref><ref id="R35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names>
<surname>Shen</surname></string-name>, <string-name><given-names>X.</given-names>
<surname>Huang</surname></string-name>, <string-name><given-names>Z.</given-names>
<surname>Gao</surname></string-name></person-group>, 
<article-title>Object-based attention underlies the rehearsal of feature binding in visual working memory</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
<volume>41</volume>, 
<fpage>479</fpage>&#x02013;<lpage>493</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25602968</pub-id>
</mixed-citation></ref><ref id="R36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Galvez-Pol</surname></string-name>, <string-name><given-names>B.</given-names>
<surname>Forster</surname></string-name>, <string-name><given-names>B.</given-names>
<surname>Calvo-Merino</surname></string-name></person-group>, 
<article-title>Beyond action observation: Neurobehavioral mechanisms of memory for visually perceived bodies and actions</article-title>. <source>Neurosci. Biobehav. Rev.</source>
<volume>116</volume>, 
<fpage>508</fpage>&#x02013;<lpage>518</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32544541</pub-id>
</mixed-citation></ref><ref id="R37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>X.</given-names>
<surname>Shen</surname></string-name>, <string-name><given-names>F.</given-names>
<surname>Tokoglu</surname></string-name>, <string-name><given-names>X.</given-names>
<surname>Papademetris</surname></string-name>, <string-name><given-names>R. T.</given-names>
<surname>Constable</surname></string-name></person-group>, 
<article-title>Groupwise whole-brain parcellation from resting-state fMRI data for network node identification</article-title>. <source>Neuroimage</source>
<volume>82</volume>, 
<fpage>403</fpage>&#x02013;<lpage>415</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23747961</pub-id>
</mixed-citation></ref><ref id="R38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. S.</given-names>
<surname>Greene</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Gao</surname></string-name>, <string-name><given-names>D.</given-names>
<surname>Scheinost</surname></string-name>, <string-name><given-names>R. T.</given-names>
<surname>Constable</surname></string-name></person-group>, 
<article-title>Task-induced brain state manipulation improves prediction of individual traits</article-title>. <source>Nat. Commun.</source>
<volume>9</volume>, 
<fpage>2807</fpage> (<year>2018</year>).<pub-id pub-id-type="pmid">30022026</pub-id>
</mixed-citation></ref><ref id="R39"><label>39</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N. U.</given-names>
<surname>Dosenbach</surname></string-name>, <string-name><given-names>B.</given-names>
<surname>Nardos</surname></string-name>, <string-name><given-names>A. L.</given-names>
<surname>Cohen</surname></string-name>, <string-name><given-names>D. A.</given-names>
<surname>Fair</surname></string-name>, <string-name><given-names>J. D.</given-names>
<surname>Power</surname></string-name>, <string-name><given-names>J. A.</given-names>
<surname>Church</surname></string-name>, <string-name><given-names>S. M.</given-names>
<surname>Nelson</surname></string-name>, <string-name><given-names>G. S.</given-names>
<surname>Wig</surname></string-name>, <string-name><given-names>A. C.</given-names>
<surname>Vogel</surname></string-name>, <string-name><given-names>C. N.</given-names>
<surname>Lessov-Schlaggar</surname></string-name>, <string-name><given-names>K. A.</given-names>
<surname>Barnes</surname></string-name>, <string-name><given-names>J. W.</given-names>
<surname>Dubis</surname></string-name>, <string-name><given-names>E.</given-names>
<surname>Feczko</surname></string-name>, <string-name><given-names>R. S.</given-names>
<surname>Coalson</surname></string-name>, <string-name><given-names>J. R.</given-names>
<surname>Pruett</surname>
<suffix>Jr.</suffix></string-name>, <string-name><given-names>D. M.</given-names>
<surname>Barch</surname></string-name>, <string-name><given-names>S. E.</given-names>
<surname>Petersen</surname></string-name>, <string-name><given-names>B. L.</given-names>
<surname>Schlaggar</surname></string-name></person-group>, 
<article-title>Prediction of individual brain maturity using fMRI</article-title>. <source>Science</source>
<volume>329</volume>, 
<fpage>1358</fpage>&#x02013;<lpage>1361</lpage> (<year>2010</year>).<pub-id pub-id-type="pmid">20829489</pub-id>
</mixed-citation></ref><ref id="R40"><label>40</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. D.</given-names>
<surname>Smyser</surname></string-name>, <string-name><given-names>N. U.</given-names>
<surname>Dosenbach</surname></string-name>, <string-name><given-names>T. A.</given-names>
<surname>Smyser</surname></string-name>, <string-name><given-names>A. Z.</given-names>
<surname>Snyder</surname></string-name>, <string-name><given-names>C. E.</given-names>
<surname>Rogers</surname></string-name>, <string-name><given-names>T. E.</given-names>
<surname>Inder</surname></string-name>, <string-name><given-names>B. L.</given-names>
<surname>Schlaggar</surname></string-name>, <string-name><given-names>J. J.</given-names>
<surname>Neil</surname></string-name></person-group>, 
<article-title>Prediction of brain maturity in infants using machine-learning algorithms</article-title>. <source>Neuroimage</source>
<volume>136</volume>, 
<fpage>1</fpage>&#x02013;<lpage>9</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">27179605</pub-id>
</mixed-citation></ref><ref id="R41"><label>41</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names>
<surname>Feng</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Yuan</surname></string-name>, <string-name><given-names>H.</given-names>
<surname>Geng</surname></string-name>, <string-name><given-names>R.</given-names>
<surname>Gu</surname></string-name>, <string-name><given-names>H.</given-names>
<surname>Zhou</surname></string-name>, <string-name><given-names>X.</given-names>
<surname>Wu</surname></string-name>, <string-name><given-names>Y.</given-names>
<surname>Luo</surname></string-name></person-group>, 
<article-title>Individualized prediction of trait narcissism from whole-brain resting-state functional connectivity</article-title>. <source>Hum. Brain Mapp.</source>
<volume>39</volume>, 
<fpage>3701</fpage>&#x02013;<lpage>3712</lpage> (<year>2018</year>).<pub-id pub-id-type="pmid">29749072</pub-id>
</mixed-citation></ref><ref id="R42"><label>42</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. M.</given-names>
<surname>Zacks</surname></string-name>, <string-name><given-names>K. M.</given-names>
<surname>Swallow</surname></string-name></person-group>, 
<article-title>Event segmentation</article-title>. <source>Curr. Dir. Psychol. Sci.</source>
<volume>16</volume>, 
<fpage>80</fpage>&#x02013;<lpage>84</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">22468032</pub-id>
</mixed-citation></ref><ref id="R43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. E.</given-names>
<surname>Dunsmoor</surname></string-name>, <string-name><given-names>V. P.</given-names>
<surname>Murty</surname></string-name>, <string-name><given-names>D.</given-names>
<surname>Clewett</surname></string-name>, <string-name><given-names>E. A.</given-names>
<surname>Phelps</surname></string-name>, <string-name><given-names>L.</given-names>
<surname>Davachi</surname></string-name></person-group>, 
<article-title>Tag and capture: How salient experiences target and rescue nearby events in memory</article-title>. <source>Trends Cogn. Sci.</source>
<volume>26</volume>, 
<fpage>782</fpage>&#x02013;<lpage>795</lpage> (<year>2022</year>).<pub-id pub-id-type="pmid">35842373</pub-id>
</mixed-citation></ref><ref id="R44"><label>44</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D.</given-names>
<surname>Clewett</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>DuBrow</surname></string-name>, <string-name><given-names>L.</given-names>
<surname>Davachi</surname></string-name></person-group>, 
<article-title>Transcending time in the brain: How event memories are constructed from experience</article-title>. <source>Hippocampus</source>
<volume>29</volume>, 
<fpage>162</fpage>&#x02013;<lpage>183</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30734391</pub-id>
</mixed-citation></ref><ref id="R45"><label>45</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. N.</given-names>
<surname>Wood</surname></string-name></person-group>, 
<article-title>Visual working memory for observed actions</article-title>. <source>J. Exp. Psychol. Gen.</source>
<volume>136</volume>, 
<fpage>639</fpage>&#x02013;<lpage>652</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17999576</pub-id>
</mixed-citation></ref><ref id="R46"><label>46</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. N.</given-names>
<surname>Wood</surname></string-name></person-group>, 
<article-title>A core knowledge architecture of visual working memory</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
<volume>37</volume>, 
<fpage>357</fpage>&#x02013;<lpage>381</lpage> (<year>2011</year>).<pub-id pub-id-type="pmid">21463083</pub-id>
</mixed-citation></ref><ref id="R47"><label>47</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names>
<surname>Shen</surname></string-name>, <string-name><given-names>Z.</given-names>
<surname>Gao</surname></string-name>, <string-name><given-names>X.</given-names>
<surname>Ding</surname></string-name>, <string-name><given-names>B.</given-names>
<surname>Zhou</surname></string-name>, <string-name><given-names>X.</given-names>
<surname>Huang</surname></string-name></person-group>, 
<article-title>Holding biological motion information in working memory</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
<volume>40</volume>, 
<fpage>1332</fpage>&#x02013;<lpage>1345</lpage> (<year>2014</year>).<pub-id pub-id-type="pmid">24842069</pub-id>
</mixed-citation></ref><ref id="R48"><label>48</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.</given-names>
<surname>Gong</surname></string-name>, <string-name><given-names>D.</given-names>
<surname>Guo</surname></string-name>, <string-name><given-names>Z.</given-names>
<surname>Gao</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Wei</surname></string-name></person-group>, 
<article-title>Atypical development of social and nonsocial working memory capacity among preschoolers with autism spectrum disorders</article-title>. <source>Autism Res.</source>
<volume>16</volume>, 
<fpage>327</fpage>&#x02013;<lpage>339</lpage> (<year>2023</year>).<pub-id pub-id-type="pmid">36374256</pub-id>
</mixed-citation></ref><ref id="R49"><label>49</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Z.</given-names>
<surname>Gao</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Bentin</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Shen</surname></string-name></person-group>, 
<article-title>Rehearsing biological motion in working memory: An EEG study</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>27</volume>, 
<fpage>198</fpage>&#x02013;<lpage>209</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25061930</pub-id>
</mixed-citation></ref><ref id="R50"><label>50</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>X.</given-names>
<surname>Lu</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Huang</surname></string-name>, <string-name><given-names>Y.</given-names>
<surname>Yi</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Shen</surname></string-name>, <string-name><given-names>X.</given-names>
<surname>Weng</surname></string-name>, <string-name><given-names>Z.</given-names>
<surname>Gao</surname></string-name></person-group>, 
<article-title>Holding biological motion in working memory: An fMRI study</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>10</volume>, 
<fpage>251</fpage> (<year>2016</year>).<pub-id pub-id-type="pmid">27313520</pub-id>
</mixed-citation></ref><ref id="R51"><label>51</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. P.</given-names>
<surname>Alloway</surname></string-name>, <string-name><given-names>S. E.</given-names>
<surname>Gathercole</surname></string-name>, <string-name><given-names>C.</given-names>
<surname>Willis</surname></string-name>, <string-name><given-names>A.-M.</given-names>
<surname>Adams</surname></string-name></person-group>, 
<article-title>A structural analysis of working memory and related cognitive skills in young children</article-title>. <source>J. Exp. Child Psychol.</source>
<volume>87</volume>, 
<fpage>85</fpage>&#x02013;<lpage>106</lpage> (<year>2004</year>).<pub-id pub-id-type="pmid">14757066</pub-id>
</mixed-citation></ref><ref id="R52"><label>52</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names>
<surname>Gray</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Green</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Alt</surname></string-name>, <string-name><given-names>T.</given-names>
<surname>Hogan</surname></string-name>, <string-name><given-names>T.</given-names>
<surname>Kuo</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Brinkley</surname></string-name>, <string-name><given-names>N.</given-names>
<surname>Cowan</surname></string-name></person-group>, 
<article-title>The structure of working memory in young children and its relation to intelligence</article-title>. <source>J. Mem. Lang.</source>
<volume>92</volume>, 
<fpage>183</fpage>&#x02013;<lpage>201</lpage> (<year>2017</year>).<pub-id pub-id-type="pmid">27990060</pub-id>
</mixed-citation></ref><ref id="R53"><label>53</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names>
<surname>Matsukura</surname></string-name>, <string-name><given-names>S. P.</given-names>
<surname>Vecera</surname></string-name></person-group>, 
<article-title>Object-based selection from spatially-invariant representations: Evidence from a feature-report task</article-title>. <source>Atten. Percept. Psychophys.</source>
<volume>73</volume>, 
<fpage>447</fpage>&#x02013;<lpage>457</lpage> (<year>2011</year>).<pub-id pub-id-type="pmid">21264725</pub-id>
</mixed-citation></ref><ref id="R54"><label>54</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. H.</given-names>
<surname>Logie</surname></string-name></person-group>, 
<article-title>The functional organization and capacity limits of working memory</article-title>. <source>Curr. Dir. Psychol. Sci.</source>
<volume>20</volume>, 
<fpage>240</fpage>&#x02013;<lpage>245</lpage> (<year>2011</year>).</mixed-citation></ref><ref id="R55"><label>55</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. P.</given-names>
<surname>Boisgontier</surname></string-name></person-group>, 
<article-title>Motor aging results from cerebellar neuron death</article-title>. <source>Trends Neurosci.</source>
<volume>38</volume>, 
<fpage>127</fpage>&#x02013;<lpage>128</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25623752</pub-id>
</mixed-citation></ref><ref id="R56"><label>56</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C. I.</given-names>
<surname>De Zeeuw</surname></string-name>, <string-name><given-names>M. M.</given-names>
<surname>Ten Brinke</surname></string-name></person-group>, 
<article-title>Motor learning and the cerebellum</article-title>. <source>Cold Spring Harb. Perspect. Biol.</source>
<volume>7</volume>, 
<fpage>a021683</fpage> (<year>2015</year>).<pub-id pub-id-type="pmid">26330521</pub-id>
</mixed-citation></ref><ref id="R57"><label>57</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R.</given-names>
<surname>Ashida</surname></string-name>, <string-name><given-names>N. L.</given-names>
<surname>Cerminara</surname></string-name>, <string-name><given-names>R. J.</given-names>
<surname>Edwards</surname></string-name>, <string-name><given-names>R.</given-names>
<surname>Apps</surname></string-name>, <string-name><given-names>J. C.</given-names>
<surname>Brooks</surname></string-name></person-group>, 
<article-title>Sensorimotor, language, and working memory representation within the human cerebellum</article-title>. <source>Hum. Brain Mapp.</source>
<volume>40</volume>, 
<fpage>4732</fpage>&#x02013;<lpage>4747</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">31361075</pub-id>
</mixed-citation></ref><ref id="R58"><label>58</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S. M.</given-names>
<surname>Ravizza</surname></string-name>, <string-name><given-names>C. A.</given-names>
<surname>McCormick</surname></string-name>, <string-name><given-names>J. E.</given-names>
<surname>Schlerf</surname></string-name>, <string-name><given-names>T.</given-names>
<surname>Justus</surname></string-name>, <string-name><given-names>R. B.</given-names>
<surname>Ivry</surname></string-name>, <string-name><given-names>J. A.</given-names>
<surname>Fiez</surname></string-name></person-group>, 
<article-title>Cerebellar damage produces selective deficits in verbal working memory</article-title>. <source>Brain</source>
<volume>129</volume>, 
<fpage>306</fpage>&#x02013;<lpage>320</lpage> (<year>2006</year>).<pub-id pub-id-type="pmid">16317024</pub-id>
</mixed-citation></ref><ref id="R59"><label>59</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. E.</given-names>
<surname>Desmond</surname></string-name>, <string-name><given-names>S. A.</given-names>
<surname>Chen</surname></string-name>, <string-name><given-names>P. B.</given-names>
<surname>Shieh</surname></string-name></person-group>, 
<article-title>Cerebellar transcranial magnetic stimulation impairs verbal working memory</article-title>. <source>Ann. Neurol.</source>
<volume>58</volume>, 
<fpage>553</fpage>&#x02013;<lpage>560</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">16178033</pub-id>
</mixed-citation></ref><ref id="R60"><label>60</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names>
<surname>Vi&#x000f1;as-Guasch</surname></string-name>, <string-name><given-names>T. H. B.</given-names>
<surname>Ng</surname></string-name>, <string-name><given-names>J. G.</given-names>
<surname>Heng</surname></string-name>, <string-name><given-names>Y. C.</given-names>
<surname>Chan</surname></string-name>, <string-name><given-names>E.</given-names>
<surname>Chew</surname></string-name>, <string-name><given-names>J. E.</given-names>
<surname>Desmond</surname></string-name>, <string-name><given-names>S. A.</given-names>
<surname>Chen</surname></string-name></person-group>, 
<article-title>Cerebellar transcranial magnetic stimulation (TMS) impairs visual working memory</article-title>. <source>Cerebellum</source>
<volume>22</volume>, 
<fpage>332</fpage>&#x02013;<lpage>347</lpage> (<year>2023</year>).<pub-id pub-id-type="pmid">35355219</pub-id>
</mixed-citation></ref><ref id="R61"><label>61</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names>
<surname>Leggio</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Tedesco</surname></string-name>, <string-name><given-names>F.</given-names>
<surname>Chiricozzi</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Clausi</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Orsini</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Molinari</surname></string-name></person-group>, 
<article-title>Cognitive sequencing impairment in patients with focal or atrophic cerebellar damage</article-title>. <source>Brain</source>
<volume>131</volume>, 
<fpage>1332</fpage>&#x02013;<lpage>1343</lpage> (<year>2008</year>).<pub-id pub-id-type="pmid">18334535</pub-id>
</mixed-citation></ref><ref id="R62"><label>62</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names>
<surname>Molinari</surname></string-name>, <string-name><given-names>F. R.</given-names>
<surname>Chiricozzi</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Clausi</surname></string-name>, <string-name><given-names>A. M.</given-names>
<surname>Tedesco</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>De Lisa</surname></string-name>, <string-name><given-names>M. G.</given-names>
<surname>Leggio</surname></string-name></person-group>, 
<article-title>Cerebellum and detection of sequences, from perception to cognition</article-title>. <source>Cerebellum</source>
<volume>7</volume>, 
<fpage>611</fpage>&#x02013;<lpage>615</lpage> (<year>2008</year>).<pub-id pub-id-type="pmid">18941861</pub-id>
</mixed-citation></ref><ref id="R63"><label>63</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names>
<surname>Heleven</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>van Dun</surname></string-name>, <string-name><given-names>F.</given-names>
<surname>Van Overwalle</surname></string-name></person-group>, 
<article-title>The posterior Cerebellum is involved in constructing Social Action Sequences: An fMRI Study</article-title>. <source>Sci. Rep.</source>
<volume>9</volume>, 
<fpage>11110</fpage> (<year>2019</year>).<pub-id pub-id-type="pmid">31366954</pub-id>
</mixed-citation></ref><ref id="R64"><label>64</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>U.</given-names>
<surname>Hasson</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Chen</surname></string-name>, <string-name><given-names>C. J.</given-names>
<surname>Honey</surname></string-name></person-group>, 
<article-title>Hierarchical process memory: Memory as an integral component of information processing</article-title>. <source>Trends Cogn. Sci.</source>
<volume>19</volume>, 
<fpage>304</fpage>&#x02013;<lpage>313</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25980649</pub-id>
</mixed-citation></ref><ref id="R65"><label>65</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>H.</given-names>
<surname>Zhou</surname></string-name>, <string-name><given-names>C.</given-names>
<surname>Su</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Wu</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Li</surname></string-name>, <string-name><given-names>X.</given-names>
<surname>Lu</surname></string-name>, <string-name><given-names>L.</given-names>
<surname>Gong</surname></string-name>, <string-name><given-names>F.</given-names>
<surname>Geng</surname></string-name>, <string-name><given-names>Z.</given-names>
<surname>Gao</surname></string-name>, <string-name><given-names>Y.</given-names>
<surname>Hu</surname></string-name></person-group>, 
<article-title>A domain-general frontoparietal network interacts with domain-preferential intermediate pathways to support working memory task</article-title>. <source>Cereb. Cortex</source>
<volume>33</volume>, 
<fpage>2774</fpage>&#x02013;<lpage>2787</lpage> (<year>2023</year>).<pub-id pub-id-type="pmid">35671498</pub-id>
</mixed-citation></ref><ref id="R66"><label>66</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. M.</given-names>
<surname>Owen</surname></string-name>, <string-name><given-names>K. M.</given-names>
<surname>McMillan</surname></string-name>, <string-name><given-names>A. R.</given-names>
<surname>Laird</surname></string-name>, <string-name><given-names>E.</given-names>
<surname>Bullmore</surname></string-name></person-group>, 
<article-title>N-back working memory paradigm: A meta-analysis of normative functional neuroimaging studies</article-title>. <source>Hum. Brain Mapp.</source>
<volume>25</volume>, 
<fpage>46</fpage>&#x02013;<lpage>59</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15846822</pub-id>
</mixed-citation></ref><ref id="R67"><label>67</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. C.</given-names>
<surname>Murphy</surname></string-name>, <string-name><given-names>M. A.</given-names>
<surname>Bertolero</surname></string-name>, <string-name><given-names>L.</given-names>
<surname>Papadopoulos</surname></string-name>, <string-name><given-names>D. M.</given-names>
<surname>Lydon-Staley</surname></string-name>, <string-name><given-names>D. S.</given-names>
<surname>Bassett</surname></string-name></person-group>, 
<article-title>Multimodal network dynamics underpinning working memory</article-title>. <source>Nat. Commun.</source>
<volume>11</volume>, 
<fpage>3035</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">32541774</pub-id>
</mixed-citation></ref><ref id="R68"><label>68</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Miyake</surname></string-name>, <string-name><given-names>N. P.</given-names>
<surname>Friedman</surname></string-name>, <string-name><given-names>M. J.</given-names>
<surname>Emerson</surname></string-name>, <string-name><given-names>A. H.</given-names>
<surname>Witzki</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Howerter</surname></string-name>, <string-name><given-names>T. D.</given-names>
<surname>Wager</surname></string-name></person-group>, 
<article-title>The unity and diversity of executive functions and their contributions to complex &#x0201c;frontal lobe&#x0201d; tasks: A latent variable analysis</article-title>. <source>Cogn. Psychol.</source>
<volume>41</volume>, 
<fpage>49</fpage>&#x02013;<lpage>100</lpage> (<year>2000</year>).<pub-id pub-id-type="pmid">10945922</pub-id>
</mixed-citation></ref><ref id="R69"><label>69</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names>
<surname>Feng</surname></string-name>, <string-name><given-names>L.</given-names>
<surname>Zhang</surname></string-name>, <string-name><given-names>C.</given-names>
<surname>Chen</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Sheng</surname></string-name>, <string-name><given-names>Z.</given-names>
<surname>Ye</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Feng</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Liu</surname></string-name>, <string-name><given-names>Y.</given-names>
<surname>Cai</surname></string-name>, <string-name><given-names>B.</given-names>
<surname>Zhu</surname></string-name>, <string-name><given-names>Z.</given-names>
<surname>Yu</surname></string-name>, <string-name><given-names>C.</given-names>
<surname>Chen</surname></string-name>, <string-name><given-names>Q.</given-names>
<surname>Dong</surname></string-name>, <string-name><given-names>G.</given-names>
<surname>Xue</surname></string-name></person-group>, 
<article-title>A cognitive neurogenetic approach to uncovering the structure of executive functions</article-title>. <source>Nat. Commun.</source>
<volume>13</volume>, 
<fpage>4588</fpage> (<year>2022</year>).<pub-id pub-id-type="pmid">35933428</pub-id>
</mixed-citation></ref><ref id="R70"><label>70</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N. P.</given-names>
<surname>Friedman</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Miyake</surname></string-name></person-group>, 
<article-title>The relations among inhibition and interference control functions: A latent-variable analysis</article-title>. <source>J. Exp. Psychol. Gen.</source>
<volume>133</volume>, 
<fpage>101</fpage>&#x02013;<lpage>135</lpage> (<year>2004</year>).<pub-id pub-id-type="pmid">14979754</pub-id>
</mixed-citation></ref><ref id="R71"><label>71</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>Z.</given-names>
<surname>Shipstead</surname></string-name>, <string-name><given-names>D. R.</given-names>
<surname>Lindsey</surname></string-name>, <string-name><given-names>R. L.</given-names>
<surname>Marshall</surname></string-name>, <string-name><given-names>R. W.</given-names>
<surname>Engle</surname></string-name></person-group>, 
<article-title>The mechanisms of working memory capacity: Primary memory, secondary memory, and attention control</article-title>. <source>J. Mem. Lang.</source>
<volume>72</volume>, 
<fpage>116</fpage>&#x02013;<lpage>141</lpage> (<year>2014</year>).</mixed-citation></ref><ref id="R72"><label>72</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.</given-names>
<surname>Vanrie</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Verfaillie</surname></string-name></person-group>, 
<article-title>Perception of biological motion: A stimulus set of human point-light actions</article-title>. <source>Behav. Res. Methods Instrum. Comput.</source>
<volume>36</volume>, 
<fpage>625</fpage>&#x02013;<lpage>629</lpage> (<year>2004</year>).<pub-id pub-id-type="pmid">15641407</pub-id>
</mixed-citation></ref><ref id="R73"><label>73</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>N.</given-names>
<surname>Unsworth</surname></string-name>, <string-name><given-names>G. J.</given-names>
<surname>Spillers</surname></string-name></person-group>, 
<article-title>Working memory capacity: Attention control, secondary memory, or both? A direct test of the dual-component model</article-title>. <source>J. Mem. Lang.</source>
<volume>62</volume>, 
<fpage>392</fpage>&#x02013;<lpage>406</lpage> (<year>2010</year>).</mixed-citation></ref><ref id="R74"><label>74</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Satorra</surname></string-name>, <string-name><given-names>P. M.</given-names>
<surname>Bentler</surname></string-name></person-group>, 
<article-title>A scaled difference chi-square test statistic for moment structure analysis</article-title>. <source>Psychometrika</source>
<volume>66</volume>, 
<fpage>507</fpage>&#x02013;<lpage>514</lpage> (<year>2001</year>).</mixed-citation></ref><ref id="R75"><label>75</label><mixed-citation publication-type="book">R. B. Kline, <italic toggle="yes">Principles and Practice of Structural Equation Modeling</italic> (Guilford Publications, 2015).</mixed-citation></ref><ref id="R76"><label>76</label><mixed-citation publication-type="book">C. DiStefano, &#x0201c;Examining fit with structural equation models&#x0201d; in <italic toggle="yes">Principles and Methods of Test Construction: Standards and Recent Advances</italic> (Hogrefe, 2016), pp. 166&#x02013;193.</mixed-citation></ref><ref id="R77"><label>77</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G. W.</given-names>
<surname>Cheung</surname></string-name>, <string-name><given-names>R. B.</given-names>
<surname>Rensvold</surname></string-name></person-group>, 
<article-title>Evaluating goodness-of-fit indexes for testing measurement invariance</article-title>. <source>Struct. Equ. Model.</source>
<volume>9</volume>, 
<fpage>233</fpage>&#x02013;<lpage>255</lpage> (<year>2002</year>).</mixed-citation></ref><ref id="R78"><label>78</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names>
<surname>Valdes-Sosa</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Cobo</surname></string-name>, <string-name><given-names>T.</given-names>
<surname>Pinilla</surname></string-name></person-group>, 
<article-title>Attention to object files defined by transparent motion</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
<volume>26</volume>, 
<fpage>488</fpage>&#x02013;<lpage>505</lpage> (<year>2000</year>).<pub-id pub-id-type="pmid">10811159</pub-id>
</mixed-citation></ref><ref id="R79"><label>79</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. C.</given-names>
<surname>Ho</surname></string-name>, <string-name><given-names>J. C.</given-names>
<surname>Walker</surname></string-name>, <string-name><given-names>G. I.</given-names>
<surname>Teresi</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Kulla</surname></string-name>, <string-name><given-names>J. S.</given-names>
<surname>Kirshenbaum</surname></string-name>, <string-name><given-names>A. J.</given-names>
<surname>Gifuni</surname></string-name>, <string-name><given-names>M. K.</given-names>
<surname>Singh</surname></string-name>, <string-name><given-names>I. H.</given-names>
<surname>Gotlib</surname></string-name></person-group>, 
<article-title>Default mode and salience network alterations in suicidal and non-suicidal self-injurious thoughts and behaviors in adolescents with depression</article-title>. <source>Transl. Psychiatry</source>
<volume>11</volume>, 
<fpage>38</fpage> (<year>2021</year>).<pub-id pub-id-type="pmid">33436537</pub-id>
</mixed-citation></ref><ref id="R80"><label>80</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names>
<surname>Langhammer</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Hilbert</surname></string-name>, <string-name><given-names>D.</given-names>
<surname>Adolph</surname></string-name>, <string-name><given-names>V.</given-names>
<surname>Arolt</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Bischoff</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Bohnlein</surname></string-name>, <string-name><given-names>J. C.</given-names>
<surname>Cwik</surname></string-name>, <string-name><given-names>U.</given-names>
<surname>Dannlowski</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Deckert</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Domschke</surname></string-name>, <string-name><given-names>R.</given-names>
<surname>Evens</surname></string-name>, <string-name><given-names>T.</given-names>
<surname>Fydrich</surname></string-name>, <string-name><given-names>B.</given-names>
<surname>Gathmann</surname></string-name>, <string-name><given-names>A. O.</given-names>
<surname>Hamm</surname></string-name>, <string-name><given-names>I.</given-names>
<surname>Heinig</surname></string-name>, <string-name><given-names>M. J.</given-names>
<surname>Herrmann</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Hollandt</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Junghoefer</surname></string-name>, <string-name><given-names>T.</given-names>
<surname>Kircher</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Koelkebeck</surname></string-name>, <string-name><given-names>E. J.</given-names>
<surname>Leehr</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Lotze</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Margraf</surname></string-name>, <string-name><given-names>J. L. M.</given-names>
<surname>Mumm</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Pittig</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Plag</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Richter</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Roesmann</surname></string-name>, <string-name><given-names>I. C.</given-names>
<surname>Ridderbusch</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Schneider</surname></string-name>, <string-name><given-names>H.</given-names>
<surname>Schwarzmeier</surname></string-name>, <string-name><given-names>F.</given-names>
<surname>Seeger</surname></string-name>, <string-name><given-names>N.</given-names>
<surname>Siminski</surname></string-name>, <string-name><given-names>T.</given-names>
<surname>Straube</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Strohle</surname></string-name>, <string-name><given-names>C.</given-names>
<surname>Szeska</surname></string-name>, <string-name><given-names>H. U.</given-names>
<surname>Wittchen</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Wroblewski</surname></string-name>, <string-name><given-names>Y.</given-names>
<surname>Yang</surname></string-name>, <string-name><given-names>B.</given-names>
<surname>Straube</surname></string-name>, <string-name><given-names>U.</given-names>
<surname>Lueken</surname></string-name></person-group>, 
<article-title>Resting-state functional connectivity in anxiety disorders: A multicenter fMRI study</article-title>. <source>Mol. Psychiatry</source>
<volume>30</volume>, 
<fpage>1548</fpage>&#x02013;<lpage>1557</lpage> (<year>2025</year>).<pub-id pub-id-type="pmid">39367057</pub-id>
</mixed-citation></ref></ref-list></back></article>