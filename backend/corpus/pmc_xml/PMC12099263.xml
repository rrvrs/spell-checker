<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><?properties manuscript?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-journal-id">8310780</journal-id><journal-id journal-id-type="pubmed-jr-id">20511</journal-id><journal-id journal-id-type="nlm-ta">IEEE Trans Med Imaging</journal-id><journal-id journal-id-type="iso-abbrev">IEEE Trans Med Imaging</journal-id><journal-title-group><journal-title>IEEE transactions on medical imaging</journal-title></journal-title-group><issn pub-type="ppub">0278-0062</issn><issn pub-type="epub">1558-254X</issn></journal-meta><article-meta><article-id pub-id-type="pmid">39475746</article-id><article-id pub-id-type="pmc">PMC12099263</article-id><article-id pub-id-type="doi">10.1109/TMI.2024.3485554</article-id><article-id pub-id-type="manuscript">NIHMS2066798</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multi-Center Fetal Brain Tissue Annotation (FeTA) Challenge 2022 Results</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-7041-0150</contrib-id><name><surname>Payette</surname><given-names>Kelly</given-names></name><aff id="A1">Center for MR Research, University Children&#x02019;s Hospital Zurich, University of Zurich, 8006 Z&#x000fc;rich, Switzerland</aff><aff id="A2">Department of Perinatal Imaging and Health, King&#x02019;s College London, SE5 9RS London, U.K.</aff></contrib><contrib contrib-type="author"><name><surname>Steger</surname><given-names>C&#x000e9;line</given-names></name><aff id="A3">Center for MR Research, University Children&#x02019;s Hospital Zurich, University of Zurich, 8006 Z&#x000fc;rich, Switzerland</aff><aff id="A4">Neuroscience Center Zurich, University of Zurich, 8057 Z&#x000fc;rich, Switzerland</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-9066-4473</contrib-id><name><surname>Licandro</surname><given-names>Roxane</given-names></name><aff id="A5">Laboratory for Computational Neuroimaging, Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital/Harvard Medical School, Boston, MA 02114 USA</aff><aff id="A6">Computational Imaging Research Laboratory, Department of Biomedical Imaging and Image-Guided Therapy, Medical University of Vienna, 1090 Vienna, Austria</aff></contrib><contrib contrib-type="author"><name><surname>de Dumast</surname><given-names>Priscille</given-names></name><aff id="A7">Medical Image Analysis Laboratory, Department of Diagnostic and Interventional Radiology, Lausanne University Hospital and University of Lausanne, 1011 Lausanne, Switzerland</aff><aff id="A8">Center for Biomedical Imaging (CIBM), 1015 Lausanne, Switzerland</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5328-6407</contrib-id><name><surname>Li</surname><given-names>Hongwei Bran</given-names></name><role>Member, IEEE</role><aff id="A9">Athinoula A. Martinos Center for Biomedical Imaging, Harvard Medical School, Boston, MA 02114 USA</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8257-423X</contrib-id><name><surname>Barkovich</surname><given-names>Matthew</given-names></name><aff id="A10">Department of Radiology and Biomedical Imaging, UCSF Benioff Children&#x02019;s Hospital, University of California, San Francisco, San Francisco, CA 94143 USA</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-2376-8162</contrib-id><name><surname>Li</surname><given-names>Liu</given-names></name><aff id="A11">Computing Department, Imperial College London, SW7 2AZ London, U.K.</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-9012-9606</contrib-id><name><surname>Dannecker</surname><given-names>Maik</given-names></name><aff id="A12">School of Computation, Information and Technology, and School of Medicine and Health, Technical University of Munich, 80333 Munich, Germany</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3525-9755</contrib-id><name><surname>Chen</surname><given-names>Chen</given-names></name><aff id="A13">Computing Department, Imperial College London, SW7 2AZ London, U.K.</aff><aff id="A14">Department of Engineering Science, University of Oxford, OX1 3PJ Oxford, U.K.</aff><aff id="A15">Department of Computer Science, The University of Sheffield, S1 4DP Sheffield, U.K.</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3069-8708</contrib-id><name><surname>Ouyang</surname><given-names>Cheng</given-names></name><aff id="A16">Department of Computing, Imperial College London, SW7 2AZ London, U.K.</aff><aff id="A17">Department of Engineering Science, University of Oxford, OX1 3PJ Oxford, U.K.</aff></contrib><contrib contrib-type="author"><name><surname>McConnell</surname><given-names>Niccol&#x000f2;</given-names></name><aff id="A18">Computer Science Department, Brunel University of London, UB8 3PH Uxbridge, U.K.</aff><aff id="A19">He is now with the Institute of Health Informatics, University College London, WC1E 6BT London, U.K.</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-0068-4495</contrib-id><name><surname>Miron</surname><given-names>Alina</given-names></name><aff id="A20">Computer Science Department, Brunel University of London, UB8 3PH Uxbridge, U.K.</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-1668-2440</contrib-id><name><surname>Li</surname><given-names>Yongmin</given-names></name><aff id="A21">Department of Computer Science, Brunel University of London, UB8 3PH Uxbridge, U.K.</aff></contrib><contrib contrib-type="author"><name><surname>Uus</surname><given-names>Alena</given-names></name><aff id="A22">Biomedical Engineering Department, School of Biomedical Engineering and Imaging Sciences, King&#x02019;s College London, WC2R 2LS London, U.K.</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-9756-3787</contrib-id><name><surname>Grigorescu</surname><given-names>Irina</given-names></name><aff id="A23">Biomedical Engineering Department, School of Biomedical Engineering and Imaging Sciences, King&#x02019;s College London, WC2R 2LS London, U.K.</aff></contrib><contrib contrib-type="author"><name><surname>Gilliland</surname><given-names>Paula Ramirez</given-names></name><aff id="A24">Biomedical Engineering Department, School of Biomedical Engineering and Imaging Sciences, King&#x02019;s College London, WC2R 2LS London, U.K.</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-7145-215X</contrib-id><name><surname>Siddiquee</surname><given-names>Md Mahfuzur Rahman</given-names></name><role>Member, IEEE</role><aff id="A25">School of Computing and Augmented Intelligence, NVIDIA, Bethesda, MD 20814 USA</aff><aff id="A26">Arizona State University, Tempe, AZ 85281 USA</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4621-881X</contrib-id><name><surname>Xu</surname><given-names>Daguang</given-names></name><aff id="A27">NVIDIA, Bethesda, MD 20814 USA</aff></contrib><contrib contrib-type="author"><name><surname>Myronenko</surname><given-names>Andriy</given-names></name><aff id="A28">NVIDIA, Bethesda, MD 20814 USA</aff></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Haoyu</given-names></name><aff id="A29">Medical Robotics, Shanghai Jiao Tong University, Shanghai 200240, China</aff></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Ziyan</given-names></name><aff id="A30">Medical Robotics, Shanghai Jiao Tong University, Shanghai 200240, China</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0667-9889</contrib-id><name><surname>Ye</surname><given-names>Jin</given-names></name><aff id="A31">Shanghai AI Laboratory, Shanghai 200240, China</aff></contrib><contrib contrib-type="author"><name><surname>Aleny&#x000e0;</surname><given-names>Mireia</given-names></name><aff id="A32">BCN-MedTech, Department of Information and Communications Technologies, Universitat Pompeu Fabra, 08002 Barcelona, Spain</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0009-0001-7512-0256</contrib-id><name><surname>Comte</surname><given-names>Valentin</given-names></name><aff id="A33">BCN-MedTech, Department of Information and Communications Technologies, Universitat Pompeu Fabra, 08002 Barcelona, Spain</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5125-6132</contrib-id><name><surname>Camara</surname><given-names>Oscar</given-names></name><aff id="A34">BCN-MedTech, Department of Information and Communications Technologies, Universitat Pompeu Fabra, 08002 Barcelona, Spain</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5484-9056</contrib-id><name><surname>Masson</surname><given-names>Jean-Baptiste</given-names></name><aff id="A35">Institut Pasteur, 75015 Paris, France</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0009-0001-3667-1709</contrib-id><name><surname>Nilsson</surname><given-names>Astrid</given-names></name><aff id="A36">&#x000c9;cole Polytechnique, 91120 Palaiseau, France</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-6995-6647</contrib-id><name><surname>Godard</surname><given-names>Charlotte</given-names></name><aff id="A37">Institut Pasteur, 75015 Paris, France</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-4444-5776</contrib-id><name><surname>Mazher</surname><given-names>Moona</given-names></name><aff id="A38">Centre for Medical Image Computing, Department of Computer Science, University College London, WC1E 6BT London, U.K.</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3102-1595</contrib-id><name><surname>Qayyum</surname><given-names>Abdul</given-names></name><aff id="A39">National Heart and Lung Institute, Imperial College London, SW3 6LY London, U.K.</aff></contrib><contrib contrib-type="author"><name><surname>Gao</surname><given-names>Yibo</given-names></name><aff id="A40">School of Data Science, Fudan University, Shanghai 200437, China</aff></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Hangqi</given-names></name><aff id="A41">School of Data Science, Fudan University, Shanghai 200437, China</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-4567-1636</contrib-id><name><surname>Gao</surname><given-names>Shangqi</given-names></name><aff id="A42">School of Data Science, Fudan University, Shanghai 200437, China</aff></contrib><contrib contrib-type="author"><name><surname>Fu</surname><given-names>Jia</given-names></name><aff id="A43">School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China</aff></contrib><contrib contrib-type="author"><name><surname>Dong</surname><given-names>Guiming</given-names></name><aff id="A44">School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8632-158X</contrib-id><name><surname>Wang</surname><given-names>Guotai</given-names></name><aff id="A45">School of Mechanical and Electrical Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China</aff></contrib><contrib contrib-type="author"><name><surname>Rieu</surname><given-names>ZunHyan</given-names></name><aff id="A46">NEUROPHET Research Institute, Seoul 03080, Republic of Korea</aff></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>HyeonSik</given-names></name><aff id="A47">NEUROPHET Research Institute, Seoul 03080, Republic of Korea</aff></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Minwoo</given-names></name><aff id="A48">NEUROPHET Research Institute, Seoul 03080, Republic of Korea</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-9411-820X</contrib-id><name><surname>P&#x00142;otka</surname><given-names>Szymon</given-names></name><aff id="A49">Sano Centre for Computational Medicine, 30-054 Krak&#x000f3;w, Poland</aff><aff id="A50">Quantitative Healthcare Analysis (qurAI) Group, Informatics Institute, University of Amsterdam, 1012 WP Amsterdam, The Netherlands</aff><aff id="A51">Department of Biomedical Engineering and Physics, Amsterdam University Medical Center, 1007 MB Amsterdam, The Netherlands</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5304-1020</contrib-id><name><surname>Grzeszczyk</surname><given-names>Michal K.</given-names></name><aff id="A52">Sano Centre for Computational Medicine, 30-054 Krak&#x000f3;w, Poland</aff></contrib><contrib contrib-type="author"><name><surname>Sitek</surname><given-names>Arkadiusz</given-names></name><role>Member, IEEE</role><aff id="A53">Center for Advanced Medical Computing and Simulation, Massachusetts General Hospital, Harvard Medical School, Boston, MA 02114 USA</aff></contrib><contrib contrib-type="author"><name><surname>Daza</surname><given-names>Luisa Vargas</given-names></name><aff id="A54">Center for Research and Formation in Artificial Intelligence, Universidad de los Andes, Bogot&#x000e1; 111711, Colombia</aff></contrib><contrib contrib-type="author"><name><surname>Usma</surname><given-names>Santiago</given-names></name><aff id="A55">Center for Research and Formation in Artificial Intelligence, Universidad de los Andes, Bogot&#x000e1; 111711, Colombia</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-5244-2407</contrib-id><name><surname>Arbelaez</surname><given-names>Pablo</given-names></name><aff id="A56">Center for Research and Formation in Artificial Intelligence, Universidad de los Andes, Bogot&#x000e1; 111711, Colombia</aff></contrib><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Wenying</given-names></name><aff id="A57">School of Electronic and Information Engineering, South China University of Technology, Guangzhou 510642, China</aff><aff id="A58">AHU-IAI AI Joint Laboratory, Anhui University, Hefei 230039, China</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8680-1743</contrib-id><name><surname>Zhang</surname><given-names>Wenhao</given-names></name><aff id="A59">School of Electronic and Information Engineering, South China University of Technology, Guangzhou 510642, China</aff></contrib><contrib contrib-type="author"><name><surname>Liang</surname><given-names>Jing</given-names></name><aff id="A60">School of Electronic and Information Engineering, South China University of Technology, Guangzhou 510642, China</aff></contrib><contrib contrib-type="author"><name><surname>Valabregue</surname><given-names>Romain</given-names></name><aff id="A61">CENIR, ICM, INSERM U 1127, CNRS UMR F7225, Sorbonne Universite&#x000ec;, 75006 Paris, France</aff></contrib><contrib contrib-type="author"><name><surname>Joshi</surname><given-names>Anand A.</given-names></name><aff id="A62">Signal and Image Processing Institute, University of Southern California, Los Angeles, CA 90089 USA</aff></contrib><contrib contrib-type="author"><name><surname>Nayak</surname><given-names>Krishna N.</given-names></name><aff id="A63">Signal and Image Processing Institute, University of Southern California, Los Angeles, CA 90089 USA</aff></contrib><contrib contrib-type="author"><name><surname>Leahy</surname><given-names>Richard M.</given-names></name><aff id="A64">Signal and Image Processing Institute, University of Southern California, Los Angeles, CA 90089 USA</aff></contrib><contrib contrib-type="author"><name><surname>Wilhelmi</surname><given-names>Luca</given-names></name><aff id="A65">Center for MR Research, University Children&#x02019;s Hospital Zurich, University of Zurich, 8006 Z&#x000fc;rich, Switzerland</aff></contrib><contrib contrib-type="author"><name><surname>D&#x000e4;ndliker</surname><given-names>Aline</given-names></name><aff id="A66">Center for MR Research, University Children&#x02019;s Hospital Zurich, University of Zurich, 8006 Z&#x000fc;rich, Switzerland</aff></contrib><contrib contrib-type="author"><name><surname>Ji</surname><given-names>Hui</given-names></name><aff id="A67">Center for MR Research, University Children&#x02019;s Hospital Zurich, University of Zurich, 8006 Z&#x000fc;rich, Switzerland</aff><aff id="A68">Neuroscience Center Zurich, University of Zurich, 8057 Z&#x000fc;rich, Switzerland</aff></contrib><contrib contrib-type="author"><name><surname>Gennari</surname><given-names>Antonio G.</given-names></name><aff id="A69">Center for MR Research, University Children&#x02019;s Hospital Zurich, University of Zurich, 8006 Z&#x000fc;rich, Switzerland</aff><aff id="A70">Department of Neuropediatrics, University Children&#x02019;s Hospital Zurich, University of Zurich, 8032 Z&#x000fc;rich, Switzerland</aff></contrib><contrib contrib-type="author"><name><surname>Jakov&#x0010d;i&#x00107;</surname><given-names>Anton</given-names></name><aff id="A71">Croatian Institute for Brain Research, School of Medicine, University of Zagreb, 10000 Zagreb, Croatia</aff></contrib><contrib contrib-type="author"><name><surname>Klai&#x00107;</surname><given-names>Melita</given-names></name><aff id="A72">Croatian Institute for Brain Research, School of Medicine, University of Zagreb, 10000 Zagreb, Croatia</aff></contrib><contrib contrib-type="author"><name><surname>Ad&#x0017e;i&#x00107;</surname><given-names>Ana</given-names></name><aff id="A73">Croatian Institute for Brain Research, School of Medicine, University of Zagreb, 10000 Zagreb, Croatia</aff></contrib><contrib contrib-type="author"><name><surname>Markovi&#x00107;</surname><given-names>Pavel</given-names></name><aff id="A74">Croatian Institute for Brain Research, School of Medicine, University of Zagreb, 10000 Zagreb, Croatia</aff></contrib><contrib contrib-type="author"><name><surname>Grabari&#x00107;</surname><given-names>Gracia</given-names></name><aff id="A75">Croatian Institute for Brain Research, School of Medicine, University of Zagreb, 10000 Zagreb, Croatia</aff></contrib><contrib contrib-type="author"><name><surname>Kasprian</surname><given-names>Gregor</given-names></name><aff id="A76">Department of Biomedical Imaging and Image-Guided Therapy, Division of Neuroradiology and Musculoskeletal Radiology, Medical University of Vienna, 1090 Vienna, Austria</aff></contrib><contrib contrib-type="author"><name><surname>Dovjak</surname><given-names>Gregor</given-names></name><aff id="A77">Department of Biomedical Imaging and Image-Guided Therapy, Division of General and Paediatric Radiology, Medical University of Vienna, 1090 Vienna, Austria</aff></contrib><contrib contrib-type="author"><name><surname>Rados</surname><given-names>Milan</given-names></name><aff id="A78">Croatian Institute for Brain Research, School of Medicine, University of Zagreb, 10000 Zagreb, Croatia</aff></contrib><contrib contrib-type="author"><name><surname>Vasung</surname><given-names>Lana</given-names></name><aff id="A79">Division of Newborn Medicine, Department of Pediatrics, Boston Children&#x02019;s Hospital and the Department of Pediatrics, Harvard Medical School, Boston, MA 02115 USA</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-2730-4285</contrib-id><name><surname>Cuadra</surname><given-names>Meritxell Bach</given-names></name><role>Member, IEEE</role><aff id="A80">Medical Image Analysis Laboratory, Department of Diagnostic and Interventional Radiology, Lausanne University Hospital and University of Lausanne, 1011 Lausanne, Switzerland</aff><aff id="A81">Center for Biomedical Imaging (CIBM), 1015 Lausanne, Switzerland</aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6291-9889</contrib-id><name><surname>Jakab</surname><given-names>Andras</given-names></name><aff id="A82">Center for MR Research, University Children&#x02019;s Hospital Zurich, University of Zurich, 8006 Z&#x000fc;rich, Switzerland</aff><aff id="A83">Neuroscience Center Zurich, University of Zurich, 8057 Z&#x000fc;rich, Switzerland</aff></contrib></contrib-group><author-notes><fn id="FN1"><p id="P1">Meritxell Bach Cuadra and Andras Jakab are co-last authors.</p></fn><corresp id="CR1">Corresponding author: Kelly Payette.</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>4</month><year>2025</year></pub-date><pub-date pub-type="ppub"><month>3</month><year>2025</year></pub-date><pub-date pub-type="epub"><day>17</day><month>3</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>23</day><month>5</month><year>2025</year></pub-date><volume>44</volume><issue>3</issue><fpage>1257</fpage><lpage>1272</lpage><permissions><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions><abstract id="ABS1"><p id="P2">Segmentation is a critical step in analyzing the developing human fetal brain. There have been vast improvements in automatic segmentation methods in the past several years, and the Fetal Brain Tissue Annotation (FeTA) Challenge 2021 helped to establish an excellent standard of fetal brain segmentation. However, FeTA 2021 was a single center study, limiting real-world clinical applicability and acceptance. The multi-center FeTA Challenge 2022 focused on advancing the generalizability of fetal brain segmentation algorithms for magnetic resonance imaging (MRI). In FeTA 2022, the training dataset contained images and corresponding manually annotated multi-class labels from two imaging centers, and the testing data contained images from these two centers as well as two additional unseen centers. The multi-center data included different MR scanners, imaging parameters, and fetal brain super-resolution algorithms applied. 16 teams participated and 17 algorithms were evaluated. Here, the challenge results are presented, focusing on the generalizability of the submissions. Both in- and out-of-domain, the white matter and ventricles were segmented with the highest accuracy (Top Dice scores: 0.89, 0.87 respectively), while the most challenging structure remains the grey matter (Top Dice score: 0.75) due to anatomical complexity. The top 5 average Dices scores ranged from 0.81&#x02013;0.82, the top 5 average 95<sup>th</sup> percentile Hausdorff distance values ranged from 2.3&#x02013;2.5mm, and the top 5 volumetric similarity scores ranged from 0.90&#x02013;0.92. The FeTA Challenge 2022 was able to successfully evaluate and advance generalizability of multi-class fetal brain tissue segmentation algorithms for MRI and it continues to benchmark new algorithms.</p></abstract><kwd-group><kwd>Deep learning</kwd><kwd>domain generalization</kwd><kwd>fetal brain MRI</kwd><kwd>multi-class image segmentation</kwd></kwd-group></article-meta></front><body><sec id="S1"><label>I.</label><title>INTRODUCTION</title><p id="P3">In-utero Magnetic Resonance Imaging (MRI) of the fetal brain allows clinicians and researchers to visualize the development of the human brain. The brain development of fetuses can be investigated with MRI starting in the second trimester up until birth, and can be used in fetuses with both typical neurodevelopment and neurological congenital disorders [<xref rid="R1" ref-type="bibr">1</xref>]. It can aid in the future development of clinical perinatal planning tools for early interventions, treatments, and clinical counseling, and can be used to explore complex neurodevelopment of different structures within the brain. Large-scale acquisition and analysis of <italic toggle="yes">in-utero</italic> fetal brain MRI requires collaboration from specialized clinical centers as image cohorts of various patient populations tend to be small at each center. A crucial step of analyzing these MR images involves quantifying the volume and morphology of different anatomical structures in the developing brain, necessitating image segmentation. Manual segmentation is time-intensive, susceptible to variability between observers and centers, making it impractical for extensive collaborative efforts. However, many challenges exist where the focus is on developing automatic segmentation tools that will work across data from different imaging centers.</p><p id="P4">Existing deep learning-based methods work well when they are tested on similar data to which they were trained on (<italic toggle="yes">i.e.</italic> in-domain data), but struggle when facing testing data that is different from the training data (<italic toggle="yes">i.e.</italic> unseen, or out-of-domain (OOD) data), such as images acquired at another site, with a different scanner, or with different scanning parameters [<xref rid="R2" ref-type="bibr">2</xref>]. Even after careful image processing, classifiers are able to tell the differences between images acquired with different scanners [<xref rid="R3" ref-type="bibr">3</xref>]. Efforts to standardize fetal MRI acquisition parameters across different imaging centers or hospitals have been limited, primarily because fetal imaging relies on specialized sequences that are fine-tuned locally. The appearance of MR images is significantly influenced by various factors, including acquisition parameters, magnetic field strength, MRI coil type, overall imaging setup, and the expertise of the technicians performing the image acquisition. These site differences (or domain shifts) have been shown to be very challenging for deep learning algorithms to handle if there is no similar data in the training dataset [<xref rid="R2" ref-type="bibr">2</xref>], [<xref rid="R4" ref-type="bibr">4</xref>], [<xref rid="R5" ref-type="bibr">5</xref>]. Domain generalizability of automatic segmentation algorithms is an urgent need and is attracting increasing attention in the medical imaging field [<xref rid="R6" ref-type="bibr">6</xref>], [<xref rid="R7" ref-type="bibr">7</xref>], [<xref rid="R8" ref-type="bibr">8</xref>], [<xref rid="R9" ref-type="bibr">9</xref>], [<xref rid="R10" ref-type="bibr">10</xref>].</p><p id="P5">In our previous Fetal Tissue Annotation Challenge (FeTA) 2021, we used the first publicly available dataset of fetal brain MRI data to encourage teams to develop automatic fetal brain tissue segmentation methods [<xref rid="R11" ref-type="bibr">11</xref>]. However, in this dataset, the training and testing datasets were from the same imaging center. For the FeTA Challenge 2022, we launched a multi-center fetal brain segmentation challenge focused on model generalizability across different imaging centers including two unseen centers.</p><p id="P6">Here, we describe the multi-center FeTA Challenge 2022 and its organization as well as give an overview of the submitted algorithms and provide a detailed analysis and evaluation of the challenge results. This paper adheres to the transparent reporting guidelines as described in the BIAS method [<xref rid="R12" ref-type="bibr">12</xref>].</p><p id="P7">The aim of the multi-center FeTA Challenge 2022 is to promote the development of domain-robust algorithms for automatically segmenting high-resolution fetal brain MRI reconstructions between 19&#x02013;35 gestational weeks into seven different classes that works on data from different imaging centers. The challenge includes data from four different imaging centers, further expanding on the FeTA dataset [<xref rid="R13" ref-type="bibr">13</xref>]. Two of the centers are included in the training dataset, and all four imaging centers are included in the hidden testing dataset on which the algorithms were evaluated to test on both seen and unseen data. Examples from each site can be seen in <xref rid="F1" ref-type="fig">Fig. 1</xref>. The algorithms are evaluated on the hidden testing dataset. The submitted algorithms are also tested on various subsets of the testing dataset to determine whether they perform better or worse on data from different imaging centers or under different circumstances such as image quality or reconstruction method.</p><p id="P8">In addition to analyzing the results of the FeTA Challenge 2022, we also propose to investigate the usage of topology as a new evaluation metric for automatic segmentation algorithms. Given that a key downstream analysis of segmentation is the extraction of surface and surface-based metrics (such as thickness and curvature), computational topology of binary masks (<italic toggle="yes">i.e.</italic> connected component, holes) are important to evaluate. We investigate whether topology errors should be added to current evaluation metrics, as challenge evaluation metrics play a significant role in challenge results [<xref rid="R14" ref-type="bibr">14</xref>]. Topology holds particular importance for the analysis of the GM segmentation, which remains one of the most challenging structures to segment in the developing brain and requires a topologically correct segmentation for certain analysis, such as determining gyrification.</p><p id="P9">The algorithms developed as part of the multi-center FeTA Challenge 2022 have the potential to transform both the clinical and research fetal MRI environment, leading to better antenatal and perinatal tools being developed across hospitals and research institutions around the world.</p></sec><sec id="S2"><label>II.</label><title>METHODS</title><sec id="S3"><label>A.</label><title>Challenge Organization</title><p id="P10">The FeTA Challenge 2022 (<ext-link xlink:href="http://feta.grand-challenge.org" ext-link-type="uri">feta.grand-challenge.org</ext-link>) was held in conjunction with the Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022. The challenge was part of a repeated annual event at MICCAI, with a fixed submission deadline. Participants were asked to submit a fully automatic segmentation algorithm that would segment high-resolution fetal brain MRI reconstructions into seven different tissue types: external cerebrospinal fluid (eCSF), grey matter (GM), white matter (WM), ventricles, cerebellum, deep grey matter (deep GM), and brainstem.</p><p id="P11">In addition to the FeTA training dataset, the participants were able to use additional data for training only if it was publicly available and were required to document the usage in their algorithm description. Participants were able to modify the provided training data as well. This modification includes the generation of additional data by image synthesis or various data augmentation strategies (for example, using numerical simulations by FaBiAN [<xref rid="R15" ref-type="bibr">15</xref>]) as long as everything was documented, and the synthetic data could be made available to challenge organizers upon request.</p><p id="P12">All teams with valid submissions and who presented their results at MICCAI 2022 were included in this paper. Each team was allowed three co-authors. Participating teams were able to publish their algorithms and results independently after the challenge, but should cite this challenge paper and the data publication paper [<xref rid="R13" ref-type="bibr">13</xref>].</p><p id="P13">The full results were announced at the MICCAI 2022 conference and were published on the challenge website. The top three teams received custom-made FeTA chocolate bars as a prize. Participating teams were able to choose whether they wished to make their submission public. The Dockers of all submissions with consent to publicly release can be found here: <ext-link xlink:href="https://hub.docker.com/u/fetachallenge22" ext-link-type="uri">https://hub.docker.com/u/fetachallenge22</ext-link>. Each team was required to provide a written description of their algorithm, which can be found in the Supplementary Information [<xref rid="R16" ref-type="bibr">16</xref>].</p><p id="P14">Participants were asked to submit a Docker container containing their fully automatic segmentation algorithm to the organizers via email. Members of the organization committee were allowed to participate but were not eligible for awards. The organizers ran the Docker container on the testing datasets using evaluation code available on the challenge website. No multiple submissions were allowed. Resubmissions were only allowed in cases of technical errors with the Docker.</p><p id="P15">The training dataset was released to participants on June 1, 2022, and the Docker submission deadline was August 3, 2022. The top-performing teams were informed that they were a top-performing team on September 3, 2022, in order for them to prepare a presentation for the day of the challenge. The challenge day was September 18, 2022, where the results were presented at the MICCAI FeTA Challenge 2022 session. For the complete overview of the challenge, see the final challenge proposal [<xref rid="R17" ref-type="bibr">17</xref>].</p></sec><sec id="S4"><label>B.</label><title>Mission of the Challenge</title><p id="P16">The mission of the FeTA Challenge 2022 was to encourage and facilitate the development of generalizable automatic multi-class segmentation algorithms that are able to segment the fetal brain into seven different tissue types plus background from MRI. To achieve this goal, clinically acquired, anonymized MRI data were used to represent the target cohort, pregnant women who underwent fetal MRI. The accuracy of the fetal brain segmentations was evaluated in the challenge cohort. Fetal brain MRI scans were acquired clinically and reconstructed using super-resolution (SR) reconstruction methods. The gestational age (GA), and a label of normal neurodevelopment or pathological neurodevelopment was included for each case in the dataset, and the cases spanned a GA range of 18&#x02013;35 weeks.</p></sec><sec id="S5"><label>C.</label><title>Challenge Dataset</title><p id="P17">The challenge dataset consisted of fetal brain MRI reconstructions acquired from four different imaging centers. Data from two centers (University Children&#x02019;s Hospital (Kispi), Medical University of Vienna (Vienna)) was included in the training dataset, and an additional two centers were included in the testing dataset (Lausanne University Hospital (CHUV), University of San Francisco (USCF)), for a total of four centers. In this challenge, one case consisted of the following: a SR reconstruction of the fetal brain MRI, a manually segmented label map consisting of eight labels (eCSF, GM, WM, ventricles, cerebellum, deep GM, brainstem, background), a GA, and the classification of normal or pathological neurodevelopment. The testing dataset was hidden from participants. In total, there were 120 cases in the training dataset and 160 cases in the testing dataset (see overview in <xref rid="T1" ref-type="table">Table I</xref>). Multiples of 40 for the dataset were used as the original FeTA dataset [<xref rid="R13" ref-type="bibr">13</xref>] contained 40 cases, and the first FeTA challenge [<xref rid="R11" ref-type="bibr">11</xref>] continued this pattern, containing 80 cases. A separate validation dataset was not provided to the participants. The distribution of GAs and the split between normal and pathological neurodevelopment was kept as equal as possible between the two centers included in both the training and testing dataset. For the two unseen imaging centers, a range of GAs, pathologies, and normal neurodevelopmental cases were included to mimic the potential real-world usage of automatic segmentation algorithms. Each case in the dataset was manually segmented using the same method. Several annotators with experience in medical imaging (co-authors (years of experience): AJako (1), MK (1), AA (1), PM (1), GG (1), HJ (5), CS (3), KP (6), AJaka (14)) were trained to segment different labels, and then the individual labels were automatically combined. All segmentations performed by individuals with 1 year of experience were reviewed by the more senior (3 years or more) annotators. Afterwards, three experts in fetal MRI (KP, CS, AJaka) reviewed and corrected each label map, where each case was reviewed by two of the three experts in a two-step process to minimize error. Exact details of the manual segmentation can be found in the supplementary information of [<xref rid="R13" ref-type="bibr">13</xref>]. An overview of the GAs included in the challenge&#x02019;s testing dataset can be found in <xref rid="F2" ref-type="fig">Fig. 2</xref>.</p><sec id="S6"><label>1)</label><title>University Children&#x02019;s Hospital Zurich (Kispi) Data:</title><p id="P18">The training and testing data from FeTA 2021, acquired at the University Children&#x02019;s Hospital Zurich (Kispi), was used in FeTA 2022, and a detailed description of the image acquisition parameters, post-processing steps, and ethical approval information can be found in [<xref rid="R11" ref-type="bibr">11</xref>]. A clinically acquired dataset of 120 brain scans (80 training cases and 40 testing cases) was used as part of the Kispi portion of the FeTA dataset. Several T2-weighted single shot Fast Spin Echo (ssFSE) images were acquired for each subject in all three planes with a reconstructed resolution of 0.5 &#x000d7; 0.5 &#x000d7; 3&#x02013;5mm<sup>3</sup>. The images were acquired on either a 1.5T or 3T clinical GE whole-body MRI scanners (Signa Discovery MR450 and MR750) without the use of maternal or fetal sedation using an 8-channel cardiac or body coil with the following sequence parameters: TR: 2000&#x02013;3500ms, TE: 120ms (minimum), flip angle: 90&#x000b0;, sampling percentage 55%. Field of view (200&#x02013;240mm<sup>2</sup><italic toggle="yes">)</italic> and image matrix (1.5T: 256 &#x000d7; 224; 3T: 320 &#x000d7; 224) were adjusted depending on the GA and size of the fetus. The data was acquired at the University Children&#x02019;s Hospital Zurich in Zurich, Switzerland by trained radiographers using clinically defined protocols.</p><p id="P19">Fetal brain SR reconstructions were performed on the acquired datasets, with a training/testing split of 40/20 using both the mial-srtk method [<xref rid="R18" ref-type="bibr">18</xref>], [<xref rid="R19" ref-type="bibr">19</xref>] and the simple-irtk method [<xref rid="R20" ref-type="bibr">20</xref>]. After reconstruction, each fetal brain volume had an isotropic resolution of approximately 0.5mm<sup>3</sup>, with some deviation in exact dimensions between the SR methods. Each reconstructed image was then histogram-matched using Slicer [<xref rid="R21" ref-type="bibr">21</xref>], and zero-padded to be 256 &#x000d7; 256 &#x000d7; 256 voxels. The testing cases were considered in-domain, as this site provides both training and testing cases.</p></sec><sec id="S7"><label>2)</label><title>University of Vienna (Vienna) Data:</title><p id="P20">The data from the Medical University of Vienna (Vienna) was acquired using 1.5 T (Philips Ingenia/Intera, Best, the Netherlands) and 3 T magnets (Philips Achieva, Best, the Netherlands), without the use of maternal or fetal sedation. All acquisitions were performed using a five-channel cardiac coil. For each case, at least 3 T2-weighted ssFSE sequences (TE=80&#x02013;140ms, TR=6000&#x02013;22000ms) in 3 orthogonal (axial, coronal, sagittal) planes with reference to the fetal brain stem axis and/or the axis of the corpus callosum were acquired. Overall, slice thickness was between 3mm and 5mm (gap 0.3&#x02013;1mm), pixel spacing 0.65&#x02013;1.17mm, acquisition time between 13.46 and 41.19 seconds.</p><p id="P21">The preprocessing pipeline [<xref rid="R22" ref-type="bibr">22</xref>] consisted of a data denoising step [<xref rid="R23" ref-type="bibr">23</xref>], followed by an in-plane super resolution [<xref rid="R24" ref-type="bibr">24</xref>] and automatic brain masking step [<xref rid="R25" ref-type="bibr">25</xref>] and concluded with a single 0.5 mm<sup>3</sup> isotropic slice-wise motion correction and volumetric SR reconstruction [<xref rid="R25" ref-type="bibr">25</xref>]. Subsequently, the resulting volumes were rigidly aligned to a common reference space [<xref rid="R26" ref-type="bibr">26</xref>].</p><p id="P22">Fetal MRI cases were provided by the Medical University of Vienna. The data was acquired as part of a retrospective single-center study and was anonymized and approved by the ethics review board and data clearing department at the Medical University of Vienna, responsible for validating data privacy and sharing regulation compliance. There were 40 training cases and 40 testing cases included in the FeTA Challenge 2022 from this site. As with the Kispi data, these testing cases were considered in-domain, as this site provided both training and testing data.</p></sec><sec id="S8"><label>3)</label><title>Lausanne University Hospital (CHUV) Data:</title><p id="P23">The data from the Lausanne University Hospital (CHUV) was acquired at 1.5T (MAGNETOM Aera, Siemens Healthcare, Erlangen, Germany), without the use of maternal or fetal sedation. Acquisitions were performed with an 18-channel body coil and a 32-channel spine coil. Images were acquired using T2-weighted (T2W) Half-Fourier Acquisition Single-shot Turbo spin Echo (HASTE) sequences in the three orthogonal orientations (axial, sagittal, coronal); usually at least two acquisitions were performed in each orientation., TR/TE, 1200ms/90ms; flip angle, 90&#x000b0;, echo train length, 224; echo spacing, 4.08ms; field-of-view, 360 &#x000d7; 360mm<sup>2</sup>; voxel size, 1.13 &#x000d7; 1.13 &#x000d7; 3.00mm<sup>3</sup>; inter-slice gap, 10%, acquisition time between 26 to 36 seconds.</p><p id="P24">For each subject, the scans were manually reviewed and the good quality scans were chosen for SR reconstruction, creating a 3D SR volume of brain morphology [<xref rid="R18" ref-type="bibr">18</xref>]. Each case was zero-padded to 256 &#x000d7; 256 &#x000d7; 256 and reoriented to a standard viewing plane. Mothers of all other fetuses included in the current work were scanned as part of their routine clinical care. Data was retrospectively collected from acquisitions done between January 2013 to April 2021. All images were anonymized. This dataset was part of a larger research protocol approved by the ethics committee of the Canton de Vaud (decision number CER-VD 2021&#x02013;00124) for re-use of their data for research purposes and approval for the release of an anonymous dataset for non-medical reproducible research and open science purposes. As no training cases were included from this site, the 40 testing cases were considered out-of-domain.</p></sec><sec id="S9"><label>4)</label><title>University of California San Francisco (UCSF) Data:</title><p id="P25">The data from the University of California (UCSF) was acquired using 3T GE Discovery MR750 or MR750W (wide bore) without the use of maternal or fetal sedation. Acquisitions were performed using a 32 channel GE cardiac coil. At least 3 T2-weighted ssFSE sequences were acquired with one scan per orientation (sagittal, axial, coronal) with the following parameters: 240 mm<sup>2</sup> FOV with 512 &#x000d7; 512 matrix gives in plane resolution of ~0.5 &#x000d7; 0.5 mm<sup>2</sup> with 3 mm slice thickness. TR is 2000&#x02013;3500 ms, TE &#x0003e; 100 ms, 90&#x000b0; flip angle.</p><p id="P26">For each subject, the scans were manually reviewed and the good quality scans were chosen for SR reconstruction, creating a 3D SR volume of brain morphology [<xref rid="R25" ref-type="bibr">25</xref>]. Each case was zero-padded to 256 &#x000d7; 256 &#x000d7; 256 and reoriented to a standard viewing plane.</p><p id="P27">Fetal MRI was acquired during routine clinical care with institutional review board approval for anonymized retrospective analysis by the FeTA team (IRB 21&#x02013;35930). As no training cases were included from this site, the 40 testing cases were considered out-of-domain.</p></sec></sec><sec id="S10"><label>D.</label><title>Evaluation Metrics</title><p id="P28">Three complementary types of evaluation metrics were used to compute the rankings. The overlap was quantified with the dice similarity coefficient (DSC) [<xref rid="R27" ref-type="bibr">27</xref>]. The similarity between the two volumes was quantified with the volume similarly measure (VS) [<xref rid="R27" ref-type="bibr">27</xref>]. The contours were evaluated with a boundary-distance-based metric: the 95th percentile of the Hausdorff distance (HD95) (<ext-link xlink:href="https://github.com/deepmind/surface-distance" ext-link-type="uri">https://github.com/deepmind/surface-distance</ext-link>). As the task is a segmentation task, the DSC was chosen, as it was the most popular segmentation metric. However, we were not only interested in the overlap, but also the shape and volume, as they are often used as clinical biomarkers. Therefore, we included the HD95 (shape) and VS (volume) metrics. The final rankings took all three metrics into account.</p></sec><sec id="S11"><label>E.</label><title>Ranking</title><p id="P29">The ranking method was the same as in FeTA 2021 [<xref rid="R28" ref-type="bibr">28</xref>]. Each of the participating teams was ranked based on each evaluation metric, and then the final rankings combined the rankings from all of the metrics (DSC, HD95, VS) for the complete dataset (both in- and out-of-domain imaging site). The DSC, HD95, and VS were calculated for each label within each of the corresponding predicted label maps of the fetal brain volumes in the complete testing dataset. The mean and standard deviation of each label for all test cases was calculated, and the participating algorithms were ranked from low to high (HD95), where the lowest score received the highest scoring rank (best), and from high to low (DSC, VS), where the highest value received highest scoring rank (best) based on the calculated mean across all labels and test cases. If there were missing results, the worst possible value was used. For example, if a label did not exist in the new segmentation label map but was present in the ground truth (GT) label map, it received a DSC and VS score of 0, and the HD95 score was double the max value of the other algorithms submitted. This ranking procedure was developed to take three different metric types equally into account.</p><p id="P30">Finally, the results of the challenge were run through the ChallengeR toolkit, specifically designed to calculate and display imaging challenge results [<xref rid="R29" ref-type="bibr">29</xref>].</p><p id="P31">Additional rankings were created based on the in-domain and OOD imaging centers, cases with and without neurological pathologies, and image reconstruction quality (Excellent, Good, Poor). These additional rankings were not part of the determination of the winner of the challenge but were presented at the FeTA Challenge 2022 event.</p></sec><sec id="S12"><label>F.</label><title>Topology Analysis</title><p id="P32">In addition to the rankings mentioned in the previous section, we assessed the topological correctness as an evaluation metric of the predicted label maps. Topology defines the properties of an object that are preserved through deformation [<xref rid="R30" ref-type="bibr">30</xref>]. Given binary maps (tissue labels), computational topology relies on connectivity of a voxel to its neighbours to quantify the number of connected components, holes, or cavities. Topology is relevant for exploring brain tissue segmentations as topological correctness is needed to quantify biomarkers important for brain development such as cortical thickness and gyration. However, fetal cortical segmentations (GM) are often discontinuous [<xref rid="R31" ref-type="bibr">31</xref>], [<xref rid="R32" ref-type="bibr">32</xref>], [<xref rid="R33" ref-type="bibr">33</xref>] but surprisingly topology correctness of predicted segmentations is rarely reported [<xref rid="R34" ref-type="bibr">34</xref>], [<xref rid="R35" ref-type="bibr">35</xref>]. Here, we propose a global topology-integrative ranking (TIR) of the methods, which includes the Betti Number Error (BNE) ranking in addition to the current three evaluation metrics (DSC, HD95, VS).</p><p id="P33">To quantitatively compare the topology of each segmented structure, we assessed the error of the topological invariant Betti numbers, also known as the BNE. The <italic toggle="yes">k</italic>-dimensional Betti numbers (<italic toggle="yes">BN</italic><sub><italic toggle="yes">k</italic></sub><italic toggle="yes">)</italic> count the topological structures in each dimension <italic toggle="yes">k.</italic>More <italic toggle="yes">s</italic>pecifically, <italic toggle="yes">BN</italic><sub>0</sub>, <italic toggle="yes">BN</italic><sub>1</sub>, <italic toggle="yes">BN</italic><sub>2</sub> represented the number of connected components, the number of holes and the number of cavities in the 3D binary object respectively. We define the <italic toggle="yes">k</italic>-dimensional Betti number error (<italic toggle="yes">BN E</italic><sub><italic toggle="yes">k</italic></sub><italic toggle="yes">)</italic> as the absolute difference of the GT expected value and the prediction measure. <italic toggle="yes">BN E</italic><sub><italic toggle="yes">k</italic></sub> are difference metrics that must be minimized. The GT expected values are as follows: the <italic toggle="yes">BN</italic><sub>1</sub> = 0 and <italic toggle="yes">BN</italic><sub>2</sub> = 0 for all brain tissue labels. For eCSF, WM, ventricles, cerebellum, deep GM, and brainstem, <italic toggle="yes">BN</italic><sub>0</sub> = 1, and for GM, <italic toggle="yes">BN</italic><sub>0</sub> = 2.</p><p id="P34">When performing the evaluation of the predicted label maps, when there was an absence of segmentation for a tissue, it was attributed twice the value of the worst performing segmentation of the same label over all submissions, in line with how missing data was handled with the HD95 evaluation metric. Once topology was quantified, we also computed the ranking of methods for each <italic toggle="yes">BN E</italic> and TIR ranking with Challenge R toolkit [<xref rid="R29" ref-type="bibr">29</xref>].</p></sec></sec><sec id="S13"><label>III.</label><title>RESULTS</title><sec id="S14"><label>A.</label><title>Challenge Submissions</title><p id="P35">There were 17 submissions from 16 different teams to the FeTA Challenge 2022. One team submitted two algorithms, but they were determined to be substantially different methodologies and as such was allowed. Each team submitted a written description of their algorithm, which can be found in the Supplementary Information [<xref rid="R16" ref-type="bibr">16</xref>]. Two teams used only one institution&#x02019;s dataset rather than the complete training dataset (deepsynth, ajoshiusc). All other teams used the complete training dataset. Seven teams used additional publicly available datasets for pre-training or training (FIT_1, FMRSK, symsense, FIT_2, DBC Pasteur, fudan_zmic, deepsynth).</p><p id="P36">All submitted models relied on deep learning. Only three teams used 2D networks (fudan_zmic, DBC Pasteur, ajoshiusc), the remainder of the teams used 3D networks. All teams used PyTorch, or PyTorch-based solutions (such as nnU-Net [<xref rid="R36" ref-type="bibr">36</xref>] or MONAI [<xref rid="R37" ref-type="bibr">37</xref>]) for their network. Many teams used a two-step strategy for segmentation (often classified as &#x02018;coarse-to-fine&#x02019;). This often involved first segmenting the brain from the outlying maternal tissue, and then segmenting the fetal brain into different tissues. Each algorithm is summarized in further detail in <xref rid="T2" ref-type="table">Table II</xref>. Institutional ranking differences in the submissions can be found in <xref rid="F3" ref-type="fig">Fig. 3</xref>.</p></sec><sec id="S15"><label>B.</label><title>In-Domain Results</title><p id="P37">In-domain evaluation was defined based on the performance on the subset of data including the Kispi and Vienna data, as data from these two imaging centers were represented in the training dataset available to the participants. A summary of the in-domain evaluation metrics for all teams can be seen in the top row of <xref rid="F4" ref-type="fig">Fig. 4</xref>. We report two aspects of the in-domain evaluation results. Firstly, we present in-domain team rankings and an in-depth evaluation of the FeTA Challenge 2022 results. Secondly, we cross-reference these rankings with the outcomes achieved in the FeTA Challenge 2021 [<xref rid="R28" ref-type="bibr">28</xref>]. Notably, the Kispi data included in the FeTA2022 is identical to the FeTA 2021 training dataset (80 cases).</p><p id="P38">In the overall ranking of the in-domain dataset, the top three submissions were <italic toggle="yes">NVAUTO</italic>, <italic toggle="yes">FIT_2</italic> and <italic toggle="yes">FIT_1</italic>. Specifically, <italic toggle="yes">FIT_1</italic> (0.8052), <italic toggle="yes">symsense</italic>(0.8047), and <italic toggle="yes">NVAUTO</italic>(0.8042) were the top three teams according to the DSC. The top three submissions according to the HD95 were <italic toggle="yes">FIT_2</italic>(2.31mm), <italic toggle="yes">Institut_Pasteur_DBC</italic>(2.40mm), and <italic toggle="yes">NVAUTO</italic> (2.46mm).</p><p id="P39">The top three submissions according to the VS were <italic toggle="yes">NVAUTO</italic> (0.914), <italic toggle="yes">symsense</italic> (0.910) and <italic toggle="yes">FIT_2</italic> (0.910). It is worth noting that no statistically significant differences were found in the rankings for the achieved DSC scores in the first four teams, (<italic toggle="yes">FIT_1</italic>, <italic toggle="yes">symsense</italic>, <italic toggle="yes">NVAUTO</italic>, <italic toggle="yes">Neurophet</italic>). In the HD95, the first ranked submission, <italic toggle="yes">FIT_2</italic>, was significantly better performing than the second ranked (<italic toggle="yes">Institut_Pasteur_DBC</italic>), while the top two teams in VS (<italic toggle="yes">NVAUTO</italic> and <italic toggle="yes">symsense)</italic> were not significantly different. Further details about the individual rankings are shown in <xref rid="F4" ref-type="fig">Fig. 4</xref>. Similar to the FeTA 2021 Challenge, a performance plateau was observed in the DSC scores, with approximately the first 12 teams achieving very similar DSC scores (DSC range for the top 12 teams: 0.765 &#x02013; 0.805), with a large drop off in scores in the last five submissions (DSC range for the last 5 teams: 0.455 &#x02013; 0.684). A similar trend was observed for the mean HD95 (highest ranked 9 submissions: 2.31 to 2.83 mm, lowest ranked 8 submissions: 3.5 to 41 mm) and for the mean VS scores (highest ranked 11 submissions: 0.902 &#x02013; 0.914, lowest ranked 6 submissions: 0.611 &#x02013; 0.880).</p><p id="P40">Not all anatomical structures were segmented equally well, which is reflected by the heterogeneity of mean DSC, HD95 and VS scores obtained in the in-domain evaluation. The WM and ventricles were the structures most successfully segmented. The mean DSC for the top three submissions for the WM were 0.885 (<italic toggle="yes">FIT_1</italic>), 0.883 (<italic toggle="yes">symsense</italic>) and 0.882 (<italic toggle="yes">Blackbean</italic>), and the ventricles were 0.889 (<italic toggle="yes">NVAUTO</italic>), 0.889 (<italic toggle="yes">symsense</italic>) and 0.888 (<italic toggle="yes">FIT_1</italic>). On the other hand, the GM was segmented rather poorly, as the mean DSC for the top three submissions were 0.726 (<italic toggle="yes">FIT_1</italic>), 0.725 (<italic toggle="yes">NVAUTO</italic>) and 0.724 (<italic toggle="yes">Neurophet</italic>). The eCSF spaces, which neighbor the GM, were similarly poorly segmented.</p><p id="P41">Compared to the FeTA Challenge 2021 results, segmentation accuracy improved marginally. The highest DSC in the FeTA Challenge 2022 in-domain evaluations was 0.805, while it was 0.786 in 2021. The lowest HD95 in the FeTA2022 in-domain evaluation was 2.31 mm, while it was 14 voxels in 2021. These two metrics are not directly comparable due to the change in evaluation tool and unit between the years, as the tool used in FeTA 2021 was not ideal when outliers were present. The highest average VS in the FeTA Challenge 2022 was 0.914, while it was 0.885 in 2021. In-domain, the per-label comparisons yielded similar results: the GM and the eCSF being the most difficult to segment, while the WM and the ventricles were the best performing. There were two teams who submitted to both FeTA 2021 and FeTA 2022 who ranked very well in the in-domain evaluation: <italic toggle="yes">NVAUTO</italic> and <italic toggle="yes">Neurophet</italic>. <italic toggle="yes">NVAUTO</italic> maintained a top in-domain ranking in both years in all three evaluation criteria (2021: DSC 1st place, HD95 1st place, VS 2nd place, 2022: DSC 3rd place, HD95 3rd place, VS 1st place), as did <italic toggle="yes">Neurophet</italic> (2021: DSC 3rd place, VS 5th place, 2022: DSC 4th place, HD95 4th place).</p></sec><sec id="S16"><label>C.</label><title>Inter-Site Generalizability Assessment: Out-of-Domain Results</title><p id="P42">Here, we evaluated the performance of the submissions on unseen datasets (i.e. on data that was not present in the training dataset). Therefore, the OOD performance rankings are presented using the CHUV and the UCSF testing data subset and compared with the in-domain results. A summary of the OOD evaluation metrics for all teams can be seen in the bottom row of <xref rid="F4" ref-type="fig">Fig. 4</xref>.</p><p id="P43">Some submissions demonstrated equivalent performance for both the in-domain and OOD subsets such as <italic toggle="yes">FIT_1</italic> (ranked 3<sup>rd</sup> in-domain and 2<sup>nd</sup> OOD), <italic toggle="yes">Symsense</italic> (ranking 4<sup>th</sup> for both in-domain and OOD), or Dolphins (ranking 9<sup>th</sup> for both in-domain and OOD). Interestingly, some methods ranked better in the OOD subset, such as <italic toggle="yes">BlueBrune,</italic> which rose from 6<sup>th</sup> place in-domain to 3<sup>rd</sup> place OOD, or <italic toggle="yes">Blackbean</italic> who rose from 7<sup>th</sup> rank in-domain to 4<sup>th</sup> OOD. However, some models dropped considerably in performance such as <italic toggle="yes">FIT_2</italic> (from 2<sup>nd</sup> to 7<sup>th</sup><italic toggle="yes">)</italic>, <italic toggle="yes">NVAUTO</italic> (from 3<sup>rd</sup> to 6<sup>th</sup>, performing poorly in many OOD cases, see <xref rid="F4" ref-type="fig">Fig. 4</xref> bottom row) or Neurophet (from 4<sup>th</sup> to 13<sup>th</sup><italic toggle="yes">)</italic>. This indicated that the domain shift present in data from different imaging centers can drastically degrade model performance when being deployed in heterogenous clinical datasets.</p><p id="P44">Overall, the median performance metrics in the OOD setting remained equivalent to the in-domain, and many of the models attained a plateau of performance around 0.80, 2.5, 0.90 in DSC, HD95 and VS respectively. However, the median of the worst performing methods dropped by a large amount (dropping to approximately 0 for DSC, or 0.25 for VS) while in-domain median performance never reached such low scores (always above 0.50 and 0.75 for DSC and VS respectively for all methods).</p><p id="P45">Not all brain tissue labels were equivalent when comparing in-domain and OOD results. Class-wise performance (see Supplementary Information, Section 12 [<xref rid="R16" ref-type="bibr">16</xref>]) indicated that major drops of performance occur in ventricles (in DSC, HD95, VS), and GM and WM volume (in VS). The achieved performance by top ranking algorithms in the other tissues (eCSF, deep GM, cerebellum, brainstem) were even slightly higher OOD than in-domain (e.g. DSC range of 0.83 to 0.36 OOD while 0.76 to 0.04 in-domain).</p></sec><sec id="S17"><label>D.</label><title>Global Ranking</title><p id="P46">The global ranking was the ranking as defined by using the complete testing dataset from all four imaging centers. The global ranking was the official ranking which determined the winners of the FeTA Challenge 2022.</p><p id="P47">Examples of results from the top 5 teams can be found in <xref rid="F5" ref-type="fig">Fig. 5</xref>. The team rankings of each evaluation metric can be seen in <xref rid="F6" ref-type="fig">Fig. 6</xref>, and the rankings based on the different labels can be found in <xref rid="F7" ref-type="fig">Fig. 7</xref>. The final rankings can be found in <xref rid="T3" ref-type="table">Table III</xref>.</p><p id="P48">The top three teams were <italic toggle="yes">FIT_1</italic>, <italic toggle="yes">Bluebrune</italic>, and <italic toggle="yes">FMRSK</italic> (with <italic toggle="yes">Bluebrune</italic> and <italic toggle="yes">FMRSK</italic> tied for second). <italic toggle="yes">FIT_1</italic> maintained a top 5 ranking across each of the labels, while the rankings were variable for all other teams across the different brain tissues. A plateau in the performance of the top 10&#x02013;12 teams was observed, in line with the in-domain and out-of-domain results. The top three global DSC scores were from teams <italic toggle="yes">FIT_1</italic> (0.816), <italic toggle="yes">symsense</italic> (0.813), and <italic toggle="yes">Bluebrune</italic>(0.812). The top three global HD95 scores were from <italic toggle="yes">FIT_1</italic> (2.35mm), <italic toggle="yes">Bluebrune</italic> (2.38mm), and <italic toggle="yes">Institute_Pasteur_DBC</italic> (2.39mm). The top three global VS scores were from team <italic toggle="yes">FMRSK</italic> (0.920), <italic toggle="yes">NVAUTO</italic> (0.915), and <italic toggle="yes">FIT_2</italic> (0.913).</p><p id="P49">In order to investigate factors which may have influenced the ratings, we looked at rankings based on quality ratings of the testing dataset (Excellent=3, Good=2, Poor=1, median rating by 3 experienced raters: MBC, AGG, AJaka), normal and pathological brains, as well as rankings based on the SR reconstruction algorithm used (NiftyMIC, mial-srtk, irtk-simple).</p><p id="P50">For the excellent quality fetal brain reconstructions, the top three teams were <italic toggle="yes">FIT_1</italic>, <italic toggle="yes">FMRSK</italic>, and 4 teams tied for 3<sup>rd</sup> (<italic toggle="yes">symsense, NVAUTO, Blackbean, BlueBrune</italic>). The &#x02018;Good Quality&#x02019; top three teams were <italic toggle="yes">FIT_1</italic>, <italic toggle="yes">FMRSK</italic>, and <italic toggle="yes">NVAUTO</italic>, and &#x02018;Low Quality&#x02019; were <italic toggle="yes">BlueBrune</italic>, <italic toggle="yes">NVAUTO</italic>, and <italic toggle="yes">FIT_1</italic>. The top three teams for fetal brains with the normal classification were <italic toggle="yes">FMRSK</italic>, <italic toggle="yes">FIT_1</italic>, and <italic toggle="yes">NVAUTO</italic> and for pathology were <italic toggle="yes">FIT_1</italic>, <italic toggle="yes">BlueBrune</italic>, and <italic toggle="yes">FMRSK</italic>. The top three teams for fetal brains reconstructed with the irtk-simple algorithm [<xref rid="R20" ref-type="bibr">20</xref>] were <italic toggle="yes">deepsynth</italic>, <italic toggle="yes">FMRSK</italic>, and <italic toggle="yes">ajoshiusc</italic>; with mial-srtk algorithm [<xref rid="R18" ref-type="bibr">18</xref>] were <italic toggle="yes">FMRSK</italic>, <italic toggle="yes">NVAUTO</italic>, and <italic toggle="yes">fudanzmic</italic>; and with the NiftyMIC algorithm [<xref rid="R25" ref-type="bibr">25</xref>] were <italic toggle="yes">FIT_1</italic>, <italic toggle="yes">BlueBrune,</italic> and <italic toggle="yes">Blackbean</italic>. When separating the rankings based on individual labels, <italic toggle="yes">BlueBrune</italic> was the top-ranking team for the eCSF, <italic toggle="yes">NVAUTO</italic> ranked first for the GM, <italic toggle="yes">FMRSK</italic> ranked first for the brainstem, and <italic toggle="yes">FIT_1</italic> was the top team for the remaining labels (WM, ventricles, cerebellum, deep GM). A complete overview of the rankings per label can be found in <xref rid="F7" ref-type="fig">Fig. 7</xref> as well as in the Supplementary Information [<xref rid="R16" ref-type="bibr">16</xref>].</p></sec><sec id="S18"><label>E.</label><title>Topological Analysis Results</title><p id="P51"><xref rid="T4" ref-type="table">Table IV (A)</xref> presents the BNE rankings of the submissions for each dimension <italic toggle="yes">k</italic> &#x02208; {0,1,2} and the global TIR. The topology rankings were similar across the three <italic toggle="yes">k</italic>-dimensional BNEs, with a maximum rank difference of less than three with one exception: <italic toggle="yes">FMRSK</italic> presented a relatively big change in its BNE rankings of dimension 1 (rank=4) and 2 (rank=13). Potentially, such inter-dimension variation may come from tissue-specific errors. Interestingly, <italic toggle="yes">hilab,</italic> which does not perform well in <italic toggle="yes">BN E</italic><sub>0</sub> (rank = 10) and <italic toggle="yes">BN E</italic><sub>1</sub> (rank = 11), is the best performing submission in <italic toggle="yes">BN E</italic><sub>2</sub> (rank=8). Nonetheless, the good <italic toggle="yes">BN E</italic><sub>2</sub> performance was not sufficient to pass on to the global BNE ranking.</p><p id="P52">Changes in the TIR (see <xref rid="T4" ref-type="table">Table IV(B)</xref>) were small compared to the global challenge ranking without topology, with a maximum of one rank difference, with one exception. Team <italic toggle="yes">Blackbean</italic> moved from rank 5 in the standard global FeTA ranking to rank 3 in the TIR. The winner and second-place teams remained the same.</p><p id="P53"><xref rid="T5" ref-type="table">Table V</xref> presents the global topology BNE ranking of the submissions per tissue class. The TIR of the individual tissues varied within each team&#x02019;s algorithm. For instance, <italic toggle="yes">hilab</italic> ranked first for the eCSF, but 13<sup>th</sup> in the WM. Examples of good and bad topology results in the GM can be found in <xref rid="F8" ref-type="fig">Fig. 8</xref></p><p id="P54">Apart from the top 2 teams according to topology (<italic toggle="yes">FIT_1</italic> and <italic toggle="yes">BlueBrune</italic>), only <italic toggle="yes">Blackbean</italic> and <italic toggle="yes">Dolphins</italic> were ranked in the upper half for all tissue class. The average tissue TIR of <italic toggle="yes">Blackbean</italic> was 3.3, while <italic toggle="yes">FMRSK</italic> (who tied for second place in the global FeTA ranking) ranked on average 9.1 based on the individual tissue BNE rankings.</p></sec></sec><sec id="S19"><label>IV.</label><title>DISCUSSION</title><p id="P55">The practical value of MRI segmentation methods in clinical settings depends on their ability to effectively generalize to previously unseen data. MRI acquisition settings and various post-processing methods, including image reconstruction, may increase differences between images of the developing fetal brain across imaging sites. Additionally, the overall image quality tends to be lower in comparison to MRI scans of the adult human brain, leading to less distinct delineation of anatomical structures.</p><sec id="S20"><label>A.</label><title>Generalizability of Submitted Algorithms</title><p id="P56">Our results have shown that generalizability across multiple sites remains a challenge for fetal brain MRI segmentation, but resources such as multi-site datasets have the potential to improve the performance of such methods. For example, the top scoring team of the Kispi dataset (ajoshiusc) did not train on the second site&#x02019;s available training dataset, and subsequently performed poorly on the data from the three additional sites, leading us to assume that this network was overfitted. For some methods (but not all), there seemed to be a preference for a given SR method in the rankings. The winning team (<italic toggle="yes">FIT_1</italic>) ranked first in the Vienna and UCSF datasets, which were both reconstructed with the NiftyMIC SR algorithm. Teams <italic toggle="yes">NVAUTO</italic> and <italic toggle="yes">FMRSK</italic> performed similarly well on the CHUV and Kispi dataset, which included reconstructions performed using the mial-srtk SR method.</p><p id="P57">Our findings further indicate that image augmentation is a critical factor in achieving good domain generalization. Traditional techniques (i.e. affine transformations, contrast adjustments) have demonstrated their effectiveness within established segmentation frameworks, including nnU-Net. However, the optimum choice of augmentation techniques remains unclear. As highlighted in <xref rid="T2" ref-type="table">Table II</xref>, it is noteworthy that the leading teams, especially <italic toggle="yes">FMRSK</italic> and <italic toggle="yes">FIT_1</italic>, utilized random bias field and motion artifact augmentations (such as MR spikes). Such data augmentation strategies are specific to MRI images and can mimic potential real-world image differences between scanners and centers. A deeper analysis of the top-performing teams approaches reveals that style and photometric augmentations (contrast, blur, sharpness, etc.), known for their ability to induce significant intensity distribution variations, could be pivotal for enhancing model generalization in fetal MRI. This concept aligns with previous research into generalizable cardiac structure segmentation [<xref rid="R38" ref-type="bibr">38</xref>], [<xref rid="R39" ref-type="bibr">39</xref>]. Importantly, a potential trade-off between in-domain and OOD data generalization should be acknowledged [<xref rid="R40" ref-type="bibr">40</xref>]. For instance, the <italic toggle="yes">NVAUTO</italic> team, which scored first place in the in-domain data performance, did not use any specialized domain generalization techniques, yet fell to fourth place for OOD data. Notable in <italic toggle="yes">NVAUTO</italic>&#x02019;s solution is that they used ensembling with 15 models. Conversely, <italic toggle="yes">FIT_1</italic>, initially third for in-domain data, rose to first place in the overall ranking, underscoring the indispensability of domain generalization in the development of robust image segmentation models. <italic toggle="yes">FIT_1</italic> also integrated ensembling in their solution, but with 5 models, plus an additional model that handled post-processing. It would be interesting to test if <italic toggle="yes">FIT_1</italic>&#x02019;s would improve if they had also used a 15-model ensemble strategy, especially considering the time and energy it takes to train such a large number of models.</p><p id="P58">Interestingly, the performance metrics of the OOD images for some algorithms were not worse than the metrics for the in-domain images. <xref rid="F4" ref-type="fig">Fig. 4</xref> shows that the range of evaluation metrics for the in-domain results was much larger than the OOD results. This is primarily driven by the quality of the fetal brain reconstructions, as the average quality ratings of the OOD datasets (UCSF: 2.33; CHUV: 2.35) were higher than the average in-domain dataset quality ratings (Kispi: 2.18, Vienna: 1.95). Therefore, the quality of the fetal brain reconstructions played a large role in the success of the automatic segmentations.</p></sec><sec id="S21"><label>B.</label><title>Overview of Top Three Teams</title><p id="P59">The top scoring teams all incorporated explicitly domain-robust solutions for the multi-site task. Teams <italic toggle="yes">FIT_1</italic> and <italic toggle="yes">Bluebrune</italic> both incorporated domain generalization strategies into their networks. Specifically, <italic toggle="yes">FIT_1</italic> used Painter by Numbers for style transfer training and <italic toggle="yes">Bluebrune</italic> used a domain adversarial approach in their training strategy. Transformer models did not seem to considerably help with domain generalization in our challenge data, as the highest-scoring team with a transformer model was <italic toggle="yes">Blackbean</italic>, who ranked 5<sup>th</sup> and used a transformer model (ViT) as well as an nnU-Net. The results of the FeTA22 challenge indicate that existing model architectures and specialized data augmentation strategies can be used successfully to generalize segmentation networks. Short descriptions of the top three submissions can be found in the following sections, and the complete algorithm descriptions can be found in [<xref rid="R16" ref-type="bibr">16</xref>].</p><sec id="S22"><label>1)</label><title><italic toggle="yes">FIT_1</italic>: Team <italic toggle="yes">FIT_1</italic>used a three-step process.</title><p id="P60">Firstly, they used data-augmentation-based domain generalization, followed by network ensembling, and an output-level denoising autoencoder that corrects implausible predicted segmentations [<xref rid="R41" ref-type="bibr">41</xref>]. <italic toggle="yes">FIT_1</italic> used nnU-Net as the network framework [<xref rid="R36" ref-type="bibr">36</xref>], and they trained five different models, each with their own data augmentation strategy. Model 1 used the default nnU-Net data augmentation steps. Model 2 added bias-field augmentation, an MR-specific data augmentation step. Model 3 adds style augmentation to the default nnU-Net and random bias field augmentations. Model 4 uses photometric augmentation, and model 5 uses MR-specific motion artifacts from moving subjects, simulated by TorchIO. These 5 networks were then ensembled based on average logit predictions. The final post-processing step is a rule-based post-processing with a denoising autoencoder (DAE), which takes the ensembled result as an input, and outputs a refined segmentation. The DAE model was trained using a self-generated dataset with noisy segmentations, and randomly dropping out features. The DAE was found to improve the segmentations when the input was poor, therefore the DAE step was only utilized with the large changes to the predictions occurred. In the training process, they generated three synthetic datasets for validation based on 24 cases, using default nnU-Net augmentation, random style augmentation [<xref rid="R42" ref-type="bibr">42</xref>], and bias-field augmentation [<xref rid="R38" ref-type="bibr">38</xref>].</p></sec><sec id="S23"><label>2)</label><title>Bluebrune:</title><p id="P61">Team <italic toggle="yes">Bluebrune</italic> utilized a domain adversarial approach [<xref rid="R43" ref-type="bibr">43</xref>] to train this network, with nnU-Net as the framework. Their method involves two steps: a 3D nnU-Net segmentation network [<xref rid="R36" ref-type="bibr">36</xref>], followed by a domain discriminator network. The goal of the discriminator is to determine which site the input originates from (as there were two sites in the training data). They trained two different discriminator models, the first of which takes the outputted feature map before the soft-max layer from the 3D nnU-Net segmentation network as input in order to learn domain invariant features. The second discriminator takes the outputted feature map from the bottleneck layer of the Segmentation network as input in order to learn domain invariant features in the U-Net&#x02019;s encoder. The discriminator networks output domain-class labels, and there is a gradient reversal layer inserted just before the discriminator networks, ensuring that the gradient passing to the segmentation networks is negative during backpropagation, ensuring adversarial network training. The two networks are trained separately. The discriminator networks are only used for training, and the two nnU-Net segmentation networks are used for inference and are combined with the softmax prediction average. They used standard nnU-Net preprocessing and data augmentation steps.</p></sec><sec id="S24"><label>3)</label><title>FMRSK:</title><p id="P62">Team <italic toggle="yes">FMRSK</italic> took on a semi-supervised approach to train the networks. Firstly, they trained a standard 3D U-Net (using the MONAI framework [<xref rid="R37" ref-type="bibr">37</xref>]) to perform brain extraction. Next, they reviewed and rated the training data, scoring each case based on the quality of the labels. They then used the high-quality labels to train an initial Attention U-Net (again using the MONAI framework), and created labels with this network for the remaining cases. Manual corrections were made on these predicted labels. This iterative process was repeated three times. They utilized standard data augmentation steps, plus MR Spikes, bias fields, and random intensity shifts. They used an external dataset, using 19 developing Human Connectome Project (dHCP) neonates [<xref rid="R44" ref-type="bibr">44</xref>] who were scanned between 26.6&#x02013;32.4 weeks, as well as a spina bifida atlas [<xref rid="R45" ref-type="bibr">45</xref>]. For training the final dataset, they used both the brain-extracted dataset and a version of the dataset with more of the surrounding structures, as well as flipping. They trained two attention U-Nets and averaged the predictions from the two models.</p></sec></sec><sec id="S25"><label>C.</label><title>Individual Labels</title><p id="P63">The top team (<italic toggle="yes">FIT_1</italic>) performed extraordinarily well across all labels, ranking first for four out of seven labels. The rankings of the other top teams were not as stable when looking at the individual fetal brain tissue labels, and no pattern could be found. As in the global rankings, the OOD volumes typically had better evaluation metrics for each individual label, apart from the ventricles and brainstem. It is uncertain why these two labels trended differently when compared to the remaining labels. The GM remains challenging to segment, which supports the importance of maintaining topology in automatic segmentations. The deep GM and brainstem were more challenging labels to segment, likely as the structures are not as clearly defined as there is not a strong demarcation and the difference in intensity from surrounding structures is reduced (unlike in structures such as the ventricles).</p></sec><sec id="S26"><label>D.</label><title>Topological Analysis</title><p id="P64">In the analysis of topology as an evaluation metric, we demonstrated the importance of considering topology in the assessment and comparison of automatic segmentation methods. In the TIR algorithms ranking, the inclusion of a topology-based metric did not drastically change the final results, although minor updates are observed, and the across-tissue reliability of FMRSK in topological accuracy was rewarded.</p></sec></sec><sec id="S27"><label>V.</label><title>CONCLUSION</title><p id="P65">Our first multi-centric fetal brain segmentation challenge has demonstrated that overall, automated fetal brain segmentation has improved since the first FeTA Challenge [<xref rid="R11" ref-type="bibr">11</xref>]. However, there is still room for improvement in the segmentation of certain structures, especially in the cortical GM.</p><p id="P66">Despite the new challenges relating to generalizability, algorithms and training strategies have not changed drastically since FeTA 2021. All entries used deep learning and primarily 3D architecture. nnU-Net remained a popular and effective tool for medical image segmentation, and the most popular loss functions were the DSC loss and cross-entropy loss, or a combination of the two. Extensive data augmentation strategies are an integral aspect of training, and the addition of external datasets did not lead to better results. As with FeTA 2021, the top performing teams demonstrated a plateau in performance, likely due to the quality of both the SR algorithms and the quality of the manual segmentations.</p><p id="P67">Our challenge has demonstrated that the inclusion of just one additional institution (Vienna) into the training dataset, algorithms can improve their generalizability. Future research directions should focus on enhancing the generalizability of the methods, including the emerging low-field fetal MRI acquisitions [<xref rid="R46" ref-type="bibr">46</xref>], [<xref rid="R47" ref-type="bibr">47</xref>], or exploring federated learning approaches. In that context, conducting a more comprehensive evaluation of the impact of data augmentation and possible biases due to SR reconstruction methods would be very valuable. Furthermore, addressing challenges associated with inaccurate voxel-wise annotations and establishing standards of minimal image quality requirements [<xref rid="R48" ref-type="bibr">48</xref>], [<xref rid="R49" ref-type="bibr">49</xref>], [<xref rid="R50" ref-type="bibr">50</xref>] should be a priority. These endeavors would be crucially important to increase the clinical acceptance of automated fetal brain MRI segmentation.</p></sec></body><back><ack id="S28"><title>ACKNOWLEDGMENT</title><p id="P68">This work was supported in part by the University Research Priority Program (URPP) Adaptive Brain Circuits in Development and Learning (AdaBD) Project; in part by the Vontobel Foundation; in part by the Anna M&#x000fc;ller Grocholski Foundation; in part by the EMDO Foundation; in part by the Prof. Dr. Max Clo&#x000eb;tta Foundation; in part by the Swiss National Science Foundation under Grant SNSF 320030_184932 and Grant 205321&#x02013;182602; in part by Austrian Science Fund under Grant FWF [P 35189-B, I 3925-B27]; in part by Vienna Science and Technology Fund under Grant WWTF [LS20&#x02013;030]; in part by the facilities and expertise of the Center for Biomedical Imaging (CIBM) Center for Biomedical Imaging, a Swiss Research Center of Excellence Founded and Supported by Lausanne University Hospital (CHUV); in part by the University of Lausanne (UNIL); in part by the &#x000c9;cole Polytechnique F&#x000e9;d&#x000e9;rale de Lausanne (EPFL); in part by the University of Geneva (UNIGE); in part by Geneva University Hospitals (HUG); and in part by the Engineering and Physical Sciences Research Council (EPSRC) under Grant EP/V034537/1.</p><p id="P69">This work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and protocols was granted by the Zurich Kantonale Ethikkommission (KEK) under Application Nos. 2016&#x02013;01019 and 2017&#x02013;00167, the Institutional Ethical Boards at AKH (University Hospital Vienna) and the Medical University of Vienna under Application No. EK-Nummer 580_2010, the Commission cantonale d&#x02019;&#x000e9;thique de la recherche sur l&#x02019;&#x000ea;tre humain under Application No. CER-VD 2021&#x02013;00124, and IRB UCSF under Application No. 21&#x02013;35930.</p></ack><ref-list><title>REFERENCES</title><ref id="R1"><label>[1]</label><mixed-citation publication-type="journal"><name><surname>Gholipour</surname><given-names>A</given-names></name>
<etal/>, &#x0201c;<article-title>Fetal MRI: A technical update with educational aspirations</article-title>,&#x0201d; <source>Concepts Magn. Reson. A</source>, vol. <volume>43</volume>, no. <issue>6</issue>, pp. <fpage>237</fpage>&#x02013;<lpage>266</lpage>, <month>Nov</month>. <year>2014</year>.</mixed-citation></ref><ref id="R2"><label>[2]</label><mixed-citation publication-type="journal"><name><surname>Yan</surname><given-names>W</given-names></name>
<etal/>, &#x0201c;<article-title>MRI manufacturer shift and adaptation: Increasing the generalizability of deep learning segmentation for MR images acquired with different scanners</article-title>,&#x0201d; <source>Radiol., Artif. Intell</source>, vol. <volume>2</volume>, no. <issue>4</issue>, <month>Jul</month>. <year>2020</year>, <fpage>Art. no. e190195</fpage>.</mixed-citation></ref><ref id="R3"><label>[3]</label><mixed-citation publication-type="journal"><name><surname>Glocker</surname><given-names>B</given-names></name>, <name><surname>Robinson</surname><given-names>R</given-names></name>, <name><surname>Castro</surname><given-names>DC</given-names></name>, <name><surname>Dou</surname><given-names>Q</given-names></name>, and <name><surname>Konukoglu</surname><given-names>E</given-names></name>, &#x0201c;<article-title>Machine learning with multi-site imaging data: An empirical study on the impact of scanner effects</article-title>,&#x0201d; <year>2019</year>, <source>arXiv</source>:<volume>1910</volume>.<fpage>04597</fpage>.</mixed-citation></ref><ref id="R4"><label>[4]</label><mixed-citation publication-type="journal"><name><surname>Zech</surname><given-names>JR</given-names></name>, <name><surname>Badgeley</surname><given-names>MA</given-names></name>, <name><surname>Liu</surname><given-names>M</given-names></name>, <name><surname>Costa</surname><given-names>AB</given-names></name>, <name><surname>Titano</surname><given-names>JJ</given-names></name>, and <name><surname>Oermann</surname><given-names>EK</given-names></name>, &#x0201c;<article-title>Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study</article-title>,&#x0201d; <source>PLOS Med</source>, vol. <volume>15</volume>, no. <issue>11</issue>, <month>Nov</month>. <year>2018</year>, <fpage>Art. no. e1002683</fpage>.</mixed-citation></ref><ref id="R5"><label>[5]</label><mixed-citation publication-type="journal"><name><surname>Guan</surname><given-names>H</given-names></name> and <name><surname>Liu</surname><given-names>M</given-names></name>, &#x0201c;<article-title>Domain adaptation for medical image analysis: A survey</article-title>,&#x0201d; <source>IEEE Trans. Biomed. Eng</source>, vol. <volume>69</volume>, no. <issue>3</issue>, pp. <fpage>1173</fpage>&#x02013;<lpage>1185</lpage>, <month>Mar</month>. <year>2022</year>.<pub-id pub-id-type="pmid">34606445</pub-id>
</mixed-citation></ref><ref id="R6"><label>[6]</label><mixed-citation publication-type="journal"><name><surname>Campello</surname><given-names>VM</given-names></name>
<etal/>, &#x0201c;<article-title>Multi-centre, multi-vendor and multi-disease cardiac segmentation: The M&#x00026;Ms challenge</article-title>,&#x0201d; <source>IEEE Trans. Med. Imag</source>, vol. <volume>40</volume>, no. <issue>12</issue>, pp. <fpage>3543</fpage>&#x02013;<lpage>3554</lpage>, <month>Dec</month>. <year>2021</year>.</mixed-citation></ref><ref id="R7"><label>[7]</label><mixed-citation publication-type="journal"><name><surname>Sun</surname><given-names>Y</given-names></name>
<etal/>, &#x0201c;<article-title>Multi-site infant brain segmentation algorithms: The iSeg-2019 challenge</article-title>,&#x0201d; <source>IEEE Trans. Med. Imag</source>, vol. <volume>40</volume>, no. <issue>5</issue>, pp. <fpage>1363</fpage>&#x02013;<lpage>1376</lpage>, <month>May</month>
<year>2021</year>.</mixed-citation></ref><ref id="R8"><label>[8]</label><mixed-citation publication-type="journal"><name><surname>Bento</surname><given-names>M</given-names></name>, <name><surname>Fantini</surname><given-names>I</given-names></name>, <name><surname>Park</surname><given-names>J</given-names></name>, <name><surname>Rittner</surname><given-names>L</given-names></name>, and <name><surname>Frayne</surname><given-names>R</given-names></name>, &#x0201c;<article-title>Deep learning in large and multi-site structural brain MR imaging datasets</article-title>,&#x0201d; <source>Frontiers Neuroinform</source>., vol. <volume>15</volume>, <month>Jan</month>. <year>2022</year>, <fpage>Art. no. 805669</fpage>.</mixed-citation></ref><ref id="R9"><label>[9]</label><mixed-citation publication-type="journal"><name><surname>Eche</surname><given-names>T</given-names></name>, <name><surname>Schwartz</surname><given-names>LH</given-names></name>, <name><surname>Mokrane</surname><given-names>F-Z</given-names></name>, and <name><surname>Dercle</surname><given-names>L</given-names></name>, &#x0201c;<article-title>Toward generalizability in the deployment of artificial intelligence in radiology: Role of computation stress testing to overcome underspecification</article-title>,&#x0201d; <source>Radiol., Artif. Intell</source>, vol. <volume>3</volume>, no. <issue>6</issue>, <month>Nov</month>. <year>2021</year>, <fpage>Art. no. e210097</fpage>.</mixed-citation></ref><ref id="R10"><label>[10]</label><mixed-citation publication-type="journal"><name><surname>Mart&#x000ed;n-Isla</surname><given-names>C</given-names></name>
<etal/>, &#x0201c;<article-title>Deep learning segmentation of the right ventricle in cardiac MRI: The M&#x00026;Ms challenge</article-title>,&#x0201d; <source>IEEE J. Biomed. Health Informat</source>, vol. <volume>27</volume>, no. <issue>7</issue>, pp. <fpage>3302</fpage>&#x02013;<lpage>3313</lpage>, <month>Jul</month>. <year>2023</year>.</mixed-citation></ref><ref id="R11"><label>[11]</label><mixed-citation publication-type="journal"><name><surname>Payette</surname><given-names>K</given-names></name>
<etal/>, &#x0201c;<article-title>Fetal brain tissue annotation and segmentation challenge results</article-title>,&#x0201d; <source>Med. Image Anal</source>, vol. <volume>88</volume>, <month>Aug</month>. <year>2023</year>, <fpage>Art. no. 102833</fpage>.</mixed-citation></ref><ref id="R12"><label>[12]</label><mixed-citation publication-type="journal"><name><surname>Maier-Hein</surname><given-names>L</given-names></name>
<etal/>, &#x0201c;<article-title>BIAS: Transparent reporting of biomedical image analysis challenges</article-title>,&#x0201d; <source>Med. Image Anal</source>, vol. <volume>66</volume>, <month>Dec</month>. <year>2020</year>, <fpage>Art. no. 101796</fpage>.</mixed-citation></ref><ref id="R13"><label>[13]</label><mixed-citation publication-type="journal"><name><surname>Payette</surname><given-names>K</given-names></name>
<etal/>, &#x0201c;<article-title>An automatic multi-tissue human fetal brain segmentation benchmark using the fetal tissue annotation dataset</article-title>,&#x0201d; <source>Sci. Data</source>, vol. <volume>8</volume>, no. <issue>1</issue>, p. <fpage>167</fpage>, <month>Jul</month>. <year>2021</year>.<pub-id pub-id-type="pmid">34230489</pub-id>
</mixed-citation></ref><ref id="R14"><label>[14]</label><mixed-citation publication-type="journal"><name><surname>Reinke</surname><given-names>A</given-names></name>
<etal/>, &#x0201c;<article-title>How to exploit weaknesses in biomedical challenge design and organization</article-title>,&#x0201d; in <source>Proc. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</source>, <year>2018</year>, pp. <fpage>388</fpage>&#x02013;<lpage>395</lpage>.</mixed-citation></ref><ref id="R15"><label>[15]</label><mixed-citation publication-type="journal"><name><surname>Lajous</surname><given-names>H</given-names></name>
<etal/>, <article-title>&#x0201c;A fetal brain magnetic resonance acquisition numerical phantom (FaBiAN)</article-title>,&#x0201d; <source>Sci. Rep</source>., vol. <volume>12</volume>, no. <issue>1</issue>, p. <fpage>8682</fpage>, <month>May</month>
<year>2022</year>.<pub-id pub-id-type="pmid">35606398</pub-id>
</mixed-citation></ref><ref id="R16"><label>[16]</label><mixed-citation publication-type="journal"><name><surname>Payette</surname><given-names>K</given-names></name>
<etal/>, &#x0201c;<article-title>Supplementary information for the fetal tissue annotation 2022 challenge results</article-title>,&#x0201d; <source>Eur. Org. Nucl. Res., CERN, Zenodo, Tech. Rep</source>., <month>Feb</month>. <year>2024</year>, doi: <pub-id pub-id-type="doi">10.5281/zenodo.10628647</pub-id>. [Online]. Available: <ext-link xlink:href="https://zenodo.org/records/10628648" ext-link-type="uri">https://zenodo.org/records/10628648</ext-link></mixed-citation></ref><ref id="R17"><label>[17]</label><mixed-citation publication-type="journal"><name><surname>Payette</surname><given-names>K</given-names></name>
<etal/>, &#x0201c;<article-title>Fetal tissue annotation challenge</article-title>,&#x0201d; <source>Eur. Org. Nucl. Res., CERN, Zenodo, Tech. Rep</source>., <month>Mar</month>. <year>2022</year>, doi: <pub-id pub-id-type="doi">10.5281/zenodo.6683366</pub-id>.</mixed-citation></ref><ref id="R18"><label>[18]</label><mixed-citation publication-type="journal"><name><surname>Tourbier</surname><given-names>S</given-names></name>, <name><surname>Bresson</surname><given-names>X</given-names></name>, <name><surname>Hagmann</surname><given-names>P</given-names></name>, <name><surname>Thiran</surname><given-names>J-P</given-names></name>, <name><surname>Meuli</surname><given-names>R</given-names></name>, and <name><surname>Cuadra</surname><given-names>MB</given-names></name>, &#x0201c;<article-title>An efficient total variation algorithm for super-resolution in fetal brain MRI with adaptive regularization</article-title>,&#x0201d; <source>NeuroImage</source>, vol. <volume>118</volume>, pp. <fpage>584</fpage>&#x02013;<lpage>597</lpage>, <month>Sep</month>. <year>2015</year>.<pub-id pub-id-type="pmid">26072252</pub-id>
</mixed-citation></ref><ref id="R19"><label>[19]</label><mixed-citation publication-type="journal"><name><surname>Deman</surname><given-names>P</given-names></name>, <name><surname>Tourbier</surname><given-names>S</given-names></name>, <name><surname>Meuli</surname><given-names>R</given-names></name>, and <name><surname>Cuadra</surname><given-names>MB</given-names></name>, &#x0201c;<article-title>Meribach/mevislabFetalMRI: MEVISLAB MIAL super-resolution reconstruction of fetal brain MRI v1.0</article-title>,&#x0201d; <source>Eur. Org. Nucl. Res., CERN, Zenodo, Tech. Rep</source>., <month>Jun</month>. <day>5</day>, <year>2020</year>, doi: <pub-id pub-id-type="doi">10.5281/zenodo.3878564</pub-id>.</mixed-citation></ref><ref id="R20"><label>[20]</label><mixed-citation publication-type="journal"><name><surname>Kuklisova-Murgasova</surname><given-names>M</given-names></name>, <name><surname>Quaghebeur</surname><given-names>G</given-names></name>, <name><surname>Rutherford</surname><given-names>MA</given-names></name>, <name><surname>Hajnal</surname><given-names>JV</given-names></name>, and <name><surname>Schnabel</surname><given-names>JA</given-names></name>, &#x0201c;<article-title>Reconstruction of fetal brain MRI with intensity matching and complete outlier removal</article-title>,&#x0201d; <source>Med. Image Anal</source>, vol. <volume>16</volume>, no. <issue>8</issue>, pp. <fpage>1550</fpage>&#x02013;<lpage>1564</lpage>, <month>Dec</month>. <year>2012</year>.<pub-id pub-id-type="pmid">22939612</pub-id>
</mixed-citation></ref><ref id="R21"><label>[21]</label><mixed-citation publication-type="book"><name><surname>Kikinis</surname><given-names>R</given-names></name>, <name><surname>Pieper</surname><given-names>SD</given-names></name>, and <name><surname>Vosburgh</surname><given-names>KG</given-names></name>, &#x0201c;<part-title>3D slicer: A platform for subject-specific image analysis, visualization, and clinical support</part-title>,&#x0201d; in <source>Intraoperative Imaging and Image-Guided Therapy</source>. <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2014</year>, pp. <fpage>277</fpage>&#x02013;<lpage>289</lpage>.</mixed-citation></ref><ref id="R22"><label>[22]</label><mixed-citation publication-type="journal"><name><surname>Schwartz</surname><given-names>E</given-names></name>
<etal/>, &#x0201c;<article-title>The prenatal morphomechanic impact of agenesis of the corpus callosum on human brain structure and asymmetry</article-title>,&#x0201d; <source>Cerebral Cortex</source>, vol. <volume>31</volume>, no. <issue>9</issue>, pp. <fpage>4024</fpage>&#x02013;<lpage>4037</lpage>, <month>Sep</month>. <year>2021</year>.<pub-id pub-id-type="pmid">33872347</pub-id>
</mixed-citation></ref><ref id="R23"><label>[23]</label><mixed-citation publication-type="journal"><name><surname>Coupe</surname><given-names>P</given-names></name>, <name><surname>Yger</surname><given-names>P</given-names></name>, <name><surname>Prima</surname><given-names>S</given-names></name>, <name><surname>Hellier</surname><given-names>P</given-names></name>, <name><surname>Kervrann</surname><given-names>C</given-names></name>, and <name><surname>Barillot</surname><given-names>C</given-names></name>, &#x0201c;<article-title>An optimized blockwise nonlocal means denoising filter for 3-D magnetic resonance images</article-title>,&#x0201d; <source>IEEE Trans. Med. Imag</source>, vol. <volume>27</volume>, no. <issue>4</issue>, pp. <fpage>425</fpage>&#x02013;<lpage>441</lpage>, <month>Apr</month>. <year>2008</year>.</mixed-citation></ref><ref id="R24"><label>[24]</label><mixed-citation publication-type="confproc"><name><surname>Dong</surname><given-names>C</given-names></name>, <name><surname>Loy</surname><given-names>CC</given-names></name>, and <name><surname>Tang</surname><given-names>X</given-names></name>, &#x0201c;<source>Accelerating the super-resolution convolutional neural network</source>,&#x0201d; in <conf-name>Proc. Eur. Conf. Comput. Vis. (ECCV)</conf-name>, <year>2016</year>, pp. <fpage>391</fpage>&#x02013;<lpage>407</lpage>.</mixed-citation></ref><ref id="R25"><label>[25]</label><mixed-citation publication-type="journal"><name><surname>Ebner</surname><given-names>M</given-names></name>
<etal/>, &#x0201c;<article-title>An automated framework for localization, segmentation and super-resolution reconstruction of fetal brain MRI</article-title>,&#x0201d; <source>NeuroImage</source>, vol. <volume>206</volume>, <month>Feb</month>. <year>2020</year>, <fpage>Art. no. 116324</fpage>.<pub-id pub-id-type="pmid">31704293</pub-id>
</mixed-citation></ref><ref id="R26"><label>[26]</label><mixed-citation publication-type="journal"><name><surname>Gholipour</surname><given-names>A</given-names></name>
<etal/>, &#x0201c;<article-title>A normative spatiotemporal MRI atlas of the fetal brain for automatic segmentation and analysis of early brain growth</article-title>,&#x0201d; <source>Sci. Rep</source>, vol. <volume>7</volume>, no. <issue>1</issue>, p. <fpage>476</fpage>, <month>Mar</month>. <year>2017</year>.<pub-id pub-id-type="pmid">28352082</pub-id>
</mixed-citation></ref><ref id="R27"><label>[27]</label><mixed-citation publication-type="journal"><name><surname>Taha</surname><given-names>AA</given-names></name> and <name><surname>Hanbury</surname><given-names>A</given-names></name>, &#x0201c;<article-title>Metrics for evaluating 3D medical image segmentation: Analysis, selection, and tool</article-title>,&#x0201d; <source>BMC Med. Imag</source>, vol. <volume>15</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>&#x02013;<lpage>28</lpage>, <month>Aug</month>. <year>2015</year>.</mixed-citation></ref><ref id="R28"><label>[28]</label><mixed-citation publication-type="journal"><name><surname>Payette</surname><given-names>K</given-names></name>
<etal/>, &#x0201c;<article-title>Fetal brain tissue annotation and segmentation challenge results</article-title>,&#x0201d; <year>2022</year>, <source>arXiv</source>:<volume>2204</volume>.<fpage>09573</fpage>.</mixed-citation></ref><ref id="R29"><label>[29]</label><mixed-citation publication-type="journal"><name><surname>Wiesenfarth</surname><given-names>M</given-names></name>
<etal/>, &#x0201c;<article-title>Methods and open-source toolkit for analyzing and visualizing challenge results</article-title>,&#x0201d; <source>Sci. Rep</source>, vol. <volume>11</volume>, no. <issue>1</issue>, p. <fpage>2369</fpage>, <month>Jan</month>. <year>2021</year>.<pub-id pub-id-type="pmid">33504883</pub-id>
</mixed-citation></ref><ref id="R30"><label>[30]</label><mixed-citation publication-type="book"><name><surname>Rote</surname><given-names>G</given-names></name> and <name><surname>Vegter</surname><given-names>G</given-names></name>, &#x0201c;<part-title>Computational topology: An introduction</part-title>,&#x0201d; in <source>Effective Computational Geometry for Curves and Surfaces</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2006</year>, pp. <fpage>277</fpage>&#x02013;<lpage>312</lpage>.</mixed-citation></ref><ref id="R31"><label>[31]</label><mixed-citation publication-type="journal"><name><surname>Dou</surname><given-names>H</given-names></name>
<etal/>, &#x0201c;<article-title>A deep attentive convolutional neural network for automatic cortical plate segmentation in fetal MRI</article-title>,&#x0201d; <source>IEEE Trans. Med. Imag</source>, vol. <volume>40</volume>, no. <issue>4</issue>, pp. <fpage>1123</fpage>&#x02013;<lpage>1133</lpage>, <month>Apr</month>. <year>2021</year>.</mixed-citation></ref><ref id="R32"><label>[32]</label><mixed-citation publication-type="confproc"><name><surname>Fetit</surname><given-names>AE</given-names></name>
<etal/>, &#x0201c;<source>A deep learning approach to segmentation of the developing cortex in fetal brain MRI with minimal manual labeling</source>,&#x0201d; in <conf-name>Proc. 3rd Conf. Med. Imag. Deep Learn</conf-name>., <year>2020</year>, vol. 121, pp. <fpage>241</fpage>&#x02013;<lpage>261</lpage>.</mixed-citation></ref><ref id="R33"><label>[33]</label><mixed-citation publication-type="journal"><name><surname>Hong</surname><given-names>J</given-names></name>
<etal/>, &#x0201c;<article-title>Fetal cortical plate segmentation using fully convolutional networks with multiple plane aggregation</article-title>,&#x0201d; <source>Frontiers Neurosci</source>, vol. <volume>14</volume>, p. <fpage>1226</fpage>, <month>Dec</month>. <year>2020</year>.</mixed-citation></ref><ref id="R34"><label>[34]</label><mixed-citation publication-type="journal"><name><surname>Caldairou</surname><given-names>B</given-names></name>
<etal/>, &#x0201c;<article-title>Segmentation of the cortex in fetal MRI using a topological model</article-title>,&#x0201d; in <source>Proc. IEEE Int. Symp. Biomed. Imag., Nano Macro</source>, <month>Mar</month>. <year>2011</year>, pp. <fpage>2045</fpage>&#x02013;<lpage>2048</lpage>.</mixed-citation></ref><ref id="R35"><label>[35]</label><mixed-citation publication-type="confproc"><name><surname>de Dumast</surname><given-names>P</given-names></name>, <name><surname>Kebiri</surname><given-names>H</given-names></name>, <name><surname>Atat</surname><given-names>C</given-names></name>, <name><surname>Dunet</surname><given-names>V</given-names></name>, <name><surname>Koob</surname><given-names>M</given-names></name>, and <name><surname>Cuadra</surname><given-names>MB</given-names></name>, &#x0201c;<source>Segmentation of the cortical plate in fetal brain MRI with a topological loss</source>,&#x0201d; in <conf-name>Proc. Uncertainty Safe Utilization Mach. Learn. Med. Imag., Perinatal Imag., Placental Preterm Image Anal., 3rd Int. Workshop (UNSURE), 6th Int. Workshop (PIPPI), Held Conjunct. (MICCAI)</conf-name>, <conf-loc>Strasbourg, France</conf-loc>, <month>Oct</month>. <year>2021</year>, pp. <fpage>200</fpage>&#x02013;<lpage>209</lpage>.</mixed-citation></ref><ref id="R36"><label>[36]</label><mixed-citation publication-type="journal"><name><surname>Isensee</surname><given-names>F</given-names></name>, <name><surname>Jaeger</surname><given-names>PF</given-names></name>, <name><surname>Kohl</surname><given-names>SAA</given-names></name>, <name><surname>Petersen</surname><given-names>J</given-names></name>, and <name><surname>Maier-Hein</surname><given-names>KH</given-names></name>, &#x0201c;<article-title>NnU-Net: A self-configuring method for deep learning-based biomedical image segmentation</article-title>,&#x0201d; <source>Nature Methods</source>, vol. <volume>18</volume>, no. <issue>2</issue>, pp. <fpage>203</fpage>&#x02013;<lpage>211</lpage>, <month>Feb</month>. <year>2021</year>.<pub-id pub-id-type="pmid">33288961</pub-id>
</mixed-citation></ref><ref id="R37"><label>[37]</label><mixed-citation publication-type="journal"><article-title>MONAI: Medical Open Network for AI, MONAI Consortium</article-title>, <source>Eur. Org. Nucl. Res., CERN, Zenodo</source>, <month>Mar</month>. <year>2020</year>. [Online]. Available: <ext-link xlink:href="https://zenodo.org/records/4323059" ext-link-type="uri">https://zenodo.org/records/4323059</ext-link></mixed-citation></ref><ref id="R38"><label>[38]</label><mixed-citation publication-type="confproc"><name><surname>Chen</surname><given-names>C</given-names></name>
<etal/>, &#x0201c;<source>Realistic adversarial data augmentation for MR image segmentation</source>,&#x0201d; in <conf-name>Proc. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</conf-name>, <publisher-name>Cham</publisher-name>, <publisher-loc>Switzerland</publisher-loc>, <year>2020</year>, pp. <fpage>667</fpage>&#x02013;<lpage>677</lpage>.</mixed-citation></ref><ref id="R39"><label>[39]</label><mixed-citation publication-type="confproc"><name><surname>Chen</surname><given-names>C</given-names></name>, <name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Ouyang</surname><given-names>C</given-names></name>, <name><surname>Sinclair</surname><given-names>M</given-names></name>, <name><surname>Bai</surname><given-names>W</given-names></name>, and <name><surname>Rueckert</surname><given-names>D</given-names></name>, &#x0201c;<source>MaxStyle: Adversarial style composition for robust medical image segmentation</source>,&#x0201d; in <conf-name>Proc. 25th Int. Conf. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</conf-name>, <conf-loc>Berlin, Germany</conf-loc>, <month>Sep</month>. <year>2022</year>, pp. <fpage>151</fpage>&#x02013;<lpage>161</lpage>.</mixed-citation></ref><ref id="R40"><label>[40]</label><mixed-citation publication-type="confproc"><name><surname>Chattopadhyay</surname><given-names>P</given-names></name>, <name><surname>Balaji</surname><given-names>Y</given-names></name>, and <name><surname>Hoffman</surname><given-names>J</given-names></name>, &#x0201c;<source>Learning to balance specificity and invariance for in and out of domain generalization</source>,&#x0201d; in <conf-name>Proc. 16th Eur. Conf. Comput. Vis</conf-name>., <publisher-loc>Glasgow, U.K</publisher-loc>. <publisher-name>Cham, Switzerland: Springer</publisher-name>, <month>Aug</month>. <year>2020</year>, pp. <fpage>301</fpage>&#x02013;<lpage>318</lpage>.</mixed-citation></ref><ref id="R41"><label>[41]</label><mixed-citation publication-type="confproc"><name><surname>Gondara</surname><given-names>L</given-names></name>, &#x0201c;<source>Medical image denoising using convolutional denoising autoencoders</source>,&#x0201d; in <conf-name>Proc. IEEE 16th Int. Conf. Data Mining Workshops (ICDMW)</conf-name>, <month>Dec</month>. <year>2016</year>, pp. <fpage>241</fpage>&#x02013;<lpage>246</lpage>.</mixed-citation></ref><ref id="R42"><label>[42]</label><mixed-citation publication-type="confproc"><name><surname>Jackson</surname><given-names>PT</given-names></name>, <name><surname>Abarghouei</surname><given-names>AA</given-names></name>, <name><surname>Bonner</surname><given-names>S</given-names></name>, <name><surname>Breckon</surname><given-names>TP</given-names></name>, and <name><surname>Obara</surname><given-names>B</given-names></name>, &#x0201c;<source>Style augmentation: Data augmentation via style randomization</source>,&#x0201d; in <conf-name>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops</conf-name>, <conf-loc>Long Beach, CA, USA</conf-loc>, <month>Jun</month>. <year>2019</year>, pp. <fpage>83</fpage>&#x02013;<lpage>92</lpage>.</mixed-citation></ref><ref id="R43"><label>[43]</label><mixed-citation publication-type="journal"><name><surname>Ganin</surname><given-names>Y</given-names></name>
<etal/>, &#x0201c;<article-title>Domain-adversarial training of neural networks</article-title>,&#x0201d; <source>J. Mach. Learn. Res</source>, vol. <volume>17</volume>, no. <issue>1</issue>, pp. <fpage>2030</fpage>&#x02013;<lpage>2096</lpage>, <month>May</month>
<year>2016</year>.</mixed-citation></ref><ref id="R44"><label>[44]</label><mixed-citation publication-type="journal"><name><surname>Edwards</surname><given-names>AD</given-names></name>
<etal/>, &#x0201c;<article-title>The developing human connectome project neonatal data release</article-title>,&#x0201d; <source>Frontiers Neurosci</source>., vol. <volume>16</volume>, <month>May</month>
<year>2022</year>, <fpage>Art. no. 886772</fpage>.</mixed-citation></ref><ref id="R45"><label>[45]</label><mixed-citation publication-type="journal"><name><surname>Fidon</surname><given-names>L</given-names></name>
<etal/>, &#x0201c;<article-title>A spatio-temporal atlas of the developing fetal brain with spina bifida aperta</article-title>,&#x0201d; <source>Open Res. Eur</source>, vol. <volume>1</volume>, p. <fpage>123</fpage>, <month>Oct</month>. <year>2021</year>.<pub-id pub-id-type="pmid">37645096</pub-id>
</mixed-citation></ref><ref id="R46"><label>[46]</label><mixed-citation publication-type="journal"><name><surname>Verdera</surname><given-names>JA</given-names></name>
<etal/>, &#x0201c;<article-title>Reliability and feasibility of low-field-strength fetal MRI at 0.55 T during pregnancy</article-title>,&#x0201d; <source>Radiology</source>, vol. <volume>309</volume>, no. <issue>1</issue>, <month>Oct</month>. <year>2023</year>, <fpage>Art. no. e223050</fpage>.</mixed-citation></ref><ref id="R47"><label>[47]</label><mixed-citation publication-type="book"><name><surname>Payette</surname><given-names>K</given-names></name>
<etal/>, &#x0201c;<part-title>An automated pipeline for quantitative T2 fetal body MRI and segmentation at low field</part-title>,&#x0201d; in <source>Proc. Med. Image Comput. Comput. Assist. Intervent. (MICCAI)</source>, <publisher-name>Cham</publisher-name>, <publisher-loc>Switzerland</publisher-loc>, <year>2023</year>, pp. <fpage>358</fpage>&#x02013;<lpage>367</lpage>.</mixed-citation></ref><ref id="R48"><label>[48]</label><mixed-citation publication-type="journal"><name><surname>Sanchez</surname><given-names>T</given-names></name>, <name><surname>Esteban</surname><given-names>O</given-names></name>, <name><surname>Gomez</surname><given-names>Y</given-names></name>, <name><surname>Eixarch</surname><given-names>E</given-names></name>, and <name><surname>Cuadra</surname><given-names>MB</given-names></name>, &#x0201c;<article-title>FetMRQC: Automated quality control for fetal brain MRI</article-title>,&#x0201d; in <source>Proc. Perinatal, Preterm Paediatric Image Anal., 8th Int. Workshop (PIPPI), Held Conjunct. (MICCAI)</source>, Vancouver, BC, Canada, <month>Oct</month>. <day>12</day>, <year>2023</year>, pp. <fpage>3</fpage>&#x02013;<lpage>16</lpage>.</mixed-citation></ref><ref id="R49"><label>[49]</label><mixed-citation publication-type="journal"><name><surname>Payette</surname><given-names>K</given-names></name>, <name><surname>Kottke</surname><given-names>R</given-names></name>, and <name><surname>Jakab</surname><given-names>A</given-names></name>, &#x0201c;<article-title>Efficient multi-class fetal brain segmentation in high resolution MRI reconstructions with noisy labels</article-title>,&#x0201d; in <source>Proc. Med. Ultrasound, Preterm, Perinatal Paediatric Image Anal</source>., <year>2020</year>, pp. <fpage>295</fpage>&#x02013;<lpage>304</lpage>.</mixed-citation></ref><ref id="R50"><label>[50]</label><mixed-citation publication-type="journal"><name><surname>Karimi</surname><given-names>D</given-names></name>, <name><surname>Rollins</surname><given-names>CK</given-names></name>, <name><surname>Velasco-Annis</surname><given-names>C</given-names></name>, <name><surname>Ouaalam</surname><given-names>A</given-names></name>, and <name><surname>Gholipour</surname><given-names>A</given-names></name>, &#x0201c;<article-title>Learning to segment fetal brain tissue from noisy annotations</article-title>,&#x0201d; <source>Med. Image Anal</source>, vol. <volume>85</volume>, <month>Apr</month>. <year>2023</year>, <fpage>Art. no. 102731</fpage>.<pub-id pub-id-type="pmid">36608414</pub-id>
</mixed-citation></ref></ref-list></back><floats-group><fig position="float" id="F1"><label>Fig. 1.</label><caption><p id="P70">Sample cases from each institution in the testing dataset. Each case is a normally developing fetal brain from gestational week 22, with a super-resolution quality rating of &#x02018;Excellent&#x02019;. The histograms of the individual labels vary between each institution (green: Kispi, orange: Vienna, blue: CHUV, red: UCSF). The inset is an enlarged view of the first peak to visualize the different histograms of the three institutions.</p></caption><graphic xlink:href="nihms-2066798-f0001" position="float"/></fig><fig position="float" id="F2"><label>Fig. 2.</label><caption><p id="P71">Overview of the gestational ages (in weeks) included within the testing dataset by institution.</p></caption><graphic xlink:href="nihms-2066798-f0002" position="float"/></fig><fig position="float" id="F3"><label>Fig. 3.</label><caption><p id="P72">Each submission was evaluated separately on the institutional subsets of the testing data in order to determine if certain algorithms performed better or worse on data from specific institutions. The rankings of the participating teams for each institutional subset are shown, with each connected line corresponding to a single FeTA submission. In-domain institutions: Vienna, Kispi; Out-of-domain institutions: CHUV, UCSF.</p></caption><graphic xlink:href="nihms-2066798-f0003" position="float"/></fig><fig position="float" id="F4"><label>Fig. 4.</label><caption><p id="P73">In-Domain and Out-of-Domain evaluation metrics by algorithm. In both in- and out-of-domain, as well as for all three evaluation metrics (Dice Similarity Coefficient, 95th Hausdorff Distance, Volume Similarity), the results plateau for the first 10 teams, after which a drop off is observed. The ranking of the teams has changed between the In-Domain and Out-of-Domain metrics.</p></caption><graphic xlink:href="nihms-2066798-f0004" position="float"/></fig><fig position="float" id="F5"><label>Fig. 5.</label><caption><p id="P74">Examples of the automatic labels created by the top 5 teams for each of the four institutions (T2w: T2-weighted fetal brain reconstruction; eCSF: external Cerebrospinal Fluid; GM: Grey Matter; WM: White Matter).</p></caption><graphic xlink:href="nihms-2066798-f0005" position="float"/></fig><fig position="float" id="F6"><label>Fig. 6.</label><caption><p id="P75">Rankings of participating teams for each metric from top to bottom (left to right). Left column: Global DSC; Middle Column: HD95; Right Column: VS. The first row are box plots of the evaluation data; the middle row visualizes the ranking stability based on bootstrap sampling, and the bottom row displays the significance maps for the ranking stability, where blue cells indicate no significant differences. All plots were generated with the ChallengeR Toolkit. DSC: Dice Similarity Coefficient: HD95: 95<sup>th</sup> Hausdorff Distance: VS: Volume Similarity.</p></caption><graphic xlink:href="nihms-2066798-f0006" position="float"/></fig><fig position="float" id="F7"><label>Fig. 7.</label><caption><p id="P76">The submissions were evaluated and ranked based on the segmentation results from each of the seven brain tissue labels (1: external Cerebrospinal Fluid, 2: Grey Matter, 3: White Matter, 4: Ventricles, 5: Cerebellum, 6: deep Grey Matter, 7: Brainstem), with each connected line corresponding to a single team&#x02019;s FeTA submission.</p></caption><graphic xlink:href="nihms-2066798-f0007" position="float"/></fig><fig position="float" id="F8"><label>Fig. 8.</label><caption><p id="P77">Examples of high and low topology scores of the grey matter segmentations in the challenge results. Left Column: Coronal view of a 34.6 GA fetus with no known neuropathology; Right Column: a 30.9 GA fetus with severe ventriculomegaly and other abnormalities. In the segmentations with high topology scores a continuous cortical ribbon can be observed. In cases with low scores, a gap in the grey matter can be observed in both examples, as well as holes in the segmentation in the ventriculomegaly example, leading to poor scores (where higher numerical BNE scores correspond with poorer results). A perfect BNE score for the cortical grey matter is (2/0/0). The corresponding Betti numbers are displayed in parentheses next to each example. GA: Gestational Age; BNE: Betti Number Error.</p></caption><graphic xlink:href="nihms-2066798-f0008" position="float"/></fig><table-wrap position="float" id="T1" orientation="landscape"><label>TABLE I</label><caption><p id="P78">Training and Testing Dataset Properties from all Imaging Centers</p></caption><table frame="hsides" rules="rows"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="middle" rowspan="1" colspan="1"/><th align="center" valign="middle" rowspan="1" colspan="1">Testing Domain</th><th align="center" valign="middle" rowspan="1" colspan="1">Institution</th><th align="center" valign="middle" rowspan="1" colspan="1">Scanner</th><th align="center" valign="middle" rowspan="1" colspan="1">N</th><th align="center" valign="middle" rowspan="1" colspan="1">Super-Resolution Method</th><th align="center" valign="middle" rowspan="1" colspan="1">Resolution (mm<sup>3</sup>)</th><th align="center" valign="middle" rowspan="1" colspan="1">TR/TE</th><th align="center" valign="middle" rowspan="1" colspan="1">Gestational Age range (weeks)</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="2" style="writing-mode:sideways-lr" colspan="1">Training</td><td align="center" valign="middle" rowspan="2" style="writing-mode:sideways-lr" colspan="1">In-Domain</td><td align="left" valign="top" rowspan="1" colspan="1">Kispi</td><td align="left" valign="top" rowspan="1" colspan="1">GE Signa Discovery MR450/MR750 (1.5T/3T respectively)<xref rid="TFN1" ref-type="table-fn">*</xref></td><td align="left" valign="top" rowspan="1" colspan="1">80</td><td align="left" valign="top" rowspan="1" colspan="1">MIALSRTK (n=40)<break/>irtk-simple (n=40)</td><td align="left" valign="top" rowspan="1" colspan="1">0.5&#x000d7;0.5&#x000d7; 0.5</td><td align="left" valign="top" rowspan="1" colspan="1">TR: 2000&#x02013;3500ms, TE: 120ms (minimum)</td><td align="center" valign="top" rowspan="1" colspan="1">20.0&#x02013;34.8</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Vienna</td><td align="left" valign="top" rowspan="1" colspan="1">Philips Ingenia/Intera (1.5T); Philips Achieva (3T)<xref rid="TFN1" ref-type="table-fn">*</xref></td><td align="left" valign="top" rowspan="1" colspan="1">40</td><td align="left" valign="top" rowspan="1" colspan="1">NiftyMIC<xref rid="TFN2" ref-type="table-fn">**</xref> (n=40)</td><td align="left" valign="top" rowspan="1" colspan="1">1.0&#x000d7;1.0&#x000d7;1.0</td><td align="left" valign="top" rowspan="1" colspan="1">TR: 6000&#x02013;22000ms<break/>TE: 80&#x02013;140ms</td><td align="center" valign="top" rowspan="1" colspan="1">19.3&#x02013;34.4</td></tr><tr><td align="center" valign="middle" rowspan="4" style="writing-mode:sideways-lr" colspan="1">Testing</td><td align="center" valign="middle" rowspan="2" style="writing-mode:sideways-lr" colspan="1">In-Domain</td><td align="left" valign="top" rowspan="1" colspan="1">Kispi</td><td align="left" valign="top" rowspan="1" colspan="1">GE Signa Discovery MR450/MR750 (1.5T/3T respectively)<xref rid="TFN1" ref-type="table-fn">*</xref></td><td align="left" valign="top" rowspan="1" colspan="1">40</td><td align="left" valign="top" rowspan="1" colspan="1">MIALSRTK (n=20)<break/>irtk-simple (n=20)</td><td align="left" valign="top" rowspan="1" colspan="1">0.5&#x000d7;0.5&#x000d7;0.5</td><td align="left" valign="top" rowspan="1" colspan="1">TR: 2000&#x02013;3500ms, TE: 120ms (minimum)</td><td align="center" valign="top" rowspan="1" colspan="1">21.3&#x02013;34.6</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Vienna</td><td align="left" valign="top" rowspan="1" colspan="1">Philips Ingenia/Intera (1.5T); Philips Achieva (3T)<xref rid="TFN1" ref-type="table-fn">*</xref></td><td align="left" valign="top" rowspan="1" colspan="1">40</td><td align="left" valign="top" rowspan="1" colspan="1">NiftyMIC<xref rid="TFN2" ref-type="table-fn">**</xref> (n=40)</td><td align="left" valign="top" rowspan="1" colspan="1">1.0&#x000d7;1.0&#x000d7;1.0</td><td align="left" valign="top" rowspan="1" colspan="1">TR: 6000&#x02013;22000ms<break/>TE: 80&#x02013;140ms</td><td align="center" valign="top" rowspan="1" colspan="1">18.1&#x02013;35.0</td></tr><tr><td align="center" valign="middle" rowspan="2" style="writing-mode:sideways-lr" colspan="1">Out-of-Domain</td><td align="left" valign="top" rowspan="1" colspan="1">CHUV</td><td align="left" valign="top" rowspan="1" colspan="1">Siemens MAGNETOM Aera(1.5T)</td><td align="left" valign="top" rowspan="1" colspan="1">40</td><td align="left" valign="top" rowspan="1" colspan="1">MIALSRTK (n=40)</td><td align="left" valign="top" rowspan="1" colspan="1">1.125&#x000d7;1.125&#x000d7;1.125</td><td align="left" valign="top" rowspan="1" colspan="1">TR: 1200ms, TE: 90ms</td><td align="center" valign="top" rowspan="1" colspan="1">21.0&#x02013;35.0</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">UCSF</td><td align="left" valign="top" rowspan="1" colspan="1">GE Discovery MR750/MR750W (3T)</td><td align="left" valign="top" rowspan="1" colspan="1">40</td><td align="left" valign="top" rowspan="1" colspan="1">NiftyMIC<xref rid="TFN2" ref-type="table-fn">**</xref> (n=40)</td><td align="left" valign="top" rowspan="1" colspan="1">0..8&#x000d7;0.8&#x000d7;0.8</td><td align="left" valign="top" rowspan="1" colspan="1">TR: 2000&#x02013;3500 ms, TE: 100 ms (minimum)</td><td align="center" valign="top" rowspan="1" colspan="1">20.0&#x02013;35.1</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><label>*</label><p id="P79">The training dataset contained data from both 1.5T and 3T scanners. However, which cases belonged to which scanner were not provided to the participants as it was part of the data anonymization process. Therefore, the breakdown of number of cases per scanner is not provided here.</p></fn><fn id="TFN2"><label>**</label><p id="P80">When the NiftyMIC algorithm was used, the image included the maternal tissue. The brain mask generated automatically by the algorithm was not used. Therefore, the NiftyMIC cases contained more maternal tissue than the fetal brains reconstructed with the MIALSRTK and irtk-simple algorithms.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="T2" orientation="landscape"><label>TABLE II</label><caption><p id="P81">FeTA 2022 Team Overview</p></caption><table frame="hsides" rules="none"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Rank</th><th align="left" valign="top" rowspan="1" colspan="1">Team Name</th><th align="left" valign="top" rowspan="1" colspan="1">Architecture</th><th align="left" valign="top" rowspan="1" colspan="1">Training Strategy</th><th align="left" valign="top" rowspan="1" colspan="1">Loss Function</th><th align="left" valign="top" rowspan="1" colspan="1">Post-processing</th><th align="left" valign="top" rowspan="1" colspan="1">Augmentation</th><th align="left" valign="top" rowspan="1" colspan="1">External Datasets</th></tr><tr><th align="left" valign="top" colspan="8" rowspan="1">
<hr/>
</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="left" valign="top" rowspan="1" colspan="1">FeTA-ICL-TUM (FIT) &#x02013; nnU-Net (FIT_1)</td><td align="left" valign="top" rowspan="1" colspan="1">nnU-Net</td><td align="left" valign="top" rowspan="1" colspan="1">Ensemble of 5 different models</td><td align="left" valign="top" rowspan="1" colspan="1">CE and soft Dice</td><td align="left" valign="top" rowspan="1" colspan="1">Ensemble + rule-based denoising autoencoder post-processing</td><td align="left" valign="top" rowspan="1" colspan="1">(i): Default nnU-Net augmentation; (ii): (i) + random bias field; (iii): (i) + style augmentation, random bias field; (iv): (i) + photometric augmentation; (v): (i) + motion artifact</td><td align="left" valign="top" rowspan="1" colspan="1">&#x02018;Painter By Numbers (PBN) for style transfer training in (iii)</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="left" valign="top" rowspan="1" colspan="1">BlueBrune*</td><td align="left" valign="top" rowspan="1" colspan="1">nnU-Net</td><td align="left" valign="top" rowspan="1" colspan="1">Data Split: 80/20; (i) tissue segmentation network; (ii) domain adversarial approach</td><td align="left" valign="top" rowspan="1" colspan="1">CE and dice</td><td align="left" valign="top" rowspan="1" colspan="1">ensemble (2 models)</td><td align="left" valign="top" rowspan="1" colspan="1">default nnU-Net augmentation</td><td align="left" valign="top" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="left" valign="top" rowspan="1" colspan="1">FMRSK*</td><td align="left" valign="top" rowspan="1" colspan="1">(i) 3D U-Net<break/>(ii) Attention U-Net (MONAI)</td><td align="left" valign="top" rowspan="1" colspan="1">(i) brain extraction; (ii) tissue segmentation</td><td align="left" valign="top" rowspan="1" colspan="1">Dice and CE (MONAI)</td><td align="left" valign="top" rowspan="1" colspan="1">Average prediction of 2 models</td><td align="left" valign="top" rowspan="1" colspan="1">motion artifact, MR spike, Bias field, affine transform, noise, blurring, gamma, random intensity shift</td><td align="left" valign="top" rowspan="1" colspan="1">19 dHCP neonates, Spina bifida atlas</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="left" valign="top" rowspan="1" colspan="1">NVAUTO</td><td align="left" valign="top" rowspan="1" colspan="1">SegResNet</td><td align="left" valign="top" rowspan="1" colspan="1">5-folds CV</td><td align="left" valign="top" rowspan="1" colspan="1">Dice Focal from MONAI</td><td align="left" valign="top" rowspan="1" colspan="1">ensemble on average prediction (15 models)</td><td align="left" valign="top" rowspan="1" colspan="1">normalize to zero mean and unit standard deviation</td><td align="left" valign="top" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="left" valign="top" rowspan="1" colspan="1">Blackbean</td><td align="left" valign="top" rowspan="1" colspan="1">(i) nnU-Net, (ii) ViT-Adaptor</td><td align="left" valign="top" rowspan="1" colspan="1">Data Split: 100/0; (i) two models, (ii)Test-time augmentation</td><td align="left" valign="top" rowspan="1" colspan="1">Dice and BCE</td><td align="left" valign="top" rowspan="1" colspan="1">Ensemble on test-time augmented (softmax mean)</td><td align="left" valign="top" rowspan="1" colspan="1">default nnU-Net augmentation</td><td align="left" valign="top" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="left" valign="top" rowspan="1" colspan="1">symsense</td><td align="left" valign="top" rowspan="1" colspan="1">nnU-Net</td><td align="left" valign="top" rowspan="1" colspan="1">5-folds CV</td><td align="left" valign="top" rowspan="1" colspan="1">modified Generalized Dice and CE</td><td align="left" valign="top" rowspan="1" colspan="1">None</td><td align="left" valign="top" rowspan="1" colspan="1">Default nnU-Net augmentation, brightness transform, contrast transform, zoom, warping, GIN-IPA</td><td align="left" valign="top" rowspan="1" colspan="1">40 early neonatal dHCP</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="left" valign="top" rowspan="1" colspan="1">FeTA-ICL-TUM (FIT) &#x02013; SWINUNETR (FIT_2)</td><td align="left" valign="top" rowspan="1" colspan="1">(i) Synthstrip, (ii) Swin UNETR (MONAI)</td><td align="left" valign="top" rowspan="1" colspan="1">(i) skull stripping; (ii) tissue segmentation</td><td align="left" valign="top" rowspan="1" colspan="1">weighted CE and soft Dice</td><td align="left" valign="top" rowspan="1" colspan="1">Resampling to original image</td><td align="left" valign="top" rowspan="1" colspan="1">flipping, rotation, affine + elastic transformation, noise, blur, gamma, ghosting, spike, motion, bias, blur, anisotropy</td><td align="left" valign="top" rowspan="1" colspan="1">neonate subjects of the dHCP</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="left" valign="top" rowspan="1" colspan="1">DBC Pasteur</td><td align="left" valign="top" rowspan="1" colspan="1">U-Net</td><td align="left" valign="top" rowspan="1" colspan="1">Ensemble of 3 2D networks (ax/sag/cor) Data split 80/20%</td><td align="left" valign="top" rowspan="1" colspan="1">CE</td><td align="left" valign="top" rowspan="1" colspan="1">Ensemble (3 models) majority vote</td><td align="left" valign="top" rowspan="1" colspan="1">Noise, blur, 2D rotations + translations + flip, zoom</td><td align="left" valign="top" rowspan="1" colspan="1">Atlas Gholipour + Atlas Scrag</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="left" valign="top" rowspan="1" colspan="1">Dolphins</td><td align="left" valign="top" rowspan="1" colspan="1">(i) 3DResUNet (coarse), (ii) nnU-Net (fine)</td><td align="left" valign="top" rowspan="1" colspan="1">5-folds CV, keep the one with best validation scores</td><td align="left" valign="top" rowspan="1" colspan="1">BCE+Dice</td><td align="left" valign="top" rowspan="1" colspan="1">None</td><td align="left" valign="top" rowspan="1" colspan="1">Flipping, random gamma</td><td align="left" valign="top" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="left" valign="top" rowspan="1" colspan="1">fudan_zmic</td><td align="left" valign="top" rowspan="1" colspan="1">BayeSeg (based on nnU-Net)</td><td align="left" valign="top" rowspan="1" colspan="1">5-folds CV</td><td align="left" valign="top" rowspan="1" colspan="1">CE, Dice, and weighted variational</td><td align="left" valign="top" rowspan="1" colspan="1">Ensemble (5 folds cv)</td><td align="left" valign="top" rowspan="1" colspan="1">cardiac cutmix augmentation to enrich the background, and nnU-Net augmentations</td><td align="left" valign="top" rowspan="1" colspan="1">ACDC dataset for cutmix</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="left" valign="top" rowspan="1" colspan="1">hilab</td><td align="left" valign="top" rowspan="1" colspan="1">(i) nnU-Net, (ii) residual 3D U-Net</td><td align="left" valign="top" rowspan="1" colspan="1">(i) coarse multi-class model, (ii) seven fine single class models</td><td align="left" valign="top" rowspan="1" colspan="1">CE and Dice loss</td><td align="left" valign="top" rowspan="1" colspan="1">Ensemble of stage (ii).</td><td align="left" valign="top" rowspan="1" colspan="1">rotation and scaling, Gaussian noise and blur, brightness and contrast adjustment, simulation of low resolution, gamma augmentation, mirroring</td><td align="left" valign="top" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="left" valign="top" rowspan="1" colspan="1">Neurophet</td><td align="left" valign="top" rowspan="1" colspan="1">3D U-Net</td><td align="left" valign="top" rowspan="1" colspan="1">3 models trained; probability-based sampling method to focus network</td><td align="left" valign="top" rowspan="1" colspan="1">Dice, CE (custom weight)</td><td align="left" valign="top" rowspan="1" colspan="1">All 3 models used, non-zero voxels measured, volumes compared</td><td align="left" valign="top" rowspan="1" colspan="1">spatial (horizontal flip, rotation, affine transform) and intensity (gaussian blur)</td><td align="left" valign="top" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="left" valign="top" rowspan="1" colspan="1">Sano</td><td align="left" valign="top" rowspan="1" colspan="1">Swin UNETR</td><td align="left" valign="top" rowspan="1" colspan="1">5-folds CV</td><td align="left" valign="top" rowspan="1" colspan="1">CE and Dice</td><td align="left" valign="top" rowspan="1" colspan="1">Ensemble learning (5 models)</td><td align="left" valign="top" rowspan="1" colspan="1">cropping, random zoom, random rotation, random gaussian noise, random adjust contrast, random flip on each axis</td><td align="left" valign="top" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="left" valign="top" rowspan="1" colspan="1">Uniandes</td><td align="left" valign="top" rowspan="1" colspan="1">ROG (from MSD)</td><td align="left" valign="top" rowspan="1" colspan="1">2-folds CV</td><td align="left" valign="top" rowspan="1" colspan="1">Dice and CE</td><td align="left" valign="top" rowspan="1" colspan="1">closure and opening of grays in segmentation map with structuring element</td><td align="left" valign="top" rowspan="1" colspan="1">Spatial Transform (random rotation and scaling), Mirror Transform and gamma correction.</td><td align="left" valign="top" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="left" valign="top" rowspan="1" colspan="1">xinlab-scut-iai-ahu</td><td align="left" valign="top" rowspan="1" colspan="1">(i) DynUNet, (ii) Swin Transformer, (iii) SwinUNETR</td><td align="left" valign="top" rowspan="1" colspan="1">5-folds CV; (i) brain extraction, (ii) Domain generalization stage, (iii) tissue segmentation</td><td align="left" valign="top" rowspan="1" colspan="1">(i) Dice, (ii) Contrastive, CE, L2 (iii) Dice</td><td align="left" valign="top" rowspan="1" colspan="1">argmax operated results of sliding window patches with 0.5 overlap</td><td align="left" valign="top" rowspan="1" colspan="1">orientation change, spacing, cropping, flipping, rotation</td><td align="left" valign="top" rowspan="1" colspan="1">No</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="left" valign="top" rowspan="1" colspan="1">deepsynth</td><td align="left" valign="top" rowspan="1" colspan="1">U-Net, SynthSeg</td><td align="left" valign="top" rowspan="1" colspan="1">Training on synthetic, fine tuning on real T2w (dHCP, FeTA)</td><td align="left" valign="top" rowspan="1" colspan="1">Mean dice score</td><td align="left" valign="top" rowspan="1" colspan="1">None</td><td align="left" valign="top" rowspan="1" colspan="1">Synthetic MRI from dHCP label-maps with SynthSeg</td><td align="left" valign="top" rowspan="1" colspan="1">80 youngest dHCP subjects</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="left" valign="top" rowspan="1" colspan="1">ajoshiusc</td><td align="left" valign="top" rowspan="1" colspan="1">R50-ViT: combo of ResNet-50 and ViT, transUNet</td><td align="left" valign="top" rowspan="1" colspan="1">Data Split: 75/5 subjects.</td><td align="left" valign="top" rowspan="1" colspan="1">CE, robust CE based on beta divergence</td><td align="left" valign="top" rowspan="1" colspan="1">None</td><td align="left" valign="top" rowspan="1" colspan="1">None</td><td align="left" valign="top" rowspan="1" colspan="1">No</td></tr></tbody></table><table-wrap-foot><fn id="TFN3"><p id="P82">BCE: Binary Cross-Entropy, CE: Cross-Entropy, CV: cross-validation, dHCP: developing Human Connectome Project, MSD: Medical Segmentation Decathlon</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="T3" orientation="landscape"><label>TABLE III</label><caption><p id="P83">Final Rankings and Results of the FeTA 2022 Challenge. In Addition, the Separate Rankings for the In-Domain Datasets (Kispi, Vienna) and the Out-of-Domain Datasets (CHUV, UCSF) are Shown</p></caption><table frame="hsides" rules="rows"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Global ranking</th><th align="center" valign="top" rowspan="1" colspan="1">Team Name</th><th align="center" valign="top" rowspan="1" colspan="1">Global Average DSC</th><th align="center" valign="top" rowspan="1" colspan="1">Global Average HD95</th><th align="center" valign="top" rowspan="1" colspan="1">Global Average VS</th><th align="center" valign="top" rowspan="1" colspan="1">In-Domain Ranking</th><th align="center" valign="top" rowspan="1" colspan="1">Out-of-Domain Ranking</th></tr></thead><tbody><tr><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">FIT_1</td><td align="center" valign="top" rowspan="1" colspan="1">0.816 &#x000b1; 0.11</td><td align="center" valign="top" rowspan="1" colspan="1">2.347 &#x000b1; 2.51</td><td align="center" valign="top" rowspan="1" colspan="1">0.910 &#x000b1; 0.12</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">2</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">2<xref rid="TFN4" ref-type="table-fn">*</xref></td><td align="center" valign="top" rowspan="1" colspan="1">Bluebrune</td><td align="center" valign="top" rowspan="1" colspan="1">0.812 &#x000b1; 0.11</td><td align="center" valign="top" rowspan="1" colspan="1">2.377 &#x000b1; 2.55</td><td align="center" valign="top" rowspan="1" colspan="1">0.908 &#x000b1; 0.11</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">3</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">2<xref rid="TFN4" ref-type="table-fn">*</xref></td><td align="center" valign="top" rowspan="1" colspan="1">FMRSK</td><td align="center" valign="top" rowspan="1" colspan="1">0.808 &#x000b1; 0.11</td><td align="center" valign="top" rowspan="1" colspan="1">2.395 &#x000b1; 2.94</td><td align="center" valign="top" rowspan="1" colspan="1">0.920 &#x000b1; 0.10</td><td align="center" valign="top" rowspan="1" colspan="1">9<xref rid="TFN4" ref-type="table-fn">*</xref></td><td align="center" valign="top" rowspan="1" colspan="1">1</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">NVAUTO</td><td align="center" valign="top" rowspan="1" colspan="1">0.810 &#x000b1;0.13</td><td align="center" valign="top" rowspan="1" colspan="1">2.608 &#x000b1; 3.30</td><td align="center" valign="top" rowspan="1" colspan="1">0.915 &#x000b1; 0.12</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">4<xref rid="TFN4" ref-type="table-fn">*</xref></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">Blackbean</td><td align="center" valign="top" rowspan="1" colspan="1">0.812 &#x000b1; 0.12</td><td align="center" valign="top" rowspan="1" colspan="1">2.506 &#x000b1; 3.66</td><td align="center" valign="top" rowspan="1" colspan="1">0.909 &#x000b1; 0.11</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">4<xref rid="TFN4" ref-type="table-fn">*</xref></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">Symsense</td><td align="center" valign="top" rowspan="1" colspan="1">0.813 &#x000b1; 0.12</td><td align="center" valign="top" rowspan="1" colspan="1">2.660 &#x000b1; 6.81</td><td align="center" valign="top" rowspan="1" colspan="1">0.907 &#x000b1; 0.12</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">4<xref rid="TFN4" ref-type="table-fn">*</xref></td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">FIT_2</td><td align="center" valign="top" rowspan="1" colspan="1">0.798 &#x000b1; 0.11</td><td align="center" valign="top" rowspan="1" colspan="1">3.421 &#x000b1; 5.51</td><td align="center" valign="top" rowspan="1" colspan="1">0.913 &#x000b1; 0.10</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">7</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">Institute Pasteur (DBC)</td><td align="center" valign="top" rowspan="1" colspan="1">0.789 &#x000b1; 0.13</td><td align="center" valign="top" rowspan="1" colspan="1">2.387 &#x000b1; 1.97</td><td align="center" valign="top" rowspan="1" colspan="1">0.901 &#x000b1; 0.12</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">8</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">Dolphins</td><td align="center" valign="top" rowspan="1" colspan="1">0.806 &#x000b1; 0.12</td><td align="center" valign="top" rowspan="1" colspan="1">4.521 &#x000b1; 11.87</td><td align="center" valign="top" rowspan="1" colspan="1">0.905 &#x000b1; 0.12</td><td align="center" valign="top" rowspan="1" colspan="1">9<xref rid="TFN4" ref-type="table-fn">*</xref></td><td align="center" valign="top" rowspan="1" colspan="1">9</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">Fudan_zmic</td><td align="center" valign="top" rowspan="1" colspan="1">0.788 &#x000b1; 0.13</td><td align="center" valign="top" rowspan="1" colspan="1">4.720 &#x000b1; 7.58</td><td align="center" valign="top" rowspan="1" colspan="1">0.903 &#x000b1; 0.12</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">10</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">Hilab</td><td align="center" valign="top" rowspan="1" colspan="1">0.774 &#x000b1; 0.13</td><td align="center" valign="top" rowspan="1" colspan="1">13.008 &#x000b1; 14.84</td><td align="center" valign="top" rowspan="1" colspan="1">0.887 &#x000b1; 012</td><td align="center" valign="top" rowspan="1" colspan="1">12<xref rid="TFN4" ref-type="table-fn">*</xref></td><td align="center" valign="top" rowspan="1" colspan="1">12</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">Neurophet</td><td align="center" valign="top" rowspan="1" colspan="1">0.739 &#x000b1; 0.22</td><td align="center" valign="top" rowspan="1" colspan="1">10.288 &#x000b1; 35.54</td><td align="center" valign="top" rowspan="1" colspan="1">0.844 &#x000b1; 0.25</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">14</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">Sano</td><td align="center" valign="top" rowspan="1" colspan="1">0.709 &#x000b1; 0.22</td><td align="center" valign="top" rowspan="1" colspan="1">7.171 &#x000b1; 12.76</td><td align="center" valign="top" rowspan="1" colspan="1">0.817 &#x000b1; 0.23</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">11</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">Uniandes</td><td align="center" valign="top" rowspan="1" colspan="1">0.652 &#x000b1; 0.22</td><td align="center" valign="top" rowspan="1" colspan="1">11.366 &#x000b1; 27.61</td><td align="center" valign="top" rowspan="1" colspan="1">0.814 &#x000b1; 0.23</td><td align="center" valign="top" rowspan="1" colspan="1">12<xref rid="TFN4" ref-type="table-fn">*</xref></td><td align="center" valign="top" rowspan="1" colspan="1">13</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">Xinlab-scut</td><td align="center" valign="top" rowspan="1" colspan="1">0.494 &#x000b1; 0.29</td><td align="center" valign="top" rowspan="1" colspan="1">23.150 &#x000b1; 20.66</td><td align="center" valign="top" rowspan="1" colspan="1">0.731 &#x000b1; 0.22</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">15</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">Deepsynth</td><td align="center" valign="top" rowspan="1" colspan="1">0.433 &#x000b1; 0.34</td><td align="center" valign="top" rowspan="1" colspan="1">36.653 &#x000b1; 62.13</td><td align="center" valign="top" rowspan="1" colspan="1">0.604 &#x000b1; 0.38</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">16</td></tr><tr><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">Ajoshiusc</td><td align="center" valign="top" rowspan="1" colspan="1">0.319 &#x000b1; 0.33</td><td align="center" valign="top" rowspan="1" colspan="1">56.598 &#x000b1; 75.01</td><td align="center" valign="top" rowspan="1" colspan="1">0.480 &#x000b1; 0.38</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">17</td></tr></tbody></table><table-wrap-foot><fn id="TFN4"><label>*</label><p id="P84">Tied</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="T4"><label>TABLE IV</label><caption><p id="P85">Topology (a) and Global (b) Rankings of the Submissions. (a) Betti Number Errors (BNE) Per Dimension and Overall. (b) Comparison of the FeTA Challenge 2022 Ranking and the Topology-Integrative Ranking (tir). Top 3 Submissions are Shown in Bold</p></caption><table frame="hsides" rules="none"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="middle" rowspan="5" colspan="1">Team Name</th><th align="center" valign="middle" colspan="3" rowspan="1">(A) Topology</th><th align="center" valign="middle" colspan="3" rowspan="1">(B) Global</th></tr><tr><th align="left" valign="top" colspan="6" rowspan="1">
<hr/>
</th></tr><tr><th align="center" valign="middle" colspan="3" rowspan="1">k-dim BNE</th><th align="center" valign="middle" rowspan="3" colspan="1">BNE</th><th align="center" valign="middle" rowspan="3" colspan="1">TIR</th><th align="center" valign="middle" rowspan="3" colspan="1">FeTA</th></tr><tr><th align="left" valign="top" colspan="3" rowspan="1">
<hr/>
</th></tr><tr><th align="center" valign="middle" rowspan="1" colspan="1">BNE0</th><th align="center" valign="middle" rowspan="1" colspan="1">BNE1</th><th align="center" valign="middle" rowspan="1" colspan="1">BNE2</th></tr><tr><th align="left" valign="top" colspan="7" rowspan="1">
<hr/>
</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">ajoshiusc</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">17</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Blackbean</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">5</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">BlueBrune</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">2</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Deepsynth</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">16</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Dolphins</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">9</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">FIT_1</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">1</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">FIT_2</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">7</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">FMRSK</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">3</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Fudan_zmic</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">10</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Hilab</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">11</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Institut_Pasteur_DBC</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">8</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Neurophet</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">12</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">NVAUTO</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">4</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Sano</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">13</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">symsense</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">6</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Uniandes</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">14</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Xinlab-scut-iai-ahu</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">15</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T5" orientation="landscape"><label>TABLE V</label><caption><p id="P86">Topology (BNE) Ranking of the Submissions Per Tissue Class and on Average</p></caption><table frame="hsides" rules="none"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" valign="top" rowspan="1" colspan="1">Team Name</th><th align="center" valign="top" rowspan="1" colspan="1">eCSF</th><th align="center" valign="top" rowspan="1" colspan="1">GM</th><th align="center" valign="top" rowspan="1" colspan="1">WM</th><th align="center" valign="top" rowspan="1" colspan="1">Ventricles</th><th align="center" valign="top" rowspan="1" colspan="1">Cerebellum</th><th align="center" valign="top" rowspan="1" colspan="1">deep GM</th><th align="center" valign="top" rowspan="1" colspan="1">Brainstem</th><th align="center" valign="top" rowspan="1" colspan="1">Average</th></tr><tr><th align="left" valign="top" colspan="9" rowspan="1">
<hr/>
</th></tr></thead><tbody><tr><td align="left" valign="top" rowspan="1" colspan="1">Ajoshiusc</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">13.4</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Blackbean</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">3.3</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">BlueBrune</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">3.1</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Deepsynth</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">14.6</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Dolphins</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">5.6</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">FIT_1 (nnU-Net)</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">2.6</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">FIT_2 (SWINUNETR)</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">2</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">5.8</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">FMRSK</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">9.1</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Fudan_zmic</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">9.3</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Hilab</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">8.6</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Institut_Pasteur_DBC</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">11.3</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Neurophet</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">14.4</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">NVAUTO</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">1</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">5</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">6.4</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Sano</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">12</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">12.7</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">symsense</td><td align="center" valign="top" rowspan="1" colspan="1">8</td><td align="center" valign="top" rowspan="1" colspan="1">9</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">10</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">4</td><td align="center" valign="top" rowspan="1" colspan="1">7.0</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Uniandes</td><td align="center" valign="top" rowspan="1" colspan="1">6</td><td align="center" valign="top" rowspan="1" colspan="1">3</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">7</td><td align="center" valign="top" rowspan="1" colspan="1">14</td><td align="center" valign="top" rowspan="1" colspan="1">11</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">10.3</td></tr><tr><td align="left" valign="top" rowspan="1" colspan="1">Xinlab-scut-iai-ahu</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">17</td><td align="center" valign="top" rowspan="1" colspan="1">16</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">15</td><td align="center" valign="top" rowspan="1" colspan="1">13</td><td align="center" valign="top" rowspan="1" colspan="1">15.4</td></tr></tbody></table></table-wrap></floats-group></article>