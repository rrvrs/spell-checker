<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Cureus</journal-id><journal-id journal-id-type="iso-abbrev">Cureus</journal-id><journal-id journal-id-type="issn">2168-8184</journal-id><journal-title-group><journal-title>Cureus</journal-title></journal-title-group><issn pub-type="epub">2168-8184</issn><publisher><publisher-name>Cureus</publisher-name><publisher-loc>Palo Alto (CA)</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40400811</article-id><article-id pub-id-type="pmc">PMC12094275</article-id><article-id pub-id-type="doi">10.7759/cureus.82705</article-id><article-categories><subj-group subj-group-type="heading"><subject>Other</subject></subj-group><subj-group><subject>Internal Medicine</subject></subj-group><subj-group><subject>Medical Education</subject></subj-group></article-categories><title-group><article-title>ChatGPT and Gemini for Patient Education: A Comparative Analysis of Common Pediatric Exanthematous Conditions</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Muacevic</surname><given-names>Alexander</given-names></name></contrib><contrib contrib-type="editor"><name><surname>Adler</surname><given-names>John R</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Reshi</surname><given-names>Amrutha</given-names></name><xref rid="aff-1" ref-type="aff">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Arora</surname><given-names>Nikhil</given-names></name><xref rid="aff-2" ref-type="aff">2</xref><xref rid="aff-3" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Singh</surname><given-names>Tanupriya</given-names></name><xref rid="aff-4" ref-type="aff">4</xref></contrib></contrib-group><aff id="aff-1">
<label>1</label>
Psychiatry, Bhagawan Sri Balagangadharanatha Swamiji (BGS) Global Institute of Medical Sciences, Bengaluru, IND </aff><aff id="aff-2">
<label>2</label>
Pediatrics, Government Medical College, Patiala, IND </aff><aff id="aff-3">
<label>3</label>
Pediatrics, Guru Gobind Singh Medical College and Hospital, Faridkot, IND </aff><aff id="aff-4">
<label>4</label>
Pediatric Medicine, The Doctor's Hub Polyclinic, Dubai, ARE </aff><author-notes><corresp id="cor1">
Nikhil Arora <email>nikhilarora4435@gmail.com</email>
</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>21</day><month>4</month><year>2025</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>4</month><year>2025</year></pub-date><volume>17</volume><issue>4</issue><elocation-id>e82705</elocation-id><history><date date-type="accepted"><day>14</day><month>4</month><year>2025</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2025, Reshi et al.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Reshi et al.</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the Creative Commons Attribution License CC-BY 4.0., which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri xlink:href="https://www.cureus.com/articles/343411-chatgpt-and-gemini-for-patient-education-a-comparative-analysis-of-common-pediatric-exanthematous-conditions">This article is available from https://www.cureus.com/articles/343411-chatgpt-and-gemini-for-patient-education-a-comparative-analysis-of-common-pediatric-exanthematous-conditions</self-uri><abstract><p>Introduction: Varicella, hand, foot, and mouth disease (HFMD), and measles are some of the common causes of fever with rash in the pediatric age group. ChatGPT and Gemini are effective large language models (LLMs)&#x000a0;for parents to understand their child&#x02019;s condition. Therefore, considering the growing popularity of artificial intelligence (AI), LLMs, and their ability to disseminate health information, assessing ChatGPT's (OpenAI, San Francisco, CA, USA) and Gemini's (Google LLC, Mountain View, CA, USA) quality and accuracy is essential.</p><p>Materials and methods: A cross-sectional study was conducted on responses generated using AI for common causes of fever with rash in the pediatric age group, namely varicella, HFMD, and measles. ChatGPT and Gemini were used for the generation of brochures for patient education. The responses generated were evaluated using the Flesch-Kincaid Calculator (Good Calculators: <ext-link xlink:href="https://goodcalculators.com" ext-link-type="uri">https://goodcalculators.com</ext-link>/), the QuillBot plagiarism tool (QuillBot, Chicago, IL, USA), and the modified DISCERN score. Statistical analysis was done using R version 4.3.2 (R Foundation for Statistical Computing, Vienna, Austria, <ext-link xlink:href="https://www.R-project.org" ext-link-type="uri">https://www.R-project.org</ext-link>/), and unpaired t-tests were used to compare the various scores. A p-value of less than 0.05 was considered statistically significant.</p><p>Results: It was found that ChatGPT generates a higher word count as compared to Gemini (p=0.047). Sentences, average words per sentence, average syllables per word, ease score, and grade level between the two AI tools were statistically insignificant (p&#x0003e;0.05). The mean reliability score was 3/5 in the case of Gemini versus 2.67/5 in ChatGPT, but the difference was statistically insignificant (p=0.725).</p><p>Conclusions: This study highlights that ChatGPT generates more word count than Gemini, and the finding was statistically significant (p=0.047). Additionally, there is no significant difference in the average ease score or grade score for common pediatric exanthematous conditions: varicella, HFMD, and measles.&#x000a0;Future research should focus on improving AI-generated health content by incorporating real-time validation mechanisms, expert reviews, and structured patient feedback.</p></abstract><kwd-group kwd-group-type="author"><kwd>ai tools</kwd><kwd>artificial intelligence (ai)</kwd><kwd>chatgpt</kwd><kwd>google gemini</kwd><kwd>patient education</kwd><kwd>pediatric</kwd><kwd>skin conditions</kwd></kwd-group></article-meta></front><body><sec sec-type="intro"><title>Introduction</title><p>Fever with rash is a common problem in the pediatric age group. It can range&#x000a0;from minor viral illness or any drug allergy to serious life-threatening causes such as dengue hemorrhagic fever&#x000a0;[<xref rid="REF1" ref-type="bibr">1</xref>]. Some common causes are varicella, hand, foot, and mouth disease (HFMD), and measles [<xref rid="REF1" ref-type="bibr">1</xref>]. Educating parents about pediatric exanthematous diseases is essential for early recognition, timely management, and reducing transmission risks.</p><p>Various artificial intelligence (AI) tools are&#x000a0;being utilized by the public for various uses, including searching for information about various health conditions. One such tool is ChatGPT (OpenAI, San Francisco, CA, USA), based on a natural language processing model that is being widely used [<xref rid="REF2" ref-type="bibr">2</xref>]. It had gained 100 million users in its initial two months of launch in 2022 [<xref rid="REF3" ref-type="bibr">3</xref>]. Later in 2023, Gemini was released (Google LLC, Mountain View, CA, USA), a large language model like that of ChatGPT [<xref rid="REF4" ref-type="bibr">4</xref>]. Now it is one of the primary competitors of ChatGPT [<xref rid="REF4" ref-type="bibr">4</xref>].</p><p>The conventional search engines find up-to-date information from the web by providing various links to relevant pages. Still, in contrast, these AI tools generate responses based on their trained knowledge and real-time web content, offer direct answers, and engage in human-like conversation. These AI models are easily accessible, convenient, and fast. While search engines provide information on diseases with references to sources, some AI tools' responses lack direct references, posing challenges to the verification of medical accuracy.</p><p>These language-based models can be an effective tool for the parents to understand their child&#x02019;s condition and become more health-literate [<xref rid="REF5" ref-type="bibr">5</xref>]. Therefore, evaluating the readability, reliability, and accuracy of ChatGPT and Gemini in generating pediatric health information is essential.</p><p>This study aimed to evaluate the accuracy, dependability, and readability of responses from ChatGPT and Gemini to frequently asked questions on varicella, HFMD, and measles&#x000a0;and compare their effectiveness in creating patient education guides based on clarity and ease of understanding.</p></sec><sec sec-type="materials|methods"><title>Materials and methods</title><p>A cross-sectional study was conducted from August 6th to August 12th, 2024. First, the responses were generated using AI for three common diseases in pediatrics, as selected, namely varicella, HFMD, and measles. Two AI tools were selected, namely ChatGPT 3.5 and Gemini, for the generation of brochures for patient education [<xref rid="REF6" ref-type="bibr">6</xref>,<xref rid="REF7" ref-type="bibr">7</xref>]. As there were no human participants, the study was deemed exempt.&#x000a0;However, the study adheres to principles of ethical AI evaluation in medical research.</p><p>Each AI tool was provided with identical standardized prompts to generate patient education guides for varicella, HFMD, and measles. The chatbots were given one prompt after the other to write a patient education guide for three diseases. The prompts given were as follows:&#x000a0;&#x0201c;Write a patient education guide for 'varicella,'" &#x0201c;Write a patient education guide for 'hand-foot-mouth disease,'" and &#x0201c;Write a patient education guide for 'measles.'"&#x000a0;The responses generated were then collected in a Microsoft Word document (Microsoft Corporation, Redmond, WA, USA) for further analysis.</p><p>All responses were further graded using the Flesch-Kincaid Calculator (Good Calculators: <ext-link xlink:href="https://goodcalculators.com" ext-link-type="uri">https://goodcalculators.com</ext-link>/), which assessed the readability, ease of understanding, and grade level required for comprehension [<xref rid="REF8" ref-type="bibr">8</xref>]. The QuillBot plagiarism tool (QuillBot, Chicago, IL, USA) was used to calculate a similarity percentage to ensure the originality of the content produced by the chatbots [<xref rid="REF9" ref-type="bibr">9</xref>]. Lastly, the reliability of AI-generated content was evaluated using the modified DISCERN score, which assesses conciseness, reliability, balance, references, and uncertainty on a 0-5 scale [<xref rid="REF10" ref-type="bibr">10</xref>]. Each criterion is scored, and the total score provides an overall measure of the quality of the information, with higher scores indicating greater reliability of the information.</p><p>Data was analyzed using R version 4.3.2 (R Foundation for Statistical Computing, Vienna, Austria, <ext-link xlink:href="https://www.R-project.org" ext-link-type="uri">https://www.R-project.org</ext-link>/), with comparisons between ChatGPT and Gemini performed using an unpaired t-test for numerical variables&#x000a0;[<xref rid="REF11" ref-type="bibr">11</xref>]. A p-value of less than 0.05 was considered significant. The correlation between ease and reliability scores was compared using Pearson&#x02019;s correlation coefficient. Welch's two-sample t-test was used to compare the means between ChatGPT and Gemini. The normality of the variables was assessed using the Shapiro-Wilk Test. The equality of variances of the two groups was assessed using Levene&#x02019;s test.</p></sec><sec sec-type="results"><title>Results</title><p>Based on the p-values obtained in Table <xref rid="TAB1" ref-type="table">1</xref>, there is a statistically significant difference between words generated by the two AI tools (p=0.047). Sentences, average words per sentence, and average syllables per word between the two AI tools were statistically insignificant.</p><table-wrap position="float" id="TAB1"><label>Table 1</label><caption><title>Characteristics of responses generated by ChatGPT and Gemini</title><p>* t-test; p-values &#x0003c;0.05 are considered statistically significant.</p><p>SD: standard deviation</p></caption><table frame="hsides" rules="groups"><tbody><tr style="background-color:#ccc"><td rowspan="2" colspan="1">Variables</td><td colspan="2" rowspan="1">ChatGPT</td><td colspan="2" rowspan="1">Gemini</td><td rowspan="2" colspan="1">p-value*</td></tr><tr><td rowspan="1" colspan="1">Mean</td><td rowspan="1" colspan="1">SD</td><td rowspan="1" colspan="1">Mean</td><td rowspan="1" colspan="1">SD</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Words</td><td rowspan="1" colspan="1">655.67</td><td rowspan="1" colspan="1">109.04</td><td rowspan="1" colspan="1">381.33</td><td rowspan="1" colspan="1">14.64</td><td rowspan="1" colspan="1">0.047</td></tr><tr><td rowspan="1" colspan="1">Sentences</td><td rowspan="1" colspan="1">61.67</td><td rowspan="1" colspan="1">9.71</td><td rowspan="1" colspan="1">49.00</td><td rowspan="1" colspan="1">8.54</td><td rowspan="1" colspan="1">0.165</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Average words per sentence</td><td rowspan="1" colspan="1">10.60</td><td rowspan="1" colspan="1">0.27</td><td rowspan="1" colspan="1">7.97</td><td rowspan="1" colspan="1">1.62</td><td rowspan="1" colspan="1">0.050</td></tr><tr><td rowspan="1" colspan="1">Average syllables per word</td><td rowspan="1" colspan="1">1.77</td><td rowspan="1" colspan="1">0.06</td><td rowspan="1" colspan="1">1.80</td><td rowspan="1" colspan="1">0.17</td><td rowspan="1" colspan="1">0.768</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Grade level</td><td rowspan="1" colspan="1">9.40</td><td rowspan="1" colspan="1">0.70</td><td rowspan="1" colspan="1">8.77</td><td rowspan="1" colspan="1">1.55</td><td rowspan="1" colspan="1">0.555</td></tr><tr><td rowspan="1" colspan="1">Ease score</td><td rowspan="1" colspan="1">46.63</td><td rowspan="1" colspan="1">5.00</td><td rowspan="1" colspan="1">46.47</td><td rowspan="1" colspan="1">13.43</td><td rowspan="1" colspan="1">0.985</td></tr><tr style="background-color:#ccc"><td rowspan="1" colspan="1">Similarity percentage</td><td rowspan="1" colspan="1">47.27</td><td rowspan="1" colspan="1">10.07</td><td rowspan="1" colspan="1">22.07</td><td rowspan="1" colspan="1">21.08</td><td rowspan="1" colspan="1">0.135</td></tr><tr><td rowspan="1" colspan="1">Reliability score</td><td rowspan="1" colspan="1">2.67</td><td rowspan="1" colspan="1">1.15</td><td rowspan="1" colspan="1">3.00</td><td rowspan="1" colspan="1">1.00</td><td rowspan="1" colspan="1">0.725</td></tr></tbody></table></table-wrap><p>The mean grade level for the three diseases was found to be 9.40 for ChatGPT versus 8.77 for Gemini. The difference was statistically insignificant (p=0.555). The mean ease score seen for ChatGPT and Gemini was 46.63 and 46.47, respectively (p=0.985). According to the modified DISCERN score, the mean reliability score was 3/5 in Gemini's case, whereas in ChatGPT's case, it was 2.67/5, but the difference was statistically insignificant (p=0.725). Figure <xref rid="FIG1" ref-type="fig">1</xref> shows the graphical representation.</p><fig position="anchor" fig-type="figure" id="FIG1"><label>Figure 1</label><caption><title>Graphical representation of the comparison between grade level, ease score, similarity percent, and reliability score for the patient education guide generated by ChatGPT and Gemini</title></caption><graphic xlink:href="cureus-0017-00000082705-i01" position="float"/></fig></sec><sec sec-type="discussion"><title>Discussion</title><p>This cross-sectional study compares responses from two AI tools, ChatGPT and Gemini, for brochures on patient education for varicella, measles, and HFMDs. This study revealed that ChatGPT generates more word count than Gemini, and the finding was found to be statistically significant (p=0.047). The mean reliability score according to the modified DISCERN score was 3/5 for Gemini, whereas in the case of ChatGPT, it was 2.67/5, but the difference was statistically not significant.</p><p>The recent popularity of AI tools has potential for&#x000a0;broad application in the field of medicine [<xref rid="REF12" ref-type="bibr">12</xref>]. To support this, Bo&#x0017e;i&#x00107; stated that AI is revolutionizing personalized medicine by tailoring treatment plans to individual patient characteristics, enhancing efficacy, and reducing side effects. Additionally, AI-driven virtual assistants and chatbots can potentially enhance patient engagement by providing instant, personalized health information, improving access to healthcare resources, and promoting health literacy [<xref rid="REF12" ref-type="bibr">12</xref>]. Furthermore, Hernandez et al. demonstrated that 98.5% of the responses regarding type 2 diabetes education, delivered by ChatGPT, were deemed appropriate, proving that the AI model consistently delivered accurate information aligned with the standard of care for managing type 2 diabetes and its related complications [<xref rid="REF13" ref-type="bibr">13</xref>].</p><p>In this study, the Flesch-Kincaid grade level is used to assess the grade level of the responses generated by ChatGPT and Gemini. The mean grade level for the three diseases was found to be 9.40 (0.70) for ChatGPT and 8.77 (1.55) for Gemini. A very similar finding was seen in a study by Rouhi et al., which revealed grade level to be 9.4 (2.0) for ChatGPT, indicating difficult readability at the ninth-grade reading level [<xref rid="REF14" ref-type="bibr">14</xref>]. Subsequently, this study showed that the mean ease score seen for ChatGPT and Gemini was 46.63 (5.00) and 46.47 (13.43), respectively. A slightly lower value was noticed in a study by Behers et al., which showed mean ease scores of 31 and 36.4 for ChatGPT and Gemini, respectively [<xref rid="REF15" ref-type="bibr">15</xref>]. However, both studies interpret that the study was suitable for people at the college level. A statistically significant finding in this study was that the mean words generated by ChatGPT was 655.67 (109.04) and by Gemini was 381.33 (14.64). Similarly, a study by Yalla et al. showed that the words generated by AI tools ChatGPT and Bing were 222.26 (29.17) and 100.77 (38.33), respectively, whose comparison led to a significant statistical value of p&#x0003c;0.0001 [<xref rid="REF16" ref-type="bibr">16</xref>]. A higher word count could indicate more in-depth information being provided by the chatbot, which could further enhance the reliability and quality of information being generated [<xref rid="REF15" ref-type="bibr">15</xref>,<xref rid="REF16" ref-type="bibr">16</xref>].</p><p>The concern for plagiarism also arises with the increasing usage of AI tools such as ChatGPT and Gemini. AI tools use preexisting literature and hence can generate sentences mimicking original work. When major plagiarism is detected, published work can severely damage the reputations of the plagiarist, coauthors, reviewers, editors, and their institutions. Retraction, the withdrawal of an article due to research misconduct like plagiarism, has increased significantly in recent decades and can permanently harm an author's reputation. Salvagno et al. noted that it's common for people to unintentionally replicate the ideas, statements, or written content of others, which can lead to plagiarism when proper attribution is not provided. AI tools like ChatGPT may fall into this form of plagiarism, but they can also be designed to paraphrase content like human writers do. Nevertheless, relying on such tools solely to reword existing texts in order to lower plagiarism detection by rewriting another author's work using different language would not be considered an acceptable practice in scientific research [<xref rid="REF17" ref-type="bibr">17</xref>].</p><p>The DISCERN score is a tool used to assess the quality of written health information, particularly patient information. Higher DISCERN scores indicate better quality information. In this study, the average DISCERN score was found to be 2.67 (1.15) and 3.00 (1.00) for ChatGPT and Gemini, respectively. A similar result was seen in a comparative study by Hanc&#x00131; et al., which showed that ChatGPT had a reliability score of 2.29 &#x000b1; 0.91 and Gemini scored 2.37 &#x000b1; 0.51. Furthermore, the comparison between the two was statistically significant (p&#x0003c;0.001) [<xref rid="REF18" ref-type="bibr">18</xref>].&#x000a0;However, a study done by Dursun et al. showed a significantly higher DISCERN score of 19.7&#x02009;&#x000b1;&#x02009;2.15 in ChatGPT and 21.1&#x02009;&#x000b1;&#x02009;2.63 in Gemini [<xref rid="REF19" ref-type="bibr">19</xref>].&#x000a0;There was also no correlation between the ease score and reliability score between the two software, indicating that there is a possibility that although easier to read and understand, the information could still be unreliable. However, a more in-depth analysis is needed.</p><p>Limitations</p><p>This study has some notable limitations, the most important one being the comparison between only two of the AI tools, ChatGPT and Gemini, at the time they were more commonly used and researched; however, with other AI chatbots being introduced into the market, it is essential to further compare and analyze the quality of information being produced by all models. It is also essential for future studies to investigate the accuracy of the information provided; this study only assessed the reliability, and an in-depth analysis of accuracy would be extremely beneficial.</p><p>The chatbots were only given the prompts once to ensure consistency, and this does not account for the variance of responses generated by the models, a feature future studies can look into. Additionally, this study demonstrated the trends related to pediatric exanthematous conditions only. Hence, this study has restricted information on patient education responses generated by AI tools on other diseases.&#x000a0;Another significant limitation is that the present study utilized ChatGPT 3.5 and may not produce the most up-to-date information. Finally, with new and pivotal medical updates every day, the promise of AI tools to keep up with the expansive information and provide suitable education to patients is questionable.</p></sec><sec sec-type="conclusions"><title>Conclusions</title><p>ChatGPT generated significantly longer responses than Gemini, but there was no significant difference in readability, ease of understanding, or reliability.&#x000a0;Additionally, there is no significant difference in the average ease score or grade score for common pediatric exanthematous conditions, varicella, HFMD, and measles. There is no correlation between the ease and reliability scores between the two software.</p><p>Further studies must focus on ensuring accuracy and reliability with up-to-date, evidence-based content. The content generated should be verified to ensure the language's personalization and simplification to enhance readability and include interactive features and visual aids, ensuring cultural sensitivity and accessibility, with options for different languages and formats. More validation tools for AI-generated medical content and investigating AI models across different languages and literacy levels should also be done.</p></sec></body><back><fn-group content-type="conflict"><title>Disclosures</title><fn fn-type="COI-statement"><p><bold>Human subjects:</bold> All authors have confirmed that this study did not involve human participants or tissue.</p><p><bold>Animal subjects:</bold> All authors have confirmed that this study did not involve animal subjects or tissue.</p><p><bold>Conflicts of interest:</bold> In compliance with the ICMJE uniform disclosure form, all authors declare the following:</p><p><bold>Payment/services info:</bold> All authors have declared that no financial support was received from any organization for the submitted work.</p><p><bold>Financial relationships:</bold> All authors have declared that they have no financial relationships at present or within the previous three years with any organizations that might have an interest in the submitted work.</p><p><bold>Other relationships:</bold> All authors have declared that there are no other relationships or activities that could appear to have influenced the submitted work.</p></fn></fn-group><fn-group content-type="other"><title>Author Contributions</title><fn fn-type="other"><p><bold>Concept and design:</bold>&#x000a0; Nikhil Arora, Amrutha Reshi, Tanupriya Singh</p><p><bold>Acquisition, analysis, or interpretation of data:</bold>&#x000a0; Nikhil Arora, Amrutha Reshi, Tanupriya Singh</p><p><bold>Drafting of the manuscript:</bold>&#x000a0; Nikhil Arora, Amrutha Reshi, Tanupriya Singh</p><p><bold>Critical review of the manuscript for important intellectual content:</bold>&#x000a0; Nikhil Arora, Amrutha Reshi, Tanupriya Singh</p><p><bold>Supervision:</bold>&#x000a0; Nikhil Arora, Amrutha Reshi, Tanupriya Singh</p></fn></fn-group><ref-list><title>References</title><ref id="REF1"><label>1</label><element-citation publication-type="journal"><article-title>The rash with maculopapules and fever in children</article-title><source>Clin Dermatol</source><person-group>
<name><surname>Muzumdar</surname><given-names>S</given-names></name>
<name><surname>Rothe</surname><given-names>MJ</given-names></name>
<name><surname>Grant-Kels</surname><given-names>JM</given-names></name>
</person-group><fpage>119</fpage><lpage>128</lpage><volume>37</volume><year>2019</year><pub-id pub-id-type="pmid">30981292</pub-id>
</element-citation></ref><ref id="REF2"><label>2</label><element-citation publication-type="journal"><article-title>OpenAI</article-title><date-in-citation content-type="access-date">
<month>9</month>
<year>2024</year>
</date-in-citation><volume>17</volume><year>2024</year><uri xlink:href="https://openai.com/product"> https://openai.com/product</uri></element-citation></ref><ref id="REF3"><label>3</label><element-citation publication-type="webpage"><article-title>ChatGPT reaches 100 million users two months after launch</article-title><source>The guardian [Internet</source><date-in-citation content-type="access-date">
<month>9</month>
<year>2024</year>
</date-in-citation><person-group>
<name><surname>Milmo D. ChatGPT reaches 100 million users two months after</surname><given-names>launch</given-names></name>
</person-group><year>2023</year><uri xlink:href="https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app">https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app</uri></element-citation></ref><ref id="REF4"><label>4</label><element-citation publication-type="webpage"><article-title>OpenAI rages at report that Google&#x02019;s new AI crushes GPT-4</article-title><date-in-citation content-type="access-date">
<month>9</month>
<year>2024</year>
</date-in-citation><person-group>
<name><surname>Dupr&#x000e9;</surname><given-names>MH</given-names></name>
</person-group><year>2023</year><uri xlink:href="https://futurism.com/the-byte/openai-report-google-ai-gpt-4?utm_medium=email&#x00026;utm_source=transaction."> https://futurism.com/the-byte/openai-report-google-ai-gpt-4?utm_medium=email&#x00026;utm_source=transaction.</uri></element-citation></ref><ref id="REF5"><label>5</label><element-citation publication-type="journal"><article-title>ChatGPT: promise and challenges for deployment in low- and middle-income countries</article-title><source>Lancet Reg Health West Pac</source><person-group>
<name><surname>Wang</surname><given-names>X</given-names></name>
<name><surname>Sanders</surname><given-names>HM</given-names></name>
<name><surname>Liu</surname><given-names>Y</given-names></name>
<etal/>
</person-group><fpage>100905</fpage><volume>41</volume><year>2023</year><pub-id pub-id-type="pmid">37731897</pub-id>
</element-citation></ref><ref id="REF6"><label>6</label><element-citation publication-type="journal"><article-title>ChatGPT</article-title><date-in-citation content-type="access-date">
<month>9</month>
<year>2024</year>
</date-in-citation><volume>17</volume><year>2024</year><uri xlink:href="https://chat.openai.com">https://chat.openai.com</uri></element-citation></ref><ref id="REF7"><label>7</label><element-citation publication-type="webpage"><article-title>Gemini</article-title><date-in-citation content-type="access-date">
<month>9</month>
<year>2024</year>
</date-in-citation><year>2024</year><uri xlink:href="https://gemini.google.com/">https://gemini.google.com/</uri></element-citation></ref><ref id="REF8"><label>8</label><element-citation publication-type="webpage"><article-title>Flesch-Kincaid calculator</article-title><date-in-citation content-type="access-date">
<month>9</month>
<year>2024</year>
</date-in-citation><year>2024</year><uri xlink:href="https://goodcalculators.com/flesch-kincaid-calculator/">https://goodcalculators.com/flesch-kincaid-calculator/</uri></element-citation></ref><ref id="REF9"><label>9</label><element-citation publication-type="journal"><article-title>Using the online paraphrasing tool Quillbot to assist students in paraphrasing the source information: English-majored students&#x02019; perceptions</article-title><source>Proceedings of the 5th Conference on Language Teaching and Learning 2023</source><person-group>
<name><surname>Xuyen</surname><given-names>NT</given-names></name>
</person-group><fpage>21</fpage><lpage>27</lpage><year>2023</year></element-citation></ref><ref id="REF10"><label>10</label><element-citation publication-type="journal"><article-title>Assessment of reliability and quality of videos on medial epicondylitis shared on YouTube</article-title><source>Cureus</source><person-group>
<name><surname>Uzun</surname><given-names>O</given-names></name>
</person-group><fpage>37250</fpage><volume>15</volume><year>2023</year></element-citation></ref><ref id="REF11"><label>11</label><element-citation publication-type="webpage"><article-title>The R Project for statistical computing</article-title><date-in-citation content-type="access-date">
<month>9</month>
<year>2024</year>
</date-in-citation><year>2024</year><uri xlink:href="https://www.r-project.org/">https://www.r-project.org/</uri></element-citation></ref><ref id="REF12"><label>12</label><element-citation publication-type="journal"><article-title>The role of artificial intelligence in increasing the health literacy of patients</article-title><source>Int J Digit Health Patient Care</source><person-group>
<name><surname>Bo&#x0017e;&#x00069;&#x00307;&#x00107;</surname><given-names>V</given-names></name>
</person-group><fpage>1</fpage><lpage>21</lpage><volume>1</volume><year>2024</year><uri xlink:href="https://ndpapublishing.com/index.php/ijdhpc/article/view/19">https://ndpapublishing.com/index.php/ijdhpc/article/view/19</uri></element-citation></ref><ref id="REF13"><label>13</label><element-citation publication-type="journal"><article-title>The future of patient education: AI-driven guide for type 2 diabetes</article-title><source>Cureus</source><person-group>
<name><surname>Hernandez</surname><given-names>CA</given-names></name>
<name><surname>Vazquez Gonzalez</surname><given-names>AE</given-names></name>
<name><surname>Polianovskaia</surname><given-names>A</given-names></name>
<etal/>
</person-group><fpage>48919</fpage><volume>15</volume><year>2023</year></element-citation></ref><ref id="REF14"><label>14</label><element-citation publication-type="journal"><article-title>Can artificial intelligence improve the readability of patient education materials on aortic stenosis? A pilot study</article-title><source>Cardiol Ther</source><person-group>
<name><surname>Rouhi</surname><given-names>AD</given-names></name>
<name><surname>Ghanem</surname><given-names>YK</given-names></name>
<name><surname>Yolchieva</surname><given-names>L</given-names></name>
<etal/>
</person-group><fpage>137</fpage><lpage>147</lpage><volume>13</volume><year>2024</year><pub-id pub-id-type="pmid">38194058</pub-id>
</element-citation></ref><ref id="REF15"><label>15</label><element-citation publication-type="journal"><article-title>Assessing the readability of patient education materials on cardiac catheterization from artificial intelligence chatbots: an observational cross-sectional study</article-title><source>Cureus</source><person-group>
<name><surname>Behers</surname><given-names>BJ</given-names></name>
<name><surname>Vargas</surname><given-names>IA</given-names></name>
<name><surname>Behers</surname><given-names>BM</given-names></name>
<name><surname>Rosario</surname><given-names>MA</given-names></name>
<name><surname>Wojtas</surname><given-names>CN</given-names></name>
<name><surname>Deevers</surname><given-names>AC</given-names></name>
<name><surname>Hamad</surname><given-names>KM</given-names></name>
</person-group><fpage>63865</fpage><volume>16</volume><year>2024</year></element-citation></ref><ref id="REF16"><label>16</label><element-citation publication-type="journal"><article-title>Performance of artificial intelligence chatbots on glaucoma questions adapted from patient brochures</article-title><source>Cureus</source><person-group>
<name><surname>Yalla</surname><given-names>GR</given-names></name>
<name><surname>Hyman</surname><given-names>N</given-names></name>
<name><surname>Hock</surname><given-names>LE</given-names></name>
<name><surname>Zhang</surname><given-names>Q</given-names></name>
<name><surname>Shukla</surname><given-names>AG</given-names></name>
<name><surname>Kolomeyer</surname><given-names>NN</given-names></name>
</person-group><fpage>56766</fpage><volume>16</volume><year>2024</year></element-citation></ref><ref id="REF17"><label>17</label><element-citation publication-type="journal"><article-title>Can artificial intelligence help for scientific writing?</article-title><source>Crit Care</source><person-group>
<name><surname>Salvagno</surname><given-names>M</given-names></name>
<name><surname>Taccone</surname><given-names>FS</given-names></name>
<name><surname>Gerli</surname><given-names>AG</given-names></name>
</person-group><fpage>75</fpage><volume>27</volume><year>2023</year><pub-id pub-id-type="pmid">36841840</pub-id>
</element-citation></ref><ref id="REF18"><label>18</label><element-citation publication-type="journal"><article-title>Assessment of readability, reliability, and quality of ChatGPT&#x000ae;, BARD&#x000ae;, Gemini&#x000ae;, Copilot&#x000ae;, Perplexity&#x000ae; responses on palliative care</article-title><source>Medicine (Baltimore)</source><person-group>
<name><surname>Hanc&#x00131;</surname><given-names>V</given-names></name>
<name><surname>Erg&#x000fc;n</surname><given-names>B</given-names></name>
<name><surname>G&#x000fc;l</surname><given-names>&#x0015e;</given-names></name>
<name><surname>Uzun</surname><given-names>&#x000d6;</given-names></name>
<name><surname>Erdemir</surname><given-names>&#x00130;</given-names></name>
<name><surname>Hanc&#x00131;</surname><given-names>FB</given-names></name>
</person-group><fpage>39305</fpage><volume>103</volume><year>2024</year></element-citation></ref><ref id="REF19"><label>19</label><element-citation publication-type="journal"><article-title>Can artificial intelligence models serve as patient information consultants in orthodontics?</article-title><source>BMC Med Inform Decis Mak</source><person-group>
<name><surname>Dursun</surname><given-names>D</given-names></name>
<name><surname>Bilici Ge&#x000e7;er</surname><given-names>R</given-names></name>
</person-group><fpage>211</fpage><volume>24</volume><year>2024</year><pub-id pub-id-type="pmid">39075513</pub-id>
</element-citation></ref></ref-list></back></article>