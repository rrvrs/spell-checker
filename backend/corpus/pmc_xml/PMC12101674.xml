<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS One</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-title-group><journal-title>PLOS One</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40408346</article-id><article-id pub-id-type="pmc">PMC12101674</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0321026</article-id><article-id pub-id-type="publisher-id">PONE-D-24-53882</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Computer Vision</subject><subj-group><subject>Target Detection</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Geoinformatics</subject><subj-group><subject>Remote Sensing Imagery</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Earth Sciences</subject><subj-group><subject>Geography</subject><subj-group><subject>Geoinformatics</subject><subj-group><subject>Remote Sensing Imagery</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Remote Sensing Imagery</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Remote Sensing</subject><subj-group><subject>Remote Sensing Imagery</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Remote Sensing</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Mathematical and Statistical Techniques</subject><subj-group><subject>Mathematical Functions</subject><subj-group><subject>Convolution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Neck</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Neck</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Recreation</subject><subj-group><subject>Sports</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Recreation</subject><subj-group><subject>Sports</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Sports Science</subject><subj-group><subject>Sports</subject></subj-group></subj-group></subj-group></article-categories><title-group><article-title>LI-YOLOv8: Lightweight small target detection algorithm for remote sensing images that combines GSConv and PConv</article-title><alt-title alt-title-type="running-head">Lightweight small target detection algorithm for remote sensing images that combines GSConv and PConv</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0006-2095-5500</contrib-id><name><surname>Yan</surname><given-names>Pingping</given-names></name><role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref><xref rid="cor001" ref-type="corresp">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Qi</surname><given-names>Xiangming</given-names></name><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="aff001" ref-type="aff">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Jiang</surname><given-names>Liang</given-names></name><role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role><role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role><role content-type="http://credit.niso.org/contributor-roles/software/">Software</role><role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role><role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="aff002" ref-type="aff">
<sup>2</sup>
</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Liaoning Technical University, School of Software, Huludao, Liaoning, China</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Tarim University, School of Information Engineering, Alar, Xinjiang, China</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Chen</surname><given-names>Yile</given-names></name><role>Editor</role><xref rid="edit1" ref-type="aff"/></contrib></contrib-group><aff id="edit1">
<addr-line>Macau University of Science and Technology, MACAO</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>No authors have competing interests.</p></fn><corresp id="cor001">* E-mail: <email>18698929685@163.com</email></corresp></author-notes><pub-date pub-type="epub"><day>23</day><month>5</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>20</volume><issue>5</issue><elocation-id>e0321026</elocation-id><history><date date-type="received"><day>23</day><month>11</month><year>2024</year></date><date date-type="accepted"><day>28</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 Yan et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Yan et al</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0321026.pdf"/><abstract><p>In the domain of remote sensing image small target detection, challenges such as difficulties in extracting features of small targets, complex backgrounds that easily lead to confusion with targets, and high computational complexity with significant resource consumption are prevalent. We propose a lightweight small target detection algorithm for remote sensing images that combines GSConv and PConv, named LI-YOLOv8. Using YOLOv8n as the baseline algorithm, the activation function SiLU in the CBS at the backbone network&#x02019;s SPPF is replaced with ReLU, which reduces interdependencies among parameters. Then, RFAConv is embedded after the first CBS to expand the receptive field and extract more features of small targets. An efficient Multi-Scale Attention (EMA) mechanism is embedded at the terminal of C2f within the neck network to integrate more detailed information, enhancing the focus on small targets. The head network incorporates a lightweight detection head, GP-Detect, which combines GSConv and PConv to decrease the parameter count and computational demand. Integrating Inner-IoU and Wise-IoU v3 to design the Inner-Wise IoU loss function, replacing the original CIoU loss function. This approach provides the algorithm with a gain distribution strategy, focuses on anchor boxes of ordinary quality, and strengthens generalization capability. We conducted ablation and comparative experiments on the public datasets RSOD and NWPU VHR-10. Compared to YOLOv8, our approach achieved improvements of 7.6% and 2.8% in mAP@0.5, and increases of 2.1% and 1.1% in mAP@0.5:0.95. Furthermore, Parameters and GFLOPs were reduced by 10.0% and 23.2%, respectively, indicating a significant enhancement in detection accuracy along with a substantial decrease in both parameters and computational costs. Generalization experiments were conducted on the TinyPerson, LEVIR-ship, brain-tumor, and smoke_fire_1 datasets. The mAP@0.5 metric improved by 2.6%, 5.3%, 2.6%, and 2.3%, respectively, demonstrating the algorithm&#x02019;s robust performance.</p></abstract><funding-group><funding-statement>This research was funded by the National Natural Science Foundation of China (no. 62173171).</funding-statement></funding-group><counts><fig-count count="15"/><table-count count="7"/><page-count count="25"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data are available at: <ext-link xlink:href="https://github.com/2470589561/datasets" ext-link-type="uri">https://github.com/2470589561/datasets</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data are available at: <ext-link xlink:href="https://github.com/2470589561/datasets" ext-link-type="uri">https://github.com/2470589561/datasets</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Remote sensing imagery is extensively researched and applied across various fields, including environmental monitoring and protection, urban planning and management, and crop yield prediction. However, detecting small targets within remote sensing images presents several challenges, such as difficulties in feature extraction, complex backgrounds that can easily be confused with targets, significant deviations in predicted bounding boxes, and stringent accuracy requirements, all of which hinder precise detection. With the advancement of intelligent manufacturing in China, deep learning-based object detection methods have gained increasing prominence. Single-stage detection algorithms, exemplified by the Single Shot MultiBox Detector (SSD [<xref rid="pone.0321026.ref001" ref-type="bibr">1</xref>]) and You Only Look Once (YOLO [<xref rid="pone.0321026.ref002" ref-type="bibr">2</xref>&#x02013;<xref rid="pone.0321026.ref006" ref-type="bibr">6</xref>]) series, have become the mainstream for small target detection in remote sensing images due to their advantages in detection speed, lower parameter counts, and high recognition rates.</p><p>In 2022, Zhang et al. [<xref rid="pone.0321026.ref007" ref-type="bibr">7</xref>] incorporated the Bottleneck Attention Module (BAM [<xref rid="pone.0321026.ref008" ref-type="bibr">8</xref>]) into YOLOv5, enhancing the focus on small target information within shallow feature maps. This approach demonstrated significant effectiveness in detecting small-scale objects but failed to control the resulting increase in the number of parameters. Similarly, Luo et al. [<xref rid="pone.0321026.ref009" ref-type="bibr">9</xref>] integrated an adaptive spatial feature fusion module into the neck network of YOLOv4, effectively capturing global information about small targets; however, this required substantial hardware storage capacity. In 2023, Zhao et al. [<xref rid="pone.0321026.ref010" ref-type="bibr">10</xref>] utilized YOLOv7 as the baseline algorithm, incorporating a small target detection head and attention mechanisms to improve detection performance for small targets on water surfaces, albeit with increased model complexity. Zhang et al. [<xref rid="pone.0321026.ref011" ref-type="bibr">11</xref>] developed a compact DSC-SE module that fuses deep separable convolution with SE attention, reducing the parameter volume of the insulator defect model, yet it falls short in extracting small targets. Xie et al. [<xref rid="pone.0321026.ref012" ref-type="bibr">12</xref>] developed a lightweight feature extraction module, CSPPartialStage, which was introduced into YOLOv7 to reduce redundant computations without compromising the accuracy of small target detection in remote sensing images; however, the computational burden remained significant. In 2024, Cheng et al. [<xref rid="pone.0321026.ref013" ref-type="bibr">13</xref>] introduced Omni-Dimensional Dynamic Convolution (ODConv [<xref rid="pone.0321026.ref014" ref-type="bibr">14</xref>]) and a global attention mechanism to suppress redundant and insignificant feature expressions; however, these techniques lacked adaptability across multiple scenarios. Finally, these methods suppress redundant and insignificant feature expressions; however, they lack adaptability across multiple scenarios. Zhu et al. [<xref rid="pone.0321026.ref015" ref-type="bibr">15</xref>] integrated an innovative lightweight Spatial Pyramid Dilated Convolution Cross-Stage Partial Channel (LSPHDCCSPC) module into the YOLOv7 backbone network, which bolsters the capability to extract features from small targets; however, this integration has led to a decrement in the detection and recognition accuracy of these targets.</p><p>In summary, despite significant advancements in the research and application of small target detection in remote sensing images, several challenges persist. These challenges include inadequate focus on small targets, high algorithmic complexity, increased rates of missed or false detections, and limited generalization capabilities. To address these issues, this paper proposes a lightweight small target detection algorithm for remote sensing images that integrates GSConv and PConv within the YOLOv8n framework. The main contributions of this study are as follows:</p><list list-type="order"><list-item><p>In the backbone network&#x02019;s Spatial Pyramid Pooling Fast (SPPF) module, the SiLU activation function within the CBS layer is replaced with ReLU to reduce parameter interdependencies. Additionally, RFAConv is integrated after the first CBS layer to enhance focus on sample areas, thereby improving small target recognition performance.</p></list-item><list-item><p>An efficient multi-scale attention mechanism (EMA) is embedded at the terminal of C2f within the neck network to capture more detailed information, enhancing the focus on the features of small target areas.</p></list-item><list-item><p>The original detection head is replaced with GP-Detect, a lightweight detection head designed by combining GSConv and PConv, reducing parameters and computational load.</p></list-item><list-item><p>The border fitting loss function of the algorithm is optimized by replacing CIoU with the Inner-Wise IoU loss function, which is designed by integrating Inner-IoU and Wise-IoU v3. This approach focuses on anchor boxes of ordinary quality through a gain allocation strategy, thereby enhancing the algorithm&#x02019;s generalization capability.</p></list-item></list></sec><sec id="sec002"><title>Fundamentals of the YOLOv8 model</title><p>YOLOv8 represents an optimized and enhanced iteration of YOLOv5, integrating advanced technologies such as the Path Aggregation Feature Pyramid (PA-FPN) network architecture, an anchor-free design, and a decoupled head. It is available in five variants: n, s, m, l, and x, each with progressively increasing sizes and parameter counts. YOLOv8 comprises three main components: Backbone, Neck, and Head, as illustrated in <xref rid="pone.0321026.g001" ref-type="fig">Fig 1</xref>.</p><fig position="float" id="pone.0321026.g001"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g001</object-id><label>Fig 1</label><caption><title>YOLOv8 structure and working principle.</title></caption><graphic xlink:href="pone.0321026.g001" position="float"/></fig><p>The backbone network comprises three modules: CBS, C2f, and SPPF [<xref rid="pone.0321026.ref016" ref-type="bibr">16</xref>]. The CBS module extracts initial image features, while C2f captures features at scales of <inline-formula id="pone.0321026.e001"><alternatives><graphic xlink:href="pone.0321026.e001.jpg" id="pone.0321026.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mn>80</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mn>80</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> (S1), <inline-formula id="pone.0321026.e002"><alternatives><graphic xlink:href="pone.0321026.e002.jpg" id="pone.0321026.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mrow><mml:mn>40</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mn>40</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> (S2), and <inline-formula id="pone.0321026.e003"><alternatives><graphic xlink:href="pone.0321026.e003.jpg" id="pone.0321026.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mn>20</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mn>20</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> (S3) pixels, thereby providing high-level semantic information across different scales. The SPPF module extends the receptive field to integrate multi-scale features. However, the activation function of the convolution layer in SPPF incurs high computational complexity, and the number of small target feature points captured is limited, making the model susceptible to missed detections of small targets.</p><p>The neck network comprises the Feature Pyramid Network (FPN [<xref rid="pone.0321026.ref017" ref-type="bibr">17</xref>]) and the Path Aggregation Network (PAN [<xref rid="pone.0321026.ref018" ref-type="bibr">18</xref>]). The FPN transfers deep semantic features downward, while the PAN propagates localization information upward. This neck network effectively integrates features across different levels, facilitating multi-scale learning that enriches the semantic information of contextual features and enhances target perception capabilities. During the feature fusion stage of the neck network, each pixel in the image undergoes multiple compressions and concatenations through CBS and Concat operations. However, during C2f feature extraction, insufficient attention to the feature areas of small targets can lead to the loss of detailed information.</p><p>The detection head primarily employs a decoupled head configuration, which segregates regression and classification tasks. Variance Focal Loss (VFL) serves as the classification loss function, balancing the weights between targets and backgrounds during the training of small target detection, thereby enhancing the predicted object class probabilities. Distribution Focal Loss (DFL), when combined with Complete Intersection over Union (CIoU [<xref rid="pone.0321026.ref019" ref-type="bibr">19</xref>]) as a regression loss function, rapidly focuses on regions proximal to the target to obtain accurate bounding box position information. However, the detection head contains convolutional redundancies, with its number of parameters and computational cost representing approximately 25% of the total parameters in YOLOv8n. This results in a significant computational burden when detecting small targets.</p></sec><sec sec-type="materials|methods" id="sec003"><title>Methods</title><sec id="sec004"><title>Algorithm implementation</title><p>The structure and working principles of LI-YOLOv8 are shown in <xref rid="pone.0321026.g002" ref-type="fig">Fig 2</xref>. SPPF-R denotes the refined Spatial Pyramid Pooling module, utilized for augmenting feature extraction of small objects. The C2f-E signifies the upgraded C2f module, aimed at elevating the focus on small objects. Lastly, GP-Detect refers to the improved detection head, engineered to decrement the network&#x02019;s complexity.</p><fig position="float" id="pone.0321026.g002"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g002</object-id><label>Fig 2</label><caption><title>Structure and working principle of LI-YOLOv8.</title></caption><graphic xlink:href="pone.0321026.g002" position="float"/></fig></sec><sec id="sec005"><title>SPPF-R enhances feature extraction</title><p>SPPF processes the input feature map through CBS to capture preliminary characteristics. Given the small pixel size of targets within the receptive field area, the initial feature hboxextraction is not comprehensive. To ameliorate this, the activation function SiLU in CBS at SPPF is replaced by ReLU, resulting in CBR. Furthermore, RFAConv is embedded in the first CBR to bolster the focus on feature information of various targets within the receptive field, thereby enhancing feature extraction. After improvement, SPPF is denoted as SPPF-R. The improvement processes for CBR and SPPF-R are shown in <xref rid="pone.0321026.g003" ref-type="fig">Figs 3</xref> and <xref rid="pone.0321026.g004" ref-type="fig">4</xref>.</p><fig position="float" id="pone.0321026.g003"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g003</object-id><label>Fig 3</label><caption><title>Improvement of CBS to CBR process.</title></caption><graphic xlink:href="pone.0321026.g003" position="float"/></fig><fig position="float" id="pone.0321026.g004"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g004</object-id><label>Fig 4</label><caption><title>Improvement of SPPF to SPPF-R process.</title></caption><graphic xlink:href="pone.0321026.g004" position="float"/></fig><p>For an input feature map of size <inline-formula id="pone.0321026.e004"><alternatives><graphic xlink:href="pone.0321026.e004.jpg" id="pone.0321026.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>H</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, RFAConv [<xref rid="pone.0321026.ref020" ref-type="bibr">20</xref>] first employs average pooling (AvgPool) to aggregate global features across each receptive field. It then utilizes three parallel 1<inline-formula id="pone.0321026.e005"><alternatives><graphic xlink:href="pone.0321026.e005.jpg" id="pone.0321026.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>1 group convolutions (Group Conv) to rapidly extract and interact with features. This is followed by a Softmax function, which emphasizes the importance of each feature within the receptive field, thereby generating attention maps with channel dimensions of <inline-formula id="pone.0321026.e006"><alternatives><graphic xlink:href="pone.0321026.e006.jpg" id="pone.0321026.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mrow><mml:mn>9</mml:mn><mml:mi>C</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mi>H</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mi>W</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. Subsequently, the input feature map undergoes a 3<inline-formula id="pone.0321026.e007"><alternatives><graphic xlink:href="pone.0321026.e007.jpg" id="pone.0321026.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>3 group convolution to capture spatial information within the receptive field. This process extracts and interacts with feature information, resulting in a receptive-field spatial feature map of dimensions <inline-formula id="pone.0321026.e008"><alternatives><graphic xlink:href="pone.0321026.e008.jpg" id="pone.0321026.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:mn>9</mml:mn><mml:mi>C</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mi>H</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mi>W</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> . The attention map and the receptive-field spatial feature map are then reweighted and dimensionally adjusted to produce a feature map with channel dimensions of <inline-formula id="pone.0321026.e009"><alternatives><graphic xlink:href="pone.0321026.e009.jpg" id="pone.0321026.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:mi>C</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mn>3</mml:mn><mml:mi>H</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mn>3</mml:mn><mml:mi>W</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, which is subjected to a 3<inline-formula id="pone.0321026.e010"><alternatives><graphic xlink:href="pone.0321026.e010.jpg" id="pone.0321026.e010g" position="anchor"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>3 convolution to output an image with the same dimensions as the original feature map. The formula for RFA is expressed as:</p><disp-formula id="pone.0321026.e011"><alternatives><graphic xlink:href="pone.0321026.e011.jpg" id="pone.0321026.e011g" position="anchor"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>&#x000d7;</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(1)</label></disp-formula><p>In the formula, <inline-formula id="pone.0321026.e012"><alternatives><graphic xlink:href="pone.0321026.e012.jpg" id="pone.0321026.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> represents a grouped convolution of size <inline-formula id="pone.0321026.e013"><alternatives><graphic xlink:href="pone.0321026.e013.jpg" id="pone.0321026.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mspace width="0.167em"/><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow><mml:mspace width="0.167em"/><mml:mi>i</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, denotes the size of the convolution kernel, and <italic toggle="yes">Norm</italic> represents normalization. <italic toggle="yes">X</italic> represents the input feature map, while <italic toggle="yes">F</italic> is obtained by multiplying the attention map <italic toggle="yes">A</italic><sub><italic toggle="yes">rf</italic></sub> with the transformed spatial feature of the receptive field <italic toggle="yes">F</italic><sub><italic toggle="yes">rf</italic></sub>. Its working principle is expressed in <xref rid="pone.0321026.g005" ref-type="fig">Fig 5</xref>:</p><fig position="float" id="pone.0321026.g005"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g005</object-id><label>Fig 5</label><caption><title>RFAConv working principle.</title></caption><graphic xlink:href="pone.0321026.g005" position="float"/></fig><p>Therefore, the refinement process of SPPF-R is specifically analyzed as follows:</p><p>(1) SiLU replacement with ReLU. The activation function in CBS is replaced with ReLU to form CBR, ensuring that negative input values are set to zero, while positive values remain intact. The replaced activation function discards complex computations, effectively avoiding unnecessary information interference and reducing the dependency among parameters.</p><p>(2) Embedding RFAConv. The embedding of RFAConv after the first CBR integrates spatial attention mechanisms with conventional convolution to achieve flexible adjustment of convolutional kernel parameters. It also focuses on different spatial feature information within each receptive field, efficiently identifying and processing local areas in images, and markedly improving the ability to perceive and extract small targets within intricate settings.</p></sec><sec id="sec006"><title>C2f-E enhances attention to small target areas</title><p>In the neck network, each pixel of the image undergoes multiple filtering or sliding window operations. The resulting feature-extracted images are then concatenated, which can lead to blurred feature information for small targets and the loss of critical features during deep extraction. To address these issues, the C2f module incorporates the Efficient Multi-Scale Attention (EMA) mechanism, enhancing the capture of pixel-level attention features for small targets and effectively reducing the loss of feature information. This enhanced unit is designated as C2f-E, with the improvement process depicted in <xref rid="pone.0321026.g006" ref-type="fig">Fig 6</xref>.</p><fig position="float" id="pone.0321026.g006"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g006</object-id><label>Fig 6</label><caption><title>C2f-E improvement process.</title></caption><graphic xlink:href="pone.0321026.g006" position="float"/></fig><p>The Efficient Multi-Scale Attention (EMA [<xref rid="pone.0321026.ref021" ref-type="bibr">21</xref>]) mechanism processes an input feature map <inline-formula id="pone.0321026.e014"><alternatives><graphic xlink:href="pone.0321026.e014.jpg" id="pone.0321026.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>H</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> by dividing it into G sub-features along the channel dimension, thereby facilitating the learning of diverse semantic representations. Specifically, <inline-formula id="pone.0321026.e015"><alternatives><graphic xlink:href="pone.0321026.e015.jpg" id="pone.0321026.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <italic toggle="yes">C</italic> denotes the number of channels, and <italic toggle="yes">H</italic> and <italic toggle="yes">W</italic> represent the height and width of the feature map, respectively. The working principle of EMA is illustrated in <xref rid="pone.0321026.g007" ref-type="fig">Fig 7</xref>.</p><fig position="float" id="pone.0321026.g007"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g007</object-id><label>Fig 7</label><caption><title>EMA attention mechanism working principle.</title></caption><graphic xlink:href="pone.0321026.g007" position="float"/></fig><p>EMA captures attention weights for segmented feature maps via three concurrent pathways, comprising two 1x1 and a single 3x3 branch, capturing multi-scale detail features.</p><p>First, the 1<inline-formula id="pone.0321026.e016"><alternatives><graphic xlink:href="pone.0321026.e016.jpg" id="pone.0321026.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>1 branches apply 1D horizontal and 1D vertical Avg pool to encode channels along two spatial dimensions, enabling channel descriptors to accurately represent global positional information. Next, following the concatenation of spatial feature vectors along two directions, a 1x1 convolution is performed, which subsequently decomposes and outputs two distinct feature vectors. The Sigmoid operation restricts its outcomes to the interval from 0 to 1, followed by a re-weighting operation with the original feature map channel weights, which helps alleviate the imbalance between complex and simple samples among categories. The 3<inline-formula id="pone.0321026.e017"><alternatives><graphic xlink:href="pone.0321026.e017.jpg" id="pone.0321026.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>3 branch captures multi-scale features using a 3<inline-formula id="pone.0321026.e018"><alternatives><graphic xlink:href="pone.0321026.e018.jpg" id="pone.0321026.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>3 Conv.</p><p>After obtaining the weighted channels, GN is applied, followed by 2D global average pooling to encapsulate global spatial attributes from the outputs across each branch, producing outputs of dimensions (<inline-formula id="pone.0321026.e019"><alternatives><graphic xlink:href="pone.0321026.e019.jpg" id="pone.0321026.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mo>/</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msup><mml:mi>&#x000d7;</mml:mi><mml:msup><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mo>/</mml:mo><mml:mi>G</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>H</mml:mi><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>). A SoftMax is utilized to linearly transform the output, followed by matrix multiplication (Matmul) for local inter-channel interaction, capturing pixel-level pairwise relationships and integrating information from both directions. Finally, the values outputted from Sigmoid are reweighted with initial feature values, resulting in an output that maintains the same dimensions as the original feature map.</p><p>EMA is embedded at the end of C2f. It employs a parallel substructure to ensure that secure an uniform distribution of spatial semantic traits in each feature collective. By aggregating multi-scale spatial structural information, it mitigates the decline in small target recognition performance attributable to complex sequential processing and profound convolution. This mechanism effectively captures pixel-level attention features, establishes dependencies between dimensions, and enhances important regions within each sub-feature based on the learned weights, resulting in precise target localization information and increased attention to small target areas.</p></sec><sec id="sec007"><title>GP-detect reduces the number of parameters and computational load</title><p>In the baseline algorithm, each of the three detection heads consists of two parallel Conv3<inline-formula id="pone.0321026.e020"><alternatives><graphic xlink:href="pone.0321026.e020.jpg" id="pone.0321026.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>3 layers and one Conv1<inline-formula id="pone.0321026.e021"><alternatives><graphic xlink:href="pone.0321026.e021.jpg" id="pone.0321026.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>1 layer. This configuration can result in convolutional redundancy during object localization and classification, leading to increased computational costs. By substituting the parallel Conv3<inline-formula id="pone.0321026.e022"><alternatives><graphic xlink:href="pone.0321026.e022.jpg" id="pone.0321026.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>3 layers with a single GSConv3<inline-formula id="pone.0321026.e023"><alternatives><graphic xlink:href="pone.0321026.e023.jpg" id="pone.0321026.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>3 and PConv3<inline-formula id="pone.0321026.e024"><alternatives><graphic xlink:href="pone.0321026.e024.jpg" id="pone.0321026.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>3 structure, both the number of parameters and the computational load are significantly reduced. The enhanced detection head is termed GP-Detect, as illustrated in <xref rid="pone.0321026.g008" ref-type="fig">Fig 8</xref>.</p><fig position="float" id="pone.0321026.g008"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g008</object-id><label>Fig 8</label><caption><title>Structure of Detect and GP-Detect.</title></caption><graphic xlink:href="pone.0321026.g008" position="float"/></fig><sec id="sec008"><title>GSConv.</title><p>GSConv [<xref rid="pone.0321026.ref022" ref-type="bibr">22</xref>] downsamples a feature map with <italic toggle="yes">c</italic><sub>1</sub> channels using Conv, outputting feature maps that process <italic toggle="yes">c</italic><sub>2</sub>/2 channels. It then employs depthwise separable convolution (DSConv [<xref rid="pone.0321026.ref023" ref-type="bibr">23</xref>]) to extract spatial and channel features. Integrates the feature maps resulting from the Conv and DSConv processes, followed by a shuffle operation to evenly distribute the features generated by the Conv throughout each part of the DSConv output. This results in a feature map with <italic toggle="yes">c</italic><sub>2</sub> channels and its working principle is shown in <xref rid="pone.0321026.g009" ref-type="fig">Fig 9</xref>.</p><fig position="float" id="pone.0321026.g009"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g009</object-id><label>Fig 9</label><caption><title>GSConv working principle.</title></caption><graphic xlink:href="pone.0321026.g009" position="float"/></fig></sec><sec id="sec009"><title>PConv.</title><p>In PConv [<xref rid="pone.0321026.ref024" ref-type="bibr">24</xref>], feature maps with input dimensions <inline-formula id="pone.0321026.e025"><alternatives><graphic xlink:href="pone.0321026.e025.jpg" id="pone.0321026.e025g" position="anchor"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>w</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> are processed using filters to extract features from channels of dimension <inline-formula id="pone.0321026.e026"><alternatives><graphic xlink:href="pone.0321026.e026.jpg" id="pone.0321026.e026g" position="anchor"/><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>w</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, while maintaining the number of remaining channels unchanged. The processed channels are then concatenated with the unprocessed channels, resulting in an output feature map that retains the same dimensions as the original. PConv reduces redundant computations and significantly preserves the original number of channels. The working principle of PConv is illustrated in <xref rid="pone.0321026.g010" ref-type="fig">Fig 10</xref>.</p><fig position="float" id="pone.0321026.g010"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g010</object-id><label>Fig 10</label><caption><title>PConv working principle.</title></caption><graphic xlink:href="pone.0321026.g010" position="float"/></fig><p>In summary, GP-Detect minimizes redundant computations and reduces the number of parameters in convolutional layers, thereby enhancing the efficiency of both classification and regression tasks. The lightweight GSConv incorporates standard convolutional information into each component of depthwise separable convolutions, facilitating the effective aggregation of global information. This approach mitigates semantic information loss associated with the compression of spatial dimensions and the expansion of channel dimensions while simultaneously decreasing parameter counts and computational requirements. Additionally, PConv selectively extracts features from specific spatial dimensions, thereby lowering computational demands and parameter scales, and significantly improving the recognition of small targets.</p></sec></sec><sec id="sec010"><title>Inner-wise IoU enhances generalization ability</title><p>The bounding box loss function CIoU focuses solely on the distance between the centers of the ground truth and predicted boxes, along with the anchor box&#x02019;s width-to-height ratio. It ignores the precision of target box labeling and the equilibrium of instance distribution. Low-quality samples, under the influence of efficient fitting, affect feature learning, leading to insufficient algorithm generalization capability. Following the substitution of the CIoU with Inner-Wise IoU, the emphasis is placed on average-quality anchor boxes, thereby enhancing the algorithm&#x02019;s generalization capability.</p><sec id="sec011"><title>Inner-IoU.</title><p>Inner-IoU [<xref rid="pone.0321026.ref025" ref-type="bibr">25</xref>] generates auxiliary bounding boxes with varying sizes by controlling the scale factor ratio. It computes the overlap between these auxiliary boxes and the ground truth boxes, facilitating more precise localization of small targets. The formula is shown as follows:</p><p>(1) Calculate the left boundary <inline-formula id="pone.0321026.e027"><alternatives><graphic xlink:href="pone.0321026.e027.jpg" id="pone.0321026.e027g" position="anchor"/><mml:math id="M27" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, right boundary <inline-formula id="pone.0321026.e028"><alternatives><graphic xlink:href="pone.0321026.e028.jpg" id="pone.0321026.e028g" position="anchor"/><mml:math id="M28" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, top boundary <inline-formula id="pone.0321026.e029"><alternatives><graphic xlink:href="pone.0321026.e029.jpg" id="pone.0321026.e029g" position="anchor"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, and lower boundary <inline-formula id="pone.0321026.e030"><alternatives><graphic xlink:href="pone.0321026.e030.jpg" id="pone.0321026.e030g" position="anchor"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> of the ground truth bounding box.</p><disp-formula id="pone.0321026.e031"><alternatives><graphic xlink:href="pone.0321026.e031.jpg" id="pone.0321026.e031g" position="anchor"/><mml:math id="M31" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(2)</label></disp-formula><disp-formula id="pone.0321026.e032"><alternatives><graphic xlink:href="pone.0321026.e032.jpg" id="pone.0321026.e032g" position="anchor"/><mml:math id="M32" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(3)</label></disp-formula><p>In the formula, <inline-formula id="pone.0321026.e033"><alternatives><graphic xlink:href="pone.0321026.e033.jpg" id="pone.0321026.e033g" position="anchor"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321026.e034"><alternatives><graphic xlink:href="pone.0321026.e034.jpg" id="pone.0321026.e034g" position="anchor"/><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> represent the coordinates of the center point of the ground truth bounding box, while <italic toggle="yes">w</italic><sup><italic toggle="yes">gt</italic></sup> and <italic toggle="yes">h</italic><sup><italic toggle="yes">gt</italic></sup> represent the width and height of the bounding box. <italic toggle="yes">ratio</italic> represents the scaling factor.</p><p>(2) Calculate the left boundary <italic toggle="yes">b</italic><sub><italic toggle="yes">l</italic></sub>, right boundary <italic toggle="yes">b</italic><sub><italic toggle="yes">r</italic></sub>, top boundary <italic toggle="yes">b</italic><sub><italic toggle="yes">t</italic></sub>, and lower boundary <italic toggle="yes">b</italic><sub><italic toggle="yes">b</italic></sub> of the predicted bounding box (or auxiliary bounding box).</p><disp-formula id="pone.0321026.e035"><alternatives><graphic xlink:href="pone.0321026.e035.jpg" id="pone.0321026.e035g" position="anchor"/><mml:math id="M35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(4)</label></disp-formula><disp-formula id="pone.0321026.e036"><alternatives><graphic xlink:href="pone.0321026.e036.jpg" id="pone.0321026.e036g" position="anchor"/><mml:math id="M36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(5)</label></disp-formula><p>In the formula, <italic toggle="yes">x</italic><sub><italic toggle="yes">c</italic></sub> and <italic toggle="yes">y</italic><sub><italic toggle="yes">c</italic></sub> represent the coordinates of the center point of the predicted bounding box, while <italic toggle="yes">w</italic> and <italic toggle="yes">h</italic> represent the width and height of the predicted bounding box, respectively.</p><p>(3) Calculate the intersection of the auxiliary bounding box and the ground truth bounding box, denoted as <italic toggle="yes">inter</italic>.</p><disp-formula id="pone.0321026.e037"><alternatives><graphic xlink:href="pone.0321026.e037.jpg" id="pone.0321026.e037g" position="anchor"/><mml:math id="M37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(6)</label></disp-formula><p>(4) Calculate the union of the auxiliary bounding box and the ground truth bounding box, denoted as <italic toggle="yes">union</italic>.</p><disp-formula id="pone.0321026.e038"><alternatives><graphic xlink:href="pone.0321026.e038.jpg" id="pone.0321026.e038g" position="anchor"/><mml:math id="M38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:msup><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo fence="true" form="prefix" stretchy="true">(</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mo fence="true" form="postfix" stretchy="true">)</mml:mo></mml:mrow><mml:mi>&#x000d7;</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:msup><mml:mi>o</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(7)</label></disp-formula><p>(5) Calculate the value of the Inner loss function, denoted as <italic toggle="yes">IoU</italic><sup><italic toggle="yes">inner</italic></sup>.</p></sec><sec id="sec012"><title>Wise-IoU v3.</title><p>Wise-IoU v3 [<xref rid="pone.0321026.ref026" ref-type="bibr">26</xref>] utilizes a dynamic non-monotonic focusing mechanism that can alleviate the influence of bounding box annotation quality on the generalization capability of the algorithm. The formula is shown as follows:</p><disp-formula id="pone.0321026.e039"><alternatives><graphic xlink:href="pone.0321026.e039.jpg" id="pone.0321026.e039g" position="anchor"/><mml:math id="M39" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mi>v</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>&#x003b4;</mml:mi><mml:mrow><mml:msup><mml:mi>&#x003b1;</mml:mi><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(8)</label></disp-formula><p><inline-formula id="pone.0321026.e040"><alternatives><graphic xlink:href="pone.0321026.e040.jpg" id="pone.0321026.e040g" position="anchor"/><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mi>U</mml:mi><mml:mi>v</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> represents the loss value, exp represents the exponential function, <inline-formula id="pone.0321026.e041"><alternatives><graphic xlink:href="pone.0321026.e041.jpg" id="pone.0321026.e041g" position="anchor"/><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b4;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321026.e042"><alternatives><graphic xlink:href="pone.0321026.e042.jpg" id="pone.0321026.e042g" position="anchor"/><mml:math id="M42" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> are parameters, <inline-formula id="pone.0321026.e043"><alternatives><graphic xlink:href="pone.0321026.e043.jpg" id="pone.0321026.e043g" position="anchor"/><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> indicates the degree of outliers, <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> refer to the coordinates of the center of the predicted bounding box, while <italic toggle="yes">x</italic><sub><italic toggle="yes">gt</italic></sub> and <italic toggle="yes">y</italic><sub><italic toggle="yes">gt</italic></sub> represent the coordinates of the center of the ground truth bounding box. <inline-formula id="pone.0321026.e044"><alternatives><graphic xlink:href="pone.0321026.e044.jpg" id="pone.0321026.e044g" position="anchor"/><mml:math id="M44" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0321026.e045"><alternatives><graphic xlink:href="pone.0321026.e045.jpg" id="pone.0321026.e045g" position="anchor"/><mml:math id="M45" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> denote the width and height of the minimum enclosing box for both the predicted and ground truth bounding boxes. * is used for calculating separation, and <italic toggle="yes">IoU</italic> represents the Intersection over Union (IoU) of the overlapping area between the predicted and ground truth bounding boxes.</p><p>Therefore, we design the bounding box loss function, Inner-Wise IoU, as a combination of the Inner-IoU and Wise-IoU v3 loss functions. Building on the principles of Inner-IoU, incorporating a scaling parameter to regulate the dimensions of the auxiliary bounding boxes, effectively addressing inconsistencies between bounding box size and the target shape, and generating accurate positioning information. Utilizing a dynamic non-monotonic focusing mechanism, Wise-IoU enhances the process of bounding box regression and evaluates sample quality based on the degree of outliers. As the loss value increases, this mechanism exhibits non-monotonic behavior, mitigating gradient gains for low-quality bounding boxes, while also decreasing gradient gains for high-quality anchor boxes, thus optimizing the model&#x02019;s learning across different quality samples. The evolution of the formula is as follows:</p><p>(1) Calculate the loss values <italic toggle="yes">L</italic><sub><italic toggle="yes">Inner</italic>&#x02212;<italic toggle="yes">IoU</italic></sub> and <italic toggle="yes">L</italic><sub><italic toggle="yes">Inner</italic>&#x02212;<italic toggle="yes">Wise</italic>&#x02212;<italic toggle="yes">IoU</italic></sub> for the functions Inner-IoU and Inner-Wise IoU, respectively.</p><disp-formula id="pone.0321026.e046"><alternatives><graphic xlink:href="pone.0321026.e046.jpg" id="pone.0321026.e046g" position="anchor"/><mml:math id="M46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>W</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mi>v</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mrow><mml:msup><mml:mi>U</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(9)</label></disp-formula><p>(2) Calculate the degree of outlierness.</p><disp-formula id="pone.0321026.e047"><alternatives><graphic xlink:href="pone.0321026.e047.jpg" id="pone.0321026.e047g" position="anchor"/><mml:math id="M47" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo accent="true">&#x02015;</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mfrac><mml:mo>&#x02208;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x0221e;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(10)</label></disp-formula><p><inline-formula id="pone.0321026.e048"><alternatives><graphic xlink:href="pone.0321026.e048.jpg" id="pone.0321026.e048g" position="anchor"/><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> represents the gradient gain value, and <italic toggle="yes">L</italic><sub><italic toggle="yes">IoU</italic></sub> represents the sliding average value.</p></sec></sec></sec><sec id="sec013"><title>Experiments and results analysis</title><sec id="sec014"><title>Dataset introduction</title><p>Ablation and comparative experiments were conducted on the RSOD [<xref rid="pone.0321026.ref027" ref-type="bibr">27</xref>] and NWPU VHR-10 [<xref rid="pone.0321026.ref028" ref-type="bibr">28</xref>&#x02013;<xref rid="pone.0321026.ref030" ref-type="bibr">30</xref>] datasets, while generalization experiments were performed on the TinyPerson, LEVIR-ship, brain-tumor, and smoke_fire_1 datasets.</p><p>The RSOD was developed by Wuhan University in 2017, and intended for applications in remote sensing. The dataset is divided using a ratio of 8:2, consisting of 782 training images and 194 testing images, totaling 976 images. It encompasses four categories: aircraft, oiltank, overpass, and playground. The size of each image varies between 512<inline-formula id="pone.0321026.e049"><alternatives><graphic xlink:href="pone.0321026.e049.jpg" id="pone.0321026.e049g" position="anchor"/><mml:math id="M49" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>512 pixels and 1083<inline-formula id="pone.0321026.e050"><alternatives><graphic xlink:href="pone.0321026.e050.jpg" id="pone.0321026.e050g" position="anchor"/><mml:math id="M50" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>923 pixels, with a total of 6,950 labeled objects.</p><p>The NWPU VHR-10 dataset, annotated by Northwestern Polytechnical University, focuses on high-resolution remote sensing. This dataset consists of 650 images of targets and 150 background images, amounting to 800 in total. From the 650 target images, 520 are selected for training and 130 for testing, following an 8:2 ratio. The dataset includes 10 categories: airplane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, and vehicle, with a total of 3,775 target instances.</p><p>The TinyPerson dataset, published by the University of Chinese Academy of Sciences in 2019, focuses on detecting tiny individuals in distant backgrounds. It contains a total of 1,610 images, with very low resolution for the individuals, where each target has fewer than 20pixel points. The dataset includes two categories: earth_person and sea_person, with a total of 72,651 labeled instances.</p><p>LEVIR-ship is a dataset designed for the detection of small ships in medium-resolution remote sensing images. It comprises 3,896 images, including 1,973 positive samples and 1,923 negative samples. Each image has a resolution of approximately 512<inline-formula id="pone.0321026.e051"><alternatives><graphic xlink:href="pone.0321026.e051.jpg" id="pone.0321026.e051g" position="anchor"/><mml:math id="M51" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>512 pixels and contains a total of 3,219 annotated instances. The dataset exclusively includes the ship category.</p><p>Brain-Tumor is a dataset for brain tumor detection derived from MRI and CT scans. It comprises 893 training images and 223 validation images, totaling 1,116 images. Each image has a resolution of 512<inline-formula id="pone.0321026.e052"><alternatives><graphic xlink:href="pone.0321026.e052.jpg" id="pone.0321026.e052g" position="anchor"/><mml:math id="M52" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>512 pixels, and the target categories are classified as positive and negative.</p><p>Smoke_Fire_1 is a dataset developed by North China University of Technology for the detection of fire and smoke. It comprises 3,711 images, each with a resolution of 640<inline-formula id="pone.0321026.e053"><alternatives><graphic xlink:href="pone.0321026.e053.jpg" id="pone.0321026.e053g" position="anchor"/><mml:math id="M53" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>640 pixels. The dataset includes two categories: smoke and fire.</p></sec><sec id="sec015"><title>Experimental environment</title><p>(1) Training Environment: NVIDIA RTX3090 GPU with 24 GB VRAM, 14 vCPUs of Intel(R) Xeon(R) Gold 6330 CPU @ 2.00 GHz, and 80 GB RAM.</p><p>(2) Testing Environment: NVIDIA RTX4060 GPU with 8 GB VRAM, 13th Gen Intel(R) Core(TM) i9-13900HX2.20 GHz, and 16 GB RAM.</p><p>(3) Software Environment: Windows 11, CUDA 11.8, PyTorch 2.0.1, and Python 3.8.0.</p><p>(4) Parameter Settings: Input feature map resolution of 640<inline-formula id="pone.0321026.e054"><alternatives><graphic xlink:href="pone.0321026.e054.jpg" id="pone.0321026.e054g" position="anchor"/><mml:math id="M54" display="inline" overflow="scroll"><mml:mrow><mml:mi>&#x000d7;</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>640, all data enhanced using Mosaic data augmentation, batch size of 16, optimizer is SGD, initial learning rate of 0.01, momentum parameter set to 0.937, learning rate updated using cosine annealing learning rate schedule, training epochs set to 200.</p></sec><sec id="sec016"><title>Evaluation metrics</title><p>The experiments utilize five frequently used assessment metrics in object recognition tasks: Precision, Recall, mAP, Parameters, and GFLOPs. The definitions and formulas for each metric are introduced below.</p><p>(1) Precision(P) represents the accuracy of recognized targets, reflecting effectiveness of the algorithm. The calculation formula is as follows:</p><disp-formula id="pone.0321026.e055"><alternatives><graphic xlink:href="pone.0321026.e055.jpg" id="pone.0321026.e055g" position="anchor"/><mml:math id="M55" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(11)</label></disp-formula><p>TP represents the quantity of true positive instances that are accurately recognized as belonging to the positive class. FP represents the quantity of false positive instances that are accurately recognized as the correct category.</p><p>(2) Recall(R) represents the recall rate of detected targets, which is the probability of predicting positive samples. FN denotes the quantity of positive instances that were not detected. The calculation formula is:</p><disp-formula id="pone.0321026.e056"><alternatives><graphic xlink:href="pone.0321026.e056.jpg" id="pone.0321026.e056g" position="anchor"/><mml:math id="M56" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(12)</label></disp-formula><p>(3) mAP represents the mean accuracy of detection across all categories. The calculation formula is:</p><disp-formula id="pone.0321026.e057"><alternatives><graphic xlink:href="pone.0321026.e057.jpg" id="pone.0321026.e057g" position="anchor"/><mml:math id="M57" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(13)</label></disp-formula><p>C represents the total number of target categories, i represents the number of detections, and AP represents the area under the PR curve for a single category. The formula for AP is:</p><disp-formula id="pone.0321026.e058"><alternatives><graphic xlink:href="pone.0321026.e058.jpg" id="pone.0321026.e058g" position="anchor"/><mml:math id="M58" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtext>&#x000a0;</mml:mtext><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(14)</label></disp-formula><p>From mAP, we derive mAP@0.5 and mAP@0.5:0.95. mAP@0.5 is the average of the average precision values (AP) at a threshold of 0.5 for all categories, while mAP@0.5:0.95 is the average mAP calculated at IoU thresholds ranging from 0.5 to 0.95 in steps of 0.05.</p><p>(4) Parameters represent the total number of parameters in the network.</p><p>(5) GFLOPs represent the count of floating-point computations, measured in G.</p><p>(6) F1-Score(F1) is the harmonic mean of precision and recall.</p><disp-formula id="pone.0321026.e059"><alternatives><graphic xlink:href="pone.0321026.e059.jpg" id="pone.0321026.e059g" position="anchor"/><mml:math id="M59" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x000d7;</mml:mi><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>&#x000d7;</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></alternatives>
<label>(15)</label></disp-formula></sec><sec id="sec017"><title>Experimental results analysis</title><sec id="sec018"><title>Ablation experiment.</title><p>Ablation analyses were conducted using the open-access datasets RSOD and NWPU VHR-10 to evaluate LI-YOLOv8&#x02019;s performance. In <xref rid="pone.0321026.t001" ref-type="table">Tables 1</xref> and <xref rid="pone.0321026.t002" ref-type="table">2</xref>, I1, I2, I3, and I4 represent the algorithmic innovations SPPF-R, C2f-E, GP-Detect, and Inner-Wise IoU, respectively.</p><table-wrap position="float" id="pone.0321026.t001"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.t001</object-id><label>Table 1</label><caption><title>Ablation study on the RSOD dataset.</title></caption><alternatives><graphic xlink:href="pone.0321026.t001" id="pone.0321026.t001g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">N</th><th align="left" rowspan="1" colspan="1">In1</th><th align="left" rowspan="1" colspan="1">In2</th><th align="left" rowspan="1" colspan="1">In3</th><th align="left" rowspan="1" colspan="1">In4</th><th align="left" rowspan="1" colspan="1">mAP@0.5</th><th align="left" rowspan="1" colspan="1">mAP@0.5:0.95</th><th align="left" rowspan="1" colspan="1">P</th><th align="left" rowspan="1" colspan="1">R</th><th align="left" rowspan="1" colspan="1">Parameters</th><th align="left" rowspan="1" colspan="1">GFLOPs</th><th align="left" rowspan="1" colspan="1">F1</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">84.3</td><td align="left" rowspan="1" colspan="1">64.9</td><td align="left" rowspan="1" colspan="1">90.6</td><td align="left" rowspan="1" colspan="1">69.4</td><td align="left" rowspan="1" colspan="1">3.0</td><td align="left" rowspan="1" colspan="1">8.2</td><td align="left" rowspan="1" colspan="1">78.6</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e060">
<alternatives><graphic xlink:href="pone.0321026.e060" id="pone.0321026.e060g" position="anchor"/><mml:math id="M60" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">85.5(<inline-formula id="pone.0321026.e061"><alternatives><graphic xlink:href="pone.0321026.e061" id="pone.0321026.e061g" position="anchor"/><mml:math id="M61" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.2)</td><td align="left" rowspan="1" colspan="1">65.2(<inline-formula id="pone.0321026.e062"><alternatives><graphic xlink:href="pone.0321026.e062" id="pone.0321026.e062g" position="anchor"/><mml:math id="M62" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.3)</td><td align="left" rowspan="1" colspan="1">92.1(<inline-formula id="pone.0321026.e063"><alternatives><graphic xlink:href="pone.0321026.e063" id="pone.0321026.e063g" position="anchor"/><mml:math id="M63" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.5)</td><td align="left" rowspan="1" colspan="1">75.1(<inline-formula id="pone.0321026.e064"><alternatives><graphic xlink:href="pone.0321026.e064" id="pone.0321026.e064g" position="anchor"/><mml:math id="M64" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>5.7)</td><td align="left" rowspan="1" colspan="1">3.0</td><td align="left" rowspan="1" colspan="1">8.2</td><td align="left" rowspan="1" colspan="1">82.7</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e065">
<alternatives><graphic xlink:href="pone.0321026.e065" id="pone.0321026.e065g" position="anchor"/><mml:math id="M65" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">87.1(<inline-formula id="pone.0321026.e066"><alternatives><graphic xlink:href="pone.0321026.e066" id="pone.0321026.e066g" position="anchor"/><mml:math id="M66" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>2.5)</td><td align="left" rowspan="1" colspan="1">65.8(<inline-formula id="pone.0321026.e067"><alternatives><graphic xlink:href="pone.0321026.e067" id="pone.0321026.e067g" position="anchor"/><mml:math id="M67" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.9)</td><td align="left" rowspan="1" colspan="1">91.0(<inline-formula id="pone.0321026.e068"><alternatives><graphic xlink:href="pone.0321026.e068" id="pone.0321026.e068g" position="anchor"/><mml:math id="M68" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.4)</td><td align="left" rowspan="1" colspan="1">71.8(<inline-formula id="pone.0321026.e069"><alternatives><graphic xlink:href="pone.0321026.e069" id="pone.0321026.e069g" position="anchor"/><mml:math id="M69" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>2.4)</td><td align="left" rowspan="1" colspan="1">3.0</td><td align="left" rowspan="1" colspan="1">8.4</td><td align="left" rowspan="1" colspan="1">80.3</td></tr><tr><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e070">
<alternatives><graphic xlink:href="pone.0321026.e070" id="pone.0321026.e070g" position="anchor"/><mml:math id="M70" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">88.8(<inline-formula id="pone.0321026.e071"><alternatives><graphic xlink:href="pone.0321026.e071" id="pone.0321026.e071g" position="anchor"/><mml:math id="M71" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.7)</td><td align="left" rowspan="1" colspan="1">65.0(<inline-formula id="pone.0321026.e072"><alternatives><graphic xlink:href="pone.0321026.e072" id="pone.0321026.e072g" position="anchor"/><mml:math id="M72" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.1)</td><td align="left" rowspan="1" colspan="1">90.9(<inline-formula id="pone.0321026.e073"><alternatives><graphic xlink:href="pone.0321026.e073" id="pone.0321026.e073g" position="anchor"/><mml:math id="M73" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.3)</td><td align="left" rowspan="1" colspan="1">74.0(<inline-formula id="pone.0321026.e074"><alternatives><graphic xlink:href="pone.0321026.e074" id="pone.0321026.e074g" position="anchor"/><mml:math id="M74" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>4.6)</td><td align="left" rowspan="1" colspan="1">2.7</td><td align="left" rowspan="1" colspan="1">6.1</td><td align="left" rowspan="1" colspan="1">81.5</td></tr><tr><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e075">
<alternatives><graphic xlink:href="pone.0321026.e075" id="pone.0321026.e075g" position="anchor"/><mml:math id="M75" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">89.2(<inline-formula id="pone.0321026.e076"><alternatives><graphic xlink:href="pone.0321026.e076" id="pone.0321026.e076g" position="anchor"/><mml:math id="M76" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.8)</td><td align="left" rowspan="1" colspan="1">65.4(<inline-formula id="pone.0321026.e077"><alternatives><graphic xlink:href="pone.0321026.e077" id="pone.0321026.e077g" position="anchor"/><mml:math id="M77" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.5)</td><td align="left" rowspan="1" colspan="1">92.3(<inline-formula id="pone.0321026.e078"><alternatives><graphic xlink:href="pone.0321026.e078" id="pone.0321026.e078g" position="anchor"/><mml:math id="M78" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.7)</td><td align="left" rowspan="1" colspan="1">73.0(<inline-formula id="pone.0321026.e079"><alternatives><graphic xlink:href="pone.0321026.e079" id="pone.0321026.e079g" position="anchor"/><mml:math id="M79" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>3.6)</td><td align="left" rowspan="1" colspan="1">3.0</td><td align="left" rowspan="1" colspan="1">8.2</td><td align="left" rowspan="1" colspan="1">81.6</td></tr><tr><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e080">
<alternatives><graphic xlink:href="pone.0321026.e080" id="pone.0321026.e080g" position="anchor"/><mml:math id="M80" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e081">
<alternatives><graphic xlink:href="pone.0321026.e081" id="pone.0321026.e081g" position="anchor"/><mml:math id="M81" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">90.6(<inline-formula id="pone.0321026.e082"><alternatives><graphic xlink:href="pone.0321026.e082" id="pone.0321026.e082g" position="anchor"/><mml:math id="M82" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>6.3)</td><td align="left" rowspan="1" colspan="1">66.5(<inline-formula id="pone.0321026.e083"><alternatives><graphic xlink:href="pone.0321026.e083" id="pone.0321026.e083g" position="anchor"/><mml:math id="M83" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.6)</td><td align="left" rowspan="1" colspan="1">92.2(<inline-formula id="pone.0321026.e084"><alternatives><graphic xlink:href="pone.0321026.e084" id="pone.0321026.e084g" position="anchor"/><mml:math id="M84" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.6)</td><td align="left" rowspan="1" colspan="1">89.5(<inline-formula id="pone.0321026.e085"><alternatives><graphic xlink:href="pone.0321026.e085" id="pone.0321026.e085g" position="anchor"/><mml:math id="M85" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>20.1)</td><td align="left" rowspan="1" colspan="1">3.0</td><td align="left" rowspan="1" colspan="1">8.4</td><td align="left" rowspan="1" colspan="1">90.9</td></tr><tr><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e086">
<alternatives><graphic xlink:href="pone.0321026.e086" id="pone.0321026.e086g" position="anchor"/><mml:math id="M86" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e087">
<alternatives><graphic xlink:href="pone.0321026.e087" id="pone.0321026.e087g" position="anchor"/><mml:math id="M87" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e088">
<alternatives><graphic xlink:href="pone.0321026.e088" id="pone.0321026.e088g" position="anchor"/><mml:math id="M88" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">91.4(<inline-formula id="pone.0321026.e089"><alternatives><graphic xlink:href="pone.0321026.e089" id="pone.0321026.e089g" position="anchor"/><mml:math id="M89" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>7.1)</td><td align="left" rowspan="1" colspan="1">66.5(<inline-formula id="pone.0321026.e090"><alternatives><graphic xlink:href="pone.0321026.e090" id="pone.0321026.e090g" position="anchor"/><mml:math id="M90" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.6)</td><td align="left" rowspan="1" colspan="1">95.1(<inline-formula id="pone.0321026.e091"><alternatives><graphic xlink:href="pone.0321026.e091" id="pone.0321026.e091g" position="anchor"/><mml:math id="M91" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>4.5)</td><td align="left" rowspan="1" colspan="1">86.6(<inline-formula id="pone.0321026.e092"><alternatives><graphic xlink:href="pone.0321026.e092" id="pone.0321026.e092g" position="anchor"/><mml:math id="M92" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>17.2)</td><td align="left" rowspan="1" colspan="1">2.7</td><td align="left" rowspan="1" colspan="1">6.3</td><td align="left" rowspan="1" colspan="1">90.6</td></tr><tr><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e093">
<alternatives><graphic xlink:href="pone.0321026.e093" id="pone.0321026.e093g" position="anchor"/><mml:math id="M93" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e094">
<alternatives><graphic xlink:href="pone.0321026.e094" id="pone.0321026.e094g" position="anchor"/><mml:math id="M94" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e095">
<alternatives><graphic xlink:href="pone.0321026.e095" id="pone.0321026.e095g" position="anchor"/><mml:math id="M95" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e096">
<alternatives><graphic xlink:href="pone.0321026.e096" id="pone.0321026.e096g" position="anchor"/><mml:math id="M96" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">91.9(<inline-formula id="pone.0321026.e097"><alternatives><graphic xlink:href="pone.0321026.e097" id="pone.0321026.e097g" position="anchor"/><mml:math id="M97" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>7.6)</td><td align="left" rowspan="1" colspan="1">67.0(<inline-formula id="pone.0321026.e098"><alternatives><graphic xlink:href="pone.0321026.e098" id="pone.0321026.e098g" position="anchor"/><mml:math id="M98" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>2.1)</td><td align="left" rowspan="1" colspan="1">95.1(<inline-formula id="pone.0321026.e099"><alternatives><graphic xlink:href="pone.0321026.e099" id="pone.0321026.e099g" position="anchor"/><mml:math id="M99" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>4.5)</td><td align="left" rowspan="1" colspan="1">86.0(<inline-formula id="pone.0321026.e100"><alternatives><graphic xlink:href="pone.0321026.e100" id="pone.0321026.e100g" position="anchor"/><mml:math id="M100" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>16.6)</td><td align="left" rowspan="1" colspan="1">2.7</td><td align="left" rowspan="1" colspan="1">6.3</td><td align="left" rowspan="1" colspan="1">90.3</td></tr></tbody></table></alternatives></table-wrap><table-wrap position="float" id="pone.0321026.t002"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.t002</object-id><label>Table 2</label><caption><title>Ablation study on the NWPU VHR-10 dataset.</title></caption><alternatives><graphic xlink:href="pone.0321026.t002" id="pone.0321026.t002g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">N</td><td align="left" rowspan="1" colspan="1">In1</td><td align="left" rowspan="1" colspan="1">In2</td><td align="left" rowspan="1" colspan="1">In3</td><td align="left" rowspan="1" colspan="1">D4</td><td align="left" rowspan="1" colspan="1">mAP@0.5</td><td align="left" rowspan="1" colspan="1">mAP@0.5:0.95</td><td align="left" rowspan="1" colspan="1">P</td><td align="left" rowspan="1" colspan="1">R</td><td align="left" rowspan="1" colspan="1">F1</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">84.3</td><td align="left" rowspan="1" colspan="1">53.8</td><td align="left" rowspan="1" colspan="1">92.0</td><td align="left" rowspan="1" colspan="1">74.6</td><td align="left" rowspan="1" colspan="1">82.4</td></tr><tr><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e102">
<alternatives><graphic xlink:href="pone.0321026.e102" id="pone.0321026.e102g" position="anchor"/><mml:math id="M101" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">84.8(<inline-formula id="pone.0321026.e103"><alternatives><graphic xlink:href="pone.0321026.e103" id="pone.0321026.e103g" position="anchor"/><mml:math id="M102" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.5)</td><td align="left" rowspan="1" colspan="1">53.9(<inline-formula id="pone.0321026.e104"><alternatives><graphic xlink:href="pone.0321026.e104" id="pone.0321026.e104g" position="anchor"/><mml:math id="M103" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.1)</td><td align="left" rowspan="1" colspan="1">93.0(<inline-formula id="pone.0321026.e105"><alternatives><graphic xlink:href="pone.0321026.e105" id="pone.0321026.e105g" position="anchor"/><mml:math id="M104" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.0)</td><td align="left" rowspan="1" colspan="1">75.1(<inline-formula id="pone.0321026.e106"><alternatives><graphic xlink:href="pone.0321026.e106" id="pone.0321026.e106g" position="anchor"/><mml:math id="M105" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.5)</td><td align="left" rowspan="1" colspan="1">83.1</td></tr><tr><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e107">
<alternatives><graphic xlink:href="pone.0321026.e107" id="pone.0321026.e107g" position="anchor"/><mml:math id="M106" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">85.7(<inline-formula id="pone.0321026.e108"><alternatives><graphic xlink:href="pone.0321026.e108" id="pone.0321026.e108g" position="anchor"/><mml:math id="M107" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.7)</td><td align="left" rowspan="1" colspan="1">54.1(<inline-formula id="pone.0321026.e109"><alternatives><graphic xlink:href="pone.0321026.e109" id="pone.0321026.e109g" position="anchor"/><mml:math id="M108" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.3)</td><td align="left" rowspan="1" colspan="1">93.3(<inline-formula id="pone.0321026.e110"><alternatives><graphic xlink:href="pone.0321026.e110" id="pone.0321026.e110g" position="anchor"/><mml:math id="M109" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.3)</td><td align="left" rowspan="1" colspan="1">75.1(<inline-formula id="pone.0321026.e111"><alternatives><graphic xlink:href="pone.0321026.e111" id="pone.0321026.e111g" position="anchor"/><mml:math id="M110" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.5)</td><td align="left" rowspan="1" colspan="1">83.2</td></tr><tr><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e112">
<alternatives><graphic xlink:href="pone.0321026.e112" id="pone.0321026.e112g" position="anchor"/><mml:math id="M111" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">84.6(<inline-formula id="pone.0321026.e113"><alternatives><graphic xlink:href="pone.0321026.e113" id="pone.0321026.e113g" position="anchor"/><mml:math id="M112" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.3))</td><td align="left" rowspan="1" colspan="1">53.9(<inline-formula id="pone.0321026.e114"><alternatives><graphic xlink:href="pone.0321026.e114" id="pone.0321026.e114g" position="anchor"/><mml:math id="M113" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.1)</td><td align="left" rowspan="1" colspan="1">93.0(<inline-formula id="pone.0321026.e115"><alternatives><graphic xlink:href="pone.0321026.e115" id="pone.0321026.e115g" position="anchor"/><mml:math id="M114" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.0)</td><td align="left" rowspan="1" colspan="1">74.8(<inline-formula id="pone.0321026.e116"><alternatives><graphic xlink:href="pone.0321026.e116" id="pone.0321026.e116g" position="anchor"/><mml:math id="M115" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.2)</td><td align="left" rowspan="1" colspan="1">82.9</td></tr><tr><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e117">
<alternatives><graphic xlink:href="pone.0321026.e117" id="pone.0321026.e117g" position="anchor"/><mml:math id="M116" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">85.3(<inline-formula id="pone.0321026.e118"><alternatives><graphic xlink:href="pone.0321026.e118" id="pone.0321026.e118g" position="anchor"/><mml:math id="M117" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.0)</td><td align="left" rowspan="1" colspan="1">54.0(<inline-formula id="pone.0321026.e119"><alternatives><graphic xlink:href="pone.0321026.e119" id="pone.0321026.e119g" position="anchor"/><mml:math id="M118" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.2)</td><td align="left" rowspan="1" colspan="1">92.1(<inline-formula id="pone.0321026.e120"><alternatives><graphic xlink:href="pone.0321026.e120" id="pone.0321026.e120g" position="anchor"/><mml:math id="M119" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.1)</td><td align="left" rowspan="1" colspan="1">75.3(<inline-formula id="pone.0321026.e121"><alternatives><graphic xlink:href="pone.0321026.e121" id="pone.0321026.e121g" position="anchor"/><mml:math id="M120" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.7)</td><td align="left" rowspan="1" colspan="1">82.9</td></tr><tr><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e122">
<alternatives><graphic xlink:href="pone.0321026.e122" id="pone.0321026.e122g" position="anchor"/><mml:math id="M121" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e123">
<alternatives><graphic xlink:href="pone.0321026.e123" id="pone.0321026.e123g" position="anchor"/><mml:math id="M122" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">85.8(<inline-formula id="pone.0321026.e124"><alternatives><graphic xlink:href="pone.0321026.e124" id="pone.0321026.e124g" position="anchor"/><mml:math id="M123" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.5)</td><td align="left" rowspan="1" colspan="1">55.3(<inline-formula id="pone.0321026.e125"><alternatives><graphic xlink:href="pone.0321026.e125" id="pone.0321026.e125g" position="anchor"/><mml:math id="M124" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.5)</td><td align="left" rowspan="1" colspan="1">93.8(<inline-formula id="pone.0321026.e126"><alternatives><graphic xlink:href="pone.0321026.e126" id="pone.0321026.e126g" position="anchor"/><mml:math id="M125" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.8)</td><td align="left" rowspan="1" colspan="1">75.7(<inline-formula id="pone.0321026.e127"><alternatives><graphic xlink:href="pone.0321026.e127" id="pone.0321026.e127g" position="anchor"/><mml:math id="M126" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.1)</td><td align="left" rowspan="1" colspan="1">83.8</td></tr><tr><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e128">
<alternatives><graphic xlink:href="pone.0321026.e128" id="pone.0321026.e128g" position="anchor"/><mml:math id="M127" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e129">
<alternatives><graphic xlink:href="pone.0321026.e129" id="pone.0321026.e129g" position="anchor"/><mml:math id="M128" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e130">
<alternatives><graphic xlink:href="pone.0321026.e130" id="pone.0321026.e130g" position="anchor"/><mml:math id="M129" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">-</td><td align="left" rowspan="1" colspan="1">86.7(<inline-formula id="pone.0321026.e131"><alternatives><graphic xlink:href="pone.0321026.e131" id="pone.0321026.e131g" position="anchor"/><mml:math id="M130" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>2.4)</td><td align="left" rowspan="1" colspan="1">54.8(<inline-formula id="pone.0321026.e132"><alternatives><graphic xlink:href="pone.0321026.e132" id="pone.0321026.e132g" position="anchor"/><mml:math id="M131" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.0)</td><td align="left" rowspan="1" colspan="1">95.3(<inline-formula id="pone.0321026.e133"><alternatives><graphic xlink:href="pone.0321026.e133" id="pone.0321026.e133g" position="anchor"/><mml:math id="M132" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>3.3)</td><td align="left" rowspan="1" colspan="1">75.3(<inline-formula id="pone.0321026.e134"><alternatives><graphic xlink:href="pone.0321026.e134" id="pone.0321026.e134g" position="anchor"/><mml:math id="M133" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>0.7)</td><td align="left" rowspan="1" colspan="1">84.0</td></tr><tr><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e135">
<alternatives><graphic xlink:href="pone.0321026.e135" id="pone.0321026.e135g" position="anchor"/><mml:math id="M134" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e136">
<alternatives><graphic xlink:href="pone.0321026.e136" id="pone.0321026.e136g" position="anchor"/><mml:math id="M135" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e137">
<alternatives><graphic xlink:href="pone.0321026.e137" id="pone.0321026.e137g" position="anchor"/><mml:math id="M136" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">
<inline-formula id="pone.0321026.e138">
<alternatives><graphic xlink:href="pone.0321026.e138" id="pone.0321026.e138g" position="anchor"/><mml:math id="M137" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">87.1(<inline-formula id="pone.0321026.e139"><alternatives><graphic xlink:href="pone.0321026.e139" id="pone.0321026.e139g" position="anchor"/><mml:math id="M138" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>2.8)</td><td align="left" rowspan="1" colspan="1">54.9(<inline-formula id="pone.0321026.e140"><alternatives><graphic xlink:href="pone.0321026.e140" id="pone.0321026.e140g" position="anchor"/><mml:math id="M139" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>1.1)</td><td align="left" rowspan="1" colspan="1">95.8(<inline-formula id="pone.0321026.e141"><alternatives><graphic xlink:href="pone.0321026.e141" id="pone.0321026.e141g" position="anchor"/><mml:math id="M140" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>3.8)</td><td align="left" rowspan="1" colspan="1">76.9(<inline-formula id="pone.0321026.e142"><alternatives><graphic xlink:href="pone.0321026.e142" id="pone.0321026.e142g" position="anchor"/><mml:math id="M141" display="inline" overflow="scroll"><mml:mrow><mml:mo>&#x02191;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>2.3)</td><td align="left" rowspan="1" colspan="1">85.3</td></tr></tbody></table></alternatives></table-wrap><p>1) In the first row, the experimental results of the baseline algorithm show mAP@0.5, mAP@0.5:0.95, precision, and recall values of 84.3%, 64.9%, 90.6%, and 69.4%, respectively. The model comprises 3.0M parameters, requires 8.2G GFLOPs, and achieves an F1 of 78.6%.</p><p>2) In the second row, we introduce the innovative component SPPF-R. The incorporation of SPPF-R resulted in increases of 1.2%, 0.3%, 1.5%, 5.7%, and 4.1% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1 respectively, while maintaining the same number of parameters and computational cost. SPPF-R effectively reduces inter-parameter dependencies by replacing the original SiLU activation function with ReLU. Additionally, embedding RFAConv after the improved CBR (CBS) module expands the receptive field and enhances the extraction of small target features. The most significant improvement was observed in recall, indicating a higher proportion of correctly predicted positive samples and a reduction in missed detections caused by insufficient feature extraction.</p><p>3) the third row, we introduce the innovative component C2f-E. The incorporation of C2f-E resulted in increases of 2.5%, 0.9%, 0.4%, 2.4%, and 1.7% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1 respectively. Computational cost increased slightly from 8.2G to 8.4G, while the number of parameters remained unchanged. The mAP@0.5 metric exhibited the most significant improvement. By embedding the EMA attention mechanism within C2f, the model effectively captures multi-scale spatial structural information, accurately locates small target regions, reduces feature information loss, and enhances both the detection rate and accuracy of small targets.</p><p>4) In the fourth row, we introduce the innovative component GP-Detect. The computational cost was reduced by 2.1 GFLOPs, while mAP@0.5, mAP@0.5:0.95, precision, recall, and F1 increased by 1.7%, 0.1%, 0.3%, 4.6%, and 2.9%, respectively. The number of parameters decreased to 2.7M. GP-Detect reduces redundant computations by pruning convolutional layers and enhances feature extraction capabilities by combining GSConv and PConv, significantly improving the detection rate of small targets.</p><p>5) In the fifth row, we introduce the innovative component Inner-Wise IoU. Without altering the number of parameters or computational cost, Inner-Wise IoU led to improvements of 1.8%, 0.5%, 1.7%, 3.6%, and 3.0% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1, respectively. By replacing the original loss function with Inner-Wise IoU, the loss is computed using auxiliary bounding boxes at different scales, and an intelligent gradient gain distribution strategy is employed, enhancing the algorithm&#x02019;s generalization capability. Maintaining the network&#x02019;s complexity, this approach better balances precision and recall, thereby improving the F1. It not only accurately identifies more positive samples but also achieves higher accuracy in recognizing these positive samples.</p><p>6) In the sixth row, we introduce the innovative components SPPF-R and C2f-E. The incorporation of SPPF-R and C2f-E resulted in improvements of 6.3%, 1.6%, 1.6%, 20.1%, and 12.3% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1, respectively, while maintaining the number of parameters unchanged and increasing computational cost by 0.2G. SPPF-R enhances feature extraction capabilities, thereby improving initial detection accuracy, whereas C2f-E optimizes feature fusion to further increase the focus on small target regions. The integration of these two components significantly enhances Recall, ensuring more accurate detection of targets.</p><p>7) In the seventh row, we integrate the innovative components SPPF-R, C2f-E, and GP-Detect to further streamline the algorithm while enhancing the extraction and focus on small target features. This integration resulted in improvements of 7.1%, 1.6%, 4.5%, 17.2%, and 12.0% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1, respectively, while reducing the number of parameters and computational cost to 2.7M and 6.3G. By further incorporating GP-Detect, the algorithm maintains high precision while decreasing both parameter count and computational load, thereby rendering the overall algorithm more lightweight.</p><p>8) In the eighth row, we integrated all the aforementioned innovations, resulting in significant enhancements to the algorithm&#x02019;s performance. The accuracy metrics mAP@0.5 and mAP@0.5:0.95 increased by 7.6% and 2.1%, respectively, while Precision, Recall, and F1 scores improved by 4.5%, 16.6%, and 11.7%, respectively. The number of parameters was slightly reduced from 3.0M to 2.7M, and the computational cost decreased substantially from 8.2G to 6.3G, marking a reduction of 1.9G (23.2%). These results demonstrate that the proposed algorithm significantly enhances the accuracy of small object detection and markedly reduces network complexity.</p><p>(2) Ablation experiments on the NWPU VHR-10 dataset</p><p>Similarly, as the four innovations contribute uniformly to the algorithm, the ablation experiments conducted on the RSOD dataset are thoroughly detailed. Experiments carried out on different datasets yield varying results. To validate the effectiveness of these innovations, eight experiments were performed using different datasets. The experimental results are presented in <xref rid="pone.0321026.t002" ref-type="table">Table 2</xref>, where rows marked with "<inline-formula id="pone.0321026.e101"><alternatives><graphic xlink:href="pone.0321026.e101.jpg" id="pone.0321026.e101g" position="anchor"/><mml:math id="M142" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">&#x0221a;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>" denote the innovations and their corresponding evaluation metrics. The number of parameters and computational costs of the algorithm remain consistent with those in <xref rid="pone.0321026.t001" ref-type="table">Table 1</xref> and are therefore omitted. The analysis is as follows.</p><p>1) In the first row, the baseline algorithm achieved experimental results with mAP@0.5, mAP@0.5:0.95, precision, recall, and F1 of 84.3%, 53.8%, 92.0%, 74.6%, and 82.4%, respectively.</p><p>2) In the second row, incorporating the innovation SPPF-R resulted in improvements of 0.5%, 0.1%, 1.0%, 0.5%, and 0.7% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1, respectively. The most significant improvement was observed in the F1 score, indicating a reduction in false positives while maintaining a high recall rate.</p><p>3) In the third row, the addition of the innovation C2f-E led to enhancements of 0.7%, 0.3&#x02009; 1.3%, 0.5%, and 0.8% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1, respectively. Precision was notably increased, further enhancing the accuracy of small object detection.</p><p>4) In the fourth row, the incorporation of GP-Detect resulted in increases of 0.3%, 0.1%, 1.0%, 0.2%, and 0.5% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1, respectively. Precision saw the highest improvement, thereby reducing false positive rates.</p><p>5) In the fifth row, integrating Inner-Wise IoU led to significant enhancements of 1.0%, 0.2%, 0.1%, 0.7%, and 0.5% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1, respectively. The most notable improvement was in mAP@0.5, increasing the detection rate of small objects.</p><p>6) In the sixth row, simultaneous inclusion of SPPF-R and C2f-E innovations resulted in substantial improvements of 1.5%, 1.5%, 1.8%, 1.1%, and 1.4% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1, respectively. All evaluation metrics demonstrated significant enhancements.</p><p>7) In the seventh row, combining SPPF-R, C2f-E, and GP-Detect innovations led to increases of 2.4%, 1.0%, 3.3%, 0.7%, and 1.6% in mAP@0.5, mAP@0.5:0.95, precision, recall, and F1, respectively. While mAP@0.5:0.95 and recall saw slight improvements, mAP@0.5 and precision showed more pronounced increases, enhancing the reliability and accuracy of the algorithm.</p><p>8) In the eighth row, integrating all aforementioned innovations resulted in improvements of 2.8% and 1.1% in mAP@0.5 and mAP@0.5:0.95, respectively, and 3.8%, 2.3%, and 2.9% in precision, recall, and F1, respectively. These results further confirm that the proposed algorithm significantly enhances the detection rate of small objects in object detection tasks.</p></sec><sec id="sec019"><title>Comparison experiment.</title><p>(1) PR Curve Comparison</p><p>1) To better demonstrate the effectiveness of the LI-YOLOv8 algorithm, we conducted training on the RSOD dataset. The outcomes are detailed within <xref rid="pone.0321026.g011" ref-type="fig">Fig 11</xref>, followed by analysis.</p><fig position="float" id="pone.0321026.g011"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g011</object-id><label>Fig 11</label><caption><title>PR comparison on RSOD dataset.</title></caption><graphic xlink:href="pone.0321026.g011" position="float"/></fig><p><xref rid="pone.0321026.g011" ref-type="fig">Fig 11</xref>(a) and <xref rid="pone.0321026.g011" ref-type="fig">11</xref>(b) present a comparison of the Precision-Recall (PR) curves obtained from training YOLOv8 and LI-YOLOv8 on the RSOD dataset. As shown in <xref rid="pone.0321026.g011" ref-type="fig">Fig 11</xref>(a), when Recall ranges from 0.2 to 0.6, Precision remains at a relatively high level. However, when Recall increases to between 0.7 and 0.8, the Precision curve exhibits a noticeable decline. This indicates that as YOLOv8 attempts to recall more positive samples, it becomes more susceptible to misclassifying background or similar interfering information, leading to an increase in false positive rates, especially when detecting small objects in complex backgrounds such as overpasses. Additionally, the latter part of the curve features a relatively stable interval, suggesting that YOLOv8 maintains strong detection performance for certain targets (e.g., oil tanks) that occupy a significant portion of the image.In contrast, <xref rid="pone.0321026.g011" ref-type="fig">Fig 11</xref>(b) displays the PR curve for LI-YOLOv8 under the same dataset and training conditions, highlighting two significant differences:</p><p>Firstly, When Recall is between 0.7 and 0.8, the Precision curve of LI-YOLOv8 remains high without a noticeable decline, unlike YOLOv8. This suggests that LI-YOLOv8 can effectively distinguish between positive samples and the background even when recalling more targets, demonstrating enhanced small object detection capabilities in complex scenarios with fewer false positives due to the integration of innovative modules.</p><p>Secondly, When Recall ranges from 0.8 to 1.0, LI-YOLOv8 experiences only a slight decrease in Precision compared to YOLOv8, with a smaller magnitude of decline. This implies that LI-YOLOv8 continues to perform effectively in small object detection without significant increases in false negatives or false positives, highlighting the effectiveness of feature enhancement and attention reinforcement specifically targeted at small objects.</p><p>Overall, the differences between <xref rid="pone.0321026.g011" ref-type="fig">Fig 11</xref>(a) and <xref rid="pone.0321026.g011" ref-type="fig">11</xref>(b) indicate that by integrating modules such as SPPF-R, C2f-E, GP-Detect, and Inner-Wise IoU, LI-YOLOv8 achieves a better balance between Precision and Recall over a broader range. This enhancement leads to improved overall detection performance and demonstrates LI-YOLOv8&#x02019;s superior adaptability to multi-scenario and multi-scale small object detection tasks.</p><p><xref rid="pone.0321026.t003" ref-type="table">Table 3</xref> shows the AP improvement rates for four target categories: aircraft, oiltank, overpass, and playground. All four categories demonstrate increased AP values, with overpass showing a significant improvement of 26.8%. Aircraft and oiltank have modest increases of 1.8% and 1.1%, respectively. The playground category, however, exhibits a minimal increase of only 0.6%, due to the baseline AP values being relatively high in the dataset, which resulted in saturation of learning capacity during training. Overall findings demonstrate that LI-YOLOv8 excels in detecting small objects in remote sensing imagery.</p><table-wrap position="float" id="pone.0321026.t003"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.t003</object-id><label>Table 3</label><caption><title>AP improvement rates for various targets on the RSOD dataset.</title></caption><alternatives><graphic xlink:href="pone.0321026.t003" id="pone.0321026.t003g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Category</th><th align="left" rowspan="1" colspan="1">YOLOv8 AP (%)</th><th align="left" rowspan="1" colspan="1">LI-YOLOv8 AP (%)</th><th align="left" rowspan="1" colspan="1">AP improvement rate (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">aircraft</td><td align="left" rowspan="1" colspan="1">91.2</td><td align="left" rowspan="1" colspan="1">93.0</td><td align="left" rowspan="1" colspan="1">1.8</td></tr><tr><td align="left" rowspan="1" colspan="1">oiltank</td><td align="left" rowspan="1" colspan="1">96.5</td><td align="left" rowspan="1" colspan="1">97.6</td><td align="left" rowspan="1" colspan="1">1.1</td></tr><tr><td align="left" rowspan="1" colspan="1">overpass</td><td align="left" rowspan="1" colspan="1">51.5</td><td align="left" rowspan="1" colspan="1">78.3</td><td align="left" rowspan="1" colspan="1">26.8</td></tr><tr><td align="left" rowspan="1" colspan="1">playground</td><td align="left" rowspan="1" colspan="1">97.9</td><td align="left" rowspan="1" colspan="1">98.5</td><td align="left" rowspan="1" colspan="1">0.6</td></tr></tbody></table></alternatives></table-wrap><p>2) The training results on the NWPU VHR-10 dataset are shown in <xref rid="pone.0321026.g012" ref-type="fig">Fig 12</xref>, followed by analysis.</p><fig position="float" id="pone.0321026.g012"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g012</object-id><label>Fig 12</label><caption><title>PR comparison on NWPU VHR-10 dataset.</title></caption><graphic xlink:href="pone.0321026.g012" position="float"/></fig><p><xref rid="pone.0321026.g012" ref-type="fig">Fig 12</xref>(a) and <xref rid="pone.0321026.g012" ref-type="fig">12</xref>(b) display the Precision-Recall (PR) curves for YOLOv8 and LI-YOLOv8 trained on the NWPU VHR-10 dataset. Overall, the trends observed are similar to those in <xref rid="pone.0321026.g011" ref-type="fig">Fig 11</xref>(a); however, Precision decreases more markedly when Recall exceeds 0.6. This phenomenon is attributable to the NWPU VHR-10 dataset encompassing a larger number of multi-scale targets (such as harbors and vehicles) and possessing higher background complexity compared to the RSOD dataset. For YOLOv8, maintaining Precision at higher Recall levels necessitates greater compromises, often resulting in a rapid decline in Precision. In contrast, <xref rid="pone.0321026.g012" ref-type="fig">Fig 12</xref>(b) illustrates the training results of LI-YOLOv8 on the same dataset, where the PR curve demonstrates a more stable and higher Precision across the board, primarily in two aspects:</p><p>Firstly, When Recall surpasses 0.6, Precision no longer experiences a significant decline but remains relatively stable. This indicates that LI-YOLOv8 possesses enhanced adaptability to small objects and complex backgrounds, thereby mitigating the risk of false positives associated with high Recall.</p><p>Secondly,When Recall exceeds 0.8, Precision continues to be maintained at a relatively substantial level. This suggests that LI-YOLOv8 effectively distinguishes similar backgrounds even when attempting to capture more targets, thereby significantly reducing both false positives and false negatives.</p><p>These differences further indicate that the enhancements made to the algorithm structure and loss function enable LI-YOLOv8 to sustain higher Precision under high Recall conditions. In summary, <xref rid="pone.0321026.g012" ref-type="fig">Fig 12</xref>(a) and <xref rid="pone.0321026.g012" ref-type="fig">12</xref>(b) unequivocally demonstrate the adaptability and robustness of LI-YOLOv8, achieving a superior balance between maximizing Recall and maintaining high Precision in the detection of small targets within remote sensing images.</p><p><xref rid="pone.0321026.t004" ref-type="table">Table 4</xref> displays the AP improvement rates for ten types of targets, including airplane, ship, and storage tank. The table indicates that the AP values for nine target categories have increased. The categories of airplane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, and bridge saw significant improvements, while the vehicle category, due to its small proportion in the images and poor learning ability during the training process, has decreased by 5.2%. Overall, the results demonstrate that the LI-YOLOv8 performs exceptionally well in detecting small objects in remote-sensing photographs.</p><table-wrap position="float" id="pone.0321026.t004"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.t004</object-id><label>Table 4</label><caption><title>AP improvement rates for various targets on the NWPU VHR-10 dataset.</title></caption><alternatives><graphic xlink:href="pone.0321026.t004" id="pone.0321026.t004g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Category</th><th align="left" rowspan="1" colspan="1">YOLOv8 AP (%)</th><th align="left" rowspan="1" colspan="1">LI-YOLOv8 AP (%)</th><th align="left" rowspan="1" colspan="1">Ap improvement rate (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">airplane</td><td align="left" rowspan="1" colspan="1">95.2</td><td align="left" rowspan="1" colspan="1">96.4</td><td align="left" rowspan="1" colspan="1">1.2</td></tr><tr><td align="left" rowspan="1" colspan="1">ship</td><td align="left" rowspan="1" colspan="1">89.0</td><td align="left" rowspan="1" colspan="1">91.9</td><td align="left" rowspan="1" colspan="1">2.9</td></tr><tr><td align="left" rowspan="1" colspan="1">storage tank</td><td align="left" rowspan="1" colspan="1">93.0</td><td align="left" rowspan="1" colspan="1">94.2</td><td align="left" rowspan="1" colspan="1">1.2</td></tr><tr><td align="left" rowspan="1" colspan="1">baseball diamond</td><td align="left" rowspan="1" colspan="1">96.5</td><td align="left" rowspan="1" colspan="1">97.3</td><td align="left" rowspan="1" colspan="1">0.8</td></tr><tr><td align="left" rowspan="1" colspan="1">tennis court</td><td align="left" rowspan="1" colspan="1">78.5</td><td align="left" rowspan="1" colspan="1">80.4</td><td align="left" rowspan="1" colspan="1">1.9</td></tr><tr><td align="left" rowspan="1" colspan="1">basketball court</td><td align="left" rowspan="1" colspan="1">68.4</td><td align="left" rowspan="1" colspan="1">79.6</td><td align="left" rowspan="1" colspan="1">11.2</td></tr><tr><td align="left" rowspan="1" colspan="1">ground track field</td><td align="left" rowspan="1" colspan="1">94.7</td><td align="left" rowspan="1" colspan="1">96.8</td><td align="left" rowspan="1" colspan="1">2.1</td></tr><tr><td align="left" rowspan="1" colspan="1">harbor</td><td align="left" rowspan="1" colspan="1">82.2</td><td align="left" rowspan="1" colspan="1">84.7</td><td align="left" rowspan="1" colspan="1">2.5</td></tr><tr><td align="left" rowspan="1" colspan="1">bridge</td><td align="left" rowspan="1" colspan="1">71.2</td><td align="left" rowspan="1" colspan="1">80.0</td><td align="left" rowspan="1" colspan="1">8.8</td></tr><tr><td align="left" rowspan="1" colspan="1">vehicle</td><td align="left" rowspan="1" colspan="1">73.9</td><td align="left" rowspan="1" colspan="1">68.7</td><td align="left" rowspan="1" colspan="1">-5.2</td></tr></tbody></table></alternatives></table-wrap><p>(2) Visual Comparison</p><p>1) On the RSOD dataset, the results comparing the LI-YOLOv8 algorithm&#x02019;s performance with that of the YOLOv8 algorithm are shown in <xref rid="pone.0321026.g013" ref-type="fig">Fig 13</xref>, and the analysis is as follows.</p><fig position="float" id="pone.0321026.g013"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g013</object-id><label>Fig 13</label><caption><title>Visualization comparison on RSOD dataset.</title><p>(a), (b), and (c) represent complex background targets, tiny targets, and multi-scale targets, whereas(A), (B), and (C) the original image, YOLOv8, and LI-YOLOv8, respectively.</p></caption><graphic xlink:href="pone.0321026.g013" position="float"/></fig><p>In complex background scenarios, YOLOv8 mistakenly identifies white smoke in the air as oiltank, whereas LI-YOLOv8 does not does not misclassify white smoke as a fuel tank. In scenes with tiny targets, YOLOv8 exhibits a high number of missed detections for tiny oiltank in the image, while the LI-YOLOv8 is capable of detecting a greater number of these tiny targets. For multi-scale targets, YOLOv8 does not detect the small playground on the right side, while LI-YOLOv8 can more accurately detect these small-scale targets.</p><p>2) On the NWPU VHR-10 dataset, the contrast of detection performance between LI-YOLOv8 and YOLOv8 is shown in <xref rid="pone.0321026.g014" ref-type="fig">Fig 14</xref>, and the analysis is provided below.</p><fig position="float" id="pone.0321026.g014"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g014</object-id><label>Fig 14</label><caption><title>Visualization comparison on NWPU VHR-10 dataset.</title><p>(a), (b), and (c) represent complex background targets, tiny targets, and multi-scale targets, whereas (A), (B), and (C) the original image, YOLOv8, and LI-YOLOv8.</p></caption><graphic xlink:href="pone.0321026.g014" position="float"/></fig><p>In complex background scenes, YOLOv8 exhibits extensive missed detections of the small target tennis court and mistakenly classifies a blue warehouse as a tennis court. In comparison, LI-YOLOv8 not only detects more small targets in complex backgrounds but also accurately identifies small targets within tennis courts. In scenes with tiny targets, LI-YOLOv8 detects more low-pixel tiny ships compared to YOLOv8. For multi-scale targets, YOLOv8 failed to detect the tennis court located above the playground, yet LI-YOLOv8 was able to identify a greater number of additional small targets.</p><p>3) Heatmap Comparison</p><p><xref rid="pone.0321026.g015" ref-type="fig">Fig 15</xref>(a), <xref rid="pone.0321026.g015" ref-type="fig">15</xref>(b), and <xref rid="pone.0321026.g015" ref-type="fig">15</xref>(c) represent the original image, the heatmap of YOLOv8, and the heatmap of LI-YOLOv8. The color gradient varies from blue to red, reflecting the model&#x02019;s attention to targets from low to high. In comparison with the baseline algorithm, LI-YOLOv8 demonstrates a higher coverage rate of red areas for small targets, demonstrating a higher level of attention to these small targets.</p><fig position="float" id="pone.0321026.g015"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.g015</object-id><label>Fig 15</label><caption><title>Heatmap Comparison.</title><p>((A), (B), and (C) the heatmap of YOLOv8, and the heatmap of LI-YOLOv8.</p></caption><graphic xlink:href="pone.0321026.g015" position="float"/></fig><p>(4) Numerical Comparison</p><p>1) We conducted comparative experiments using the RSOD dataset alongside YOLOv3-tiny, YOLOv5n, YOLOv5s, YOLOv6s, YOLOv8n, YOLOv9c [<xref rid="pone.0321026.ref031" ref-type="bibr">31</xref>], YOLOv10n [<xref rid="pone.0321026.ref032" ref-type="bibr">32</xref>] and LI-YOLOv8. The results obtained from experiments are showcased in <xref rid="pone.0321026.t005" ref-type="table">Table 5</xref>, followed by the analysis.</p><table-wrap position="float" id="pone.0321026.t005"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.t005</object-id><label>Table 5</label><caption><title>Algorithm comparison experiment on the RSOD dataset.</title></caption><alternatives><graphic xlink:href="pone.0321026.t005" id="pone.0321026.t005g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">mAP@0.5 (%)</th><th align="left" rowspan="1" colspan="1">mAP@0.5:0.95 (%)</th><th align="left" rowspan="1" colspan="1">Precision (%)</th><th align="left" rowspan="1" colspan="1">Recall (%)</th><th align="left" rowspan="1" colspan="1">Parameters (M)</th><th align="left" rowspan="1" colspan="1">GFLOPs (G)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">YOLOv3-tiny</td><td align="left" rowspan="1" colspan="1">88.8</td><td align="left" rowspan="1" colspan="1">64.3</td><td align="left" rowspan="1" colspan="1">88.4</td><td align="left" rowspan="1" colspan="1">88.0</td><td align="left" rowspan="1" colspan="1">12.1</td><td align="left" rowspan="1" colspan="1">19.0</td></tr><tr><td align="left" rowspan="1" colspan="1">YOLOv5n</td><td align="left" rowspan="1" colspan="1">84.4</td><td align="left" rowspan="1" colspan="1">66.1</td><td align="left" rowspan="1" colspan="1">98.1</td><td align="left" rowspan="1" colspan="1">69.7</td><td align="left" rowspan="1" colspan="1">2.5</td><td align="left" rowspan="1" colspan="1">7.2</td></tr><tr><td align="left" rowspan="1" colspan="1">YOLOv5s</td><td align="left" rowspan="1" colspan="1">89.9</td><td align="left" rowspan="1" colspan="1">67.7</td><td align="left" rowspan="1" colspan="1">95.0</td><td align="left" rowspan="1" colspan="1">83.4</td><td align="left" rowspan="1" colspan="1">24.0</td><td align="left" rowspan="1" colspan="1">9.2</td></tr><tr><td align="left" rowspan="1" colspan="1">YOLOv6s</td><td align="left" rowspan="1" colspan="1">89.7</td><td align="left" rowspan="1" colspan="1">67.5</td><td align="left" rowspan="1" colspan="1">91.9</td><td align="left" rowspan="1" colspan="1">84.6</td><td align="left" rowspan="1" colspan="1">16.3</td><td align="left" rowspan="1" colspan="1">44.2</td></tr><tr><td align="left" rowspan="1" colspan="1">YOLOv8n</td><td align="left" rowspan="1" colspan="1">84.3</td><td align="left" rowspan="1" colspan="1">64.9</td><td align="left" rowspan="1" colspan="1">90.6</td><td align="left" rowspan="1" colspan="1">69.4</td><td align="left" rowspan="1" colspan="1">3.0</td><td align="left" rowspan="1" colspan="1">8.2</td></tr><tr><td align="left" rowspan="1" colspan="1">YOLOv9c</td><td align="left" rowspan="1" colspan="1">91.7</td><td align="left" rowspan="1" colspan="1">69.3</td><td align="left" rowspan="1" colspan="1">94.8</td><td align="left" rowspan="1" colspan="1">86.0</td><td align="left" rowspan="1" colspan="1">25.5</td><td align="left" rowspan="1" colspan="1">103.0</td></tr><tr><td align="left" rowspan="1" colspan="1">YOLOv10n</td><td align="left" rowspan="1" colspan="1">90.1</td><td align="left" rowspan="1" colspan="1">65.0</td><td align="left" rowspan="1" colspan="1">93.0</td><td align="left" rowspan="1" colspan="1">84.1</td><td align="left" rowspan="1" colspan="1">2.7</td><td align="left" rowspan="1" colspan="1">8.4</td></tr><tr><td align="left" rowspan="1" colspan="1">LI-YOLOv8(Ours)</td><td align="left" rowspan="1" colspan="1">91.9</td><td align="left" rowspan="1" colspan="1">67.0</td><td align="left" rowspan="1" colspan="1">95.1</td><td align="left" rowspan="1" colspan="1">86.0</td><td align="left" rowspan="1" colspan="1">2.7</td><td align="left" rowspan="1" colspan="1">6.3</td></tr></tbody></table></alternatives></table-wrap><p>YOLOv3-tiny exhibits mAP@0.5, mAP@0.5:0.95, and Precision values that are 3.1%, 2.7%, and 6.7% lower than those of LI-YOLOv8, respectively. It also possesses 9.4M more Parameters and 12.7G more GFLOPs, although its Recall is 2.0% higher. This suggests that, aside from Recall, YOLOv3-tiny underperforms in other performance metrics and demands higher hardware storage capacity.</p><p>YOLOv5n shows an increase in mAP@0.5:0.95 and Precision by 1.8% and 9.7% compared to YOLOv3-tiny, but it has lower values for mAP@0.5 and Recall by 4.4% and 18.3%, respectively. Its Parameters and GFLOPs are lower at 9.6M and 11.8G. Compared to LI-YOLOv8, YOLOv5n is lower in mAP@0.5, mAP@0.5:0.95, and Recall by 7.5%, 0.9%, and 16.3%, respectively, while Precision is 3.0% higher, with Parameters slightly lower by 0.2M and GFLOPs higher by 0.9G, suggesting that multiple metrics of YOLOv5n require further enhancement.</p><p>YOLOv5s performs better than YOLOv5n in mAP@0.5, mAP@0.5:0.95, and Recall, showing increases of 5.5%, 1.6%, and 13.7%, respectively. However, its Precision is 3.1% lower, with a substantial increase in Parameters to 21.5M and a slight rise in GFLOPs to 2.0G. Compared to LI-YOLOv8, YOLOv5s is lower in mAP@0.5, Precision, and Recall by 2.0%, 0.1%, and 2.6%, respectively, while mAP@0.5:0.95 is higher by 0.7%. Its Parameters and GFLOPs are also greater by 21.3M and 2.9G, indicating that YOLOv5s is more complex and challenging to deploy.</p><p>YOLOv6s performs marginally below YOLOv5s in terms of mAP@0.5, mAP@0.5:0.95, and Precision, with reductions of 0.2%, 0.2%, and 3.1%, respectively. However, its Parameters and GFLOPs are excessively high at 16.3M and 44.2G. Compared to LI-YOLOv8, YOLOv6s is lower in mAP@0.5, Precision, and Recall by 2.2%, 3.2%, and 1.%, respectively, and mAP@0.5:0.95 is lower by 0.5%. Additionally, its Parameters and GFLOPs increase significantly by 13.6M and 37.9G, indicating a substantial computational burden and deployment challenges.</p><p>YOLOv8n shows lower values in mAP@0.5, mAP@0.5:0.95, Precision, and Recall than YOLOv6s by 4.8%, 2.6%, 1.3%, and 15.2%, respectively, with Parameters and GFLOPs that are 13.3M and 36G lower. Compared to LI-YOLOv8, YOLOv8n is lower in all metrics by 7.6%, 2.1%, 4.5%, and 16.6%, while its Parameters and GFLOPs are higher by 0.3M and 1.9G. This indicates that LI-YOLOv8 is smaller in scale and offers superior detection rates.</p><p>YOLOv9c achieves mAP@0.5, mAP@0.5:0.95, Precision, and Recall values of 91.7%, 69.3%, 94.8%, and 86.0%, respectively, which significantly surpass the first five algorithms. However, its mAP@0.5 and Precision are lower than the LI-YOLOv8 by 0.2% and 0.3%, while its mAP@0.5:0.95 is higher by 2.3%. It also has the highest Parameters and GFLOPs among the compared algorithms, reaching 25.5M and 103.0G, making it difficult to deploy.</p><p>Although YOLOv10n shows a significant improvement in Parameters and GFLOPs compared to YOLOv9c, being lower by 22.8M and 94.6G, its mAP@0.5, mAP@0.5:0.95, Precision, and Recall are lower by 1.6%, 4.3%, 1.8%, and 1.9%, respectively. Compared to LI-YOLOv8, it is lower in all metrics by 1.8%, 2.0%, 2.1%, and 1.9%, indicating that several metrics need improvement.</p><p>LI-YOLOv8 outperforms the seven algorithms mentioned above, achieving mAP@0.5, mAP@0.5:0.95, Precision, and Recall values of 91.9%, 67.0%, 95.1%, and 86.0%, respectively. It also significantly reduces the Parameters and GFLOPs to just 2.7M and 6.3G. The aggregate experimental results suggest that LI-YOLOv8 achieves superior performance in detecting small objects in remote sensing imagery.</p><p>2) On the NWPU VHR-10 dataset, comparative experiments are performed using YOLOv3-tiny, YOLOv5n, YOLOv8n, YOLOv10n, and LI-YOLOv8. The findings can be found in <xref rid="pone.0321026.t006" ref-type="table">Table 6</xref>, followed by the analysis.</p><table-wrap position="float" id="pone.0321026.t006"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.t006</object-id><label>Table 6</label><caption><title>Algorithm comparison experiment on the NWPU VHR-10 dataset.</title></caption><alternatives><graphic xlink:href="pone.0321026.t006" id="pone.0321026.t006g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">mAP@0.5 (%)</th><th align="left" rowspan="1" colspan="1">mAP@0.5:0.95 (%)</th><th align="left" rowspan="1" colspan="1">Precision (%)</th><th align="left" rowspan="1" colspan="1">Recall (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">YOLOv3-tiny</td><td align="left" rowspan="1" colspan="1">87.0</td><td align="left" rowspan="1" colspan="1">54.5</td><td align="left" rowspan="1" colspan="1">91.7</td><td align="left" rowspan="1" colspan="1">80.2</td></tr><tr><td align="left" rowspan="1" colspan="1">YOLOv5n</td><td align="left" rowspan="1" colspan="1">84.1</td><td align="left" rowspan="1" colspan="1">53.5</td><td align="left" rowspan="1" colspan="1">93.9</td><td align="left" rowspan="1" colspan="1">71.9</td></tr><tr><td align="left" rowspan="1" colspan="1">YOLOv8n</td><td align="left" rowspan="1" colspan="1">84.3</td><td align="left" rowspan="1" colspan="1">53.8</td><td align="left" rowspan="1" colspan="1">92.0</td><td align="left" rowspan="1" colspan="1">74.6</td></tr><tr><td align="left" rowspan="1" colspan="1">YOLOv10n</td><td align="left" rowspan="1" colspan="1">78.7</td><td align="left" rowspan="1" colspan="1">46.7</td><td align="left" rowspan="1" colspan="1">72.5</td><td align="left" rowspan="1" colspan="1">71.6</td></tr><tr><td align="left" rowspan="1" colspan="1">LI-YOLOv8(Ours)</td><td align="left" rowspan="1" colspan="1">87.1</td><td align="left" rowspan="1" colspan="1">54.9</td><td align="left" rowspan="1" colspan="1">95.8</td><td align="left" rowspan="1" colspan="1">76.9</td></tr></tbody></table></alternatives></table-wrap><p>YOLOv3-tiny has a Recall that is 3.3% higher than the LI-YOLOv8, but it shows lower values in mAP@0.5, mAP@0.5:0.95, and Precision by 0.1%, 0.4%, and 4.1%, respectively. This indicates that, aside from Recall, other performance metrics of YOLOv3-tiny require improvement.</p><p>YOLOv5n&#x02019;s Precision is 2.2% superior to YOLOv3-tiny; nevertheless, it registers lower scores in mAP@0.5, mAP@0.5:0.95, and Recall by 3.3%, 1.0%, and 8.3%, respectively. When compared to our method, YOLOv5n displays inferior metrics across all indicators by 3.0%, 1.9%, 1.9%, and 5.0%, demonstrating its weaker detection capabilities.</p><p>YOLOv8n&#x02019;s metrics are close to those of YOLOv5n, with mAP@0.5, mAP@0.5:0.95, and Recall only 0.2%, 0.3%, and 3.0% higher, respectively, while Precision is 1.9% lower. However, all metrics of YOLOv8n are still below those of the LI-YOLOv8, lower by 2.8%, 1.1%, 3.8%, and 2.3%.</p><p>YOLOv10n&#x02019;s metrics are lower than those of LI-YOLOv8, achieving only 78.7%, 46.7%, 72.%, and 71.6% for mAP@0.5, mAP@0.5:0.95, Precision, and Recall, respectively. These values are lower than the LI-YOLOV8 by 8.4%, 8.2%, 23.3%, and 5.3%, indicating a low detection rate for YOLOv10n.</p><p>Compared to the aforementioned algorithms, LI-YOLOv8 outperforms the others with mAP@0.5, mAP@0.5:0.95, and Precision values of 87.1%, 54.9%, and 95.8%, respectively. Additionally, as shown in <xref rid="pone.0321026.t005" ref-type="table">Table 5</xref>, the LI-YOLOv8 has the lowest number of parameters and computational load.</p><p>Considering the analysis presented, the average precision of LI-YOLOv8 has significantly improved, with a reduction in the number of parameters and computational cost, and it exhibits a stronger recognition capability for small targets in remote sensing imagery.</p></sec><sec id="sec020"><title>Generalization experiment.</title><p>Generalization experiments were conducted using YOLOv8 and LI-YOLOv8 on the TinyPerson, LEVIR-ship, brain-tumor, and smoke_fire_1 datasets. The results are presented in <xref rid="pone.0321026.t007" ref-type="table">Table 7</xref>, and the analysis is discussed below.</p><table-wrap position="float" id="pone.0321026.t007"><object-id pub-id-type="doi">10.1371/journal.pone.0321026.t007</object-id><label>Table 7</label><caption><title>Generalization experiment on the TinyPerson dataset.</title></caption><alternatives><graphic xlink:href="pone.0321026.t007" id="pone.0321026.t007g" position="float"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Dataset</th><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">mAP@0.5 (%)</th><th align="left" rowspan="1" colspan="1">mAP@0.5:0.95 (%)</th><th align="left" rowspan="1" colspan="1">Precision (%)</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">TinyPerson</td><td align="left" rowspan="1" colspan="1">YOLOv8</td><td align="left" rowspan="1" colspan="1">44.2</td><td align="left" rowspan="1" colspan="1">21.8</td><td align="left" rowspan="1" colspan="1">85.8</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LI-YOLOv8</td><td align="left" rowspan="1" colspan="1">46.8</td><td align="left" rowspan="1" colspan="1">22.0</td><td align="left" rowspan="1" colspan="1">92.8</td></tr><tr><td align="left" rowspan="1" colspan="1">LEVIR-ship</td><td align="left" rowspan="1" colspan="1">YOLOv8</td><td align="left" rowspan="1" colspan="1">72.0</td><td align="left" rowspan="1" colspan="1">29.8</td><td align="left" rowspan="1" colspan="1">79.6</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LI-YOLOv8</td><td align="left" rowspan="1" colspan="1">77.3</td><td align="left" rowspan="1" colspan="1">33.1</td><td align="left" rowspan="1" colspan="1">79.7</td></tr><tr><td align="left" rowspan="1" colspan="1">brain-tumor</td><td align="left" rowspan="1" colspan="1">YOLOv8</td><td align="left" rowspan="1" colspan="1">42.8</td><td align="left" rowspan="1" colspan="1">31.9</td><td align="left" rowspan="1" colspan="1">50.9</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LI-YOLOv8</td><td align="left" rowspan="1" colspan="1">45.4</td><td align="left" rowspan="1" colspan="1">35.4</td><td align="left" rowspan="1" colspan="1">54.6</td></tr><tr><td align="left" rowspan="1" colspan="1">smoke_fire_1</td><td align="left" rowspan="1" colspan="1">YOLOv8</td><td align="left" rowspan="1" colspan="1">80.8</td><td align="left" rowspan="1" colspan="1">58.0</td><td align="left" rowspan="1" colspan="1">70.1</td></tr><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">LI-YOLOv8</td><td align="left" rowspan="1" colspan="1">83.1</td><td align="left" rowspan="1" colspan="1">59.3</td><td align="left" rowspan="1" colspan="1">74.5</td></tr></tbody></table></alternatives></table-wrap><p>On the TinyPerson dataset, LI-YOLOv8 achieved improvements of 2.6% in mAP@0.5, 0.2% in mAP@0.5:0.95, and 7.0% in Precision, compared to YOLOv8. On the LEVIR-ship dataset, LI-YOLOv8 outperformed YOLOv8 with increases of 5.3% in mAP@0.5, 3.3% in mAP@0.5:0.95, and 0.1% in Precision. These results demonstrate that LI-YOLOv8 is not only effective for the RSOD and NWPU VHR-10 datasets but also enhances the detection of small targets in complex scenarios across other remote sensing datasets.In the brain-tumor dataset, generalization experiments comparing YOLOv8 and LI-YOLOv8 revealed that LI-YOLOv8 improved mAP@0.5 by 2.6%, mAP@0.5:0.95 by 3.5%, and Precision by 3.7%. This indicates that LI-YOLOv8 is effective not only for specific target detection tasks in remote sensing images but also enhances the detection of non-small targets in medical images. Similarly, on the smoke_fire_1 dataset, LI-YOLOv8 achieved increases of 2.3% in mAP@0.5, 1.3% in mAP@0.5:0.95, and 4.4% in Precision compared to YOLOv8. These improvements validate that the LI-YOLOv8 algorithm effectively detects fire and smoke targets in urban surveillance settings, significantly enhancing their detection rates.In summary, LI-YOLOv8 not only demonstrates superior performance in detecting targets within remote sensing images but also exhibits outstanding results in detecting medium and large-sized targets across various other domains, highlighting its versatility and general applicability.</p></sec></sec></sec><sec sec-type="conclusions" id="sec021"><title>Conclusions</title><p>To address the challenges in identifying small targets in remote sensing images, such as difficulties in feature extraction, confusion between background and targets, significant deviations in prediction boxes, high rates of missed detections and false positives, computational complexity, and high resource consumption, we propose a lightweight small target detection algorithm for remote sensing images that combines GSConv and PConv, named LI-YOLOv8. We enhance feature extraction by improving SPPF to SPPF-R, increase the focus on small target areas by upgrading the C2f in the neck network to C2f-E, design a lightweight detection head (GP-Detect) to reduce network complexity, and replace the bounding box loss function from CIoU to Inner-Wise IoU to improve the algorithm&#x02019;s generalization capability. Experimental results demonstrate that our proposed algorithm outperforms baseline methods and other recent YOLO algorithms in small target detection within remote sensing images, effectively enhancing detection performance and exhibiting strong generalizability.</p><p>Future research will continue to focus on optimizing model training time by reducing training duration while maintaining model lightweightness and accuracy. Additionally, efforts will be made to extend the model&#x02019;s applicability to a broader range of real-world scenarios, thereby enhancing its practical value and efficiency.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0321026.ref001"><label>1</label><mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>W</given-names></name>, <name><surname>Anguelov</surname><given-names>D</given-names></name>, <name><surname>Erhan</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>SSD: Single shot multibox detector.</article-title> In: <source>Computer Vision&#x02013;ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11&#x02013;14, 2016, Proceedings, Part I 14</source>. <publisher-name>Springer</publisher-name>; <year>2016</year>. p. <fpage>21</fpage>&#x02013;<lpage>37</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref002"><label>2</label><mixed-citation publication-type="book"><name><surname>Redmon</surname><given-names>J</given-names></name>, <name><surname>Divvala</surname><given-names>S</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <etal>et al</etal>. <article-title>You only look once: unified, real-time object detection.</article-title> In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2016</year>. p. <fpage>779</fpage>&#x02013;<lpage>88</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref003"><label>3</label><mixed-citation publication-type="book"><name><surname>Redmon</surname><given-names>J</given-names></name>, <name><surname>Farhadi</surname><given-names>A</given-names></name>. <article-title>YOLO9000: better, faster, stronger.</article-title> In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>. <publisher-name>IEEE</publisher-name>; <year>2017</year>. p. <fpage>7263</fpage>&#x02013;<lpage>71</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Redmon</surname><given-names>J</given-names></name>, <name><surname>Farhadi</surname><given-names>A</given-names></name>. <article-title>YOLOv3: an incremental improvement</article-title>. <source>arXiv preprint</source>. <year>2018</year>. <ext-link xlink:href="https://arxiv.org/abs/1804.02767" ext-link-type="uri">https://arxiv.org/abs/1804.02767</ext-link></mixed-citation></ref><ref id="pone.0321026.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Bochkovskiy</surname><given-names>A</given-names></name>, <name><surname>Wang</surname><given-names>C</given-names></name>, <name><surname>Liao</surname><given-names>H</given-names></name>. <article-title>YOLOv4: Optimal speed and accuracy of object detection</article-title>. <source>arXiv preprint</source>. <year>2018</year>. <ext-link xlink:href="https://arxiv.org/abs/2004.10934" ext-link-type="uri">https://arxiv.org/abs/2004.10934</ext-link></mixed-citation></ref><ref id="pone.0321026.ref006"><label>6</label><mixed-citation publication-type="book"><name><surname>Wang</surname><given-names>C</given-names></name>, <name><surname>Bochkovskiy</surname><given-names>A</given-names></name>, <name><surname>Liao</surname><given-names>H</given-names></name>. <article-title>YOLOv7: trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.</article-title> In: <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>. <year>2023</year>. p. <fpage>7464</fpage>&#x02013;<lpage>75</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Feng</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>S</given-names></name>. <article-title>Finding nonrigid tiny person with densely cropped and local attention object detector networks in low-altitude aerial images</article-title>. <source>IEEE J Select Topics Appl Earth Observ Remote Sens</source>. <year>2022</year>;<volume>15</volume>:<fpage>4371</fpage>&#x02013;<lpage>85</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Park</surname><given-names>J</given-names></name>. <article-title>BAM: bottleneck attention module</article-title>. <source>arXiv preprint</source>. <year>2018</year>. <ext-link xlink:href="https://arxiv.org/abs/1807.06514" ext-link-type="uri">https://arxiv.org/abs/1807.06514</ext-link></mixed-citation></ref><ref id="pone.0321026.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Luo</surname><given-names>X</given-names></name>, <name><surname>Wu</surname><given-names>Y</given-names></name>, <name><surname>Zhao</surname><given-names>L</given-names></name>. <article-title>YOLOD: a target detection method for UAV aerial imagery</article-title>. <source>Remote Sens</source>. <year>2022</year>;<volume>14</volume>(<issue>14</issue>):<fpage>3240</fpage>.</mixed-citation></ref><ref id="pone.0321026.ref010"><label>10</label><mixed-citation publication-type="book"><name><surname>Zhao</surname><given-names>H</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Zhao</surname><given-names>Y</given-names></name>. <article-title>YOLOv7-SEA: object detection of maritime UAV images based on improved YOLOv7.</article-title> In: <source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source>. <year>2023</year>. p. <fpage>233</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>J</given-names></name>, <name><surname>Fu</surname><given-names>W</given-names></name>, <name><surname>Ma</surname><given-names>J</given-names></name>, <name><surname>Wang</surname><given-names>G</given-names></name>. <article-title>A lightweight YOLOv7 insulator defect detection algorithm based on DSC-SE</article-title>. <source>PLoS One</source>. <year>2023</year>;<volume>18</volume>(<issue>12</issue>):e0289162. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0289162</pub-id>
<pub-id pub-id-type="pmid">38117838</pub-id>
</mixed-citation></ref><ref id="pone.0321026.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Xie</surname><given-names>S</given-names></name>, <name><surname>Zhou</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>C</given-names></name>, <etal>et al</etal>. <article-title>CSPPartial-YOLO: a lightweight YOLO-based method for typical objects detection in remote sensing images</article-title>. <source>IEEE J Select Topics Appl Earth Observ Remote Sens</source>. <year>2023</year>.</mixed-citation></ref><ref id="pone.0321026.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>C</given-names></name>, <name><surname>Wang</surname><given-names>C</given-names></name>, <name><surname>Yang</surname><given-names>D</given-names></name>. <article-title>Underwater small target detection based on dynamic convolution and attention mechanism</article-title>. <source>Front Marine Sci</source>. <year>2024</year>;<volume>11</volume>:<fpage>1348883</fpage>.</mixed-citation></ref><ref id="pone.0321026.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>C</given-names></name>, <name><surname>Zhou</surname><given-names>A</given-names></name>, <name><surname>Yao</surname><given-names>A</given-names></name>. <article-title>Omni-dimensional dynamic convolution</article-title>. <source>arXiv preprint</source>. <year>2022</year>. <ext-link xlink:href="https://arxiv.org/abs/2209.07947" ext-link-type="uri">https://arxiv.org/abs/2209.07947</ext-link></mixed-citation></ref><ref id="pone.0321026.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Zhu</surname><given-names>S</given-names></name>, <name><surname>Miao</surname><given-names>M</given-names></name>. <article-title>Lightweight high-precision SAR ship detection method based on YOLOv7-LDS</article-title>. <source>PLoS One</source>. <year>2024</year>;<volume>19</volume>(<issue>2</issue>):e0296992. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0296992</pub-id>
<pub-id pub-id-type="pmid">38349872</pub-id>
</mixed-citation></ref><ref id="pone.0321026.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <article-title>Spatial pyramid pooling in deep convolutional networks for visual recognition</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2015</year>;<volume>37</volume>(<issue>9</issue>):<fpage>1904</fpage>&#x02013;<lpage>16</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TPAMI.2015.2389824</pub-id>
<pub-id pub-id-type="pmid">26353135</pub-id>
</mixed-citation></ref><ref id="pone.0321026.ref017"><label>17</label><mixed-citation publication-type="book"><name><surname>Lin</surname><given-names>T</given-names></name>, <name><surname>Doll&#x000e1;r</surname><given-names>P</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>. <article-title>Feature pyramid networks for object detection.</article-title> In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2017</year>. p. <fpage>2117</fpage>&#x02013;<lpage>25</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref018"><label>18</label><mixed-citation publication-type="book"><name><surname>Chen</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>. <article-title>Panet: a context based predicate association network for scene graph generation.</article-title> In: <source>2019 IEEE International Conference on Multimedia and Expo (ICME)</source>. <publisher-name>IEEE</publisher-name>; <year>2019</year>. p. <fpage>508</fpage>&#x02013;<lpage>13</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Zheng</surname><given-names>Z</given-names></name>, <name><surname>Wang</surname><given-names>P</given-names></name>, <name><surname>Ren</surname><given-names>D</given-names></name>, <name><surname>Liu</surname><given-names>W</given-names></name>, <name><surname>Ye</surname><given-names>R</given-names></name>, <name><surname>Hu</surname><given-names>Q</given-names></name>, <etal>et al</etal>. <article-title>Enhancing geometric factors in model learning and inference for object detection and instance segmentation</article-title>. <source>IEEE Trans Cybern</source>. <year>2022</year>;<volume>52</volume>(<issue>8</issue>):<fpage>8574</fpage>&#x02013;<lpage>86</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TCYB.2021.3095305</pub-id>
<pub-id pub-id-type="pmid">34437079</pub-id>
</mixed-citation></ref><ref id="pone.0321026.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>, <name><surname>Yang</surname><given-names>D</given-names></name>. <article-title>RFAConv: Innovating spatial attention and standard convolutional operation</article-title>. <source>arXiv preprint</source>. <year>2023</year>. <ext-link xlink:href="https://arxiv.org/abs/2304.03198" ext-link-type="uri">https://arxiv.org/abs/2304.03198</ext-link></mixed-citation></ref><ref id="pone.0321026.ref021"><label>21</label><mixed-citation publication-type="book"><name><surname>Ouyang</surname><given-names>D</given-names></name>, <name><surname>He</surname><given-names>S</given-names></name>, <name><surname>Zhang</surname><given-names>G</given-names></name>. <article-title>Efficient multi-scale attention module with cross-spatial learning.</article-title> In: <source>ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source>. <publisher-name>IEEE</publisher-name>; <year>2023</year>. p. <fpage>1</fpage>&#x02013;<lpage>5</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>H</given-names></name>, <name><surname>Li</surname><given-names>J</given-names></name>, <name><surname>Wei</surname><given-names>H</given-names></name>. <article-title>Slim-neck by GSConv: a better design paradigm of detector architectures for autonomous vehicles</article-title>. <source>arXiv preprint</source>. <year>2022</year>. <ext-link xlink:href="https://arxiv.org/abs/2206.02424" ext-link-type="uri">https://arxiv.org/abs/2206.02424</ext-link>.</mixed-citation></ref><ref id="pone.0321026.ref023"><label>23</label><mixed-citation publication-type="book"><name><surname>Chollet</surname><given-names>F</given-names></name>. <article-title>Xception: deep learning with depthwise separable convolutions.</article-title> In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2017</year>. p. <fpage>1251</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref024"><label>24</label><mixed-citation publication-type="book"><name><surname>Chen</surname><given-names>J</given-names></name>, <name><surname>Kao</surname><given-names>S</given-names></name>, <name><surname>He</surname><given-names>H</given-names></name>. <article-title>Run, don&#x02019;t walk: chasing higher flops for faster neural networks.</article-title> In: <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>. <year>2023</year>. p. <fpage>12021</fpage>&#x02013;<lpage>31</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Xu</surname><given-names>C</given-names></name>, <name><surname>Zhang</surname><given-names>S</given-names></name>. <article-title>Inner-IoU: more effective intersection over union loss with auxiliary bounding box</article-title>. <source>arXiv preprint</source>. <year>2023</year>. <comment>doi: arXiv:2311.02877</comment></mixed-citation></ref><ref id="pone.0321026.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Tong</surname><given-names>Z</given-names></name>, <name><surname>Chen</surname><given-names>Y</given-names></name>, <name><surname>Xu</surname><given-names>Z</given-names></name>. <article-title>Wise-IoU: bounding box regression loss with dynamic focusing mechanism</article-title>. <source>arXiv preprint</source>. <year>2023</year>. <ext-link xlink:href="https://arxiv.org/abs/2301.10051" ext-link-type="uri">https://arxiv.org/abs/2301.10051</ext-link></mixed-citation></ref><ref id="pone.0321026.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Long</surname><given-names>Y</given-names></name>, <name><surname>Gong</surname><given-names>Y</given-names></name>, <name><surname>Xiao</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>Q</given-names></name>. <article-title>Accurate object localization in remote sensing images based on convolutional neural networks</article-title>. <source>IEEE Trans Geosci Remote Sensing</source>. <year>2017</year>;<volume>55</volume>(<issue>5</issue>):<fpage>2486</fpage>&#x02013;<lpage>98</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/tgrs.2016.2645610</pub-id></mixed-citation></ref><ref id="pone.0321026.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>G</given-names></name>, <name><surname>Han</surname><given-names>J</given-names></name>, <name><surname>Zhou</surname><given-names>P</given-names></name>, <name><surname>Guo</surname><given-names>L</given-names></name>. <article-title>Multi-class geospatial object detection and geographic image classification based on collection of part detectors</article-title>. <source>ISPRS J Photogram Remote Sens</source>. <year>2014</year>;<volume>98</volume>:<fpage>119</fpage>&#x02013;<lpage>32</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.isprsjprs.2014.10.002</pub-id></mixed-citation></ref><ref id="pone.0321026.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>G</given-names></name>, <name><surname>Han</surname><given-names>J</given-names></name>, <name><surname>Zhou</surname><given-names>P</given-names></name>. <article-title>A survey on object detection in optical remote sensing images</article-title>. <source>SPRS J Photogram Remote Sens</source>. <year>2016</year>;<volume>117</volume>:<fpage>11</fpage>&#x02013;<lpage>28</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>G</given-names></name>, <name><surname>Zhou</surname><given-names>P</given-names></name>, <name><surname>Han</surname><given-names>J</given-names></name>. <article-title>Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images</article-title>. <source>IEEE Trans Geosci Remote Sens</source>. <year>2016</year>;<volume>54</volume>(<issue>12</issue>):<fpage>7405</fpage>&#x02013;<lpage>15</lpage>.</mixed-citation></ref><ref id="pone.0321026.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>CY</given-names></name>, <name><surname>Yeh</surname><given-names>IH</given-names></name>, <name><surname>Liao</surname><given-names>HYM</given-names></name>. <article-title>YOLOv9: Learning what you want to learn using programmable gradient information</article-title>. <source>arXiv preprint</source>. <year>2024</year>. <ext-link xlink:href="https://arxiv.org/abs/2402.13616" ext-link-type="uri">https://arxiv.org/abs/2402.13616</ext-link></mixed-citation></ref><ref id="pone.0321026.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>A</given-names></name>, <name><surname>Chen</surname><given-names>H</given-names></name>, <name><surname>Liu</surname><given-names>L</given-names></name>. <article-title>YOLOv10: real-time end-to-end object detection</article-title>. <source>arXiv preprint</source>. <year>2024</year>. <ext-link xlink:href="https://arxiv.org/abs/2405.14458" ext-link-type="uri">https://arxiv.org/abs/2405.14458</ext-link></mixed-citation></ref></ref-list></back><sub-article article-type="author-comment" id="pone.0321026.r001" specific-use="rebutted-decision-letter-unavailable"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0321026.r001</article-id><title-group><article-title>Author response to Decision Letter 0</article-title></title-group><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">23 Nov 2024</named-content>
</p></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0321026.r002" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0321026.r002</article-id><title-group><article-title>Decision Letter 0</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Yile</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Yile Chen</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Yile Chen</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0321026" id="rel-obj002" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>0</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">16 Dec 2024</named-content>
</p><p><!--<div>-->PONE-D-24-53882<!--</div>--><!--<div>-->LI-YOLOv8: Lightweight Small Target Detection Algorithm for Remote Sensing Images that Combines GSConv and PConv<!--</div>--><!--<div>-->PLOS ONE</p><p>Dear Dr.&#x000a0;Yan,</p><p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE&#x02019;s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p><p>Please submit your revised manuscript by Jan 30 2025 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at&#x000a0;<email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p><p>Please include the following items when submitting your revised manuscript:<!--</div>--></p><p><list list-type="bullet"><list-item><p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p></list-item><list-item><p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p></list-item><list-item><p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p></list-item></list>
</p><p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p><p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xlink:href="https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">https://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xlink:href="https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&#x00026;utm_source=authorletters&#x00026;utm_campaign=protocols</ext-link>.</p><p>We look forward to receiving your revised manuscript.</p><p>Kind regards,</p><p>Yile Chen, Ph.D. in Architecture</p><p>Academic Editor</p><p>PLOS ONE</p><p>
<bold>Journal Requirements:</bold>
</p><p>1. When submitting your revision, we need you to address these additional requirements.</p><p>Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at&#x000a0;</p><p><ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and&#x000a0;</p><p>
<ext-link xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
</p><p>2. Please note that PLOS ONE has specific guidelines on code sharing for submissions in which author-generated code underpins the findings in the manuscript. In these cases, we expect all author-generated code to be made available without restrictions upon publication of the work. Please review our guidelines at <ext-link xlink:href="https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code" ext-link-type="uri">https://journals.plos.org/plosone/s/materials-and-software-sharing#loc-sharing-code</ext-link> and ensure that your code is shared in a way that follows best practice and facilitates reproducibility and reuse.</p><p>3. Thank you for stating the following financial disclosure:&#x000a0;</p><p>This research was funded by the National Natural Science Foundation of China (no. 62173171).</p><p>Please state what role the funders took in the study.&#x000a0; If the funders had no role, please state: "The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."&#x000a0;</p><p>If this statement is not correct you must amend it as needed.&#x000a0;</p><p>Please include this amended Role of Funder statement in your cover letter; we will change the online submission form on your behalf.</p><p>4. We note that your Data Availability Statement is currently as follows: All relevant data are within the manuscript and its Supporting Information files.</p><p>Please confirm at this time whether or not your submission contains all raw data required to replicate the results of your study. Authors must share the &#x0201c;minimal data set&#x0201d; for their submission. PLOS defines the minimal data set to consist of the data required to replicate all study findings reported in the article, as well as related metadata and methods (<ext-link xlink:href="https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition)." ext-link-type="uri">https://journals.plos.org/plosone/s/data-availability#loc-minimal-data-set-definition</ext-link>).</p><p>For example, authors should submit the following data:</p><p>- The values behind the means, standard deviations and other measures reported;</p><p>- The values used to build graphs;</p><p>- The points extracted from images for analysis.</p><p>Authors do not need to submit their entire data set if only a portion of the data was used in the reported study.</p><p>If your submission does not contain these data, please either upload them as Supporting Information files or deposit them to a stable, public repository and provide us with the relevant URLs, DOIs, or accession numbers. For a list of recommended repositories, please see <ext-link xlink:href="https://journals.plos.org/plosone/s/recommended-repositories." ext-link-type="uri">https://journals.plos.org/plosone/s/recommended-repositories</ext-link>.</p><p>If there are ethical or legal restrictions on sharing a de-identified data set, please explain them in detail (e.g., data contain potentially sensitive information, data are owned by a third-party organization, etc.) and who has imposed them (e.g., an ethics committee). Please also provide contact information for a data access committee, ethics committee, or other institutional body to which data requests may be sent. If data are owned by a third party, please indicate how others may request data access.</p><p>5. When completing the data availability statement of the submission form, you indicated that you will make your data available on acceptance. We strongly recommend all authors decide on a data sharing plan before acceptance, as the process can be lengthy and hold up publication timelines. Please note that, though access restrictions are acceptable now, your entire data will need to be made freely accessible if your manuscript is accepted for publication. This policy applies to all data except where public deposition would breach compliance with the protocol approved by your research ethics board. If you are unable to adhere to our open data policy, please kindly revise your statement to explain your reasoning and we will seek the editor's input on an exemption. Please be assured that, once you have provided your new statement, the assessment of your exemption will not hold up the peer review process.</p><p>6. We note that Figure 14 in your submission contain satellite images which may be copyrighted. All PLOS content is published under the Creative Commons Attribution License (CC BY 4.0), which means that the manuscript, images, and Supporting Information files will be freely available online, and any third party is permitted to access, download, copy, distribute, and use these materials in any way, even commercially, with proper attribution. For these reasons, we cannot publish previously copyrighted maps or satellite images created using proprietary data, such as Google software (Google Maps, Street View, and Earth). For more information, see our copyright guidelines: <ext-link xlink:href="http://journals.plos.org/plosone/s/licenses-and-copyright." ext-link-type="uri">http://journals.plos.org/plosone/s/licenses-and-copyright</ext-link>.</p><p>We require you to either present written permission from the copyright holder to publish these figures specifically under the CC BY 4.0 license, or remove the figures from your submission:</p><p>a. You may seek permission from the original copyright holder of Figure 14 to publish the content specifically under the CC BY 4.0 license.&#x000a0;&#x000a0;</p><p>We recommend that you contact the original copyright holder with the Content Permission Form (<ext-link xlink:href="http://journals.plos.org/plosone/s/file?id=7c09/content-permission-form.pdf)" ext-link-type="uri">http://journals.plos.org/plosone/s/file?id=7c09/content-permission-form.pdf</ext-link>) and the following text:</p><p>&#x0201c;I request permission for the open-access journal PLOS ONE to publish XXX under the Creative Commons Attribution License (CCAL) CC BY 4.0 (<ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/)." ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>). Please be aware that this license allows unrestricted use and distribution, even commercially, by third parties. Please reply and provide explicit written permission to publish XXX under a CC BY license and complete the attached form.&#x0201d;</p><p>Please upload the completed Content Permission Form or other proof of granted permissions as an "Other" file with your submission.</p><p>In the figure caption of the copyrighted figure, please include the following text: &#x0201c;Reprinted from [ref] under a CC BY license, with permission from [name of publisher], original copyright [original copyright year].&#x0201d;</p><p>b. If you are unable to obtain permission from the original copyright holder to publish these figures under the CC BY 4.0 license or if the copyright holder&#x02019;s requirements are incompatible with the CC BY 4.0 license, please either i) remove the figure or ii) supply a replacement figure that complies with the CC BY 4.0 license. Please check copyright information on all replacement figures and update the figure caption with source information. If applicable, please specify in the figure caption text when a figure is similar but not identical to the original image and is therefore for illustrative purposes only.</p><p>The following resources for replacing copyrighted map figures may be helpful:</p><p>USGS National Map Viewer (public domain): <ext-link xlink:href="http://viewer.nationalmap.gov/viewer/" ext-link-type="uri">http://viewer.nationalmap.gov/viewer/</ext-link></p><p>The Gateway to Astronaut Photography of Earth (public domain): <ext-link xlink:href="http://eol.jsc.nasa.gov/sseop/clickmap/" ext-link-type="uri">http://eol.jsc.nasa.gov/sseop/clickmap/</ext-link></p><p>Maps at the CIA (public domain): <ext-link xlink:href="https://www.cia.gov/library/publications/the-world-factbook/index.html" ext-link-type="uri">https://www.cia.gov/library/publications/the-world-factbook/index.html</ext-link> and <ext-link xlink:href="https://www.cia.gov/library/publications/cia-maps-publications/index.html" ext-link-type="uri">https://www.cia.gov/library/publications/cia-maps-publications/index.html</ext-link></p><p>NASA Earth Observatory (public domain): <ext-link xlink:href="http://earthobservatory.nasa.gov/" ext-link-type="uri">http://earthobservatory.nasa.gov/</ext-link></p><p>Landsat: <ext-link xlink:href="http://landsat.visibleearth.nasa.gov/" ext-link-type="uri">http://landsat.visibleearth.nasa.gov/</ext-link></p><p>USGS EROS (Earth Resources Observatory and Science (EROS) Center) (public domain): <ext-link xlink:href="http://eros.usgs.gov/#" ext-link-type="uri">http://eros.usgs.gov/#</ext-link></p><p>Natural Earth (public domain): <ext-link xlink:href="http://www.naturalearthdata.com/" ext-link-type="uri">http://www.naturalearthdata.com/</ext-link></p><p>
<bold>Additional Editor Comments:</bold>
</p><p>The reviewers have given very detailed revision suggestions, please refer to and improve them carefully, especially in the analysis of the interpretation of the algorithm results. The manuscript needs further improvement.</p><p>[Note: HTML markup is below. Please do not edit.]</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->2. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->3. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->4. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #1:&#x000a0;Yes</p><p>Reviewer #2:&#x000a0;Yes</p><p>Reviewer #3:&#x000a0;No</p><p>**********</p><p><!--<font color="black">-->5. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p><bold>Reviewer #1:&#x000a0;</bold>The paper titled "LI-YOLOv8: Lightweight Small Target Detection Algorithm for Remote Sensing Images that Combines GSConv and PConv" proposes a lightweight small target detection algorithm for remote sensing images based on YOLOv8n, which integrates GSConv and PConv, termed LI-YOLOv8. The algorithm improves recognition performance through modifications such as replacing the activation function in CBS, embedding an efficient multi-scale attention mechanism (EMA), designing a lightweight detection head (GP-Detect head), and refining the boundary fitting loss function of the algorithm.</p><p>While the structure of the paper is generally reasonable, the language lacks conciseness. The limitations of the LI-YOLOv8 model's application scenarios are not discussed, and the use of only two datasets to validate the model's usability is evidently unconvincing.</p><p>Suggestions for Improvement:</p><p>1. Language and Clarity: Simplify the language to make sentences more concise. Some sections lack clear structure, such as overlapping content in the "Methods" and "Algorithm Implementation" sections. Redundant sentences, especially in the background introduction, affect readability. Overly detailed explanations of terms and formulas may confuse non-specialist readers.</p><p>2. Use of Figures and Explanations: There is insufficient explanation and referencing of figures. For example, in figures such as Fig.11 and Fig.12, only the improvement in PR curves is mentioned without detailing the reasons for the changes and their impact on results. It is recommended to provide more analysis and interpretation of the figures in the text.</p><p>3. Description of GP-Detect Module: The explanation of how the GP-Detect module reduces redundant computations is somewhat superficial and lacks in-depth theoretical support.</p><p>4. Ablation Experiments: Provide more quantitative or qualitative discussions on the specific contributions of each module in the ablation experiments. For instance, what are the primary reasons behind the 1.2% improvement attributed to SPPF-R?</p><p>5. Broader Validation: Supplement the study by validating the proposed model and methodology on at least two additional datasets or datasets from related domains to demonstrate the model's generalizability and practical applicability.</p><p>Conclusion:</p><p>The paper holds certain practical value and is recommended for acceptance after revisions.</p><p><bold>Reviewer #2:</bold>&#x000a0;1. LI-YOLOv8 shows significant improvements on remote sensing datasets like RSOD and NWPU VHR-10, how does it perform on more diverse and challenging datasets that are not specifically focused on small object detection? Could there be a risk of overfitting the small object characteristics of these datasets?</p><p>2. How does LI-YOLOv8 perform on real-world, noisy images with significant occlusions, varying lighting, or seasonal changes (detecting small objects in aerial imagery from different times of day or weather conditions)?</p><p>3. The reduction in parameters and GFLOPs is a notable advantage, but how sensitive is the model to changes in hyperparameters like learning rate, batch size, or anchor box configuration? What effect do these parameters have on performance, especially in terms of detection accuracy for small objects?</p><p>4. How does the training time and convergence behavior of LI-YOLOv8 compared to other algorithms like YOLOv8 and YOLOv5? Are there any challenges in training this model on large-scale remote sensing datasets?</p><p>5. Below Table-6 rephrase the sentence ("YOLOv8n's metrics are close to those of YOLOv5n" "YOLOv8n's metrics are similar to those of YOLOv5n.")</p><p>6. While the ablation studies demonstrate improvements when adding different innovations (SPPF-R, C2fE, GP-Detect, Inner-Wise), can you provide more insights into how each innovation specifically addresses challenges in small object detection? Which innovation contributes the most to improving recall or precision?</p><p>7. The experiments are mainly conducted on datasets like RSOD, NWPU VHR-10, and TinyPerson, which focus on specific types of remote sensing imagery. How does LI-YOLOv8 perform on datasets from other domains, such as medical imaging or urban surveillance, where small object detection might have different characteristics?</p><p>8. Does the model's emphasis on small object detection affect its ability to detect large objects, especially when these appear in similar scenes? Could there be a trade-off where performance on larger objects (e.g., buildings, vehicles) is compromised?</p><p>9. In the visual comparison with YOLOv8, LI-YOLOv8 shows fewer false detections in complex backgrounds. However, was there any trade-off between false positives and false negatives in the cases with small targets or occlusions? How does the model handle ambiguous cases?</p><p>10. While mAP@0.5 and mAP@0.5:0.95 are widely used, could there be additional evaluation metrics or domain-specific metrics (F1 score for small objects or intersection over union for very small objects) that would give a clearer picture of the model&#x02019;s real-world performance?</p><p><bold>Reviewer #3:&#x000a0;</bold>1. The manuscript may require some improvement to the English.</p><p>2. From the experimental results presented in Table 1, it can be seen that there is no significant improvement in mAP and other parametersbefore and after using Inner Wise IoU. Similarly, the improvements observed in Table 2 are also insignificant. Is the use of Inner-Wise IoU appropriate?</p><p>3. The manuscript mentioned that LI-YOLOv8 not only detects more small targets in complex backgrounds but also does not incur any false detections.However, the detection accuracy of the vehicle in Table 4 has decreased by 5.2%. Does this contradict the conclusion of the manuscript&#x0fffd;<!--&#x0fffd;--><!--&#x0ff1f;--></p><p>4. Tables 5 and 6 seem to be simple stacks of algorithms. Please explain the significance of comparing YOLO algorithms at different stages and versions?</p><p>5.Line502-Line504: The previous research compared LI-YOLOv8 with YOLO series algorithms, and this paragraph also compared it with R-cnn. What does the author want to express? The logic of the manuscript is a bit confusing.</p><p>It is suggested to revise the manuscript before submitting it</p><p>**********</p><p><!--<font color="black">-->6. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #1:&#x000a0;<bold>Yes:&#x000a0;</bold>Qingfang He</p><p>Reviewer #2:&#x000a0;<bold>Yes:&#x000a0;</bold>Muhammad Wahab Hanif</p><p>Reviewer #3:&#x000a0;No</p><p>**********</p><p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p><p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool,&#x000a0;<ext-link xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at&#x000a0;<email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p><supplementary-material id="pone.0321026.s001" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">review.docx</named-content></p></caption><media xlink:href="pone.0321026.s001.docx"/></supplementary-material></body></sub-article><sub-article article-type="author-comment" id="pone.0321026.r003"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0321026.r003</article-id><title-group><article-title>Author response to Decision Letter 1</article-title></title-group><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0321026" id="rel-obj003" related-article-type="editor-report"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="author-response-date">16 Jan 2025</named-content>
</p><p>Response to Journal Requirements:</p><p>1. We have shared the code, and the address is: <ext-link xlink:href="https://github.com/2470589561/LI-YOLOv8" ext-link-type="uri">https://github.com/2470589561/LI-YOLOv8</ext-link></p><p>2. In our paper, regarding&#x0fffd;<!--&#x0fffd;--><!--&#x0ff1a;-->This research was funded by the National Natural Science Foundation of China (no. 62173171).</p><p>The funding statement needs to be revised to&#x0201d;The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.&#x0201d;</p><p>3. We will share the minimum dataset, the address is: <ext-link xlink:href="https://github.com/2470589561/datasets" ext-link-type="uri">https://github.com/2470589561/datasets</ext-link></p><p>The option that the URL/accession number/DOI will only be available after the manuscript is accepted has been checked.</p><p>4. The 14th figure in the manuscript contains satellite images, which are from the publicly available geospatial object detection dataset NWPU VHR-10 for research purposes. The source of the data is: <ext-link xlink:href="https://gcheng-nwpu.github.io/#Datasets" ext-link-type="uri">https://gcheng-nwpu.github.io/#Datasets</ext-link>, In order to respect the labor achievements and intellectual property rights of the data providers, the relevant papers have been cited in the manuscript. There is no copyright issue.&#x02003;</p><p>Dear reviewers, thank you for your comments. We have completed the revisions to the manuscript according to the reviewer's suggestions.</p><p>The response to reviewers is as follows&#x0fffd;<!--&#x0fffd;--><!--&#x0ff1a;--></p><p>Response to reviewer 1</p><p>1. Language and Clarity: Simplify the language to make sentences more concise. Some sections lack clear structure, such as overlapping content in the "Methods" and "Algorithm Implementation" sections. Redundant sentences, especially in the background introduction, affect readability. Overly detailed explanations of terms and formulas may confuse non-specialist readers.</p><p>Response: The content of the manuscript has been simplified in language and improved in terms of enhancing the coherence between sentences.</p><p>2. Use of Figures and Explanations: There is insufficient explanation and referencing of figures. For example, in figures such as Fig.11 and Fig.12, only the improvement in PR curves is mentioned without detailing the reasons for the changes and their impact on results. It is recommended to provide more analysis and interpretation of the figures in the text.</p><p>Response: Detailed supplementary explanations are provided in Figures 11(a), (b) and Figures 12(a), (b). These not only describe the degree of fluctuation in the PR curve and its causes, but also include a comparative analysis of the two PR graphs (a) and (b), as well as the effects achieved by the four improvement points in the model.</p><p>3. Description of GP-Detect Module: The explanation of how the GP-Detect module reduces redundant computations is somewhat superficial and lacks in-depth theoretical support.</p><p>Response: Supplementary description of GP-Detect.</p><p>4. Ablation Experiments: Provide more quantitative or qualitative discussions on the specific contributions of each module in the ablation experiments. For instance, what are the primary reasons behind the 1.2% improvement attributed to SPPF-R?</p><p>Response: In the ablation experiments, a quantitative and qualitative discussion of the specific contributions of each module has been added.</p><p>5. Broader Validation: Supplement the study by validating the proposed model and methodology on at least two additional datasets or datasets from related domains to demonstrate the model's generalizability and practical applicability.</p><p>Response: Based on the feedback from another reviewer regarding the differing performance of the algorithm in medical imaging and urban surveillance, three datasets (LEVIR-ship, brain-tumor, and smoke_fire_1) have been added in the generalization experiments section to validate the generality and generalization capability of LI-YOLOv8. The LEVIR-ship dataset is a small vessel dataset of remote sensing images, which verifies that the algorithm still has good recognition performance for small targets in the same domain. The brain-tumor dataset is used for detecting brain tumors in medical images, demonstrating the algorithm's versatility and indicating that it performs well with non-specific targets in other fields. The smoke_fire_1 dataset is used for detecting fire targets in urban surveillance, validating the algorithm's generality.</p><p>Response to reviewer 2</p><p>1. LI-YOLOv8 shows significant improvements on remote sensing datasets like RSOD and NWPU VHR-10, how does it perform on more diverse and challenging datasets that are not specifically focused on small object detection? Could there be a risk of overfitting the small object characteristics of these datasets?</p><p>Response: Based on your suggestion, the generalization experiments section has been updated to include the brain tumor detection dataset and the fire smoke monitoring dataset, both of which are non-small target datasets in the areas of medical imaging and urban surveillance. The experimental results indicate that LI-YOLOv8 performs well in detecting non-small targets, and there is no risk of overfitting.</p><p>2. How does LI-YOLOv8 perform on real-world, noisy images with significant occlusions, varying lighting, or seasonal changes (detecting small objects in aerial imagery from different times of day or weather conditions)?</p><p>Response: Our research focuses on small target detection in remote sensing images, which have their own unique characteristics. Remote sensing images are typically high-definition and high-resolution, leading to significant differences in processing these images compared to general noisy images. In remote sensing images, the likelihood of target occlusion is relatively low. Due to the high vantage point from which remote sensing images are captured, there is a reduced probability of targets blocking each other. Regarding lighting conditions, remote sensing images generally have stable lighting, as they are often acquired under specific conditions that provide relatively uniform illumination. Additionally, because remote sensing images cover a wide area, local variations in lighting have a minimal impact on the overall image. Therefore, our study does not include images from multiple time periods or different weather conditions (such as nighttime, rainy days, or snowy days).</p><p>3. The reduction in parameters and GFLOPs is a notable advantage, but how sensitive is the model to changes in hyperparameters like learning rate, batch size, or anchor box configuration? What effect do these parameters have on performance, especially in terms of detection accuracy for small objects?</p><p>Response: Firstly, regarding the anchor box configuration issue you mentioned. In fact, YOLOv8 is an anchor-free detection model. During the detection phase, the model does not rely on predefined anchor boxes to propose candidate object locations; the final object detection is directly based on the detected features. Therefore, in our research, there is no issue of anchor box configuration affecting the model. Secondly, the experimental environment for LI-YOLOv8 is a NVIDIA RTX3090 GPU with 24GB VRAM, 14 vCPUs of Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz, and 80GB RAM. Ablation, comparison, and generalization experiments are conducted on different datasets and models. Model improvement and training must be carried out under the premise of the same configured hyperparameters and batch size. Setting hyperparameters is crucial for shaping model performance and ensuring the successful enhancement of the algorithm. In the process of refining the YOLOv8 model, it is essential to adhere to a unified hyperparameter configuration. This consistency is key; it confirms the effectiveness of model improvements and accelerates the precise performance evaluation conducted before and after the improvements. Adjusting hyperparameters during the algorithm refinement process may obscure the root causes of performance changes&#x02014;whether they are due to inherent algorithm enhancements or the results of hyperparameter changes. Therefore, to determine a clear and rigorous assessment of progress, this paper firmly follows a set of standardized hyperparameters.</p><p>4. How does the training time and convergence behavior of LI-YOLOv8 compared to other algorithms like YOLOv8 and YOLOv5? Are there any challenges in training this model on large-scale remote sensing datasets?</p><p>Response: Thank you to the reviewer for your valuable comments. We have conducted a detailed analysis of the training time and convergence behavior of LI-YOLOv8:</p><p>(1) Training Time and Convergence Behavior:</p><p>Under the same hardware conditions (RTX 3090 with 24GB of video memory), the training time of LI-YOLOv8 is about 15% longer than that of YOLOv8n. This is mainly due to the addition of modules such as RFAConv and EMA attention mechanisms, which increase the computational load per iteration. However, from the analysis of convergence behavior, the loss value of LI-YOLOv8 is lower than that of YOLOv8n. This indicates that although the training time per round is slightly longer, the convergence is better.</p><p>(2) In the training of large-scale remote sensing datasets, there are two main challenges: First, the attention modules consume a large amount of video memory when processing high-resolution images. We alleviated this issue by optimizing the batch size and using mixed-precision training. Second, the features of small targets vary significantly across different scenes. By designing the Inner-Wise IoU loss function, we enhanced the model's ability to learn from samples of varying quality.</p><p>Although there is a slightly longer training time, considering the significant advantages of LI-YOLOv8 in detection accuracy and model lightweighting, we believe this is an acceptable trade-off. The convergence behavior of LI-YOLOv8 is better than that of YOLOv8n, but compared to other algorithms in the YOLO series, its convergence behavior is not the optimal choice. Future work will be dedicated to further optimizing training efficiency to make the model more suitable for large-scale practical applications. In the conclusion section, we have supplemented the expectations and future plans for the model, and further optimization of the model will be carried out in the future.</p><p>5. Below Table-6 rephrase the sentence ("YOLOv8n's metrics are close to those of YOLOv5n" "YOLOv8n's metrics are similar to those of YOLOv5n.")</p><p>Response: The sentence has been revised.</p><p>6. While the ablation studies demonstrate improvements when adding different innovations (SPPF-R, C2fE, GP-Detect, Inner-Wise), can you provide more insights into how each innovation specifically addresses challenges in small object detection? Which innovation contributes the most to improving recall or precision?</p><p>Response: The contributions of each innovation to the model have been added and described in the ablation experiment section.</p><p>7. The experiments are mainly conducted on datasets like RSOD, NWPU VHR-10 and TinyPerson, which focus on specific types of remote sensing imagery. How does LI-YOLOv8 perform on datasets from other domains, such as medical imaging or urban surveillance, where small object detection might have different characteristics?</p><p>Response: In the generalization experiment section, three datasets (LEVIR-ship, brain-tumor, and smoke_fire_1) were added to validate the universality and generalization capability of LI-YOLOv8. Among them, LEVIR-ship is a remote sensing image dataset for small vessels, validating the algorithm's recognition performance for small targets within the same domain. The brain-tumor dataset is for detecting brain tumors in medical images, demonstrating the algorithm's versatility and indicating that it performs well for non-specific targets in other fields. The smoke_fire_1 dataset is used for detecting fire targets in urban surveillance, further validating the algorithm's generality.</p><p>8. Does the model's emphasis on small object detection affect its ability to detect large objects, especially when these appear in similar scenes? Could there be a trade-off where performance on larger objects (e.g., buildings, vehicles) is compromised?</p><p>Response: The model's detection of small objects does not affect its ability to detect large objects. The RSOD dataset contains a large number of small objects, as well as some medium and large objects. LI-YOLOv8 has significantly improved the detection of all four types of objects in the RSOD dataset, which can prove this point. When designing the LI-YOLOv8 model, we did indeed place special emphasis on the detection capability of small objects. This is because in remote sensing images, the detection of small objects (such as small vehicles, boats, etc.) is often more challenging and is of great significance in many application scenarios. However, we also fully considered the detection performance of large objects (such as buildings, large vehicles, etc.) in the model design to ensure the balance of the model in detecting objects of different scales. At the same time, three datasets were added in the generalization experiments to verify the universality and versatility of the algorithm, indicating that the algorithm also has a good improvement in the detection effect of non-specific objects in other fields.</p><p>9. In the visual comparison with YOLOv8, LI-YOLOv8 shows fewer false detections in complex backgrounds. However, was there any trade-off between false positives and false negatives in the cases with small targets or occlusions? How does the model handle ambiguous cases?</p><p>Response: In situations involving small targets or occlusion, the design and optimization of LI-YOLOv8 indeed require a trade-off between false positives and false negatives. Our model addresses this trade-off by incorporating SPPF-R, C2f-E, GP-Detect, and Inner-Wise: SPPF-R enhances small target feature extraction, C2f increases the focus on small targets, GP-Detect provides a lightweight algorithm while maintaining model performance, and Inner-Wise improves generalization capability. The remote sensing image datasets consist of high-resolution images and do not exhibit significant blurriness. LI-YOLOv8 aims to improve the detection and accuracy of small targets as much as possible while remaining lightweight.</p><p>10. While mAP@0.5 and mAP@0.5:0.95 are widely used, could there be additional evaluation metrics or domain-specific metrics (F1 score for small objects or intersection over union for very small objects) that would give a clearer picture of the model&#x02019;s real-world performance?</p><p>Response: F1-score has been introduced as an evaluation metric, and the F1-score experimental results have been added to Tables 1 and 2 in the ablation experiments to validate the model's overall performance.</p><p>Response to reviewer 3</p><p>1. The manuscript may require some improvement to the English.</p><p>Response: The content of the manuscript has been simplified in terms of language, and improvements have been made to enhance the coherence between sentences.</p><p>2. From the experimental results presented in Table 1, it can be seen that there is no significant improvement in mAP and other parameters before and after using Inner Wise IoU. Similarly, the improvements observed in Table 2 are also insignificant. Is the use of Inner-Wise IoU appropriate?</p><p>Response: From the overall experimental design and result analysis of the paper, it is true that the improvement of Inner-Wise IoU is relatively limited in Table 1 and Table 2. However, whether it is "appropriate" cannot be judged solely based on the single numerical increase.</p><p>Firstly, from the stability and robustness of boundary regression, even if some experimental tables show that the improvement of mAP after introducing Inner-Wise IoU is not significant, it may be helpful in the stability of location regression and the recognition of hard - to - detect samples. Our manuscript mentions that Inner-Wise IoU introduces a "dynamic non - monotonic focusing mechanism" and an "auxiliary bounding box" strategy, aiming to provide more reasonable gradient allocation for ordinary quality anchor boxes (not particularly high-quality or low-quality annotations), thereby further stabilizing the regression process in complex backgrounds or when the target scale varies. If we only look at the single mAP value, the increase may not be large; but when considering aspects such as positioning error or robustness performance, such improveme</p><supplementary-material id="pone.0321026.s002" position="float" content-type="local-data"><label>Attachment</label><caption><p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers .docx</named-content></p></caption><media xlink:href="pone.0321026.s002.docx"/></supplementary-material></body></sub-article><sub-article article-type="aggregated-review-documents" id="pone.0321026.r004" specific-use="decision-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0321026.r004</article-id><title-group><article-title>Decision Letter 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Yile</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Yile Chen</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Yile Chen</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0321026" id="rel-obj004" related-article-type="reviewed-article"/><custom-meta-group><custom-meta><meta-name>Submission Version</meta-name><meta-value>1</meta-value></custom-meta></custom-meta-group></front-stub><body><p>
<named-content content-type="letter-date">28 Feb 2025</named-content>
</p><p>LI-YOLOv8: Lightweight Small Target Detection Algorithm for Remote Sensing Images that Combines GSConv and PConv</p><p>PONE-D-24-53882R1</p><p>Dear Dr. Yan,</p><p>We&#x02019;re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p><p>Within one week, you&#x02019;ll receive an e-mail detailing the required amendments. When these have been addressed, you&#x02019;ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p><p>An invoice will be generated when your article is formally accepted. Please note, if your institution has a publishing partnership with PLOS and your article meets the relevant criteria, all or part of your publication costs will be covered. Please make sure your user information is up-to-date by logging into Editorial Manager at <ext-link xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">Editorial Manager&#x000ae;</ext-link>&#x000a0;and clicking the &#x02018;Update My Information' link at the top of the page. If you have any questions relating to publication charges, please contact our Author Billing department directly at authorbilling@plos.org.</p><p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they&#x02019;ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>Kind regards,</p><p>Yile Chen, Ph.D. in Architecture</p><p>Academic Editor</p><p>PLOS ONE</p><p>Additional Editor Comments (optional):</p><p>Reviewers' comments:</p><p>Reviewer's Responses to Questions</p><p>
<!--<font color="black">-->
<bold>Comments to the Author</bold>
</p><p>1. If the authors have adequately addressed your comments raised in a previous round of review and you feel that this manuscript is now acceptable for publication, you may indicate that here to bypass the &#x0201c;Comments to the Author&#x0201d; section, enter your conflict of interest statement in the &#x0201c;Confidential to Editor&#x0201d; section, and submit your "Accept" recommendation.<!--</font>--></p><p>Reviewer #1:&#x000a0;(No Response)</p><p>Reviewer #4:&#x000a0;All comments have been addressed</p><p>**********</p><p><!--<font color="black">-->2. Is the manuscript technically sound, and do the data support the conclusions?</p><p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. <!--</font>--></p><p>Reviewer #1:&#x000a0;(No Response)</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->3. Has the statistical analysis been performed appropriately and rigorously? <!--</font>--></p><p>Reviewer #1:&#x000a0;(No Response)</p><p>Reviewer #4:&#x000a0;No</p><p>**********</p><p><!--<font color="black">-->4. Have the authors made all data underlying the findings in their manuscript fully available?</p><p>The <ext-link xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data&#x02014;e.g. participant privacy or use of data from a third party&#x02014;those must be specified.<!--</font>--></p><p>Reviewer #1:&#x000a0;(No Response)</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->5. Is the manuscript presented in an intelligible fashion and written in standard English?</p><p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.<!--</font>--></p><p>Reviewer #1:&#x000a0;(No Response)</p><p>Reviewer #4:&#x000a0;Yes</p><p>**********</p><p><!--<font color="black">-->6. Review Comments to the Author</p><p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)<!--</font>--></p><p>Reviewer #1:&#x000a0;(No Response)</p><p>Reviewer #4:&#x000a0;Authors have revised paper well, but I still have a suggestion: the inference time should be considered as a additional evaluation metric.</p><p>**********</p><p><!--<font color="black">-->7. PLOS authors have the option to publish the peer review history of their article (<ext-link xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p><p>If you choose &#x0201c;no&#x0201d;, your identity will remain anonymous but your review may still be made public.</p><p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.<!--</font>--></p><p>Reviewer #1:&#x000a0;No</p><p>Reviewer #4:&#x000a0;No</p><p>**********</p></body></sub-article><sub-article article-type="editor-report" id="pone.0321026.r005" specific-use="acceptance-letter"><front-stub><article-id pub-id-type="doi">10.1371/journal.pone.0321026.r005</article-id><title-group><article-title>Acceptance letter</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Yile</given-names></name><role>Academic Editor</role></contrib></contrib-group><permissions><copyright-statement>&#x000a9; 2025 Yile Chen</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Yile Chen</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" xlink:href="10.1371/journal.pone.0321026" id="rel-obj005" related-article-type="reviewed-article"/></front-stub><body><p>PONE-D-24-53882R1</p><p>PLOS ONE</p><p>Dear Dr. Yan,</p><p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now being handed over to our production team.</p><p>At this stage, our production department will prepare your paper for publication. This includes ensuring the following:</p><p>* All references, tables, and figures are properly cited</p><p>* All relevant supporting information is included in the manuscript submission,</p><p>* There are no issues that prevent the paper from being properly typeset</p><p>If revisions are needed, the production department will contact you directly to resolve them. If no revisions are needed, you will receive an email when the publication date has been set. At this time, we do not offer pre-publication proofs to authors during production of the accepted work. Please keep in mind that we are working through a large volume of accepted articles, so please give us a few weeks to review your paper and let you know the next and final steps.</p><p>Lastly, if your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact onepress@plos.org.</p><p>If we can help with anything else, please email us at customercare@plos.org.</p><p>Thank you for submitting your work to PLOS ONE and supporting open access.</p><p>Kind regards,</p><p>PLOS ONE Editorial Office Staff</p><p>on behalf of</p><p>Dr. Yile Chen</p><p>Academic Editor</p><p>PLOS ONE</p></body></sub-article></article>