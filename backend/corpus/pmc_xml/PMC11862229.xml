<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40000712</article-id><article-id pub-id-type="pmc">PMC11862229</article-id>
<article-id pub-id-type="publisher-id">86471</article-id><article-id pub-id-type="doi">10.1038/s41598-025-86471-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Deep learning-based debris flow hazard detection and recognition system: a case study</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4196-0289</contrib-id><name><surname>Wu</surname><given-names>Fei</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5284-2942</contrib-id><name><surname>Zhang</surname><given-names>Jianlin</given-names></name><address><email>jlin@ioe.ac.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6256-7713</contrib-id><name><surname>Liu</surname><given-names>Dunlong</given-names></name><address><email>ldl@cuit.edu.cn</email></address><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9550-5284</contrib-id><name><surname>Maier</surname><given-names>Andreas</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0455-3799</contrib-id><name><surname>Christlein</surname><given-names>Vincent</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05qbk4x57</institution-id><institution-id institution-id-type="GRID">grid.410726.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 1797 8419</institution-id><institution>School of Electrical, Electronics and Communication Engineering, </institution><institution>University of Chinese Academy of Sciences, </institution></institution-wrap>Beijing, 100049 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02bn68w95</institution-id><institution-id institution-id-type="GRID">grid.458437.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 0644 7356</institution-id><institution>National Key Laboratory of Optical Field Manipulation Science and Technology, Key Laboratory of Optical Engineering, </institution><institution>Institute of Optics and Electronics, Chinese Academy of Sciences, </institution></institution-wrap>Chengdu, 610209 China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00f7hpc57</institution-id><institution-id institution-id-type="GRID">grid.5330.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 3311</institution-id><institution>Pattern Recognition Lab, Department of Computer Science, </institution><institution>Friedrich-Alexander-Universit&#x000e4;t Erlangen-N&#x000fc;rnberg, </institution></institution-wrap>91058 Erlangen, Germany </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01yxwrh59</institution-id><institution-id institution-id-type="GRID">grid.411307.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 1790 5236</institution-id><institution>Software Automatic Generation and Intelligent Service Key Laboratory of Sichuan Province, </institution><institution>Chengdu University of Information and Technology, </institution></institution-wrap>Chengdu, 610225 China </aff></contrib-group><pub-date pub-type="epub"><day>25</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="pmc-release"><day>25</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><elocation-id>6789</elocation-id><history><date date-type="received"><day>22</day><month>8</month><year>2024</year></date><date date-type="accepted"><day>10</day><month>1</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Debris flows are characterized by their suddenness, rapidity, large scale and destructive power, causing serious threat to the population in mountainous areas. Surveillance cameras are widely used in geological hazard monitoring and early warning projects. So far, video cameras are used as a passive tool for post inspection and not as an active role for debris flow monitoring and early warning. Inspired by recent developments of anomaly detection in the field of computer vision, in this paper, we propose a novel automatic debris flow detection and recognition system based on deep learning. It consists of a video feature extraction network using a 3D convolutional neural network (CNN), a debris flow hazard detection network using a multi-layer perceptron (MLP), and a debris flow hazard recognition network for verification employing another CNN. The proposed system takes the video sequences captured by the cameras as inputs and enables the detection and recognition of debris flow hazards. All the networks are optimized and evaluated on a newly annotated image dataset called Debrisflow23. Extensive experimental evaluations with a detection accuracy of 86.3&#x000a0;% AUC, a recognition accuracy of 83.7&#x000a0;% AUC, and an overall identification accuracy of 88.1&#x000a0;% AUC on the test dataset demonstrate that the proposed method possesses accurate and reliable debris flow warning capability. Thus, further precautions can be taken in advance to reduce the damage to human settlements and infrastructure caused by debris flows.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Transfer learning</kwd><kwd>Convolutional neural network</kwd><kwd>Debris flow</kwd><kwd>Hazard detection and recognition</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Natural hazards</kwd><kwd>Computer science</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>42302336</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Dunlong</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004829</institution-id><institution>Science and Technology Department of Sichuan Province</institution></institution-wrap></funding-source><award-id>2024YFHZ0098</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Dunlong</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Software Engineering Technology Research Support Center of Informatization Application of Sichuan</institution></funding-source><award-id>760115027</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Dunlong</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Debris flows are common and destructive mountain disasters, which contain a fluid mixture of loose muds, silts, soils, rocks, and water that flow downstream at high speed<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR3">3</xref></sup>. They occur suddenly and are difficult to ward off, so that people in the mountain regions have suffered great loss of life and property<sup><xref ref-type="bibr" rid="CR4">4</xref>&#x02013;<xref ref-type="bibr" rid="CR6">6</xref></sup>. The extent and frequency of debris flows have increased in recent years, causing devastating damage in the affected areas<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. The scale of debris flow disasters in China is large<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, making it impossible to provide comprehensive engineering treatment. Therefore, as an important non-engineer measure for disaster reduction and an effective measure for debris flow disaster prevention, debris flow monitoring and early warning technology is highly valued by academic and engineering domains<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>.</p><p id="Par3">Most of the current debris flow monitoring and early warning technologies are based on natural indicators, such as rainfall, soil moisture content, infrasound, earth sound or mud water level<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR10">10</xref>&#x02013;<xref ref-type="bibr" rid="CR13">13</xref></sup>. The movement of debris flow masses can produce corresponding ground vibration and infrasound waves and are thus a major technique in debris flow monitoring process<sup><xref ref-type="bibr" rid="CR14">14</xref>&#x02013;<xref ref-type="bibr" rid="CR21">21</xref></sup>. Zhang and Yu<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup> developed a debris flow infrasound monitoring equipment for debris flow observation in Jiangjia gully, China. It can be used as an early warning system for debris flows by integrating rain gauges, image transmission and other functions. Kogelnig et al.<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup> compared and verified the infrasound signal of debris flows from different flow depth and corresponding ground sound. They pointed out that the joint application of infrasound and ground sound has great potential for monitoring debris flows. Li et al.<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> developed a debris flow infrasound monitoring device to observe the dynamic changes of debris flows along a railroad line in China. Later, Liu et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> analyzed the characteristic difference between debris flow infrasound and environmental interference infrasound, and proposed a recognition system for debris flow infrasound by adjusting the threshold of characteristic parameters. Liu et al.<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> established a sound source locative model based on an infrasound monitoring array and a cross-correlation time delay estimation algorithm. They realized a real-time debris flow movement localization and tracking based on a GIS platform. Based on the KNN algorithm, Liu et al.<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> proposed an automatic signal recognition model for analyzing the key characteristics of landslide infrasound and various common environmental interference infrasound from the time and frequency domains, respectively. Conversely, Zhang et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> proposed a debris flow I-D threshold curve for plotting the relationship between rainfall parameters and debris flow density based on a physical model of debris flow formation mechanism and fluid properties. It can provide real-time debris flow warnings based on the monitored rainfall data. Kai-heng and Chao<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> established a debris flow early warning method by constructing the empirical relationship among the critical soil moisture content, the soil permeability coefficient, and the porosity and particle curvature coefficients. Concurrently, Zhao et al.<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> utilized rainfall gauge, mud level gauge, infrasound alarm instrument and video monitoring equipment to formulate warning thresholds and determine warning levels. Xie et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> selected daily maximum temperature, daily rainfall, mud water level, surface displacement, and water content as debris flow monitoring and early warning indicators. By hierarchically weighting these indicators, they proposed a glacier rainfall debris flow warning model based on the excitation conditions and the stability of the accumulation body.</p><p id="Par4">However, sensor alarm systems comprise a complex set of equipment that is costly to install, operate and maintain, resulting in a lack of widespread application in many undeveloped areas of the world<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. In contrast, an increasing number of video cameras are being installed along the torrents that are prone to debris flows. However, they can only be used as a passive tool for retrospective inspection to capture, verify, and record the course of disasters, losing the role of real-time monitoring and early warning. Therefore, effective utilization of debris flow events captured by video cameras can provide valuable information for debris flow detection and monitoring. Several works used video cameras for assessing the movement of debris flows. Uddin et al.<sup><xref ref-type="bibr" rid="CR32">32</xref>&#x02013;<xref ref-type="bibr" rid="CR34">34</xref></sup> developed computer-based spatial-filtering velocimeters to measure the surface velocity of natural debris flows from Mt. Yakedake Volcano while Arattano and Marchi<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> proposed an image processing technique to calculate velocity distribution of a debris flow event in Moscardo. Nevertheless, these methods only analyzed the debris flow velocity distribution, and thus cannot achieve recognition and early warning from the appearance of debris flow images acquired by the video cameras.</p><p id="Par5">In recent years, with the development of computer vision technology, various deep learning methods based on digital images have been proposed for applications in the field of disaster response. Rahnemoonfar et al.<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> proposed an end to end densely connected convolutional neural network (CNN) and recurrent neural network (RNN) model for detecting flooded areas in unmanned aerial vehicles (UAV) aerial images. Based on transfer learning, Pi et al.<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> evaluated eight You-Only-Look-Once (YOLO) models<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> for detecting post-disaster ground objects from aerial views. Similarly, Pham and Kim<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> used an advanced YOLO network YOLOv4<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> for debris flow detection and identification while Ji et al.<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> equipped a residual neural network (ResNet-50) model<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> with a novel attention module for landslides detection from satellite images. So far, these existing CNN-based methods have succeeded in accurately detecting objects from the images of cameras, UAVs, and drones. However, to the best of our knowledge, there is no previous research reported on using continuous surveillance video sequences for monitoring and early warning of debris flow hazards. As discussed above, Pham and Kim<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> used object detection model YOLOv4<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> to directly detect debris flow hazards with rectangular bounding boxes on the debris flow images, while our goal is to predict the debris flow motion tendency based on inter-frame context information of consecutive video frames, and detail the extent of debris flow hazards.</p><p id="Par6">On the other hand, anomaly detection is one of the most challenging and perennial problems in computer vision, e.&#x000a0;g.,&#x000a0; detecting violence or aggression in video sequences<sup><xref ref-type="bibr" rid="CR42">42</xref>&#x02013;<xref ref-type="bibr" rid="CR48">48</xref></sup>. Datta et al.<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> proposed to detect human violence by exploiting the trend of human movement and limb direction. Kooij et al.<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> analyzed video and audio data to detect attacks in surveillance videos while Gao et al.<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> proposed violent flow descriptors to detect human violence. At the same time, Mohammadi et al.<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> proposed a new classification method to identify violent or nonviolent videos based on behavioral heuristics. More recently, Sultani et al.<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> proposed an anomaly detection framework to detect video segment level anomaly based on deep learning, which arouses lots of attention from researchers<sup><xref ref-type="bibr" rid="CR54">54</xref>&#x02013;<xref ref-type="bibr" rid="CR57">57</xref></sup>.</p><p id="Par7">The aim of this work is to propose a new method for detecting debris flow hazards and to further verify the occurrence of debris flows based on the video sequence inputs recorded by cameras. We built a unified debris flow detection and recognition system based on a combination of a video feature extraction network, a debris flow hazard detection network, and a debris flow hazard recognition network. The first step in debris flow hazard detection is the conversion of the captured video clips into corresponding anomaly scores. Then, an additional hazard identification process is designed to scrutinize the results of the debris flow detection based on pre-determined hazard thresholds. As a consequence, the proposed system provides accurate and reliable identification and early warning for debris flow hazards. Additionally, realistic natural videos acquired from fixed cameras at different sampling sites in China were collected and annotated to optimize and evaluate the deep learning models within the proposed system. Combined with state-of-the-art computer vision techniques, we believe that this study can provide new inspirations for debris flow detection and identification in early warning and monitoring systems.</p><p id="Par8">The rest of the paper is organized as follows: &#x0201c;Methodology&#x0201d; demonstrates the details of the proposed method by elaborating the datasets, network architectures, and component structures. &#x0201c;Experiments&#x0201d; shows the experimental setups and the evaluation results for the proposed method, while its limitations and future works are discussed in &#x0201c;Discussion&#x0201d;. Finally, we conclude the article in &#x0201c;Conclusion&#x0201d;.<fig id="Fig1"><label>Fig. 1</label><caption><p>The overall methodology of the proposed method. It consists of a video feature extraction branch, a debris flow hazard detection branch and a debris flow hazard recognition branch.</p></caption><graphic xlink:href="41598_2025_86471_Fig1_HTML" id="MO1"/></fig></p></sec><sec id="Sec2"><title>Methodology</title><p id="Par9">In this section, we present the deep learning-based debris flow detection and recognition system. The collected data used in this work are presented in &#x0201c;Dataset&#x0201d;. The overall methodology of this work can be seen in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The system consists of a video feature extraction branch (&#x0201c;Transfer learning&#x0201d;), a debris flow hazard detection branch (&#x0201c;Debris flow hazard detection network&#x0201d;) and a debris flow hazard recognition branch (&#x0201c;Debris flow hazard recognition network&#x0201d;). The network architecture and hyper-parameter settings of each subbranch are detailed in the corresponding subsections.<fig id="Fig2"><label>Fig. 2</label><caption><p>Cameras and signal preprocessing and transmission equipment were installed to monitor debris flow hazards at the sampling sites in China.</p></caption><graphic xlink:href="41598_2025_86471_Fig2_HTML" id="MO2"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><p>The 7&#x000a0;sampling sites for monitoring debris flow hazards in China.</p></caption><graphic xlink:href="41598_2025_86471_Fig3_HTML" id="MO3"/></fig></p><sec id="Sec3"><title>Dataset</title><p id="Par10">To facilitate the understanding of the subsequent experiments, we first give a summary of the collected dataset used in this work. As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, cameras with signal preprocessing and transmission equipment were installed at 7 sampling sites in China that are vulnerable to debris flow hazards (Peilong Gully, Tianmo Gully and Guxiang Gully of Tibet, Jiangjia Gully of Yunnan, and Nadi Gully, Zhongjijie Gully and Maidi Gully of Sichuan). Figure <xref rid="Fig3" ref-type="fig">3</xref> illustrates the locations of these sites. We collected 12 debris flow hazard videos and 12 normal natural videos from these sampling sites. To further extend the available data, we searched and collected 11 additional debris flow hazard videos and 11 normal natural videos from the Internet. All these collected videos were mixed and then annotated with image-level labels: debris flow vs. normal natural. Except for the debris flow images in debris flow hazard videos, the rest are normal natural images. The resulting image dataset is named Debrisflow23. With a random selection scheme, we split the Debrisflow23 dataset into a training set (12 debris flow hazard videos and 12 normal natural videos), a validation set (4 debris flow hazard videos and 4 normal natural videos), and a test set (7 debris flow hazard videos and 7 normal natural videos). Table <xref rid="Tab1" ref-type="table">1</xref> summarizes the detailed properties of the Debrisflow23 dataset used in this work.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Debrisflow23 dataset statistics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Video label</th><th align="left">Number</th><th align="left">Total frames</th><th align="left">Debris flow frames</th></tr></thead><tbody><tr><td align="left" rowspan="2">Training</td><td align="left">Debris flow</td><td align="left">12</td><td align="left">18960</td><td align="left">13697</td></tr><tr><td align="left">Normal natural</td><td align="left">12</td><td align="left">21552</td><td align="left">0</td></tr><tr><td align="left" rowspan="2">Validation</td><td align="left">Debris flow</td><td align="left">4</td><td align="left">9072</td><td align="left">4437</td></tr><tr><td align="left">Normal natural</td><td align="left">4</td><td align="left">7648</td><td align="left">0</td></tr><tr><td align="left" rowspan="2">Test</td><td align="left">Debris flow</td><td align="left">7</td><td align="left">13837</td><td align="left">8993</td></tr><tr><td align="left">Normal natural</td><td align="left">7</td><td align="left">12431</td><td align="left">0</td></tr><tr><td align="left" rowspan="2">Total</td><td align="left">Debris flow</td><td align="left">23</td><td align="left">41869</td><td align="left">27127</td></tr><tr><td align="left">Normal natural</td><td align="left">23</td><td align="left">41631</td><td align="left">0</td></tr></tbody></table></table-wrap></p><p id="Par11">
<fig id="Fig4"><label>Fig. 4</label><caption><p>A&#x000a0;3D convolutional operation<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>, H, W, D denotes the height, width and depth of the video data cube, d denotes kernel temporal depth and k denotes kernel spatial size.</p></caption><graphic xlink:href="41598_2025_86471_Fig4_HTML" id="MO4"/></fig>
<fig id="Fig5"><label>Fig. 5</label><caption><p>The C3D network architecture<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> used for transfer learning.</p></caption><graphic xlink:href="41598_2025_86471_Fig5_HTML" id="MO5"/></fig>
</p></sec><sec id="Sec4"><title>Transfer learning</title><p id="Par12">As the first step towards monitoring and early warning of debris flow hazards from 3D surveillance videos, a video feature extraction network is required. The C3D network<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> was a representative method proposed for video feature learning. It uses a 3D convolutional neural network for feature extraction on video sequence images. A 3D convolutional operation<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>, as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, is a key design for extracting 3D feature of video sequences. That means multiple continuous images of the sequence in the video are stacked to form a cube, while the 3D convolutional kernel is used to perform sliding window convolution in the height, width and depth direction of this cube to generate 3D feature maps. In contrast to standard 2D convolution<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>, the kernel depth of 3D convolution is smaller than the number of cube channels (<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {d&#x0003c;D}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq1.gif"/></alternatives></inline-formula>), thus the 3D convolutional kernel can convolve the video cube in the depth direction. As a result, the motion information along the time direction of the stacked images cube can be captured, resulting in various successful applications in computer vision tasks, such as action recognition<sup><xref ref-type="bibr" rid="CR60">60</xref>,<xref ref-type="bibr" rid="CR61">61</xref></sup>, 3D medical segmentation<sup><xref ref-type="bibr" rid="CR62">62</xref>,<xref ref-type="bibr" rid="CR63">63</xref></sup>, etc.</p><p id="Par13">To adapt C3D<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> to our debris flow hazard detection task, transfer learning is applied to the original C3D network<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. The architecture of C3D network is illustrated in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. We follow the original hyperparameter settings of C3D to fine-tune the pretrained C3D model with the video-level-label-based training set of Debrisflow23 (&#x0201c;Dataset&#x0201d;), the resulting model, named H-C3D, is used to extract 3D features of video sequences used in this work.<fig id="Fig6"><label>Fig. 6</label><caption><p>Overview of the combination of the trimmed H-C3D (green box) and the debris flow hazard detection network (blue box). The debris flow hazard detection network takes the 3D features of the video clips generated by H-C3D as input and outputs the corresponding 0 to 1 anomaly score.</p></caption><graphic xlink:href="41598_2025_86471_Fig6_HTML" id="MO6"/></fig></p></sec><sec id="Sec5"><title>Debris flow hazard detection network</title><p id="Par14">Inspired by the recent developments of abnormal behavior detection in computer vision<sup><xref ref-type="bibr" rid="CR53">53</xref>&#x02013;<xref ref-type="bibr" rid="CR58">58</xref></sup>, we design a debris flow hazard detection network to detect the debris flow hazard from the obtained 3D features of H-C3D in consecutive video frames. The aim of the debris flow hazard detection network is to use the 3D features generated by H-C3D of corresponding video clips as input and output anomaly scores w.&#x000a0;r.&#x000a0;t. the 3D features, thus enabling an initial detection of debris flow hazards.</p><p id="Par15">The overview of the combination of the trimmed H-C3D and the debris flow hazard detection network is shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. In particular, H-C3D takes a 16-frame video clip as input and outputs the corresponding 3D feature from the first fully connected layer (FC6). The FC6 can generate a 4096-dimensional feature vector which represents high-level semantic information. Each video in the training dataset is sequentially divided into 16 consecutive image frames to produce an <inline-formula id="IEq2"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N\times 4096$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq2.gif"/></alternatives></inline-formula> dimensional feature vectors through the H-C3D network, where N denotes the maximum value of the total number of corresponding video frames divided by 16. Afterwards, these feature vectors are recombined and sorted into a <inline-formula id="IEq3"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$48\times 4096$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq3.gif"/></alternatives></inline-formula> dimensional feature vector in a chronological order. The debris flow hazard detection network adopts a five-layer fully connected neural network aiming to transform the 4096-dimensional 3D vector into a 1-dimensional 0 to 1 anomaly score which is obtained by a Sigmoid activation function. All other layers employ ReLU activation and&#x000a0;50&#x000a0;% dropout for non-linearity and regularization, respectively. In this way, each 3D feature generates an anomaly score ranging from 0 to 1, resulting in a total of <inline-formula id="IEq4"><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$48\times 1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq4.gif"/></alternatives></inline-formula> dimensional score sequences corresponding to the input video.</p><p id="Par16">To smooth the anomaly response curve while reducing false alarms in the early warning of debris flow disaster, we specifically designed a loss function based on multiple instance learning (MIL) for optimizing the debris flow hazard detection network. The main idea of this design is as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \mathop {\textrm{max}}\limits _{{i\in {B_{d}}}}\varphi {(V_{d}^{i})}-\mathop {\textrm{min}}\limits _{i\in {B_{d}}}\varphi {(V_{d}^{i})}&#x0003e;\mathop {\textrm{max}}\limits _{j\in {B_{n}}}\varphi {(V_{n}^{j})}-\mathop {\textrm{min}}\limits _{j\in {B_{n}}}\varphi {(V_{n}^{j})} \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_86471_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq5"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi (*)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq5.gif"/></alternatives></inline-formula> represents the debris flow hazard detection network embedding. <inline-formula id="IEq6"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$B_{d}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$B_{n}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq7.gif"/></alternatives></inline-formula> represent the generated video clips (instances) from all the debris flow hazard videos and all the normal natural videos, respectively. <inline-formula id="IEq8"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V_{d}^{i}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq8.gif"/></alternatives></inline-formula> and <inline-formula id="IEq9"><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V_{n}^{j}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq9.gif"/></alternatives></inline-formula> represent the <inline-formula id="IEq10"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i^\text {th}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq10.gif"/></alternatives></inline-formula> and <inline-formula id="IEq11"><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j^\text {th}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq11.gif"/></alternatives></inline-formula> video sequence from debris flow hazard videos and normal natural videos, respectively. Equation (<xref rid="Equ1" ref-type="disp-formula">1</xref>) aims to make the difference between the maximum and the minimum anomaly score of debris flow hazard videos larger than the difference between the maximum and the minimum score of normal natural videos. This can be formulated into the following loss function:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathscr {L}}_1 = \textrm{max}\{0, 1-\big (\mathop {\textrm{max}}\limits _{i\in {B_{d}}}\varphi {(V_{d}^{i})}-\mathop {\textrm{min}}\limits _{i\in {B_{d}}}\varphi {(V_{d}^{i})} -(\mathop {\textrm{max}}\limits _{j\in {B_{n}}}\varphi {(V_{n}^{j})}-\mathop {\textrm{min}}\limits _{j\in {B_{n}}}\varphi {(V_{n}^{j})})\big )\} . \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_86471_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>Similar to previous anomaly detection works<sup><xref ref-type="bibr" rid="CR53">53</xref>,<xref ref-type="bibr" rid="CR56">56</xref>,<xref ref-type="bibr" rid="CR57">57</xref></sup>, temporal smoothness and sparsity smoothness constraints are added to form the final loss function. The temporal smoothness constraint aims to minimize the response differences between the temporally adjacent debris flow hazard video segments:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathscr {L}}_2 = \sum _{k=1}^{47}(\varphi {(V_{d}^{i^{k}})-\varphi {(V_{d}^{i^{k+1}})}})^2 , \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_86471_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq12"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi {(V_{d}^{i^{k}})}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq12.gif"/></alternatives></inline-formula> indicates the <inline-formula id="IEq13"><alternatives><tex-math id="M16">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k^\text {th}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq13.gif"/></alternatives></inline-formula> instance score of <inline-formula id="IEq14"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i^\text {th}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq14.gif"/></alternatives></inline-formula> video sequence from debris flow hazard videos. The Sparsity smoothness constraint aims to minimize the response of each debris flow hazard video segments as follows:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M18">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathscr {L}}_3 = \sum _{k=1}^{48}\varphi {(V_{d}^{i^{k}})} . \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_86471_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>Eventually, the objective loss function of the debris flow hazard detection network can be summarized as:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathscr {L}}_{\text {final}} = {\mathscr {L}}_1 + \lambda _t \,{\mathscr {L}}_2 + \lambda _s \, {\mathscr {L}}_3 , \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_86471_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq15"><alternatives><tex-math id="M20">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _t,\lambda _s$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq15.gif"/></alternatives></inline-formula> are trade-off hyper-parameters that represent temporal (<italic>t</italic>) and spatial (<italic>s</italic>) smoothness constraints, respectively. In this way, the loss function makes the maximum anomaly score of debris flow hazard video segments to be 1, while suppressing the maximum anomaly score of normal natural video segments to be 0.<fig id="Fig7"><label>Fig. 7</label><caption><p>Architectural details of the debris flow hazard recognition network.</p></caption><graphic xlink:href="41598_2025_86471_Fig7_HTML" id="MO7"/></fig></p></sec><sec id="Sec6"><title>Debris flow hazard recognition network</title><p id="Par17">The combination of the H-C3D and the debris flow hazard detection network predicts the debris flow motion tendency based on inter-frame context information of consecutive video frames from a video-level perspective. To further check the potential false alarms associated with high anomaly scores in some noise cases, the initial debris flow detection results generated from the debris flow hazard detection network are re-evaluated. In particular, after obtaining the initial debris flow detection results, a debris flow hazard recognition network is designed to verify the occurrence of a debris flow hazard disaster from an image-level perspective, thereby reducing the false alarm rate. The architectural details of the debris flow hazard recognition network is shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. It adopts five convolutional layers, two fully connected layers and a final linear layer with Sigmoid activation. The image-level samples with a size of 233 <inline-formula id="IEq16"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq16.gif"/></alternatives></inline-formula> 233 pixels from the debris flow hazard videos and normal natural videos of the Debrisflow23 training set (Section 2.1) are used as input for the debris flow hazard recognition network. While the binary cross entropy (BCE) loss is used for network optimization. During inference, we first set up a threshold (empirically set to <inline-formula id="IEq17"><alternatives><tex-math id="M22">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta =0.6$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq17.gif"/></alternatives></inline-formula>) based on the anomaly score <inline-formula id="IEq18"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\delta \in [0,1]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq18.gif"/></alternatives></inline-formula> generated by the debris flow hazard detection network to identify the initial detection of a debris flow hazard. The verification process is then conducted on this initial debris flow hazard detection result, which can be represented as follows:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M24">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Recognition result}= {\left\{ \begin{array}{ll} \text {Abnormal}\quad \delta&#x0003e;\theta \quad {\left\{ \begin{array}{ll} \text {Debris flow}\quad &#x00026; \phi (I)&#x0003e;0.5 \\ \text {False alarm}\quad &#x00026; \text {otherwise} \end{array}\right. } \\ \text {Normal}\quad \text {otherwise} \end{array}\right. } \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_86471_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq19"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi (I)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq19.gif"/></alternatives></inline-formula> represents the debris flow hazard recognition network embedding <inline-formula id="IEq20"><alternatives><tex-math id="M26">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi (*)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq20.gif"/></alternatives></inline-formula> for the image-level input <italic>I</italic>. The verification process makes the final identification of the proposed system to be either a debris flow hazard or a normal case (normal event and false alarm).<table-wrap id="Tab2"><label>Table 2</label><caption><p>Model optimization settings for the proposed method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Network</th><th align="left">Optimizer</th><th align="left">Loss function</th><th align="left">Batch size</th><th align="left">Iteration</th><th align="left">Initial learning rate</th></tr></thead><tbody><tr><td align="left">#&#x000a0;1</td><td align="left">SGD</td><td align="left">Softmax loss</td><td align="left">50</td><td align="left">20000</td><td align="left">0.001</td></tr><tr><td align="left">#&#x000a0;2</td><td align="left">Adagrad</td><td align="left">Equation (<xref rid="Equ5" ref-type="disp-formula">5</xref>)</td><td align="left">24</td><td align="left">20000</td><td align="left">0.01</td></tr><tr><td align="left">#&#x000a0;3</td><td align="left">Adagrad</td><td align="left">BCE loss</td><td align="left">128</td><td align="left">8000</td><td align="left">0.01</td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec7"><title>Experiments</title><p id="Par18">In this section, we present the implementation details of our experimental setup as well as analyzing the experimental results in both quantitative and qualitative perspectives.</p><sec id="Sec8"><title>Implementation details</title><p id="Par19">The proposed debris flow hazard detection and recognition system is implemented on a PC equipped with an Intel Core i7-9700k@3.6 GHz CPU and a single Nvidia GeForce RTX 2070SUPER GPU. The user interface is developed using Python 3.7 PyQt5. The hyperparameter settings for model optimization of the video feature extraction network (#&#x000a0;1), debris flow hazard detection network (#&#x000a0;2), and debris flow hazard recognition network (#&#x000a0;3) are listed in Table <xref rid="Tab2" ref-type="table">2</xref>. The <inline-formula id="IEq21"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda _t,\lambda _s$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq21.gif"/></alternatives></inline-formula> in Eq. (<xref rid="Equ5" ref-type="disp-formula">5</xref>) are both set to 8e-5. During training, the validation set of Debrisflow23 (Section 2.1) is used to validate the model performance and define the best model.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Confusion matrix.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left" colspan="2">Prediction</th></tr><tr><th align="left"/><th align="left"/><th align="left">Predicted positive</th><th align="left">Predicted negative</th></tr></thead><tbody><tr><td align="left" rowspan="2">Ground truth</td><td align="left">Positive</td><td align="left">True positives (TP)</td><td align="left">False negatives (FN)</td></tr><tr><td align="left">Negative</td><td align="left">False positives (FP)</td><td align="left">True negatives (TN)</td></tr></tbody></table></table-wrap></p><p id="Par20">
<fig id="Fig8"><label>Fig. 8</label><caption><p>ROC curves of the debris flow hazard detection network&#x000a0;(<bold>a</bold>) and debris flow hazard recognition network&#x000a0;(<bold>b</bold>).</p></caption><graphic xlink:href="41598_2025_86471_Fig8_HTML" id="MO8"/></fig>
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Effectiveness and efficiency evaluation for the proposed method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Network</th><th align="left">Model size</th><th align="left">Speed (FPS)</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left">#&#x000a0;1</td><td align="left">312.0M</td><td align="left">113</td><td align="left">N/A</td></tr><tr><td align="left">#&#x000a0;2</td><td align="left">18.9M</td><td align="left">849</td><td align="left">0.863</td></tr><tr><td align="left">#&#x000a0;3</td><td align="left">367.9M</td><td align="left">415</td><td align="left">0.837</td></tr><tr><td align="left">#&#x000a0;4</td><td align="left">698.8M</td><td align="left">68</td><td align="left">0.881</td></tr></tbody></table></table-wrap>
</p><p id="Par21">
<fig id="Fig9"><label>Fig. 9</label><caption><p>Qualitative results of the proposed method on several test videos (best viewed zoomed in).</p></caption><graphic xlink:href="41598_2025_86471_Fig9_HTML" id="MO9"/></fig>
<fig id="Fig10"><label>Fig. 10</label><caption><p>Failure cases of the debris flow hazard detection network with false alarms (<bold>a</bold>) and wrong detections (<bold>b</bold>). The yellow dotted boxes indicate that relying solely on the detection network is unable to identify the debris flow hazard correctly (best viewed zoomed in).</p></caption><graphic xlink:href="41598_2025_86471_Fig10_HTML" id="MO10"/></fig>
</p></sec><sec id="Sec9"><title>Quantitative results</title><p id="Par22">To consistently evaluate the debris flow detection and recognition performance from the debris flow hazard detection network and debris flow hazard recognition network, a frame-based receiver operating characteristics (ROC) curve is used as evaluation criterion. The ROC curve is a widely applied standard for visualizing the trade-offs between sensitivity and specificity in a binary classifier with the corresponding area under the curve (AUC) including all the possible decision thresholds from a diagnostic test result. The ROC criterion consists of true positive rate (TPR) and false positive rate (FPR). TPR and FPR are defined as follows:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M28">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {TPR} =\mathrm {\frac{TP}{TP+FN}} \quad \text {and} \quad \text {FPR} =\mathrm {\frac{FP}{FP+TN}} \;, \end{aligned}$$\end{document}</tex-math><graphic xlink:href="41598_2025_86471_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where TP, FP, TN, FN are defined from the confusion matrix, as shown in Table <xref rid="Tab3" ref-type="table">3</xref>.</p><p id="Par23">To the best of our knowledge, there is no previous research reported on directly using continuous surveillance video sequences for monitoring and early warning of debris flow hazards. Therefore, we compare our method with a binary classifier. The experimental results are performed on the test set of Debrisflow23 (Section 2.1). The ROC curves of the debris flow hazard detection network and debris flow hazard recognition network are shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. The debris flow hazard detection network achieves 0.863 AUC (Fig. <xref rid="Fig8" ref-type="fig">8</xref>a) which outperforms the binary classifier by&#x000a0;36.3&#x000a0;%. We can see the apex of the ROC curve appears at a FPR of 0.3, which is consistent with the analysis of the qualitative experimental results (&#x0201c;Qualitative results&#x0201d;), indicating the network&#x02019;s accurate and stable debris flow detection capability due to the strongly suppressed false negatives based on the customized loss function (Eq. <xref rid="Equ5" ref-type="disp-formula">5</xref>). Figure <xref rid="Fig8" ref-type="fig">8</xref>b shows the ROC curve of the debris flow hazard recognition network. It achieves an AUC of 0.837 which outperforms the binary classifier by&#x000a0;33.7&#x000a0;%, making the verification process of potential debris flow hazard false alarms reliable. Note that the performance of the above two models are separately evaluated on the test set, i. e., without the definition of threshold &#x003b8; in Eq. (6).&#x000a0;More importantly, Table <xref rid="Tab4" ref-type="table">4</xref> presents a comprehensive effectiveness and efficiency evaluation for each component of the proposed system, i.&#x000a0;e.,&#x000a0; the video feature extraction network (#&#x000a0;1), debris flow hazard detection network (#&#x000a0;2), debris flow hazard recognition network (#&#x000a0;3), as well as the complete system (#&#x000a0;4) which involves #&#x000a0;1, #&#x000a0;2, and #&#x000a0;3. We can see that #&#x000a0;2 is the lightest model compared to #&#x000a0;1 and #&#x000a0;3, while the 3D feature extraction process of #&#x000a0;1 occupies most of the resource and reduces the efficiency of systematic operation. The overall system (#&#x000a0;4) achieves an AUC of 0.881 with the speed of 68 FPS, outperforming the AUC of #&#x000a0;2 and #&#x000a0;3 by&#x000a0;1.8&#x000a0;% and&#x000a0;4.4&#x000a0;%, respectively. It reveals that the combination of the video-level-based debris flow hazard detection network and the image-level-based debris flow hazard recognition network improves performance effectively and enhances the system robustness.</p></sec><sec id="Sec10"><title>Qualitative results</title><p id="Par24">In Fig. <xref rid="Fig9" ref-type="fig">9</xref>, we present several qualitative inference results of the proposed system. Intuitively, it can be observed that our method reflects a dynamic anomaly score corresponding to the input video sequence. In particular, anomaly scores are correlated with the magnitude of the corresponding debris flow hazard, where the largest scores occurring at the time of the most severe debris flow movement. At the same time, the dynamic verification process of the debris flow recognition network provides an additional supervision (see the figure titles) to the debris flow hazard detection results, which reduces the false alarm rates. Note that the recognition results of the debris flow recognition network are constantly changing w.&#x000a0;r.&#x000a0;t. the input video frames. On the contrary, for normal natural videos, the anomaly scores are kept at very low values (nearly zero), suggesting there are no debris flow hazards.</p></sec><sec id="Sec11"><title>Failure analyses</title><p id="Par25">The above quantitative and qualitative experimental results and analyses demonstrate the effectiveness of the proposed method. Additionally, we also show two typical failure cases of the debris flow hazard detection network with false alarms (a) and wrong detections (b) in Fig. <xref rid="Fig10" ref-type="fig">10</xref>. It can be observed that in the case (a), the yellow dotted box indicates the beginning frame (<inline-formula id="IEq22"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$53\text {th}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq22.gif"/></alternatives></inline-formula>) of a debris flow hazard video, where the debris flow hazard does not occur but the debris flow hazard detection network produces high anomaly scores (false positives). In this case, the introduced debris flow hazard recognition network enables the correct verification of debris flow hazard. In the case (b), the yellow dotted box indicates the ending frame (<inline-formula id="IEq23"><alternatives><tex-math id="M30">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$602^\text {th}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_86471_Article_IEq23.gif"/></alternatives></inline-formula>) of a debris flow hazard video, where the debris flow hazard is still ongoing but the debris flow hazard detection network produces low anomaly scores (false negatives). The reason for these failures is mainly due to the limited availability of debris flow hazard videos, which leads to a lack of data diversity (not all possible combinations of different weather conditions, flow behavior, areas, etc. are presented in the data), which impairs the model performance.</p></sec></sec><sec id="Sec12"><title>Discussion</title><p id="Par26">In this study, we introduce a self-assembled Debrisflow23 dataset collected from 7 sampling sites of China and previous recorded debris flow events. The proposed system achieves high accuracy on the current test set. However, the data volume is still relative small compared to popular computer vision tasks, i.&#x000a0;e.,&#x000a0; object detection<sup><xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>, object tracking<sup><xref ref-type="bibr" rid="CR65">65</xref>,<xref ref-type="bibr" rid="CR66">66</xref></sup>, object segmentation<sup><xref ref-type="bibr" rid="CR67">67</xref>&#x02013;<xref ref-type="bibr" rid="CR69">69</xref></sup>, etc. The generalization capability of the system may be degraded due to the complexity and variability of the field environment. Therefore, in our follow-up research, we plan to collect more debris flow hazard videos in larger areas to cover different environmental conditions, thus increasing the data diversity and improving the scalability of the system.</p></sec><sec id="Sec13"><title>Conclusion</title><p id="Par27">In this paper, we proposed a unified deep learning-based system for detecting and recognizing debris flow hazards from video streams. It consists of a video feature extraction network, a debris flow hazard detection network, and a debris flow hazard recognition network. These three networks are connected one after the other aiming to provide accurate debris flow hazard identification information. In addition, we introduced the Debrisflow23 dataset, which were collected from 7 sampling sites of China and the previous recorded debris flow events. Extensive experimental results based on the Debrisflow23 dataset show that the proposed system achieves a detection accuracy of 86.3% AUC, a recognition accuracy of 83.7% AUC, and an overall identification accuracy of 88.1% AUC on the test dataset. The promising performance of debris flow hazard detection and recognition indicates the positive impact of the proposed system on mitigating damage and improving early warning capabilities in debris flow-prone areas, which can be implemented for debris flow hazard monitoring and early warning applications.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This research was supported by the Project of the National Natural Science Foundation of China (No. 42302336), and the Project of the Department of Science and Technology of Sichuan Province (No. 2024YFHZ0098; No. 2023NSFSC0751), and Open Project of Software Engineering Technology Research Support Center of Informatization Application of Sichuan (No. 760115027). The first author received funding by the China Scholarship Council (CSC) from the Ministry of Education of P.R. China. The authors gratefully acknowledge the scientific support and High Performance Computing (HPC) resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universit&#x000e4;t Erlangen-N&#x000fc;rnberg (FAU) under the NHR project b194dc. NHR funding is provided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by the German Research Foundation (DFG) - 440719683.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization, F.W., J.Z., and D.L.; datasets, D.L.; methodology, F.W. and D.L.; hardware and software, F.W.,&#x000a0;J.Z., D.L., A.M., and V.C.; experiments and evaluation, F.W. and D.L.; writing-original draft preparation, F.W.; writing-review and editing, D.L. and V.C.; project administration, D.L.. All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The datasets used and analyzed in this study are available from D.L. upon reasonable request.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par31">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name><surname>Iverson</surname><given-names>RM</given-names></name></person-group><article-title>The physics of debris flows</article-title><source>Rev. Geophys.</source><year>1997</year><volume>35</volume><fpage>245</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1029/97RG00426</pub-id></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Iverson, R. M. The physics of debris flows. <italic>Rev. Geophys.</italic><bold>35</bold>, 245&#x02013;296 (1997).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name><surname>Hungr</surname><given-names>O</given-names></name><name><surname>Leroueil</surname><given-names>S</given-names></name><name><surname>Picarelli</surname><given-names>L</given-names></name></person-group><article-title>The Varnes classification of landslide types, an update</article-title><source>Landslides</source><year>2014</year><volume>11</volume><fpage>167</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1007/s10346-013-0436-y</pub-id></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Hungr, O., Leroueil, S. &#x00026; Picarelli, L. The Varnes classification of landslide types, an update. <italic>Landslides</italic><bold>11</bold>, 167&#x02013;194 (2014).</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name><surname>Pham</surname><given-names>M-V</given-names></name><name><surname>Kim</surname><given-names>Y-T</given-names></name></person-group><article-title>Debris flow detection and velocity estimation using deep convolutional neural network and image processing</article-title><source>Landslides</source><year>2022</year><volume>19</volume><fpage>2473</fpage><lpage>2488</lpage><pub-id pub-id-type="doi">10.1007/s10346-022-01931-6</pub-id></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Pham, M.-V. &#x00026; Kim, Y.-T. Debris flow detection and velocity estimation using deep convolutional neural network and image processing. <italic>Landslides</italic><bold>19</bold>, 2473&#x02013;2488 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>F-Q</given-names></name><name><surname>Hu</surname><given-names>K-H</given-names></name><name><surname>Cui</surname><given-names>P</given-names></name></person-group><article-title>Decision support system of debris flow mitigation for mountain towns</article-title><source>J. Nat. Disasters</source><year>2002</year><volume>11</volume><fpage>31</fpage><lpage>36</lpage></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Wei, F.-Q., Hu, K.-H. &#x00026; Cui, P. Decision support system of debris flow mitigation for mountain towns. <italic>J. Nat. Disasters</italic><bold>11</bold>, 31&#x02013;36 (2002).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Zhang, N. <italic>A Study on the Single Debris Flow Monitoring and Warning System and Its Application in the Earthquake-stricken Area of Sichuan Province</italic>. Ph.D. Thesis (China University of Geosciences (Beijing), 2013).</mixed-citation></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Hu</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><article-title>Infrasound monitoring and early warning of debris flow along montanic railway line</article-title><source>Tech. Acoust./Shengxue Jishu</source><year>2012</year><volume>31</volume><fpage>351</fpage><lpage>356</lpage></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Li, C., Hu, X. &#x00026; Wang, L. Infrasound monitoring and early warning of debris flow along montanic railway line. <italic>Tech. Acoust./Shengxue Jishu</italic><bold>31</bold>, 351&#x02013;356 (2012).</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name><surname>Chaoyang</surname><given-names>H</given-names></name><name><surname>Qiang</surname><given-names>X</given-names></name><name><surname>Nengpan</surname><given-names>J</given-names></name><name><surname>Jian</surname><given-names>H</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name></person-group><article-title>Real-time early warning technology of debris flow based on automatic identification of rainfall process</article-title><source>J. Eng. Geol.</source><year>2018</year><volume>26</volume><fpage>703</fpage><lpage>710</lpage></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Chaoyang, H., Qiang, X., Nengpan, J., Jian, H. &#x00026; Yang, X. Real-time early warning technology of debris flow based on automatic identification of rainfall process. <italic>J. Eng. Geol.</italic><bold>26</bold>, 703&#x02013;710 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>H-T</given-names></name><name><surname>Chang</surname><given-names>Y-L</given-names></name><name><surname>Zhang</surname><given-names>S-C</given-names></name></person-group><article-title>Acoustic signals and geophone response of rainfall-induced debris flows</article-title><source>J. Chin. Inst. Eng.</source><year>2013</year><volume>36</volume><fpage>335</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1080/02533839.2012.730269</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Chou, H.-T., Chang, Y.-L. &#x00026; Zhang, S.-C. Acoustic signals and geophone response of rainfall-induced debris flows. <italic>J. Chin. Inst. Eng.</italic><bold>36</bold>, 335&#x02013;347 (2013).</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name><surname>Diviacco</surname><given-names>P</given-names></name><name><surname>Rebesco</surname><given-names>M</given-names></name><name><surname>Camerlenghi</surname><given-names>A</given-names></name></person-group><article-title>Late Pliocene mega debris flow deposit and related fluid escapes identified on the Antarctic peninsula continental margin by seismic reflection data analysis</article-title><source>Mar. Geophys. Res.</source><year>2006</year><volume>27</volume><fpage>109</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1007/s11001-005-3136-8</pub-id></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Diviacco, P., Rebesco, M. &#x00026; Camerlenghi, A. Late Pliocene mega debris flow deposit and related fluid escapes identified on the Antarctic peninsula continental margin by seismic reflection data analysis. <italic>Mar. Geophys. Res.</italic><bold>27</bold>, 109&#x02013;128 (2006).</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name><surname>H&#x000fc;rlimann</surname><given-names>M</given-names></name><etal/></person-group><article-title>Debris-flow monitoring and warning: Review and examples</article-title><source>Earth-Sci. Rev.</source><year>2019</year><volume>199</volume><fpage>102981</fpage><pub-id pub-id-type="doi">10.1016/j.earscirev.2019.102981</pub-id></element-citation><mixed-citation id="mc-CR10" publication-type="journal">H&#x000fc;rlimann, M. et al. Debris-flow monitoring and warning: Review and examples. <italic>Earth-Sci. Rev.</italic><bold>199</bold>, 102981 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name></person-group><article-title>Application of early warning technology to automatic monitoring of debris-flowa case study of Wangmo river debris flow in wangmo county, guizhou province</article-title><source>J. Eng. Geol.</source><year>2014</year><volume>22</volume><fpage>443</fpage><lpage>449</lpage></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Zhao, X., Li, L., Li, Y. &#x00026; Liu, T. Application of early warning technology to automatic monitoring of debris-flowa case study of Wangmo river debris flow in wangmo county, guizhou province. <italic>J. Eng. Geol.</italic><bold>22</bold>, 443&#x02013;449 (2014).</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Wu, H. <italic>Study on the Advanced Technology of Forecast and Early Warning for Debris Flow and Its Application in Demonstration Area</italic>. Ph.D. Thesis, Chongqing: Chongqing University (2015).</mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>C</given-names></name><name><surname>Ding</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>N</given-names></name></person-group><article-title>A debris flow hazard monitoring and early warning system based on formation and motion processes of debris flow</article-title><source>J. Nat. Disasters</source><year>2014</year><volume>23</volume><fpage>1</fpage><lpage>9</lpage></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Yang, C., Ding, H. &#x00026; Chen, N. A debris flow hazard monitoring and early warning system based on formation and motion processes of debris flow. <italic>J. Nat. Disasters</italic><bold>23</bold>, 1&#x02013;9 (2014).</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name><surname>Marchi</surname><given-names>L</given-names></name><name><surname>Arattano</surname><given-names>M</given-names></name><name><surname>Ten Deganutti</surname><given-names>AM</given-names></name></person-group><article-title>years of debris-flow monitoring in the Moscardo torrent (Italian alps)</article-title><source>Geomorphology</source><year>2002</year><volume>46</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1016/S0169-555X(01)00162-3</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Marchi, L., Arattano, M. &#x00026; Ten Deganutti, A. M. years of debris-flow monitoring in the Moscardo torrent (Italian alps). <italic>Geomorphology</italic><bold>46</bold>, 1&#x02013;17 (2002).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Huang, C.-J., Yin, H.-Y., Chen, C.-Y., Yeh, C.-H. &#x00026; Wang, C.-L. Ground vibrations produced by rock motions and debris flows. <italic>J. Geophys. Res. Earth Surf.</italic><bold>112</bold> (2007).</mixed-citation></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name><surname>Vilajosana</surname><given-names>I</given-names></name><etal/></person-group><article-title>Rockfall induced seismic signals: Case study in Montserrat, Catalonia</article-title><source>Nat. Hazards Earth Syst. Sci.</source><year>2008</year><volume>8</volume><fpage>805</fpage><lpage>812</lpage><pub-id pub-id-type="doi">10.5194/nhess-8-805-2008</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Vilajosana, I. et al. Rockfall induced seismic signals: Case study in Montserrat, Catalonia. <italic>Nat. Hazards Earth Syst. Sci.</italic><bold>8</bold>, 805&#x02013;812 (2008).</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name><surname>Coviello</surname><given-names>V</given-names></name><name><surname>Arattano</surname><given-names>M</given-names></name><name><surname>Turconi</surname><given-names>L</given-names></name></person-group><article-title>Detecting torrential processes from a distance with a seismic monitoring network</article-title><source>Nat. Hazards</source><year>2015</year><volume>78</volume><fpage>2055</fpage><lpage>2080</lpage><pub-id pub-id-type="doi">10.1007/s11069-015-1819-2</pub-id></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Coviello, V., Arattano, M. &#x00026; Turconi, L. Detecting torrential processes from a distance with a seismic monitoring network. <italic>Nat. Hazards</italic><bold>78</bold>, 2055&#x02013;2080 (2015).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Arnold, A. et al. A study of infrasonic signals of debris flows. <italic>Ital. J. Eng. Geol. Environ.</italic> 563&#x02013;572 (2011).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Hsien-Ter Chou, Y.-L.&#x000a0;C. &#x00026; Zhang, S.-C. Acoustic signals and geophone response of rainfall-induced debris flows. <italic>J. Chin. Inst. Eng.</italic><bold>36</bold>, 335&#x02013;347 (2013).</mixed-citation></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name><surname>Marchetti</surname><given-names>E</given-names></name><etal/></person-group><article-title>Infrasound array analysis of debris flow activity and implication for early warning</article-title><source>J. Geophys. Res. Earth Surf.</source><year>2019</year><volume>124</volume><fpage>567</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1029/2018JF004785</pub-id></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Marchetti, E. et al. Infrasound array analysis of debris flow activity and implication for early warning. <italic>J. Geophys. Res. Earth Surf.</italic><bold>124</bold>, 567&#x02013;587 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name><surname>Schimmel</surname><given-names>A</given-names></name><name><surname>H&#x000fc;bl</surname><given-names>J</given-names></name></person-group><article-title>Automatic detection of debris flows and debris floods based on a combination of infrasound and seismic signals</article-title><source>Landslides</source><year>2016</year><volume>13</volume><fpage>1181</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1007/s10346-015-0640-z</pub-id></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Schimmel, A. &#x00026; H&#x000fc;bl, J. Automatic detection of debris flows and debris floods based on a combination of infrasound and seismic signals. <italic>Landslides</italic><bold>13</bold>, 1181&#x02013;1196 (2016).</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Zhang, S. &#x00026; Yu, N. Infrasonic behaviour of debris flow and infrasonic warning device. In <italic>Debris Flow: Disasters, Risk, Forecast, Protection</italic>. 206&#x02013;209 (2008).</mixed-citation></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Yu</surname><given-names>N</given-names></name></person-group><article-title>Early warning system to debris flow</article-title><source>J. Mt. Sci.</source><year>2010</year><volume>28</volume><fpage>379</fpage><lpage>384</lpage></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Zhang, S. &#x00026; Yu, N. Early warning system to debris flow. <italic>J. Mt. Sci.</italic><bold>28</bold>, 379&#x02013;384 (2010).</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Kogelnig, A. et al. A study of infrasonic signals of debris flow. In <italic>Proceedings of 5th International Conference on Debris-flow Hazards: Mitigation, Mechanics, Prediction and Assessment</italic>. 563&#x02013;572 (2011).</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name><surname>Kogelnig</surname><given-names>A</given-names></name><name><surname>H&#x000fc;bl</surname><given-names>J</given-names></name><name><surname>Suri&#x000f1;ach</surname><given-names>E</given-names></name><name><surname>Vilajosana</surname><given-names>I</given-names></name><name><surname>McArdell</surname><given-names>BW</given-names></name></person-group><article-title>Infrasound produced by debris flow: Propagation and frequency content evolution</article-title><source>Nat. Hazards</source><year>2014</year><volume>70</volume><fpage>1713</fpage><lpage>1733</lpage><pub-id pub-id-type="doi">10.1007/s11069-011-9741-8</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Kogelnig, A., H&#x000fc;bl, J., Suri&#x000f1;ach, E., Vilajosana, I. &#x00026; McArdell, B. W. Infrasound produced by debris flow: Propagation and frequency content evolution. <italic>Nat. Hazards</italic><bold>70</bold>, 1713&#x02013;1733 (2014).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>D-L</given-names></name><name><surname>Leng</surname><given-names>X-P</given-names></name><name><surname>Wei</surname><given-names>F-Q</given-names></name><name><surname>Zhang</surname><given-names>S-J</given-names></name><name><surname>Hong</surname><given-names>Y</given-names></name></person-group><article-title>Monitoring and recognition of debris flow infrasonic signals</article-title><source>J. Mt. Sci.</source><year>2015</year><volume>12</volume><fpage>797</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1007/s11629-015-3471-4</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Liu, D.-L., Leng, X.-P., Wei, F.-Q., Zhang, S.-J. &#x00026; Hong, Y. Monitoring and recognition of debris flow infrasonic signals. <italic>J. Mt. Sci.</italic><bold>12</bold>, 797&#x02013;815 (2015).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>D</given-names></name><name><surname>Leng</surname><given-names>X</given-names></name><name><surname>Wei</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Hong</surname><given-names>Y</given-names></name></person-group><article-title>Visualized localization and tracking of debris flow movement based on infrasound monitoring</article-title><source>Landslides</source><year>2018</year><volume>15</volume><fpage>879</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1007/s10346-017-0898-4</pub-id></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Liu, D., Leng, X., Wei, F., Zhang, S. &#x00026; Hong, Y. Visualized localization and tracking of debris flow movement based on infrasound monitoring. <italic>Landslides</italic><bold>15</bold>, 879&#x02013;893 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>D</given-names></name><etal/></person-group><article-title>Method for feature analysis and intelligent recognition of infrasound signals of soil landslides</article-title><source>Bull. Eng. Geol. Environ.</source><year>2021</year><volume>80</volume><fpage>917</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1007/s10064-020-01982-w</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Liu, D. et al. Method for feature analysis and intelligent recognition of infrasound signals of soil landslides. <italic>Bull. Eng. Geol. Environ.</italic><bold>80</bold>, 917&#x02013;932 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name><etal/></person-group><article-title>A physics-based model to derive rainfall intensity-duration threshold for debris flow</article-title><source>Geomorphology</source><year>2020</year><volume>351</volume><fpage>106930</fpage><pub-id pub-id-type="doi">10.1016/j.geomorph.2019.106930</pub-id></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Zhang, S. et al. A physics-based model to derive rainfall intensity-duration threshold for debris flow. <italic>Geomorphology</italic><bold>351</bold>, 106930 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Kai-heng, H. &#x00026; Chao, M. Critical soil moisture for debris flow initiation and its application in forecasting. <italic>J. Earth Sci. Environ.</italic><bold>36</bold> (2014).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Xie, T., Yin, Q., Gao, H., Chen, S.&#x000a0;N., Fang &#x00026; Daming, L. Study on early warning model of glacial-rainfall debris flow based on excitation condition and stability of accumulation body. <italic>J. Glaciol. Geocryol.</italic><bold>41</bold>, 884 (2019).</mixed-citation></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name><surname>Uddin</surname><given-names>MS</given-names></name><name><surname>Inaba</surname><given-names>H</given-names></name><name><surname>Itakura</surname><given-names>Y</given-names></name><name><surname>Kasahara</surname><given-names>M</given-names></name></person-group><article-title>Estimation of the surface velocity of debris flow with computer-based spatial filtering</article-title><source>Appl. Opt.</source><year>1998</year><volume>37</volume><fpage>6234</fpage><lpage>6239</lpage><pub-id pub-id-type="doi">10.1364/AO.37.006234</pub-id><pub-id pub-id-type="pmid">18286122</pub-id>
</element-citation><mixed-citation id="mc-CR32" publication-type="journal">Uddin, M. S., Inaba, H., Itakura, Y. &#x00026; Kasahara, M. Estimation of the surface velocity of debris flow with computer-based spatial filtering. <italic>Appl. Opt.</italic><bold>37</bold>, 6234&#x02013;6239 (1998).<pub-id pub-id-type="pmid">18286122</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name><surname>Uddin</surname><given-names>MS</given-names></name><name><surname>Inaba</surname><given-names>H</given-names></name><name><surname>Itakura</surname><given-names>Y</given-names></name><name><surname>Yoshida</surname><given-names>Y</given-names></name><name><surname>Kasahara</surname><given-names>M</given-names></name></person-group><article-title>Adaptive computer-based spatial-filtering method for more accurate estimation of the surface velocity of debris flow</article-title><source>Appl. Opt.</source><year>1999</year><volume>38</volume><fpage>6714</fpage><lpage>6721</lpage><pub-id pub-id-type="doi">10.1364/AO.38.006714</pub-id><pub-id pub-id-type="pmid">18324210</pub-id>
</element-citation><mixed-citation id="mc-CR33" publication-type="journal">Uddin, M. S., Inaba, H., Itakura, Y., Yoshida, Y. &#x00026; Kasahara, M. Adaptive computer-based spatial-filtering method for more accurate estimation of the surface velocity of debris flow. <italic>Appl. Opt.</italic><bold>38</bold>, 6714&#x02013;6721 (1999).<pub-id pub-id-type="pmid">18324210</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name><surname>Uddin</surname><given-names>M</given-names></name><name><surname>Inaba</surname><given-names>H</given-names></name><name><surname>Yoshida</surname><given-names>Y</given-names></name><name><surname>Itakura</surname><given-names>Y</given-names></name><name><surname>Kasahara</surname><given-names>M</given-names></name></person-group><article-title>Large motion estimation by gradient technique&#x02014;Application to debris flow velocity field</article-title><source>Phys. Chem. Earth Part C Solar Terrest. Planet. Sci.</source><year>2001</year><volume>26</volume><fpage>633</fpage><lpage>638</lpage></element-citation><mixed-citation id="mc-CR34" publication-type="journal">Uddin, M., Inaba, H., Yoshida, Y., Itakura, Y. &#x00026; Kasahara, M. Large motion estimation by gradient technique&#x02014;Application to debris flow velocity field. <italic>Phys. Chem. Earth Part C Solar Terrest. Planet. Sci.</italic><bold>26</bold>, 633&#x02013;638 (2001).</mixed-citation></citation-alternatives></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name><surname>Arattano</surname><given-names>M</given-names></name><name><surname>Marchi</surname><given-names>L</given-names></name></person-group><article-title>Video-derived velocity distribution along a debris flow surge</article-title><source>Phys. Chem. Earth Part B Hydrol. Oceans Atmos.</source><year>2000</year><volume>25</volume><fpage>781</fpage><lpage>784</lpage><pub-id pub-id-type="doi">10.1016/S1464-1909(00)00101-5</pub-id></element-citation><mixed-citation id="mc-CR35" publication-type="journal">Arattano, M. &#x00026; Marchi, L. Video-derived velocity distribution along a debris flow surge. <italic>Phys. Chem. Earth Part B Hydrol. Oceans Atmos.</italic><bold>25</bold>, 781&#x02013;784 (2000).</mixed-citation></citation-alternatives></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Rahnemoonfar, M., Murphy, R., Miquel, M.&#x000a0;V., Dobbs, D. &#x00026; Adams, A. Flooded area detection from uav images based on densely connected recurrent neural networks. In <italic>IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium</italic>. 1788&#x02013;1791 (2018).</mixed-citation></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name><surname>Pi</surname><given-names>Y</given-names></name><name><surname>Nath</surname><given-names>ND</given-names></name><name><surname>Behzadan</surname><given-names>AH</given-names></name></person-group><article-title>Convolutional neural networks for object detection in aerial imagery for disaster response and recovery</article-title><source>Adv. Eng. Inform.</source><year>2020</year><volume>43</volume><fpage>101009</fpage><pub-id pub-id-type="doi">10.1016/j.aei.2019.101009</pub-id></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Pi, Y., Nath, N. D. &#x00026; Behzadan, A. H. Convolutional neural networks for object detection in aerial imagery for disaster response and recovery. <italic>Adv. Eng. Inform.</italic><bold>43</bold>, 101009 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Redmon, J. &#x00026; Farhadi, A. Yolo9000: Better, faster, stronger. In <italic>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>. 6517&#x02013;6525 (2017).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Bochkovskiy, A., Wang, C.-Y. &#x00026; Liao, H.-Y.&#x000a0;M. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2004.10934">arXiv:2004.10934</ext-link> (2020).</mixed-citation></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>S</given-names></name><name><surname>Yu</surname><given-names>D</given-names></name><name><surname>Shen</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Xu</surname><given-names>Q</given-names></name></person-group><article-title>Landslide detection from an open satellite imagery and digital elevation model dataset using attention boosted convolutional neural networks</article-title><source>Landslides</source><year>2020</year><volume>17</volume><fpage>1337</fpage><lpage>1352</lpage><pub-id pub-id-type="doi">10.1007/s10346-020-01353-2</pub-id></element-citation><mixed-citation id="mc-CR40" publication-type="journal">Ji, S., Yu, D., Shen, C., Li, W. &#x00026; Xu, Q. Landslide detection from an open satellite imagery and digital elevation model dataset using attention boosted convolutional neural networks. <italic>Landslides</italic><bold>17</bold>, 1337&#x02013;1352 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &#x00026; Sun, J. Deep residual learning for image recognition. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> (2016).</mixed-citation></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Nayak</surname><given-names>NM</given-names></name><name><surname>Roy-Chowdhury</surname><given-names>AK</given-names></name></person-group><article-title>Context-aware activity recognition and anomaly detection in video</article-title><source>IEEE J. Sel. Top. Signal Process.</source><year>2012</year><volume>7</volume><fpage>91</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1109/JSTSP.2012.2234722</pub-id></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Zhu, Y., Nayak, N. M. &#x00026; Roy-Chowdhury, A. K. Context-aware activity recognition and anomaly detection in video. <italic>IEEE J. Sel. Top. Signal Process.</italic><bold>7</bold>, 91&#x02013;101 (2012).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Xu, D., Ricci, E., Yan, Y., Song, J. &#x00026; Sebe, N. Learning deep representations of appearance and motion for anomalous event detection. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1510.01553">arXiv:1510.01553</ext-link> (2015).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Cui, X., Liu, Q., Gao, M. &#x00026; Metaxas, D.&#x000a0;N. Abnormal detection using interaction energy potentials. In <italic>CVPR 2011</italic>. 3161&#x02013;3167 (IEEE, 2011).</mixed-citation></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Mahadevan</surname><given-names>V</given-names></name><name><surname>Vasconcelos</surname><given-names>N</given-names></name></person-group><article-title>Anomaly detection and localization in crowded scenes</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2013</year><volume>36</volume><fpage>18</fpage><lpage>32</lpage></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Li, W., Mahadevan, V. &#x00026; Vasconcelos, N. Anomaly detection and localization in crowded scenes. <italic>IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>36</bold>, 18&#x02013;32 (2013).</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Lu, C., Shi, J. &#x00026; Jia, J. Abnormal event detection at 150 fps in Matlab. In <italic>Proceedings of the IEEE International Conference on Computer Vision</italic>. 2720&#x02013;2727 (2013).</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Zhao, B., Fei-Fei, L. &#x00026; Xing, E.&#x000a0;P. Online detection of unusual events in videos via dynamic sparse coding. In <italic>CVPR 2011</italic>. 3313&#x02013;3320 (IEEE, 2011).</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Hasan, M., Choi, J., Neumann, J., Roy-Chowdhury, A.&#x000a0;K. &#x00026; Davis, L.&#x000a0;S. Learning temporal regularity in video sequences. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>. 733&#x02013;742 (2016).</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Datta, A., Shah, M. &#x00026; Lobo, N. D.&#x000a0;V. Person-on-person violence detection in video data. In <italic>Object Recognition Supported by User Interaction for Service Robots</italic>. Vol.&#x000a0;1. 433&#x02013;438 (IEEE, 2002).</mixed-citation></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name><surname>Kooij</surname><given-names>JF</given-names></name><name><surname>Liem</surname><given-names>MC</given-names></name><name><surname>Krijnders</surname><given-names>JD</given-names></name><name><surname>Andringa</surname><given-names>TC</given-names></name><name><surname>Gavrila</surname><given-names>DM</given-names></name></person-group><article-title>Multi-modal human aggression detection</article-title><source>Comput. Vis. Image Understand.</source><year>2016</year><volume>144</volume><fpage>106</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.cviu.2015.06.009</pub-id></element-citation><mixed-citation id="mc-CR50" publication-type="journal">Kooij, J. F., Liem, M. C., Krijnders, J. D., Andringa, T. C. &#x00026; Gavrila, D. M. Multi-modal human aggression detection. <italic>Comput. Vis. Image Understand.</italic><bold>144</bold>, 106&#x02013;120 (2016).</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name></person-group><article-title>Violence detection using oriented violent flows</article-title><source>Image Vis. Comput.</source><year>2016</year><volume>48</volume><fpage>37</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2016.01.006</pub-id></element-citation><mixed-citation id="mc-CR51" publication-type="journal">Gao, Y., Liu, H., Sun, X., Wang, C. &#x00026; Liu, Y. Violence detection using oriented violent flows. <italic>Image Vis. Comput.</italic><bold>48</bold>, 37&#x02013;41 (2016).</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Mohammadi, S., Perina, A., Kiani, H. &#x00026; Murino, V. Angry crowds: Detecting violent events in videos. In <italic>European Conference on Computer Vision</italic>. 3&#x02013;18 (Springer, 2016).</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Sultani, W., Chen, C. &#x00026; Shah, M. Real-world anomaly detection in surveillance videos. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>. 6479&#x02013;6488 (2018).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Miech, A. et al. End-to-end learning of visual representations from uncurated instructional videos. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>. 9879&#x02013;9889 (2020).</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Ionescu, R.&#x000a0;T., Khan, F.&#x000a0;S., Georgescu, M.-I. &#x00026; Shao, L. Object-centric auto-encoders and dummy anomalies for abnormal event detection in video. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>. 7842&#x02013;7851 (2019).</mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Zhong, J.-X. et al. Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>. 1237&#x02013;1246 (2019).</mixed-citation></ref><ref id="CR57"><label>57.</label><citation-alternatives><element-citation id="ec-CR57" publication-type="journal"><person-group person-group-type="author"><name><surname>Pang</surname><given-names>G</given-names></name><name><surname>Shen</surname><given-names>C</given-names></name><name><surname>Cao</surname><given-names>L</given-names></name><name><surname>Hengel</surname><given-names>AVD</given-names></name></person-group><article-title>Deep learning for anomaly detection: A review</article-title><source>ACM Comput. Surv. (CSUR)</source><year>2021</year><volume>54</volume><fpage>1</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1145/3439950</pub-id></element-citation><mixed-citation id="mc-CR57" publication-type="journal">Pang, G., Shen, C., Cao, L. &#x00026; Hengel, A. V. D. Deep learning for anomaly detection: A review. <italic>ACM Comput. Surv. (CSUR)</italic><bold>54</bold>, 1&#x02013;38 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Tran, D., Bourdev, L., Fergus, R., Torresani, L. &#x00026; Paluri, M. Learning spatiotemporal features with 3d convolutional networks. In <italic>Proceedings of the IEEE International Conference on Computer Vision</italic>. 4489&#x02013;4497 (2015).</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Krizhevsky, A., Sutskever, I. &#x00026; Hinton, G.&#x000a0;E. Imagenet classification with deep convolutional neural networks. In <italic>Advances in Neural Information Processing Systems</italic> (Pereira, F., Burges, C., Bottou, L. &#x00026; Weinberger, K. eds.) . Vol.&#x000a0;25 (2012).</mixed-citation></ref><ref id="CR60"><label>60.</label><citation-alternatives><element-citation id="ec-CR60" publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>W</given-names></name><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>K</given-names></name></person-group><article-title>3d convolutional neural networks for human action recognition</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2013</year><volume>35</volume><fpage>221</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2012.59</pub-id><pub-id pub-id-type="pmid">22392705</pub-id>
</element-citation><mixed-citation id="mc-CR60" publication-type="journal">Ji, S., Xu, W., Yang, M. &#x00026; Yu, K. 3d convolutional neural networks for human action recognition. <italic>IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>35</bold>, 221&#x02013;231 (2013).<pub-id pub-id-type="pmid">22392705</pub-id>
</mixed-citation></citation-alternatives></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">Tran, D. et al. A closer look at spatiotemporal convolutions for action recognition. In <italic>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>. 6450&#x02013;6459 (2018).</mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="other">&#x000c7;i&#x000e7;ek, O., Abdulkadir, A., Lienkamp, S.&#x000a0;S., Brox, T. &#x00026; Ronneberger, O. 3D u-net: Learning dense volumetric segmentation from sparse annotation. In <italic>Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II</italic>. 424-432 (Springer, 2016).</mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Chen, C., Liu, X., Ding, M., Zheng, J. &#x00026; Li, J. 3D dilated multi-fiber network for real-time brain tumor segmentation in MRI. In <italic>Medical Image Computing and Computer Assisted Intervention&#x02014; MICCAI 2019</italic> (Shen, D. et al. eds.) . 184&#x02013;192 (Springer, 2019).</mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Lin, T.-Y. et al. Feature pyramid networks for object detection. In <italic>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>. 936&#x02013;944 (2017).</mixed-citation></ref><ref id="CR65"><label>65.</label><citation-alternatives><element-citation id="ec-CR65" publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Maier</surname><given-names>A</given-names></name><name><surname>Christlein</surname><given-names>V</given-names></name></person-group><article-title>Updating siamese trackers using peculiar mixup</article-title><source>Appl. Intell.</source><year>2023</year><volume>53</volume><fpage>22531</fpage><lpage>22545</lpage><pub-id pub-id-type="doi">10.1007/s10489-023-04546-z</pub-id></element-citation><mixed-citation id="mc-CR65" publication-type="journal">Wu, F., Zhang, J., Xu, Z., Maier, A. &#x00026; Christlein, V. Updating siamese trackers using peculiar mixup. <italic>Appl. Intell.</italic><bold>53</bold>, 22531&#x02013;22545 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR66"><label>66.</label><citation-alternatives><element-citation id="ec-CR66" publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name></person-group><article-title>Stably adaptive anti-occlusion siamese region proposal network for real-time object tracking</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>161349</fpage><lpage>161360</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3019206</pub-id></element-citation><mixed-citation id="mc-CR66" publication-type="journal">Wu, F., Zhang, J. &#x00026; Xu, Z. Stably adaptive anti-occlusion siamese region proposal network for real-time object tracking. <italic>IEEE Access</italic><bold>8</bold>, 161349&#x02013;161360 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR67"><label>67.</label><citation-alternatives><element-citation id="ec-CR67" publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>F</given-names></name><etal/></person-group><article-title>Amd-hooknet for glacier front segmentation</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2023</year><volume>61</volume><fpage>1</fpage><lpage>12</lpage></element-citation><mixed-citation id="mc-CR67" publication-type="journal">Wu, F. et al. Amd-hooknet for glacier front segmentation. <italic>IEEE Trans. Geosci. Remote Sens.</italic><bold>61</bold>, 1&#x02013;12 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR68"><label>68.</label><citation-alternatives><element-citation id="ec-CR68" publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>F</given-names></name><etal/></person-group><article-title>Contextual hookformer for glacier calving front segmentation</article-title><source>IEEE Trans. Geosci. Remote Sens.</source><year>2024</year><volume>62</volume><fpage>1</fpage><lpage>15</lpage></element-citation><mixed-citation id="mc-CR68" publication-type="journal">Wu, F. et al. Contextual hookformer for glacier calving front segmentation. <italic>IEEE Trans. Geosci. Remote Sens.</italic><bold>62</bold>, 1&#x02013;15 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Minaee, S. et al. Image Segmentation Using Deep Learning: A Survey. <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence</italic>, <bold>44</bold>(7), 3523&#x02013;3542. 10.1109/TPAMI.2021.3059968 (2022).</mixed-citation></ref></ref-list></back></article>