<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="other"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">F1000Res</journal-id><journal-id journal-id-type="iso-abbrev">F1000Res</journal-id><journal-title-group><journal-title>F1000Research</journal-title></journal-title-group><issn pub-type="epub">2046-1402</issn><publisher><publisher-name>F1000 Research Limited</publisher-name><publisher-loc>London, UK</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">39989910</article-id><article-id pub-id-type="pmc">PMC11847151</article-id><article-id pub-id-type="doi">10.12688/f1000research.127142.3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Software Tool Article</subject></subj-group><subj-group><subject>Articles</subject></subj-group></article-categories><title-group><article-title>Med-ImageTools: An open-source Python package for robust data processing pipelines and curating medical imaging data</article-title><fn-group content-type="pub-status"><fn><p>[version 3; peer review: 2 approved]</p></fn></fn-group></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kim</surname><given-names>Sejin</given-names></name><role content-type="http://credit.niso.org/">Conceptualization</role><role content-type="http://credit.niso.org/">Data Curation</role><role content-type="http://credit.niso.org/">Formal Analysis</role><role content-type="http://credit.niso.org/">Investigation</role><role content-type="http://credit.niso.org/">Methodology</role><role content-type="http://credit.niso.org/">Project Administration</role><role content-type="http://credit.niso.org/">Software</role><role content-type="http://credit.niso.org/">Validation</role><role content-type="http://credit.niso.org/">Visualization</role><role content-type="http://credit.niso.org/">Writing &#x02013; Original Draft Preparation</role><role content-type="http://credit.niso.org/">Writing &#x02013; Review &#x00026; Editing</role><xref rid="a1" ref-type="aff">1</xref><xref rid="a2" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Kazmierski</surname><given-names>Michal</given-names></name><role content-type="http://credit.niso.org/">Conceptualization</role><role content-type="http://credit.niso.org/">Methodology</role><role content-type="http://credit.niso.org/">Software</role><xref rid="a1" ref-type="aff">1</xref><xref rid="a2" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Qu</surname><given-names>Kevin</given-names></name><role content-type="http://credit.niso.org/">Data Curation</role><role content-type="http://credit.niso.org/">Methodology</role><role content-type="http://credit.niso.org/">Software</role><role content-type="http://credit.niso.org/">Validation</role><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5936-3581</contrib-id><xref rid="a1" ref-type="aff">1</xref><xref rid="a3" ref-type="aff">3</xref></contrib><contrib contrib-type="author"><name><surname>Peoples</surname><given-names>Jacob</given-names></name><role content-type="http://credit.niso.org/">Conceptualization</role><role content-type="http://credit.niso.org/">Data Curation</role><role content-type="http://credit.niso.org/">Methodology</role><role content-type="http://credit.niso.org/">Validation</role><role content-type="http://credit.niso.org/">Writing &#x02013; Review &#x00026; Editing</role><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0191-7446</contrib-id><xref rid="a4" ref-type="aff">4</xref></contrib><contrib contrib-type="author"><name><surname>Nakano</surname><given-names>Minoru</given-names></name><role content-type="http://credit.niso.org/">Resources</role><role content-type="http://credit.niso.org/">Software</role><xref rid="a1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Ramanathan</surname><given-names>Vishwesh</given-names></name><role content-type="http://credit.niso.org/">Methodology</role><role content-type="http://credit.niso.org/">Software</role><role content-type="http://credit.niso.org/">Validation</role><xref rid="a2" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Marsilla</surname><given-names>Joseph</given-names></name><role content-type="http://credit.niso.org/">Methodology</role><role content-type="http://credit.niso.org/">Validation</role><xref rid="a1" ref-type="aff">1</xref><xref rid="a2" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Welch</surname><given-names>Mattea</given-names></name><role content-type="http://credit.niso.org/">Conceptualization</role><role content-type="http://credit.niso.org/">Methodology</role><role content-type="http://credit.niso.org/">Resources</role><role content-type="http://credit.niso.org/">Supervision</role><xref rid="a1" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Simpson</surname><given-names>Amber</given-names></name><role content-type="http://credit.niso.org/">Resources</role><role content-type="http://credit.niso.org/">Supervision</role><role content-type="http://credit.niso.org/">Writing &#x02013; Review &#x00026; Editing</role><xref rid="a4" ref-type="aff">4</xref><xref rid="a5" ref-type="aff">5</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Haibe-Kains</surname><given-names>Benjamin</given-names></name><role content-type="http://credit.niso.org/">Conceptualization</role><role content-type="http://credit.niso.org/">Funding Acquisition</role><role content-type="http://credit.niso.org/">Methodology</role><role content-type="http://credit.niso.org/">Project Administration</role><role content-type="http://credit.niso.org/">Resources</role><role content-type="http://credit.niso.org/">Supervision</role><role content-type="http://credit.niso.org/">Writing &#x02013; Review &#x00026; Editing</role><xref rid="c1" ref-type="corresp">a</xref><xref rid="a1" ref-type="aff">1</xref><xref rid="a2" ref-type="aff">2</xref><xref rid="a4" ref-type="aff">4</xref><xref rid="a5" ref-type="aff">5</xref></contrib><aff id="a1">
<label>1</label>Princess Margaret Cancer Centre, University Health Network, Canada, Toronto, ON, Canada</aff><aff id="a2">
<label>2</label>Department of Medical Biophysics, University of Toronto, Toronto, ON, Canada</aff><aff id="a3">
<label>3</label>Faculty of Engineering, University of Toronto, Toronto, ON, Canada</aff><aff id="a4">
<label>4</label>Centre for Health Innovation, Queen's University and Kingston Health Science Centre, Kingston, ON, Canada</aff><aff id="a5">
<label>5</label>Vector Institute, Toronto, ON, Canada</aff></contrib-group><author-notes><corresp id="c1">
<label>a</label>
<email xlink:href="mailto:benjamin.haibe.kains@utoronto.ca">benjamin.haibe.kains@utoronto.ca</email>
</corresp><fn fn-type="COI-statement"><p>No competing interests were disclosed.</p></fn></author-notes><pub-date pub-type="epub"><day>7</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>118</elocation-id><history><date date-type="accepted"><day>3</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>Copyright: &#x000a9; 2025 Kim S et al.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access article distributed under the terms of the Creative Commons Attribution Licence, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="f1000research-12-177718.pdf"/><abstract><sec><title>Background</title><p>Machine learning and AI promise to revolutionize the way we leverage medical imaging data for improving care but require large datasets to train computational models that can be implemented in clinical practice. However, processing large and complex medical imaging datasets remains an open challenge.</p></sec><sec><title>Methods</title><p>To address this issue, we developed Med-ImageTools, a new Python open-source software package to automate data curation and processing while allowing researchers to share their data processing configurations more easily, lowering the barrier for other researchers to reproduce published works.</p></sec><sec><title>Use cases</title><p>We have demonstrated the efficiency of Med-ImageTools across three different datasets, resulting in significantly reduced processing times.</p></sec><sec><title>Conclusions</title><p>The AutoPipeline feature will improve the accessibility of raw clinical datasets on public archives, such as the Cancer Imaging Archive (TCIA), the largest public repository of cancer imaging, allowing machine learning researchers to process analysis-ready formats without requiring deep domain knowledge.</p></sec></abstract><kwd-group kwd-group-type="author"><kwd>medical imaging</kwd><kwd>deep learning</kwd><kwd>open source</kwd><kwd>data processing</kwd><kwd>dicom</kwd><kwd>nifti</kwd><kwd>nnunet</kwd></kwd-group><funding-group><award-group id="fund-1" xlink:href="http://dx.doi.org/10.13039/501100000521"><funding-source>Canadian Cancer Society</funding-source><award-id>707609</award-id></award-group><award-group id="fund-2" xlink:href="http://dx.doi.org/10.13039/501100000024"><funding-source>Canadian Institutes of Health Research</funding-source><award-id>426366</award-id></award-group><funding-statement>This study has been supported by the Canadian Institutes for Health Research (Project Scheme grant #426366) and the Canadian Cancer Society (Data Transformation grant #707609).</funding-statement><funding-statement>
<italic toggle="yes">The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</italic>
</funding-statement></funding-group></article-meta><notes notes-type="version-changes"><sec sec-type="version-changes"><label>Revised</label><title>Amendments from Version 2</title><p>Updated DOI and Documentation URLs, after streamlining CI-CD pipelines based on user/reviewer feedback.</p></sec></notes></front><body><sec sec-type="intro" id="sec1"><title>Introduction</title><p>Radiology is a powerful modality of data for clinical work &#x02014; it gives clinicians the ability to see the inner workings of the human body, that cannot be seen from the outside.
<sup>
<xref rid="ref1" ref-type="bibr">1</xref>
</sup> They can inspect in 2D or 3D, the anatomy surrounding the disease, enabling key information to make life-altering clinical decisions. While regular images taken on cameras or phones are stored in a variety of accessible formats, medical images are encoded in the Digital Imaging and Communications in Medicine (DICOM) standard file format.
<sup>
<xref rid="ref2" ref-type="bibr">2</xref>
</sup>
</p><p>The DICOM standard was developed in the 1980s due to the increasing need for an interoperable standard for 3D medical images across various manufacturers.
<sup>
<xref rid="ref2" ref-type="bibr">2</xref>
</sup> A key feature of DICOM is the plethora of metadata fields that store information beyond imaging data, such as patient information, clinical variables, and acquisition parameters. As modern medical practice evolved over the years, the DICOM standard has grown to accommodate more metadata fields and encompass new imaging modalities or therapy information.
<sup>
<xref rid="ref2" ref-type="bibr">2</xref>
</sup> This type of data format is unsuitable for imaging analysis as the relevant voxel array must be manually accessed through DICOM hierarchy.
<sup>
<xref rid="ref2" ref-type="bibr">2</xref>
</sup> Furthermore, 3D scans are acquired on a slice-by-slice basis; thus, researchers must stitch together data from multiple files to create one 3D image, adding delays caused by disk reads and consolidation processes.
<sup>
<xref rid="ref2" ref-type="bibr">2</xref>
</sup>
</p><p>Specialties that rigorously use imaging data are heavily reliant on DICOM, one of which is radiation oncology. Images are used along every step of the clinical workflow: from deriving a precise diagnosis, to designing personalized radiation therapy plans, and delivering each radiation dose with the appropriate alignment and orientation with brief scans. While the defined standard serves as a good guideline, each manufacturer has slightly different implementations. This is especially the case for DICOM-RT (radiotherapy), a subset of modalities for communicating radiotherapy data.
<sup>
<xref rid="ref2" ref-type="bibr">2</xref>
</sup> The DICOM-RT standard includes additional modalities such as RTStruct for contour data, and RTDose for radiotherapy dose maps and dose-volume histograms (DVH).
<sup>
<xref rid="ref2" ref-type="bibr">2</xref>
</sup> While the broad adoption of the DICOM standard to accommodate for various use cases has allowed it to become the defacto standard for encoding, storing, and transferring of medical images, its comprehensive nature has made it difficult for researchers to navigate for the purposes of imaging projects.
<sup>
<xref rid="ref2" ref-type="bibr">2</xref>
</sup>
</p></sec><sec id="sec2"><title>Current workflows</title><p>The Cancer Imaging Archive (TCIA) (RRID:SCR_008927) is one of the largest public repositories of DICOM images available, with over 140 datasets consisting of more than 60,000 patients.
<sup>
<xref rid="ref3" ref-type="bibr">3</xref>
</sup> The datasets undergo a quality assurance process to ensure the recorded clinical variables are coherent and the DICOM files are not missing any important metadata fields.
<sup>
<xref rid="ref3" ref-type="bibr">3</xref>
</sup> These stringent processes and infrastructure have allowed TCIA to become one of the most comprehensive repositories for biomedical imaging datasets, inviting researchers from different fields to explore new ideas and methods on high quality datasets.
<sup>
<xref rid="ref3" ref-type="bibr">3</xref>
</sup>
</p><p>While the underlying data and its annotations are of clinical quality, processing the dataset for subsequent analysis requires a non-trivial amount of effort: manually reorganizing directories and matching radiation therapy structures (RTStruct), referred to as DICOM-RT contours, and radiation therapy plans (RTDose/RTPlan) to its corresponding images.
<sup>
<xref rid="ref4" ref-type="bibr">4</xref>
</sup> This is partly due to the inherently complex nature of clinical datasets, as data is collected on the basis of need and iterative improvements, not structured scientific inquiries. It is also sometimes due to the lack of familiarity from machine learning (ML) and artificial intelligence (AI) researchers in handling the DICOM files for their analytical pipelines. Typical AI imaging datasets have pairwise associations of one image to a single ground-truth label.
<sup>
<xref rid="ref5" ref-type="bibr">5</xref>
</sup> However, one patient in clinical datasets may have multiple RTStruct and RTDose files with one imaging acquisition, one RTStruct and RTDose with multiple images, or worst of all, multiple RTStruct and RTDose files with multiple images.
<sup>
<xref rid="ref6" ref-type="bibr">6</xref>
</sup> In any of these cases, the directories are not always intuitively structured to help researchers understand which files correspond with another.</p><p>Once researchers have successfully curated the dataset into an organized structure for analysis, in order to process the raw dataset into analysis-ready format, they must choose from a variety of processing parameters, ranging from voxel spacing, RTStruct name parsing, and hounsfield unit (HU) window levels, based on the design of their analysis. While these implicit decisions for image processing are often arbitrary, they can greatly impact model training and performance, but are not transparently disclosed in publications.
<sup>
<xref rid="ref7" ref-type="bibr">7</xref>
</sup> This leads to the difficulty of reproducibility of medical deep learning research, adding another deterrence to clinical adoption.</p><p>Furthermore, there are a limited number of software packages that researchers can use to quickly parse DICOM-RT files into analysis-ready arrays (
<xref rid="T1" ref-type="table">
Table 1</xref>). Chief among those, SlicerRT,
<sup>
<xref rid="ref8" ref-type="bibr">8</xref>
</sup> an extension of 3Dslicer
<sup>
<xref rid="ref9" ref-type="bibr">9</xref>
</sup> (RRID:SCR_005619), an open-source DICOM visualization tool, has been widely used by the medical imaging community. Despite its broad adoption, batch data processing with Slicer requires custom scripting in Python, to be executed in the Slicer ecosystem. Rather than simply installing a package within their Python environment, users must install the Slicer application and add any other dependencies, not provided by Slicer, into the application environment. As a result, machine learning projects relying on Python for data preprocessing will have their code fragmented across multiple environments&#x02013;the Slicer environment for data processing, and another Python environment for data analysis and machine learning.
<sup>
<xref rid="ref10" ref-type="bibr">10</xref>
</sup>
<ext-link xlink:href="https://github.com/qurit/rt-utils" ext-link-type="uri">RT-Utils
</ext-link> is a lightweight, open-source Python package designed to handle RTStruct files with relative ease and simplicity, allowing users to easily export contours into segmentation masks in arrays. However, the functionalities of RT-Utils are limited to the RTStruct modality.
<ext-link xlink:href="https://github.com/pyplati/platipy" ext-link-type="uri">PlatiPy</ext-link> is a recent processing library and analysis toolkit for medical imaging, mainly designed for the context of radiation therapy. It features a comprehensive set of image manipulation functions such as registration and atlas-based segmentation methods, allowing researchers the flexibility to process imaging data into any format they need. However, PlatiPy does not solve the inherent complexity of clinical datasets, and researchers must spend hours reorganizing the data into a structured set of samples and labels. The current landscape of open-source medical imaging tools highlights the need for a native Python package that can parse large DICOM/DICOM-RT datasets to an analysis-ready format for ML/AI development in a consistent, reproducible workflow.</p><table-wrap position="float" id="T1"><label>
Table 1. </label><caption><title>Comparison of existing medical imaging processing packages and their features.</title><p>RTSTRUCTs: DICOM-RT Contours; RTDOSEs: DICOM-RT Dose; CT: Computed Tomography; MRI: Magnetic Resonance Imaging; NifTI: Neuroimaging Informatics Technology Initiative; Nrrd: Nearly raw raster data; DICOM: Digital Imaging and Communications in Medicine.</p></caption><table frame="hsides" rules="groups" content-type="article-table"><thead><tr><th align="left" colspan="1" rowspan="1" valign="top"/><th align="left" colspan="1" rowspan="1" valign="top">3Dslicer + SlicerRT</th><th align="left" colspan="1" rowspan="1" valign="top">RT-Utils
</th><th align="left" colspan="1" rowspan="1" valign="top">PlatiPy</th><th align="left" colspan="1" rowspan="1" valign="top">Med-ImageTools</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1" valign="top">Native Python interface</td><td colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td></tr><tr><td align="left" colspan="1" rowspan="1" valign="top">Command-line interface</td><td colspan="1" rowspan="1"/><td colspan="1" rowspan="1"/><td colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td></tr><tr><td align="left" colspan="1" rowspan="1" valign="top">Handles RTSTRUCTs</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td></tr><tr><td align="left" colspan="1" rowspan="1" valign="top">Handles RTDOSEs</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td></tr><tr><td align="left" colspan="1" rowspan="1" valign="top">Handles images (CT/MRI)</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td></tr><tr><td align="left" colspan="1" rowspan="1" valign="top">Built-in image transformations</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td></tr><tr><td align="left" colspan="1" rowspan="1" valign="top">Exports to analysis-ready NifTI/Nrrd</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td></tr><tr><td align="left" colspan="1" rowspan="1" valign="top">Image registration</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1" valign="top">Built-in bulk processing of entire datasets</td><td colspan="1" rowspan="1"/><td colspan="1" rowspan="1"/><td colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td></tr><tr><td align="left" colspan="1" rowspan="1" valign="top">Automatic parsing of DICOM metadata</td><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td><td colspan="1" rowspan="1"/><td colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1" valign="top">&#x02b24;</td></tr></tbody></table></table-wrap><p>To address the limitations of the current software packages used to process medical images, we developed Med-ImageTools,
<sup>
<xref rid="ref11" ref-type="bibr">11</xref>
</sup> a new Python package designed to help researchers transform complex medical datasets into analysis-ready format with few lines of code. It is also focused on helping researchers develop transparent and reproducible medical image processing pipelines by addressing most of the boilerplate code required for image transformations and processing parallelization. While Med-ImageTools has many modular functions for image, contour, and dose input/output (IO) built on popular frameworks such as SimpleITK, TorchIO and PyDicom, such functionalities are redundant and available in other open-source packages as well. We have tailored our core features towards broader use cases and development workflows, instead of modality or disease type specific workflows, such as BIDS.
<sup>
<xref rid="ref18" ref-type="bibr">12</xref>
</sup> Our main contribution is in the development of AutoPipeline and will mainly discuss its functionalities and implementation.</p></sec><sec sec-type="methods" id="sec3"><title>Methods</title><sec id="sec4"><title>AutoPipeline</title><p>AutoPipeline is the main feature of Med-ImageTools that allows users to easily process raw DICOM clinical datasets into analysis-ready Nrrd or NifTI files, which are commonly used file formats for 3D volumetric data. It is interfaced using the command line, so the user only needs to submit a single command into the terminal to execute the three core steps in the AutoPipeline process (
<xref rid="f1" ref-type="fig">
Figure 1</xref>):
<list list-type="simple"><list-item><label>1.</label><p>
<bold>Crawl</bold>: The crawler opens every DICOM file in the dataset using Pydicom, indexing important metadata, such as unique identifiers, modality information, and references to other modalities. This produces a database of every unique image and DICOM-RT modality.</p></list-item><list-item><label>2.</label><p>
<bold>Connect</bold>: In this step, each patient&#x02019;s indices of unique files of different modalities are connected to form one coherent sample. There are various heuristics the user may choose to connect samples. The default option is through DICOM metadata, as datasets derived from clinical practice are expected to have corresponding metadata that references unique identifiers of the parent image or RTPlan. Alternative heuristics allow users to deal with anomalies with corrupted or missing metadata.</p></list-item><list-item><label>3.</label><p>
<bold>Process</bold>: All identified samples are processed according to imaging and transformation parameters defined by the user. The user can configure parameters such as the pixel spacing in mm(s), which specific modalities to process, the number of cores they want to use for multiprocessing, and define nnU-Net specific flags as well. The images are manipulated using SimpleITK without requiring any user intervention.</p></list-item></list>
</p><fig position="float" fig-type="figure" id="f1"><label>
Figure 1. </label><caption><title>Overview of Med-ImageTools features.</title><p>Raw datasets are indexed by the Crawler module and automatically processed by AutoPipeline. DICOM: Digital Imaging and Communications in Medicine; RTSTRUCTs: DICOM-RT Contours; PET: Positron emission tomography. NifTI: Neuroimaging Informatics Technology Initiative; CSV: Comma-separated values.</p></caption><graphic xlink:href="f1000research-12-177718-g0000" id="gr1" position="float"/></fig><p>The AutoPipeline feature can be directly interfaced once Med-ImageTools is installed. To install Med-ImageTools, the recommended method is
<italic toggle="yes">via</italic> the PyPI package repository in a virtual environment. Running the &#x02018;
<italic toggle="yes">pip install med-imagetools
</italic>&#x02019; command will install the latest version of Med-ImageTools and its associated dependencies defined by the requirements.txt file. Now, whenever the user is in the virtual environment where Med-ImageTools is installed, they can directly interact with the AutoPipeline feature in the command line. The simplest way to use it is through &#x02018;
<italic toggle="yes">autopipeline input_directory output_directory</italic>&#x02019;. This will automatically process the dataset located in &#x02018;
<italic toggle="yes">input_directory</italic>&#x02019; and process them at &#x02018;
<italic toggle="yes">output_directory</italic>&#x02019; using the default parameters. Med-ImageTools will generate files that are not analysis-ready images, such as the autogenerated &#x02018;Dataset Index&#x02019; (
<xref rid="f1" ref-type="fig">
Figure 1</xref>), and store them in a folder named &#x0201c;.imgtools&#x0201d; at the path of the &#x02018;input_directory&#x02019;. This is to ensure a convenient user experience and intuitive folder structure by hiding extraneous components. An extended tutorial of AutoPipeline and all its associated parameters are available
<ext-link xlink:href="https://med-imagetools.readthedocs.io/en/documentation/AutoPipeline/" ext-link-type="uri">here</ext-link>. At the present time, there are no minimum system requirements for Med-ImageTools as it will run regardless of number of processor cores or memory (RAM). However, if the researcher can leverage greater number of cores and RAM, it will allow the AutoPipeline to be parallelized and process the data faster.</p><p>As the crawl and process steps are computationally intensive, all steps in AutoPipeline are automatically parallelized using the joblib backend to efficiently leverage all available computational resources. While the output of the AutoPipeline processing can result in terabytes of images, the crawl is limited to a few kilobytes of a metadata database, making it an ideal asset to share with other researchers as a detailed descriptor of a medical imaging dataset. We therefore propose to attach the crawled metadata spreadsheet to large TCIA datasets to allow Med-ImageTools users to process large datasets much faster and more efficiently. These databases are expected to save up to 1000 core-hours of crawling per dataset, accumulating over 2000 core-hours of computation saved per user. By standardizing a commonly repeated imaging processing pipeline into a single unified package, we hope to improve the reproducibility and transparency of future medical imaging research.</p></sec></sec><sec id="sec5"><title>Use cases</title><p>We showcase the value of the AutoPipeline implemented in Med-ImageTools v1.0.0 for processing three medical imaging datasets, namely Pancreatic-CBCT-SEG from TCIA, liver metastasis private dataset and RADCURE pending public release on TCIA, in order of complexity and sizes. In each use case, we initially describe the process.</p><sec id="sec6"><title>Using pre-crawled datasets on the Pancreatic Cone-beam Computed Tomography (CBCT)
<sup>
<xref rid="ref12" ref-type="bibr">13</xref>
</sup>
</title><p>
<italic toggle="yes">40 patients with abdominal CBCT scans and their associated contours of regions of interest and other organs, publicly available on TCIA</italic>
</p><p>The Pancreatic-CT-CBCT-SEG dataset was processed twice using AutoPipeline: once from scratch, and once using the pre-crawled dataset, available on the tcia-crawls branch of Med-ImageTools. Processing from scratch, the dataset took 10.77 core-hours (10:46), whereas using a pre-crawled database allowed the processing to finish in 9.14 core-hours (9:08) (
<xref rid="f2" ref-type="fig">
Figure 2a</xref>). The pre-crawled database reduced processing time by 1.63 core-hours, representing an 18% increase in total processing speed or 2 minutes 27 seconds per patient. The time saved from pre-crawled databases is not a substantial quantity for datasets with less than 100 patients. However, when scaled up to larger TCIA datasets such as OPC-Radiomics
<sup>
<xref rid="ref13" ref-type="bibr">14</xref>
</sup> (n=606) and NLST
<sup>
<xref rid="ref14" ref-type="bibr">15</xref>
</sup> (n=26254), it can save researchers 24.7 core-hours and 1072 core-hours, respectively (
<xref rid="f2" ref-type="fig">
Figure 2b</xref>). The resources saved are reported in core-hours to allow a hardware-agnostic estimation of time and cost savings. While it may vary depending on the research infrastructure utilized, these databases can result in significant savings in billing.</p><fig position="float" fig-type="figure" id="f2"><label>
Figure 2. </label><caption><title>a) Time taken to process the Pancreatic-CT-CBCT-SEG dataset with (Pre-crawl) and without (Default) the pre-crawled databases. b) Amount of core-hours saved by using pre-crawled databases across various public TCIA datasets. c) Time taken to process the liver metastasis dataset manually vs AutoPipeline. d) Time taken to process the RADCURE dataset manually vs AutoPipeline.</title></caption><graphic xlink:href="f1000research-12-177718-g0001" id="gr2" position="float"/></fig></sec><sec id="sec7"><title>Comparing manual vs automatic processing of the liver metastasis dataset</title><p>
<italic toggle="yes">97 patients with abdominal CT scans and their associated contours of liver and gross tumour volumes (GTV), data access described in data availability section below.</italic>
</p><p>The liver metastasis dataset was processed using the Slicer API to export a CT scan along with segmentations of the liver and GTVs for each patient. In the initial DICOM dataset, each patient had a single RTSTRUCT file along with one or more CT series, one of which was referenced by the RTSTRUCT. The first step of the process was to load the entire DICOM dataset into the Slicer DICOM database
<italic toggle="yes">via</italic> the graphical user interface (GUI), to make the data available inside the Slicer scripting environment. Then, we wrote a script to export an initial set of candidate segmentations for the liver and GTVs for each patient, along with the referenced CT scan. This script leveraged the Slicer API to identify the CT series that was referenced by the patient&#x02019;s RTSTRUCT and used an
<italic toggle="yes">ad hoc</italic> string filtering function to identify candidate segmentations. Finally, we iteratively refined the set of exported RTSTRUCT contours on a patient-by-patient basis, based on visual checking, and physician feedback. Although this step was time-consuming, there is, at present, no substitute for manual verification to ensure data quality and correctness. On the other hand, the initial database construction and export script using the Slicer API achieved results roughly analogous to the automatic output of AutoPipeline. The export script is approximately 170 lines long and took 7.5 hours to run on the whole dataset-this script is available on our GitHub repository. In contrast, on the same 6-core machine (16GB RAM, Windows 10), AutoPipeline took only 2.3 hours with 1 command line, mainly accelerated by parallelization (
<xref rid="f2" ref-type="fig">
Figure 2c</xref>). The authors involved in validating Med-ImageTools&#x02019; effectiveness on this private dataset had no active involvement in the development of the package before its application. This highlights the package&#x02019;s robustness on unseen data and its potential utility for multi-centre collaborations to ensure consistent processing. One use case might be to enable federated learning platforms to automatically process each node&#x02019;s datasets without requiring any user intervention.</p></sec><sec id="sec8"><title>Comparing manual vs automatic processing of the RADCURE dataset</title><p>
<italic toggle="yes">3,219 patients with head and neck CT scans and their associated contours of organs at risk (OAR) for radiotherapy, pending public release on TCIA.</italic>
</p><p>The RADCURE dataset is a large dataset of 3,219 head and neck cancer patients and their radiotherapy planning data. The dataset was extracted from two separate treatment planning software systems, meaning the directories and DICOM metadata were structured differently. The directories from each system were restructured using general heuristics, and any abnormal cases were flagged and manually organized. We used SimpleITK and PyDicom to extract the imaging and contour data from the DICOM files, which underwent a similar iterative process as the liver metastatic dataset. The script is over 1000 lines long and takes 30-40 hours to run on the whole dataset using a single core-this script is available on our GitHub repository. AutoPipeline scales dramatically based on the number of cores available, which enables the entire dataset to be processed in 40 minutes using 32 cores, automatically managing the directories from different systems and extracting the contours (
<xref rid="f2" ref-type="fig">
Figure 2d</xref>). These results demonstrate Med-ImageTools&#x02019; design does not bottleneck any multiprocessing backends and brings meaningful acceleration on very large datasets, such as RADCURE.</p><p>One caveat of these comparisons is that the iterative process of filtering appropriate contours, which can add weeks to months of cooperation between the researchers and the physicians, were already conducted in the original processing steps. Various data cleaning steps that require human intervention, such as sorting contour names or selecting specific subseries acquisitions, cannot be fully automated for the foreseeable future. The Med-ImageTools team aims to add features to the package that will assist researchers in these steps, such as adding a flag to visualize all unique contours without requiring code, adding subseries detection to the crawler, and publishing a set of regular expressions (regex) that can be used to automatically choose contours from prominent head and neck datasets on TCIA. Also, these comparisons do not take into account the time it takes for researchers to develop the manual processing scripts. Hence, the actual amount of time saved for the researcher may be greater than the reported times.</p></sec></sec><sec sec-type="discussion" id="sec9"><title>Discussion</title><p>Although ML and AI promise to revolutionize the way we leverage medical imaging data for improving care, they require large datasets to train computational models that can be implemented in clinical practice. However, processing large and complex medical imaging datasets remains an open challenge. To address this issue, we developed Med-ImageTools, a new open-source software package to automate data curation and processing while allowing researchers to share their data processing configurations more easily, lowering the barrier for other researchers to reproduce published works.</p><p>The AutoPipeline feature will improve the accessibility of raw clinical datasets on public archives, such as TCIA, allowing machine learning researchers to process analysis-ready formats without requiring deep domain knowledge. Another exciting potential of Med-ImageTools lies in building automated workflows using AutoPipeline. For a researcher to build an end-to-end automated pipeline starting from clinical DICOM datasets to outputting an inference-ready deep learning model, they could easily develop a reproducible processing step by configuring only the command line interaction of Med-ImageTools, making debugging and custom configurations simpler since the developer would not have to rely on a static script.</p><p>While our package aims to address challenges encountered across a few medical imaging labs, we acknowledge that there may be infinite other issues that may arise in DICOM datasets. This is one of the key reasons why our package is open-source for community involvement and contribution. Also, as stated previously, there are certain onerous tasks that cannot be automated and must undergo human supervision. These aspects of researcher-clinician collaboration are an inevitable part of medical imaging research and are subject to delay.</p><p>No single solution can completely solve the reproducibility crisis of medical deep learning research, due to a variety of issues ranging from ambiguous data processing techniques to stochasticity of model training. However, community-centered open-source solutions and increased clinical adherence to data standards, such as contour nomenclature,
<sup>
<xref rid="ref15" ref-type="bibr">16</xref>
</sup> can incrementally improve research quality and reproducibility, and make medical deep learning research more accessible for everyone.</p></sec></body><back><sec sec-type="data-availability" id="sec13"><title>Data availability</title><p>The Pancreatic-CT-CBCT-SEG dataset is available on The Cancer Imaging Archive at:
<ext-link xlink:href="https://doi.org/10.7937/TCIA.ESHQ-4D90" ext-link-type="uri">https://doi.org/10.7937/TCIA.ESHQ-4D90</ext-link>.</p><p>The pre-crawled Med-ImageTools database of the Pancreatic-CT-CBCT-SEG dataset is available on GitHub at:
<ext-link xlink:href="https://github.com/bhklab/med-imagetools/raw/tcia-crawls/csvs/imgtools_Pancreatic-CT-CBCT-SEG.csv" ext-link-type="uri">https://github.com/bhklab/med-imagetools/raw/tcia-crawls/csvs/imgtools_Pancreatic-CT-CBCT-SEG.csv</ext-link>.</p><p>The liver dataset was collected as part of the study titled
<italic toggle="yes">Radiomics artificial intelligence modelling for prediction of local control for colorectal liver metastases treated with radiotherapy</italic>
<sup>
<xref rid="ref16" ref-type="bibr">17</xref>
</sup> with Institutional Review Board and informed consent. Interested individuals should contact the senior author for access to the dataset, which will be made available following standard institutional rules for IRB and HIPAA.</p><p>The RADCURE dataset is pending public release on TCIA and the article will be updated upon release. Data published on TCIA are subject to TCIA data usage policies and restrictions.</p></sec><sec id="sec10"><title>Software availability</title><p>Source code available from:
<ext-link xlink:href="https://github.com/bhklab/med-imagetools" ext-link-type="uri">github.com/bhklab/med-imagetools
</ext-link> also available on the PyPI software repository:
<ext-link xlink:href="http://pypi.org/project/med-imagetools" ext-link-type="uri">pypi.org/project/med-imagetools</ext-link>.</p><p>Scripts for use cases are available from:
<ext-link xlink:href="http://github.com/bhklab/med-imagetools/tree/F1000Research" ext-link-type="uri">github.com/bhklab/med-imagetools/tree/F1000Research</ext-link>
</p><p>Archived source code at time of publication:
<ext-link xlink:href="https://doi.org/10.5281/zenodo.14766792" ext-link-type="uri">https://doi.org/10.5281/zenodo.14766792</ext-link>
<sup>
<xref rid="ref17" ref-type="bibr">18</xref>
</sup>
</p><p>License:
<ext-link xlink:href="https://opensource.org/licenses/MIT" ext-link-type="uri">MIT</ext-link>
</p><p>Its associated documentation and tutorials are available at:
<ext-link xlink:href="https://bhklab.github.io/med-imagetools" ext-link-type="uri">https://bhklab.github.io/med-imagetools</ext-link>
</p></sec><ref-list><title>References</title><ref id="ref1"><label>1</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Brant</surname><given-names>WE</given-names></name>
<name><surname>Helms</surname><given-names>CA</given-names></name>
</person-group>:
<article-title>Fundamentals of Diagnostic Radiology.</article-title>
<year>2012</year>.</mixed-citation></ref><ref id="ref2"><label>2</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Mildenberger</surname><given-names>P</given-names></name>
<name><surname>Eichelberg</surname><given-names>M</given-names></name>
<name><surname>Martin</surname><given-names>E</given-names></name>
</person-group>:
<article-title>Introduction to the DICOM standard.</article-title>
<source>
<italic toggle="yes">Eur. Radiol.</italic>
</source>
<year>2002</year>;<volume>12</volume>:<fpage>920</fpage>&#x02013;<lpage>927</lpage>.
<pub-id pub-id-type="doi">10.1007/s003300101100</pub-id>
<pub-id pub-id-type="pmid">11960249</pub-id>
</mixed-citation></ref><ref id="ref3"><label>3</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Clark</surname><given-names>K</given-names></name>
<etal/>
</person-group>:
<article-title>The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository.</article-title>
<source>
<italic toggle="yes">J. Digit. Imaging.</italic>
</source>
<year>2013</year>;<volume>26</volume>:<fpage>1045</fpage>&#x02013;<lpage>1057</lpage>.
<pub-id pub-id-type="doi">10.1007/s10278-013-9622-7</pub-id>
<!--<pub-id pub-id-type="pmcid">PMC3824915</pub-id>-->
<pub-id pub-id-type="pmid">23884657</pub-id>
</mixed-citation></ref><ref id="ref4"><label>4</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Law</surname><given-names>MYY</given-names></name>
<name><surname>Liu</surname><given-names>B</given-names></name>
</person-group>:
<article-title>DICOM-RT and Its Utilization in Radiation Therapy.</article-title>
<source>
<italic toggle="yes">RadioGraphics.</italic>
</source>
<year>2009</year>;<volume>29</volume>:<fpage>655</fpage>&#x02013;<lpage>667</lpage>.
<pub-id pub-id-type="doi">10.1148/rg.293075172</pub-id>
<pub-id pub-id-type="pmid">19270073</pub-id>
</mixed-citation></ref><ref id="ref5"><label>5</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Deng</surname><given-names>J</given-names></name>
<etal/>
</person-group>:
<article-title>ImageNet: A large-scale hierarchical image database.</article-title>
<italic toggle="yes">In 2009 IEEE Conference on Computer Vision and Pattern Recognition.</italic>
<year>2009</year>; pp.<fpage>248</fpage>&#x02013;<lpage>255</lpage>.
<pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id>
</mixed-citation></ref><ref id="ref6"><label>6</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Valli&#x000e8;res</surname><given-names>M</given-names></name>
<name><surname>Kay-Rivest</surname><given-names>E</given-names></name>
<etal/>
</person-group>:
<article-title>Data from Head-Neck-PET-CT.</article-title>
<source>
<italic toggle="yes">The Cancer Imaging Archive.</italic>
</source>
<year>2017</year>.
<pub-id pub-id-type="doi">10.7937/K9/TCIA.2017.8oje5q00</pub-id>
</mixed-citation></ref><ref id="ref7"><label>7</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Sabottke</surname><given-names>CF</given-names></name>
<name><surname>Spieler</surname><given-names>BM</given-names></name>
</person-group>:
<article-title>The Effect of Image Resolution on Deep Learning in Radiography.</article-title>
<source>
<italic toggle="yes">Radiol. Artif. Intell.</italic>
</source>
<year>2020</year>;<volume>2</volume>:<fpage>e190015</fpage>.<pub-id pub-id-type="pmid">33937810</pub-id>
</mixed-citation></ref><ref id="ref8"><label>8</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Pinter</surname><given-names>C</given-names></name>
<name><surname>Lasso</surname><given-names>A</given-names></name>
<name><surname>Wang</surname><given-names>A</given-names></name>
<etal/>
</person-group>:
<article-title>SlicerRT: radiation therapy research toolkit for 3D Slicer.</article-title>
<source>
<italic toggle="yes">Med. Phys.</italic>
</source>
<year>2012</year>;<volume>39</volume>:<fpage>6332</fpage>&#x02013;<lpage>6338</lpage>.
<pub-id pub-id-type="doi">10.1118/1.4754659</pub-id>
<pub-id pub-id-type="pmid">23039669</pub-id>
</mixed-citation></ref><ref id="ref9"><label>9</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Fedorov</surname><given-names>A</given-names></name>
<etal/>
</person-group>:
<article-title>3D Slicer as an image computing platform for the Quantitative Imaging Network.</article-title>
<source>
<italic toggle="yes">Magn. Reson. Imaging.</italic>
</source>
<year>2012</year>;<volume>30</volume>:<fpage>1323</fpage>&#x02013;<lpage>1341</lpage>.
<pub-id pub-id-type="doi">10.1016/j.mri.2012.05.001</pub-id>
<!--<pub-id pub-id-type="pmcid">PMC3466397</pub-id>-->
<pub-id pub-id-type="pmid">22770690</pub-id>
</mixed-citation></ref><ref id="ref10"><label>10</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<name><surname>Yli-Heikkil&#x000e4;</surname><given-names>M</given-names></name>
</person-group>:
<part-title>Data Science Languages.</part-title>
<source>
<italic toggle="yes">DSNS '19.</italic>
</source>
<publisher-name>University of Helsinki</publisher-name>;<year>2019</year>.</mixed-citation></ref><ref id="ref11"><label>11</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Kazmierski</surname><given-names>M</given-names></name>
<name><surname>Kim</surname><given-names>S</given-names></name>
<name><surname>Ramanathan</surname><given-names>V</given-names></name>
<etal/>
</person-group>:
<article-title>bhklab/med-imagetools: v4.4.0.</article-title>
<source>
<italic toggle="yes">Zenodo.</italic>
</source>
<year>2022</year>.
<pub-id pub-id-type="doi">10.5281/ZENODO.7021436</pub-id>
</mixed-citation></ref><ref id="ref18"><label>12</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Gorgolewski</surname><given-names>K</given-names></name>
<name><surname>Auer</surname><given-names>T</given-names></name>
<name><surname>Calhoun</surname><given-names>VD</given-names></name>
<etal/>
</person-group>:
<article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments.</article-title>
<source>
<italic toggle="yes">Sci. Data.</italic>
</source>
<year>2016</year>;<volume>3</volume>: 160044.
<pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id>
<!--<pub-id pub-id-type="pmcid">PMC4978148</pub-id>-->
<pub-id pub-id-type="pmid">27326542</pub-id>
</mixed-citation></ref><ref id="ref12"><label>13</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Camp</surname><given-names>B</given-names></name>
</person-group>:
<article-title>Pancreatic-CT-CBCT-SEG - The Cancer Imaging Archive (TCIA) Public Access - Cancer Imaging Archive Wiki.</article-title>
<pub-id pub-id-type="doi">10.7937/TCIA.ESHQ-4D90</pub-id>
</mixed-citation></ref><ref id="ref13"><label>14</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Kwan</surname><given-names>JYY</given-names></name>
<etal/>
</person-group>:
<article-title>Radiomic Biomarkers to Refine Risk Models for Distant Metastasis in HPV-related Oropharyngeal Carcinoma.</article-title>
<source>
<italic toggle="yes">Int. J. Radiat. Oncol. Biol. Phys.</italic>
</source>
<year>2018</year>;<volume>102</volume>:<fpage>1107</fpage>&#x02013;<lpage>1116</lpage>.
<pub-id pub-id-type="doi">10.1016/j.ijrobp.2018.01.057</pub-id>
<pub-id pub-id-type="pmid">29506884</pub-id>
</mixed-citation></ref><ref id="ref14"><label>15</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Clark</surname><given-names>K</given-names></name>
</person-group>:
<article-title>National Lung Screening Trial.</article-title>
<pub-id pub-id-type="doi">10.7937/TCIA.HMQ8-J677</pub-id>
</mixed-citation></ref><ref id="ref15"><label>16</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Mayo</surname><given-names>CS</given-names></name>
<etal/>
</person-group>:
<article-title>American Association of Physicists in Medicine Task Group 263: Standardizing Nomenclatures in Radiation Oncology.</article-title>
<source>
<italic toggle="yes">Int. J. Radiat. Oncol. Biol. Phys.</italic>
</source>
<year>2018</year>;<volume>100</volume>:<fpage>1057</fpage>&#x02013;<lpage>1066</lpage>.
<pub-id pub-id-type="doi">10.1016/j.ijrobp.2017.12.013</pub-id>
<!--<pub-id pub-id-type="pmcid">PMC7437157</pub-id>-->
<pub-id pub-id-type="pmid">29485047</pub-id>
</mixed-citation></ref><ref id="ref16"><label>17</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<name><surname>Hu</surname><given-names>R</given-names></name>
<name><surname>Chen</surname><given-names>I</given-names></name>
<etal/>
</person-group>:
<article-title>Radiomics artificial intelligence modelling for prediction of local control for colorectal liver metastases treated with radiotherapy.</article-title>
<source>
<italic toggle="yes">Phys. Imaging Radiat. Oncol.</italic>
</source>
<year>2022</year>;<volume>24</volume>:<fpage>36</fpage>&#x02013;<lpage>42</lpage>.
<pub-id pub-id-type="doi">10.1016/j.phro.2022.09.004</pub-id>
<!--<pub-id pub-id-type="pmcid">PMC9485899</pub-id>-->
<pub-id pub-id-type="pmid">36148155</pub-id>
</mixed-citation></ref><ref id="ref17"><label>18</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<name><surname>Cranmer</surname><given-names>K</given-names></name>
<name><surname>Kreiss</surname><given-names>S</given-names></name>
</person-group>:
<article-title>decouple software associated to arXiv:1401.0080.</article-title>
<source>
<italic toggle="yes">[Code] Zenodo.</italic>
</source>
<year>2014</year>.
<pub-id pub-id-type="doi">10.5281/zenodo.14766792</pub-id>
</mixed-citation></ref></ref-list></back><sub-article article-type="reviewer-report" id="report366375"><front-stub><article-id pub-id-type="doi">10.5256/f1000research.177718.r366375</article-id><title-group><article-title>Reviewer response for version 3</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Faouzi</surname><given-names>Johann</given-names></name><xref rid="r366375a1" ref-type="aff">1</xref><role>Referee</role><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0542-9968</contrib-id></contrib><aff id="r366375a1">
<label>1</label>ENSAI, Rennes, France</aff></contrib-group><author-notes><fn fn-type="COI-statement"><p>
<bold>Competing interests: </bold>No competing interests were disclosed.</p></fn></author-notes><pub-date pub-type="epub"><day>22</day><month>2</month><year>2025</year></pub-date><permissions><copyright-statement>Copyright: &#x000a9; 2025 Faouzi J</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access peer review report distributed under the terms of the Creative Commons Attribution Licence, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><related-article related-article-type="peer-reviewed-article" ext-link-type="doi" id="relatedArticleReport366375" xlink:href="10.12688/f1000research.127142.3">Version 3</related-article><custom-meta-group><custom-meta><meta-name>recommendation</meta-name><meta-value>approve</meta-value></custom-meta></custom-meta-group></front-stub><body><p>I would like to thank the authors for their detailed response. The software development aspects have been substantially improved since the previous version, and I am happy to approve the current version of the article.</p><p>Are the conclusions about the tool and its performance adequately supported by the findings presented in the article?</p><p>Yes</p><p>Is the rationale for developing the new software tool clearly explained?</p><p>Yes</p><p>Is the description of the software tool technically sound?</p><p>Yes</p><p>Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others?</p><p>Yes</p><p>Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool?</p><p>Yes</p><p>Reviewer Expertise:</p><p>Open source software, Python programming, machine learning, medical applications</p><p>I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard.</p></body></sub-article><sub-article article-type="reviewer-report" id="report274815"><front-stub><article-id pub-id-type="doi">10.5256/f1000research.164713.r274815</article-id><title-group><article-title>Reviewer response for version 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sparks</surname><given-names>Rachel</given-names></name><xref rid="r274815a1" ref-type="aff">1</xref><role>Referee</role><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1553-7903</contrib-id></contrib><aff id="r274815a1">
<label>1</label>King's College London, London, England, UK</aff></contrib-group><author-notes><fn fn-type="COI-statement"><p>
<bold>Competing interests: </bold>No competing interests were disclosed.</p></fn></author-notes><pub-date pub-type="epub"><day>12</day><month>6</month><year>2024</year></pub-date><permissions><copyright-statement>Copyright: &#x000a9; 2024 Sparks R</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access peer review report distributed under the terms of the Creative Commons Attribution Licence, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><related-article related-article-type="peer-reviewed-article" ext-link-type="doi" id="relatedArticleReport274815" xlink:href="10.12688/f1000research.127142.2">Version 2</related-article><custom-meta-group><custom-meta><meta-name>recommendation</meta-name><meta-value>approve</meta-value></custom-meta></custom-meta-group></front-stub><body><p>I am happy with the revisions made to the document and responses to my specific queries.</p><p>Are the conclusions about the tool and its performance adequately supported by the findings presented in the article?</p><p>Yes</p><p>Is the rationale for developing the new software tool clearly explained?</p><p>Yes</p><p>Is the description of the software tool technically sound?</p><p>No</p><p>Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others?</p><p>No</p><p>Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool?</p><p>No</p><p>Reviewer Expertise:</p><p>Software development; open source software; medical image computing</p><p>I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard.</p></body></sub-article><sub-article article-type="reviewer-report" id="report274816"><front-stub><article-id pub-id-type="doi">10.5256/f1000research.164713.r274816</article-id><title-group><article-title>Reviewer response for version 2</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Faouzi</surname><given-names>Johann</given-names></name><xref rid="r274816a1" ref-type="aff">1</xref><role>Referee</role><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0542-9968</contrib-id></contrib><aff id="r274816a1">
<label>1</label>ENSAI, Rennes, France</aff></contrib-group><author-notes><fn fn-type="COI-statement"><p>
<bold>Competing interests: </bold>No competing interests were disclosed.</p></fn></author-notes><pub-date pub-type="epub"><day>15</day><month>5</month><year>2024</year></pub-date><permissions><copyright-statement>Copyright: &#x000a9; 2024 Faouzi J</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access peer review report distributed under the terms of the Creative Commons Attribution Licence, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><related-article related-article-type="peer-reviewed-article" ext-link-type="doi" id="relatedArticleReport274816" xlink:href="10.12688/f1000research.127142.2">Version 2</related-article><custom-meta-group><custom-meta><meta-name>recommendation</meta-name><meta-value>approve-with-reservations</meta-value></custom-meta></custom-meta-group></front-stub><body><p>I would like to thank the authors for replying to all my comments. I think that the manuscript has been improved since the first version.</p><p> Nonetheless, I still have some concerns regarding the source code. Despite some cleaning, there are still numerous flake8 errors. Flake8 is not used in the CI even though it is installed. Code coverage is still not included in the CI. PR with failing CI are still merged into in the main branch. Overall, the software aspect of this contribution, which is the main contribution, has still important limitations.</p><p>Are the conclusions about the tool and its performance adequately supported by the findings presented in the article?</p><p>Yes</p><p>Is the rationale for developing the new software tool clearly explained?</p><p>Yes</p><p>Is the description of the software tool technically sound?</p><p>Yes</p><p>Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others?</p><p>Yes</p><p>Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool?</p><p>Yes</p><p>Reviewer Expertise:</p><p>Open source software, Python programming, machine learning, medical applications</p><p>I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above.</p></body><sub-article article-type="response" id="comment11676-274816"><front-stub><contrib-group><contrib contrib-type="author"><name><surname>Haibe-Kains</surname><given-names>Benjamin</given-names></name><aff>Princess Margaret Cancer Centre, Canada</aff></contrib></contrib-group><author-notes><fn fn-type="COI-statement"><p>
<bold>Competing interests: </bold>No competing interests were disclosed.</p></fn></author-notes><pub-date pub-type="epub"><day>29</day><month>5</month><year>2024</year></pub-date></front-stub><body><p>Thank you Dr Faouzi for your feedback regarding our CI/CD pipeline and we are glad to inform you that we have made the appropriate updates.&#x000a0;</p><p> We have switched from flake8 to
<ext-link xlink:href="https://github.com/astral-sh/ruff" ext-link-type="uri"> ruff</ext-link> (Rust-based Python linter) as our main linter and have made appropriate modifications to our codebase to address all linting issues. The linter is also integrated into the CI/CD pipeline as well.&#x000a0;</p><p> The main failure of our previous tests was due to the deprecation of .egg uploads to PyPI, despite the unit tests always passing. We have resolved these issues by rewriting our CI/CD pipeline, which has led to reliable tests and continuous deployments to PyPI.&#x000a0;</p><p> We have also incorporated code coverage into our CI/CD pipeline and have added a badge to the README. While the current code coverage of the tests is still low, we believe after our upcoming refactoring efforts, the coverage of the tests will significantly improve.&#x000a0;</p><p> Finally, we have implemented semantic versioning to our package to adhere to industry standards and ensure consistent, trackable versioning changes.</p></body></sub-article></sub-article><sub-article article-type="reviewer-report" id="report189038"><front-stub><article-id pub-id-type="doi">10.5256/f1000research.139617.r189038</article-id><title-group><article-title>Reviewer response for version 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Faouzi</surname><given-names>Johann</given-names></name><xref rid="r189038a1" ref-type="aff">1</xref><role>Referee</role><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0542-9968</contrib-id></contrib><aff id="r189038a1">
<label>1</label>ENSAI, Rennes, France</aff></contrib-group><author-notes><fn fn-type="COI-statement"><p>
<bold>Competing interests: </bold>No competing interests were disclosed.</p></fn></author-notes><pub-date pub-type="epub"><day>17</day><month>10</month><year>2023</year></pub-date><permissions><copyright-statement>Copyright: &#x000a9; 2023 Faouzi J</copyright-statement><copyright-year>2023</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access peer review report distributed under the terms of the Creative Commons Attribution Licence, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><related-article related-article-type="peer-reviewed-article" ext-link-type="doi" id="relatedArticleReport189038" xlink:href="10.12688/f1000research.127142.1">Version 1</related-article><custom-meta-group><custom-meta><meta-name>recommendation</meta-name><meta-value>approve-with-reservations</meta-value></custom-meta></custom-meta-group></front-stub><body><p>The authors present their Python package for data processing pipelines and curating medical imaging data in this manuscript. Overall the manuscript is well written and clearly presents the main contributions of this package to the community. Nonetheless, some points on the software side could be improved. Please find my comments below:</p><p> * How does the output format used compare to other formats such as other popular formats such as BIDS (Brain Imaging Data Structure, which is maybe less general since it's intended for brain imaging data)?</p><p> * Figure 2b (and the corresponding text): Raw saved running time, even though useful, is irrelevant without knowing the total running time. Please provide also the saved time as a percentage of the total running time.</p><p> * Figures 2c and 2d: "1 lines" =&#x0003e; "1 line"</p><p> * The authors should consider renaming the "master" branch as the "main" branch (see https://sfconservancy.org/news/2020/jun/23/gitbranchname/).</p><p> * Is there a reason for Python 3.8 not being tested in the continuous integration (CI) (maybe a limit on the number of jobs)? By the way, Python 3.7 is no longer supported and Python 3.11 has been released for over 9 months (https://devguide.python.org/versions/). The Python versions tested in the CI should probably be updated.</p><p> * Flake8 is installed in the CI but never used. I ran the command locally (flake8 | wc -l) and obtained 2163 errors or warnings, which is not good. It would be nice to clean the source code a bit.</p><p> * There is no code coverage associated to the tests, which is an important limitation. Please add code coverage to your project. There are "only" 16 tests for the moment, so I suspect that a substantial part of the source code is not tested.</p><p> * I don't understand the point of using the "-s" option with pytest. I don't find the output (when using this option) to be clear or useful to identify tests that would fail for instance. In particular, I don't think that printing stuff when running the tests in the CI is relevant.</p><p>Are the conclusions about the tool and its performance adequately supported by the findings presented in the article?</p><p>Yes</p><p>Is the rationale for developing the new software tool clearly explained?</p><p>Yes</p><p>Is the description of the software tool technically sound?</p><p>Yes</p><p>Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others?</p><p>Yes</p><p>Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool?</p><p>Yes</p><p>Reviewer Expertise:</p><p>Open source software, Python programming, machine learning, medical applications</p><p>I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above.</p></body><sub-article article-type="response" id="comment11385-189038"><front-stub><contrib-group><contrib contrib-type="author"><name><surname>Haibe-Kains</surname><given-names>Benjamin</given-names></name><aff>Princess Margaret Cancer Centre, Canada</aff></contrib></contrib-group><author-notes><fn fn-type="COI-statement"><p>
<bold>Competing interests: </bold>There are no competing interests to disclose.</p></fn></author-notes><pub-date pub-type="epub"><day>5</day><month>4</month><year>2024</year></pub-date></front-stub><body><p>We thank Dr Faouzi for his feedback regarding our manuscript titled &#x02018;Med-ImageTools: An open-source Python package for robust data processing pipelines and curating medical imaging data&#x02019;. We have added our responses to their comments here in subdivided sections of comments/questions (Q), in bold, our corresponding responses/answers (A), and any relevant sections of the manuscript as an italicized bullet point, with changes emphasized in bold.&#x000a0;</p><p> Q: The authors present their Python package for data processing pipelines and curating medical imaging data in this manuscript. Overall the manuscript is well written and clearly presents the main contributions of this package to the community. Nonetheless, some points on the software side could be improved. Please find my comments below:</p><p>
<bold>A: We thank the reviewer for their positive understanding of the authors&#x02019; work and constructive feedback, especially on the technical aspects of the package.</bold>
</p><p> Q: How does the output format used compare to other formats such as other popular formats such as BIDS (Brain Imaging Data Structure, which is maybe less general since it's intended for brain imaging data)?</p><p>
<bold>A: We thank the reviewer for bringing the BIDS standard to the authors&#x02019; attention. The BIDS standard is very effective in standardizing brain imaging datasets by adhering to an intuitive structure describing various modalities of brain MRI. Med-ImageTools is mainly geared towards use cases that are broader and less specific than BIDS and its implementation would require fundamental reworkings of the package. However, the benefit of having a BIDS-native output cannot be understated and this feature will be added to the development team&#x02019;s ongoing development goals. We have revised the relevant section from &#x02018;Current Workflows&#x02019; as follows:</bold>
<list list-type="bullet"><list-item><p>
<italic toggle="yes">While Med-ImageTools has many modular functions for image, contour, and dose input/output (IO) built on popular frameworks such as SimpleITK, TorchIO and PyDicom, such functionalities are redundant and available in other open-source packages as well.
<bold>We have tailored our core features towards broader use cases and development workflows, instead of modality or disease type specific workflows, such as BIDS. </bold>Our main contribution is in the development of AutoPipeline and will mainly discuss its functionalities and implementation.&#x000a0;</italic>
</p></list-item></list>
</p><p> Q: Figure 2b (and the corresponding text): Raw saved running time, even though useful, is irrelevant without knowing the total running time. Please provide also the saved time as a percentage of the total running time.</p><p>
<bold>A: We appreciate the reviewer&#x02019;s point about Figure 2b. However, the reported time saved is reported in core-hours, which is a comprehensive unit of total running time. This is because the crawling occurred independently from the end-to-end processing of the datasets to ensure an accurate measure of core-hours saved. Reporting core-hours saved allows the reader to estimate their time/cost savings based on their specific hardware availabilities, either local machine, server-based, or cloud infrastructure, agnOSTIC to the hardware used by the authors. We have emphasIZED this point in our manuscRIPT as fOLLOWS:</bold>
<list list-type="bullet"><list-item><p>
<italic toggle="yes">However, when scaled up to larger TCIA datasets such as OPC-Radiomics 13 (n=606) and NLST 14 (n=26254), it can save researchers 24.7 core-hours and 1072 core-hours, respectively (Figure 2b).
<bold>The resources saved are reported in core-hours to allow a hardware-agnostic estimation of time and cost savings. </bold>While it may vary depending on the research infrastructure utilized, these databases can result in significant savings in billing.&#x000a0;</italic>
</p></list-item></list> Q: Figures 2c and 2d: "1 lines" =&#x0003e; "1 line"</p><p> Q: The authors should consider renaming the "master" branch as the "main" branch (see https://sfconservancy.org/news/2020/jun/23/gitbranchname/).</p><p> Q: Is there a reason for Python 3.8 not being tested in the continuous integration (CI) (maybe a limit on the number of jobs)? By the way, Python 3.7 is no longer supported and Python 3.11 has been released for over 9 months (https://devguide.python.org/versions/). The Python versions tested in the CI should probably be updated.</p><p>
<bold>A: We thank the reviewer for their comments. The figures and repository have been updated to address these points.</bold>
</p><p> Q:&#x000a0;Flake8 is installed in the CI but never used. I ran the command locally (flake8 | wc -l) and obtained 2163 errors or warnings, which is not good. It would be nice to clean the source code a bit.</p><p>
<bold>A: We thank the reviewer for raising the concern about the source code&#x02019;s sanitation. The authors have reviewed the flake8 output and incorporated it into our codebase. You can find all changes in the following commit:
<ext-link xlink:href="https://github.com/bhklab/med-imagetools/commit/a8c0c3be53b4c9b43da12603b222d261d417a18c" ext-link-type="uri">https://github.com/bhklab/med-imagetools/commit/a8c0c3be53b4c9b43da12603b222d261d417a18c</ext-link>.&#x000a0;</bold>
</p><p> Q: I don't understand the point of using the "-s" option with pytest. I don't find the output (when using this option) to be clear or useful to identify tests that would fail for instance. In particular, I don't think that printing stuff when running the tests in the CI is relevant.</p><p>
<bold>A: We thank the reviewer&#x02019;s attention to detail in Med-ImageTools&#x02019; CI/CD framework. The -s option is enabled for verbose outputs from Med-ImageTools to be carried through to the standard output of the tests in CI. In the past we have had OS-specific issues that were elucidated through the -s flag, but have removed it for future releases.</bold>
</p><p> Q: There is no code coverage associated to the tests, which is an important limitation. Please add code coverage to your project. There are "only" 16 tests for the moment, so I suspect that a substantial part of the source code is not tested.</p><p>
<bold>A: We thank the reviewer for highlighting the lack of code coverage association of these tests. We agree that this is a clear limitation of the Med-ImageTools&#x02019; development pipeline, and have significantly increased the number of unit tests and integrated them into our CI/CD process. Future development efforts will continue to add more tests to our pipeline, including a badge detailing code coverage on our README.&#x000a0;</bold>
</p></body></sub-article></sub-article><sub-article article-type="reviewer-report" id="report189015"><front-stub><article-id pub-id-type="doi">10.5256/f1000research.139617.r189015</article-id><title-group><article-title>Reviewer response for version 1</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sparks</surname><given-names>Rachel</given-names></name><xref rid="r189015a1" ref-type="aff">1</xref><role>Referee</role><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1553-7903</contrib-id></contrib><aff id="r189015a1">
<label>1</label>King's College London, London, England, UK</aff></contrib-group><author-notes><fn fn-type="COI-statement"><p>
<bold>Competing interests: </bold>No competing interests were disclosed.</p></fn></author-notes><pub-date pub-type="epub"><day>17</day><month>10</month><year>2023</year></pub-date><permissions><copyright-statement>Copyright: &#x000a9; 2023 Sparks R</copyright-statement><copyright-year>2023</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open access peer review report distributed under the terms of the Creative Commons Attribution Licence, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><related-article related-article-type="peer-reviewed-article" ext-link-type="doi" id="relatedArticleReport189015" xlink:href="10.12688/f1000research.127142.1">Version 1</related-article><custom-meta-group><custom-meta><meta-name>recommendation</meta-name><meta-value>approve-with-reservations</meta-value></custom-meta></custom-meta-group></front-stub><body><p>I think the motivation and justification for why a light weight Python module to support data loading and preprocessing of radiotherapy DICOM formats was very well articulated and clear. And I applaud the authors on making such a useful tool available to the community.</p><p> However, I have two major concerns that must be addressed prior to this article being acceptable for indexing.</p><p> First there are no details as to the design of Med-Image-Tools this includes important information such as:</p><p> (1) dependencies on other python packages</p><p> (2) what functions or other tools are being leveraged within the tool and how these are arranged.</p><p> (3) the structure of the outputs of this pipeline specifically the csv file corresponding to the 'Dataset Index', the imaging and non-imaging feature outputs. For instance for imaging data how are pre-operative scans and segmentations saved and named (is there a convention is this something the user must write)</p><p> (4) While there is a reference to the tutorial about the processing pipeline I think a high level overview listing the possible options would be appropriate to help the reader understand what this tool can offer.</p><p> My other concern is related to the use cases. While lines of code is an easy metric to report it is mostly meaningless, some lines of code may be boilerplate or easy to write while others may obviously be different. It is also easy to pad or alter depending on the level of skill of the programmer. I think reporting run time is far more meaningful to your argument and I would stick to these.</p><p> Finally, specifically, for the RADCURE dataset, it is not clear to me why you are comparing run time on a single core versus run time on a 32 core machine. I suspect the reported run time for the comparison could be significantly reduced by using a very simple parallelization which should be achievable tools available in python. I would suggest a fairer comparison for this specific example, it wont detract from the best argument you have which is there you are presenting a nice package so that other developers and researchers do not have to spend time re-writing code. Highlighting where some time savings may be gained is in some ways a secondary advantage.</p><p> Finally, as a minor note the statement that SlicerRT is not easily interfaced with python is not a fully justified statement. One can use the python interface in Slicer to run python scripts and even run from the command prompt by called s'licer.exe&#x000a0; --python-code'. However, I think you can modify this to make a true and equally motivated statement which is that SlicerRT requires one to install Slicer/SlicerRT and stay within the slicer ecosystem to process data. Your tool lacks such dependencies so can be a more light weight and does not require running python through another software system.</p><p>Are the conclusions about the tool and its performance adequately supported by the findings presented in the article?</p><p>Yes</p><p>Is the rationale for developing the new software tool clearly explained?</p><p>Yes</p><p>Is the description of the software tool technically sound?</p><p>No</p><p>Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others?</p><p>No</p><p>Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool?</p><p>No</p><p>Reviewer Expertise:</p><p>Software development; open source software; medical image computing</p><p>I confirm that I have read this submission and believe that I have an appropriate level of expertise to confirm that it is of an acceptable scientific standard, however I have significant reservations, as outlined above.</p></body><sub-article article-type="response" id="comment11384-189015"><front-stub><contrib-group><contrib contrib-type="author"><name><surname>Haibe-Kains</surname><given-names>Benjamin</given-names></name><aff>Princess Margaret Cancer Centre, Canada</aff></contrib></contrib-group><author-notes><fn fn-type="COI-statement"><p>
<bold>Competing interests: </bold>There are no competing interests to disclose.</p></fn></author-notes><pub-date pub-type="epub"><day>5</day><month>4</month><year>2024</year></pub-date></front-stub><body><p>We thank Dr Sparks for her feedback regarding our manuscript titled &#x02018;Med-ImageTools: An open-source Python package for robust data processing pipelines and curating medical imaging data&#x02019;. We have added our responses to their comments here in subdivided sections of comments/questions (Q), in bold, our corresponding responses/answers (A), and any relevant sections of the manuscript as an italicized bullet point, with changes emphasized in bold.&#x000a0;</p><p> Q: I think the motivation and justification for why a light weight Python module to support data loading and preprocessing of radiotherapy DICOM formats was very well articulated and clear. And I applaud the authors on making such a useful tool available to the community.</p><p>
<bold>A: We thank the reviewer for their recognition of Med-ImageTools&#x02019; contribution to the medical imaging community and positively evaluating the authors&#x02019; works.</bold>
</p><p> Q: However, I have two major concerns that must be addressed prior to this article being acceptable for indexing.</p><p> First there are no details as to the design of Med-Image-Tools this includes important information such as:</p><p> (1) dependencies on other python packages</p><p>
<bold>A: We thank the reviewer for pointing out the need to communicate the technical dependencies of Med-ImageTools. We have updated our &#x02018;AutoPipeline&#x02019; section as follows, to include information on how to find the list of package dependencies.&#x000a0;</bold>
<list list-type="bullet"><list-item><p>
<italic toggle="yes">Running the &#x02018; pip install med-imagetools&#x02019; command will install the latest version of Med-ImageTools and its associated dependencies
<bold>defined by the requirements.txt file.</bold>
</italic>
</p></list-item></list>
<bold>The &#x02018;Current workflows&#x02019; section also contains a concise description of what frameworks Med-ImageTools leverages such as follows:</bold>
<list list-type="bullet"><list-item><p>
<italic toggle="yes">While Med-ImageTools has many modular functions for image, contour, and dose input/output (IO) built on popular frameworks such as SimpleITK, TorchIO and PyDicom&#x02026;</italic>
</p></list-item></list>
</p><p> Q: (2) what functions or other tools are being leveraged within the tool and how these are arranged.</p><p>
<bold>A: We thank the reviewer for calling attention to the need to clarify how other tools/packages function within Med-ImageTools. We have added some information to our &#x02018;AutoPipeline&#x02019; section to clarify major operations performed using external tools/packages as follows. For a granular understanding of the integration behind every external tool, we defer the reader to learn about the design choices made throughout the package and its implications by gaining a deeper understanding of the source code available through our GitHub repository.&#x000a0;</bold>
<list list-type="bullet"><list-item><p>
<italic toggle="yes">1. Crawl: The crawler opens every DICOM file in the dataset using Pydicom, indexing important metadata, such as unique identifiers, modality information, and references to other modalities. This produces a database of every unique image and DICOM-RT modality.</italic>
</p></list-item><list-item><p>
<italic toggle="yes">3. Process: All identified samples are processed according to imaging and transformation parameters defined by the user. The user can configure parameters such as the pixel spacing in mm(s), which specific modalities to process, the number of cores they want to use for multiprocessing, and define nnU-Net specific flags as well.
<bold>The images are manipulated using SimpleITK without requiring user intervention.&#x000a0;</bold>
</italic>
</p></list-item><list-item><p>
<italic toggle="yes">As the crawl and process steps are computationally intensive, all steps in AutoPipeline are automatically parallelized
<bold>using the joblib backend</bold> to efficiently leverage all available computational resources.</italic>
</p></list-item></list>
</p><p> Q: (3) the structure of the outputs of this pipeline specifically the csv file corresponding to the 'Dataset Index', the imaging and non-imaging feature outputs. For instance for imaging data how are pre-operative scans and segmentations saved and named (is there a convention is this something the user must write)</p><p>
<bold>A: We thank the reviewer for this question and for highlighting the lack of clarity of our description of AutoPipeline. The &#x02018;Dataset Index&#x02019; is designed to be an autogenerated output of the Crawler and does not require any user interaction to be created; this is simply there to ensure convenient user experience. We have made this and other aspects of the outputs clearer by adding the following sentences to the &#x02018;AutoPipeline&#x02019; section.</bold>
<list list-type="bullet"><list-item><p>
<bold>
<italic toggle="yes">Med-ImageTools also generates files that are not analysis-ready images, such as the autogenerated &#x02018;Dataset Index&#x02019; (Figure 1), and stores them in a folder named &#x0201c;.imgtools&#x0201d; at the path of the &#x02018;input_directory&#x02019;. This is to ensure a convenient user experience and intuitive folder structure by hiding extraneous components.</italic>
</bold>
</p></list-item></list>
</p><p> Q: (4) While there is a reference to the tutorial about the processing pipeline I think a high level overview listing the possible options would be appropriate to help the reader understand what this tool can offer.</p><p>
<bold>A: We agree with the reviewer that adding this information to our description of AutoPipeline can help readers quickly understand the tool&#x02019;s strengths before interfacing with the GitHub repository or ReadTheDocs. We have updated the &#x02018;AutoPipeline&#x02019; section accordingly.</bold>
<list list-type="bullet"><list-item><p>
<italic toggle="yes">3. Process: All identified samples are processed according to imaging and transformation parameters defined by the user.
<bold> The user can configure parameters such as: the pixel spacing in mm(s), which specific modalities to process, the number of cores they want to use for multiprocessing, and define nnU-Net specific flags as well. The images are manipulated using SimpleITK without requiring user intervention.&#x000a0;</bold>
</italic>
</p></list-item></list>
</p><p> Q: My other concern is related to the use cases. While lines of code is an easy metric to report it is mostly meaningless, some lines of code may be boilerplate or easy to write while others may obviously be different. It is also easy to pad or alter depending on the level of skill of the programmer. I think reporting run time is far more meaningful to your argument and I would stick to these.</p><p>
<bold>A: We appreciate the reviewer&#x02019;s consideration in the metrics reported in Figure 2. However, we believe this metric emphasizes Med-ImageTools&#x02019; strength allowing users to process DICOM datasets with a single command line interaction, in contrast to writing dozens of lines of Python/R/MATLAB scripts. One particular reason why we believe a single command line is useful to highlight is to express the ease-of-use for users that may not be comfortable writing their own code for DICOM to NifTI/Nrrd processing, especially if they lack deep domain knowledge.&#x000a0;</bold>
</p><p>
<bold>Another, perhaps more, exciting reason is to show the potential of building automated workflows on top of Med-ImageTools. If one were to build an end-to-end automated pipeline starting from clinical DICOM datasets to outputting an inference-ready deep learning model, they could easily develop a reproducible processing step by configuring only the command line interaction of Med-ImageTools, making debugging and custom configurations simpler since the developer would not have to rely on a static script. We have elaborated on Med-ImageTools&#x02019; contributions for improved scientific reproducibility in the &#x02018;Discussion&#x02019; section as follows.&#x000a0;</bold>
<list list-type="bullet"><list-item><p>
<italic toggle="yes">The AutoPipeline feature will improve the accessibility of raw clinical datasets on public archives, such as TCIA, allowing machine learning researchers to process analysis-ready formats without requiring deep domain knowledge.
<bold>Another exciting potential of Med-ImageTools lies in building automated workflows using AutoPipeline. For a researcher to build an end-to-end automated pipeline starting from clinical DICOM datasets to outputting an inference-ready deep learning model, they could easily develop a reproducible processing step by configuring only the command line interaction of Med-ImageTools, making debugging and custom configurations simpler since the developer would not have to rely on a static script.&#x000a0;</bold>
</italic>
</p></list-item><list-item><p>
<italic toggle="yes">No single solution can completely solve the reproducibility crisis of medical deep learning research, due to a variety of issues ranging from ambiguous data processing techniques to stochasticity of model training. However, community-centered open-source solutions and increased clinical adherence to data standards, such as contour nomenclature,
<ext-link xlink:href="https://f1000research.com/articles/12-118#ref15" ext-link-type="uri">15</ext-link> can incrementally improve research quality and reproducibility, and make medical deep learning research more accessible for everyone.</italic>
</p></list-item></list>
</p><p> Q: Finally, specifically, for the RADCURE dataset, it is not clear to me why you are comparing run time on a single core versus run time on a 32 core machine. I suspect the reported run time for the comparison could be significantly reduced by using a very simple parallelization which should be achievable tools available in python. I would suggest a fairer comparison for this specific example, it wont detract from the best argument you have which is there you are presenting a nice package so that other developers and researchers do not have to spend time re-writing code. Highlighting where some time savings may be gained is in some ways a secondary advantage.</p><p>
<bold>A: We thank the reviewer for their comment about Figure 2d. The goal of the comparison was not only to show a reduction in processing time based on parallelization, but to show that our package&#x02019;s design does not bottleneck any multiprocessing backends and brings meaningful acceleration on large datasets. In other words, we wanted to show that a 32x increase in computational hardware was going to bring at least 32x improvement in performance as observed in Figure 2d. This is in contrast to Figure 2c which does not show similar levels hardware acceleration, but its usefulness in parsing private, unseen data. We have expanded on these ideas in the manuscript as follows:</bold>
<list list-type="bullet"><list-item><p>
<italic toggle="yes">In contrast, on the same 6-core machine (16GB RAM, Windows 10), AutoPipeline took only 2.3 hours with 1 command line, mainly accelerated by parallelization ( Figure 2c).
<bold> The authors involved in validating Med-ImageTools' effectiveness on this private dataset had no active involvement in the development of the package before its application. This highlights the package&#x02019;s robustness on unseen data and its potential utility for multi-centre collaborations to ensure consistent processing. One use case might be to enable federated learning platforms to automatically process each node&#x02019;s datasets without requiring any user intervention.</bold>
</italic>
</p></list-item><list-item><p>
<italic toggle="yes">AutoPipeline scales dramatically based on the number of cores available, which enables the entire dataset to be processed in 40 minutes using 32 cores, automatically managing the directories from different systems and extracting the contours ( Figure 2d).
<bold>These results demonstrate Med-ImageTools' design does not bottleneck any multiprocessing backends and brings meaningful acceleration on very large datasets, such as RADCURE.&#x000a0;</bold>
</italic>
</p></list-item></list>
</p><p> Q: Finally, as a minor note the statement that SlicerRT is not easily interfaced with python is not a fully justified statement. One can use the python interface in Slicer to run python scripts and even run from the command prompt by called s'licer.exe&#x000a0; --python-code'. However, I think you can modify this to make a true and equally motivated statement which is that SlicerRT requires one to install Slicer/SlicerRT and stay within the slicer ecosystem to process data. Your tool lacks such dependencies so can be a more light weight and does not require running python through another software system.</p><p>
<bold>A: We thank the reviewer for this point. We have revised the relevant section from &#x02018;Current Workflows&#x02019; as follows to better justify our statement.</bold>
<list list-type="bullet"><list-item><p>
<italic toggle="yes">Despite its broad adoption,
<bold>batch data processing with Slicer requires custom scripting in Python, to be executed in the Slicer ecosystem. Rather than simply installing a package within their Python environment, users must install the Slicer application and add any other dependencies, not provided by Slicer, into the application environment. As a result, machine learning projects relying on Python for data preprocessing will have their code fragmented across multiple environments&#x02013;the Slicer environment for data processing, and another Python environment for data analysis and machine learning.</bold>
</italic>
</p></list-item></list>
</p></body></sub-article></sub-article></article>