<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id><journal-id journal-id-type="publisher-id">sensors</journal-id><journal-title-group><journal-title>Sensors (Basel, Switzerland)</journal-title></journal-title-group><issn pub-type="epub">1424-8220</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmid">40006466</article-id><article-id pub-id-type="pmc">PMC11860017</article-id><article-id pub-id-type="doi">10.3390/s25041238</article-id><article-id pub-id-type="publisher-id">sensors-25-01238</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>GeoIoU-SEA-YOLO: An Advanced Model for Detecting Unsafe Behaviors on Construction Sites</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jia</surname><given-names>Xuejun</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01238" ref-type="aff">1</xref><xref rid="af2-sensors-25-01238" ref-type="aff">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhou</surname><given-names>Xiaoxiong</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role><xref rid="af1-sensors-25-01238" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Shi</surname><given-names>Zhihan</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01238" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Qi</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-sensors-25-01238" ref-type="aff">1</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9566-7865</contrib-id><name><surname>Zhang</surname><given-names>Guangming</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-sensors-25-01238" ref-type="aff">1</xref><xref rid="c1-sensors-25-01238" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Hamza</surname><given-names>Ben</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-sensors-25-01238"><label>1</label>College of Electrical Engineering and Control Science, Nanjing Tech University, Nanjing 211816, China; <email>jxj@njtech.edu.cn</email> (X.J.); <email>zhouxx@njtech.edu.cn</email> (X.Z.); <email>szh@njtech.edu.cn</email> (Z.S.); <email>xenon@njtech.edu.cn</email> (Q.X.)</aff><aff id="af2-sensors-25-01238"><label>2</label>China Construction Second Engineering Bureau Co., Ltd., Beijing 100160, China</aff><author-notes><corresp id="c1-sensors-25-01238"><label>*</label>Correspondence: <email>zgm@njtech.edu.cn</email></corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>2</month><year>2025</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2025</year></pub-date><volume>25</volume><issue>4</issue><elocation-id>1238</elocation-id><history><date date-type="received"><day>12</day><month>1</month><year>2025</year></date><date date-type="rev-recd"><day>16</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>2</month><year>2025</year></date></history><permissions><copyright-statement>&#x000a9; 2025 by the authors.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Unsafe behaviors on construction sites are a major cause of accidents, highlighting the need for effective detection and prevention. Traditional methods like manual inspections and video surveillance often lack real-time performance and comprehensive coverage, making them insufficient for diverse and complex site environments. This paper introduces GeoIoU-SEA-YOLO, an enhanced object detection model integrating the Geometric Intersection over Union (GeoIoU) loss function and Structural-Enhanced Attention (SEA) mechanism to improve accuracy and real-time detection. GeoIoU enhances bounding box regression by considering geometric characteristics, excelling in the detection of small objects, occlusions, and multi-object interactions. SEA combines channel and multi-scale spatial attention, dynamically refining feature map weights to focus on critical features. Experiments show that GeoIoU-SEA-YOLO outperforms YOLOv3, YOLOv5s, YOLOv8s, and SSD, achieving high precision (mAP@0.5 = 0.930), recall, and small object detection in complex scenes, particularly for unsafe behaviors like missing safety helmets, vests, or smoking. Ablation studies confirm the independent and combined contributions of GeoIoU and SEA to performance gains, providing a reliable solution for intelligent safety management on construction sites.</p></abstract><kwd-group><kwd>unsafe behavior detection</kwd><kwd>GeoIoU</kwd><kwd>Structural-Enhanced Attention (SEA)</kwd><kwd>YOLO</kwd><kwd>construction safety</kwd></kwd-group><funding-group><award-group><funding-source>China Civil Engineering Society Research Project</funding-source><award-id>CCES2024KT10</award-id></award-group><award-group><funding-source>China State Construction Engineering Corporation Technology R&#x00026;D Program</funding-source><award-id>CSCEC-2024-Q-74</award-id></award-group><award-group><funding-source>Chongqing Construction Science and Technology Program</funding-source><award-id>CQCS2024-8-5</award-id></award-group><funding-statement>This work was partly funded by the China Civil Engineering Society Research Project (CCES2024KT10), the China State Construction Engineering Corporation Technology R&#x00026;D Program (CSCEC-2024-Q-74), and the Chongqing Construction Science and Technology Program (CQCS2024-8-5).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-sensors-25-01238"><title>1. Introduction</title><p>Safety management on construction sites has always been a critical research area in the construction industry. The complex and dynamic nature of construction site environments, combined with the unpredictability of worker behaviors and the heavy reliance on machinery, has led to frequent safety incidents, such as falls, electric shocks, and mechanical injuries [<xref rid="B1-sensors-25-01238" ref-type="bibr">1</xref>,<xref rid="B2-sensors-25-01238" ref-type="bibr">2</xref>]. Statistics indicate that the majority of construction accidents are directly related to unsafe worker behaviors, such as failing to wear personal protective equipment (PPE) properly, entering hazardous areas without authorization, or engaging in improper operations. These incidents not only endanger workers&#x02019; lives, but also result in significant economic losses and societal impacts.</p><p>Traditional methods for construction site safety management, including manual inspections and video surveillance, suffer from poor real-time performance, low efficiency, and limited coverage, making them inadequate to meet the growing safety management demands of modern construction sites [<xref rid="B3-sensors-25-01238" ref-type="bibr">3</xref>,<xref rid="B4-sensors-25-01238" ref-type="bibr">4</xref>]. To address these issues, researchers have begun leveraging computer vision technologies to identify and intervene in unsafe behaviors through automated monitoring systems. In particular, the rapid development of object detection technology has provided technical support for construction site safety management.</p><p>Object detection technology, which involves identifying and localizing objects of interest within images, has become a cornerstone for monitoring unsafe behaviors on construction sites. Early methods, such as Region-based Convolutional Neural Networks (R-CNN) [<xref rid="B5-sensors-25-01238" ref-type="bibr">5</xref>] and single-stage detection models like SSD [<xref rid="B6-sensors-25-01238" ref-type="bibr">6</xref>], achieved a balance between detection accuracy and speed. However, due to the complexity of construction environments, including the prevalence of small objects and partial occlusions, the performance of existing methods remains limited in handling these challenges. In recent years, the YOLO (You Only Look Once) series of algorithms have gained widespread application due to their efficiency and robustness, demonstrating exceptional performance in tasks such as safety helmet detection [<xref rid="B7-sensors-25-01238" ref-type="bibr">7</xref>], reflective vest usage monitoring [<xref rid="B8-sensors-25-01238" ref-type="bibr">8</xref>], and the identification of other unsafe behaviors on construction sites. Moreover, deep learning-based approaches, such as convolutional neural networks (CNNs), have been employed to enhance detection accuracy and real-time monitoring capabilities. Zhang developed a CNN-based framework capable of automatically analyzing surveillance footage to identify unsafe behaviors and provide real-time alerts [<xref rid="B9-sensors-25-01238" ref-type="bibr">9</xref>]. Similarly, Yu introduced a skeleton-based motion analysis technique that effectively detects high-risk worker postures in real time [<xref rid="B10-sensors-25-01238" ref-type="bibr">10</xref>].</p><p>In addition to object detection, pose estimation is another critical technology for detecting unsafe behaviors. By locating and analyzing the key skeletal points of workers, pose estimation methods can identify improper actions or abnormal postures during high-altitude operations. For example, studies have developed detection systems combining deep learning and pose analysis, capable of analyzing workers&#x02019; postures in real time and identifying potential hazardous behaviors [<xref rid="B11-sensors-25-01238" ref-type="bibr">11</xref>,<xref rid="B12-sensors-25-01238" ref-type="bibr">12</xref>]. A recent deep learning framework, DeepSafety, has been introduced to detect unsafe behaviors using both object detection and pose estimation. Liu and Ying proposed an approach that integrates YOLOv3 for worker detection with long short-term memory (LSTM) networks to analyze movement patterns and identify hazardous actions [<xref rid="B13-sensors-25-01238" ref-type="bibr">13</xref>]. Despite its unique advantages in behavior analysis, pose estimation still faces challenges in scenarios involving multi-object detection and occlusions.</p><p>In summary, unsafe behavior monitoring systems based on object detection and pose estimation have proven to be effective tools for improving safety management on construction sites. Numerous studies in recent years have not only validated the effectiveness of these technologies, but also laid a solid foundation for the realization of real-time and efficient construction site safety monitoring systems [<xref rid="B14-sensors-25-01238" ref-type="bibr">14</xref>,<xref rid="B15-sensors-25-01238" ref-type="bibr">15</xref>]. In addition, behavior-based safety (BBS) approaches are increasingly being enhanced by artificial intelligence and computer vision technologies. Fang reviewed advancements in computer vision for BBS and highlighted the challenges associated with occlusions, varied lighting conditions, and viewpoint changes [<xref rid="B16-sensors-25-01238" ref-type="bibr">16</xref>]. Furthermore, Huang examined the role of incentive structures in reducing unsafe behaviors, demonstrating that a combination of positive reinforcement and penalties can effectively promote safer workplace practices [<xref rid="B17-sensors-25-01238" ref-type="bibr">17</xref>].</p><p>To address the diverse and complex requirements of unsafe behavior recognition in construction site environments, researchers have continually worked to improve object detection frameworks and their key components, aiming to enhance detection accuracy and real-time performance. However, challenges such as small objects, occlusions, and multi-object interactions remain significant in construction scenarios, limiting the applicability of the existing methods. To overcome these difficulties and meet growing demands, this paper proposes a construction site unsafe behavior detection model that integrates the Geometric IoU (GeoIoU) loss function with Structural-Enhanced Attention (SEA). Built on the YOLO framework, the model focuses on optimizing two critical aspects: bounding box regression and multi-scale feature extraction. The goal is to achieve efficient and accurate detection of various unsafe behaviors in complex construction environments.</p><p>This paper&#x02019;s primary innovations and contributions are summarized as follows:<list list-type="simple"><list-item><label>(<bold>1</bold>)</label><p><bold>Proposal of an efficient object detection model for unsafe behavior detection on construction sites: GeoIoU-SEA-YOLO</bold></p></list-item></list></p><p>An efficient object detection model, GeoIoU-SEA-YOLO, is proposed based on the classical YOLO framework. By integrating the GeoIoU loss function and Structural-Enhanced Attention (SEA), the model achieves a balance between detection accuracy and real-time performance in complex construction environments. Compared with the existing methods, GeoIoU-SEA-YOLO demonstrates superior adaptability and robustness in handling small objects, partial occlusions, and multi-object scenarios.</p><list list-type="simple"><list-item><label>(<bold>2</bold>)</label><p>
<bold>Design and implementation of a novel GeoIoU loss function</bold>
</p></list-item></list><p>Unlike traditional IoU and its variants (GIoU, DIoU, CIoU), GeoIoU places greater emphasis on the geometric distribution relationships of bounding boxes, providing more stable and accurate gradient signals for bounding box regression. In experiments on unsafe behavior detection in construction sites, GeoIoU significantly improves the localization accuracy of the model.</p><list list-type="simple"><list-item><label>(<bold>3</bold>)</label><p>
<bold>Introduction of Structural-Enhanced Attention (SEA)</bold>
</p></list-item></list><p>The SEA mechanism integrates static/dynamic channel attention and multi-scale spatial attention through residual connections and a learnable fusion strategy, enhancing the model&#x02019;s focus on critical features. Dual-Scale Attention addresses, to some extent, the insufficient feature representation caused by the diversity of and scale variations in targets in construction site scenarios.</p><list list-type="simple"><list-item><label>(<bold>4</bold>)</label><p>
<bold>Construction and public release of a multi-source synthetic dataset for unsafe behavior detection</bold>
</p></list-item></list><p>A multi-source synthetic dataset of unsafe behaviors on construction sites was constructed and thoroughly evaluated. Although comprehensive testing across multiple datasets and extreme environments was not performed, experimental results show that GeoIoU-SEA-YOLO outperforms various benchmark models in recognizing typical unsafe behaviors, such as detecting safety helmets, reflective vests, and certain violations. This evaluation demonstrates the model&#x02019;s practical application potential in terms of both accuracy and inference speed.</p></sec><sec id="sec2-sensors-25-01238"><title>2. Related Work</title><p>In recent years, object detection technology has been extensively studied, with the YOLO series of algorithms becoming a focal point of research due to their efficiency and robustness. Additionally, advancements in Intersection over Union (IoU) improvements and the introduction of attention mechanisms have further enhanced object detection performance. This section provides an overview of the related work, focusing on three key areas: major variants of YOLO, IoU improvements, and attention mechanisms.</p><sec id="sec2dot1-sensors-25-01238"><title>2.1. Main Variants of YOLO</title><p>The YOLO series of algorithms have evolved rapidly since their inception, with key variants including Tiny-YOLO, Anchor-Free YOLO, and Transformer-YOLO, among others. Tiny-YOLO is a lightweight variant designed for embedded systems and real-time detection, capable of delivering satisfactory performance in low-resource environments, albeit at the cost of reduced detection accuracy [<xref rid="B18-sensors-25-01238" ref-type="bibr">18</xref>]. Anchor-Free YOLO eliminates the traditional anchor box mechanism in favor of center point prediction, effectively reducing computational demands in complex scenarios and improving the detection of occluded objects [<xref rid="B19-sensors-25-01238" ref-type="bibr">19</xref>]. Moreover, Transformer-YOLO leverages the global feature modeling capabilities of Transformers, demonstrating superior performance in unstructured data and long-range dependency scenarios [<xref rid="B20-sensors-25-01238" ref-type="bibr">20</xref>].</p><p>Recently, road scene understanding datasets such as Rsud20k [<xref rid="B21-sensors-25-01238" ref-type="bibr">21</xref>] have driven the development of object detection approaches tailored for autonomous driving. These datasets provide diverse and challenging real-world conditions, including occlusions, varying lighting conditions, and dynamic objects, which necessitate robust and efficient detection models. Several recent detection models, such as DETR [<xref rid="B22-sensors-25-01238" ref-type="bibr">22</xref>] and DINO, have shown promising results by leveraging self-attention mechanisms to enhance long-range dependencies and improve detection performance in such complex scenarios. Additionally, the latest versions of YOLO, such as YOLOv7 and YOLOv8, have introduced optimizations in architectural design and training strategies, further improving efficiency and accuracy in real-time applications.</p><p>Dense-YOLO is a variant tailored for dense object detection tasks, significantly enhancing detection performance in crowded scenes through improved feature fusion modules [<xref rid="B21-sensors-25-01238" ref-type="bibr">21</xref>]. Meanwhile, YOLOx optimizes target matching through a dynamic label assignment mechanism and incorporates efficient loss functions, excelling in multi-object detection tasks [<xref rid="B22-sensors-25-01238" ref-type="bibr">22</xref>]. Another notable variant is YOLOv5, an unofficial version of the YOLO series that has gained widespread application due to its outstanding performance and open-source nature. YOLOv5 introduces innovations in network architecture and training methods, achieving a better balance between accuracy, speed, and model size. It also supports various post-processing techniques, such as Non-Maximum Suppression (NMS) and Soft-NMS, effectively improving detection precision. <xref rid="sensors-25-01238-f001" ref-type="fig">Figure 1</xref> illustrates the network structure of YOLOv5, highlighting its optimizations, which make it a pivotal tool in modern object detection. The diagram depicts the flow of data through the network, starting from the input layer (on the left) and progressing through various components. These include the convolutional layers, which extract features from the input image, as well as the fully connected layers responsible for detecting objects. The output layer (on the far right) produces the final object detection results. Notably, <xref rid="sensors-25-01238-f001" ref-type="fig">Figure 1</xref> also emphasizes the introduction of key architectural components, such as the modified residual blocks and advanced up-sampling strategies, which contribute to YOLOv5&#x02019;s superior performance, especially in real-time detection scenarios.</p></sec><sec id="sec2dot2-sensors-25-01238"><title>2.2. Advances in IoU-Based Research</title><p>Intersection over Union (IoU) is a crucial metric in object detection for measuring the overlap between predicted and ground truth bounding boxes. However, IoU performs poorly for objects with significant aspect ratio differences. To address this limitation, several enhanced variants of IoU have been proposed. DIoU incorporates the distance between the center points of bounding boxes, optimizing localization accuracy and significantly accelerating model convergence [<xref rid="B23-sensors-25-01238" ref-type="bibr">23</xref>]. CIoU builds on DIoU by additionally considering the similarities in the aspect ratios of bounding boxes, improving performance in small object detection tasks [<xref rid="B24-sensors-25-01238" ref-type="bibr">24</xref>]. SIoU targets issues in Non-Maximum Suppression for complex scenarios, optimizing the allocation strategy of bounding boxes and substantially reducing false positive and false negative rates [<xref rid="B25-sensors-25-01238" ref-type="bibr">25</xref>]. Adaptive IoU further addresses challenges in multi-scale object detection by dynamically adjusting the weight of the IoU loss function. This approach is particularly effective for detection tasks in complex backgrounds [<xref rid="B26-sensors-25-01238" ref-type="bibr">26</xref>]. These advancements have significantly enhanced the robustness of object detection models in complex environments.</p></sec><sec id="sec2dot3-sensors-25-01238"><title>2.3. Applications of Attention Mechanisms</title><p>In recent years, attention mechanisms have been extensively applied in object detection tasks, significantly enhancing the representation of critical features through dynamic weight allocation. The Squeeze-and-Excitation (SE) module leverages global pooling to compute feature weights, amplifying the representation of essential channels and delivering outstanding performance in multi-object detection [<xref rid="B27-sensors-25-01238" ref-type="bibr">27</xref>]. The Convolutional Block Attention Module (CBAM) combines channel and spatial attention, achieving notable improvements in precision in the detection of small objects [<xref rid="B28-sensors-25-01238" ref-type="bibr">28</xref>]. Triplet Attention introduces a contextual enhancement mechanism that extends interactions across both channel and spatial dimensions, enabling the precise detection of occluded objects [<xref rid="B29-sensors-25-01238" ref-type="bibr">29</xref>]. Transformer Attention, by employing a multi-head attention mechanism, captures global features with remarkable effectiveness, particularly in scenarios involving long-range dependencies [<xref rid="B20-sensors-25-01238" ref-type="bibr">20</xref>]. Lightweight modules such as Efficient Channel Attention (ECA) further enhance feature extraction capabilities while maintaining minimal computational cost, making them exceptionally suitable for real-time detection tasks [<xref rid="B30-sensors-25-01238" ref-type="bibr">30</xref>]. These advancements underscore the critical role of attention mechanisms in improving the accuracy, robustness, and efficiency of modern object detection systems.</p></sec></sec><sec id="sec3-sensors-25-01238"><title>3. Method</title><sec id="sec3dot1-sensors-25-01238"><title>3.1. Overall Architecture</title><p>To comprehensively improve the detection accuracy and real-time performance of unsafe behaviors on construction sites, this paper proposes the GeoIoU-SEA-YOLO model. The architecture of this model is designed to enhance both feature extraction and bounding box regression, addressing challenges such as small object detection, occlusions, and multi-object detection in complex construction site environments. The network flow of the SEA-YOLO model is shown in <xref rid="sensors-25-01238-f002" ref-type="fig">Figure 2</xref>.</p><p>The model follows a multi-stage workflow, starting from the input image and ending with the final detection results. The first step in the process involves feature extraction, where the input image (size 640 &#x000d7; 640) passes through a series of CBS layers (Convolution, Batch Normalization, and Activation). These layers extract low-level features, such as edges and textures, from the input image. Next, the extracted features undergo feature fusion, where information from multiple feature maps is combined. This process is enhanced by the SEA (Structural-Enhanced Attention) module, which focuses on the most critical regions of the feature map, allowing the model to adaptively allocate attention based on spatial and channel-specific characteristics. The SEA module enhances the model&#x02019;s ability to capture both local and global dependencies, which is particularly important in scenarios where objects may be partially occluded or of varying sizes. Following feature fusion, the model proceeds with down-sampling and up-sampling operations, which help maintain the balance between computational efficiency and the model&#x02019;s ability to capture fine-grained features. The up-sampled features are then passed to the next stage, where further fusion and attention mechanisms are applied. After the final feature fusion, the processed features are passed to the YOLO head, which predicts the bounding boxes and class labels for the detected objects. The output layer then provides the final detection results, which include the predicted bounding boxes and associated confidence scores.</p><p>Throughout the process, the GeoIoU loss function is applied during training to refine the bounding box predictions. The GeoIoU loss function improves the localization accuracy by considering geometric relationships such as the Manhattan distance, Euclidean distance, and aspect ratio similarity between the predicted and ground truth bounding boxes. This approach ensures more stable and accurate gradient signals during the optimization process.</p></sec><sec id="sec3dot2-sensors-25-01238"><title>3.2. GeoIoU Loss Function</title><p>In object detection tasks, the design of the loss function is critical to the performance of the model. The traditional Intersection over Union (IoU), widely used in various object detection models as a metric to measure the overlap between predicted and ground truth bounding boxes, has proven effective. However, for the specific application scenario of detecting unsafe behaviors on construction sites, the traditional IoU exhibits significant limitations when dealing with small objects, partial occlusions, and multi-object interactions in complex environments. To address these challenges, this paper proposes a novel GeoIoU loss function, which aims to enhance the precision and stability of bounding box regression by incorporating geometric characteristics.</p><sec id="sec3dot2dot1-sensors-25-01238"><title>3.2.1. Motivation for the Design of GeoIoU</title><p>The traditional IoU loss function primarily relies on the overlap between predicted and ground truth bounding boxes to guide model optimization. However, in the context of detecting unsafe behaviors on construction sites, several key challenges arise:<list list-type="order"><list-item><p><bold>Difficulty in Detecting Small Objects</bold>: Many unsafe behaviors on construction sites involve detecting small objects, such as safety helmets and reflective vests. In these cases, the traditional IoU struggles because the small size of the bounding boxes amplifies the impact of localization errors, causing significant fluctuations in the IoU values. This, in turn, affects the stability of model training and slows convergence;</p></list-item><list-item><p><bold>Partial Occlusion and Multi-Object Interaction</bold>: The complexity of construction site environments often results in partial occlusions or overlaps between objects. In such scenarios, the intersection area between bounding boxes may decrease significantly, leading to reduced IoU values. This can misguide the optimization process, making it difficult for the predicted bounding boxes to accurately cover the true objects;</p></list-item><list-item><p><bold>Neglect of Geometric Relationships</bold>: Traditional IoU focuses solely on the degree of overlap and neglects the geometric distribution between bounding boxes. For example, two bounding boxes may have similar IoU values but differ significantly in relative position or shape, directly impacting detection performance.</p></list-item></list></p><p>To address these issues, GeoIoU introduces the geometric characteristics of bounding boxes, comprehensively considering the distance between their centers and their shape similarity. By providing more precise and stable loss signals, GeoIoU aims to enhance the model&#x02019;s detection performance in complex construction site environments.</p></sec><sec id="sec3dot2dot2-sensors-25-01238"><title>3.2.2. Mathematical Definition of GeoIoU</title><p>The GeoIoU loss function extends the traditional IoU by incorporating the geometric distribution information of bounding boxes to provide richer optimization signals. Its mathematical definition is as follows:</p><p>Let the predicted bounding box be <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the ground truth bounding box be <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the center coordinates of the bounding box, and <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent its width and height, respectively. First, the traditional IoU is computed as follows:<disp-formula id="FD1-sensors-25-01238"><label>(1)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Next, we introduce the Manhattan distance <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> between the center points of the bounding boxes, the Euclidean center distance <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and the aspect ratio similarity <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD2-sensors-25-01238"><label>(2)</label><mml:math id="mm10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn><mml:mo>=</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The definition of <italic toggle="yes">GeoIoU</italic> is as follows:<disp-formula id="FD3-sensors-25-01238"><label>(3)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mi>o</mml:mi><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>
where</p><p><inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are weight parameters used to balance the influence of center distance and shape difference.</p><p><inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is a normalization factor, where <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> are the average widths and heights of the predicted and ground truth bounding boxes, respectively.</p><p><inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a small constant introduced to prevent division by zero.</p><p>In this formula,</p><p><inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the Manhattan distance between the center points of the predicted and ground truth bounding boxes, reflecting the degree of horizontal and vertical offset.</p><p><inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the Euclidean distance between the center points of the predicted and ground truth bounding boxes, providing a more precise quantification of spatial displacement.</p><p><inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the squared difference in aspect ratios between the bounding boxes, measuring the shape discrepancy between the predicted and ground truth bounding boxes.</p><p>A normalization factor <inline-formula><mml:math id="mm21" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is introduced to ensure that the geometric distances and shape differences have consistent impacts across varying scales. By incorporating geometric distances <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, center distances <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and shape similarity <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mo>_</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, GeoIoU extends beyond overlap-based metrics to include spatial relationships and shape consistency in the loss calculation. This design enables GeoIoU to provide more stable and accurate gradient signals during bounding box regression, significantly improving localization accuracy, particularly for small objects and in complex scenarios. The components of the formula are illustrated in <xref rid="sensors-25-01238-f003" ref-type="fig">Figure 3</xref>.</p></sec><sec id="sec3dot2dot3-sensors-25-01238"><title>3.2.3. Comparison of GeoIoU with Other IoU Variants</title><p>In recent years, with the advancement of object detection technologies, several IoU variants have been proposed to address the limitations of traditional IoU in specific application scenarios. Compared to GIoU, DIoU, and CIoU, GeoIoU offers the following advantages:<list list-type="order"><list-item><p><bold>Comprehensive Geometric Relationships</bold>: GeoIoU not only considers the overlap of bounding boxes, but also incorporates center distance and shape similarity, providing more comprehensive optimization signals. This enables GeoIoU to better guide model optimization for object localization, especially in scenarios involving small objects and partial occlusions;</p></list-item><list-item><p><bold>Stable Gradient Signals</bold>: By integrating geometric features, GeoIoU provides more stable and accurate gradient signals during bounding box regression, facilitating faster model convergence and precise localization;</p></list-item><list-item><p><bold>Adaptation to Complex Scenarios</bold>: GeoIoU is designed with the complexity and diversity of construction site environments in mind. It demonstrates greater robustness and adaptability in handling multi-object interactions, partial occlusions, and small object detection.</p></list-item></list></p><p>In addition to detecting unsafe behaviors on construction sites, GeoIoU is applicable to other scenarios requiring high-precision bounding box regression, such as small object detection, dense object detection, and multi-object interaction detection in complex scenarios.</p><p>In summary, GeoIoU enhances the precision and stability of bounding box regression by integrating geometric features, building on the foundation of traditional IoU and its variants. Notably, in the challenging application of detecting unsafe behaviors on construction sites, GeoIoU demonstrates significant advantages, effectively addressing challenges in small object detection and localization in complex scenarios.</p></sec></sec><sec id="sec3dot3-sensors-25-01238"><title>3.3. Structural-Enhanced Attention (SEA)</title><p>To further enhance the feature representation capabilities of the model in complex construction site environments, this paper proposes the Structural-Enhanced Attention (SEA) mechanism. By combining channel attention, self-attention, and spatial attention, the SEA mechanism dynamically adjusts the importance of different regions in the feature maps, thereby strengthening the model&#x02019;s focus on critical features. The following sections provide a detailed introduction to the design principles, modular composition, and workflow of SEA.</p><sec id="sec3dot3dot1-sensors-25-01238"><title>3.3.1. Design Philosophy of SEA</title><p>In the task of detecting unsafe behaviors on construction sites, targets exhibit diverse characteristics and are often surrounded by complex backgrounds. This poses challenges in feature extraction and target recognition. Traditional attention mechanisms improve feature representation but still face limitations, particularly in multi-scale feature fusion and selection under complex conditions. To address these, SEA introduces the following optimizations:<list list-type="order"><list-item><p>Channel Attention with Dynamic Weight Generation: SEA begins by capturing channel-wise information at different scales through a combination of global average pooling and max pooling. This allows the model to consider both coarse and fine-grained features. A 1D convolution is then used instead of a fully connected layer to model local channel relationships more effectively. This convolution-based approach provides more efficient learning of inter-channel dependencies. Additionally, SEA introduces dynamic weight generation through learnable fusion parameters <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which balance static and dynamic components, allowing for flexible and adaptive channel attention;</p></list-item><list-item><p>Integration of Self-Attention Mechanism: The self-attention module in SEA captures global dependencies across the entire feature map by considering relationships between all positions. This is particularly useful in scenarios where partial occlusion or complex backgrounds occur. Instead of relying only on localized features, the self-attention mechanism enables the model to focus on crucial areas by considering the full context of the image, helping to better recognize occluded or hidden targets;</p></list-item><list-item><p>Multi-Scale Spatial Attention: SEA employs multi-scale spatial attention using depth-wise separable convolutions to extract spatial features at different scales. This approach enables the model to detect objects of varying sizes by capturing features at multiple resolutions. Unlike single-scale mechanisms, this multi-scale method enhances the model&#x02019;s robustness in detecting both small and large objects in diverse settings;</p></list-item><list-item><p>Residual Bottleneck Module: To ensure effective information flow and prevent gradient vanishing, SEA integrates a <bold>residual bottleneck module</bold>. This module introduces residual connections that help maintain important features throughout the network, ensuring efficient backpropagation and better overall model performance, especially in deeper architectures.</p></list-item></list></p></sec><sec id="sec3dot3dot2-sensors-25-01238"><title>3.3.2. Components of SEA</title><p>The SEA mechanism is primarily composed of the following modules:<list list-type="order"><list-item><p>Channel Attention Module</p></list-item></list></p><p>The channel attention module is designed to dynamically adjust the weights of each channel to emphasize critical features. The specific steps are as follows:</p><p>Pooling Operations:<disp-formula id="FD4-sensors-25-01238"><label>(4)</label><mml:math id="mm26" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>AdaptiveAvgPool</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mn>1,1</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>AdaptiveMaxPool</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mn>1,1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the input feature map, <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the batch size, and <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the number of channels.</p><p>1D Convolution and Fusion:<disp-formula id="FD6-sensors-25-01238"><label>(5)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Conv</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">d</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mo>(</mml:mo><mml:mi>BN</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">D</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mn>1,1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">d</mml:mi><mml:mi mathvariant="bold">y</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">m</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mo>(</mml:mo><mml:mi>BN</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">D</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">v</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mn>1,1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">d</mml:mi><mml:mi mathvariant="bold">y</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">m</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mn>1,1</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a learnable fusion parameter and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represents the Sigmoid activation function.</p><p>Feature Re-scaling:<disp-formula id="FD7-sensors-25-01238"><label>(6)</label><mml:math id="mm33" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">h</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">e</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm34" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>&#x02297;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes element-wise multiplication.</p><list list-type="simple"><list-item><label>2.</label><p>Self-Attention Module</p></list-item></list><p>The self-attention module enhances feature representation by capturing global dependencies through the computation of relationships between all positions in the feature map. The specific steps are outlined below:</p><p>
<bold>Feature Transformation</bold>
<disp-formula id="FD8-sensors-25-01238">
<label>(7)</label>
<mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b8;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
</disp-formula>
<disp-formula id="FD9-sensors-25-01238">
<label>(8)</label>
<mml:math id="mm36" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>
</disp-formula>
<disp-formula id="FD10-sensors-25-01238">
<label>(9)</label>
<mml:math id="mm37" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math>
</disp-formula>
</p><p>Here, <inline-formula><mml:math id="mm38" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>&#x02032;</mml:mo><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm40" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm42" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> represent convolution operations.</p><p>Attention Weight Calculation:<disp-formula id="FD11-sensors-25-01238"><label>(10)</label><mml:math id="mm43" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup></mml:msqrt></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mo stretchy="false">[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Feature Aggregation and Output:<disp-formula id="FD12-sensors-25-01238"><label>(11)</label><mml:math id="mm44" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">T</mml:mi></mml:mrow></mml:msup><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:mi>Reshape</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:mi>Conv</mml:mi><mml:mn>2</mml:mn><mml:mi mathvariant="normal">d</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold">out</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mo>&#x022c5;</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="mm45" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>&#x003b3;</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is a learnable parameter.</p><list list-type="simple"><list-item><label>3.</label><p>Spatial Attention Module</p></list-item></list><p>The spatial attention module leverages multi-scale depth-wise separable convolutions to dynamically adjust the importance of various spatial positions in the feature map. The specific steps are outlined below:</p><p>Pooling Operations:<disp-formula id="FD13-sensors-25-01238"><label>(12)</label><mml:math id="mm46" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">g</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi>keepdim</mml:mi><mml:mo>=</mml:mo><mml:mi>True</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi>keepdim</mml:mi><mml:mo>=</mml:mo><mml:mi>True</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Multi-Scale Convolution and Fusion:<disp-formula id="FD15-sensors-25-01238"><label>(13)</label><mml:math id="mm47" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Concat</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">g</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>DepthwiseSeparableConv</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>DepthwiseSeparableConv</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>DepthwiseSeparableConv</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">n</mml:mi><mml:mi mathvariant="bold">c</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>[</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Feature Re-scaling:<disp-formula id="FD16-sensors-25-01238"><label>(14)</label><mml:math id="mm48" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>&#x02297;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mi mathvariant="bold">p</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mi mathvariant="bold">i</mml:mi><mml:mi mathvariant="bold">a</mml:mi><mml:mi mathvariant="bold">l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><list list-type="simple"><list-item><label>4.</label><p>
<bold>Residual Bottleneck</bold>
</p></list-item></list><p>The residual bottleneck module introduces residual connections to maintain the flow of information and prevent gradient vanishing, thereby further optimizing feature representation. The specific steps are as follows:</p><p>
<bold>Feature Transformation:</bold>
<disp-formula id="FD17-sensors-25-01238">
<label>(15)</label>
<mml:math id="mm49" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">bottleneck</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mn>1</mml:mn><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">bottleneck</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ReLU</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mn>2</mml:mn><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi>bottleneck</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math>
</disp-formula>
</p><p>Residual Connection:<disp-formula id="FD18-sensors-25-01238"><label>(16)</label><mml:math id="mm50" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">u</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">bottleneck</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">u</mml:mi><mml:mi mathvariant="bold">t</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">U</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">o</mml:mi><mml:mi mathvariant="bold">u</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The workflow of the SEA mechanism is shown in <xref rid="sensors-25-01238-f004" ref-type="fig">Figure 4</xref>.</p></sec></sec></sec><sec id="sec4-sensors-25-01238"><title>4. Experiments</title><sec id="sec4dot1-sensors-25-01238"><title>4.1. Experimental Environment and Datasets</title><p>All experiments and tests in this study were conducted under a consistent platform environment and training strategy, with the specific hardware and software configurations outlined in <xref rid="sensors-25-01238-t001" ref-type="table">Table 1</xref>.</p><p>The datasets used are classified into three primary categories: safety helmets, reflective vests, and cigarettes. However, four detection labels are defined: safety helmets, humans, reflective vests, and smoking. The &#x0201c;human&#x0201d; label is included in all three datasets, and since there is already sufficient training data for this category, no additional datasets were incorporated.</p><p>The complete dataset comprises 19,961 images. To evaluate the model&#x02019;s generalization performance, the dataset is divided into a training set containing 14,835 images and a test set containing 5126 images. While most data originate from publicly available datasets, considering the focus on detecting unsafe behaviors on construction sites, additional datasets collected by our team from actual construction sites were included. Examples of typical images in this dataset are presented in <xref rid="sensors-25-01238-f005" ref-type="fig">Figure 5</xref>.</p></sec><sec id="sec4dot2-sensors-25-01238"><title>4.2. Comparison with Mainstream Methods</title><p>To intuitively showcase the effectiveness of our improved model, we employed the Gradient Weighted Class Activation Mapping (Grad-CAM) method to visualize the key focus areas of the model. Comparisons were made with other mainstream detection methods, including YOLOv3, YOLOv5s, YOLOv8s, and SSD. This visualization highlights the attention regions of different models when processing the same image, shown as <xref rid="sensors-25-01238-f006" ref-type="fig">Figure 6</xref>.</p><p>The experimental results demonstrate that our proposed method achieves a more precise focus on target objects compared to other mainstream algorithms. This advantage is particularly notable in the detection of small objects, such as safety helmets, reflective vests, and cigarettes, where our model exhibits stronger robustness. Additionally, in complex backgrounds, such as construction site scenarios with numerous distractions and challenging environmental factors, our model consistently maintains high detection accuracy. Grad-CAM visualizations further validate that our model not only enhances detection accuracy, but also has an improved ability to focus on the fine details of target objects. These results confirm the improved method&#x02019;s adaptability and reliability in complex environments, making it highly effective for practical applications.</p><p><xref rid="sensors-25-01238-t002" ref-type="table">Table 2</xref> compares the predictive performance of the proposed GeoIoU-SEA-YOLO method with several mainstream algorithms, including YOLOv3, YOLOv5s, YOLOv8s, and SSD, on the specified dataset. The evaluation metrics include precision, recall, and mAP@0.5. The results demonstrate that GeoIoU-SEA-YOLO outperforms the other models across all metrics. In particular, the model achieves an AP of 0.957 for the <bold>Smoking</bold> target, highlighting its robustness in detecting complex targets such as small or occluded objects. Moreover, GeoIoU-SEA-YOLO achieves an mAP@0.5 of 0.930, significantly surpassing YOLOv3, YOLOv5s, YOLOv8s, and SSD, reflecting its superior overall detection performance. With a precision of 0.870 and a recall of 0.800, GeoIoU-SEA-YOLO demonstrates a strong balance between high detection accuracy and good recall capability. Although YOLOv8s is close in terms of AP and mAP@0.5, it lags behind GeoIoU-SEA-YOLO in detecting complex targets. SSD and YOLOv3 show comparatively lower performances, underscoring their limitations in handling challenging detection scenarios.</p><p>These findings affirm that GeoIoU-SEA-YOLO possesses significant advantages in object detection tasks, particularly in scenarios requiring high precision and robustness in complex environments.</p><p>From the above results, it can be seen that the model proposed in this paper can effectively improve the accuracy of detection on construction sites. However, in the actual construction site, the monitoring system needs to process a large amount of video data in real time. Therefore, the computational complexity and operation efficiency of the model are particularly critical. <xref rid="sensors-25-01238-t003" ref-type="table">Table 3</xref> lists the relevant parameters and data comparisons.</p><p>As can be seen from the above table, the GeoIoU-SEA-YOLO model is between YOLOv8s and SSD in terms of the number of parameters and FLOPs, and the inference speed is slightly lower than YOLOv8s, but significantly higher than Faster R-CNN. This shows that GeoIoU-SEA-YOLO optimizes computational complexity and operating efficiency while maintaining high accuracy, and is suitable for application scenarios that require real-time processing.</p><p>In the GeoIoU-SEA-YOLO model, we introduced the GeoIoU loss function and the Structural-Enhanced Attention (SEA) mechanism to improve the detection accuracy of the model in complex environments, especially when dealing with challenges such as small objects, occlusions, and multi-object interactions. Although these enhancements improve the accuracy of the model, they also increase the number of model parameters to a certain extent, and lead to a longer inference time. The purpose of introducing the GeoIoU loss function is to enhance the model&#x02019;s positioning ability in small object detection, especially in complex scenes. GeoIoU provides a more stable and accurate gradient signal by considering the geometric relationship of the bounding box (such as center distance, Euclidean distance, and aspect ratio similarity), thereby improving the detection performance of the model. At the same time, the SEA mechanism further improves the model&#x02019;s attention to key areas by dynamically adjusting the attention of channels and spaces in the feature map. Especially when the complex background or the target is partially occluded, SEA can effectively improve the detection accuracy.</p><p>Although the introduction of these technologies has increased the computational complexity of the model, the experimental results show that the GeoIoU-SEA-YOLO model has achieved a good balance between inference speed and accuracy. We verified in the experiment that the model can run at a speed of 28.6 frames per second, which fully meets the requirements of real-time monitoring. In addition, in the future, the model can be further optimized through model pruning, quantization, and other technologies to reduce the consumption of computing resources, so as to better meet the needs of actual deployment.</p></sec><sec id="sec4dot3-sensors-25-01238"><title>4.3. Ablation Study</title><p>A series of ablation experiments were designed to comprehensively evaluate the contributions of individual components in the proposed GeoIoU-SEA-YOLO model. By systematically removing or substituting key modules, the experiments quantify the impact of each component on the model&#x02019;s overall performance. Furthermore, the independent contributions of the GeoIoU loss function and the SEA module were analyzed, providing further evidence of their effectiveness and the value they add to the model.</p><sec><title>Independent Impact of GeoIoU and SEA</title><p>To investigate the independent contributions of the GeoIoU loss function and the SEA mechanism to the model&#x02019;s performance, two experiments were designed:<list list-type="simple"><list-item><label>(a)</label><p><bold>Introducing GeoIoU Loss Function Only:</bold></p></list-item></list></p><p>This experiment keeps the SEA module unchanged while replacing the traditional IoU loss function with the GeoIoU loss function. The comparison of performance before and after the introduction of GeoIoU quantifies its impact on bounding box regression.</p><list list-type="simple"><list-item><label>(b)</label><p>
<bold>Introducing SEA Mechanism Only:</bold>
</p></list-item></list><p>This experiment retains the traditional IoU loss function while incorporating the SEA mechanism into the YOLO framework. Through a comparison with the baseline model that excludes SEA, the experiment evaluates SEA&#x02019;s contribution to feature extraction and attention allocation.</p><p>The results in <xref rid="sensors-25-01238-t004" ref-type="table">Table 4</xref> demonstrate the significant contributions of the GeoIoU loss function and the SEA mechanism to model performance. The baseline YOLOv5s model achieved an mAP@0.5 of 0.822. When the GeoIoU loss function was incorporated, the mAP@0.5 improved to 0.829, highlighting its enhancement of precision. Introducing the SEA module further increased the mAP@0.5 to 0.855, emphasizing its role in improving recall. When both components were integrated into the GeoIoU-SEA-YOLO model, the mAP@0.5 reached 0.870, with a precision of 0.800 and a recall of 0.930, demonstrating the best overall performance. These results confirm the synergistic effect of the two modules in enhancing the model&#x02019;s capabilities.</p><p><xref rid="sensors-25-01238-f007" ref-type="fig">Figure 7</xref> provides a comparison of the detection results from the ablation experiments. The baseline YOLOv5s model detected most targets, but exhibited limitations in certain scenarios. For instance, in the first example, the baseline model failed to detect a small target due to its size. In the second example, a small target (cigarette) was not recognized, and an occluded safety helmet was missed. Although the model correctly identified larger targets, its confidence scores were relatively low.</p><p>Replacing the IoU function with GeoIoU significantly improved confidence scores, but small object detection challenges remained. Introducing the SEA mechanism enabled the model to accurately detect small targets, even those occupying minimal pixel areas, while also markedly improving the confidence levels for all detected objects. These results validate the effectiveness of the SEA module and its ability to enhance detection performance in complex scenarios.</p><p><xref rid="sensors-25-01238-f008" ref-type="fig">Figure 8</xref> illustrates the comparison of confidence loss and classification loss between the proposed GeoIoU-SEA-YOLO method and the baseline model. The results show that GeoIoU-SEA-YOLO consistently achieves lower loss values throughout the training process. In particular, the proposed method converges faster during the early stages and maintains more stable loss values in the later stages, with the performance gap persisting across the entire training duration.</p><p>This demonstrates the advantages of GeoIoU-SEA-YOLO in terms of optimization strategies, model architecture, and loss function design. These enhancements enable GeoIoU-SEA-YOLO to improve model performance effectively, potentially leading to higher detection accuracy and improved regression results. Consequently, GeoIoU-SEA-YOLO provides a more reliable and robust solution for object detection tasks, particularly in scenarios that demand high precision and reliability.</p><p>Although the GeoIoU-SEA-YOLO model has achieved good performance in the unsafe behavior detection task, there are still some limitations. First, the model is trained based on a specific construction site dataset, which is not yet publicly available, so the generalization ability of the model may be limited. Especially when applied to other different scenarios or datasets, the performance may be reduced. Second, although the introduction of the GeoIoU loss function and the SEA mechanism helps to improve detection accuracy, it also leads to an increase in the computational complexity and number of parameters of the model, which may affect the deployment and operation of the model on devices with limited computing resources. Nevertheless, the model can still maintain a high inference speed in real-time processing and is suitable for most real-time monitoring needs. Finally, although the model performs well in complex background and small object detection, the detection performance may be reduced when dealing with extremely complex environments (such as scenarios with multiple targets densely appearing or severely occluded). Therefore, future research can explore model optimization, dataset diversification, and customized adjustments in specific scenarios to further improve the model&#x02019;s performance in different environments.</p></sec></sec></sec><sec sec-type="conclusions" id="sec5-sensors-25-01238"><title>5. Conclusions</title><p>This paper proposes GeoIoU-SEA-YOLO, an object detection model that integrates the Geometric IoU (GeoIoU) loss function and the Structural-Enhanced Attention (SEA) mechanism, designed for detecting unsafe behaviors in complex construction site scenarios. By optimizing the geometric distribution characteristics of bounding box regression and enhancing the model&#x02019;s ability to extract critical features, this model effectively addresses challenges such as small object detection, target occlusion, and multi-object interactions commonly encountered in construction environments.</p><p>The experimental results demonstrate that GeoIoU-SEA-YOLO outperforms mainstream detection methods, including YOLOv3, YOLOv5s, YOLOv8s, and SSD, across metrics such as precision, recall, and mAP@0.5. Particularly, it exhibits superior robustness and adaptability in detecting small objects such as safety helmets, reflective vests, and smoking behaviors.</p><p>Additionally, ablation experiments validate the independent contributions and synergistic effects of the GeoIoU loss function and the SEA module in improving model performance. GeoIoU significantly enhances the stability and accuracy of bounding box localization by introducing geometric features, while the SEA mechanism dynamically allocates feature weights, boosting the model&#x02019;s feature representation capabilities in complex scenarios.</p><p>In summary, GeoIoU-SEA-YOLO not only provides an efficient and reliable solution for detecting unsafe behaviors on construction sites, but also offers new insights into addressing small object detection and multi-object interaction challenges in complex environments. Future research could focus on optimizing the lightweight design of the model to improve its deployment capabilities in resource-constrained environments and explore its generalizability and scalability across more complex scenarios and diverse tasks.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors would like to express their gratitude to all those who helped them during the writing of this paper. The authors would like to thank the reviewers for their valuable comments and suggestions.</p></ack><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, X.J. and X.Z.; methodology, X.J. and Z.S.; software, X.Z. and Q.X.; validation, X.J., Z.S. and Q.X.; formal analysis, X.Z. and Z.S.; investigation, Z.S.; resources, G.Z. and X.J.; data curation, X.J., X.Z. and Q.X.; writing&#x02014;original draft preparation, Z.S. and Q.X.; writing&#x02014;review and editing, X.J., Z.S. and Q.X.; visualization, X.J. and X.Z.; supervision, G.Z.; project administration, G.Z.; funding acquisition, G.Z. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Informed Consent Statement</title><p>Informed consent was obtained from all subjects involved in the study.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The original contributions presented in the study are included in the article, further inquiries can be directed to the corresponding authors.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>Author Xuejun Jia was employed by the company China Construction Second Engineering Bureau Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-sensors-25-01238"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nguyen</surname><given-names>N.T.</given-names></name>
<name><surname>Bui</surname><given-names>D.Q.T.</given-names></name>
<name><surname>Tran</surname><given-names>C.N.N.</given-names></name>
<name><surname>Tran</surname><given-names>D.H.</given-names></name>
</person-group><article-title>Improved detection network model based on YOLOv5 for warning safety in construction sites</article-title><source>Int. J. Constr. Manag.</source><year>2024</year><volume>24</volume><fpage>1007</fpage><lpage>1017</lpage></element-citation></ref><ref id="B2-sensors-25-01238"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Zhao</surname><given-names>L.</given-names></name>
<name><surname>Geng</surname><given-names>J.</given-names></name>
<name><surname>Hu</surname><given-names>J.</given-names></name>
<name><surname>Ma</surname><given-names>L.</given-names></name>
<name><surname>Zhao</surname><given-names>Z.</given-names></name>
</person-group><article-title>Unsafe behaviour detection with the improved YOLOv5 model</article-title><source>IET Cyber-Phys. Syst. Theory Appl.</source><year>2024</year><volume>9</volume><fpage>87</fpage><lpage>98</lpage></element-citation></ref><ref id="B3-sensors-25-01238"><label>3.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Q.</given-names></name>
<name><surname>Han</surname><given-names>F.</given-names></name>
</person-group><article-title>Research on an improved yolov5s algorithm for detecting helmets on construction sites</article-title><source>Proceedings of the 7th International Conference on Electronic Information Technology and Computer Engineering</source><conf-loc>Xiamen, China</conf-loc><conf-date>20&#x02013;22 October 2023</conf-date></element-citation></ref><ref id="B4-sensors-25-01238"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>Q.</given-names></name>
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Qiu</surname><given-names>Y.</given-names></name>
<name><surname>Zheng</surname><given-names>W.</given-names></name>
</person-group><article-title>Object detection for construction waste based on an improved YOLOv5 model</article-title><source>Sustainability</source><year>2022</year><volume>15</volume><elocation-id>681</elocation-id><pub-id pub-id-type="doi">10.3390/su15010681</pub-id></element-citation></ref><ref id="B5-sensors-25-01238"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Ren</surname><given-names>S.</given-names></name>
<name><surname>He</surname><given-names>K.</given-names></name>
<name><surname>Girshick</surname><given-names>R.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Faster R-CNN: Towards real-time object detection with region proposal networks</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2016</year><volume>39</volume><fpage>1137</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id><pub-id pub-id-type="pmid">27295650</pub-id>
</element-citation></ref><ref id="B6-sensors-25-01238"><label>6.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>W.</given-names></name>
<name><surname>Anguelov</surname><given-names>D.</given-names></name>
<name><surname>Erhan</surname><given-names>D.</given-names></name>
<name><surname>Erhan</surname><given-names>D.</given-names></name>
<name><surname>Szegedy</surname><given-names>C.</given-names></name>
<name><surname>Reed</surname><given-names>S.</given-names></name>
<name><surname>Fu</surname><given-names>C.</given-names></name>
<name><surname>Berg</surname><given-names>A.C.</given-names></name>
</person-group><article-title>SSD: Single Shot MultiBox Detector</article-title><source>Proceedings of the 14th European Conference on Computer Vision (ECCV)</source><conf-loc>Amsterdam, The Netherlands</conf-loc><conf-date>11&#x02013;14 October 2016</conf-date></element-citation></ref><ref id="B7-sensors-25-01238"><label>7.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liang</surname><given-names>X.</given-names></name>
<name><surname>Jiang</surname><given-names>T.</given-names></name>
<name><surname>Fu</surname><given-names>Q.</given-names></name>
<name><surname>Wang</surname><given-names>Q.</given-names></name>
</person-group><article-title>Multi-Object Detection and Classification in Construction Sites Based on YOLOv5</article-title><source>Proceedings of the 5th International Conference on Video, Signal and Image Processing</source><conf-loc>Harbin, China</conf-loc><conf-date>24&#x02013;26 November 2023</conf-date></element-citation></ref><ref id="B8-sensors-25-01238"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chang</surname><given-names>R.</given-names></name>
<name><surname>Li</surname><given-names>B.</given-names></name>
<name><surname>Dang</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>C.</given-names></name>
<name><surname>Pan</surname><given-names>A.</given-names></name>
<name><surname>Yang</surname><given-names>Y.</given-names></name>
</person-group><article-title>Real-time intelligent detection system for illegal wearing of on-site power construction worker based on edge-YOLO and low-cost edge devices</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><elocation-id>8287</elocation-id><pub-id pub-id-type="doi">10.3390/app13148287</pub-id></element-citation></ref><ref id="B9-sensors-25-01238"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>S.</given-names></name>
</person-group><article-title>Research on the Identification Method of Unsafe Behavior of Construction Workers Based on Deep Learning</article-title><source>Eng. Adv.</source><year>2024</year><volume>4</volume><fpage>33</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.26855/ea.2024.02.006</pub-id></element-citation></ref><ref id="B10-sensors-25-01238"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yu</surname><given-names>Y.</given-names></name>
<name><surname>Guo</surname><given-names>H.</given-names></name>
<name><surname>Ding</surname><given-names>Q.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Skitmore</surname><given-names>M.</given-names></name>
</person-group><article-title>An experimental study of real-time identification of construction workers&#x02019; unsafe behaviors</article-title><source>Autom. Constr.</source><year>2017</year><volume>82</volume><fpage>193</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.autcon.2017.05.002</pub-id></element-citation></ref><ref id="B11-sensors-25-01238"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Mei</surname><given-names>X.</given-names></name>
<name><surname>Xu</surname><given-names>F.</given-names></name>
<name><surname>Zhang</surname><given-names>Z.</given-names></name>
<name><surname>Tao</surname><given-names>Y.</given-names></name>
</person-group><article-title>Unsafe behavior identification on construction sites by combining computer vision and knowledge graph&#x02013;based reasoning</article-title><source>Eng. Constr. Archit. Manag.</source><year>2024</year><pub-id pub-id-type="doi">10.1108/ECAM-05-2024-0622</pub-id></element-citation></ref><ref id="B12-sensors-25-01238"><label>12.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Mahmud</surname><given-names>S.S.</given-names></name>
<name><surname>Islam</surname><given-names>M.A.</given-names></name>
<name><surname>Ritu</surname><given-names>K.J.</given-names></name>
<name><surname>Hasan</surname><given-names>M.</given-names></name>
<name><surname>Kobayashi</surname><given-names>Y.</given-names></name>
<name><surname>Mohibullah</surname><given-names>M.</given-names></name>
</person-group><article-title>Safety Helmet Detection of Workers in Construction Site using YOLOv8</article-title><source>Proceedings of the 26th International Conference on Computer and Information Technology (ICCIT)</source><conf-loc>Cox&#x02019;s Bazar, Bangladesh</conf-loc><conf-date>13&#x02013;15 December 2023</conf-date></element-citation></ref><ref id="B13-sensors-25-01238"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>C.C.</given-names></name>
<name><surname>Ying</surname><given-names>J.J.C.</given-names></name>
</person-group><article-title>DeepSafety: A deep learning framework for unsafe behaviors detection of steel activity in construction projects</article-title><source>Proceedings of the 2020 International Computer Symposium (ICS)</source><conf-loc>Tainan, Taiwan</conf-loc><conf-date>17&#x02013;19 December 2020</conf-date><fpage>135</fpage><lpage>140</lpage></element-citation></ref><ref id="B14-sensors-25-01238"><label>14.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>S.</given-names></name>
<name><surname>Lu</surname><given-names>G.</given-names></name>
<name><surname>Jiang</surname><given-names>Z.</given-names></name>
<name><surname>Huang</surname><given-names>L.</given-names></name>
</person-group><article-title>Improved YOLOv5 network model and application in safety helmet detection</article-title><source>Proceedings of the IEEE International Conference on Intelligence and Safety for Robotics (ISR)</source><conf-loc>Tokoname, Japan</conf-loc><conf-date>10 May 2021</conf-date></element-citation></ref><ref id="B15-sensors-25-01238"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ge</surname><given-names>P.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
</person-group><article-title>An automatic detection approach for wearing safety helmets on construction site based on YOLOv5</article-title><source>Proceedings of the 11th Data Driven Control and Learning Systems Conference (DDCLS)</source><conf-loc>Chengdu, China</conf-loc><conf-date>3&#x02013;5 August 2022</conf-date></element-citation></ref><ref id="B16-sensors-25-01238"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wan</surname><given-names>C.</given-names></name>
<name><surname>Pang</surname><given-names>Y.</given-names></name>
<name><surname>Lan</surname><given-names>S.</given-names></name>
</person-group><article-title>Overview of YOLO Object Detection Algorithm</article-title><source>Int. J. Comput. Inf. Technol.</source><year>2022</year><volume>2</volume><fpage>11</fpage><pub-id pub-id-type="doi">10.56028/ijcit.1.2.11</pub-id></element-citation></ref><ref id="B17-sensors-25-01238"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fang</surname><given-names>W.</given-names></name>
<name><surname>Love</surname><given-names>P.E.D.</given-names></name>
<name><surname>Luo</surname><given-names>H.</given-names></name>
<name><surname>Ding</surname><given-names>L.</given-names></name>
</person-group><article-title>Computer vision for behaviour-based safety in construction: A review and future directions</article-title><source>Adv. Eng. Inform.</source><year>2020</year><volume>43</volume><fpage>100980</fpage><pub-id pub-id-type="doi">10.1016/j.aei.2019.100980</pub-id></element-citation></ref><ref id="B18-sensors-25-01238"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Huang</surname><given-names>J.</given-names></name>
<name><surname>Wu</surname><given-names>Y.</given-names></name>
<name><surname>Han</surname><given-names>Y.</given-names></name>
<name><surname>Yin</surname><given-names>Y.</given-names></name>
<name><surname>Gao</surname><given-names>G.</given-names></name>
<name><surname>Chen</surname><given-names>H.</given-names></name>
</person-group><article-title>An evolutionary game-theoretic analysis of construction workers&#x02019; unsafe behavior: Considering incentive and risk loss</article-title><source>Front. Public Health</source><year>2022</year><volume>10</volume><elocation-id>991994</elocation-id><pub-id pub-id-type="doi">10.3389/fpubh.2022.991994</pub-id><pub-id pub-id-type="pmid">36176527</pub-id>
</element-citation></ref><ref id="B19-sensors-25-01238"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Liu</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>C.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
</person-group><article-title>Using Anchor-Free Object Detectors to Detect Surface Defects</article-title><source>Processes</source><year>2024</year><volume>12</volume><elocation-id>2817</elocation-id><pub-id pub-id-type="doi">10.3390/pr12122817</pub-id></element-citation></ref><ref id="B20-sensors-25-01238"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Bai</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Zhang</surname><given-names>R.</given-names></name>
<name><surname>Wang</surname><given-names>C.</given-names></name>
<name><surname>Guo</surname><given-names>J.</given-names></name>
<name><surname>Gao</surname><given-names>X.</given-names></name>
</person-group><article-title>RKformer: Runge-Kutta Transformer with Random-Connection Attention for Infrared Small Target Detection</article-title><source>Proceedings of the 30th ACM International Conference on Multimedia</source><conf-loc>Lisboa, Portugal</conf-loc><conf-date>10&#x02013;14 October 2022</conf-date></element-citation></ref><ref id="B21-sensors-25-01238"><label>21.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Zunair</surname><given-names>H.</given-names></name>
<name><surname>Khan</surname><given-names>S.</given-names></name>
<name><surname>Hamza</surname><given-names>A.B.</given-names></name>
</person-group><article-title>Rsud20K: A Dataset for Road Scene Understanding in Autonomous Driving</article-title><source>Proceedings of the 2024 IEEE International Conference on Image Processing (ICIP)</source><conf-loc>Abu Dhabi, United Arab Emirates</conf-loc><conf-date>27&#x02013;30 October 2024</conf-date><fpage>708</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1109/ICIP51287.2024.10648203</pub-id></element-citation></ref><ref id="B22-sensors-25-01238"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Fang</surname><given-names>R.</given-names></name>
<name><surname>Gao</surname><given-names>P.</given-names></name>
<name><surname>Zhou</surname><given-names>A.</given-names></name>
<name><surname>Cai</surname><given-names>Y.</given-names></name>
<name><surname>Liu</surname><given-names>S.</given-names></name>
<name><surname>Dai</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
</person-group><article-title>FeatAug-DETR: Enriching One-to-Many Matching for DETRs With Feature Augmentation</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2024</year><volume>46</volume><fpage>6402</fpage><lpage>6415</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2024.3381961</pub-id></element-citation></ref><ref id="B23-sensors-25-01238"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Jiang</surname><given-names>F.</given-names></name>
<name><surname>Zhu</surname><given-names>F.</given-names></name>
<name><surname>Ren</surname><given-names>L.</given-names></name>
</person-group><article-title>Enhanced Multi-Target Detection in Complex Traffic Using an Improved YOLOv8 with SE Attention, DCN_C2f, and SIoU</article-title><source>World Electr. Veh. J.</source><year>2024</year><volume>15</volume><elocation-id>586</elocation-id><pub-id pub-id-type="doi">10.3390/wevj15120586</pub-id></element-citation></ref><ref id="B24-sensors-25-01238"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Yan</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Yan</surname><given-names>M.</given-names></name>
<name><surname>Diao</surname><given-names>W.</given-names></name>
<name><surname>Sun</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
</person-group><article-title>IoU-adaptive deformable R-CNN: Make full use of IoU for multi-class object detection in remote sensing imagery</article-title><source>Remote Sens.</source><year>2019</year><volume>11</volume><elocation-id>286</elocation-id><pub-id pub-id-type="doi">10.3390/rs11030286</pub-id></element-citation></ref><ref id="B25-sensors-25-01238"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Sheng</surname><given-names>H.</given-names></name>
<name><surname>Cai</surname><given-names>S.</given-names></name>
<name><surname>Zhao</surname><given-names>N.</given-names></name>
<name><surname>Deng</surname><given-names>B.</given-names></name>
<name><surname>Huang</surname><given-names>J.</given-names></name>
<name><surname>Hua</surname><given-names>X.</given-names></name>
<name><surname>Zhao</surname><given-names>M.</given-names></name>
<name><surname>Lee</surname><given-names>G.H.</given-names></name>
</person-group><article-title>Rethinking IoU-based optimization for single-stage 3D object detection</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Tel Aviv, Israel</conf-loc><conf-date>23&#x02013;27 October 2022</conf-date></element-citation></ref><ref id="B26-sensors-25-01238"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Yue</surname><given-names>X.</given-names></name>
<name><surname>Meng</surname><given-names>L.</given-names></name>
</person-group><article-title>A comprehensive survey on object detection YOLO</article-title><source>Proceedings</source><year>2023</year><volume>1613</volume><fpage>73</fpage></element-citation></ref><ref id="B27-sensors-25-01238"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>W.</given-names></name>
<name><surname>Liu</surname><given-names>K.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Cheng</surname><given-names>F.</given-names></name>
</person-group><article-title>Object detection based on an adaptive attention mechanism</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><elocation-id>11307</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-67529-x</pub-id><pub-id pub-id-type="pmid">32647299</pub-id>
</element-citation></ref><ref id="B28-sensors-25-01238"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hao</surname><given-names>S.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Ma</surname><given-names>X.</given-names></name>
<name><surname>Sun</surname><given-names>S.</given-names></name>
<name><surname>Wen</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>J.</given-names></name>
</person-group><article-title>Foreign object detection in coal mine conveyor belt based on CBAM-YOLOv5</article-title><source>China Coal Soc.</source><year>2022</year><volume>47</volume><fpage>4149</fpage><lpage>4158</lpage></element-citation></ref><ref id="B29-sensors-25-01238"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Gan</surname><given-names>W.</given-names></name>
<name><surname>Gu</surname><given-names>K.</given-names></name>
<name><surname>Geng</surname><given-names>J.</given-names></name>
<name><surname>Qiu</surname><given-names>C.</given-names></name>
<name><surname>Yang</surname><given-names>R.</given-names></name>
<name><surname>Wang</surname><given-names>H.</given-names></name>
<name><surname>Hu</surname><given-names>X.</given-names></name>
</person-group><article-title>A Novel Three-Stage Collision-Risk Pre-Warning Model for Construction Vehicles and Workers</article-title><source>Buildings</source><year>2024</year><volume>14</volume><elocation-id>2324</elocation-id><pub-id pub-id-type="doi">10.3390/buildings14082324</pub-id></element-citation></ref><ref id="B30-sensors-25-01238"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Shen</surname><given-names>Z.</given-names></name>
<name><surname>Zhang</surname><given-names>M.</given-names></name>
<name><surname>Zhao</surname><given-names>H.</given-names></name>
<name><surname>Yi</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
</person-group><article-title>Efficient attention: Attention with linear complexities</article-title><source>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</source><conf-loc>Waikoloa, HI, USA</conf-loc><conf-date>3&#x02013;8 January 2021</conf-date></element-citation></ref></ref-list></back><floats-group><fig position="float" id="sensors-25-01238-f001"><label>Figure 1</label><caption><p>YOLOv5 network structure diagram.</p></caption><graphic xlink:href="sensors-25-01238-g001" position="float"/></fig><fig position="float" id="sensors-25-01238-f002"><label>Figure 2</label><caption><p>The architecture of the SEA-YOLO network.</p></caption><graphic xlink:href="sensors-25-01238-g002" position="float"/></fig><fig position="float" id="sensors-25-01238-f003"><label>Figure 3</label><caption><p>Schematic diagram of the GeoIoU structure.</p></caption><graphic xlink:href="sensors-25-01238-g003" position="float"/></fig><fig position="float" id="sensors-25-01238-f004"><label>Figure 4</label><caption><p>Workflow of the Structural-Enhanced Attention (SEA) mechanism.</p></caption><graphic xlink:href="sensors-25-01238-g004" position="float"/></fig><fig position="float" id="sensors-25-01238-f005"><label>Figure 5</label><caption><p>Example of datasets.</p></caption><graphic xlink:href="sensors-25-01238-g005" position="float"/></fig><fig position="float" id="sensors-25-01238-f006"><label>Figure 6</label><caption><p>Comparison of Grad-CAM visualizations across different mainstream algorithms. (<bold>a</bold>&#x02013;<bold>c</bold>) represent different picture examples.</p></caption><graphic xlink:href="sensors-25-01238-g006" position="float"/></fig><fig position="float" id="sensors-25-01238-f007"><label>Figure 7</label><caption><p>Comparison of detection results in ablation experiments.</p></caption><graphic xlink:href="sensors-25-01238-g007a" position="float"/><graphic xlink:href="sensors-25-01238-g007b" position="float"/><graphic xlink:href="sensors-25-01238-g007c" position="float"/></fig><fig position="float" id="sensors-25-01238-f008"><label>Figure 8</label><caption><p>Comparison of loss function graph with baseline method.</p></caption><graphic xlink:href="sensors-25-01238-g008" position="float"/></fig><table-wrap position="float" id="sensors-25-01238-t001"><object-id pub-id-type="pii">sensors-25-01238-t001_Table 1</object-id><label>Table 1</label><caption><p>Hardware, software environment, and training strategy for model training.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td rowspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Hardware</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Intel(R)Core(TM)i7-13700KF (Intel, Santa Clara, CA, USA)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RAM</td><td align="center" valign="middle" rowspan="1" colspan="1">32 GB</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GPU</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NVIDIA GeForce RTX 4070 (NVIDIA, Santa Clara, CA, USA)</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Software</td><td align="center" valign="middle" rowspan="1" colspan="1">CUDA</td><td align="center" valign="middle" rowspan="1" colspan="1">12.7</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Python</td><td align="center" valign="middle" rowspan="1" colspan="1">3.8.0</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Torch</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.3.1</td></tr><tr><td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Training strategy</td><td align="center" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">Adam</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="center" valign="middle" rowspan="1" colspan="1">16</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Learning rate</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.001</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01238-t002"><object-id pub-id-type="pii">sensors-25-01238-t002_Table 2</object-id><label>Table 2</label><caption><p>Comparison of results between the proposed method and mainstream algorithms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Model</th><th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">AP (%)</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Precision</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Recall</th><th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">mAP@0.5 (%)</th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Hat</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Person</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Reflective</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Smoking</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv3</td><td align="center" valign="middle" rowspan="1" colspan="1">0.811</td><td align="center" valign="middle" rowspan="1" colspan="1">0.846</td><td align="center" valign="middle" rowspan="1" colspan="1">0.860</td><td align="center" valign="middle" rowspan="1" colspan="1">0.837</td><td align="center" valign="middle" rowspan="1" colspan="1">0.784</td><td align="center" valign="middle" rowspan="1" colspan="1">0.742</td><td align="center" valign="middle" rowspan="1" colspan="1">0.839</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5s</td><td align="center" valign="middle" rowspan="1" colspan="1">0.854</td><td align="center" valign="middle" rowspan="1" colspan="1">0.882</td><td align="center" valign="middle" rowspan="1" colspan="1">0.891</td><td align="center" valign="middle" rowspan="1" colspan="1">0.870</td><td align="center" valign="middle" rowspan="1" colspan="1">0.822</td><td align="center" valign="middle" rowspan="1" colspan="1">0.765</td><td align="center" valign="middle" rowspan="1" colspan="1">0.874</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv8s</td><td align="center" valign="middle" rowspan="1" colspan="1">0.881</td><td align="center" valign="middle" rowspan="1" colspan="1">0.910</td><td align="center" valign="middle" rowspan="1" colspan="1">0.922</td><td align="center" valign="middle" rowspan="1" colspan="1">0.903</td><td align="center" valign="middle" rowspan="1" colspan="1">0.851</td><td align="center" valign="middle" rowspan="1" colspan="1">0.787</td><td align="center" valign="middle" rowspan="1" colspan="1">0.904</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">SSD</td><td align="center" valign="middle" rowspan="1" colspan="1">0.826</td><td align="center" valign="middle" rowspan="1" colspan="1">0.861</td><td align="center" valign="middle" rowspan="1" colspan="1">0.873</td><td align="center" valign="middle" rowspan="1" colspan="1">0.850</td><td align="center" valign="middle" rowspan="1" colspan="1">0.803</td><td align="center" valign="middle" rowspan="1" colspan="1">0.750</td><td align="center" valign="middle" rowspan="1" colspan="1">0.852</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GeoIou-SEA-YOLO</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.894</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.933</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.935</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.957</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.870</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.800</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.930</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01238-t003"><object-id pub-id-type="pii">sensors-25-01238-t003_Table 3</object-id><label>Table 3</label><caption><p>Comparison of different model parameters and efficiency.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Parameters (M)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FLOPs (G)</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">FPS</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">YOLOv8s</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">11.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.6</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">105</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SSD</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">31.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Faster R-CNN</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">41.0</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">160</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">18</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GeoIou-SEA-YOLO</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.5</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.8</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">35</td></tr></tbody></table></table-wrap><table-wrap position="float" id="sensors-25-01238-t004"><object-id pub-id-type="pii">sensors-25-01238-t004_Table 4</object-id><label>Table 4</label><caption><p>Ablation study results on the independent impact of GeoIoU and SEA.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Model Configuration</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">GeoIoU</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">SEA</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">mAP@0.5</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Baseline</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.822</td><td align="center" valign="middle" rowspan="1" colspan="1">0.765</td><td align="center" valign="middle" rowspan="1" colspan="1">0.874</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5s + GeoIoU</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.829</td><td align="center" valign="middle" rowspan="1" colspan="1">0.774</td><td align="center" valign="middle" rowspan="1" colspan="1">0.862</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">YOLOv5s + SEA</td><td align="center" valign="middle" rowspan="1" colspan="1">
</td><td align="center" valign="middle" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" rowspan="1" colspan="1">0.855</td><td align="center" valign="middle" rowspan="1" colspan="1">0.792</td><td align="center" valign="middle" rowspan="1" colspan="1">0.921</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">GeoIoU-SEA-YOLO</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&#x0221a;</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.870</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.800</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.930</td></tr></tbody></table></table-wrap></floats-group></article>